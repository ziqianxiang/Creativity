Published as a conference paper at ICLR 2022
References
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of
convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving
low-bit quantization through learnable offsets and better initialization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696-697,
2020.
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. High-capacity expert binary networks.
arXiv preprint arXiv:2010.03558, 2020.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave gaussian quantization. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 5918-5926, 2017.
Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical
framework for low-bitwidth training of deep neural networks. arXiv preprint arXiv:2010.14298,
2020.
Xi Chen, Xiaolin Hu, Hucheng Zhou, and Ningyi Xu. Fxpnet: Training a deep convolutional neural
network in fixed-point representation. In 2017 International Joint Conference on Neural Networks
(IJCNN), pp. 2494-2501. IEEE, 2017.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan,
and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks.
arXiv preprint arXiv:1805.06085, 2018.
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks
for efficient inference. In ICCV Workshops, pp. 3009-3018, 2019.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna
Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al. Pushing the limits of narrow
precision inferencing at cloud scale with microsoft floating point. Advances in Neural Information
Processing Systems, 33, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian
aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 293-302, 2019.
Lukas Enderich, Fabian Timm, Lars Rosenbaum, and Wolfram Burgard. Fix-net: pure fixed-point
representation of deep neural networks. 2019a.
Lukas Enderich, Fabian Timm, Lars Rosenbaum, and Wolfram Burgard. Learning multimodal
fixed-point weights using gradient descent. arXiv preprint arXiv:1907.07220, 2019b.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph
Hassoun. Near-lossless post-training quantization of deep neural networks via a piecewise linear
approximation. arXiv preprint arXiv:2002.00104, pp. 4, 2020a.
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H
Hassoun. Post-training piecewise linear quantization for deep neural networks. In European
Conference on Computer Vision, pp. 69-86. Springer, 2020b.
10
Published as a conference paper at ICLR 2022
Yonggan Fu, Haoran You, Yang Zhao, Yue Wang, Chaojian Li, Kailash Gopalakrishnan, Zhangyang
Wang, and Yingyan Lin. Fractrain: Fractionally squeezing bit savings both temporally and spatially
for efficient dnn training. arXiv preprint arXiv:2012.13113, 2020.
Yonggan Fu, Han Guo, Meng Li, Xin Yang, Yining Ding, Vikas Chandra, and Yingyan Lin. Cpt:
Efficient deep neural network training via cyclic precision. arXiv preprint arXiv:2101.09868, 2021.
Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell Nahmias. Dynamic precision analog computing for
neural networks. arXiv preprint arXiv:2102.06365, 2021.
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
A survey of quantization methods for efficient neural network inference. arXiv preprint
arXiv:2103.13630, 2021.
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and
Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks.
In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 4852-4861,
2019.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Rishabh Goyal, Joaquin Vanschoren, Victor Van Acht, and StePhan Nijssen. Fixed-Point quantization
of convolutional neural networks for quantized inference on embedded Platforms. arXiv preprint
arXiv:2102.02147, 2021.
Nianhui Guo, JosePh Bethge, Haojin Yang, Kai Zhong, Xuefei Ning, ChristoPh Meinel, and
Yu Wang. Boolnet: Minimizing the energy consumPtion of binary neural networks. arXiv
preprint arXiv:2106.06991, 2021.
PhiliPP Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for
emPirical study of resource-efficient inference in convolutional neural networks. IEEE transactions
on neural networks and learning systems, 29(11):5784-5789, 2018.
Hai Victor Habi, Roy H Jennings, and Arnon Netzer. Hmq: Hardware friendly mixed Precision
quantization block for cnns. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow,
UK, AUgUSt 23-28, 2020, Proceedings, PartXXVI16, pp. 448-463. Springer, 2020.
Song Han, Huizi Mao, and William J Dally. DeeP comPression: ComPressing deeP neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Xiangyu He and Jian Cheng. Learning compression from limited unlabeled data. In Proceedings of
the European Conference on Computer Vision (ECCV), pp. 752-769, 2018.
Joshua Ho. Qualcomm details hexagon 680 dsp in snapdragon 820 accelerated imag-
ing, 2015. URL https://www.anandtech.com/show/9552/qualcomm-details-
hexagon- 680- dsp- in- snapdragon- 820- accelerated- imaging.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. Advances in neural information processing systems, 29, 2016.
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post
training neural quantization: Layer-wise calibration and integer programming. arXiv preprint
arXiv:2006.10518, 2020.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient
integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2704-2713, 2018.
11
Published as a conference paper at ICLR 2022
Sambhav R Jain, Albert Gural, Michael Wu, and Chris H Dick. Trained quantization thresholds for ac-
curate and efficient fixed-point inference of deep neural networks. arXiv preprint arXiv:1903.08066,
2019.
Qing Jin, Linjie Yang, and Zhenyu Liao. Adabits: Neural network quantization with adaptive bit-
widths. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 2146-2156, 2020a.
Qing Jin, Linjie Yang, Zhenyu Liao, and Xiaoning Qian. Neural network quantization with scale-
adjusted training. In BMVC, 2020b.
Qing Jin, Jian Ren, Oliver J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, and Sergey
Tulyakov. Teachers do more than teach: Compressing image-to-image models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13600-13611, 2021.
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-
only bert quantization. arXiv preprint arXiv:2101.01321, 2021.
Sungrae Kim and Hyun Kim. Zero-centered fixed-point quantization with iterative retraining for
deep convolutional neural network-based object detectors. IEEE Access, 9:20828-20839, 2021.
Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A
whitepaper. arXiv preprint arXiv:1806.08342, 2018.
Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah:
Mixed low-precision hardware & software co-design framework for dnns on the edge. arXiv
preprint arXiv:1908.02386, 2019.
Zhengang Li, Geng Yuan, Wei Niu, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan Shen, Zheng Zhan,
Zhenglun Kong, Qing Jin, et al. Npas: A compiler-aware framework of unified network pruning
and architecture search for beyond real-time mobile acceleration. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 14255-14266, 2021.
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization
for deep neural network acceleration: A survey. Neurocomputing, 461:370-403, 2021.
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy. Fixed point quantization of deep con-
volutional networks. In International conference on machine learning, pp. 2849-2858. PMLR,
2016.
Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin, Jian Ren, Jian Tang,
Sijia Liu, and Yanzhi Wang. Lottery ticket preserves weight correlation: Is it desirable or not? In
International Conference on Machine Learning, pp. 7011-7020. PMLR, 2021.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
international conference on computer vision, pp. 2736-2744, 2017.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. arXiv preprint arXiv:1810.05270, 2018.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization in
batch normalization. arXiv preprint arXiv:1809.00846, 2018.
Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang Chen,
Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time inference
on mobile devices. In European Conference on Computer Vision, pp. 629-645. Springer, 2020.
Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai
Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning ticket
really win the jackpot? Advances in Neural Information Processing Systems, 34, 2021a.
Yuexiao Ma, Taisong Jin, Xiawu Zheng, Yan Wang, Huixia Li, Guannan Jiang, Wei Zhang, and
Rongrong Ji. Ompq: Orthogonal mixed precision quantization. arXiv preprint arXiv:2109.07865,
2021b.
12
Published as a conference paper at ICLR 2022
Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille, and Jianchao Yang.
Atomnas: Fine-grained end-to-end neural architecture search. arXiv preprint arXiv:1912.09640,
2019.
Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn: Wide reduced-precision
networks. arXiv preprint arXiv:1709.01134, 2017.
Rahul Mishra, Hari Prabhat Gupta, and Tanima Dutta. A survey on deep neural network compression:
Challenges, overview, and solutions. arXiv preprint arXiv:2010.03954, 2020.
Norbert Mitschke, Michael Heizmann, Klaus-Henning Noffz, and Ralf Wittmann. A fixed-point
quantization technique for convolutional neural networks based on weight scaling. In 2019 IEEE
International Conference on Image Processing (ICIP), pp. 3836-3840. IEEE, 2019.
Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization
through weight equalization and bias correction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1325-1334, 2019.
Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tij-
men Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295,
2021.
Nvidia. Nvidia models, 2021. URL https://ngc.nvidia.com/catalog/models/
nvidia:resnet5 0_pyt_amp.
Sangyun Oh, Hyeonuk Sim, Sugil Lee, and Jongeun Lee. Automated log-scale quantization for
low-cost deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 742-751, 2021.
Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo. Weighted-entropy-based quantization for deep neu-
ral networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5456-5464, 2017.
Eunhyeok Park, Sungjoo Yoo, and Peter Vajda. Value-aware quantization for training and inference
of neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
580-595, 2018.
QCOM. Qualcomm® hexagontm dsp, 2019. URL https://developer.qualcomm.com/
software/hexagon-dsp-sdk.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European conference on computer
vision, pp. 525-542. Springer, 2016.
Oleg Semery. Pytorchcv library, 2021. URL https://pypi.org/project/pytorchcv/.
Steven W Smith et al. The scientist and engineer’s guide to digital signal processing. 1997.
Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. Degree-quant: Quantization-aware
training for graph neural networks. arXiv preprint arXiv:2008.05000, 2020.
Lizhe Tan and Jean Jiang. Digital signal processing: fundamentals and applications. Academic
Press, 2018.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467, 2020.
Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated
quantization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 8612-8620, 2019.
Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. Two-step
quantization for low-bit neural networks. In Proceedings of the IEEE Conference on computer
vision and pattern recognition, pp. 4376-4384, 2018.
13
Published as a conference paper at ICLR 2022
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization
for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602,
2020.
Linjie Yang and Qing Jin. Fracbits: Mixed precision quantization via fractional bit-widths. arXiv
preprint arXiv:2007.02017, 1, 2020.
Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and Chang Xu.
Searching for low-bit weights in quantized neural networks. arXiv preprint arXiv:2009.08695,
2020.
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang,
Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization.
In International Conference on Machine Learning, 2021.
Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng
Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training
framework on the edge. Advances in Neural Information Processing Systems, 34, 2021.
Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo,
Zidong Du, Tian Zhi, et al. Fixed-point back-propagation training. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 2330-2338, 2020.
Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui Xu. Dis-
tribution adaptive int8 quantization for training cnns. In Proceedings of the Thirty-Fifth AAAI
Conference on Artificial Intelligence, 2021a.
Sijie Zhao, Tao Yue, and Xuemei Hu. Distribution-aware adaptive multi-bit quantization. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9281-9290, 2021b.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization:
Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong Chen. Explicit loss-error-aware quantization
for low-bit deep neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 9426-9435, 2018.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and
Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969-1979, 2020.
14
Published as a conference paper at ICLR 2022
7	Appendix