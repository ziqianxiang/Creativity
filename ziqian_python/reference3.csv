论文题目,会议名称
 Post-training 4-bit quantization ofconvolution networks for rapid-deployment, arXiv preprint arXiv:1810
 Lsq+: Improvinglow-bit quantization through learnable offsets and better initialization, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
 High-capacity expert binary networks,arXiv preprint arXiv:2010
 Deep learning with low precision byhalf-wave gaussian quantization, In Proceedings of the IEEE conference on computer vision andpattern recognition
 A statisticalframework for low-bitwidth training of deep neural networks, arXiv preprint arXiv:2010
 Fxpnet: Training a deep convolutional neuralnetwork in fixed-point representation, In 2017 International Joint Conference on Neural Networks(IJCNN)
 Pact: Parameterized clipping activation for quantized neural networks,arXiv preprint arXiv:1805
 Low-bit quantization of neural networksfor efficient inference, In ICCV Workshops
 Binaryconnect: Training deep neuralnetworks with binary weights during propagations, In Advances in neural information processingsystems
 Pushing the limits of narrowprecision inferencing at cloud scale with microsoft floating point, Advances in Neural InformationProcessing Systems
 Imagenet: A large-scalehierarchical image database, In 2009 IEEE conference on computer vision and pattern recognition
 Hawq: Hessianaware quantization of neural networks with mixed-precision, In Proceedings of the IEEE/CVFInternational Conference on Computer Vision
 Learning multimodalfixed-point weights using gradient descent, arXiv preprint arXiv:1907
 Learned step size quantization, arXiv preprint arXiv:1902
 Near-lossless post-training quantization of deep neural networks via a piecewise linearapproximation, arXiv preprint arXiv:2002
 Post-training piecewise linear quantization for deep neural networks, In EuropeanConference on Computer Vision
 Fractrain: Fractionally squeezing bit savings both temporally and spatiallyfor efficient dnn training, arXiv preprint arXiv:2012
 Cpt:Efficient deep neural network training via cyclic precision, arXiv preprint arXiv:2101
 Dynamic precision analog computing forneural networks, arXiv preprint arXiv:2102
A survey of quantization methods for efficient neural network inference, arXiv preprintarXiv:2103
 Differentiable soft quantization: Bridging full-precision and low-bit neural networks,In Proceedings ofthe IEEE/CVF International Conference on Computer Vision
" Accurate, large minibatch sgd: Training imagenetin 1 hour", arXiv preprint arXiv:1706
 Fixed-Point quantizationof convolutional neural networks for quantized inference on embedded Platforms, arXiv preprintarXiv:2102
 Boolnet: Minimizing the energy consumPtion of binary neural networks, arXivpreprint arXiv:2106
 Ristretto: A framework foremPirical study of resource-efficient inference in convolutional neural networks, IEEE transactionson neural networks and learning systems
 Hmq: Hardware friendly mixed Precisionquantization block for cnns, In Computer Vision-ECCV 2020: 16th European Conference
" DeeP comPression: ComPressing deeP neural networkswith pruning, trained quantization and huffman coding", arXiv preprint arXiv:1510
 Learning compression from limited unlabeled data, In Proceedings ofthe European Conference on Computer Vision (ECCV)
" Qualcomm details hexagon 680 dsp in snapdragon 820 accelerated imag-ing, 2015", URL https://www
 Binarizedneural networks, Advances in neural information processing systems
 Improving posttraining neural quantization: Layer-wise calibration and integer programming, arXiv preprintarXiv:2006
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Trained quantization thresholds for ac-curate and efficient fixed-point inference of deep neural networks, arXiv preprint arXiv:1903
 Adabits: Neural network quantization with adaptive bit-widths, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Neural network quantization with scale-adjusted training, In BMVC
 Teachers do more than teach: Compressing image-to-image models, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 I-bert: Integer-only bert quantization, arXiv preprint arXiv:2101
 Zero-centered fixed-point quantization with iterative retraining fordeep convolutional neural network-based object detectors, IEEE Access
 Quantizing deep convolutional networks for efficient inference: Awhitepaper, arXiv preprint arXiv:1806
 Cheetah:Mixed low-precision hardware & software co-design framework for dnns on the edge, arXivpreprint arXiv:1908
 Npas: A compiler-aware framework of unified network pruningand architecture search for beyond real-time mobile acceleration, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Pruning and quantizationfor deep neural network acceleration: A survey, Neurocomputing
 Fixed point quantization of deep con-volutional networks, In International conference on machine learning
" Lottery ticket preserves weight correlation: Is it desirable or not? InInternational Conference on Machine Learning, pp", 7011-7020
 Learn-ing efficient convolutional networks through network slimming, In Proceedings of the IEEEinternational conference on computer vision
 Rethinking the value ofnetwork pruning, arXiv preprint arXiv:1810
 Towards understanding regularization inbatch normalization, arXiv preprint arXiv:1809
 An image enhancing pattern-based sparsity for real-time inferenceon mobile devices, In European Conference on Computer Vision
 Ompq: Orthogonal mixed precision quantization, arXiv preprint arXiv:2109
 Wrpn: Wide reduced-precisionnetworks, arXiv preprint arXiv:1709
" A survey on deep neural network compression:Challenges, overview, and solutions", arXiv preprint arXiv:2010
 A fixed-pointquantization technique for convolutional neural networks based on weight scaling, In 2019 IEEEInternational Conference on Image Processing (ICIP)
 Data-free quantizationthrough weight equalization and bias correction, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 A white paper on neural network quantization, arXiv preprint arXiv:2106
" Nvidia models, 2021", URL https://ngc
 Weighted-entropy-based quantization for deep neu-ral networks, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Value-aware quantization for training and inferenceof neural networks, In Proceedings of the European Conference on Computer Vision (ECCV)
" Qualcomm® hexagontm dsp, 2019", URL https://developer
" Pytorchcv library, 2021", URL https://pypi
 Digital signal processing: fundamentals and applications, AcademicPress
 Pruning neural networkswithout any data by iteratively conserving synaptic flow, arXiv preprint arXiv:2006
 Haq: Hardware-aware automatedquantization with mixed precision, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Two-stepquantization for low-bit neural networks, In Proceedings of the IEEE Conference on computervision and pattern recognition
 Integer quantizationfor deep learning inference: Principles and empirical evaluation, arXiv preprint arXiv:2004
 Fracbits: Mixed precision quantization via fractional bit-widths, arXivpreprint arXiv:2007
Searching for low-bit weights in quantized neural networks, arXiv preprint arXiv:2009
 Hawq-v3: Dyadic neural network quantization,In International Conference on Machine Learning
 Mest: Accurate and fast memory-economic sparse trainingframework on the edge, Advances in Neural Information Processing Systems
 Fixed-point back-propagation training, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Dis-tribution adaptive int8 quantization for training cnns, In Proceedings of the Thirty-Fifth AAAIConference on Artificial Intelligence
 Distribution-aware adaptive multi-bit quantization, InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Incremental network quantization:Towards lossless cnns with low-precision weights, arXiv preprint arXiv:1702
 Explicit loss-error-aware quantizationfor low-bit deep neural networks, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients, arXiv preprintarXiv:1606
 Trained ternary quantization, arXivpreprint arXiv:1612
 Towards unified int8 training for convolutional neural network, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
