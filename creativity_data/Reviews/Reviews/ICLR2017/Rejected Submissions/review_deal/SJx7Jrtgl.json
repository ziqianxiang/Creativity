{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."
    },
    "Reviews": [
        {
            "title": "",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). \n\nA general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. \n\nAlso, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).\n\nA negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \\eta (eq. (3)) and \\alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.\n\nI really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.\n\n\nOverall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.\nThe use of a mixture of VAE is an incremental idea if novel.\nI would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.\nThe results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the \"Best Run\". Was the best run maximizing the validation performance or the test performance ? Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that \"we have advanced the state of the art in deep unsupervised clustering both in theory and practice\".\nThe consistency violation regularization might be interesting. But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models. So making an experiment on this simpler model on a synthetic dataset should also be done.\nIn general, I would recommend running more experiments as to solidify your claims.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors posit a mixture of Gaussian prior for variational\nauto-encoders. They also consider a regularization term motivated\nfrom information theory.\n\nThe modeling extension is simple and the inference follows\nmechanically from what's already standard in the literature. Instead\nof using discrete latent variable samples they collapse the expected\nKL; this works for few mixture components and has been considered\nbefore in more general contexts, e.g., Titsias and Lazaro-Gredilla\n(2015). It will not scale to many mixture components.\n\nI find the discussion in Section 3.2.2 difficult to parse and, if I\nunderstood it correctly, not necessarily correct. Many arguments are\nintroduced and few fleshed out. First, there is a claim that a\nmultinomial prior with equal class probabilities assigns the same\nnumber of data points to each class on average; this is true a priori\nbut certainly not true given data. Second, they claim the KL\nregularizer forces the approximate posterior to be close to this\nuniform; this is only true for small data, certainly the energy term\nin the ELBO (expected log-likelihood) will overpower the regularizer;\nis this not the case in a mean-field approximation to a mixture of\nGaussians model? Third, there is a claim that \"under the mean-field\napproximation, this constraint is enforced on each sample\"; how does\nthe mean-field approximation enforce a constraint on the effect of\nMonte Carlo sampling? Fourth, they argue Johnson et al. (2016) can\novercome this issue partly due to SVI; how does data subsampling\naffect this behavior? Fifth, they derive the exact posterior in\nEquation 6; so to what extent are these arguments relevant?\n\nThe experiments are limited on toy data and only a few mixture\ncomponents are considered (not enough where the collapsed approach\nwill not scale).\n\n+ Titsias, M. K., & LÃ¡zaro-Gredilla, M. (2015). Local Expectation Gradients for Black Box Variational Inference. In Neural Information Processing Systems.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}