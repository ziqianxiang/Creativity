{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The paper is an interesting contribution, primarily in its generalization of Weston's et al's work on semi-supervised embedding method. You have shown convincingly that it can work with multiple architectures, and with various forms of graph. And the PubMed results are good. To improve the paper in the future, I'd recommend 1) relating better to prior work, and 2) extending your exploration of its application to graphs without features."
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "3: Clear rejection",
            "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers â€“ e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very similar to previous work, rebranded.",
            "rating": "3: Clear rejection",
            "review": "The authors introduce a semi-supervised method for neural networks, inspired from label propagation.\n\nThe method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).\n\nAs possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.\n\nExperiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.\n\nIn summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very similar to previous work.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.  The proposed model, however, is almost identical to that of Weston et al. 2012.\n\nAs the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:\n\n1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.\n2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs\n3. graph augmented training works on a variety of different kinds of graphs.\n\nHowever, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.  I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}