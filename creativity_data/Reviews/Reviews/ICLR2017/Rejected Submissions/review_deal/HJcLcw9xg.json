{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "This paper studies the invertibility properties of deep rectified networks, and more generally the piecewise linear structure that they implicitly define. The authors introduce a 'pseudocode' to compute preimages of (generally non-invertible) half-rectified layers, and discuss potential implications of their method with manifold-type models for signal classes. \n \n The reviewers agreed that, while this is an interesting and important question, the paper is currently poorly organized, and leaves the reader a bit disoriented, since the analysis is incomplete. The AC thus recommends rejection of the manuscript. \n \n As an addendum, the AC thinks that the authors should make an effort to reorganize the paper and clearly state the contributions, and not expect the reader to find them out on their own. In this field of machine learning, I see contributions as being either (i) theoretical, in which case we expect to see theorems and proofs, (ii) algorithmical, in which case an algorithmic is presented, studied, extensively tested, and laid out in such a way that the interested reader can use it, or (iii) experimental, in which case we expect to see improved numerical performance in some dataset. Currently the paper has none of these contributions. \n Also, a very related reference seems to be missing: \"Signal Recovery from Pooling Representations\", Bruna ,Szlam and Lecun, ICML'14, in which the invertibility of ReLU layers is precisely characterized (proposition 2.2)."
    },
    "Reviews": [
        {
            "title": "review of ``THE PREIMAGE OF RECTIFIER NETWORK ACTIVITIES''",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "not ready yet",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I really appreciate the directions the authors are taken and I think something quite interesting can come out of it. I hope the authors continue on this path and are able to come up with something quite interesting soon. However I feel this paper right now is not quite ready. Is not clear to me what the results of this work are yet. The preimage construction is not obviously (at least not to me) helpful. It feels like the right direction, but it didn't got to a point where we can use it to identify the underlying mechanism behind our models. We know relu models need to split apart and unite different region of the space, and I think we can agree that we can construct such mechanism (it comes from the fact that relu models are universal approximators) .. though this doesn't speak to what happens in practice.  All in all I think this work needs a bit more work yet. "
        },
        {
            "title": "Nice premise but somewhat incomplete?",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary:\nThis paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces.\n\nPros:\nFormalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks\n\nCons:\nAnalysis seems quite preliminary, and no novel theoretical results or clear practical conclusions.\n\nThe main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn't seem ready yet.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}