{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work. \n \n Pros:\n - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.\n \n Mixed: \n - Some found the paper clear, praising it as a \"well-written paper\", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved\n - Reviewers were also split on results. Some found the results quite \"compelling\" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work\n - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR\n \n Cons:\n - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering. \n - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison."
    },
    "Reviews": [
        {
            "title": "Good paper, accept",
            "rating": "7: Good paper, accept",
            "review": "The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks.  The quantitative results are quite good and the qualitative results are quite encouraging.\n\nFirst, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that itâ€™s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.\n\nSecond, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model.\n\nOn the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5.\n\nTo conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-executed paper with good analysis but little novelty",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Update after reading the authors' responses & the paper revision dated Dec 21:\nI have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5.\nThe main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.\n\n-----\n\nThis is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). \n\nMoreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.\n\nOne minor comment: annotate h_t in Figure 1.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation\nwith Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A well written paper",
            "rating": "6: Marginally above acceptance threshold",
            "review": "\n* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.\n\n\n* Review:\n     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. \n     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?\n     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.\n                   \n* Some Requests:\n -Can you add the size of the models to the Table 1? \n- Can you add some of the failure cases of your model, where the model failed to translate correctly?\n\n* An Overview of the Review:\n\nPros:\n    - The paper is well written\n    - Extensive analysis of the model on various language pairs\n    - Convincing experimental results.    \n    \nCons:\n    - The model is complicated.\n    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.\n    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.\n\n[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}