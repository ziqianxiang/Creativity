{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.\n \n Positive:\n \n -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods\n -- Solid experiments \n -- A well-written paper\n \n Negative:\n \n -- A straightforward application of existing methods\n \n Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community."
    },
    "Reviews": [
        {
            "title": "interesting problem and promising results",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.\n\nI think the problem is interesting and the results are promising. However, there are some problems about technical details:\n\n- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?\n- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?\n- What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "accept",
            "rating": "7: Good paper, accept",
            "review": "The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).\n\nThe first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \"bag-of-sentences\", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.\n\nThe model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences. The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).\n\nThis paper is relatively well written, and easy to follow. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering. In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.\n\nOverall, this is a pretty solid and well executed paper.\n\npros:\n - sound model for sentence ordering\n - strong experimental results\ncons:\n - might be a bit incremental\n - usefulness of sentence ordering",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "\nThis paper presents an empirical study on sentence ordering using RNN variants.\n\nIâ€™m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.\n\nThat said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}