{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The paper presents an evaluation of off-the-shelf image classification architectures. The findings are not too surprising and don't provide much new insight."
    },
    "Reviews": [
        {
            "title": "solid work but not surprising",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me:\n\n- Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage).\n\n- Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship.\nHowever, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner.\n\n- Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds.\n\n- Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations.\n\nMaybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power.\n\nAdditionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper. Some flaws.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "A few issues with this paper:\n1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.\n2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.\n3- Paper is unfriendly to colorblind readers (or those with B/W printers)\n\nOverall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important.\n\nHowever, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances.\n\nMain concerns:\n1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance!\n\n2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks.\n\n3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well.\n\nMinor concern:\n1) Why did the authors choose to use batch normalization in NiN and AlexNet?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}