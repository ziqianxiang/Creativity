{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled."
    },
    "Reviews": [
        {
            "title": "Poor presentation, lack of novelty.",
            "rating": "3: Clear rejection",
            "review": "The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. \n\nThe paper’s presentation is inefficient and muddled, and the results seem incremental.\n\nPresentation:\n\nThe abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.\n\nWhile it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper).\n\nIt is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction.\n\nFully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.\n\nThe results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.\n\nResults: \n\nThe take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "improved Architectures for Computer Go",
            "rating": "7: Good paper, accept",
            "review": "This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.\nOverall a good paper.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Contribution is unclear",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.\n\nThe paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. \n\nThe paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear paper with unsurprising results",
            "rating": "3: Clear rejection",
            "review": "The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.\n\nI found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:\n1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?\n2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?\n3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.\n4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? \n\nPlease also see my original questions.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}