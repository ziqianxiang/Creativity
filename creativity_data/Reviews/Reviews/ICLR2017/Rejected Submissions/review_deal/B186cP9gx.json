{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "This is quite an important topic to understand, and I think the spectrum of the Hessian in deep learning deserves more attention. However, all 3 official reviewers (and the public reviewer) comment that the paper needs more work. In particular, there are some concerns that the experiments are too preliminary/controlled and about whether the algorithm has actually converged. One reviewer also comments that the work is lacking a key insight/conclusion. I like the topic of the paper and would encourage the authors to pursue it more deeply, but at this time all reviewers have recommended rejection."
    },
    "Reviews": [
        {
            "title": "Important problem, but should be better situated in related work",
            "rating": "3: Clear rejection",
            "review": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.\n\nThe overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. “Almost All Learning Machines are Singular”, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further—a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. “Identifying and attacking the saddle point problem in high dimensional non-convex optimization” NIPS 2014 or the work of Amari and others.\n\nExperimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.\n\nThis paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents empirical evidence for the singularity of the Hessian. This works has interesting experiments and observations but the paper needs more work and it is not a complete work.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.\n\nQuality:\nIt seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:\n1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.\n2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.\n3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.\n4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.\n\nClarity:\n1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.\n2- The texts in the figures are very small and hard to read.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting insights, but lack of control experiments",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions. The authors study in particular, the eigenspectrum of the Hessian, which relates to the difficulty and the local convexity of the optimization problem.\n\nWhile there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and Hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.\n\nFor example, regarding the concentration of eigenvalues to zero in Figure 6, it is unclear whether the concentration effect is really caused by training (e.g. increasing insensitivity to local perturbations), or the consequence of a specific choice of scale for the initial parameters.\n\nIn Figure 8, the complexity of the data is not defined. It is not clear whether two fully overlapping distributions (the Hessian would then become zero?) is considered as complex or simple data.\n\nSome of the plots legends (Fig. 1 and 2) and labels are unreadable in printed format. Plots of Figure 3 don't have the same range for the x-axis. The image of Hessian matrix of Figure 1 does not render properly in printed format.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}