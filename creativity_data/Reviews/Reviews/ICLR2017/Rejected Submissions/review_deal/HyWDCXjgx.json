{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search, and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation."
    },
    "Reviews": [
        {
            "title": "Contribution not clear enough; concerns about data set itself",
            "rating": "3: Clear rejection",
            "review": "The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. \n\nThe writing could be improved. There are numerous grammatical errors.\n\nThe experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. \"That is a competitive result\" is vague. A footnote links to \"http://image-net.org/challenges/LSVRC/2015/results\" which doesn't seem to even show the same task you are evaluating. ResCeption: \"The best validation error is reached at 23.37% and 6.17% at top-1 and top-5, respectively\". Single model ResNet-152 gets 19.38 and 4.49, respectively. Resnet-34 is 21.8 and 5.7, respectively. VGGv5 is 24.4 and 7.1, respectively.  [source: Deep Residual Learning for Image Recognition, He et al. 2015]. I think it would be more honest for you to report results of competitors and say that your model is worse than ResNet and slightly better than VGG on ImageNet classification.\n\n3.5, retrieval on Holidays, is a bit too much of a diversion from the goal of this paper. If this paper is more about the novel architecture and less about the particular fashion attribute task then the narrative needs to change accordingly.\n\nPerhaps my biggest concern is that this paper is missing baselines (e.g. non recurrent models, attribute classification instead of detection) and comparisons to prior work by Berg et al.\n\n\"Our policy restricts to reveal much more details about the internal dataset\" This is a significant issue. The dataset used in this work cannot be shared? How are future works going to compare to your benchmark?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting exploration but several major concerns",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. \n\nHowever, there are several concerns.\n\n1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g., \n-  “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 \n- “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015,\n- “Retrieving Similar Styles to Parse Clothing, TPAMI 2014,\netc\nIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.\n\n2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?\n\n3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?\n\nWhile the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good practical visual search system but lack novelty",
            "rating": "3: Clear rejection",
            "review": "This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.\n\nThe paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}