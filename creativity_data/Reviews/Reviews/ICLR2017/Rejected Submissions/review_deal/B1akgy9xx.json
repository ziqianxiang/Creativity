{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection."
    },
    "Reviews": [
        {
            "title": "The connection between different models is interesting, except for Bayesian net which is superficial and need to discuss more; MNIST results are interesting but more tasks need to be explored.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting connection between DNN and simplified SFNN but its practical significance is unknown",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "authors": []
        }
    ]
}