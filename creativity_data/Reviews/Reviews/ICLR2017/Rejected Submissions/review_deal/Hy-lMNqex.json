{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),\n given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.\n \n Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear \"take home\" message for ML research, and the authors did post a clear statement in this regard.\n \n Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.\n \n Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards \"reject\" in terms of this being an inspirational paper for ICLR."
    },
    "Reviews": [
        {
            "title": "Stripes and Tartan are interesting architectures but the contribution over the three previous publications on this idea is extremely small",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\n\nThe paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.\n\nStrengths:\n\nUsing variable precision for each layer of the network is useful - but was previously reported in Judd (2015)\n\nGood evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)\n\nWeaknesses:\n\nThe idea of combining bit-serial arithmetic with the DaDN architecture is a small one.\n\nThe authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.\n\nThe energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.\n\nThe authors don’t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.\n\nOverall:\n\nThe Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "consider a better venue for submission",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.\n\nThe first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. \n\nThe second concern is the \"take-away for machine learning community\", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers. \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Improving inference speed and energy-efficiency in (simulated) hardware implementations by exploiting per-layer differences in numerical precision requirements.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This seems like a reasonable study, though it's not my area of expertise. I found no fault with the work or presentation, but did not follow the details or know the comparable literature. \n\nThere seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task. The tasks chosen (Alexnet / VGG) seem reasonable. The results are in simulation rather than in actual hardware.\n\nThe topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes \"hardware\".  I think the appeal among attendees will be rather limited. \n\nPlease learn to use parenthetical references correctly. As is your references make reading harder. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Incremental, perhaps better suited for an architecture conference (ISCA/ ASPLOS)",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.\n\nComments:\n\n1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.  \n\n2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units).\n\n3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "My thoughts too",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}