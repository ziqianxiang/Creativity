{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "While the reviewers found some interest in this work, I'm afraid I have to agree with the critique that the model studied is too simple that its relevance for deep learning is questionable."
    },
    "Reviews": [
        {
            "title": "Inconsistent notations with DL, and room for improvements in the presentation and the theory  ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\n\n\nSummary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf).\n\n2- The pooling operation is modified to be shrinkage operator, that keeps the maximum value in a block  and sets to the zero other values, hence the dimensionality of the pooled representation is the same of the un-pooled representations. Note that in that cases the locations of where the maximum happens is known. This is not the case in the ’standard’ max pooling definition. IHT does not map to a forward of a CNN as described in the paper. (see next point)\n\n3- It maybe that the notations  used in the paper are implying some confusions wrt to the standard notations in deep learning. \n\n  - Let z be the standard  pooled representation of dimension ‘k’. \n  - Define U as the unpooling  operation U(z, locations of maximum) :=  M(Wx,k).\nHence your model of inversion is assuming the knowledge of the ’standard’ pooling representation and the switches (max locations). Referring to M as a pooling operation is misleading and confusing, it is the ‘standard’ unpooling operation.\n - Under this notations W^{\\top} U(z, locations of maximum) is a sensible reconstruction algorithm , with the simplification of ignoring the RELU.\n- Under these notations,  IHT is similar to a backward of the encoding neural network not  a forward. It is known if you take the derivative with respect to the input in a neural network, you get the transpose convolution also known as ‘deconvolution’ (which is not a correct naming) , hence this IHT iteration is nothing else then the well known transposed convolution.\n\n4- I suggest the authors to rewrite the paper taking into account standard definitions and notation in deep learning as discussed in point 3, and to clearly state that the recovery is done under the knowledge of the switches. \n\n=====\nAfter reading the authors rebuttal and the revisions , the reviewer maintains his main concerns with the paper. Many improvements are still needed for the paper to be ready to be published in ICLR, at the notation , presentation and the theory levels. \n===== \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "\nThe authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.\n\nThis is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.\n\nA few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n\nThe relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n\nAlthough it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).\n\nFinally, the filter coherence measure must be defined either mathematically or with a proper reference.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}