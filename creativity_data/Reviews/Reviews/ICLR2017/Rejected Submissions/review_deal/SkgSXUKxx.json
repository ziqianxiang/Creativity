{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
    },
    "Reviews": [
        {
            "title": "Interesting analysis, though a clear theoretical relationship between feature regularization and low-shot learning seems missing",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Summary\n===\nThis paper extends and analyzes the gradient regularizer of Hariharan and\nGirshick 2016. In that paper a regularizer was proposed which penalizes\ngradient magnitudes and it was shown to aid low-shot learning performance.\nThis work shows that the previous regularizer is equivalent to a direct penalty\non the magnitude of feature values weighted differently per example.\n\nThe analysis goes to to provide two examples where a feature penalty\nfavors a better representation. The first example addresses the XOR\nproblem, constructing a network where a feature penalty encourages\na representation where XOR is linearly separable.\nThe second example analyzes a 2 layer linear network, showing improved stability\nof a 2nd order optimizer when the feature penalty is added.\nOne last bit of analysis shows how this regularizer can be interpreted as\na Gaussian prior on both features and weights. Since the prior can be\ninterpreted as having a soft whitening effect, the feature regularizer\nis like a soft version of Batch Normalization.\n\nExperiments show small improvements on a synthetic XOR test set.\nOn the Omniglot dataset feature regularization is better than most baselines,\nbut is worse than Moment Matching Networks. An experiment on ImageNet similar\nto Hariharan and Girshick 2016 also shows effective low-shot learning.\n\n\nStrengths\n===\n\n* The core proposal is a simple modification of Hariharan and Girshick 2016.\n\n* The idea of feature regularization is analyzed from multiple angles\nboth theoretically and empirically.\n\n* The connection with Batch Normalization could have broader impact.\n\n\nWeaknesses\n===\n\n* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.\nWhile introducing the concept, some concern is expressed about the motivation:\n\"And it is not very clear why small gradients on every sample produces\ngood generalization experimentally.\" This seems to be the central issue to me.\nThe paper details some related analysis, it does not offer a clear answer to\nthis problem.\n\n\n* The purpose and generality of section 2.1 is not clear.\n\nThe analysis provides a specific case (XOR with a non-standard architecture)\nwhere feature regularization intuitively helps learn a better representation.\nHowever, the intended take-away is not clear.\n\nThe take-away may be that since a feature penalty helps in this case it\nshould help in other cases. I am hesitant to buy that argument because of the\nspecific architecture used in this section. The result seems to rely on the\nchoice of an x^2 non-linearity, which is not often encountered in recent neural\nnet literature.\n\nThe point might also be to highlight the difference between a weight\npenalty and a feature penalty because the two seem to encourage\ndifferent values of b in this case. However, there is no comparison to\na weight penalty on b in section 2.1.\n\n\n* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy\nloss. A more general class of losses for which eq. 3 holds is not provided. This\nshould be made clear before eq. 3 is presented.\n\n\n* The Omniglot and ImageNet experiments are performed with Batch Normalization,\nyet the paper points out that feature regularization may be similar in effect\nto Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are\nclear improvements over that baseline, the proposed regularizer has a clear\nadditional positive effect. However, results should be provided without\nBatch Norm so a 1-1 comparison between the two methods can be performed.\n\n\n* The ImageNet experiment should be more like Hariharan and Girshick.\nIn particular, the same split of classes should be used (provided in\nthe appendix) and performance should be measured using n > 1 novel examples\nper class (using k nearest neighbors).\n\n\nMinor:\n\n* A brief comparison to Matching Networks is provided in section 3.2, but the\nperformance of Matching Networks should also be reported in Table 1.\n\n* From the approach section: \"Intuitively when close to convergence, about half\nof the data-cases recommend to update a parameter to go left, while\nthe other half recommend to go right.\"\n\nCould the intuition be clarified? There are many directions in high\ndimensional space and many ways to divide them into two groups.\n\n* Is the SGM penalty of Hariharan and Girshick implemented for this paper\nor using their code? Either is acceptable, but clarification would be appreciated.\n\n* Should the first equal sign in eq. 13 be proportional to, not equal to?\n\n* The work is dense in nature, but I think the presentation could be improved.\nIn particular, more detailed derivations could be provided in an appendix\nand some details could be removed from the main version in order to increase\nfocus on the results (e.g., the derviation in section 2.2.1).\n\n\nOverall Evaluation\n===\n\nThis paper provides an interesting set of analyses, but their value is not clear.\nThere is no clear reason why a gradient or feature regularizer should improve\nlow-shot learning performance. Despite that, experiments support that conclusion,\nthe analysis is interesting by itself, and the analysis may help lead to a\nclearer explanation.\n\nThe work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.\nSome points are not completely clear, as mentioned above.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.\n\nFirst, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \"results can be derived for Ï†(x) with convex differentiable non-linear activation functions such as ReLU\", both via analysis and experimentation to measure numerical stability.\n\nSecond, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.\n\nFinally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.\n\nI commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.\n\nnotes:\n- \"an expectation taken with respect to the empirical distribution generated by the training set\", generally the training set is viewed as a \"montecarlo\" sample of the underlying, unknown data distribution \\mathcal{D}.\n- \"we can see that our model learns meaningful representations\", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.\n- \"Table 13.2\" should be \"Table 2\".\n- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}