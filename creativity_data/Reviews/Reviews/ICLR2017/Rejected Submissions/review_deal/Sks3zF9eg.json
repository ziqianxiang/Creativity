{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The reviewers unanimously recommend rejecting the paper."
    },
    "Reviews": [
        {
            "title": "Nice preliminary theoretical results of using sin activations, but more evidence needed",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Mainly theoretical idea with insufficient evidence of being practical",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.\nThis change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. \nThe experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}