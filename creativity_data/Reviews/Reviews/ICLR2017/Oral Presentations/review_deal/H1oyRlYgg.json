{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "All reviews (including the public one) were extremely positive, and this sheds light on a universal engineering issue that arises in fitting non-convex models. I think the community will benefit a lot from the insights here.",
        "decision": "Accept (Oral)"
    },
    "Reviews": [
        {
            "title": "Good paper",
            "rating": "6: Marginally above acceptance threshold",
            "review": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Little novelty but valuable empirical evidence",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "review": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. \n\nPros and Cons:\nAlthough there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. \n\nSignificance:\nI think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n\nComments:\nEarlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.\n\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Analysis of large batch training",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}