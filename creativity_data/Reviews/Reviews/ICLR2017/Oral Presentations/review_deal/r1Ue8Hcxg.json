{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This was one of the most highly rated papers submitted to the conference. The reviewers all enjoyed the idea and found the experiments rigorous, interesting and compelling. Especially interesting was the empirical result that the model found architectures that outperform those that are used extensively in the literature (i.e. LSTMs).",
        "decision": "Accept (Oral)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.\n\nThis is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.\n\nIt would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.\n\nOverall, an excellent and interesting paper.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.\n\nThe paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.\n\nThis is a well written paper on an interesting topic with strong results. I recommend it be accepted.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice paper",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}