{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Important task (attention models), interesting distillation application, well-written paper. The authors have been responsive in updating the paper, adding new experiments, and being balanced in presenting their findings. I support accepting this paper.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Some nice results, but it is not clear what are the advantages/drawbacks of the different attention maps",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes to investigate attention transfers between a teacher and a student network. \n\nAttention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.\nAuthors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). \n\nThey evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.  However, the student networks performs worst than the teacher, even with attention.\n\nFew remarks/questions:\n- in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.\n- how did you choose the hyperparameter values, it would be nice to see what is the impact of $\\beta$.\n- it would be nice to report teacher train and validation loss in Figure 7 b)\n- from the experiments, it is not clear what at the pros/cons of the different attention maps\n- AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?\n\nIn summary:\nPros:\n- Clearly written and well motivated.\n- Consistent improvement of the student with attention compared to the student alone.\nCons:\n- Students have worst performances than the teacher models.\n- It is not clear which attention to use in which case?\n- Somewhat incremental novelty relatively to Fitnet\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes a new way of transferring knowledge.\nI like the idea of transferring attention maps instead of activations.\nHowever, the experiments donâ€™t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section.\nI would consider updating the score if the authors extend the last section 4.2.2.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. \n\nPros:\n+ The author evaluated the proposed methods on various computer vision dataset \n+ The paper is in general well-written\n\nCons:  \n- The method seems to be limited to the convolutional architecture\n- The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map.\n- The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the \"attention-based\" transfer. It seems like a parallel idea added to the paper that does not seem to add much value.\n- It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \\in \\mathbb{R}^{H \\times W}  whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?\n\nOverall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.  \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}