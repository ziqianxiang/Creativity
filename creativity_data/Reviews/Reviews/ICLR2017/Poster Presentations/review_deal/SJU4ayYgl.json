{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853–\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid results.",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and reasonable approach",
            "rating": "7: Good paper, accept",
            "review": "The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. \n\nExperiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}