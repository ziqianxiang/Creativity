{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Simple premise, well argued, useful result, exhaustive analysis.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).\n\nThe relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.\n\nFurthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).\n\nI would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.\nI would have also liked to see discussion of the wall time to result using this training procedure.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Somewhat interesting, but incremental work lacking sufficient practical motivation",
            "rating": "3: Clear rejection",
            "review": "The paper shows a different approach to a ternary quantization of weights.\nStrengths:\n1. The paper shows performance improvements over existing solutions\n2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.\n\nWeaknesses:\n1. The paper is very incremental.\n2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is \"what is new in the topic\" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience.\n3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation \"it is related to mobile, therefore it is cool\" sufficient.\n\nThis paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.\n\nAlso - the code was not released is my understanding.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values. Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters. This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.\n\nStrengths:\n\n- Overall well written and algorithm is presented clearly.\n- Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance.\n- I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.\n\nSome points:\n\n- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Can the authors provide average activation sparsity for each network to help verify this assumption. Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.\n\n- In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum). In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set? Does this multiple threshold style network appear in any of the tables or figures? Can it be added?\n\n- The authors claim \"ii) Quantized weights play the role of \"learning rate multipliers\" during back propagation.\" as a benefit of using trained quantization factors. Why is this a benefit? \n\n- Figure and table captions are not very descriptive.\n\nPreliminary Rating:\nI think this is an interesting paper with convincing results but is somewhat lacking in novelty. \n\nMinor notes:\n- Table 3 lists FLOPS rather than Energy for the full precision model. Why?\n- Section 5 'speeding up'\n- 5.1.1 figure reference error last line\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Trained Ternary Quantization",
            "rating": "7: Good paper, accept",
            "review": "This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.\nThe group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts. \nI also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}