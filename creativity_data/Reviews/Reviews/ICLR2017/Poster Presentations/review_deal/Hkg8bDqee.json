{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Interesting paper and clear accept. Not recommended for an oral presentation because of weaknesses in the empirical contribution that make the significance of the results unclear.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "novel idea but requires more details / experimentation",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper reads well and the idea is new.\nSadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing. \nThe training of the introspection network could have been described in more detail. \nAlso, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing.\nDue to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Valuable insight but needs careful analysis",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "EDIT: Updated score. See additional comment.\n\nI quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.\n\nHowever the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:\n\n- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.\n\n- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.\n\n- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.\n\n- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.\n\n- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does \"didn't work\" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.\n\n- A question about jump points: \nThe I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.\n\nSince the main idea is very interesting, I will be happy to update my score if the above concerns are addressed. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}