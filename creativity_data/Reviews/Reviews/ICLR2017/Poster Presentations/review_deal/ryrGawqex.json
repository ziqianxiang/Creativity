{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "All reviewers viewed the paper favorably as a nice/helpful contribution to the implementation of this important class of methods.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.\n\nThe presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.\n\nUpdate on Jan. 17th:\nafter the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Description of a promising software package.",
            "rating": "7: Good paper, accept",
            "review": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.\n\nThey show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.\n\nIn the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.\n\nThe reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new method to optimize computation graphs",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.\n\n\nPros:\n\n- significant speed improvements through dynamic batching\n- source code provided\n\n\nCons:\n\n- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context\n- presentation/vizualisation can be improved ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}