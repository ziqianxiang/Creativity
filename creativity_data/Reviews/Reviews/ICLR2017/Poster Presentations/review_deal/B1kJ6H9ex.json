{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  \n\nPresentation: \nAlthough that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network â€“ for me that would be a more intuitive exposition of the new algorithm and findings. \n\nSmall concern in general case derivation: \nSection 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.\n\nResults:\nA comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)\n\nOverall: strong paper, good theoretical insights. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting links between policy-based and value-based methods",
            "rating": "7: Good paper, accept",
            "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}