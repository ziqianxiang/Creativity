{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "All reviewers agreed that this is an interesting contribution, but all agreed that the testing environment was somewhat small-scale and that significant difficulties could arise is scaling it up. However, the overall sentiment was still sufficiently positive.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "authors": []
        },
        {
            "title": "Review - Distributed K-FAC",
            "rating": "7: Good paper, accept",
            "review": "In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD.\n\nThe main contributions of the paper are:\n1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating.\n2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers.\n3) Empirically illustrate the performance of the method, and show:\n- Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)\n- K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)\n- Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)\n- Favorable scaling properties of K-FAC with mini-batch size\n\nPros:\n- Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. \n- Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).\n\nCons: \n- “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point.\n- Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective.\n\nThe paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. \n\nTypos:\n- “updates that accounts for” — “updates that account for”\n- “Kronecker product of their inverse” — “Kronecker product of their inverses”\n- “where P is distribution over” — “where P is the distribution over”\n- “back-propagated loss derivativesas” — “back-propagated loss derivatives as”\n- “inverse of the Fisher” — “inverse of the Fisher Information matrix”\n- “which amounts of several matrix” — “which amounts to several matrix”\n- “The diagram illustrate the distributed” — “The diagram illustrates the distributed”\n- “Gradient workers computes” — “Gradient workers compute” \n- “Stat workers computes” — “Stat workers compute” \n- “occasionally and uses stale values” — “occasionally and using stale values” \n- “The factors of rank-1 approximations” — “The factors of the rank-1 approximations”\n- “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.”\n- “\\Psi is captures” — “\\Psi captures”\n- “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices”\n- “which is a nested applications of the reshape” — “which is a nested application of the reshape”\n- “provides a computational feasible alternative” — “provides a computationally feasible alternative”\n- “according the geometric mean” — “according to the geometric mean”\n- “analogous to shrink” — “analogous to shrinking”\n- “applied to existing model-specification code” — “applied to the existing model-specification code”\n- “: that the alternative parametrization” — “: the alternative parameterization”\n\nMinor Issues:\n- In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example:\n1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231)\n3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1)\n4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html)\n5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl)\n- Page 2, equation s = WA, is there a dimension issue in this expression?\n- x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots.\n- “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes an asynchronous distributed K-FAC method for efficient optimization of \ndeep networks. The authors introduce interesting ideas that many computationally demanding \nparts of the original K-FAC algorithm can be efficiently implemented in distributed fashion. The\ngradients and the second-order statistics are computed by distributed workers separately and \naggregated at the parameter server along with the inversion of the approximate Fisher matrix \ncomputed by a separate CPU machine. The experiments are performed in CIFAR-10 and ImageNet\nclassification problems using models such as AlexNet, ResNet, and GoogleReNet.\n\nThe paper includes many interesting ideas and techniques to derive an asynchronous distributed \nversion from the original K-FAC. And the experiments also show good results on a few \ninteresting cases. However, I think the empirical results are not thorough and convincing \nenough yet. Particularly, experiments on various and large number of GPU workers (in the same machine, \nor across multiple workers) are desired. For example, as pointed by the authors in the answer of a comment,\nChen et.al. (Revisiting Distributed Synchronous SGD, 2015) used 100 workers to test their distributed deep \nlearning algorithm. Even considering that the authors have a limitation in computing resource under the \nacademic research setting, the maximum number of 4 or 8 GPUs seems too limited as the only test case of \ndemonstrating the efficiency of a distributed learning algorithm.  ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}