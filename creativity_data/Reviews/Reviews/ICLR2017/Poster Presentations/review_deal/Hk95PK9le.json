{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The area chair disagrees with the reviewers and actually thinks this is a very important contribution. This paper has the potential to be have huge impact as it sets a new state of the art on a 20+ year old benchmark with a simple model. The simplicity of the model is what is so impressive, because it resets how people think about syntactic parsing as a task. The contributions in this paper have the potential to unleash a series of model simplifications in a number of areas and the area chair therefore strongly suggests accepting the paper.\n \n It is true that the techniques used in this paper are not new inventions but rather a careful combination of ideas proposed in other places. However, in the area chair's opinion this is a substantial contribution. There are lots of ideas out there and knowing which ideas to pick and how to combine them is very valuable. Showing that a simple model can beat more complicated models advances our understanding much more than a new technique that adds unnecessary additional complexity. The focus on novel models in academia is too big, leading to a proliferation of models that nobody needs.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Final review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This is primarily an engineering paper. The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages.\n\nThe architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015). The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters.\n\nThe performance of the model is impressive, but how the performance is achieved is not very impressive. I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper brings the new STOA in PTB dependency parsing. The numbers are very impressive.\n\nBuilt upon the framework of K&G parser, this improvement is achieved by mainly two things -- (1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer.\n\nAlthough I think the bilinear modification make some sense intuitively, I don't think this contribution alone is strong enough for a conference publication. The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "final review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it's a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB.\n\nThe paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model \"deeper\", whereas adding linear bias terms isn't confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). \n\nI think that this paper is more proper for NLP conferences. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}