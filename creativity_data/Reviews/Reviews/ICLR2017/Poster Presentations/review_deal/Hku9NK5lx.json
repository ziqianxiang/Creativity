{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The reviewers unanimously recommended accepting the paper.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Good results",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Very good paper. Extremely easy to read and understand. Exciting ideas. Very good results. a few typos.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.\n\nThis density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.\n\nRegularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.\n\nAs weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.\n\nThe training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.\n\nThe experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.\n\n\nThe paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.\n\nThe result tables are a bit confusing unfortunately.\n\nminor issues:\n\np1\nenglish mistake: “while networks *that* consist of convolutional layers”.\n\np6-p7\nTable 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:\n In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.\nI assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.\n\nThe approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?\n\nI think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. \n\nA quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?\n\nPreliminary rating:\nI think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.\n\nMinor notes:\nPlease resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}