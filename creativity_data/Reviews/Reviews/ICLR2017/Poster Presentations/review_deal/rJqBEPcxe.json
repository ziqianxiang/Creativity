{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.\n \n Pros:\n - Improved results\n - Very simple method\n \n Cons:\n - Almost the best results (aside from Variational Dropout)",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Incremental improvement, not convincing enough",
            "rating": "7: Good paper, accept",
            "review": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.\n\nThis is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simple idea, well executed.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Paper Summary\nThis paper proposes a variant of dropout, applicable to RNNs, in which the state\nof a unit is randomly retained, as opposed to being set to zero. This provides\nnoise which gives the regularization effect, but also prevents loss of\ninformation over time, in fact making it easier to send gradients back because\nthey can flow right through the identity connections without attenuation.\nExperiments show that this model works quite well. It is still worse that\nvariational dropout on Penn Tree bank language modeling task, but given the\nsimplicity of the idea it is likely to become widely useful.\n\nStrengths\n- Simple idea that works well.\n- Detailed experiments help understand the effects of the zoneout probabilities\n  and validate its applicability to different tasks/domains.\n\nWeaknesses\n- Does not beat variational dropout (but maybe better hyper-parameter tuning\n  will help).\n\nQuality\nThe experimental design and writeup is high quality.\n\nClarity\nThe paper clear and well written, experimental details seem adequate.\n\nOriginality\nThe proposed idea is novel.\n\nSignificance\nThis paper will be of interest to anyone working with RNNs (which is a large\ngroup of people!).\n\nMinor suggestion-\n- As the authors mention - Zoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}