{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This is a nice contribution and that present some novel and interesting ideas. At the same time, the empirical evaluation is somewhat thin and could be improved. Nevertheless, the PCs believe this will make a good contribution to the Conference Track.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper focuses on bilingual word representation learning with the following setting:\n\n1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.\n2. There is no direct word to word alignments available between the source and target language.\n\nThis is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:\n\n1. Theory for offline bilingual representation learning.\n2. Inverted softmax.\n3. Using cognate words for languages that share similar scripts.\n4. Showing that this method also works at sentence level (to some extent).\n\nAuthors have addressed all my pre-review questions and I am ok with their response. I have few more comments:\n\n1. Header for table 3 which says “word frequency” is misleading. “word frequency” could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.\n2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.\n3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.\n4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Inverted Softmax is nice",
            "rating": "7: Good paper, accept",
            "review": "This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.\n\nThe inverted Softmax idea is very nice.\n\nA few minor issues that ought to be addressed in a published version of this paper:\n\n1) There is no mention of Haghighi et al (2008) \"Learning Bilingual Lexicons from Monolingual Corpora.\", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.\n2) Likewise, Hermann & Blunsom (2013) \"Multilingual distributed representations without word alignment.\" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.\n3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages\n4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion\n5) I don't have a better suggestion, but is there an alternative to using the term \"translation (performance/etc.)\" when discussing word alignment across languages? Translation implies something more complex than this in my mind.\n6) The Mikolov citation in the abstract is messed up",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}