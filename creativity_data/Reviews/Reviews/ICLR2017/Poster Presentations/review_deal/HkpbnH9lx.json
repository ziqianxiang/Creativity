{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that “unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.” Where is the evidence for this claim? I didn’t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author’s intuitions around the “fixed reconstruction cost of L2” are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas, great empirical work, overall good contribution",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and important work",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. \nThe paper nicely presents this core idea and a way to achieve this - by choosing special \"routings\" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. \n\nOn the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. \nI would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.\n\nIn summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}