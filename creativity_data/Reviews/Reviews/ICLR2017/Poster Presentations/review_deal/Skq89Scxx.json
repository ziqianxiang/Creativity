{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "All reviewers viewed the paper favourably, with the only criticism being that seeing how the method complements other approaches (momentum, Adam) would make the paper more complete. We encourage the authors to include such a comparison in the camera ready version of their paper.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Great trick worth publishing, but is there enough material for a full paper?",
            "rating": "7: Good paper, accept",
            "review": "This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks\n\nWorking on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.\n\nRevision:\n-  the authors substantially improved the contents of the paper, including experiments on another set than Cifar\n-  the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate\nI have therefore improved my rating",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An effective method to improve convergence of neural network training",
            "rating": "7: Good paper, accept",
            "review": "This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.\n\n\nPros:\n\n- Simple and effective method to improve convergence\n- Good evaluation on well known database\n\n\nCons:\n\n- Connection of introduction and topic of the paper is a bit unclear\n- Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge\n\nRemarks:\nAn loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning. The paper does a thorough study on non-trivial datasets, and while the outcomes are not fully conclusive, the results are very good and the approach is novel enough to warrant publication. \n\nI thank the authors for revising the paper based on my concerns.\n\nTypos:\n- “flesh” -> “flush”",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}