{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The idea of using a denoising autoencoder on features of the discriminator is sensible, and explored and described well here. The qualitative results are pretty good, but it would be nice to try some of the more recent likelihood-based methods for quantitative evaluation, as the inception score is not very satisfying. Also it would be interesting to see if this additional term helps in scaling up to larger images.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "GANtastic paper",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this \"extra information\" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work? \n\nAlso, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here. \n\nI do think the idea of exploiting \"extra information\" (like discriminator features) is interesting both inside and outside the context of this paper. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.\n\nMy overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.\n\nIn addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN.  \n \nExtra comment:\nPlease add more discussion with EBGAN in next version. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}