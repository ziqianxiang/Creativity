{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "A summary of strengths and weaknesses brought up in the reviews:\n \n Strengths\n -Paper presents a novel way to evaluate representations on generalizability to out-of-domain data (R2)\n -Experimental results are encouraging (R2)\n -Writing is clear (R1, R2)\n \n Weaknesses\n -More careful controls are needed to ascertain generalization (R2)\n -Experimental analysis is preliminary and lack of detailed analysis (R1, R2, R3)\n -Novelty and discussion of past related work (R3)\n \n The reviewers are in consensus that the idea is exciting and at least of moderate novelty, however the paper is just too preliminary for acceptance as-is. The authors did not provide a response. This is surprising because specific feedback was given to improve the paper and it seems that the paper was just under the bar. Therefore I have decided to align with the 3 reviewers in consensus and encourage the authors to revise the paper to respond to the fairly consistent suggestions for improvement and re-submit. Mentime, I'd like to invite the authors to present this work at the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "This work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.",
            "rating": "3: Clear rejection",
            "review": "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good work, though more detailed analysis would be helpful",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary\n===\nThis paper trains models to predict whether block towers will fall down\nor not. It shows that an additional model of how blocks fall down\n(predicting a sequence of frames via unsupervised learning) helps the original\nsupervised task to generalize better.\n\nThis work constructs a synthetic dataset of block towers containing\n3 to 5 blocks places in more or less precarious positions. It includes both\nlabels (the tower falls or not) and video frame sequences of the tower's\nevolution according to a physics engine.\n\nThree kinds of models are trained. The first (S) simply takes an image of a\ntower's starting state and predicts whether it will fall or not. The\nother two types (CD and CLD) take both the start state and the final state of the\ntower (after it has or has not fallen) and predict whether it has fallen or not,\nthey only differ in how the final state is provided. One model (ConvDeconv, CD)\npredicts the final frame using only the start frame and the other\n(ConvLSTMDeconv) predicts a series of intermediate frames before coming\nto the final frame. Both CD and CLD are unsupervised.\n\nEach model is trained on towers of a particular heigh and tested on\ntowers with an unseen height. When the height of the train towers\nis the same as the test tower height, all models perform roughly the same\n(with in a few percentage points). However, when the test height is\ngreater than the train height it is extremely helpful to explicitly\nmodel the final state of the block tower before deciding whether it has\nfallen or not (via CD and CLD models).\n\n\nPros\n===\n\n* There are very clear (large) gains in accuracy from adding an unsupervised\nfinal frame predictor. Because the generalization problem is also particularly\nclear (train and test with different numbers of blocks), this makes for\na very nice toy example where unsupervised learning provides a clear benefit.\n\n* The writing is clear.\n\n\nCons\n===\n\nMy one major concern is a lack of more detailed analysis. The paper\nestablishes a base result, but does not explore the idea to the extent\nto which I think an ICLR paper should. Two general directions for potential\nanalysis follow:\n\n* Is this a limitation of the particular way the block towers are rendered?\n\nThe LSTM model could be limited by the sub-sampling strategy. It looks\nlike the sampling may be too coarse from the provided examples. For the\ntwo towers in figure 2 that fall, they have fallen after only 1 or 2\ntime steps. How quickly do most towers fall? What happens if the LSTM\nis trained at a higher frame rate? What is the frame-by-frame video\nprediction accuracy of the LSTM? (Is that quantity meaningful?)\nHow much does performance improve if the LSTM is provided ground truth\nfor only the first k frames?\n\n* Why is generalization to different block heights limited?\n\nIs it limited by model capacity or architecture design?\nWhat would happen if the S-type models were made wider/deeper with the CD/CLD\nfall predictor capacity fixed?\n\nIs it limited by the precise task specification?\nWhat would happen if networks were trained with towers of multiple heights\n(apparently this experiment is in the works)?\nI appreciate that one experiment in this direction was provided.\n\nIs it limited by training procedure? What if the CD/CLD models were trained\nin an end-to-end manner? What if the double frame fall predictor were trained\nwith ground truth final frames instead of generated final frames?\n\n\nMinor concerns:\n\n* It may be asking too much to re-implement Zhang et. al. 2016 and PhysNet\nfor the newly proposed dataset, but it would help the paper to have baselines\nwhich are directly comparable to the proposed results. I do not think this\nis a major concern because the point of the paper is about the role of\nunsupervised learning rather than creating the best fall prediction network.\n\n* The auxiliary experiment provided is motivated as follows: \n\"One solution could be to train these models to predict how many blocks have\nfallen instead of a binary stability label.\"\nIs there a clear intuition for why this might make the task easier?\n\n* Will the dataset, or code to generate it, be released?\n\n\n\nOverall Evaluation\n===\nThe writing, presentation, and experiments are clear and of high enough\nquality for ICLR. However the experiments provide limited analysis past\nthe main result (see comments above). The idea is a clear extension of ideas behind unsupervised\nlearning (video prediction) and recent results in intuitive physics from\nLerer et. al. 2016 and Zhang et. al. 2016, so there is only moderate novelty.\nHowever, these results would provide a valuable addition to the literation,\nespecially if more analysis was provided.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good preliminary work, more controls and detailed analysis are needed.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Paper Summary\nThis paper evaluates the ability of two unsupervised learning models to learn a\ngeneralizable physical intuition governing the stability of a tower of blocks.\nThe two models are (1) A model that predicts the final state of the tower given\nthe initial state, and (2) A model that predicts the sequence of states of this\ntower over time given the initial state. Generalizability is evaluated by\ntraining a model on towers made of a certain number of blocks but testing on\ntowers made of a different number of blocks.\n\nStrengths\n- This paper explores an interesting way to evaluate representations in terms of\n  their generalizability to out-of-domain data, as opposed to more standard\nmethods which use train and test data drawn from the same distribution.\n- Experiments show that the predictions of deep unsupervised learning models on\n  such out-of-domain data do seem to help, even though the models were not\ntrained explicitly to help in this way.\n\nWeaknesses\n\n- Based on Fig 4, it seems that the models trained on 3 blocks (3CD, 3CLD)\n  ``generalize\" to 4 and 5 blocks.  However, it is plausible that these models\nonly pay attention to the bottom 3 blocks of the 4 or 5 block towers in order to\ndetermine their stability. This would work correctly a significant fraction of\nthe time. Therefore, the models might actually be overfitting to 3 block towers\nand not really generalizing the physics of these blocks. Is this a possibility ?\nI think more careful controls are needed to make the claim that the features\nactually generalize. For example, test the 3 block model on a 5 block test set\nbut only make the 4th or 5th block unstable. If the model still works well, then\nwe could argue that it is actually generalizing.\n\n- The experimental analysis seems somewhat preliminary and can be improved. In\n  particular, it would help to see visualizations of what the final state looks\nlike for models trained on 3 blocks but test on 5 (and vice-versa). That would\nhelp understand if the generalization is really working. The discriminative\nobjective gives some indication of this, but might obfuscate some aspects of\nphysical realism that we would really want to test.  In Figure 1 and 2, it is\nnot mentioned whether these models are being tested on the same number of blocks\nthey were trained for.\n\n- It seems that the task of the predicting the final state is really a binary\n  task - whether or not to remove the blocks and replace them with gray\nbackground. The places where the blocks land in case of a fall is probably quite\nhard to predict, even for a human, because small perturbations can have a big\nimpact on the final state. It seems that in order to get a generalizable\nphysics model, it could help to have a high frame rate sequence prediction task.\nCurrently, the video is subsampled to only 5 time steps.\n\nQuality\nA more detailed analysis and careful choices of testing conditions can increase\nthe quality of this paper and strengthen the conclusions that can be drawn from\nthis work.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe particular setting explored in this paper is novel.\n\nSignificance\nThis paper provides a valuable addition to the growing work on\ntransferability/generalizability as an evaluation method for unsupervised\nlearning. However, more detailed experiments and analysis are needed to make\nthis paper significant enough for an ICLR paper.\n\nMinor comments and suggestions\n- The acronym IPE is used without mentioning its expansion anywhere in the text.\n\n- There seems to be a strong dependence on data augmentation. But given that\n  this is a synthetic dataset, it is not clear why more data was not generated\nin the first place.\n\n- Table 3 : It might be better to draw this as a 9 x 3 grid : 9 rows corresponding to the\nmodels and 3 columns corresponding to the test sets. Mentioning the train set is\nredundant since it is already captured in the model name. That might make it\neasier to read.\n\nOverall\nThis is an excellent direction to work and preliminary results look great.\nHowever, more controls and detailed analysis are needed to make strong\nconclusions from these experiments.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}