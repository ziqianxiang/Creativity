{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Unfortunately, none of the reviewers, nor the AC, have strongly supported for the acceptance of this paper. The fact that fixed-point arithmetic is the focus of this work, while floating-point arithmetic is much more common, is also a concern. The PCs thus don't think this work can be accepted to ICLR. However, we are happy to invite the authors to present their work at the Workshop Track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Good read, some questions about performance in practice.",
            "rating": "7: Good paper, accept",
            "review": "Interesting and timely paper. Lots of new neural network accelerators popping up.\n\nI'm not an expert in this domain and to familiarize myself with the topic, I browsed through related work and skimmed the DaDianNao paper.\nMy main question is about the choice of technology. What struck me is that your paper contains very few implementation details except for the technology (PRA 65nm vs DaDianNao 28nm). \nCombined with the fact that the main improvement of your work appears to be performance rather than energy efficiency, I was wondering about the maximum clock estimated frequency of the PRA implementation due to the added complexity? Based on the explanation in the methodology section, I assume that the performance comparison is based on number of clock cycles. Do you have any numbers/estimates about the performance in practices (taking into account clock frequency)?\n\n\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An interesting but very narrow DNN hardware accelerator",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents a hardware accelerator architecture for deep neural network inference, and a simulated performance evaluation thereof. The central idea of the proposed architecture (PRA) revolves around the fact that the regular (parallel) MACC operations waste a considerable amount of area/power to perform multiplications with zero bits. Since in the DNN scenario, one of the multiplicands (the weight) is known in advance, the multiplications by the zero digits can be eliminated without affecting the calculation and lowest non-zero bits can be further dropped at the expense of precision. The paper proposes an architecture exploiting this simple idea implementing bit-serial evaluation of multiplications with throughput depending on the number of non-zero bits in each weight. \n\nWhile the idea is in general healthy, it is limited to fixed point arithmetics. Nowadays, DNNs trained on regular graphics hardware have been shown to work well in floating point down to single (32bit) and even half-precision (16bit) in many cases with little or no additional adjustments. However, this is generally not true for 16bit (not mentioning 8bit) fixed point. Since it is not trivial to quantize a network to 16 or 8 bits using standard learning, recent efforts have shown successful incorporation of quantization into the training process. One of the extreme cases showed quanitzation to 1bit weights with negligible loss in performance (arXiv:1602.02830). 1-bit DNNs involve no multiplication at all; moreover, the proposed multiplier-dependent representation of multiplication discussed in the present paper can be implemented as a 1-bit DNN. I think it would be very helpful if the authors could address the advantages their architecture brings to the evaluation of 1-bit DNNs. \n\nTo summarize, I believe that immediately useful hardware DNN accelerators still need to operate in floating point (a good example are Movidius chips -- nowadays Intel). Fixed point architectures promise additional efficiency and are important in low-power applications, but they depend very much on what has been done at training. In view of this -- and this is my own extreme opinion -- it makes sense to build an architecture for 1-bit DNNs. I have the impression that the proposed architecture could be very suitable for this, but the devil is in the details and currently evidence is missing to make such claims.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Evaluation of DNN inference hardware approach in simulator. Avoid processing zero-bits and achieve speed improvements. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "An interesting idea, and seems reasonably justified and well-explored in the paper, though this reviewer is no expert in this area, and not familiar with the prior work. \nPaper is fairly clear. Performance evaluation (in simulation) is on a reasonable range of recent image conv-nets, and seems thorough enough.\n\nRather specialized application area may have limited appeal to ICLR audience. (hence the \"below threshold rating\", I don't have any fundamental structural / methodological criticism for this paper.)\n\n\nImprove your bibliography citation style - differentiate between parenthetical citations and inline citations where only the date is in parentheses. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}