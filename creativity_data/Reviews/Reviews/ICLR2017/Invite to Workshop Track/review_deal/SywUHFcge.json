{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The authors propose a framework to analyze \"robustness\" to adversarial perturbations using topological concepts. The authors conduct an empirical study using a siamese networks. \n \n The paper generated extensive discussions. The authors implemented many changes following the reviewers' suggestions. The resulting version of the paper is rather dense. It is unclear whether a conference is appropriate for reviewing such material in limited time. \n \n We invite the authors to present their main results in the workshop track. Revising the material of the paper will generate a stronger submission to a future venue such as a journal.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Connection with generalization capabilities",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples. I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv. noise but not better for new unseen samples from the task in consideration. Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotationâ€¦ than robustness to adv. noise.\n\nI can not see any direct conclusion from table 5 results. \n\nEssentially i am not convinced about the necessity to measure the robustness against adversarial noise.\n\n\n\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea, good insights, but have some flaws",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function. In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space. \n\nThe main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important. \n\nHowever, this paper also has the following problems. \n\n1)\tIt is not clear how the classifier c can affect the overall robustness to adversarial noises. The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter. (Please correct me if it is not true) This is counter-intuitive. For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold. \nI am also wondering if the metric d has anything to do with the classifier c.\n2)\tA very relevant problem is how to decompose f into g and c. For examples, one can take any intermediate layer or the input space as the feature space for a neural network. Will this affect the analysis of the adversarial robustness?\n3)\tThe oracle is a good concept. However, it is hard to explicitly define it. In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric. This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion). \n\nDue to the above problems, I feel that some aspects of the paper are not ready. If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper. \n\nIn addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise.\n \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Flawed topological considerations, interesting practical results",
            "rating": "3: Clear rejection",
            "review": "This paper aims at making three contributions:\n- Charecterizing robustness to adversarials in a topological manner.\n- Connecting the topological characterization to more quantitative measurements and evaluating deep networks.\n- Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties.\n\nIn my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether.\n\nA central notion of the paper is the abstract characterization of robustness. The main weakness is the notion of strong robustness itself, which is an extremely rigid notion. It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle. This robustness is almost never the case in real life: it requires that the predictor is almost perfect.\n\nThe main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier. Continuity of the classifier wrt. a discrete output is also never really satisfied. However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance. Based on those definition, several uninteresting, trivial consequences follow. They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes.\n\nThe second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint. The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials. This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness.\n\nThe experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}