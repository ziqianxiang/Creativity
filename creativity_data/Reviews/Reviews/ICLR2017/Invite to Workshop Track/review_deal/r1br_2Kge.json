{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The reviewers present a detailed set of concerns regarding the paper. In particular, the paper lacks comparison to other sketching works. The sketches used in the paper are rudimentary and in practice, there are more sophisticated sketches employed.\n \n Sketching has a different goal from function approximation. Sketching aims to reconstruct (with a certain error). The paper uses sketching in a naive manner, in the sense that the error due to the sketch is incorporated into the overall error. But presumably, networks can be compressed without achieving reconstruction on each instance, while maintaining a guarantee on the overall accuracy. I would find such a framework to be more interesting and practically relevant.\n \n On the other hand, it is important for the ICLR community to be exposed to sketching frameworks. Hence, I would like to invite the paper to be presented in the workshop, in the hope that it can spur further ideas.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "I think the paper presents interesting ideas but some theoretical and experimental settings have room for improvement",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Sketching is a powerful tool that has found application in a number of classical machine learning methods like least squares, kernel learning and tensor methods. It is very exciting that such application can be extended to deep neural networks, which represent the state-of-the-art for numerous learning tasks.\n\nHowever, I have doubts on some aspects of the experimental and analytical settings adopted in this paper, which (in my opinion) might limit its impact in both theoretical understanding and practical applicability.\n\n1. On the theoretical side, I found the sparse linear/polynomial classifiers assumption very troublesome. If the classifier that one wishes to learn is indeed sparse linear or sparse polynomial, why should neural networks be a good option compared to, say, Lasso or sparse logistic regression models, which are theoretically sound and even optimal for the particular function class to be learnt?\n\nI think a more relevant setting is where one wants to learn a feedforward neural network, with perhaps sparse weights on each of its layers. It is an interesting question of whether dimensionality reduction preserves the richness of such sparse weight networks, and how aggressively one can reduce the dimension into (i.e., t in the paper), which leads to much smaller parameter space.\n\nAlso, from a learning theoretical perspective, the existence of a small neural network that approximates a sparse linear/polynomial classifier does not mean such a network can be effectively learnt. It would be good that some VC dimension /generalization aspects are analyzed as well. For example, intuitively network that operates on sketched inputs should have smaller model complexity. Does it lead to improved generalization, and how such improvement should be traded off for the richness of the network (which desires *larger* sketch length)?\n\n2. On the practical side, it seems the most relevant application would be in NLP where inputs are naturally sparse high-dimensional vectors. Reducing the input dimension indeed reduces the size of the model and at least save storage cost. However, in practice the high dimensionality is typically dealt with using a LookupTable structure (encoding), which is not difficult to train at all, because the gradient for each data point only involves one entry in the LookupTable. I think the experiments should include comparison with such traditional approaches and comment on both accuracy and training/testing time. Also, if the argument is that the vocabulary size is so large that LookupTable cannot be stored in GPU memory, such an example should be demonstrated and show how sketching helps. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea, some mismatch between theory and experiments",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary of the paper:\n\nThe paper introduces sketches that approximates linear and sparse polynomials on binary data. The paper shows that such sketches can be represented as a one layer neural network. Experiments are conducted on some language processing tasks. \n\nNovelty:\n\nApproximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel. Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).\n\nThe main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.\n\nClarity: \n\nThe paper is  written in a clear way.\n\nComments :\n\n- The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model. \nWhen Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper. When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1]. On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w. \n\n- Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown. Many works have tackled that issue see for instance http://www.di.ens.fr/~fbach/IEEE_TSP_shervashidze_bach_2015.pdf\n\n- The experiments are limited to small sets.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary: This paper presents a novel sketching method in solving sparse polynomial function of a sparse binary vector within a single-layer neural network. \n\nPros:1. The paper is written clearly. \n     2. I like the idea of using improper learning to reduce the dimensionality.\n     3. It's novel to use neural network to do sketching decoding.\nCons:1. The experiments part is kind of weak in comparison with other sketching/projection methods.\n     2. Any theoretical analysis to show why this approach needs fewer paramerters than feature hash, etc. My other concern is that since t and m are tunable, how to set these parameter for different problems? \n     3. The authors claim they have faster training. However it's not shown in experiments.\nComments: It may be interesting to compare with sparse feed-forward neural networks.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}