{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable. \n \n Originality, Significance:\n  The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.\n The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.\n That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "very interesting but probably too derivative from earlier already published work",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Weak accept",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not sure if very interesting to the ICLR community",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i’m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn’t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper discusses a range of modelling choices for designing differentiable programming languages. Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as \"length of the list\", \"return k-th element from the list\", etc. The solutions are learnt from input/output example pairs (5 for training, 25 for test).\n\nThe main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.\n\nMy main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work. Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as \"sorting\", \"merging\", \"adding\"). I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure.\n\nIt is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has “loop” fails to solve “list length” task in 84% of the runs).\n\n\nPro:\n- generates code rather than black-box neural architecture\n- nice that it can learn from very few examples\n\nCons:\n- weak results, works only for very simple tasks, missing comparison to neural architectures",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "This paper presents design decisions of TerpreT [1] and experiments about learning simple loop programs and list manipulation tasks. The TerpreT line of work (is one of those which) bridges the gap between the programming languages (PL) and machine learning (ML) communities. Contrasted to the recent interest of the ML community for program induction, the focus here is on using the design of the programming language to reduce the search space. Namely, here, they used the structure of the control flow (if-then-else, foreach, zipWithi, and foldli \"templates\"), immutable data (no reuse of a \"neural\" memory), and types (they tried penalizing ill-typedness, and restricting the search only to well-typed programs, which works better). My bird eye view would be that this stands in between \"make everything continuous and perform gradient descent\" (ML) and \"discretize all the things and perform structured and heuristics-guided combinatorial search\" (PL).\n\nI liked that they have a relevant baseline (\\lambda^2), but I wished that they also included a fully neural network program synthesis baseline. Admittedly, it would not succeed except on the simplest tasks, but I think some of their experimental tasks are simple enough for \"non-generating code \" NNs to succeed on.\n\nI wished that TerpreT was available, and the code to reproduce these experiments too.\n\nI wonder if/how the (otherwise very interesting!) recommendations for the design of programming languages to perform gradient descent based-inductive programming would hold/perform on harder task than these loops. Even though these tasks are already interesting and challenging, I wonder how much of these tasks biased the search for good subset of constraints (e.g. those for structuring the control flow).\n\nOverall, I think that the paper is good enough to appear at ICLR, but I am no expert in program induction / synthesis.\n\nWriting:\n - The paper is at times hard to follow. For instance, the naming scheme of the model variants could be summarized in a table (with boolean information about the features it embeds).\n - Introduction: \"basis modern computing\" -> of\n - Page 3, training objective: \"minimize the cross-entropy between the distribution in the output register r_R^{(T)} and a point distribution with all probability mass on the correct output value\" -> if you want to cater to the ML community at large, I think that it is better to say that you treat the output of r_R^{(T)} as a classification problem with the correct output value (you can give details and say exactly which type of criterion/loss, cross-entropy, you use).\n\n\n[1] \"TerpreT: A Probabilistic Programming Language for Program Induction\", Gaunt et al. 2016",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}