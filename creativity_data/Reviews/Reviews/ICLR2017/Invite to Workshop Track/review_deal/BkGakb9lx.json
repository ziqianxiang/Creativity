{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "RenderGAN: Generating Realistic Labeled Data",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as “Our method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. “. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}