{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. The proposed theoretical analysis, based on stability arguments, is quite interesting. Experiments on MNIST and CIFAR-10 illustrate the potential of the approach. \n \n Reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. “ … the interval [-L+T, L+T] …” should be “ … the interval [-L+T, L-T] … “ ?\n- Pg 2., Equation for f(x), should it be “ (-L+T) i \\pi x / L “ in both sin and cos terms, or without “ x “ ?\n- Theorem 4.2 “ … some algorithm \\eps-uniformly stable …” remove the word “algorithm”\n- Theorem 4.5.  SGM undefined",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "strong submission, interesting from both theoretical and experimental point of view",
            "rating": "7: Good paper, accept",
            "review": "This paper provides a principled framework for nonparametrically learning activation\nfunctions in deep neural networks. A theoretical justification for authors' choice of \nnonparametric activation functions is given. \nTheoretical results are satisfactory but I particularly like the experimental setup\nwhere their methods are tested on image\nrecognition datasets and achieve up to a 15% relative increase in test performance\ncompared to the baseline.\nWell-written paper and novel theoretical techniques. \nThe intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the\nAppendix clarifies everything.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, room for more  empirical study",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Summary:\n\nThe paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.\n\nsignificance:\n\nThe paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. \n \nA more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. \n\n\nComments: \n\n- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?\n\n- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. \n\n- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? \n\n- Does normalization affect  the conclusion that polynomial basis fail? \n\n\n\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}