{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.\n \n Pros:\n - interesting idea with interesting analysis of the gradient norms\n - claims to need less computation\n \n Cons:\n - Experiments are not very convincing and only focus on only a small set of lm tasks.\n - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.\n \n With more experimental evidence the paper should be a nice contribution.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Sloppy writing, unsufficient experimental validation",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method’s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.\n\nPoints\n\n1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.\n\n2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author’s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.\n\n3) Section 4 is nice, and I applaud the authors for doing such an analysis.\n\n\nList of typos etc.\n\n- maintain -> maintain\n- requisits -> requisites\n- a LSTM -> an LSTM\n- \"The gradients of ot and ft are equivalent to equation 25.” Gradients cannot be equivalent to an equation.\n- “beacause\"-> because\n- One of the γx > γh at the end of page 5 is wrong.\n\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "incremental",
            "rating": "6: Marginally above acceptance threshold",
            "review": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. \n\nAll in all I think is good incremental work, but maybe is not yet significant enough for ICLR.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}