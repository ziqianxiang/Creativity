{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This work is stood out for many reviewers in terms of it's clarity (\"pleasure to read\") and originality, with reviewers calling it \"very ambitious\" and \"provocative\". Reviewers find the approach novel, and to fill an interesting niche in the area. All the reviewers were interested in the results, even if they did not buy completely the motivation (what \"practically gained from this formulation\", how does this fit in with prob programming).\n \n The main quality and impact issue is the lack of experimental results and baselines. Several reviewers find that the experiments \"do not fit the claims\", and ask for any type of baselines, even just enumeration. Lacking empirical evidence, there is a desire for a future plan showing what this type of approach could be useful for, even if it cannot really scale. I recommend this paper to be submitted to the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper develops a differentiable interpreter for the Forth programming\nlanguage. This enables writing a program \"sketch\" (a program with parts left\nout), with a hole to be filled in based upon learning from input-output\nexamples. The main technical development is to start with an abstract machine\nfor the Forth language, and then to make all of the operations differentiable.\nThe technique for making operations differentiable is analogous to what is done\nin models like Neural Turing Machine and Stack RNN. Special syntax is developed\nfor specifying holes, which gives the pattern about what data should be read\nwhen filling in the hole, which data should be written, and what the rough\nstructure of the model that fills the hole should be. Motivation for why one\nshould want to do this is that it enables composing program sketches with other\ndifferentiable models like standard neural networks, but the experiments focus\non sorting and addition tasks with relatively small degrees of freedom for how\nto fill in the holes.\n\nExperimentally, result show that sorting and addition can be learned given\nstrong sketches.\n\nThe aim of this paper is very ambitious: convert a full programming language to\nbe differentiable, and I admire this ambition. The idea is provocative and I\nthink will inspire people in the ICLR community.\n\nThe main weakness is that the experiments are somewhat trivial and there are no\nbaselines. I believe that simply enumerating possible values to fill in the\nholes would work better, and if that is possible, then it's not clear to me what\nis practically gained from this formulation. (The authors argue that the point\nis to compose differentiable Forth sketches with neural networks sitting below,\nbut if the holes can be filled by brute force, then could the underlying neural\nnetwork not be separately trained to maximize the probability assigned to any\nfilling of the hole that produces the correct input-output behavior?)\n\nRelated, one thing that is missing, in my opinion, is a more nuanced outlook of\nwhere the authors believe this work is going. Based on the small scale of the\nexperiments and from reading other related papers in the area, I sense that it\nis hard to scale up differentiable forth to large real-world problems. It\nwould be nice to have more discussion about this, and perhaps even an experiment\nthat demonstrates a failure case. Is there a problem that is somewhat more\ncomplex than the ones that appear in the paper where the approach does not work?\nWhat has been tried to make it work? What are the failure modes? What are the\nchallenges that the authors believe need to be overcome to make this work.\n\nOverall, I think this paper deserves consideration for being provocative.\nHowever, I'm hesitant to strongly recommend acceptance because the experiments\nare weak.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents an approach to do (structured) program induction based on program sketches in Forth (a simple stack based language). They turn the overall too open problem of program induction into a slot filling problem, with a differentiable Forth interpreter, for which one can backprop through  the slots (as they are random variables). The point of having sketches/partial programs is that one can learn more complex programs than starting from scratch (with no prior information). The loss that they optimize (end to end through the program flow) is a L2 (RMSE) of the program memory (at targeted/non-masked adresses) and the desired output. They show that they can learn addition, and bubble sort, both with a Permute (3-way) sketch and with a Compare (2-way) sketch.\n\nThe idea of making a language fully differentiable to write partial programs (sketches) and have them completed was previously explored in the  probabilistic programming community and more recently with TerpreT. I think that using Forth (a very simple stack-based language) as the sketch definition language is interesting in itself, as it is between machine code (Neural Turing Machine, Stack RNN, Neural RAM approaches...) and higher level languages (Church, TerpreT, ProbLog...).\n\nSection 3.3.1 (and Figure 2) could be made clearer (explain the color code, explain the parallel between D and the input list).\n\nThe experimental section is quite sparse, even for learning to sort, there is only one experimental setting (train on length 3 and test on length 8), and .e.g no study of the length at which the generalization breaks (it seems that it possibly does not), no study of the \"relative runtime improvement\" w.r.t. the training set (in size and length of input sequences). There are no baselines (not even at least exhaustive search, one of the neural approaches would be a plus) to compare to. Similarly, the \"addition\" experiment (section 4.2) is very shortly described, and there are no baselines either (whereas this is a staple of \"neural approaches\" to program induction). Does \"The presented sketch, when trained on single-digit addition examples, successfully learns the addition, and generalises to longer sequences.\" mean that it generalizes to three digits or more?\n\nOverall, the paper is very interesting, but it seems to me like the experiments do not support the claims, nor the usefulness, enough.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "This paper presents an approach to make a programming language (Forth) interpreter differentiable such that it can learn the implementation of high-level instruction from provided examples. The paper is well-written and the research is well-motivated. Overall, I find this paper is interesting and pleasure to read.  However, the experiments only serve as proof of concept. A more detailed empirical studies can strength the paper. \n\nComments:\n\n- To my knowledge, the proposed approach is novel and nicely bridge programming by example and sketches by programmers. The proposed approach borrow some ideas from probabilistic programming and Neural Turing Machine, but it is significantly different from these methods. It also presents optimisations of the interpreter to speed-up the training. \n\n- It would be interesting to present results on different types of programming problems and see how complex of low-level code can be generated. \n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}