{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 (http://ieeexplore.ieee.org/abstract/document/4700287/) when you consider a complete graph, which has been used in the setup of full set equivariance for example in 'Learning Multiagent communication with backpropagation', Sukhbaatar et al NIPS'16; see also 'Order Matters: sequence to sequence for sets', Vinyals et al. https://arxiv.org/abs/1511.06391. \n The general question of how to model point-cloud data, or more generally data defined over graphs, with neural networks is progressing rapidly; see for example https://arxiv.org/abs/1611.08097 for a recent survey.\n \n The question then is what is the contribution of the present work relative to this line of work. The authors should answer this question explicitly in the revised manuscript, either with a new application of the model, or with theory that advances our understanding of these models, or with new numerical applications.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Interesting formalization of invariance in neural networks, but too abstract and weak experimental results",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\nThis paper discusses ways to enforce invariance in neural networks using weight sharing.  The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a “set-invariant” function, which is used in an anomaly detection setting and a point cloud classification problem.  \n\n“Invariance” is, at a high level, an important issue of course, since we don’t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.  However, there are a few weaknesses that I feel prevent this from being a strong submission.  First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning.\n\nSecond, “set invariance”, which is the main type of invariance studied in the paper is defined via the author’s formalization of invariance, but is never explicitly related to what I might think of as “set invariance” — e.g. to permutations of input or output dimensions.  Explicitly defining set invariance in some other way, then relating it to the  “structural invariance” formulation may be a better way to explain things.  It is never made clear, for example, why Figure 1(b) is *the* set data-structure.\n\nI like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).  But the authors have ignored the other kind of compositionality that is important to neural networks — specifically that relating the proposed notion of invariance to function composition seems important — i.e. under what conditions do compositions of invariant functions remain invariant?  And  It is clear to me that just by having one layer of invariance in a network doesn’t make the entire network invariant, for example.  So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is “set invariant” in some sense?  \n\nRegarding experiments, there are no baselines presented for anomaly detection.  Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.  (I should say that I don’t know enough about the dataset to say whether these are exactly fair comparisons or not).  It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.  As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.  Also, what about classical set kernels?\n\nOther random things:\n* Example 2.2: Shouldn’t |S|=5 in the case of left-right and up-down symmetry?\n* “Parameters shared within a relation” is vague and undefined.\n* Why is “set convolution” called “set convolution” in the appendix?  What is convolutional about it?\n* Is there a relationship to symmetric function theory?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A promising work!",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting topic, but hard to follow for a non-expert ",
            "rating": "7: Good paper, accept",
            "review": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. ",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}