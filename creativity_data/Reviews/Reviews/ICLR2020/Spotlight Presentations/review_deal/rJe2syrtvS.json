{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes.\n\nMany of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work. These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "  *Synopsis*:\n  This paper focuses on current limitations of deploying RL approaches onto real world robotic systems. They focus on three main points: the need to use raw sensory data collected by the robot, the difficulty of handcrafted reward functions without external feedback, the lack of algorithms which are robust outside of episodic learning. They propose a complete system which addresses these concerns, combining approaches from the literature and novel improvements. They then provide an empirical evaluation and ablation testing of their approach and other popular systems, and show a demonstration on a real robotic system.\n \n  Main Contributions:\n  - A discussion of the current limitations of RL on real robotic systems\n  - A framework for doing real world robotic RL without extra instrumentation (outside of the robot).\n\n  *Review*: \n  Overall, I think the paper is well written and provides some nice analysis of the current state of RL and robotics. I am not as familiar with the RL for robotics literature, but from some minor snooping around I believe these ideas to be novel and useful for the community. I have a few suggestions for the authors, and a few critical pieces I would like added to the main text.\n\n  Critical additions:\n  1. I would like some more details on your simulation experiments. Specifically:\n    - How many runs were your experiments? \n    - What are the error bars on your plots?\n    - What ranges of hyper-parameters did you test for tuning?\n\n  2. I would quite like the discussion of the real world tasks from the appendix to appear in the main text. Specifically, giving the evaluation metrics you mentioned in the appendix. \n\n  Suggestions/Questions:\n\n  S1: It is not clear if a VAE is the best choice for unsupervised representation learning for RL agents. Although a reasonable choice, Yashua Bengio recently released a look at several unsupervised techniques for representation learning in Atari which you may want to look at: https://arxiv.org/pdf/1906.08226.pdf. \n\n  Q1: Did you try any of the other approaches on the real robotics system? Or was there no way to deploy these algorithms to your specific setup without instrumentation?"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents approaches to handle three aspects of real-world RL on robotics: (1)learning from raw sensory inputs (2) minimal reward design effort (3) no manual resetting. Key components:(1) learn a perturbation policy that allows the main policy to explore a wide variety of state. (2) learn a variational autoencoder to transform images to low dimensional space.\n\nExperiments in simulation on the physical robots are performed to demonstrate the effectiveness of these components. Close related work is also used for comparison. The only concern I have is that the tasks considered involve robots that can automatically reset themselves pretty easily. I doubt that this will scale to unstable robots such as biped/quadruped, where once they fail, the recovering/resetting tasks will be as much or more difficult than the main locomotion tasks. But I understand this is too much to address in one paper and limitation is also briefly discussed in the final section.\n\nOverall I think this is a good paper and valuable to the community.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper takes seriously the question of having a robotic system learning continuously without manual reset nor state or reward engineering. The authors propose a first approach using vison-based SAC, shown visual goals and VICE, and show that it does not provide a satisfactory solution. Then they add a random pertubation controller which brings the robot or simulated system away from the goal and a VAE to encode a compressed state, and show that it works better.\n\nThe paper is a nice read, it contains useful messages thus I'm slightly in favor of accepting it, but I may easily change my mind as it suffers from serious weaknesses.\n\nFirst, and most importantly, the experimental study is very short, the authors have chosen to spend much more space on careful writing of the problem they are investigating.\n\nTo mention a few experimental weaknesses, in Section 6.2 the authors could have performed much more detailed ablation studies and stress in more details the impact of using the VAE alone versus using the random pertubation controller alone, they could say more about the goals they show to the system, etc. There is some information in Figure 7, but this information is not exploited in a detailed way. Furthermore, Figure 7 is far to small, it is hard to say from the legend which system is which.\n\nAbout Fig.8, we just have a qualitative description, the authors claim that without instrumenting they cannot provide a quantitative study, which I don't find convincing: you may instrument for the sake of science (to measure the value of what you are doing, even if the real-world system won't use this instrumentation).\n\nSo the authors have chosen to spend more space on the positionning than on the empirical study, which may speak in favor of sending this paper to a journal or magazine rather than a technical conference. But there is an issue about the positionning too: the authors fail to mention a huge body of literature trying to address very close or just similar questions. Namely, their concern is one the central leitmotives of Developmental Robotics and some of its \"subfields\", such as Lifelong learning, Open-ended learning, Continual learning etc.  The merit of the paper in this respect is to focus on a specific question and provide concrete results on this question, but this work should be positionned with respect to the broader approaches mentioned above. The authors will easily find plenty of references in these domains, I don't want to give my favorite selection here.\n\nKnowing more about the literature mentioned above, the authors could reconsider their framework from a multitask learning perspective: instead of a random perturbation controller, the agent may learn various controllers to bring the system into various goal states (using e.g. goal-conditioned policies), and switching from goal to goal to prevent the system fro keeping stuck close to some goal.\n\nMore local points:\n\nIn the middle of page 5, it is said that the system does not learn properly just because it is stuck at the goal. This information comes late, and makes the global message weaker.\n\nin Fig. 4, I would like to know what is the threshold for success.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}