{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The reviewers are unanimous in their opinion that this paper offers a novel approach to causal learning.  I concur.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Authors propose a new Dataset CLEVRER, a simulated video dataset involving interaction between objects. It is discussed, the existing state-of-the-art models for visual question answering, doesn’t capture the causal structure between the objects and their claim is supported by their experiments.  Authors also proposed a model which captures the dynamics of the objects involved in the video, through experiments they have shown their model performs better than the existing models. \n\nThe CLEVRER dataset is designed to test a model capability to answer the queries involving causal relationship between the objects involved in the video. \n\nDetails.\nThis work begins with a well motivated problem by pointing out the drawback in existing VQA(Visual Question Answering)  models, that existing works focus on visual and input language patterns to answer the queries and doesn’t tackle the task involving causal structure.  It  explores the current literature around the problem related to visual question answering(both real world data and simulated data). Through experiments they have shown the existing state-of-the-art work on visual question answering doesn’t perform well on the dataset CLEVRER. \n\nTo prove the incompetence of existing models to capture causal structure, authors designed an artificial dataset with questions which can be answered only when the model is capable of capturing the causal structure between the objects.  \n\nThe process involving the creation of CLEVRER dataset is well explained. But, it is unclear how the questions are generated. \n\nThrough experiments authors revealed the drawbacks of existing models on capturing the dynamics between the objects and  proposed a model which is said to be inspired by previous VQA[1] model. An important modification by incorporating neural dynamics predictor module to the existing model is key, and also achieves good performance on the dataset. \n\nComments:\n\n- The paper is well written, but it is unclear how the questions are generated during dataset creation process.\n- The main contribution of the paper is to show the incompetence of existing models to capture dynamics. Which is a form of analysis.\n- It is shown that,  learning dynamics of the objects the model can achieve better performance.\n- This dataset is created in more restricted environment like height of the objects should be same. How can this be generalized to a more real world setting ?\n\nSome minor issues:\nIn few places causal is misspelled as casual(page 2,8).\nEquation 2, in the appendix the subscripts are not proper. \n\n[1] Yi, Kexin, et al. \"Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.\" Advances in Neural Information Processing Systems. 2018."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper studies the temporal and causal structures in videos. Specifically, the authors first introduce a new dataset called CLEVRER drawing motivation from CLEVR, a well-known visual reasoning dataset. They further evaluate a set of state-of-the-art methods on the newly introduced dataset to confirm their initial beliefs of the challenges posed by causal reasoning. Based on empirical clues, they also suggest neural-symbolic based framework for causal reasoning.\nOn the whole, I think this is a good paper and it addresses one of the most challenging and exciting tasks in visual reasoning. The introduction of the CLEVRER dataset is likely valuable for the community to facilitate research in this area. I have some concerns as follows:\n(1) As the fact that questions are generated algorithmically, I believe many of them are either ill-posed or degenerate similar to what described in the CLEVR. Did you manage to filter out those questions? Please provide more details on the question generation process.\n(2) I have a doubt on the reported results in table 2 as you extract visual features not fairly between all methods. Outputs of pool5 feature basically kill all spatial information while 14x14 feature map used as input of MAC, IEP and TbD-net keep the spatial information. I also not sure if the temporal attention is a better option than vectorizing the whole video features (Some pooling layers might be helpful) before feeding into those methods. It would be fairer to evaluate the state of the art methods on the CLEVRER more carefully.\n(3) As the number of objects in CLEVRER is limited, using class labels from Mask R-CNN may introduce noises to the model due to incorrect detections. The authors may need to explain more clearly at this point in the implementation details.\n(4) Have you tried to incorporate flow features, for example, C3D/I3D feature?\n\nMinor comments:\nThere are some typos in the paper in both main paper and supplementary document -causal vs. casual. Please fix these."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "A new benchmark is presented, which requires to reason on spatio-temporal data (videos) and incorporates physics (videos of physical dynamics), language (questions are formulated through language) and causality. The benchmark builds on the well-known CLEVR benchmark and adds several interesting contributions, in particular reasoning over time.\n\nCompared to other benchmarks proposed on reasoning on physical dynamics, this benchmark adds to the language component (which is presented in classical VQA benchmarks), and an interesting counterfactual component known from the causal inference literature. This is fairly new for the computer vision and statistical machine learning literature (a few papers on counterfactual reasoning exist), but in contrast to the causal inference literature, here the do-operator is observable during training: we do have access to do-interventions and even the outcome during training and therefore we can even learn counterfactuals using supervised learning. This statement is not meant as criticism, as learning counterfactuals without supervision from high-dimensional input like images seems to be currently out of a reach, or at least has not yet been demonstrated up to my knowledge.\n\nOn the downside, compared to other physical reasoning benchmarks, the answers here are multiple choice instead of regressions of the future motion, which requires more fine-grained reasoning. This will guide solutions to a certain type and will also favor solutions, requiring the network to detect certain binary concepts and combine them instead of regressing complex functions. It also favors solutions of the type presented in the paper.\n\nAs for other benchmarks, the dataset is quite large (20 000 videos) and, given its synthetic nature, is accompanied by functional programs. As for CLEVR, the simulations have been performed by a physics engine and then separately rendered with Blender to maximize visual quality. The result is a very interesting benchmark, and I have no doubt that it will be very useful for us (the learning reasoning community).\n\nAnother positive point is the number of baselines tested on the benchmark, among which we can find strong papers on VQA/VQA2\n\nI have a couple of questions:\n\nHow is the causal graph created? Are the experiments rendered and outcomes examined, creating the causal graph for the counterfactual answers, or are the experiments selected with a given outcome already decided?\n\nThe distribution of question types has been provided, but how about the biases? Distributions of the answers would have been helpful. How did the authors avoid biases during construction of the dataset, in particular in the counterfactual case (see remarks on the causal graph).\n\nThe paper also comes with a method for solving, which is very similar to existing methods on learning through functional programs. The method itself is unfortunately described only very briefly and the reader needs to look it up almost entirely in the appendix of the paper, which is surprising, as the paper is only 8 pages long instead of 10.\n\nAs for CLEVR, one of the downsides of this type of benchmark is the synthetic nature of the images and the limited range of different objects in the scene. Of course this comes with the advantage of being able to study compositional reasoning in detail, as a scene graph can be calculated easily (and is available during simulation). However, it also makes reasoning through functional programs much easier, as the proposed filters are limited in number and can strong respond to the small number of shapes and colors available in the data. I have strong doubts that this kind of approach extends to real life scenarios.\n\nFor VQA type of scenarios, GQA is a nice compromise between natural looking images and the availability of scene graphs and the restriction of questions to compositional reasoning. The optimal choice would be a similar compromise for spatio-temporal data, but of course this would be a huge effort and it would be up to impossible to have access to counterfactuals.\n\nLast point, and this question is not restricted to this paper, as the name came up elsewhere, why is the model called neuro-symbolic reasoning? While it could be argued that the questions require a sort of “symbolic reasoning”, I am not sure that the reasoning method itself is symbolic even partially. Other than selecting functional programs out of a discrete set, the reasoning itself is connectionist and performed with graph networks.\n"
        }
    ]
}