{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This is a mostly theoretical paper concerning online and stochastic optimization for convex loss functions that are not Lipschitz continuous. The authors propose a method for replacing the Lipschitz continuity condition with a more general Riemann-Lipschitz continuity condition, under which they are able to provide regret bounds for the online mirror descent algorithm, as well as extending to the stochastic setting. They follow up by evaluating their algorithm on Poisson inverse problems. \n\nThe reviewers all agree that this is a well-written paper that makes a clear contribution. To the best of our knowledge, the theory and derivations are correct, and the authors were highly responsive to reviewers’ (minor) comments. I’m therefore happy to recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "The paper establishes optimal regret bounds of the order O(\\sqrt{T}) for Follow The Regularised Leader (FTRL) and Online Mirror Descent (OMD) for convex loss functions and potentials (a.k.a. Riemannian regularizers) that are, respectively, Lipschitz continuous and strongly convex with respect to a given Riemannian metric. These conditions naturally generalize the classical conditions typically considered in the literature, which are defined with respect to a global norm and, as such, are not well-suited to problems where the loss functions and its gradient present singularities at the boundary of the feasibility region. The authors suggest a principled way to choose both the Riemannian metric and the potential function based on the singularity landscape of the gradient of the loss function. Via standard online-to-batch conversion, the authors also address the offline setting and give O(1/\\sqrt{T}) error bounds for ergodic averages in convex problems and for last iterates in non-convex problems satisfying a weak secant inequality. The authors include numerical experiments involving a Poisson inverse problem.\n\nThe paper is well-written, with a very clean narrative highlighting the main ideas and results. To the best of my knowledge, the literature review is complete and rightly highlights the fact that most results in the literature on Riemannian mirror descent methods have so far primarily addressed offline deterministic problems with exact oracle gradients. The contribution of this work lies not only in the focus on online and noisy setting but also on establishing natural results upon natural generalizations of well-known conditions in standard (non-Riemannian) settings. The techniques used are extensions of the classical theory and follow quite naturally, with the exception of the non-trivial primal-dual inequality (20).\n\nQUESTIONS/SUGGESTIONS:\n1) Can the authors be more explicit about how the (OMD) equations are derived from (16). While this is standard, I feel currently there is a bit of a jump in the narrative—which otherwise is very good.\n2) The workhorse behind the established results is the primal-dual inequality (20) which relies on the introduction of the Fenchel coupling. Can the authors be more explicit about the use of this inequality, and what makes the Riemann generalization difficult in general? In particular, can the authors comment on the applicability of this inequality (or similar) to the smooth setting?\n3) Typo: sometimes the notation $\\mathcal{U}$ seems to be used instead of the notation $\\mathcal{X}$. See, for instance, equation (7) in Definition 1 and the definition of the set $\\mathcal{Z}$ in Remark 2.\n\nAFTER REBUTTAL: I thank the authors for addressing my questions.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThe paper generalizes regret analysis results from convex online learning to\nfunctions that are not Lipschitz but Riemann Lipschitz continuous.\n\nExample:\n$f(x) = -\\log(x) + x$ is convex on the convex domain $X = [0, 2]$ but $f$ is\nnot Lipschitz on $D$ since $f(x) \\to \\infty$ as $x \\to 0$.\n\nA possible Riemannian metric that can be used on the domain of such a function\nis the Poincare metric $g(x) = 1 / x^2$.\nThe norm defined by that Riemannian metric is $\\| z \\|^2_x = z^T g(x) z$.\nThis norm can be used to measure infinitesimal distances in $X$.\nThe Riemannian distance $dist(x, x')$ on $X$ that follows from this is defined\nas integrating over the infinitesimal distances on a curve connecting\n$x$ and $x'$ as measured by the norm defined above.\n\nIntuitively, an infinitesimal distance Lipschitz bound can be seen to be\nconstructed by\n$| f(x + \\delta x) - f(x) | \\approx \\| f'(x) \\| \\| \\delta x \\|$ as $\\delta x \\to 0$\nwhich in the case of our example does not exist since $f'(x) \\to \\infty$.\nBut when using a Riemannian metric based norm\n$\\| f'(x) \\| = \\sqrt{ f'(x)^T g^{-1}(x) f'(x) }$\non the right-hand side we have\n$f'(x) = -\\frac{1}{x} + 1$\n$f'(x)^2 = \\frac{1}{x^2} - \\frac{2}{x} + 1$\n$f'(x)^2 g^{-1}(x) = O(1)$\nwith $g(x) = 1 / x^2$.\n\nIn this way it is possible to bound changes of $f(x)$ relative to changes in\n$x$ for functions that are not Lipschitz continuous.\n\nThe authors show how a suitable Riemannian metric can be transformed into\na regularizer usuable in online optimization.\nThey present various rates that appear to be otherwise known for similar\nregularizers.\nMy knowledge of the online learning literature is very limited so I cannot\nmake a qualified statement about these formal analyses in a reasonable amount\nof time.\n\nThe authors also transfer the results from the convex online setting to the\nconvex stochastic setting and the nonconvex setting.\n\nBased on my limited understanding I would recommed to accept the paper.\nThe analysis to me seems both rigorous and useful in practice\n(at least with regard to the formal definitions of Riemannian metrics and\nRiemannian Lipschitz condition for singular functions).\n\nRemarks / Suggestions:\n- With a similar knowledge of the underlying function it would perhaps be\n\tpossible to perform a nonlinear transformation of the input space that\n\tleads to Lipschitz continuous function.\n\tCan something be said about this?\n\n- Definition 2: Write out l.s.c. (lower semi-continuous?) as it does not seem\n\tto be defined everywhere and not every reader is necessarily familiar\n\tenough with convex analysis\n\n- Page 8: Typo: \"Eucldiean stochastic gradient method\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper investigates online and stochastic convex optimization problems in which the objective function is not Lipschitz continuous. The originality of this study lies in the use of Riemannian geometry. Specifically, the standard condition of Lipschitz continuity is replaced with a more general condition involving Riemannian distances and called Riemann-Lipschitz Continuity (RLC). Based on an appropriate definition of Riemannian regularizer and a generalization of Fenchel coupling to Riemannian geometry, the authors provide $O(\\sqrt T)$ regret (resp. risk) bounds for the online (resp. stochastic) mirror descent algorithm, under the Riemann-Lipchitz condition. The performance of the algorithm is validated on Poisson inverse problems.\n\nOverall, this is a dense, yet interesting, paper. I am not an expert in Riemannian geometry but, as far as I could check, the proofs look correct. Notably, the analysis of OMD is relatively standard, once we get a bound (Prop B.1) on the Fenchel coupling, using the Riemannian dual norm. \n\nI have essentially one main comment. Clearly, the concept of “Riemann-Lipschitz continuity” is different from the notions of “relative continuity” and “relative smoothness” that have been recently proposed in the literature. But it is not clear that the Riemann-Lipschitz condition can tackle convex optimization tasks in which relative continuity and relative smoothness do not hold. In particular, Poisson (linear) inverse problems have already been handled under the relative smoothness condition, using Mirror Descent or Bregman proximal methods (Hanzely and Richtarik, 2018; Hanzely et. al. 2018). Thus,\n* from a conceptual viewpoint, it would be interesting to provide some applications in which the RLC condition hold, but the relative smoothness condition does not; \n* from an experimental viewpoint, in Sec. 6, it would be legitimate to compare the present “Riemannian Mirror Descent” algorithm with respect to the APBG method (Hanzely et. al. 2018) and the relSGD method (Hanzely and Richtarik, 2018). \n"
        }
    ]
}