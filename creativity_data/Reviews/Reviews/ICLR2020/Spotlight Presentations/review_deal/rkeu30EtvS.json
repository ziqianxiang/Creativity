{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents a feature normalization method for CNNs by decorrelating channel-wise and spatial correlation simultaneously. Overall all reviewers are positive to the acceptance and I support their opinions. The idea and implementation is relatively straightforward but well-motivated and reasonable. Experiments are well-organized and intensive, providing enough evidence to convince its effectiveness in terms of final accuracy and convergence speed. Also, itâ€™s analogy to biological center-surrounded structure is thought provoking. The novelty of the method seems somewhat incremental considering that there already exists a channel-wise decorrelation method, but I think the findings of the paper are interesting and valuable enough for ICLR community and would like to recommend acceptance.\nMinor comments: I recommend authors to mention about zero-component analysis (ZCA) normalization, which has been a standard input normalization method for CIFAR datasets. I guess it is quite similar to the proposed method considering 1x1 convolution. Also, comparison with other recent normalization methods (e.g., Group Norm) would be useful. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses the correlation present in the input data, which may affect learning kernels with redundancy. Therefore, they introduce  a deconvolution mechanism on the input features to remove spatial and channel-wise correlation,  before these features are fed to network layers. They also show the deconvolution effect in the first layer resembles the center-surround effects found in biological neuron and induce sparsity in representation. They draw analogy to batch normalization, and show superiority in terms of  convergence and accuracy. \n\nOverall, the concept of the paper is pretty simple and straightforward - basically it removes the correlation present in the input data, specifically in the case of convolution.The experimental results are promising and show fast convergence over using batch normalization, slightly better accuracy.\n\nHowever, it seems that the deconvolution to avoid correlation is helping for classification tasks, but may not be applicable for other related tasks such as semantic segmentation, where simply using BN may work.\n\nWhat is G in the overall complexity? No. of groups? Please define. \n \nOne of the questions that comes to mind is considering PCA normalization for the same purpose. Essentially, the step of computing the covariance is the same and decorrelating the data by  using the orthonormal basis vectors, pretty much with a similar motivation. Also, PCA is a linear transformation, which points to a way of learning in the network training. So, my question is what will be the problem of doing PCA transformation and then performing convolutions on that transformed decorrelated data? Maybe layerwise PCA transformation may also help in performance and reduce complexity? I think this discussion should be added to the normalization and whitening section of related works.\n\nMinor note: The recommended page limit is 8 pages, the paper has now 10 pages. For me, Figure 1 is not conveying much information, and can be easily replaced with text. Also, I do not see the need of an extra section for the neurological basis for deconvolution; the brief discussion in the intro is enough.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an operation for removing the pixel-wise and channel-wise correlations of input features. This operation can be considered as a generalization form of the former proposed decorrelated batch normalization. The approach has a well-sounded neurological inspired motivation and a solid explanation of the relationship with deconvolution. In the experimental analysis, the authors demonstrate a good performance of the proposed method compared with batch normalization. Also, the authors provide the CPU time of the proposed method, which is very appreciated. \n\nOverall, this paper has reasonable contributions to the learning algorithm of the deep neural network, so I recommend the AC to accept this paper.\n\nHowever, I have 2 negative points on this paper.\nFirst, the computation cost for the im2col based convolution operation in a large kernel (eg. 7x7) is insanely large, and the authors only show us the results using VGG, which only has small 3x3 kernels.\nSecond, the arguments made on the sparse representations is somehow not convincing to me, it is really difficult to say the sparse representations has mad regularizations more effective with only 2 learning curves"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes \"network deconvolution\", a neural network primitive aimed at whitening the activations of each layer of the network. The method is a generalization of batch normalization that not only whitens per channel, but also removes correlations between channels and across spatial locations. Experiments show that the proposed methods improves training speed and predictive accuracy on a number of image classification models.\n\nThe method is novel and the proposed implementation details constitute a significant technical contribution. The experiments are not exhaustive and leave some open questions, but the first results are highly promising. The paper is clearly written, and embeds the proposed method in the literature.\n\nI'd like to see more discussion and experiments on:\n- What's the dependence on batch size? Does the method work better for larger batches? Or are small batches better for the added regularization effect, like what occurs with batch norm?\n- Are results sensitive to the epsilon in algo 1? How is this chosen in practice and how does it interact with the approximate inversion of the covariance matrix?\n- Under what conditions do you observe a speed-up in convergence? The only training curves shown in the main paper are on Fashion-MNIST which I don't think is very interesting. The appendix does offer a bit more information but I feel this deserves more attention.\n- How does this method interact with regularization methods? For batch normalization much of the benefit seems to come from the noise it adds to network activations; is the same true for your method? Does network deconvolution still improve final accuracy if the baseline uses more extensive data augmentation or other regularization?"
        }
    ]
}