{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper addresses individual fairness scenario (treating similar users similarly) and proposes a new definition of algorithmic fairness that is based on the idea of robustness, i.e. by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased.\nAll reviewers and AC agree that this work is clearly of interest to ICLR, however the reviewers have noted the following potential weaknesses: (1) presentation clarity -- see R3’s detailed suggestions e.g. comparison to Dwork et al, see R2’s comments on how to improve, (2) empirical evaluations -- see R1’s question about using more complex models, see R3’s question on the usefulness of the word embeddings. \nPleased to report that based on the author respond with extra experiments and explanations, R3 has raised the score to weak accept. All reviewers and AC agree that the most crucial concerns have been addressed in the rebuttal, and the paper could be accepted - congratulations to the authors! The authors are strongly urged to improve presentation clarity and to include the supporting empirical evidence when preparing the final revision.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "General:\nThe authors propose a method to train individually fair ML models by pursuing robustness of the similarity loss function among the comparable data points. The main algorithmic tool of training is borrowed from the recent adversarial training, and the paper also gives the theoretical analyses on the convergence property of their method. \n\nPros:\n1. They make the point that the individual fairness is important. \n2. The paper proposes a practical algorithm for achieving the robustness and the indivdual fairness. Formulating that the main criterion for checking the fainess is Eq.(2.1), the paper takes a sensible route of using dual and minimax optimization problem (2.4).\n3. The experimental results are compelling – while the proposed method loses the accuracy a bit, but shows very good individual fairness under their used metric. \n\nCons & Questions:\n1. What is the empirical convergence property of the algorithm? How long does it take to train for the experiments given?\n2. It seems like the main tools for algorithm and theory are borrowed from other papers in adversarial training e.g., (Madry 2017). Are their any algorithmic alternatives for solving (2.4)?\n3. Why do you use d_z^2 instead of d_z for defining c(z_1,z_2)?\n4. What happens when you use more complex models than 1 layer neural net?"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "\nSummary\nThe authors propose training to optimize individual fairness using sensitive subspace robustness (SenSR) algorithm.\n\nDecision\nOverall, I recommend borderline as the paper seems legit in formulating the individual fairness problem into a minmax robust optimization problem. The authors show improvement in gender and racial biases compared to non-individual fair approaches. However, I think some sections are hard to follow for people not in the field.\n\nSupporting argument:\n1. End of P3, it is not clear to me why solving the worst case is better.\n2. Though this paper studied individual fairness, can it also work for group fairness? I am not sure whether this is the only work in this direction (baselines are not for individual fairness).\n3. Some of the metrics in the experiments are not precisely defined such as Race gap, Cuis. gap, S-Con, GR-Con. It is hard to follow from the text description. \n4. Some baseline models are not clearly defined such as “Project” in Table 1.\n5. Not sure how Section 3 connects with the rest of the paper.\n\n\nAdditional feedback:\n1. Missing reference: https://arxiv.org/abs/1907.12059\n2. What’s TV distance in introduction?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This paper proposes a new definition of algorithmic fairness that is based on the idea of individual fairness. They then present an algorithm that will provably find an ML model that satisfies the fairness constraint (if such a model exists in the search space). One needed ingredient for the fairness constraint is a distance function (or \"metric\") in the input space that captures the fact that some features should be irrelevant to the classification task. That is, under this distance function, input that differ only in sensitive attributes like race or gender should be close-by. The idea of the fairness constraint is that by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased. Thus, this fairness constraint is very much related to robustness.\n\n---\n\nOverall, I like the basic idea of the paper but I found the presentation lacking.\n\nI do think their idea for a fairness constraint is very interesting, but it gets too bogged down in the details of the mathematical theory. They mention Dwork et al. at the beginning but don't really compare it to their idea in detail, even though I think there would be a lot of interesting things to say about this. For example, the definition by Dwork et al. seems to imply that some labels in the training set might be incorrect, whereas the definition in this paper does not seem to imply that (which I think is a good thing).\n\nThe main problem in section 2 is that the choice of distance function is barely discussed although that's what's most important to make the result fair. For all the mathematical rigor in section 2, the paragraph that is arguing that the defined constraint encourages fairness is somewhat weak. Here a comparison to other fairness definitions and an in-depth discussion of the distance function would help.\n\n(In general I felt that this part was more trying to impress the reader than trying to explain, but I will try to not hold it against this paper.)\n\nAs it is, I feel the paper cannot be completely understood without reading the appendix.\n\nThere is also this sentence at the bottom of page 5: \"A small gap implies the investigator cannot significantly increase the loss by moving samples from $P_*$ to comparable samples.\" This should have been at the beginning of section 2 in order to motivate the derivation.\n\nIn the experiments, I'm not sure how useful the result of the word embedding experiment really is. Either someone is interested in the sentiment associated with names, in which case your method renders the predicted sentiments useless or someone is not interested in the sentiment associated with names and your method doesn't even have any effect.\n\nFinal point: while I like the idea of the balanced TPR, I think the name is a bit misleading because, for example, in the binary case it is the average of the TPR and the TNR. Did you invent this terminology? If so, might I suggest another name like balanced accuracy?\n\nI would change the score (upwards) if the following things are addressed:\n\n- make it easier to understand the main point of the paper\n- make more of a comparison to Dwork et al. or other fairness definitions\n- fix the following minor mistakes\n\nMinor comments:\n\n- page 2, beginning of section 2: you use the word \"regulator\" here once but everywhere else you use \"investigator\"\n- equation 2.1: as far as I can tell $M$ is not defined anywhere; you might mean $\\Delta (\\mathcal{Z})$\n- page 3, sentence before Eq 2.3: what does the $\\#$ symbol mean?\n- page 3, sentence before Eq 2.3: what is $T$? is it $T_\\lambda$?\n- Algorithm 2: what is the difference between $\\lambda^*_t$ and $\\hat{\\lambda}_t$?\n- page 7: you used a backslash between \"90%\" and \"10%\" and \"train\" and \"test\". That would traditionally be a normal slash.\n- in appendix B: the explanation for what $P_{ran(A)}$ means should be closer to the first usage\n- in the references, you list one paper twice (the one by Zhang et al.)\n\nEDIT: changed the score after looking at the revised version",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}