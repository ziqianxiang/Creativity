{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposes a novel GAN formulation where the discriminator outputs discrete distributions instead of a scalar. The objective uses two \"anchor\" distributions that correspond to real and fake data. There were some concerns about the choice of these distributions but authors have addressed it in their response. The empirical results are impressive and the method will be of interest to the wide generative models community. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes to improve upon GANs by considering to infer the distribution of realness instead of binary true/false labels in the discriminator side. They carry out this idea with some theoretical arguments, and their method is shown to perform very well in empirical experiments on one synthetic dataset, and three real world datasets:CelebA, CIFAR-10, and FFHQ. Overall I feel this is a well presented paper with a simple yet interesting idea and solid results. The authors are encouraged to share their code and results to public.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Update: I raised my score from 3 to 6 after the authors addressed most of my comments.\n\n====================================================\n\nThis paper propose a new GAN formulation where the Discriminator outputs a discrete probability distribution instead of a scalar for each inputs. This discrete probability distribution outputted by the discriminator is then compared, using the KL divergence, to two different reference distributions according to if the image is from the dataset or generated. The paper show that the proposed approach is a generalization of the standard GAN. They then prove that under some condition that similarly to GAN at the optimum $p_g = p_data$. In addition the paper propose two tricks 1) they include an additional term in the loss for the generator, such that the generator is also trying to minimize the KL between the discriminator distribution for a generated and the discriminator distribution for a real samples.  2) They propose some procedure to resample the logits of the Discriminator. \nThey show on a toy example how increasing the dimension of the distribution of the Discriminator also increases the performance. They also show that their method can slightly improve performance on CelebA and CIFAR10 and that it can also scale to high resolution images on FFHQ.\n\nI'm in favour of rejecting this paper. In particular I find the method not very well motivated for several reasons. First it's never explained in the text how the reference distribution $A_0$ and $A_1$ are chosen, and so it's not clear what they represent. Second it's not clear why we need the two tricks or wether the method would also work without the proposed tricks.\n\nMain Argument:\n- Please provide an explanation how $A_0$ and $A_1$ are chosen and what are the $A_0$ and $A_1$ chosen in the experiments. This seems very critical to the performance of the method as shown in table 2 but explained nowhere in the paper.\n- What is the effect of the \"Relativistic\" loss ? Does the method work if we remove it ? An extended ablation study would be nice and give more insight on what is really important for the performance of the method ?\n- The author claim in the experimental section that their method perform betters on both datasets, however when looking at table 1, WGAN-GP performs better on CIFAR10 and when looking at the standard deviation we can see that the improvment is not significative.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "Post rebuttal: The authors' responses have addressed most of my concerns, and I've raised my rating from 3 to 6.\n\n----------------------------------------\n\nSummary:\nThis paper extends the discriminator of GAN to use a distributional output (multiple scalars) instead of a single scalar. As a result, the trained GAN becomes robust to the mode collapse.\n\nPros:\n- The proposed method is clearly written and well-justified (e.g., Theorem 2).\n- Extension of the relativistic GAN [1] to the proposed setting is interesting.\n- The authors demonstrate that vanilla DCGAN architecture can generate high-fidelity (1024x1024) images.\n\nCons:\n\n1. An ensemble of discriminators?\n\nThe authors use multiple scalars to consider diverse factors of the realness. However, it is simply an ensemble of discriminators [2] in a spirit. As each discriminator focus on different factors, it is not surprising that the generator becomes robust to the mode collapse. Also, recent work on mode collapse (e.g., [3]) shows better results on the mixture of gaussian experiments even using a single discriminator. At least, the authors should compare their method with the ensemble methods and claim the advantage over them.\n\n2. Choice of the anchor distributions.\n\nThe choice of anchor distributions A_0 and A_1 are not specified. While the authors provide some partial results in Table 2, it would be worthwhile to clarify the experimental details and justify them.\n\n3. Role of each outcome u_i?\n\nThe authors claim that each outcome u_i corresponds to the different factors of realness. However, the role of learned u_i is not investigated. Also, one may enforce u_i to learn different factors by promoting diversity of them, e.g., decrease their cosine similarity [4].\n\nMinor comments:\n- The word \"support\" [5] is misused. The support itself means the set of non-zero elements, hence the authors should use the word \"outcome\" (or \"sample\") instead of \"support\".\n- The notation is not consistent. For example, the authors may use \"x \\sim p_data(x)\" (specify variable) or \"z \\sim p_z\" (omit variable), but not both.\n- Numbering is not consistent. For example, \"Tab.4.2.\" should be changed to \"Tab.2.\" for consistency.\n\n\n[1] Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard GAN. ICLR 2019.\n[2] Durugkar et al. Generative Multi-Adversarial Networks. ICLR 2017.\n[3] Xiao et al. BourGAN: Generative Networks with Metric Embeddings. NeurIPS 2018.\n[4] Elfeki et al. GDPP: Learning Diverse Generations Using Determinantal Point Process. ICML 2019.\n[5] https://en.wikipedia.org/wiki/Support_(mathematics)",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}