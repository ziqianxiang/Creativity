{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposes a doubly robust off-policy evaluation method that uses both stationary density ratio as well as a learned value function in order to reduce bias.\nThe reviewers unanimously recommend acceptance of this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a new algorithm for the off-policy evaluation problem in reinforcement learning. It combines the value function learning method and the stationary distribution ratio estimators. The proposed method achieves double robustness, which means the proposed estimator is consistent as long as the value function or ratio estimator is consistent. Empirical results on some control domains are presented to verify the effectiveness of the algorithm.\n\nI think this paper has some nice contribution to the area, by introducing a doubly robust estimator based on the density ratio, and also a new idea to achieve double robustness. I will vote for accept, but I think there is room for improvement of this paper.\n\nDetailed comments:\n - The proposed estimator is not using control variate but using dual structure between value function and stationary distribution ratio, which is a novel idea comparing with similar doubly robust estimators. \n - Theorem 3.2, or at least the way it is presented, is less intuitive and makes me confused. If the variance of the DR estimator is always larger than the variance of value function, why I should use this estimator instead of value function. If the argument is the MSE of value function is potentially larger due to the bias. Then an effective analysis would be about MSE instead of variance.\n - If I have an oracle of density ratio, is the doubly robust estimator still unbiased, which is generally true for DR using control variate? This would be an important point to compare this work with control variate methods.\n - Very recent work https://arxiv.org/abs/1908.08526 also proposes a doubly robust estimator in similar settings. It's worth to mention it in the related work."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Comments : \n\nThis paper provides an approach for reducing bias in long horizon off-policy evaluation (OPE) problems, extending recent work from Liu et al., 2018 that estimates the ratio of the stationary state distributions in off-policy evaluation for reducing variance. The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function. The key idea of the paper is to provide low variance, low bias OPE since their approach relies on accurately estimating the state distribution ratio and also the estimation of the value function. \n\nThe paper provides a follow up on Liu et al., 2018 and other recent works in off-policy learning that tries to estimate the state distribution ratio directly, but can introduce high inaccuracy if the obtained ratio estimates are inaccurate. In line of that, this approach seems useful as it tries to reduce the error from inaccurate ratio estimates by incorporating prior works with an additional value function estimation. \n\nFurthermore, the paper introduces a new perspective to doubly robust estimation that tries to reduce bias, instead of variance in previously known OPE literature (Jiang et al., 2016; Thomas et al., 2016). It is interesting to see how doubly robust can be related to primal-dual relations and the connections between these approaches, which is a novel contribution to the best of my knowledge. \n\n- The fundamental connection comes from observing OPE withdoubly robust estimator that estimates \\hat{V} and equation 6 that incorporates the ratio of the state density. This is clearly written in equations 7 and 8, that are two ways of evaluating the value function with off-policy samples known in the literature.\n\n- It uses the property of the Bellman recursive expression for the estimated value function of doubly robust OPE estimator and uses it in the OPE with state density ratio, leading to equation 9, and obtaining the bridge estimator that now relies on both estimates of value function \\hat{V} and the state density ratio. Although initially this does not seem useful, but the authors clarify this and how a bias reducting method can be achieved as in equation 11. \n\n- It is a nice and elegant trick, exploiting the connections between OPE estimators leading to a bias reduction method that seems quite interesting. \n\n- The most interesting part of the paper comes from section 4 that exploits the connection between doubly robust approaches with Lagrangian duality, and that their approach is equivalent to a primal dual formulation of policy evaluation. This stands out in itself as a novel contribution of the paper. Equations 14 and 15 best writes down the connections, as to how policy evaluation can be formualted as a Lagrangian function for optimization. \n\n- Although the authors point out how equation 15 is equivalent to equation 11 - this does not seem straight forward at first unless carefully looked at. I would encourate the authors to perhaps add more clarity that exploits this connection, to make the paper more self-contained. \n\n- In terms of experiments, the paper compares the doubly robust approach with previous works that estimates the density ratio, along with other baseline comparisons such as weighted DR. In both the two evaluated problems, their approach seems useful in terms of obtaining better accuracy (lower MSE). \n\n- However, I am not sure to what extent this approach can be scaled to more complicated tasks for OPE? Are there any example domains where the proposed method fails, or is difficult to scale up to more complicated tasks? \n\n- The current set of experiment results seems adequate, given the theoretical contribution and that most OPE papers evaluate on such domains. But overall, it might be useful to analyse the significance of this approach when scaling to more complicated domains. \u000b\n\n\nOverall, I think the paper has a neat and elegant theoretical contribution, exploiting the connection of OPE with primal-dual frameworks that seems quite novel to me. Experimental set of results are properly presented too showing significance of the approach compared to previously known baselines. I think such papers exploting connections with other literature is useful for the community in general, and this paper has significantly novel theoretical contribution. Hence, I would recommend for acceptance of this paper. \n\n\u000b"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "  *Synopsis*:\n  This paper provides a new doubly robust estimator for off-policy policy evaluation, based on the new infinite horizon technique (i.e. using an estimate of the state density ratio as opposed to long products of action importance weights). They show the doubly robust estimator's bias is dependent on a product of the error of the value function estimate and stationary distribution ratio estimate, which provides improvements over the initial infinite horizon estimator. They also provide some nice discussion of the relation of their method and Lagrangian duality, which was quite interesting and insightful. Finally, the paper shows the usual empirical comparisons.\n\n  Main Contributions:\n  - Doubly robust estimator for infinite horizon off-policy policy evaluation\n\n  *Review*:\n  Overall, I quite like this paper and think the quality is at a high level. The proposed doubly robust estimator is well supported theoretically and empirically and improves on the prior art. I am recommending a weak accept for this paper as it will be a nice contribution for the community, but I have some clarifications/updates I would like to see to improve the readability of the paper (specifically C4).\n\n  I also have a few questions, and clarifications that I would like the authors to address during the rebuttal period.\n\n  *Clarifications*:\n  C1: In the section \"Off-policy state visitation importance sampling\", doesn't equation 4 involve an expectation over the target policy? Or am I missing something here?\n\n  C2: The bias of the estimator decreases as our estimate of the density ratio and value function improves. It might be useful to more clearly compare this bias to the original estimator proposed by Liu.\n\n  C3: Proof of theorem 3.1: I would like you to clearly finish the proof. I think clarity and completeness in appendices is importance over conciseness as there is usually no limit on pages.\n\n  C4: The proof of theorem 3.2 is not obviously linked to theorem A.3. The proof for theorem 3.2 should be clearly stated and included in the appendix, without an unstated implicit link to A.3. This will make your theoretical analysis more clearly understandable, and expendable for future work. I will happily increase my score if this is clarified in a future version. I also will decrease my score if this is not clarified.\n\n  *Competitors*\n\n  - You may want to include a model based approach, just for completeness (as in the infinite horizon paper from Liu).\n\n  *Questions*\n  \n   Q1: I'm curious about the difference between using an ANN for estimating the density ratio (as opposed to a kernel method). Have you run experiments with the kernel method proposed in Liu's paper? While I don't think it is necessarily needed for the paper to be complete, I think the difference would be interesting to see what factors contribute to the algorithm's better performance.\n\n\n  Minor typos not taken into account for the review:\n  - Section 3.1 \"it is useful to exam the\": exam -> examine\n  - Section 4: \"hence yielding an true expected reward\": an -> a\n  - Section 4: \"it is natural to intuitize\": intuitize -> intuit?\n  - The sentence right before section 4 could use a rewrite.\n  - For readability it would be useful to include theorem statements ahead of proofs in the appendix.\n  - Theorem A.3: do you mean with \\hat{R} defined in 3.2 (not the variance as this is not defined in 3.2).\n\nEdit:\n\n- Due to the author rebuttal and updates to the paper I've increased my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}