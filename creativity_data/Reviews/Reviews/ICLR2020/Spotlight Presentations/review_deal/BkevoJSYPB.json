{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a method for efficiently training neural networks combined with blackbox implementations of exact combinatorial solvers.\n\nReviewers and AC agree that it is a well written paper with a novel idea supported by good experimental results. Experimental results are of small scale and can be further improved, but the authors acknowledged this aspect well.\n\nHence, I recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper shows how end-to-end learning can be done through\ncombinatorial solvers by using the derivative of\ncontinuous surrogate function in the backward pass.\nOne elegant part of the method is that no modification\nor relaxation is done to the combinatorial solver in\nthe forward pass and that the backward pass just requires\nanother call to the blackbox solver.\n\nThe idea of constructing continuous surrogate functions\nand using them for differentiating through solvers with\npiecewise-constant output spaces is thought-provoking and\nI can see it inspiring many new directions of work.\nFor example looking at Figure 2 for intuition, one could\nimagine other ways of making the solution space continuous.\nThe solution space of linear programs over continuous spaces,\nas considered in [Elmachtoub & Grigas], the Sudoku example in\n[Amos & Kolter], and related papers, is also piecewise constant and\nit seems like a similar method could be used to bring more\ninformative derivative information to linear programs ---\nhave you considered this as a future direction?\n\nOne of my concerns with this work is that the ResNet baseline in the\nexperimental results seems like too much of a straw man for the tasks.\nI do not see why they should have the capacity to generalize well.\nThis paper shows the ResNet baseline achieve near-zero\ntest accuracy but doesn't compare to other relevant baselines\nthat are mentioned in the related work section:\nfor example [Bello et al, Deudon et al., Kool et al.] for the TSP.\n\nAnd one smaller comment: If one wanted to squeeze the performance even\nmore, would starting the training process with a large \\lamdba\nand annealing it to zero help?\n\n----\n\nElmachtoub, A. N., & Grigas, P. Smart \"predict, then optimize\". arXiv 2017.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a straightforward method for training black box solvers of a restricted kind (namely those with inputs in R^n and linear cost functions). The proposed algorithm is tested on path finding, the travelling salesman problem, and a min-cost-perfect-matching problem, with promising results.\n\nI would recommend accepting this paper. It is a well written paper with a novel idea supported by good experimental results.\n\nThe caveat is that I did not have the time to thoroughly review all the mathematical details. From a high-level they looked correct, and the math is sufficiently illustrated with figures and examples that it is easy for a reader to follow in detail given enough time.\n\nThe main shortcomings I see are that there are no experimental results comparing this method against any existing results; the authors do compare against their own ResNet18 implementation, but this is not ideal.\n\nI found the discussion a bit cryptic: Why are approximate solvers needed for real-world problems? Are there no real-world problems where exact solvers are still applicable?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "=== Summary ===\nThe authors propose a method for efficiently backpropagating through unmodified blackbox implementations of exact combinatorial solvers with linear objective functions. \nThe gradient of such exact combinatorial solvers exists almost everywhere but is zero. The authors remark that the loss has the same gradient wrt to the solver's input as its linearization around the solver's input.  They therefore propose to interpolate the loss' linearization with a continuous (piecewise affine) function and use the gradient of this interpolation to backpropagate through the solver. This gradient is obtained efficiently by simply calling the solver on a single perturbed input (the perturbation depends on the incoming gradient, ie the gradient of the loss wrt to the solver's output).\nThe authors further study the properties of this piecewise affine interpolation and characterize its interpolation behavior as a function of a hyperparameter which controls the trade-off between \"how informative the gradient is\" and \"how faithful the interpolation is to the original solver\".\nThe authors validate their method with experiments on synthetic tasks that have both a visual processing aspect and a combinatorial aspect:\n    - Shortest Path on Warcraft II terrain maps\n    - TSP between country capitals where the inputs to the convnet are country flags\n    - Min-cost perfect matching from Mnist digits.\nSpecifically, they feed the output of a convnet to the relevant solver (depending on the task) and learn end-to-end by backpropagating through the solver with their proposed method. They show that their method successfully solves the tasks where baseline ConvNet architectures fail.\n\n=== Recommendation ===\n\nThis paper addresses an important problem and presents a novel approach.\n\nMethods for combining combinatorial optimization algorithms and machine learning usually rely on modifiying or relaxing the combinatorial problem itself which prevents using solvers as-is. \nIn contrast, the presented method allows to efficiently backpropagate through unmodified implementations of blackbox exact solvers with a linear objective. AFAIK this is the first method that allows this.\n\nA weakness of the paper is that the experiments only validate proof of concept (as noted by the authors). They are small-scale and only compare against conventional ConvNets baselines (as opposed to other approaches to backpropagate through relaxed combinatorial problems).\nAdditionally, the characterization of the interpolation (whose gradient is used) doesn't directly explain why the gradient of the interpolation is a reasonable choice.\n\nOverall, I recommend for acceptance.\n\n=== Questions / Comments ===\n- The authors show properties related to the interpolation behavior of the proposed interpolation function. What is the actual point/benefit of satisfying these properties? Are there arguments for why this is important besides the experimental results?  Is the point that since lambda controls how \"faithful vs informative\" the gradient is , there must be a range of values for lambda for which the method works? \n- It would be interesting to have experiments with non-exact solvers\n- It would be interesting to optimize directly for the combinatorial objective in the experiments (using a policy gradient for example) rather than perform supervised learning on the solutions.\n- Consider adding related work subsection on argmin optimization and meta-learning.\n\n\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}