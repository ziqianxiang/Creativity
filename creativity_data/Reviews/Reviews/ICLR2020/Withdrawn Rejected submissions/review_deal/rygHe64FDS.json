{
    "Decision": {
        "decision": "Reject",
        "comment": "Main content:\n\nBlind review #2 summarizes it well:\n\nThis paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a “reference” gradient computed on a “secret” validation set.  If the score is under a given threshold, then the worker gradient is discarded. \n\nAuthors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.\n\n--\n\nDiscussion:\n\nReviews are generally weak on the limited novelty of the approach compared with Zeno, but the rebuttal of the authors on Nov 15 is fair (too long to summarize here).\n\n--\n\nRecommendation and justification:\n\nI do not feel strongly enough to override the weak reviews (but if there is room in the program I would support a weak accept).",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThis paper proposes an approach to Byzantine fault tolerance in asynchronous distributed SGD. The approach appears to be novel. Theoretical convergence guarantees are provided, and an empirical evaluation illustrates very promising results.\n\nOverall, the paper is well-written and the results appear to be novel and interesting. Although I have a few questions, listed below, I generally lean towards accepting this paper.\n\n\nThe assumption of a lower bound on validation gradients is somewhat troubling, especially for over-parameterized problems where so-called \"interpolation\" may be possible. I realize that validation samples are never used explicitly for stochastic gradient updates, but the algorithm does ensure that the stochastic gradients used are similar to gradients of validation samples. If one is converging to a (local) minimizer, one wants the gradient to vanish. How do we reconcile these points? Also, to properly set $\\rho$, for the theory to be valid, one needs to know this bound (or a lower bound on $V_2$). Is this practical?\n\nThe paper claims that the computational overhead of Zeno+ is too great to evaluate for comparison with Zeno++. From my reading of the two methods, it isn't immediately obvious to me why this is the case. Including experiments which at least compare the per-iteration runtime (even if not running Zeno+ for training to completion) would make the paper more compelling. After all, Zeno++ still involves periodically evaluating the gradient at a validation sample.\n\nThe paper makes the reasonable point that it is not reasonable to assume a bounded number of adversaries in the asynchronous setting, and the theorem statements make no assumption about the number of adversaries or rate at which received gradients are from a Byzantine worker. However, there are also no guarantees about whether the algorithm will ever make progress (i.e., will line 8 ever be reached?). This should be stated more transparently in the paper. Also, I was wondering, given that a gradient has been computed on the parameter server's validation set, which is assumed to be \"clean\", why not take a step using this gradient when the test in line 7 fails?\n\nFinally, the paper titles includes SGD, but the description in Def 1 doesn't appear to involve stochastic gradients. Typical parameter server implementations have workers compute mini-batch stochastic gradients, not full gradients on their shard of the training set. Does Zeno++ need to be modified to run in this setting? Does the theory still hold?\n\n\nMinor:\n- Is there a typo in line 5 of Zeno++? Should this be $\\nabla f_s$ instead of $f_s$? Otherwise, what does it mean to take the inner product of $v$ and $g$ in line 7?\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "Summary:\nThis paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a “reference” gradient computed on a “secret” validation set.  If the score is under a given threshold, then the worker gradient is discarded. \n\nAuthors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.\n\nOriginality:\nI would argue that the paper novelty is limited.  Paper builds upon the Zeno algorithm.  From the paper, the main changes with respect to Zeno algorithm is the use of a hard-threshold instead of a ranking to adapt Zeno to the asynchronous case, and the use of a first-order Tayler approximation of the score to speed up its computations.\n\nClarity:\nPaper is clear and easy to follow.\n\nQuality:\nMy main concerns are related to the experimental section. Authors only report results for one model on one dataset. It is unclear how those results would transfer to different tasks and architectures. In addition, the experiments are relatively small scale (10 workers), how does the system scale as you increase the number of workers?\n\nThe top-1 reported on CIFAR10 seems pretty low with respect to the current state-of-art. It would be nice to use a more competitive model such as a Resnet-18 to verify that one can achieve similar performance with Zeno++ compared to AR-SGD (without attack).\n\nAuthors introduced Zeno++ to reduce the computation overhead over Zeno+. Did you empirically quantify this speed-up, and do you achieve similar accuracy than Zeno+?\n\nSignificance:\nAuthors only compare with asynchronous baseline. It would be nice to demonstrate the advantage of asynchronous methods over synchronous one (such as Zeno and All-Reduce SGD without attack). Can you show a speed benefit of asynchronous Zeno++ and show similar accuracy than synchronous approaches? \n\nMinor:\n-\tIt would be to reports use a log scale in Fig 1 b), c) and Fig 2. b), d).\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper addresses security of distributed optimization algorithm under Byzantine failures. These failures usually prevent convergence of training neural network models. Focusing on the asynchronous SGD algorithm implemented with a parameter-server, the authors propose to use stochastic line search ideas to detect whether the gradients are good descent directions or not. It applies to a general scenario including repeated and unbounded Byzantine failures. \n\nTheoretical results of this paper seem to me to have some flaw. In the proof of Theorem 1, line 6, it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s (x_tau) ||^2. Clearly the g comes from any worker which can be very different to grad f_s (x_tau). Therefore, I recommend the authors check the proof of both theorem 1 and 2 more carefully.\n\nNumerical results show that the proposed algorithm Zeno++ works well with sign-flipping attacks. However, this scenario is very limited to validate all imaginable Byzantine failures that this paper would like to address. For example, one can use random gradients instead of sign-flipped gradients as a Byzantine attack, would the algorithm still work? \n"
        }
    ]
}