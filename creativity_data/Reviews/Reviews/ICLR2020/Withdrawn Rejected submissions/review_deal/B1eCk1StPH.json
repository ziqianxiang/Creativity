{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that \"Bayesian ideas\" explain what's going on seems obviously wrong to me. The third reviewer seems to think there's a tautology lurking here and that doesn't seem to be true to me either. It is disappointing that the reviewers did not re-engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year. \n\nEven though I'm inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse.\n\n\n[1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies a puzzling question: if larger parameter counts (over-parameterization) leads to better generalization (less overfitting), how does pruning parameters improve generalization? To answer this question, the authors analyzed the behaviour of pruning over training and finally attribute the pruning's effect on generalization to the instability it introduces.\n\nI tend to vote for a rejection because \n(1) The explanation of instability and noise injection is not new. Pruning algorithms have long been interpreted from Bayesian perspective. Some parameters with large magnitude or large importance contribute to large KL divergence with the prior (or equivalently large description length), therefore it's not surprising that removing those weights would improve generalization. \n(2) To my knowledge, the reason why over-parameterization improves generalization (or reduces overfitting) is because over-parameterized networks can find good solution which is close to the initialization (the distance to the initialization here can be thought of as a complexity measure). In this sense, the effect of over-parameterization is on neural network training. However, pruning is typically conducted after training, so I don't think the fact that pruning parameters improves generalization contradicts the recent generalization theory of over-parameterized networks. Particularly, these two phenomena can both be explained from Bayesian perspective."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This paper mainly studies the relationship between the generalization error and mean/variance of the test accuracy. The authors first propose a new score for pruning called E[BN]. Then, the authors observe the generalization error and the test accuracy mean/variance for pruning large score weights and small score weights for VGG11, ResNet18, and Conv4 models. From these experiments, the authors observe that pruning large score weights generates instable but high test accuracy and smaller generalization gap compared to pruning small score weights. The authors additionally study some other aspects of pruning (e.g., pruning as a noise injection) and conclude the paper.\n\nOverall, I am not sure whether the observation holds in general due to the below reasons. \n\n- The authors proposed a new score E[BN] and all experiments are performed on this. However, how it differs from the usual magnitude-based pruning in practice is unclear. I would like to know whether similar behavior is observed for the naïve magnitude-based pruning.\n\n- I think that the author’s observation is quite restricted and cannot extend to a general statement since the experiments are only done for pruning small score/large score weights. To verify the generalization and instability trade-off, I believe that it is necessary to examine several (artificial) pruning methods controlling the instability of test accuracies and check whether the proposed trade-off holds. For example, one can design pruning methods that disconnect (or almost disconnect) the network connection from the bottom to the top (i.e., pruned network always outputs constant) with some probability to extremely increase the instability.\n\n- The authors did not report the results for high sparsity.\n\nBesides, I am not sure the meaning of the instability since when the test accuracy of the pruned model is higher than that of the unpruned model, the instability could be large.\n\nOther comments:\n- The first paragraph mentions that the generalization gap might be a function of the number of parameters. However, I think that it is quite trivial that the generalization gap is not a function of the number of parameters while it only provides the upper bound.\n----------------------------------------------------------\nI have read the authors' response. Thanks for clarifying the definition of instability and additional experiments with high sparsity. However, I will maintain my score due to the following concern. \n\nThe remaining concern is that the current evidence for verifying generalization-stability tradeoff is not convincing as the authors presented only some examples having small and large instability (e.g., pruning smallest/largest weights) under the same pruning algorithm. I think that the results would be more convincing if the authors add a test accuracy plots given a fixed prune ratio, whose x-axis is controlled instabilities (e.g., from 10% to 90%) among various pruning algorithms (other than magnitude-based ones, e.g., Hessian based methods). It would be much more interesting if the same instability results same test accuracy even for different pruning algorithms.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The paper is an empirical study that looks into the effect of neural network pruning on both the model accuracy as well as the generalization risk (defined as the difference between the training error and the test error). It concludes that while some pruning methods work, others fail. The authors argue that such discrepancy can be explained if we look into the impact of pruning on \"stability\". \n\nThe first major issue I have with the paper is in their definition of stability. I don't believe that this definition adds any value. Basically, the authors define stability by the difference between the test accuracy pre-pruning and post-pruning. This makes the results nearly tautological (not very much different from claiming that the test accuracy changes if the test accuracy is going to change!). One part where this issue is particularly important is when the authors conclude that \"instability\" leads to an improved performance. However, if we combine both the definition of test accuracy and the definition of \"instability\" used the paper, what the authors basically say is that pruning improves performance. To see this, note that a large instability is equivalent to the statement that the test accuracy changes in any direction (there is an absolute sign). So, the authors are saying that the test accuracy after pruning improves if it changes, which is another way of saying that pruning helps. \n\nThe second major issue is that some of the stated contributions in the paper are not discussed in the main body of the paper, but rather in the appendix. For example, the authors mention that one of their contributions is a new pruning method but that method is not described in the paper at all, only in the appendix. If it is a contribution, the authors should include it in the main body of the paper. \n\nThird, there are major statements in the paper that are not well-founded. Take, for example, the experiment in Section 4.4, where they apply zeroing noise multiple times. The authors claim that since the weights are only forced to zero every few epochs, the network should have the same capacity as the full network (i.e. VC capacity). I disagree with this. The capacity should reduce since those weights are not allowed to be optimized and they keep getting reset to zero every few epochs. They are effectively as if they were removed permanently. \n\n "
        }
    ]
}