{
    "Decision": {
        "decision": "Reject",
        "comment": "This work a \"Seatbelt-VAE\" algorithm to improve the robustness of VAE against adversarial attacks. The proposed method is promising but the paper appears to be hastily written and leave many places to improve and clarify. This paper can be turned into an excellent paper with another round of throughout modification. \n\n\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper examines adversarial attacks to a VAE. It is known that by small norm perturbations on the conditioning input x of a VAE can dramatically change the generated output. This paper\nempirically illustrates that alternative objectives can improve robustness, in the sense of previously proposed adversarial attacks such as (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This paper is concerned with the stability of reconstructions and their quality, and proposes Seatbelt-VAE to remedy some of the shortcomings in the original VAE objective. \t\t\n\nThe key idea of the Seatbelt-VAE is introducing a conditionally Gaussian chains (of length L) in the encoder and decoder distributions of a VAE. This is a plausible and sensible idea. Then the authors evaluate the robustness of reconstructions under various output attacks.\n\nThe methodological part of the paper is quite well written and easy to follow, despite the fact that it is somewhat overloaded with too many abbreviations. The experimental section is harder to read as the motivations and its organization is not clearly stated. Overall, this section feels as if it is too hastily written, many results put into appendix without much discussion. The organization can be much more improved.\n\nThe disentanglement achieved by this novel representation is characterized only anecdotally and by contrasting the resulting objectives to a beta-VAE. It would have been much more informative to illustrate and discuss further the representations learned by such a conditionally Gaussian architecture. Figure 6 and 7 partially try to achieve this by showing the interplay of depth L and the inverse-dispersion parameter beta but I found it hard to interpret this results, for which an entire page is devoted.\n\nIn the experiments, the ELBO is reported for various methods. I would argue that the ELBO is not a very representative proxy for robustness. For example VAE ELBO and beta-VAE ELBO are both lower bounds of the true marginal likelihood and it is possible that beta-VAE is much lower while attaining a higher robustness in the sense of being resilient to suitably defined attacks.\n\nThe authors claim that there are no clear classification tasks for the datasets -- but this is not accurate as both celeb-a has clear classification tasks in the form of predicting attributes. It would have been really quite informative if adversarial accuracy on downstream tasks would have been reported. Relying on qualitative results in Figure 1 is only providing partial evidence about the approach.\n\nRobustness to independent noise, as the authors have, is a good experiment to have -- however typical adversarial examples may be quite structured and such a randomized strategy may not give an accurate indication about the nature of the representation.\n\nOverall, the paper is quite promising but I feel that one more iteration maybe needed."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors of this paper propose a new VAE model called seatbelt-VAE and investigate its robustness to output and latent adversarial attacks. Inspired by beta-TCVAE, DLGM, and BIVA, seatbelt-VAE allows multiple latent layers, enforce disentanglement via weighted total correlation on the top latent layer, and conditions the likelihood on all latent layers. Robustness to adversarial attacks is the focus in experiments. Visual and quantitative comparisons show that seatbelt-VAE is more robust for latent attack than benchmarks. Specifically I have the following three concerns:\n\n1. Defining ELBO using samples from the entire dataset may bring in some benefits, but it complicates the calculation of ELBO and related distributions when minibatch or single sample are used in learning and inference. Please explicitly discuss this issue.\n\n2. I would like to see how seatbelt-VAE performs in sampling and generating new samples, instead of just reconstruction. \n\n3. Similarly, it would be beneficial to investigate disentanglement, that is the interpretation of the top latent factors in seatbelt-VAE. \n\nMinors: \nFactor analysis -> factor analysis\nsection X -> Section X\nfigure X -> Figure X\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "I am a bit confused about the model. While i understand the generative model in Fig 2a, i am a bit puzzled about the choice of proposal distribution eq(7)/Fig 2b for the Seatbelt-VAE. The key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12). However, Figure 2b and eq(7) claim that all deeper layers only depend on the previous layer in the approximate posterior. Since in (12) we rely on the posterior for the attack, a successful attack on layer m < L should immediately generate the correct values from q_phi(z_{i+1}|z_i) i=m,...L-1.  So from that point of view it is not a seatbelt, as since in Fig 2b if the attacker manages to control z^1, he has immediate control of z_2 and therefore the now  attacker controlled z_2 will directly feed into the generated target. What might be is that the optimization problem (12) becomes harder to solve because of the increased model-complexity. \n\nI am not too impressed by the attacks presented in Fig1 as well as the appendix. One of the key points of the old adversarial attacks was that the attack-image was indistinguishable from the true image by a human.  However, the adversarial inputs, even for VAEs are clearly not part of the distribution and the errors reported for eq (12) are very large to the point where attacking via the target image would probably be harder to spot. If we for example look at page 24, second row: there is no way, that the adversarial image has a likelihood similar to the target. This looks more like the algorithm did not manage to find a suitable direction.\n\nI am therefore not sure whether the evaluation is correct: if we did not manage to find an attack image, does it proof there is none? And is it meaningful to report their error values if we did not manage an attack?\nBtw: did the optimization of (12) begin with d=x_t-x or d=0? maybe starting with d=x_t-x would be more meaningful because it would make it easiest for an attacker to ensure the correct reconstruction.\n\nGiven the quality of the attack images, the error of eq(12) should be reported when choosing d=x_t-x as a baseline in Fig 5. It would also be good to see the actual VAE values.\n\nUnfortunately, the reconstructions on dsprites are bad. But an important experiment could be to check whether you can attack the orientation of an object. Orientation is difficult to regularize via TC, since the parameterisation is inherently circular. Thus TC might make it difficult to encode orientation in higher layers and it should be easier to attack.\n"
        }
    ]
}