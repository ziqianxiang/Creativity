{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes techniques for improving the compression rate of Knowledge Distillation on BERT models. There are two main ideas. The first is the reduction of the parameter by reducing the vocabulary size of WordPiece model. The observation that a large fraction of parameters in BERT are dedicated to WordPiece embeddings is interesting, as this hasn't received much attention in other works on BERT compression. The second idea is to add a loss function that measures the difference of parameters between teacher and student in the learned projection space. These two ideas are quite complementary in that one can apply one idea to vanilla knowledge distillation without having to apply another simultaneously.\n\nMy major concern on this paper is that experiments are not designed to measure the impact of each design choice. First, it is unclear whether DualTrain-based methods outperform PKD with 3 layers because having more layers is more advantageous than having more hidden dimensions. While PKD doesn't straightforwardly allow different hidden dimensions between teacher and student, it is a reasonably straightforward idea to train a shared projection matrix to enable it; concurrent work https://openreview.net/forum?id=rJx0Q6EFPB actually does that. If this was considered as too significant extension of PKD, then authors should've experimented with vanilla knowledge distillation with fewer hidden dimensions. Second, DualTrain-based models have both smaller vocabulary size and smaller hidden dimension size, and it is unclear which one played the greater role in successful distillation.\n\nI would suggest authors to carefully set up baselines that we shall precisely measure the impact of each design choice.  Vanilla knowledge distillation with the same vocabulary as teacher and smaller hidden units shall allow us to understand the impact of decreasing hidden units. By comparing this with DualTrain models with the same vocabulary size, we shall now understand the benefit of decreasing the WordPiece vocabulary size.\n\nAuthors make a very insightful note in the discussion that smaller WordPiece vocabulary would imply longer sequence lengths, which may degrade the performance of BERT models. This is an interesting observation, and authors should not avoid running experiments on tasks with longer sequences because it is likely to have negative results. By actually running experiments and breaking down metrics with different sequence length buckets, the research community shall better understand the impact of vocabulary size and the paper will be made stronger.\n\nMinor comments:\n\n1) Table 3 is somewhat difficult to understand because each approach is making a different tradeoff between accuracy and model size. Plotting the accuracy metric vs. model size curve may more effectively visualize the tradeoff.\n\n2) I don't think the variable C in equation 3 was defined. Since there are two vocabularies, it might be useful to clarify the variable and clarify whether there is a loss function for words in teacher vocabulary.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\nThis paper aims to compress the size of large pretrained BERT models for faster inference and cheaper storage. Their method is based on knowledge distillation into a smaller student BERT architecture that has a smaller vocabulary, as well as smaller embedding and hidden dimensions. Their method has 2 steps: 1) dual training which randomly chooses tokens from the sequence to segment using the student vocabulary, and the rest of the tokens are segmented using the teacher vocabulary, as well as 2) shared linear projection layers to align the trainable parameters of student and teacher models by minimizing l2 loss. They present experimental results showing that they can compress BERT base by up to 60x with only a small decrease in accuracy.\n\nStrengths:\n- This paper comes at an important time where larger models have been shown to achieve strong performance while requiring too many resources to train and evaluate. This paper makes a step towards smaller models that retain performance while enabling faster inference and cheaper storage.\n- The paper is generally well-written and easy to understand. Figure 1 was really helpful.\n- The authors evaluate on several downstream tasks and show that compression can be achieved while retaining performance, outperforming the knowledge distillation method proposed in [8].\n\nWeaknesses:\n- The novelty is somewhat limited as compared to [8], essentially the difference is to also vary the vocab used between student and teacher models, and also to use up and down projections for knowledge distillation in case the intermediate layer dimensions are different (in which simple matching does not work).\n- Why is a random subset of words segmented using the student vocabulary? Since the student model has a much smaller vocab size, it seems to make sense that common words are 'selected' by the student model and segmented, while rare words that are not important in masked language modeling or downstream tasks are not used in the student model, similar to [1,2] where common words are used in the vocab for language modeling and remaining words ignored. Could you expand more on the design decisions here?\n- One concern I have is the lack of comparisons with other approaches that do not use knowledge distillation but are still relevant in terms of reducing the vocab size/model parameter size. For example, [1,2] consider only modeling the common words (wordpieces/subwords in this case) for language modeling. A similar approach could be used to train a student BERT model with distillation to a smaller vocabulary. Likewise, you cite many related papers on quantization but do not compare to the performance of quantizing pre-trained BERT models and only storing the quantized weights for downstream tasks. There are also papers that studying reducing the vocab size/embedding matrix using low-rank approximations [3,4], codebook learning [5,6], and dropout [7]. I believe these are all fair comparisons. The only comparison is to [8].\n- There are some ablation studies to show that both dual training and shared projection layers are important. However, from Table 3, there are certain datasets and compression factors where only using DualTrain gives best performance.\n- From the experimental results, it is inconclusive as to whether shared up or down projection is preferred. Equation (4) simply says L_p, and not whether up or down projection was specifically better, so I'm assuming the authors tried both.  From Table 3 the results are quite inconclusive. It seems unreasonable to train 2 models and take the better one, so I was wondering whether the authors had any intuitions on which one in different settings.\n\n[1] Luong et al. Effective approaches to attention-based neural machine translation, EMNLP 2015\n[2] Chen et al. Compressing neural language models by sparse word representations, ACL 2016\n[3] Nam and Quoc. Integrating low-rank approximation and word embedding for feature transformation in the high-dimensional text classification, 2017\n[4] Grachev et al., Compression of recurrent neural networks for efficient language modeling, 2019\n[5] Shu and Nakayama. Compressing word embeddings via deep compositional code learning, ICLR 2018\n[6] Chen et al., Learning k-way d-dimensional discrete codes for compact embedding representations, ICML 2018\n[7] Chen et al., How large a vocabulary does text classification need? A variational approach to vocabulary selection, NAACL 2019\n[8] Sun et al. Patient knowledge distillation for bert model compression, arXiv 2019"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "Summary: This work looks at knowledge distillation: using a large teacher model to train a smaller student model that could have, in this case, as little as 62x fewer parameters. They look at BERT models and propose two ideas to improve effectiveness of knowledge distillation: dual training (encouraging alignment between the teacher and student wordpiece embeddings) and shared variable projection (aids in direct layer wise transfer to the student model).\n\nPositives: Model compression is important, especially for cases like BERT where models are massive. There are lots of works that work on similar things showing that compression is possible for these sorts of models, so studying best practices is an important area of research. The overall experimentation, idea development, and execution shows promise in both techniques.\n\nConcerns: I think the work is a great start of a line of research but don't think the experimentation has enough threads to weave together to tell a super convincing story. I'd like to see an investigation of different ways of compression rather than just at the hidden dimension level. What happens if you vary the number of layers? What happens if you compress non-uniformly for each layer, keeping later layers less compressed.\n\nOverall I think this paper requires more experimentation and analysis to be complete."
        }
    ]
}