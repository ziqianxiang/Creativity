{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a novel (self)-attention mechanism dubbed \"doubly normalized attention\". When taking the attention between keys and query, standard attention proceeds by computing o_i = 1/Z \\sum_j exp(q_i * k_j) * v_j. This corresponds to normalizing the logits exp(q_i * k_j) over the keys (j). The authors point out that this can lead to an \"explaining away\" phenomenon: basically, the score for a particular j, exp(q_i * k_j), could be 0 for all i, which means that information coming from position \"j\" is basically discarded, and the authors argue this is bad. Moreover, they argue that standard attention suffers from \"mode collapse\" (i didn't quite get / found clear what this effect corresponds to in standard architectures). To palliate the two problems, they propose to re-normalize the attention weights along the columns. Basically, first computing \"responsibilities\" (in a EM flavor) p_j = 1/Z \\sum_i exp(q_i * k_j) , and then computing the attention by renormalizing p_j, o_i = (1 / \\sum p) \\sum_j pi_j * v_j. The author show that this corresponds to one step of EM (or basically kmeans if q_i are cluster centers and k_i = v_i are the data points). The authors test their model on headline generation, a visual qa setting and some natural language understanding task, reporting modest gains (0.2-0.4 on SQUAD/MNLI) w.r.t. standard attention.\n\nThis work has some quite interesting aspects due to the fact that it tries to interpret and generalize attention computations under a more general framework. I appreciated the theoretical work done by the authors, even if sometimes it appeared to me more complex than needed to be. However, this paper also has some weaknesses which prevent me for putting it above the acceptance bar, namely (i) lack novelty of proposed method and (ii) somewhat weak motivation. I'd be happy to increase my score if the author shed light on the problems below.\n\n1) About the lack of novelty:\n1.1) The doubly normalized attention appears to me identical to EM routing proposed in the capsule network (ICLR '18, https://openreview.net/pdf?id=HJWLfGWRb). I think the author mention this in the appendix. If it is, what's the main contribution of the first part of the paper ? I agree that the analogue to Gaussian Mixture Models appears novel but solving GMMs are just a special applications of EM-type algorithms.\n\n2) About weak motivation:\n2.1) The authors mention the “explaining away effect” as a problem to solve. It is however unclear as to whether/how the performance of the architecture tested suffered in virtue of this phenomena. Can you justify or quantify this ? Can you attempt at actually computing how much information is discarded by looking at the attention distribution ? You could maybe link this to works analyzing \"pruning heads\" https://arxiv.org/abs/1905.09418 ? This could give more depth to the paper.\n2.2) I didn't quite get the section on \"mode collapse\" and why adding residual layers would help in solving mode collapse in standard transformers architectures (as written in a note in the paper). Could you clarify this point ?\n2.3) The analogue to optimal transport felt less well-motivated and it was unclear what new understanding was gained from the reframing of self-attention as constrained optimization. I don't know what to get out of it. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a doubly-normalized attention scheme, and draws connections between the proposed method and GMM and optimal transport. The authors demonstrate two benefits using DNAS, i.e. avoiding explaining away and mode collapse. Some experiments are conducted to show the performance. The paper is written well, and easy to follow. Below are my main concerns:\n\n1. Mode collapse: I do not quite understand how the figures in Appendix are drawn. Also from Fig. 2 I do not see why DNAS is *always* the upper bound of LNAS. Some theoretical analysis should be derived here. \n\n2. Sparse v.s. Dense as in LNAS v.s. DNAS: From the optimization perspective in Eq. 9 and 10, and Lemma 1, the weights in LNAS should be much sparser than those in DNAS. This leads to the benefit of computational complexity/time. However, the authors have never discussed it in the paper, neither theoretically nor empirically.\n\n3. Significance of results: Unfortunately I do not see the significance of the experimental results here. The improvements are often less than 0.5%, compared with LNAS (then why is the proposed method important???). Many experimental details are missing, e.g. how many trials are reported here? Are the numbers the best ones or averaged? What are the stds? How many epochs for LNAS and DNAS? Without knowing such details, I can hardly justify the significance.  \n\n4. HNAS experiments: Showing Fig. 3 and 4 without any comparison with other approaches like LNAS does not make sense. How do we know whether hybrid is good or not? Table 1 shows the results from DNAS without HNAS, Table 2 and 3 use HNAS but no DNAS. To me this way of showing results is weird. Why not showing all the results of DNAS and HNAS for all the tasks? "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes \"doubly normalized attention\". Practically, it is a very simple modification to existing attention mechanisms (which is a good thing!), and involves normalizing over both rows and columns. The authors motivate this modification based on interpreting the output of an attention layer as being the mode of a mixture of Gaussians, where the mean parameters are given by the key vector. This motivates the doubly-normalized approach given that for encoding, information should flow from the other direction (section 3).\n\nThe approach is tested on three tasks: VQA, summarization, and BERT-pretraining, where it is found to slightly improve upon existing \"singlely\" normalized attention layers.\n\nOverall I found the exposition to be quite clear. And I appreciated the authors' experiments on a diverse range of tasks/benchmarks. Having said that, I must admit that I do not find the motivation very compelling. If the argument is that doubly-normalized attention is more suitable for encoding than decoding, it should be possible to test this. Namely, in a seq2seq task, the best performing model should be one that uses DNAS in the encoder and LNAS in the decoder. Is this borne out in practice? In my opinion the paper could be better motivated/presented as simply a modification to the existing attention mechanism (nothing wrong with that!).\n\nI had some other minor comments/questions:\n\n- Do you achieve similar performance gains on translation? While summarization experiments are appreciated, the real test-bed for whether an architectural modification to the Transformer results in empirical gains is translation. (However I very much appreciated the experiments on pretraining BERT in Table 3, so thank you for this).\n\n- Did you try analyzing the attention layers, and seeing how DNAS qualitatively differs from LNAS?\n\n- What if you tune over \"u\" as a fixed hyperparameter? What if it is predicted by an auxiliary network as a function of the input x?\n\n\n\n "
        }
    ]
}