{
    "Decision": {
        "decision": "Reject",
        "comment": "This submission proposes a statistically consistent saliency estimation method for visual model explainability.\n\nStrengths:\n-The method is novel, interesting, and passes some recently proposed sanity checks for these methods.\n\nWeaknesses:\n-The evaluation was flawed in several aspects.\n-The readability needed improvement.\n\nAfter the author feedback period remaining issues were:\n-A discussion of two points is missing: (i) why are these models so sensitive to the resolution of the saliency map? How does the performance of LEG change with the resolution (e.g. does it degrade for higher resolution?)? (ii) Figure 6 suggests that SHAP performs best at identifying \"pixels that are crucial for the predictions\". However, the authors use Figure 7 to argue that LEG is better at identifying salient \"pixels that are more likely to be relevant for the prediction\". These two observations are contradictory and should be resolved.\n-The evaluation is still missing some key details for interpreting the results. For example, how representative are the 3 images chosen in Figure 7? Also, in section 5.1 the authors don't describe how many images are included in their sanity check analysis or how those images were chosen.\n-The new discussion section is not actually a discussion section but a conclusion/summary section.\n\nBecause of these issues, AC believes that the work is theoretically interesting but has not been sufficiently validated experimentally and does not give the reader sufficient insight into how it works and how it compares to other methods. Note also that the submission is also now more than 9 pages long, which requires that it be held to a higher standard of acceptance.\n\nReviewers largely agreed with the stated shortcomings but were divided on their significance.\nAC shares the recommendation to reject.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #5",
            "review": "Summary\nThis paper proposes an attribution method, linearly estimated gradient (LEG) for deep networks in the image setting.\nThe paper also introduces a variant of the estimator called LEG-TV, which includes a TV penalty, and provides a \ntheorem on the convergence rate of the estimator. The paper finds that the LEG attributions pass sanity\nchecks. \n\nMy recommendation\nOverall, I am recommending this paper as a weak accept. There are several points to address with\nregards to the exposition and flow of the paper, which is my biggest issue with this paper. I believe\nthe authors can address this point and I am willing to raise my point on this basis. The paper also\nprovide some theoretical analysis of the proposed method, which is typically lacking for most\nof the interpretation methods in this domain. \n\n\nPossible Improvements\n- The LEG method is not sufficiently motivated. Here, I am specifically referring to the functional\nform of the estimated itself in definition 1. See the question section  for some of the issues \nI raised there. \n- From figure 4, we see that the method passes the proposed sanity checks which seem like a \nkey motivation for this work, however, the authors don't give an explanation for why this is the\ncase.\n- The paper notes that LEG can be estimated using an LP; it would have been great for the authors\nto completely spell this out in the appendix or somewhere in the text. What is the exact form of the\nLP? What are the constraints? \n- As the authors know, the two evaluations presented in the paper: sanity checks, and the zeroing\nout procedure (in figure 5) don't actually tell us which method is a good method, just rule out a method.\nI would encourage the authors to design a toy task where the ground truth attributions are know, then\ntrain a model to be 100 percent or so accurate on this task. You can then obtain LEG-TV estimates\nfrom this model and compare to the ground-truth. \n- I found the proof of lemma 1 confusing, the authors say it follows trivially, but I don't see it. For example, \nthere should be a factor of 2 somewhere after taking the derivative wrt to $vec(g)$, but I don't see it. It is\nfine for the authors to spell out the derivation here if possible. \n-The paper ends quite abruptly with no conclusion or discussion. It would be great to include a wrap up\nsection that puts the contributions into context. \n- I get the sense that this method should be computationally intensive, though the paper says otherwise.\nIt is fine for a method to be computationally intensive, but can the authors speak to this issue?\n\n\nSome Questions\nDefinition 1: I had a difficult time understanding this definition. What is $g$ here? I assume\nit is the gradient based on the reference to the first order Taylor expansion. In addition, why\nis the estimand squared? Further, What does it mean to take expectation wrt $F + x_0$. I was\nparticularly confused by the last point, because F is a continuous distribution, while presumably\n$x_0$ is the point of interest. The paper notes in several places that it can sample from $F + x_0$,\nis this equivalent to sampling from $F$ and adding point $x_0$?\n\nWhat is LEG0 in figure 5?\n\nIs $\\kappa$ in your theorem 1, the condition number of the covariance matrix of the perturbation?\n\n\nConclusion\nOverall, the paper provides a nice method along with analysis on convergence rates and other statistical\nproperties. Several of the key issues/questions I have about the paper are raised above. None of these\nshould be dealbreakers but would require the authors to flesh out more details and possibly justify\ncertain choices. In general, I think more effort should be put into the flow and writing of the paper.\nOverall, this is an interesting contribution.\n\n\n## After reading author responses\nI believe the authors have clarified and improved the readability of the paper and clarified several of the questions\nthat I had. I am raising my score to an accept. While I believe this is a valuable contribution to the sea of attribution methods that have now been published, like the authors noted, it is still not clearly if attribution methods as a whole\nare useful of decision making or understanding of a model by either a generic end user or the model developer. \nThis is a huge problem in this area that deserves significant attention. This said, the goal of this current paper is to\ntake a step towards developing a principled method, so this is a step in that direction perhaps. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "The authors propose a nice framework for interpreting differentiable models without access to model details. The framework also comes with two empirical estimates, with solid theoretical back-up. \n\nDefinition 1 in Section 2 provides a definition for the method to study (LEG). LEG is a nice generalization of multiple existing approaches. There also seems to be a lot more to investigate for future work based on this framework. \n\nAfter the authors proposed Definition 2 (seems to be a bit too straightforward, to be discussed later), Theorem 1 is proposed to characterize its convergence. It also provides guidelines for selecting the covariance matrix $\\Sigma$, as discussed in Section 4.\n\nOverall, the paper proposes a nice framework for model interpretation. It is well backed up by theory. It should clearly be accepted, although the paper is relatively weak in experiments. Below I will discuss some weaknesses and potential improvements. \n\nWeakness: \n1. Theorem 1 provides a nice characterization for the property of the proposed LEG-TV estimate. But there are two weaknesses. \n\nFirst, the authors should note that the proposed definition 2 is still a bit too straightforward without intuitive explanations. For example, what is the general form of 2D Fused Lasso? How is it applied to approximate Definition 1 / Equation (2) to get the expression in Definition 2? More details may be explained to help readers understand.\n\nSecond, it should be carefully discussed that the proof of Theorem 1 depends on existing work if the connection is close. For example, the authors may add in the main content \"the proof of Theorem 1 is built on top of ...\" or statements like that, probably with a short sketch / intuition on the entire proof if space permits.\n\n2. Can the authors be more specific on the time complexity and sample complexity of the proposed algorithms?\n\n3. It seems the experimental section of the paper is not satisfactory. Almost all results are single-image analysis, instead of systematic empirical analysis on an image data set. Without such analysis, it is hard to see the advantage of the proposed method over other comparing saliency maps and model-agnostic methods. For example, is the proposed method more sample-efficient than LIME or SHAP, or other more efficient procedures such as L(C)-Shapley? Is the proposed method really selecting meaningful segments for the model? (It may be tested by evaluating the log-odds-ratio after the top selected features are masked.) It is observed that LEG is able to select connected regions (Figure 6). The same phenomenon has been observed for C-Shapley. It may be helpful if the connection is discussed (such as the connection between sampling procedure of C-Shapley and the procedure imposed by LEG-TV).\n\nThe reviewer is not conditioning the \"accept\" decision on adding any of the suggested improvements on experiments given the limited rebuttal period and limited space, although it may benefit the paper of some of them can be addressed.\n-----------------------------------------------------------------------------------------------------\n--------------------------------------Post Author Response--------------------------------\n-----------------------------------------------------------------------------------------------------\n\nThe authors have addressed most of my concerns. Theorem 1 and the sketch of the proof have been discussed. Also, complexity has been discussed (the definition of $s$ is embedded in the theorem and may be made clearer). Last but not least, authors have carried out experiments in a larger scale to get more stable results. \n      \n      The performance, in terms of log-odds-ratio, may not be as good as some of the comparing methods. However, the paper provides a creative framework for incorporating structure into feature attribution scores in model interpretation. So I will keep my score.\n      \n      A typo: The first paragraph of Section 3: \" less model evaluations.\" should be \" fewer model evaluations.\"\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work proposes a statistical framework for saliency estimation for black box\ncomputer vision models, based on solving a convex program in (4). It also gives theoretical analysis on its consistency in Theorem 1, and run a few simulations to show the empirical performance of the proposed method.\n\nThe method proposed seems to be novel and reasonable. As a result, I tend to accept this paper. However, I would like to remark that solving (4) might be empirically difficult (depending on the size of the problem) even though it is convex and can be solved in polynomial time theoretically. I wonder if the authors could clarify the setup of their experiments (instead of writing \"The problem in equation 4 can be\nsolved by any linear programming software, for which many open-source implementations exist\"), and if the author could remark on the empirical running time.\n\nAlso, I am not sure if \"Note that if L = 0, then the TV-penalization has no effect and the solution of the above procedure reduces to the empirical estimate,\" as the objective function is in L1 norm."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Author propose a statistical framework and a theoretically consistent procedure for saliency estimation that is close to the empirical solution and has sparse differences on the grid. I think the idea of the paper is quite interesting and the results are significant. \n\nIn particular, having an upper bound on the number of model evaluations to recover the region of\nimportance with high probability is significant. Moreover, they have proposed a new perturbation scheme for estimation of gradients that works better than random perturbation schemes.\n\nI have some questions listed below:\n- Can the proposed approach be integrated with different types of Saliency map methods?\n\n- Can you explain the differences of the proposed approach and the group feature formulation of https://arxiv.org/abs/1902.00407 ?\n\n- Is there a quantitative way to assess the performance of the proposed approach?"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "Authors proposes an interesting statistical method to detect saliency in images. Authors provides a specific estimator that is fast to compute and characterize its performance w.r.t. parameters.\n\nMy main concern is the experiment section. \"For computational efficiency, we compute saliency maps on a 28 by 28 grid (i.e. γ˜ ∈ R 28×28) although the standard input for VGG-19 is 224 by 224. T\". Shouldn't we do the same thing for all baselines? The seemingly good sailency results might stem from this artifact.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}