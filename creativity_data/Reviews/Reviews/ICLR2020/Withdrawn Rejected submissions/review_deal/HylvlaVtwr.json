{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers the problem of learning to synthesis human motion through leveraging mocap data and RL. To improve upon previous approaches, learning policies for an interactive task of ‘sitting onto a chair’ in considered. A hierarchical reinforcement learning (HRL) method is proposed. The proposed HRL, learns a collection of simpler tasks through imitation and deploys a meta controller to perform short subtasks in order to complete the longer interactive task. Experimental results are presented on single synthetic domain. Also, results on image conditioned motion prediction are demonstrated.\n\n+ the interactive tasks exampled in this paper are interesting and if these set of tasks expanded, learning suitable policies can be impactful. \n\n+ The paper is well written and mostly clear to understand. \n\n- From the technical side, this paper does not offer any technical novelty. The approach is heavily built upon prior works of Schulman et al., 2017, Peng et al. 2018a.\n\n- Only one task (i.e. sitting onto a chair) is considered as the core problem in this paper. \n\n- The experimental results are very weak. It is not clear if the proposed approach is offering any benefit over prior HRL approaches. \n\n- Comparison with prior works is not conducted.\n\n- There has been only a limited of demonstrations on how the approach could be applied on real images and it is not clear what would be the implication of this work on real problems or in real world setup o even more generic real images. \n\nBased on the above explanations, this paper does not have technical novelty and the experimental results are not convincing. Therefore, it does not meet the bar for publication. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper tackles the interesting problem of motion synthesis for an interactive task, namely, sitting down on a chair from various starting positions. Solving such tasks is quite desirable specifically for the computer graphics community. However, not enough progress has been made in this direction. The authors present a method that tries to solve this problem using a hierarchical approach and they compare their method with several other methods. Finally, the authors use their method to synthesize the motion of a virtual character interacting with a real scene.\n\nI believe that in the current state, this work is below the acceptance threshold. The authors succeed in proposing a method that solves the original problem with a non-trivial success rate (just above thirty percent on the Easy setting). However, the motion quality is well below satisfactory, specifically for a method that makes use of motion capture data. Moreover, the proposed method is lacking in novelty and the baselines seem inappropriate. I believe the authors should compare their methods with more appropriate methods such as [1, 2, 3] which are cited below. Finally, the synthesis of a virtual character interacting with a real scene is an interesting use-case of this work. However, I believe that it has little to do with answering the question: what is the best method to solve the original task?\n\nSome questions or points of discussion:\n - (major) Would the authors please describe the kinematics-based method in more detail? I did not understand this method at all.\n - (major) As a follow-up, it seems like both the non-hierarchical methods are ignorant of the position of the chair. Maybe I am missing something, but what exactly is the point of comparing with a method that has no chance of success?\n - What is the definition of success for the task of turning left or right?\n - (minor) Do the controllers directly control the torques is any method such as a PD-controller used?\n - (minor) The activations used in the networks are not specified.\n - (minor) Equation 7 requires a minor edit\n - (very minor) The reward formulations used in the paper, i.e. the exponential of negative of a distance, are usually used for their interpretability and as such the weights are usually set so that they add up to 1. It just seems odd to me that this is not the case here and was wondering if there was any reason to do so.\n\n\nMissing references:\n [1] Xue Bin Peng and Michael Chang and Grace Zhang and Pieter Abbeel and Sergey Levine. MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies.\n [2] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, Nicolas Heess. Neural probabilistic motor primitives for humanoid control.\n [3] Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Greg Wayne. Hierarchical visuomotor control of humanoids."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a hierarchical approach to learn how to sit. \n\npro: the learning to sit task is novel.\n\nHere are some questions:\n\n(1) \"However, their metatask only requires a single subtask (i.e. walk), and the meta controller focuses solely on steering. We address a more complex task (i.e. sitting onto a chair) which requires the execution of diverse subtasks (e.g. walk, turn, and sit).\"\nThis is not accurate. Peng et al also handles turning, although not in the context of subtask, see e.g, this video clip:https://youtu.be/hd1yvLWm6oA?t=289.\nAlso see Robust Recovery Controller for a QuadrupedalRobot using Deep Reinforcement Learning:https://arxiv.org/pdf/1901.07517.pdf?fbclid=IwAR21IneQ5Lusw62bCyh0oDzRJYh2nKercXP53vp35dIGtT-edIcITZBeetc and NEURAL PROBABILISTIC MOTOR PRIMITIVES FOR HUMANOID CONTROL:https://arxiv.org/pdf/1811.11711.pdf, https://arxiv.org/pdf/1811.11711.pdf. These papers also present hierarchical learning with several subtasks.\n\n(2)The authors use curriculum learning first to learn easy tasks, then to learn more difficult tasks. How do you prevent forgetting the easy task while learning the more difficult ones?\n\n(3) kinematic non-hierarchical baseline: since the kinematic motion couldn't complete the task, I don't see any reason why using it as a reference motion for RL will constitute a reasonable baseline. At the very least, a reward for distance to the chair should be available.\n\n(4) I am confused about the differences between the two hierarchical baselines. Looks like they are both just trying to do the subtasks in one particular order. And it is not surprising that one side turning has a lower success rate. I will expect that it only has half the success rate as the full model, while in fact, the success rate is quite comparable to the full model (left turning only). Any particular insight from the authors about this?\n\n(5) Why the different success rates between the left turn and right turn? If the humanoid model is symmetric, I will imagine left turn and right turn tasks are very similar. And I am confused about the success rate. If the initial pose is the same and the simulation is deterministic, why will different runs produce different results? Is the policy during test time stochastic?\n\n(6)The video shows a lot of repeated (or very similar) motions. It will be interesting to show some of the failure cases instead.\n\nWhile this paper tries to solve a novel task. Based on the success rate of the result (32 % for easy setting and 10 % for the hard setting) and various problems mentioned above, I think there is still a lot of improvement to be made."
        }
    ]
}