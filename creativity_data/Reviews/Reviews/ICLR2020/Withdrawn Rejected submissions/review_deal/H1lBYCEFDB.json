{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors analyze the natural gradient algorithm for training a neural net from a theoretical perspective and prove connections to the K-FAC algorithm. The paper is poorly written and contains no experimental evaluation or well established implications wrt practical significance of the results.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Natural gradient (NG) has been proven efficient in statistical learning, and one of its attractive properties is being invariant under smooth transformations of the parameter space. Computing NG is often difficult as one has to derive the Fisher matrix and its inverse. K-FAC offers an approximate method for approximating the NG with the risk of losing the invariant property.\n\nThis present paper offers a different approach: rather than approximating the exact NG directly as K-FAC does, it views the approximate K-FAC natural gradient as the exact \"natural gradient\" under the K-FAC metric. This is an interesting idea and the paper goes on to show that the new \"exact\" NG under the K-FAC metric (rather than the Fisher-Rao metric) is invariant under certain affine transformations.\n\nI find the paper extremely hard to follow. My main concern is that many concepts and math objects are at risk of being not mathematically rigorous or well defined. For example, equation (2.5) is not correct, this is not a genuine update based on exponential map. Similarly, the update equation in the first graph of page 4 doesn't make sense as \\mathcal{W} is an abstract manifold and the subtract here is not defined. The way paper is written has a high risk of causing confusion between coordinate-dependent and coordinate-free objects. On page 3, line -5, I have trouble understanding the abstract distribution P_{v|\\psi}(\\omega). On page 7, I don't understand what \"evaluating $w$ at $a$\" means, and even $a$ here isn't defined yet I think. In equation (2.3), $-w_k$ should be $w_k$ (i.e. no minus sign). There are many other typos and confusions that I opt not to  point out here.\n\nIn summary, the main issue of the paper in its current form is that its presentation isn't clear with many typos. I suggest the authors to re-write the paper carefully so that it is more accessible.\n\nI know that this paper focuses on theoretical properties of NG, but it would also be good if the authors offer some discussion on the use of their method in practice.\n\n  \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper analyzes the invariance properties of the K-FAC algorithm by reconstructing the algorithm in a coordinate-free way where the neural network is viewed as a series of affine mappings alternating with nonlinear activation functions. It converts the original metric into an approximate metric, whose coordinate representation matches the K-FAC approximation. So K-FAC can be viewed as the exact natural gradient under the new metric rather than an approximation under the Fisher metric.\n\nWhy is the invariance important? How does the proposed framework help us develop better algorithms? Without empirical studies, it is not easyÂ to see the significance of this work."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper is concerned with tractable (approximate) forms of natural gradient updates for neural networks, in particular with the recent K-FAC approximation, which applies a set of approximation (layer-wise independence, Kronecker structure for affine maps) in order to obtain a Hessian that can be computed and inverted efficiently. K-FAC has been introduced for MLPs, and has previously been generalized to convolutional and certain recurrent NNs.\n\nThe stated goal of this paper is to provide a mathematical re-formulation of K-FAC in terms of Riemannian metrics. While K-FAC has been developed as approximation to the exact natural gradient update, they come up with a different Riemannian metric, definition of space, etc., such that in the end, K-FAC is the exact natural gradient for that. The authors here also obtain a more precise answer to invariance properties and, given some heavy maths, what they claim to be more elegant proofs of previously known properties of K-FAC.\n\nThe paper uses very heavy math, well \"over my head\" and likely most ICLR attendees. Along with me, they'll ask the obvious question of what this is good for. As far as I can see, there is nothing really new being proposed here in terms of practical consequences. The authors also do not make much effort to explain why their viewpoint is useful, say to obtain practically relevant insights in future work. So, as far as I am concerned, I do not see why this work should be of much relevance to ICLR, which is not an abstract maths conference.\n\nA final comment is that people have for a very long time tried to use second-order optimization for MLPs. The aspect that always was tricky there, is that SGD is *stochastic*, and the second-order info is hard to estimate from a mini-batch. The sets of approximations of K-FAC are pretty extreme, but they may just be needed to make things work in the end, because they may stabilize that \"stochastic inverse Fisher info matrix\" enough to not make the optimization process fail altogether. Now, all theoretical arguments, like \"invariance to this and that\", always ignore the crucial fact that you are talking about a stochastic estimate over a mini-batch, and your theory is always for E_x[...] \"being the truth\". It is not, it is just over a small mini-batch. I am not saying the additional theoretical insight from this work here (over previous K-FAC work), whatever it may be in the end, is not useful. I am just saying I'd be a lot more confident if the authors would specifically address the stochastic property."
        }
    ]
}