{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The paper studies the problem of filling in the frames of a video between a start and an end frame of the order of one second apart, by proposing a residual 3D CNN based architecture that exploits the natural biases of convolution to enforce temporal coherence in the video. Important features of the proposed approach include the treatment of the video synthesis problem entirely in a latent feature space, and the temporally coarse-to-fine synthesis of intermediate frames. Results on standard datasets for video prediction show improvements against some ablated variants of the proposed approach and promising preliminary comparisons against prior work.  I particularly like the idea of using the natural bias of 3D convolution to do smooth interpolation by synthesizing a full video bookended by the first and last frames. Architectural choices are interesting and well-motivated and combine good ideas from many prior works, including prediction in latent space, and coarse-to-fine synthesis (mainly used before along spatial axes for image generation, but used here temporally for video generation). \n\nThe limited experimental settings are, in my view, the main weakness of this work: \n(i) In its chosen setting of video interpolation over about 16 frames, given that there is limited prior work that directly studies this problem, it falls to this paper to make the case that prior approaches studying interpolation over shorter horizons fail in this setting. However, it fails to do this convincingly: the only comparison to prior work is in the short apparently hastily written paragraph at the very end of the paper (Tab 3). Prior methods are not compared against in an apples-to-apples manner: as best I can tell from the incomplete specifications in this paragraph, the 3 prior methods all generate 7 frames between the start and the goal frame, while the proposed approach generates 14. For these 7 frames, the proposed approach appears to perform worse on BAIR and KTH and better on arguably the hardest dataset, UCF101, compared to prior work. From these experiments, the gain of this new approach over prior work on video interpolation is far from settled.\n\n(ii) The proposed approach is complex, containing many interesting non-obvious choices: coarse to fine synthesis, latent space prediction, the gated accumulation of information in z, the additional stochastic component n^(l) in Eq 4, an adversarial loss rather than L2, and multiple discriminators for the adversarial loss. Of these, the only choices studied in experimental ablations is latent space prediction, and (one aspect of) the gating. This leaves me wondering about the value of all the rest of this complexity.\n\nHere are a few other queries/comments for the authors to consider answering in rebuttal or addressing in revision:\n- It is not clear to me why the proposed method is specific to interpolation/\"inbetweening\": as the authors themselves say, the problem of forward prediction is much better studied and offers many more baselines. This perhaps offers an easier opportunity for comparison against prior work.\n- It is also not clear why prior work on video prediction such as SVG [Denton 2018] could not be extended to work well on this task by additionally conditioning it on the final frame. \n- I could not find any scholarly references to \"inbetweening\" in prior work. Given this, why not use a term from the literature like video interpolation or infilling?\n- Intro claims the proposed method allows deeper nets and more stable training, but these are never evaluated. \n- While the paper claims longer time scales than prior work, the scales are still quite short in absolute terms, since the time scales under consideration are less than a second on 2 of 3 datasets. It would be interesting to see if the approach can handle much longer interpolation, over say, 10-20 seconds, or even more.\n- The related work section is large but unhelpful about offering connections of prior work to this work. For example, Vondrick et al 2016 is described in video generation, but without reference to the usage of 3D CNNs also used in this work. There are also missing references I would have expected: time-agnostic prediction for generating intermediate frames, and structVRNN, deepmind transporter, and contrastive predictive coding for latent space prediction. \n- Typos: \n   - Fig 1 \"max\" block is missing a \"0, \". The [1, 9, 17] is not explained. \n   - SxSxC below Eq 4 should be HxWxC?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Overall the proposed pipeline for video inbetweening, which gradually increases the resolution in temporal dimension using a fully convolutional network instead of rnns, seems very good. \n\nHowever, I think current experiments do not fully evaluate the effectiveness of the proposed method. Specifically,\n\n1. I do not see much discussion on the motivations of various architectural designs in section 3. \n\n2. how do you obtain the initial noise tensor u? And seems u already introduces diversity in the proposed method, it looks redundant to add n^(l) at each level. I do not see much discussion on this in section 3, and the ablation study also doesn't include this.  This brings another concern about the ablation study, which I think could be conducted more thoroughly, since the proposed method includes many components. (e.g. why using max(0, 1- g_e - g_s) rather than a third gate g_z ?)\n\n3.  it is hard to say whether the proposed method is better than baselines according to Table 3 only. At first, I agree FVD is better than SSIM considering the diversity. However, Table 3 compares the proposed method with baselines in terms only SSIM. Secondly, in terms of SSIM the proposed method performs worse than baselines on BAIR and KTH with large margins, but the comparison is conducted between 7 frames of baselines and 14 frames of the proposed method.  Table 3 indeed needs a lot of improvement.\n\n4. Finally, the authors suggest video inbetweening could handle the cases where the temporal distance between consecutive frames is large. I would like to see such samples or related experiments. \n\n5. Minor point, the qualitative samples are too small to view.\n\nConsidering these points, I think the experiments could be improved before publication."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a deep model for the task of video inbetweening, which consists of a 2d Conv image encoder, a stochastic 3d Conv representation generator, and a 3d Conv video decoder. The authors also measured the diversity and quality of the generated in-between video sequences.\n\n1. This proposed model follows the typical encoder—stochastic sequence modeling—decoder framework. The major novelty is on the stochastic sequence modeling module. But I find the architecture of this module not fully motivated. Did the authors engineer the architecture through a lot of trials?\n\n2. The authors might consider the unique characteristics of the video inbetweening task since as far as I see, the proposed model can also be used for the video prediction task given the first two frames.\n\n3. Because of the 3d Convs, the proposed model cannot be directly used for generating missing frames with variable length. How did the author cope with this problem?\n\n4. Since the proposed model has multiple 3d Convs, the authors might show the computational cost, e.g. training/inference speed, memory footprint, and the number of parameters, in experiments. \n\n5. The authors might include more existing work for experimental comparison."
        }
    ]
}