{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a hardware-aware neural architecture search (NAS) approach. The main idea is to customize the search space with hardware profiling information, and then iteratively search for the top half and bottom half network structure using coordinate ascent. Overall, the hardware-related analysis is quite inspiring and the results look promising.\n\nPros:\n\n  - Well-motivated and well-written paper.\n  - Hardware profiling information in Figure 1 and Table 2 are quite interesting.\n  - Results look promising. In particular, the DSP model speedup than FBNet/ProxylessNAS/Oneshot-NAS is quite impressive.\n\nCons:\n\nMy main concern is that it couples many things together (new search space, new coordinate ascent, and new search algorithm), and it is not clear where the performance gain comes from.  More ablation studies would be helpful:\n\n- The new search space includes a lot more options than other NAS works presented in Table 2.  For example, none of  FBNet/ProxylessNAS/SinglePath uses SE. For fair comparison, the authors should consider comparing to other NAS works with SE included (such as MnasNet/MobileNetV3).\n- On top of the new search space, the authors propose a new search algorithms with two optimizations: op-level search space reduction and layer-level coordinate ascent. It is not clear how important is each optimization.\n\nI also have a few minor questions to the authors:\n\n- The latency numbers on Table 2 seem to be much lower than expected: for example, FBNet-S8 reports 19.84ms on iPhone CPU and 23.33ms on Samsung S8 CPU in the original paper, but in Table 2, this paper reports >300ms on CPU and >100ms on DSP. Do you have any idea what could be the potential reasons?\n- It is great to see HURRICANE(DSP) model is almost 10x faster than FBNet/ProxylessNAS/SinglePath models. Could you add more discussion about the reasons? This would be helpful to guide future DSP model design.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a framework for neural architecture search which the authors\ndub Hurricane. They describe their approach and evaluate it empirically.\n\nAs far as I understood the presented approach, its main novelty lies in being\nable to take hardware characteristics into account when designing a network,\nwhich mainly manifests itself in reduced latency and prediction time on a\nparticular platform. The flip side of this approach is that hardware-specific\ninformation is necessary.\n\nThe overall approach is unclear in some places and not sufficiently motivated.\nFor example, what is the significance of the number 4 in algorithm 1? Is this\nsimply because 4 operators were chosen here? Why was this not made generic for n\noperators?\n\nThe paragraph that describes the reduction of the search space for each layer\nseems to imply that layers are configured largely independently, i.e.\ninteractions between layers are not taken into account. While this would\ncertainly lead to a reduction of the search space, it also potentially reduces\nthe performance of the final network.\n\nThe description of the latency models is unclear. In section 4.1, the authors\nmention that 21.84ms corresponds to 4.2% error, but the section after that\nmentions that the CPU latency is set to 310ms. It is unclear how the 4.2% were\ncomputed.\n\nIn summary, I feel that the paper cannot be accepted in its current form."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this manuscript, authors extend one-shot NAS with coordinate ascent. The motivation that different layers have different impacts on latency and accuracy is interesting. It implies that layers should be tuned individually. However, the developed algorithm cannot serve the purpose well. \n1.\tAuthors provides a large set of candidate operators for each layer, which enlarges the search space. However, the set is shrunk by an ad-hoc function before running NAS algorithm, which turns it to a standard NAS problem.\n2.\tThe algorithm applies the concept of coordinate ascent algorithm while they only divide the whole network into two parts. The simple splitting is not sufficient to illustrate the effectiveness of a coordinate ascent algorithm. Besides, the Fig. 2 in the Appendix C shows that the splitting will reduce the performance and the only benefit is the efficiency.\n3.\tThe performance on ImageNet is not state-of-the-art. For example, it is worse the the performance of MobileNetV3 under the similar FLOPS constraint."
        }
    ]
}