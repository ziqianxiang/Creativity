{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "I believe this paper violates the ICLR anonymous submission policy by including an acknowledgments section that thanks several people by name.  Acknowledgments should not appear in the review version.  I believe this violation is sufficient cause for automatic rejection of the paper and recommend such action.\n\nMy review of the paper content, not factoring in this rule violation, follows.\n\n---\n\nThis work addresses management of GPU memory resources during deep network training by proposing to learn a policy whose action space includes both swapping variables between CPU and GPU memory, as well as recomputing variables on the GPU (trading compute for memory).  A deep Q-network (DQN) is used to learn the policy.  Experiments across training a range of networks demonstrate superior memory savings at lower overhead compared to prior approaches.\n\nThe paper claims a reinforcement learning approach (DQN) is valuable in order to handle training of networks with variable execution flow.  However, most of the benchmark networks, including VGG, ResNet, and UNet have a predetermined static architecture.  Even with the example of stochastic depth ResNet, the expected depth and its variance are known.  GPU and CPU resources and memory capacity are also known ahead of time.  It is not clear to me why reinforcement learning should be a necessary or even preferred approach to scheduling in such scenarios.  Shouldn't it be possible to predetermine the best schedule using some form of constrained optimization?  Why is it appropriate to use reinforcement learning?\n\nCoupled to the above questions, might the experimentally observed performance advantages be attributable to simply having a wider range of scheduling actions (e.g. more flexible combinations of recomputation and swapping) rather than the DQN approach?  An ablation study or statically scheduled baseline that has the same available action set would be a helpful reference point.\n\nResults speak only to memory usage.  How much training time is actually saved?\n\nFinally, I am unsure whether ICLR is the best venue for this work; a systems conference might be more appropriate."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\nThis paper proposes to use DQN to learn recomputation decisions in training deep neural networks. The authors build a solution that can decide between variable swapping and recomputation with a given memory limits.\n\nStrengths\n\nFirst of all, I like the idea of applying learning for recomputation decisions, the authors propose a reasonable solution to the problem.\n\nIt is nice to have a memory limit as an input to the algorithm, so that most of the memory can be utilized. It is also good to have mixed decisions between swapping and recompute.\n\nWeakness\n\nOne potential weakness of the  method is how generalizable the DQN model could be. From what I can read, it is unclear whether the authors have trained the DQN model on the same set of model architectures and then use the same model to predict the decision on the architectures. If that is the case, it is unclear whether the gain comes from exhaustive random search during the training phase or due to the generalization of the model.\n\nIt would be great to address the generalizability of the proposed method. What will happen if you apply the same DQN to ResNet1k without retraining the models on that architecture? What will happen if you change the memory limit setting to cases that you have not seen before?\n\nMy second complaint is about the evaluations. While the authors have proposed to take both swapping and recompute as set of actions. It is unclear what is the additional gains. The paper can be improved by showing experimental comparisons between methods that only applies recompute or swapping. \n\nFinally, please note that acknowledgement section breaches the anonymous requirement of the reviewing process. Although the above review was written without taking this into consideration.\n\nBecause the above weakness of the paper, I think this paper should not be accepted to the program. I would encourage the authors to improve the paper according to the comments and submit to future venues.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, an RL-based recomputation method for GPU memory allocation is proposed. The idea is to learn which intermediate variables should be stored or discarded to save memory while reducing overhead. In experiments, several existing methods are compared with the proposed one for various NN architectures such as ResNet and LSTM. \n\nThe idea of applying RL to recomputation is interesting and reasonable. However, the paper is not solid enough due to the lack of comparison with other recomputation methods.\n\nIf I'm understanding correctly, the key benefits of the proposed methods are the versatility (applicable to any NN architectures) and the ability that takes into account the tradeoff between memory reduction and runtime overhead. In this area, however, this paper is not a seminal work. For example, Kusumoto et al. (2019) studied a recomputation method based on dynamic programming in the same setting. The paper should contain the discussion toward this work (pros/cons against the proposed method) and also should compare in the experiments. \n\nAlso, it is better to explain the main limitation of the proposed method. For example, in what scenario the proposed method may perform worse than the existing methods?\n\nKusumoto, Mitsuru, et al. \"A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation.\" arXiv preprint arXiv:1905.11722 (2019)."
        }
    ]
}