{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Specific Problem Tackled and Approach\n\nThe paper presents a novel method for low level image feature extraction called pyramid non-local block (PNB) which is the family of pyramid non-local enhanced networks (PNENs).  The authors showed its primary utility in the application of edge preserving image smoothing. However, one additional and important impact of the paper is that the PNB approach can be used as a plug and play component in other image processing and computer vision applications such image denoising and image super resolution and this adds to the generality of the approach which is commendable. The paper sets forth quantitative analysis of all the applications against state-of-the-art approaches and demonstrate the high efficiency without loss of performance in case of PNB. \n\nImpact and motivation\n\nIn terms of impact and motivation, PNB’s impact is threefold. Firstly, the pyramid approach uses multiple convolutions with different kernel sizes for generating embedding features for self-similarity (correlation) estimation. This improves the robustness of estimating correlation across pattern scales. Also, besides this, the PNB approach preserves long range dependencis in non-local features in a convolution network while remaining computationally efficient. This is one of the primary impacts of the paper. \n\nFinally, the pyramid non-local block can be incorporated into existing convolution based models used for low-level image processing in applications like denoising or for single image super-resolution. \n\nExperiments have been presented on several datasets to indicate that the pyramid non-local block improves the performance in image smoothing, denoising and super-resolution.\n\nStrengths and Rooms for Improvement\n\nThe abstract, while clear, can be improved with some linguistic clarity. Some of the sentences are fragment but this does not hinder understanding. \n\nFollowing the abstract, the introduction properly presents the motivation and related literature in an impressive manner. It might have been stronger if a brief introduction of pyramid based approaches and its applications were discussed. Still, the motivation of the fact that capturing long-range dependencies is a limitation in most modern convolution approaches has been conveyed very well. \n\nAs for Section 2 (Related Works), instead of mentioning so many works in literature, I would recommend you to cite review papers which can hold more information without taking much too much space. This way you can elaborate a bit more than just listing them including the sentence, “We do not elaborate due to limited space,” would then be unneeded.\n\nThe sections following Section 3 (Method) are the strongest. The mathematical presentation is detailed and very well presented. Figure 2 serves as a very useful architecture and motivation as to why the method presented in the paper is novel. \n\nEquation 9 and 10 does a splendid job in explaining why this is computationally efficient. \nThe two equation mathematically shows that it is motivated by two ideas. Firstly, the computation and memory requirement of non-local operations can be controlled within an acceptable load; and secondly, the robustness of self-similarity estimation is enhanced since the self-similarity is measured at various resolutions. \n\nThe experimental findings are well presented for image smoothing. I appreciate showing the spatial/image output using multiple approaches in figure 4. As a support, the quantitative findings showing PNB’s superiority over several state-of-the-art methods in table 1 is welcome. However, if this pattern were to continue for also image denoising (as from salt-and -pepper noise etc.) and image super-resolution, then the paper would be even stronger. Currently, the quantitative metrics are measured in tables 3-5 which is great but if the reader is presented with demonstrations like figure 4, then it can be excellent. \n\nAs a whole, the novelty and impact of the approach was conveyed clearly and in a scholarly manner. The work is commendable. Some clarity issues can be improved. For example, the sentence “Besides it is a very common phenomenon that the same type of patterns appear to own various spatial scales, which has not been taken into consideration in existing non-local operations” – is long and unclear. There are several such examples and I encourage the authors to break the long sentences and replace them with smaller and simpler sentences.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\nThis work proposes a pyramid non-local block that can use  different scales of content patterns to enhance network capability. This work uses different stride sizes and window sizes to obtain information from difference scales.\n\nThe major issue for this work is that the contributions are very incremental. Compared to the non-local operator, this work uses convolutions with stride to create different Q, K, and V for attention operator. I didn't see much novelty in this work. The detail comments are as below:\n\n1. This work is very incremental. Attention operators have been widely used in various scenarios.\n\n2. Pyramid thing to extract information from different scales has been a widely used method in various tasks.\n\n3. It is very hard to see the improvement in Figure 4.\n\nSuggestions:\n\nMore novelty work should be added to be accepted by top AI conferences like ICLR."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this work the authors propose the Pyramid Non-local Block (PNB) as a neural network module to estimate similarity coefficients between parts of an image. The PNB, unlike a convolution, can efficiently compute correlations between spatially distant parts of the input. In the pyramid non-local enhanced networks, the PNB is combined with standard dilated residual blocks. Applications of this new network are shown and evaluated on edge-preserving smoothing, image denoising and image restoration, where in all cases the evaluations show a small but consistent gain in PSRN compared to baseline models.\n\nThe PNB is based on prior work proposing such non-local blocks, and the main innovation is the introduction of multi-scale processing in the form of a scale pyramid for the reference and embedding feature maps (obtained with strided convolutions with larger kernels). This makes it possible to compute correlations over larger spatial scales without paying the exponential computational cost.\n\nThe paper cites prior work appropriately and lists the relevant hyperparameters used for training. The examples in the appendices show interesting visual improvements.\n\nAs is, I believe this paper would be a better match for a more specialized venue (e.g. a computer vision conference). The main shortcomings of the paper are its limited novelty (multiscale pyramids are an old \"trick\" in computer vision), and the use of baselines that are behind SOTA for the discussed problems -- e.g. Dai el al, \"Second-order Attention Network for Single Image Super-Resolution\" could be a more current baseline for SISR (instead of MemNet) and Liu et al, \"Non-Local Recurrent Network for Image Restoration\" for denoising (which the authors cite and briefly discuss in the introduction). Also, in the RDN paper (which the authors used as a baseline for denoising) applications to superresolution are presented, with reported results better than the ones achieved with PNEN in Table 5. I found it surprising that a less well performing method (MemNet) was used as the baseline for the SISR task in the present work.\n\nFor edge preserving smoothing, where the largest relative gains in evaluation scores are seen in the present work, the baselines and evaluation protocols seem to be less well established than for the remaining two tasks (the original paper using ResNets for this doesn't use PSNR/SSIM for evaluation). It should also be noted that Zhu et al (2019) formed the ground truth set by asking for human preferences between results of existing algorithms, which makes the interpretation of the score gains less clear than in SISR and denoising where the actual target image is completely known.\n\nSuggestions for improvements:\n* The visual examples in the appendices do a good job of showing the improvements from using PNB. Are there any new failure modes that the baseline networks do not exhibit?\n* For superresolution evaluation, it would be more informative to list (at least in appendices) results for more downsampling factors than 3x (related work seems to examine 2x, 3x, 4x, and 8x). Similarly, for denoising, data for various noise levels would provide more insight into the behavior of the proposed model.\n* In Table 4, and 5, what are the averages computed over?\n* In Zhu et al (2019) which was used a ground truth for the edge-preserving smoothing experiments, each undistorted image is associated with multiple filtered results selected as preferred by human evaluators. How is this information incorporated into the PSNR/SSIM scores in Table 1?"
        }
    ]
}