{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a very simple strategy to reduce the variance of a batch of training data. It applies data augmentation operations to expand a training example, so that the mini-batch solely consists of the variants of this example. Since the correlations among examples within the batch is stronger than those in a batch with different examples, the gradient has less variance. While it can reduce the gradient variance during training, which leads to faster convergence, it also has better generalization performance. This is confirmed by empirical studies on difference datasets with a variety of neural networks, such lstm, transformer and convnet. \n\nOverall, this seems to be a simple yet effective method to improve the convergence and generalization at the same time, which is orthogonal to existing approaches. It might be better to try this method and improve the state-of-the-art on at least one dateset.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes increasing the size of the mini-batches by using existing data augmentation methods performed on the same samples of the mini-batch, called Batch Augmentation (BA). The authors claim that this technique provides better generalization.\n\nThe improvement is justified by reduced variance of the gradient, therefore it might be relevant that the authors compare (computation resources/performances) with Variance Reduced Gradient baselines (see [1]) and analyze it empirically with a better proxy for it (see below).\n\nThe authors use the L2 norm to empirically analyze the variance of gradients. As the above justification is an important claim, in my opinion using a better proxy for the variance could be useful: for example measuring the variance exactly as done in [1] or using the second-moment estimate as in [2].  \n\nAs the idea is fairly simple, and the use case seems somewhat limited for a distributed setting, I could be wrong but, in my opinion, the results are not significantly improving: for example in Fig. 4a the baseline outperforms the validation error of BA in the early iterations and marginally down performs at the later iterations. Also, it is not clear to me if this is due to a poor choice of learning rate for the baseline relative to BA (in this case, could be due to using a larger learning rate then its optimal as the validation error is better in the early iterations).\n\n- For a more fair comparison, each plot could also compare with a baseline where the batch size is M*B using no data augmentation (e.g. in Fig.1,2,4). \n\n- Page 5, 2nd paragraph: the authors mention that increasing M continues the trend of a reduced validation error, whereas from Fig. 3b we see that increasing M starts to saturate the reduction of the validation error.\n\n- In my opinion, more detailed ablation studies could be useful. For example, if Adam is used, it would be interesting to see the average effective learning rates for the parameters throughout iterations. Also, a wall-clock comparison on sequential and on distributed settings would be useful.\n\n---Minor---\n- Page 4, Sec.2.1: what do the authors mean by “BA reduces variance less” \n- Page 5, Fig. 4b: the caption is inconsistent\n\n[1]  On the Ineffectiveness of Variance Reduced Optimization for Deep Learning, A. Defazio and  L. Bottou, 2019. \n[2] Reducing Noise in GAN Training with Variance Reduced Extragradient, T. Chavdarova, G. Gidel, F. Fleuret, S. Lacoste-Julien, 2019.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduce batch augmentation, replicating instances of samples within the same batch with different data augmentation, to improve both optimization and generalization of standard optimization algorithms. It has been shown in the paper that batch augmentation reduces gradient variance and can serves as a regularizer.\n\nHowever, I’m not fully convinced by the experiments and vote for a rejection for now. My main concern is that the authors didn’t quantitatively measure the convergence improvement over standard small-batch baselines. The goal of large-batch training is to get linear scaling within certain range. I doubt if batch augmentation would retain the convergence speed-up of standard large-batch training.\n\nMain argument:\nThe authors claim that batch augmentation reduces gradient variance, thereby improving convergence. However, I didn’t find any experiments quantitatively measuring the speed-up. Ideally, large-batch training enjoys perfect scaling, i.e., the required training steps get reduced by the increasing factor of batch size. Otherwise, we have to use more computation for training the networks to the same accuracy. If batch augmentation fails to enjoy perfect batch size scaling, then its main contribution would reduce to an effective regularizer and the contribution is minimal in this sense. To this end, I suggest the authors to conduct some experiments as done in Shallue et al, (2018) to check the batch size scaling of batch augmentation.\n\nBeyond that, I hope to see some experiments on understanding the regularization effect of batch augmentation. Currently, the authors just show BA is effective but fail to explain why.\n\nMinor comments:\n- In the main paragraph, the authors misused citet and citep (e.g. the third paragraph of the introduction). \n- The Table. 2 wasn’t even mentioned in the paper.\n"
        }
    ]
}