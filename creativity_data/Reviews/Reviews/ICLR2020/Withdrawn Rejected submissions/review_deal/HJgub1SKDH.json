{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This is an empirical study that looks into the similarities/differences among neural network architectures. Here, similarity is defined by the predictions made on the training and test examples (i.e. two binary vectors for each neural net architecture). In addition, the authors look into how such similarities/differences evolve over time as training takes more epochs. The main conclusion obtained is that neural network architectures are nearly equivalent.  \n\nI have reservations against the conclusions of the paper and I believe the experimental results do not support the main conclusion. The experiments point to a much simpler explanation: that some examples are easy to classify while some examples are hard to classify correctly. First, if you look into the figures, such as Figure 1.a, you will find that bimodality is actually quite weak. Yes, about 77% of the examples are classified correctly by all neural nets, and about 6% are classified incorrectly by all neural nets, but there is still about 17% of examples that are classified differently by different neural nets. If all neural nets were need equivalent, then a much smaller proportion should fall in the middle. \n\nSecond, as pointed out by the authors themselves in Section 4.3, other learning algorithms such as SVM and Decision Tree, tend to fit those examples which neural networks learn first. This is a strong indicator that the experimental results can be explained using the data, not the \"equivalence\" of neural network architectures. \n\nThird, the authors demonstrate that bimodality is not an inherent part of neural networks by demonstrating that it does not hold for the Gabor patches datasets. This is a third clear evidence that bimodality was an artifact of the data itself, not the architecture. \n\nAside from this, the paper itself is not ready for publication. There are other issues such as: \n1- The authors repeatedly refer to the \"order of learning\" in the introduction without formally defining it first. I was able to understand what this means indirectly after reading their definition of \"learned at least as fast as\" another example. I suggest a formal definition is mentioned in the introduction since they repeatedly mention this property in that section. \n2-  The claimed relation to effective generalization is weak. The authors only show that when the labels are shuffled, bi-modality disappears. Against, as I mentioned earlier, this is consistent with the explanation mentioned above since if labels are random, then all examples are equally hard. \n3-  The term \"consistency\" already has a well-defined meaning in the literature. I suggest that the authors use a different term, otherwise it can be quite confusing to the readers. \n4- All of the experiments are done on images. The authors mention that the same phenomena is observed in other domains. If so, I suggest to include such experiments in the paper. \n5- Figure 3 seems to be a wrong figure. It is supposed to show \"consensus\", not consistency. \n\n\n\nSome additional minor comments: \n- There are phrases that are ambiguous. For example: what is \"perpetual similarity\" in Page 1?\n- \"Reproduce-able\" in Page 2 should be one word \"Reproducible\". \n- In all figures, the axis are hard to read. \n- \"networks instances\" should be \"network instances\" \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper analyzes collections of deep neural network classifiers on different dataset: the only constraint is that the different models are trained on the same classification problem.\nTheir similarity is based on the accuracy performance on both the training and test sets.\nThe paper shows that the accuracy and pattern of classification are similar across the different models. In other words, most data points in both the training and test sets are classified correctly by all the networks, or by none at all.\n\n\nThe paper is well-written, and the motivation is clear. However, multiple papers in the literature already exist about the importance of initialization, and the difference in generalization for different local minima obtained by neural networks.\n\n\nMy vote is borderline reject for the following reasons:\n- Although the paper has some interesting insights, the whole interpretation of the model is based on the two evaluation metrics introduced in Section 2. These two evaluation metrics measure how much different models agree for the prediction of examples, and allow to measure the \"easiness\" of examples. The \"easiness\" of examples was already studied in curriculum learning. The paper could be improved by using other qualitative evaluation metrics.\n- The paper studies the consensus of models for different types of architecture/datasets/optimizers (all the compared models share the architecture/datasets/optimizers). However, all the models also seem to share the same type of inialization: according to Section 2, all the neural networks have the same type of initialization (Xavier initialization in the first paragraph).\nDoes using different types of initialization also have an impact? Maybe different types of initialization wouldn't lead to the same conclusions.\n- The paper seems to consider only the agreements on the training and test sets, what about the validation set?\n- the \"examples which are easy for linear networks are for the most part also easy for non-linear networks, but not vice versa. Thus, the non-linear networks classify correctly most of the examples that are classified by the linear networks\". This result is related to curriculum learning: although curriculum learning does not explicitly consider linear models, it considers simpler models that are easier/more efficient to train.\n- \"Curiously, the deeper the network is and the more non-linearities it has, and even though the model has more learning parameters, the progress of learning in different network instances becomes more similar to each other. Un-intuitively, this suggests that in a sense the number of degrees of freedom in the learning process is reduced, and that there are fewer ways to learn the data\". Isn't this observation related to the choice of activation functions? For what kinds of activation functions did you observe that phenomenon?\n- The results about the \"out of sample test sets\" seem straightforward. Test examples from a different distribution are more likely to meet less consensus since the neural networks are trained for the distribution of the training set.\n- Before deep learning became popular, most machine learning models (e.g. SVMs) were interested in convex optimization to avoid this kind of phenomenon: even if linear problems were trained with SGD, the model that was learned was close to a global minimum of the (convex) optimization problem, and the performance of the models was robust to their initialization. In Section 4.2, a NN is trained with linear activation and average pooling, which makes the optimization problem convex if the loss is convex. This part of the paper is related to the argument of most ML research from the 2010s (that obtained a solution close to the global minimum), just like the part where SVMs are learned.\n- I did not understand the KNN classification part which usually does not learn parameters and should then not depend on initialization. How are the conclusions drawn for KNN classification?\n\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors trained many deep networks and compared the represented functions by which test examples they predict correctly or correctly throughout training. At points during training, there is a subset of test examples that all models get right and a subset that all models fail on. For models that generalize well, these two subsets tend to be large.\n\nStrengths:\n- Understanding why deep learning works is an important research area.\n- The authors conducted many experiments using different models and datasets.\n\nWeaknesses:\n- The paper overstates the results in my opinion.\n- It is not surprising that some train/test/OOD/artificial examples are easier or have higher probability under the training distribution and are thus correctly classified by all models, some are incorrectly classified by all models, and for others the models disagree. This is expected to break down when no generalization is possible in Figure 9, where all examples have low probability under the training distribution.\n- The consensus score that directly measures similarity of the learned functions does not seem to fully support the conclusions of the authors. For example, Figure 3 shows no clear order in which examples are learned --- most mass is spread out until the models become good at the whole dataset.\n- The histograms show many bins, where the left most and right most bins indicate model agreement. This can be misleading because there are many more bins for the disagreement. What do these add up to?\n\nClarity:\n- The paper is lacking structure. For example, the scores should all be defined in Section 2, which currently defines two scores with more definitions added in the experiment section. It could also help to introduce paragraph headlines.\n- The writing is often verbose. For example, redundancy in the introduction should be reduced.\n- The use of notation and equations is imprecise and more confusing than helpful. While the equations could be improved, I think the scores can more easily and clearly be described in plain words.\n- Text in the figures is unreadable because it is too small.\n\nQuestions:\n- What is the motivation for including the consistency score, which is just average accuracy for each example, compared to the consensus score? It seems that the latter better compares similarity of the learned functions.\n- What dataset was used to train the fully connected models in Figure 10? Why did the models use ELU activations and dropout, did this affect the results? Perhaps the results are explained by simplicity of the dataset, so all examples can be learned together rather than after another. The smoothness in the distribution might be explained by the higher epoch resolution compared to the other figures.\n\nSuggestions:\n- The paper could be strengthened by investigating what the easy/hard examples have in common. Do the models learn certain classes first? Or do they learn the least ambiguous examples first?"
        }
    ]
}