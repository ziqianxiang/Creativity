{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "This paper tackles the problem of continual (online) learning. Their approach is based on training subcomponents that can be specialized for each different task while the prediction of each subcomponent is getting mixed with a linear weight vector, which allows fast adaptation when the task is changed. They tested the suggested method on two new online learning datasets based on language modeling. The results show that the proposed method is effective in online learning, especially when tasks share transferrable knowledge due to the modular nature of the proposed method.\n\nThe approach is straightforward and general, so it has much potential to be applied to general continual learning problems. Also, the presented online learning scenarios and datasets are beneficial to the community in that the scenario is more realistic. \n\nHowever, I am not fully convinced of the contribution of this paper. My concerns are the following:\n\n- The experiment results show that the suggested method does not suffer from catastrophic forgetting by recalling the appropriate module through the weight vector. However, these results are not surprising since the whole parameters of a module are kept as LTM. While working well on the domain tested, the approach has a clear limitation in scaling due to the computation and hardware limit of a system. Ad-hoc method to address the problem is introduced and works empirically, but it is not justified clearly. More fundamental questions should be answered, such as, when LTM should be created and removed, how to reuse, consolidate, or merge prebuilt LTM modules.\n\n- The experiment details are missing. How exactly is LSTM trained?; when do you initialize the starting state of LSTM? (Or, when do you truncate the gradient signal in performing BPTT?) How do you define “batch”? Since the proposed method is targeted to online-learning, a precise description of the experiment is very important, especially when others want to reproduce or compare with the results.\n\n- There is previous work that tries to solve a similar online setup, which does not contain an explicit task boundary [1,2,3]. As it is noted in the paper, some of the settings are somewhat contrived and artificial. However, to clearly show the advantage of the proposed method, it would be necessary to have an experiment on a more widely accepted training scenario. The dataset proposed in [4] could be used to show the efficacy and flexibility of the proposed method.\n\nThe following are general questions:\n\n- It seems like the training procedure for the mixture weights will result in selecting memory modules that have a relatively low loss, especially when the weight vector is trained with more iteration. Then, don’t this become a winner-takes-all type of rules?\n\n- Why fixing up the parameters of some module is beneficial, i.e. 20/10 STM/LTM split is better than 30 STMs? Do you have an intuitive explanation about this?\n\n- Doesn’t the post-switch confusion penalize the network having a low average loss? I understand the intention of the measure, but it might be harsh for a good network. \n\nHere are minor modifications to improve the paper.\n\n- page 2, section 3, “Or” -> “Our”\n- page 3, “MoE” is used, and I am assuming this is Mixture-of-Experts, but the acronym is never introduced.\n- page 3, “STEM” -> “STM”\n- page 5, “orpus” -> “corpus”\n- I am unsure that “memory” is the right term to use; it is understandable conceptually, but “a parameterized module” or something similar to that is more precisely describing the proposed architecture.\n- page 5, “10 for each language”. Is this correct? There are 5 languages and if the training procedure alternates over each task 10 times in a random order, then there will be only 50 sequences, not 100. Is this typo, or am I misunderstanding something?\n\n-----\n\nReferences\n\n[1] van de Ven GM, Tolias AS. \"Three scenarios for continual learning.\" arXiv preprint. 2019:arXiv:1904.07734\n[2] Sprechmann et al., “Memory-based Parameter Adaptation”, ICLR 2018\n[3] Rolnick et al., “Experience replay for continual learning” arXiv preprint. 2018:arXiv:1811.11682\n[4] Lomonaco et al., “CORe50: a New Dataset and Benchmark for Continuous Object Recognition.” CoRL 2017"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\n\nThe paper introduces Growing Long-Term Memory Networks  (GLTMNs) which has both long-term memory (LTM) and short-term memory (STM). STMs are the main modules used for computations and  LTMs are used as long-term storage which can be swapped with STMs at any point in time. The authors also propose 2 lifelong language learning tasks and show that GLTMNs perform better than vanilla LSTMs in these tasks.\n\nMy comments:\n\nThe paper proposes an interesting way of using long-term memory modules. This helps in growing the capacity of the network as and when needed. I have no issues with the experiments.\n\n1. It is not very clear if only  STMs are used while making predictions or LTMs are also used.  If LTMs are also used, then is it correct they are used with frozen weights (i.e. no backprop through LTMs? This is very crucial information. Please clarify.\n2. The model description is not very clear. It would be beneficial if the authors can describe the model in a precise way and also include pseudocode for the learning algorithm. I cannot reimplement this paper by just reading the current model description.\n3. I do not buy the argument that having a task id is not realistic. In fact, it is more realistic. Humans do not do an unknown task. They are told what task they are doing.\n4. Growing the model dynamically to avoid catastrophic forgetting is also explored in Sodhani et al. 2018 (Towards Training Recurrent Neural Networks for Lifelong Learning) who use Net2Net to do zero-shot expansion of the model parameters.\n5. What does p/lang mean in Figure 2? It was never introduced.\n6. I assume that the authors will release the code and data upon acceptance of the paper.\n7. I love the first and last line of the paper!\n\n\nMinor comments:\n\n1. Section 2.3: Fix the brackets for Joulin & Mikolov 2015 citation.\n2. Page 2: second last line: “Or” should be “Our”.\n3. Page 3: STEM should be STM.\n4. Fix the grammar of the last line before section 4.2\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper claims two contribution towards ML. Recurrent networks that can grow its memory by creating new modules.  The second one is a language character and word-based task language modelling task. The main contribution is in my opinion the first contribution. If this would work very well this would be quite useful. The paper seems to provide a step in this direction. \n\nWhile the paper claim quite general usefulness of the task the test application is more narrow / only one has been tested. I would have loved to see more applications. Why not apply it to Machine Translation in the same fashion as for language modelling.   \n\nThe paper could have applied the method as well to another architecture for instance transformers to prove their claim that it would fit to other frameworks too. For language modelling this has become a standard now. Thinking on this the method might not be easily transferable as mentioned.\n\nThe paper could also state how much modules cells the final network uses. Does it use up all 30 at the end?\n\nThe proposed Growing Long-Term Memory Network are well investigated in a multilingual setting which is nicely able to showcase benefits of such a model to be potentially be more memory efficient and faster for the language needed. However, I guess it was finally not smaller as the largest LSTM model compared with and longer to train. The authors should made this clear.  The authors should have assessed more metrics like training times, steps, units.  Overall, I found the paper interesting while a bit weak on the empirical side, I would have preferred deeper analysis. It remained a bit difficult to judge efficiency of the final model and training time. \n\nThe captions of the tables could explain the tables better (some of the abbreviations are not explained - informed helps but make it more explicit.)   \n\n"
        }
    ]
}