{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduces a method to solve, what they call, the “Generalized Few-Shot Learning” problem. This problem involves learning a set of novel classes from few labeled examples, while still being able to classify examples from the base classes for which we have a lot of data. \n\nThe method is the following: they first learn an embedding network and fully-connected classification weights for the base classes for which we have a lot of data. Then, there is a meta-learning stage, in which we treat some classes from the base classes as novel classes, and learn how to generate new fully-connected classification weights for these novel classes so that these new weights combined with the previous weights are able to classify a held-out set of examples from both base and novel classes. The generated weights are generated using a learned dictionary, where we learn a set of keys and values. The prototype from each novel class is used as a query in an attention mechanism scheme. A weighted mean of the values is computed, where the weight is the similarity of the corresponding key and original query. The prototype and this weighed mean are added together to serve as the new classification weight for the novel class. Thus, during the meta-learning stage, we are learning keys and values so that this weight generation process works for novel classes. The method is then tested on a new set of novel classes to see how well it is able to learn each new few-shot learning task, while still being able to generalize to the base classes. Experiments are performed on mini-ImageNet and Tiered-ImageNet.\n\nThis paper is very similar to the work by Gidaris & Komodakis. To me, the only difference between that work and this submission is: in the attention mechanism, rather than directly using the fully-connected layer weights of the base classes as the values during weight generation, here a separate set of bases are learned to serve as values.\n\nI believe that the pre-training step, the loss used during meta-learning, sampling base classes to use as novel classes during meta-learning, and even using multiple episodes in a batch (where different base classes serve as novel classes) are all borrowed from Gidaris & Komodakis; However, in the paper, it seemed to me that many of these techniques are presented as being novel. In the related work, it is mentioned that “[compared to Gidaris & Komodakis]… we differ in how we compose classifiers and the unified learning objective.” As mentioned, the generation scheme is a bit different, but I don’t see a difference in the learning objective between the two papers?\n\nThus, if it is true that the only difference between the two methods is a minor difference in the attention mechanism of the weight generation scheme, I feel that should be better reflected in the writing of the paper and I don’t think this is enough of a contribution to merit acceptance of the submission.\n\nReferences\nGidaris & Komodakis. Dynamic Few-Shot Visual Learning without Forgetting. CVPR 2018.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a method for generalized few-shot learning (GFSL) where the test data set contains samples of both base and novel classes. This is a natural extension of the dominant conventional FSL setting, under which the test data comes only from the novel classes. The proposed method, termed CASTLE follows a very similar pipeline as the method of Gidaris & Komodakis (2018). Specifically, the synthesized classifier weight vector is a combination of both class prototype (mean of the few shots)  and a weighted sum of some basis vectors. The difference is that in Gidaris & Komodakis (2018), the bases are the base class weight vectors, and in this work, the bases are learned from the data. These bases or neural dictionary of classifiers are based on the models in (Changpinyo et al., 2016; 2018) developed for zero-shot learning. \n\nGFSL clearly should be the next focus for few-shot learning and this paper does advance the state-of-the-art under this setting. However, I can’t help but notice the similarity between this work and that in Gidaris & Komodakis (2018). The changes are based on borrowing ideas from existing zero-shot learning works in (Changpinyo et al., 2016; 2018). So the contribution is incremental. There are indeed some improvements on performance over Gidaris & Komodakis (2018). However, the multi-classifier learning brings 1-2% from the Appendix. The residual formation in Eq. (5) also helps. So one would expect that adding these two tricks to Gidaris & Komodakis (2018) would bring the performance closer. \n\nSection 3.2 is unclear to me: is the base class classifier parameter \\thetha_S updated during the episodic training process? Eq. (7) suggests it is, but the text suggests otherwise. \n\nAny reason for not using the commonly used low-shot imagenet dataset and CUB for the GFSL experiments? It would also be useful to examine different CNN backbones including the less powerful conv-4-64 and more powerful wide ResNet based architectures. It is well known that it makes a big difference. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThis paper tackles the generalized few-shot learning (GFSL) problem [1-2], that learns (few-shot) novel classes while not forgetting original classes. Built upon [1], the authors propose a different form of the weight generator for novel classes.\n\nPros:\n- The proposed method outperforms the prior work, in both GFSL and the standard few-shot setting.\n- The authors suggest the harmonic mean as an evaluation metric for GFSL.\n\nCons:\n\n1. Limited novelty & Prior works are not properly cited.\n\nThe problem of generalized few-shot learning (GFSL) is proposed in [1-2]. Hence, the authors should cite [1-2] in the last paragraph on page 1, where they introduce the GFSL problem. Also, many parts of the method are from [1], that [1] proposes an attention-based weight generator and this work only differs from the detailed attention mechanism. However, the authors do not cite [1] in the method section but only shortly mentions it on the last page. I strongly believe that the authors should clarify their contributions, as newcomers may misunderstand contributions and give wrong credits.\n\n2. The source of the improvements?\n\nThe authors should explain why the proposed method is better than [1]. One possible reason is that [1] uses a dictionary of size |S| (the size of the seen dataset S), but the proposed method uses 2-3 times of them, as stated in Appendix C.1 and Figure A8. Other reasons, e.g., the attention coefficients (Eq. (7) of [1] vs Eq. (4) of this paper), the combination weights (Eq. (8) of [1] vs Eq. (5) of this paper), or the classifier form (cosine similarity of [1] and linear of this paper) could also be a candidate. The authors should identify the source of improvements.\n\n3. Results in the standard few-shot setting.\n\nThe authors should clarify if the results are reproduced or copied from prior work. It seems that they are copied since the numbers are identical, e.g., see Table 1 of [3] and Table 1 of [4]. If so, the authors should specify where the numbers are from. Also, the caption of Table 4 and Table 5 seems to be wrong, as LEO [3] uses WRN-28-10 backbone instead of ResNet-12. Finally, the source of the gain also should be investigated. If the joint learning of many-shot and few-shot is the reason, DFSL [1] also should outperform other methods.\n\nMinor comments:\n- In Table 1, the authors state the reference for IFSL but not for L2ML' and DFSL'. For consistency, the authors should state all references or none.\n- Why DFSL' has a quotation mark? L2ML' is an inductive version of L2ML, but DFSL is already designed for the inductive setting.\n- OptNet in Table 4 and Table 5 should be changed to MetaOptNet [4].\n\n\n[1] Gidaris and Komodakis. Dynamic Few-Shot Visual Learning without Forgetting. CVPR 2018.\n[2] Ren et al. Incremental Few-Shot Learning with Attention Attractor Networks. NeurIPS 2019.\n[3] Rusu et al. Meta-Learning with Latent Embedding Optimization. ICLR 2019.\n[4] Lee et al. Meta-Learning with Differentiable Convex Optimization. CVPR 2019."
        }
    ]
}