{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a solution to learn Granger temporal-causal network for multivariate time series by adding attention named prototypical Granger causal attention in LSTM. \n\nThe work aims to address an important problem. The proposed solution seems effective empirically. However, two major issues have not been fully addressed in the current version: (1) the connection between Granger causality and the attention mechanism is not fully justified; (2) the complex design overkills the whole concept of Granger causality (since its popularity is due to the simplicity). \n\nThe paper would be a strong publication in the future if the two issues can be addressed in a satisfactory way. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "--\nComments after reading the reply from the authors:\nThanks for the clarification which resolves most of my concerns, and I have updated the score accordingly.  Besides, it will be more convincing to add more baselines for comparison or provide more explanation why linear Granger (which is proposed decades ago) is a strong baseline.\n\n--\nThis paper investigates the important problem of inferring Granger casual structures from multi-variate time-series data, and propose the Granger casual structure reconstruction (GASER) framework with prototypical Granger causal attention.  The paper is in general well written with clear notations and is easy to follow. The proposed attention mechanism is also  intuitive. Experiments on  both simulated/synthetic and real-world datasets show the proposed approach achieved both improved casual recovery and more accurate predictions.\n\nHere are some concerns, and the reviewer is willing to adjust the rating if these concerns are resolved.  \n\n- Baselines. Have enough strong baseline algorithms been included? \n (1) It seems that GASER outperforms state-of-the-art prediction algorithm IMV-LSTM by a large margin (Table 5) even if it is not designed for the prediction task. \n(2) In Table 1-3, the other two baselines usually perform similarly or worse than the simple linear Granger baseline. Does it indicate the compared baselines are not strong enough? \n(3) Why linear Granger, i.e, VAR with L1 regularization, is not included in Table 5 for prediction? \n\n- Model design and trade-off.  According to Table 1-5, it seems that by adding the proposed attention mechanism, we can achieve both improved results (better than SOTA IMV-LSTM) and better interpretability.\n(1) Is there any trade-off in the design?  \n(2) Is the proposed approach sensitive to different parameters, e.g., 1) the number of prototypes, $K$ (when it is not the same with the ground-truth), 2) $\\alpha$ in Equation 9 (is 0.5 is a generally good default choice?), 3) $\\lambda_1, \\lambda_2$ in Equation 13, 4) $\\gamma$ in Equation 12.\n\n- Prototype learning. It seems that the prototype learning is used to deal with the lack of data. However, adding prototype learning increases the number of parameters to be learnt, i.e., $[p_1, â€¦ p_K]$.  It will be helpful to provide more intuition. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "This paper proposes a new way of finding the Granger temporal-causal network based on attention mechanism on the predictions obtained by individual time series. It describes a surprisingly complex procedure for computing the attention vector based on combining Granger-inspired attentions with attentions obtained during a diverse prototype generation process. There are also extensive experiments demonstrating the success of the proposed method in uncovering the underlying temporal-causal graph.\n\nThere is a major theoretical and conceptual issue in this paper: the proposed attention vector depends on all of the time series. Thus, theoretically, it is a mistake to think that the $i$th time series causes the target if its attention value is higher.  In other words, the attention values leak the information about all time series in prediction of the target time series. The following paper is helpful on this topic:\n\nSarthak Jain and Byron C Wallace. \"Attention is not explanation\". In NAACL 2019.\n\nThe authors should be extremely careful in terms of calling Granger causality the actual causality. Granger causality only discovers time order. Also, it is unclear why the authors use terms such as \"inductive\" Granger causality, which is redundant.\n\nIt is unclear for me if the results in the paper are statistically significant, given the provided intervals. \n\nThe authors should expand the motivation for the prototypical design. In the experiments they show that having the prototype part improves robustness and accuracy, but the authors never show what interesting prototypes they learn. Also, this model is super complex. A key reason for popularity of the Granger causality is the simplicity of the underlying VAR model. Finally, the authors need to provide the run-time speed of training and inference for the model.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a novel way of reconstructing Granger causal structures using a differentiable neural network architecture that contains attention modules that are proportional to the Granger causality of the input layers. Furthermore, the architecture blends individual-specific induced causal structures and cross-population prototypical causal structures. The paper has an extensive experimental section on which the proposed method shows impressive improvements in causal discovery performance and predictive performance on par with state-of-the-art.\n\nAs main contributions the paper:\n* proposes a novel architecture \n* shows its values using extensive experiments\n\nOverall, I find the paper well-written with a clear description of the proposed architecture and clear experiments showing the importance of the architectural choices. A possible downside is the relative lack of novelty, since the method seems like a reasonable extension of the existing work. However, I think this counterbalanced by the excellent results on the causal discovery task and the extensive nature of the experiments.\n \nIn terms of suggestions, I think an illustrative example of the granger attention on an artificial / toy example would help a lot to give an intuitive understanding on how the method works and how the causal structure is being built.\n"
        }
    ]
}