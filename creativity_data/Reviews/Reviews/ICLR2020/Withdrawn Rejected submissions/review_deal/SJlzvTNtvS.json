{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a new objective for one-shot neural architecture search (NAS) using gradient-based methods. The authors hypothesize that the finite differences approximation in the second-order approximation in DARTS [1] directs the gradients towards bad local minimas in the architecture space. To this end, the authors reformulate the architecture update rule in DARTS as a sum of the one-shot model training loss and the one-shot model validation loss (scaled by some factor \\lambda). The algorithm is evaluated on the same standard image classification benchmark used in DARTS[1].\n\nI vote for rejecting this submission, mainly for the following reasons: (1) The fixes they propose to previous gradient-based NAS algorithms such as DARTS, SNAS [2], etc., are not clearly motivated and justified, both in theory and practice. (2) Almost no novelty. Except the modified loss, I do not see any difference compared to first-order DARTS. (3) Since the authors used different hyperparameter settings compared to DARTS, it is not clear if the better results in the benchmark results come due to these hyperparameters or their proposed fixes.\n\nMain arguments\n\nWhat this paper is trying to improve, is solely based on claims made throughout the paper, and there are no empirical or theoretical evidences to support these claims. For instance saying that not solving \"exactly\" the inner optimization problem the bi--level settings is always worse than the counterpart, is not true for non-convex inner functions. Performing a few gradient steps to approximate the inner problem solution can act as a regularizer (see [3]). \n\nThe authors also mention quite often the term \"overfitting\". It is not clear in which level the overfitting occurs? Is it in the parameter space or in the architecture space (see [4])? Furthermore, in the NAS context (not only in the single-level optimization scenario), a low value of the validation loss that is minimized during the NAS loop, does not necessarily correlate with the true objective: minimizing the validation/test error of discrete architectures. [4] shows that actually the test accuracy of the final architectures does not correlate with the one-shot validation accuracy, and empirically demonstrate that there is an overfitting on the architectural level.\n\nFinally, I did not find the empirical results and experimental settings to be entirely clear and convincing. There are some crucial issues, such as: different hyperparameter settings compared to baselines in the final training of found architectures. Therefore, it is not clear where the contribution in better resuls comes from; or just one run of their methods in some settings in Table 1 and in Table 2.\n\nOther comments\n\n- In Section 1: SNAS [2] updates both w and \\alpha using all the training data\n-  Throughout the paper: One-shot validation accuracy or derived architecture validation accuracy when retrained from scratch? Which one do you refer with validation accuracy=\n- Section 3.3: How did you tune the learning rate (0.03 vs 0.025 in DARTS)?\n- What are the settings you use for your algorithm in Table 1?\n\nSuggested Improvements\n\n- Empirical results proving the claims made throughout the paper\n- Experiments showing how the performance of the architectures found by bi-level optimization relates with the solution returned by the inner problem, i.e. the number of gradient steps towards the approximated w^*.\n- Evaluation of their methods on other image classification benchmarks and other tasks, such as language modelling.\n\n[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang.  DARTS: Differentiable architecture search.  In ICLR, 2019.\n[2] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR, 2019\n[3] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning.  In ICML, 2018\n[4] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter. Understanding and Robustifying Differentiable Architecture Search. ArXiv, 2019\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose a mixed-level optimization based NAS (MiLeNAS) to alleviate the gradient error caused by second-order gradient approximation in bilevel optimization. In MiLeNAS, the authors minimize both the training loss and validation loss to train the architecture parameters, which is different from existing methods that only minimize the loss of validation data. Equipped with the mixed-level optimization scheme, the proposed MiLeNAS has less training cost and yields better performance than the considered baseline methods. However, there are many questionable arguments. More explanations and experiments are required to demonstrate the effectiveness of the proposed method.\n\nStrengths:\n1.\tThe proposed MiLeNAS aims to alleviate the gradient error caused by second-order gradient approximation in bilevel optimization. By reformulating the NAS problem as a mixed-level optimization, MiLeNAS obtains better results than the baseline bilevel optimization methods.\n\n2.\tThe proposed MiLeNAS is computational efficiency. Compared with existing bilevel optimization methods, MiLeNAS has lower search costs.\n\n3.\tThe proposed MiLeNAS is a generic framework for NAS. Experimental results on two search space settings demonstrate the effectiveness of MiLeNAS.\n\nQuestions and points needed to be improved:\n1.\tWhat is the difference between the proposed method and the first-order variant of DARTS? Why does the proposed MiLeNAS yield better results than the first-order variant of DARTS? Why does MiLeNAS (0.3 GPU day) take less training cost than the first-order variant of DARTS (1.5 GPU day)? More explanations are required.\n\n2.\tThe authors argue that DARTS fails to converge to an optimal solution since the optimal w*(alpha) cannot be obtained via a single step training. However, MiLeNAS also uses the same single step training technique. Thus, this issue also occurs in the proposed MiLeNAS method.\n\n3.\tWhat would happen if the proposed MiLeNAS method uses the second-order approximation?\n\n4.\tThe results in Figure 3 are not convincing. For example, bilevel (1st) method achieves higher accuracy than bilevel (2nd) method, which conflicts with results in DARTS[1]. More discussions on this phenomenon are required.\n\n5.\tSome experiment results are very confusing. In Table 1, the authors provide four results of MiLeNAS but do not mention the differences among them. The similar issue also exists in Table 2.\n\n6.\tIn Section 3.3, the authors mention that the proposed method can further improve XNAS[2]. However, there are no experiments to verify this argument.\n\n7.\tSeveral state-of-the-art NAS methods should be compared in the experiments, such as PROXYLESSNAS[3] and P-DARTS[4].\n\n8.\tThe authors do not release the source code of the proposed method. The page of the released hyperlink is empty.\n\n\n \nMinor issues:\n1.\tThe authors mention that “evolutionary algorithms and reinforcement learning-based methods require thousands of GPU days to achieve state-of-the-art performance”. However, this argument is wrong. In fact, ENAS is a typical reinforcement learning-based method and only takes 0.5 GPU days to train the model.\n\n2.\tIn the third paragraph of Page 2, “gap between the training loss and the evaluation loss” should be “gap between the training accuracy and the evaluation accuracy”.\n\n3.\tThe structure of this paper can be improved. The main content of Section 2.3 is to describe how to demonstrate the effectiveness of the proposed method, which is more relevant to Section 3. Therefore, it is better to put Section 2.3 in Section 3.\n\n4.\tIn Figure 2, the caption should be put below the graphic.\n\n\nReferences:\n[1]\tLiu, Hanxiao, Karen Simonyan, and Yiming Yang. \"Darts: Differentiable architecture search.\" ICLR, 2019.\n[2]\tNayman, Niv, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi Zelnik-Manor. \"XNAS: Neural Architecture Search with Expert Advice.\" NeurIPS, 2019.\n[3]\tCai, Han, Ligeng Zhu, and Song Han. \"Proxylessnas: Direct neural architecture search on target task and hardware.\" ICLR, 2019.\n[4]\tChen, Xin, Lingxi Xie, Jun Wu, and Qi Tian. \"Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation.\" ICCV, 2019.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a new method for  Neural Architecture Search (NAS) by using a mixed-level reformulation. In the bilevel based NAS method, it has to completely train a network to update the network architecture alpha. Based on a reformulation, the proposed method relaxed the hard constraint in the bilevel NAS to a soft constraint.  As a result, it can update the model weights and architecture alternately. The results showed the proposed method achieved comparable results on CIFAR10 with much less training time and better accuracy on ImageNet transfer learning.\n\nThe proposed method seems very reasonable but somehow incremental. Instead of solving the w subproblem exactly, it relaxed the problem and solve the alpha and w subproblems alternately. In this case, a large speed-up is expected. However, reformulating Eq. (1)(2) to Eq. (4) is incremental and completely not new in the optimization field.\n\nSeveral of the statements in the paper is over-claimed. It says that \"The existing second order failed to converge to a optimal solution but the proposed method can be optimized more reliably\". There is no evidence to support this, neither theoretically or empirically. In the results, the proposed approximation obtained comparable results as the second order approximation method DARTS, e.g. in Table 1.\n\nIt also claimed that both single and bilevel optimization are special case of the proposed method. This is not correct. The proposed method is more like approximation solver for the bilevel optimization problem.\n\nOverall, the proposed method does have its value: it brings a first order solver for the NAS problem, and obtained good results with much less training time. However, the benefit is over-claimed."
        }
    ]
}