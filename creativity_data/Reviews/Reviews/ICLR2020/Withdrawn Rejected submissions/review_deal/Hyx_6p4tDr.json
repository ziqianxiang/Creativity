{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper tackles dimensionality problems arising in bilinear pooling algorithms. These algorithms typically involve bilinear multiplication of third-order tensors (neural network weights) against second-order data features. When the features are high-dimensional, the tensor-product becomes computationally very expensive. The paper proposes to impose low-rank on the third-mode of the tensor; i.e., write the tensor mode as a sum of rank-one outer-products. In this case, the tensor-product reduces to an inner-product between data features projected into these rank-one components, which can then be evaluated cheaply. However, the question remains how close these projections are to the original tensor. The paper explores three possible distributions from which the projection matrices can be sampled from and their kernel linearization quality: i) Radamacher distribution, ii) Gaussian distribution, and iii) random orthogonal frames. The approximation quality of the tensors with these choices are theoretically investigated. Experiments are presented on two datasets, and show some promise.\n\nPros:\n1. The specific approach of low-rank tensor approximation and investigations into kernel approximation qualities, as is done in this paper, is perhaps marginally novel in the context of bilinear pooling.\n2. Theorem 2 is perhaps interesting, and it sort of extends some of the observations in Yu et al., 2016 into the specific setting of tensor-products.\n3. Experiments show some marginal improvements against baselines.\n\nCons:\n1. The key contribution in this paper do not appear significantly novel. The low-rank approximations are perhaps straightforward, as all it does is a direct re-writing of the standard tensor product as a matrix inner product along a mode.  The analysis of approximation qualities are mostly taken from prior works such as Yu et al., 2016 and Kar and Karnick, 2012. The proofs are almost direct extensions of technicalities from these papers. \n\n2. While, the motivations for the paper are to use bilinear pooling in a deep learning context, the analysis and technical details are assuming kernel linearization, and thus for kernel-based methods. This looks to me like a disconnect in what the paper wants to achieve. How would you justify the theoretical results when the low-rank factors are learned via back-propagation? How would you impose a specific distribution to these projection matrices? The experiments presented in the paper only consider using bilinear neural networks. Thus, it is unclear what is the purpose of the theoretical results, and what is it attempting to justify. In this light, I think the theoretical results are misplaced with regard to the goals of the paper.\n\n3. The experimental results are not entirely convincing; the computational complexity arguments the paper used in its favor do not appear that compelling in the experiments, (O(D\\sqrt(d)+d) against O(D+dlog d) of Gao et al, 2016 in Table 1), the accuracy numbers are  not consistently better, and are often inferior due to the approximation. \n\nOverall, the contributions appear marginal in comparison to prior results, and the results are not sufficiently convincing."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #6",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper proposes a computationally efficient bilinear form to extract high-order information for visual information. The key idea is to assume the frontal slices of the projection matrix T to be low-rank. Also, it performs random sampling from a distribution to fill the entries of the project matrices E and F, making them random instead of learning from data. The effectiveness of this random projection is theoretically analyzed based on different distributions. Experimental study is conducted on action segmentation to compare the proposed scheme with the existing ones. \n\nThis paper is well-organised and carefully presented. Nevertheless, with respect to the literature, the technical novelty (low-rank assumption and random matrix projection) seems to be limited. Moreover, the experimental study is not as strong as the expected. The improvement over the existing methods is not sufficiently signficant or consistent. For action recognition, only the results of GTEA in Table 2 are promising. Experimental study on other vision tasks and benchmark datasets shall to be conducted to provide more validation, for example, by using Fine-grained image recognition datasets which have been widely used to assess bilinear based methods."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper, a bilinear feature mapping based on random matrices is studied. The problem is to construct a map taking two vectors as inputs and outputting a single vector. Although a naive implementation requires some multiplication with a three order tensor, the proposed method avoids the expensive computation by inducing low-rank assumption. The proposed method is empirically evaluated with two action recognition tasks. \n\nAlthough the paper contains some interesting aspects, I find there are several critical limitations, especially (A) lack of motivation, (B) gap of theoretical results, and (C) insufficient experiments. \n\n(A) So the paper considers the setting that we have two different vectors x and y as input features. I think this is not a standard-setting in both supervised and unsupervised tasks. In particular, I couldn't find any necessity of limiting the pattern of second-order interaction in the form of Eq. (1). I mean, (1) only considers the interaction of (x, y), but it excludes the interaction of (x, x) and (y, y). So there needs to explain why we have to consider the interaction pattern (1). \n\n(B-1) I couldn't understand what Theorem 1 tries to convey. What we need to evaluate, if I understand correctly, is how the random projection preserves the interaction between x and y. However, Theorem 1 evaluates completely different thing (the inner products <x_1, x_2>, <y_1, y_2>). I really confused here; why we have to consider two x (x_1, x_2) and two y (y_1, y_2) while we only have single x and y in the learning setting? \n\n(B-2) Theorem 2 is also unsatisfactory because of the same reason above. Also, the implementation uses the linear approximation of sin and cos, which makes Theorem 2 unapplicable. \n\n(C) First, the proposed method is evaluated only by a single task, temporal action recognition. I feel it is not enough to show the validity as a general NN module. For example, what would happen for image classification tasks? Second, I don't see the proposed method is quite superior to the existing methods. For example, Hadamard is comparable in terms of runtime."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this paper, a bilinear form based on a 3D tensor with low rank frontal slices is proposed in order to reduce the number of operations and memory requirements. Additionally, it is proved that, if factor matrices are randomly chosen and following predefined distributions, then the obtained fusion map is an unbiased estimator of some specific RKHS operators. I found interesting the fact that, instead of learning the projection matrices, this method proposes using randomly generated matrices. However, it is not clear the advantage of using randomly generated matrices in terms of computation cost and performance compared with matrices learned from data. Below, I list the issues of the paper:\n-\tIn this paper, it is suggested that using random matrices is better than learning them from data, but it is not clear what is the advantage. From the computational point of view, it is intuitive that learning matrices will increase the computation cost but there is no comparison or theoretical evaluation for this. Also, it is not clear if the performance using learned matrices will be better or worse than the proposed in the randomized algorithm. A comparison against learned projection matrices would clarify this point.\n-\tThe authors prove that using random matrices can be seen as an approximation of using RKHS (Theorem 1 and 2). However, what is the computational cost of using the RKHS operators instead? This should be clarified to highlight the advantage of the method. Additionally, comparisons with experiments using RKHS operators could give a better evaluation of the method.\n-\tIn the Introduction, it is suggested that standard deep convolutional neural networks extract only first-order information. I cannot agree with this. In fact, the use of nonlinear mappings such as sigmoid or ReLu functions has the effect of introducing higher order information. I think this should be clarified in the paper.\n-\tIn page 3, it is mentioned that “Via sampling tensor entries from different distributions…”, but I see that what is actually controlled is the distribution of projection matrices entries instead, which, of course affects the distribution of tensor entries. I think this should be clarified in the paper.\n-\tQuality of plots in Fig. 1 are rather low. I would suggest to use bigger fonts, to set the same y-axis limits in each row, and to use thicker lines, etc.\n"
        }
    ]
}