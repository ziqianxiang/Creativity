{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes an approach for learning class-level and individual-level (token-level) representations based on Wasserstein distances between data subsets.  The idea is appealing and seems to have applicability to multiple tasks.  The reviewers voiced significant concerns with the unclear writing of the paper and with the limited experiments.  The authors have improved the paper, but to my mind it still needs a good amount of work on both of these aspects.  The choice of wording in many places is imprecise.  The tasks are non-standard ones so they don't have existing published numbers to compare against; in such a situation I would expect to see more baselines, such as alternative class/instance representations that would show the benefit specifically of the Wasserstein distance-based approach.  I cannot tell from the paper in its current form whether or when I would want to use the proposed approach.  In short, despite a very interesting initial idea, I believe the paper is too preliminary for publication.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors proposed a template-based interpretable representation that works based on the earth mover's distance of each class to a number of \"environments\", which could be taken as union of a few random classes. To achieve this, they train several critics based on Fisher GAN. The method is evaluated based on classification and retrieval tasks. \nThe representation, by construction, is aimed towards interpretation and is specially useful in multi-class classification tasks. \nHere are my concerns:\n- Since the environments are taken randomly in the experiments, it is not investigated how sensitive the method is with respect to the choices of environments. Also, does it make any sense to design environments to include related (and not random) classes?\n- It seems necessary to include some experiments to assess sensitivity of the interpretation with regard to the small perturbations that are not changing the class label. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper defines a representation learning strategy based upon estimation\nof a matrix of Wasserstein distances.\n\nThe idea is excellent.  The ability to \"solve\" IPMs reliably is a recent\ndevelopment in deep learning whose ramifications are still being explored.\nIntuitively this line of research could plausibly result in general\nmethods which are theoretically intelligible and broadly applicable.\nIndexing at least one side of the matrix of estimated WDs with events\n(rather than classes) has interpretability properties useful for\ninformation retrieval and also conveys benefits reminiscent of learning\nwith privileged information.\n\nHowever, the exposition could be greatly improved by using the\nstandard language of probability theory.  The discussion in 3.1\nwas particularly painful to read.  What is the difference between\n\"existing in an environment\" and \"conditioning on a measurable event\"?\nPhrases like \"belonging to any random subset of the dataset\" suggest\na non-deterministic method of selecting an element of the power set of\nthe training data, but it is unclear what to do if more training data\narrives in this case.  \n\nThroughout the entire paper the word \"random\" is apparently used in the \ncolloquial sense of \"arbitrary\".  *Correct every instance of this.*\nIf you actually are referring to generating samples from a distribution,\nbe explicit about the generative process.\n\nSection 3.5 was more confusing than enlightening.  In general I understand \nthat environments can be leveraged for intelligibility and admit manipulation \nfor information retrieval.  The exact strategy remains somewhat opaque.  If \nyou are under space constraints refer to an appendix with more explicit details.\n\nIn the experiments section phrases like \"environments consist of random \ncombinations of classes\" is also not helpful.  Do you mean something like \n\"uniformly selected from the set of all class pairs?\"  Or something like \n\"uniformly selected from the power set of all classes?\"  How volatile\nare the experimental results with respect to the non-deterministic choice \nof environments? \n\nI want to accept this paper if the exposition is improved, which I think\nis possible during the response period.\n\nMy other comments are not blocking issues, but would either improve the\ncurrent paper or inform future directions of research.\n\nThe technique bears some resemblance to Wasserstein Discriminant\nAnalysis.[1]  That paper seeks a projection that maximizes the ratio of \nWasserstein distance between classes vs. within classes.  Here, \nalthough the common representation is a nonlinear mapping \nanalogous to a projection, we merely try to estimate all the\nWasserstein distances rather than maximize them, so it is not trained\nto be discriminative per se.   That is ok since the representation is\ndesigned to be used for a variety of tasks (modulo section 4.2), but it \ndoes leave open the question \"what if the matrix of estimated \nWasserstein distances isn't informative, e.g., due to poor choice of \nenvironments?\"  There is no attempt to assess the representation \nexcept via utility in downstream tasks.\n\nThe common representation was justified computationally, but I suspect\nis beneficial statistically.  It might facilitate safely including a\nlarge number of environments and then spectrally compressing (i.e., SVD)\nthe resulting matrix without overfitting the data.  However clearly if\nthe capacity of this layer is too small, then all estimated WDs will\nbe close to zero.  If we posit a low Bayes error classifier for the\nmulti-class problem associated with the dataset, that might imply there\nis some conditioning of the input under which the matrix of (actual) WDs\nhas rank equal to the number of classes, which would in turn provide a\nuseful diagnostic to guard against an insufficiently discriminative choice\nof environments or insufficient capacity in the common representation. If\nthe matrix is full rank with a flat spectrum, however, that might indicate\nthe choice of environments is too granular and overfitting has occurred,\nit's not immediately obvious to me how to guard against this.  \n\nI am curious what the results in appendix A.1. look like relative to the spectral \nnorm or the smallest eigenvalue of the estimated WD matrix (smallest \neigenvalue assuming number of environments < number of classes,\notherwise the k-th eigenvalue where k = number of classes).\n\n[1] https://arxiv.org/abs/1608.08063\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Paper contributions\n=================\n- This paper proposes a method for constructing representations using a matrix of Wasserstein distances. These distances measure the discrepancy between each class and each environment, that is a random combination of some classes.\n- The paper evaluates this approach on a task of image retrieval.\n\nGeneral notes\n============\nThe general idea of measuring the distribution divergence for a set of classes is interesting and seems to be novel. But I argue that this representation can be limiting:\n- A set of divergences doesn't contain any pixel-level information, only divergences to some predefined classes\n- As a consequence, this representation will not be able to discover information that is not covered by the labels\nBecause of these limitations, it seems that this particular representation may be less useful for some applications than others.\n\nI don't follow why the paper proposes to use 'environments'  -- random combinations of classes. It seems that a square matrix (n_c x n_c) with all classes should do the same job.\n\nThe experimentation is very weak and does very little to support the claims. The paper considers only one substantial task to test the representation. This task is image retrieval by image query. The paper doesn't provide any comparison to existing methods or simple baselines.\n\nThe second contribution that the representations are interpretable and composable is not addressed.  I seems that it should be hard to interpret a large vector of distances to randomly chosen subsets of classes. There is no experiment demonstrating interpretability of the proposed approach. The compositionality is not addressed either. The samples provided in the appendix are not convincing.\n\nThe paper is generally well written and it is easy to follow. The literature review can be improved by providing prior work where \"approaches use hidden state vector of LSTM\" and \"features extracted from CNNs\" instead of generic references. \n\nSome of the claims are vague and excessively broad:\n- The proposed technique can be used with any task, but the paper is clearly limited to the retrieval task\n- The environments are too vaguely described and can be misinterpreted in the introduction\n\nConclusion\n=========\n\nI recommend to reject on the basis that \n- the approach is more limited than the paper advocates\n- the experimentation is weak\n- some claims are not addressed\n\nOther notes\n==========\nI recommend using term divergence instead of distance when it is not symmetrical."
        }
    ]
}