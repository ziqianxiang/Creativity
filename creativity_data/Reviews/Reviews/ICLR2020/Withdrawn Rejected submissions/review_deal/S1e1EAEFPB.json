{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a new mechanism to visualize the latent space of a neural network. The idea is simple and the paper includes several experiments to test the effectiveness of the method. However, the method bears similarity to previous work and the evaluation does not sufficiently show quantitative improvements over other introspection techniques. The reviewers found this was a substantial problem and for this reason the paper is not ready for publication. The paper should improve its discussion of prior work and better establish its place in this regard.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This paper discusses the use of a reconstruction network which is fed an internal representation of a deep network and is trained to reconstruct the input. The classification and reconstruction networks are trained simultaneously with a combined loss function. Various experiments are carried out to test whether this approach can help investigate adversarial examples and the quality of learned features for transfer learning.\n\nReview: The idea presented in this paper is simple and easy to understand and the paper includes experiments testing its effectiveness in various settings. However, there are various issues with the paper in its current form: First, the idea of reconstructing hidden activations is not new and the paper fails to cite future work. \"Dynamic Routing Between Capsules\" by Sabour et al. consider adding a similar reconstruction network to Capsule networks. \"Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions\" considers the use of this reconstruction network for detecting adverarsarial examples (like this paper) and also includes an unconditional reconstruction network which is added to a convnet (like this paper). I imagine there are other examples, but those are ones I know of. Second, the experiments are limited - they only include simple datasets (SVHN, MNIST, CelebA) which are normalized so that the object appears in the center of the image. This makes reconstructions difficult. The experiments also lack various important details like the model architectures and other hyperparameters, and overall are mostly \"proof of concept\" experiments rather than exhaustively testing out the proposed method. Finally, some of the claims made in the paper (particularly in the introduction) are dubious, for example that adversarial examples are a security concern and that transfer learning has been unsuccessful because it can harm performance. Based on these issues, I won't recommend acceptance. I think if the authors better situated the novelty of their work, included more exhaustive and difficult experiments, and/or focused on one specific application area it would be a stronger submission.\n\nSpecific comments:\n- \"to be understandable\" - can you be more specific? Internal representations of deep neural networks are not understandable whatsoever on their own, we usually use some post-hoc method (like your reconstruction network) to try to interpret them somehow.\n- \"[adversarial examples] raise many security concerns, such as the reliability of driverless cars, or the trustworthiness of facial recognition systems\" The field is moving towards a consensus that adversarial examples do not pose a realistic security threat in any of the settings they have been tested in. The most significant description of this viewpoint is in \"Motivating the Rules of the Game for Adversarial Example Research\" by Gilmer et al.\n- \"but has made the transferability of features to new tasks difficult, sometimes even harming performance\" This statement completely omits a huge body of work showing how effective transfer learning is, not only in computer vision (typically going from an ImageNet classifier to some downstream task with a small training set) but also in NLP (where transfer learning-based methods have state-of-the-art in virtually every benchmark). A good discussion of when transfer learning works and doesn't in CV is provided in \"Do Better ImageNet Models Transfer Better?\" by Kornblith et al.\n- The variables X and Y (without mathcal) in the \"(Perceptual regularization)\" definition equation are not defined.\n- The classification and reconstruction models are not described in Section 3. This is incredibly important. The quality of the reconstructions is likely extremely dependent on these architectural choices as they provide an implicit prior over the image space. This detail needs to be filled in. E.g. the pooling/downsampling in your classification network  will have a strong impact on the reconstruction quality, as will the size, depth, and configuration of the reconstruction network. Furthermore, I imagine the regularization parameter lambda is extremely important (as shown in section 4.2) but its value is not mentioned.\n- The experiments are on SVHN, MNIST, and CelebA which are all \"normalized\" datasets, i.e. all of their constituent images are normalized so that the object of interest appears in the same position in the center of the image. This makes reconstruction much, much easier. Results should be presented on non-normalized datasets (e.g. CIFAR-10, ImageNet, etc) in order to be convincing.\n- \"But, as the network gets more complicated it becomes computationally difficult to decode the feature map.\" Can you quantify this claim or back it up with experiments?\n- While the results on transferring to different CelebA classification tasks are interesting, it is much more common to pre-train on a diverse task like ImageNet than an extremely fine-grained task like classifying smiling or not smiling. I think these results would be more impressive if you followed modern experimental practice more closely.\n- \"The cost is that our method performs slightly poorer than the standard training method when S = T\" I would not call a difference of upwards of 8% absolute small."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\nIn this study, the authors propose a new architecture for visualizing the latent space of a network. This architecture is achieved by appending a secondary decoder 'head' to visualize the latent space by reconstructing the inputs.\n\nIn summary, I found the paper to provide a simple architectural change for attempting to address this issue of network interpretability and improve transfer learning. The results all appear sensible and expected, but the experiments are somewhat weak. My primary concerns are:\n\n1. The network architecture described is not terribly novel as many papers have explored pairing an auto-encoder with a classification task. Thus, I don't find the methods to be much of a contribution. The main contribution of the paper is thus in the interpretation and analysis of the method.\n\n2. The results on network interpretability are all qualitative.\nThat is, I must make a judgement on the resulting reconstructed image. I find this unsatisfactory for providing any quantitative assessment of the performance of the method. In particular, I would want to see a quantitative assessment of this method and compare it with other network introspection techniques. Currently, there is no benchmark nor any other methods to compare against.\n\n3. The results on transfer learning are quantitative but weak.\nIt is not clear to me how other simple transfer learning methods may perform on the task presented. That is, I do not know how hard the actual task is. (Would retraining a logistic classifier  with SIFT features work just as well?) I would expect to see far more experimental studies to justify these claims. Additionally, I would expect to see comparisons with other transfer learning techniques to see how well this method fares.\n\nMajor Comments:\n\n- Why do the authors allow for the decoder to regularize the latent space? Why not just stop the gradients at the latent space to provide a microscope to visualize the latent space without effecting it?\n\n- This reference appears to be in the same spirit as this paper and probably should be cited.\nAlain, Guillaume, and Yoshua Bengio.\n\"Understanding intermediate layers using linear classifier probes.\"\narXiv preprint arXiv:1610.01644 (2016).\n\n\n- Figure 2, 3 and 4 provide very nice qualitative demonstrations of the reconstruction providing insight into how an image was misclassified, however some issues remain:\n  1) A human must interpret the reconstruction. This is a subjective process and is not systematic.\n  2) Can you see examples in cross-validated test images where the visualization would predict a misclassification? Can you see counter-examples where the resulting image does not make sense?\n\n- Figure 6 and 7 provide interesting results indicating that the unsupervised objective improves transfer learning across several classification tasks related to faces. The results are encouraging but I am concerned that (a) the task is too easy, (b) tuning the hyperparameter for \\lambda can be difficult. For (b), I would like to ensure that \\lambda was tuned independent on a third validation test (and not the test set whose numbers are reported). For (a), I suspect that an important benchmark is to just take set \\lambda = \\infty and then perform linear classification on the embedding. It would be interesting to see how well these numbers fare as a baseline compared to the reported numbers."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Claims: The authors present perceptual regularization as a method for learning a visualization of deep representations for promoting interpretability and understanding of vulnerability to adversarial attacks. Second, they show their method can explain negative transfer to new tasks. Finally, they show that the representations learned with this regularization method transfer to unseen tasks better than without the reconstruction regularization.\n\nDecision: Weak accept. The authors tackle an important problem with a very simple regularization technique and show how it can aid interpretability in adversarial attacks and improve transferability through strengthening attention of features to new tasks and in the multi-task setting.  While the two problems are important, results on multiple datasets would be necessary to strengthen the paper. Section 3 on perception claims to give results on MNIST and CelebA datasets, but there only seem to be results on CelebA. For transfer in section 5, MNIST or SVHN should also be included by removing classes of digits and checking performance. The conclusion addresses the use of different types of generative models, but I would be extremely interested to see how VAE compares with a deterministic auto encoder, as they are known to have much better generative properties. \n\nFor Fig 6, it would also be nice to know how robust these numbers are to lambda, and include some variation for different values of lambda, or at least some discussion of how much tuning is required."
        }
    ]
}