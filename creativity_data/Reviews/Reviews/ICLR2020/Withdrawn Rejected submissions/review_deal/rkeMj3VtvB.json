{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an architecture to be used in multitask learning of audio tasks. They propose an encoder which provides input to layers at various depths. They employ a regularizer on the weights of the layers, in order to minimize the total amount of computation whilst having a shared representation pipeline to increase the overall performance of the system. What they are suggesting seems to improve the performance over a multitask learning baseline where the sharing mechanism is not employed (if I am understanding correctly).\n\nOverall, I found the paper little difficult to follow. Especially the experimental section is hard to follow as I found the figure captions lacking required information. For instance in Figure 3, I am not sure what the numbers under 'num. params.' section signify? I think you are choosing different values for the tradeoff parameter lambda, and then this corresponds to different accuracies? I don't understand why you are reporting different accuracy values when using the same Lambda value? Also on the figures I am guessing that different traces, different tasks. It would be better to plot the average trace as well. There is no mention of hyper-parameter search as far as I can tell from the experimental section. Also, I don't understand why you are not reporting the accuracy when lambda=0 in Figures 2,3? This would have helped us to better assess the whole potential of the proposed architecture. \n\n\nTo sum up, I think there seems to be value in this work, as the shared architecture seem to improve the overall performance when compared to a multitask learning baseline. This being said, I think very crucial information we are missing is the performance of the multihead baseline when it has the same number of FLOPS or trainable parameters as the proposed architecture. That is: You should train a multihead model baseline with the same number of trainable parameters as the suggested algorithm. I also think you should improve the writing, especially in the experimental section. Therefore I am currently suggesting a weak rejection. If you add the result of the multihead baseline with equal number of parameters with the proposed model, I would be open reconsider my decision.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors present a method for the audio classification problem, using a pre-trained network (shared encoder) and creating adapters for each new task. This is an interesting alternative to transfer learning, and it would be great if minimal adapters can be designed to work with a baseline network.\n\nThe adapter network is of the same depth as the shared encoder network, and uses both its own output, and the encoder's output from the previous layer for its next layer. Using learnable gates, the channels of its previous layers can be selected to be on / off. The idea is to create a network that can be tuned for accuracy and cost (params and FLOPs) for on-device inference.\n\nSince the motivation behind the paper is networks that are suitable for on-device inference and multi-task adapters, I find the paper lacking in the following respect:\n\n1. Describing how exactly they will implement this architecture on-device. The channels that have been turned off by the gates, can have their parameters trimmed, and the next layer can ignore them. However, it would be great to get actual numbers in terms of latency, size, etc. from a model that is running on-device, with explanation of the work done to translate the training model graph to an efficient inference graph.\n\n2. Comparison with other on-device model compression techniques like quantization, distillation, pruning etc.\n\n3. Comparison with the transfer learning variant, where only the last few layers are trainable.\n\nWhile this is an interesting approach, it would be more convincing to know that this approach:\na) Translates to actual improvements in on-device inference, along with implementation details.\nb) Is better than / competitive with other techniques for compression and transfer learning.\n\nWith the paper as it currently stands, I would recommend consideration for a dedicated workshop, but weak reject for the main conference. I would be willing to be convinced otherwise, if the authors can fill in more details as mentioned."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Overview\n\nThis paper presents a model that can perform multiple audio classification task using a number of shared layers. The main motivation for the proposed model  is the fact that it consumes the same input (audio) to estimate various properties (like language ID, musical pitch, bird song) and therefore it might be useful to share some of the lower  level computations while also having some task specific parameters/layers. The main contribution of the paper is an additive cost term that learns to distribute parameters between the various tasks while penalising parameters/number of operations. \n\nThe main feature of the proposed model is task specific layers that are placed in parallel to the shared embedding layers that are used by all tasks. The task specific layers receive inputs from all the shared layers and the task specific layers in the layer below. The design is such that the task specific layers/parameters for each task are independent and therefore during inference only the computations for a given task need to performed (in addition to the shared computations). The cost function is then used to distribute parameters between these layers given a fixed compute budget.\n\n Although I think the idea and the area of application are extremely interesting and relevant, there are some shortcomings that should be addressed before the paper is accepted. My main criticism is the fact that the comparison between the multi-head model and the proposed architectures is not fair. The multi-head architecture does not have any task-specific parameters and contains N softmax layers for the N tasks considered, on top of the final shared encoder layer. This architecture is quite restrictive because not only does the multi-head layer have fewer parameters, it is also forced to share the same fixed-dimensional encoding across all N-tasks. It is therefore unsurprising that the multi-head architecture performs worse since it uses the same fixed-dimensional embedding in the final layer to represent information using for all N tasks, some of which are quite unrelated, for example detecting musical pitches and bird songs. A better baseline architecture would be to have a number of shared layers, followed by a number of tasks specific layers in series rather than parallel. This architecture might yield similar accuracies, would also have independent parameters for each task and the same cost function could then be used to distributed parameters between tasks. Secondly, the authors do not comment on the relationship between tasks. Some of the tasks like speaker identification and language identification are clearly related. However, detecting musical pitches, bird songs, instruments and environmental sounds etc are quite unrelated and it doesn't make sense for a single network to be good at performing all these tasks simultaneously. This is clearly reflected in the evaluation where the single-task accuracy is always better than any of the multi-task results for some tasks. The paper does not provide any insight into what tasks can be usefully shared. I think this needs to be addressed first before investigating the best method for sharing computation between tasks. Finally, I found the evaluation section quite difficult to follow. The figures should be improved for clarity and the main arguments and inferences drawn from these figures are not clear. Overall I think the proposed cost function has limited novelly and although the area of application is extremely interesting, the paper fails to address some important basic questions about the nature of the tasks. Furthermore, the baseline comparison isn't fair for the reasons outlined above. \n\nMinor Comments:  \n\nIntroduction\n\n1. “characterized by a large number of parameters and floating point operations (FLOPs)”, the authors mention floating point operations however all models that are deployed on embedded devices are quantised to have integer weights. Floating point operations are only performed during training. Given that this paper is about on-device inference, this sentence is a bit confusing. The authors should replace FLOPs with number of operations. \n2. “the audio embeddings might fail to capture all the information needed to solve all tasks”, the authors here argue that the shared embeddings might fail to capture information related across all tasks, which is why I believe that baseline multi-head system should contain task specific layers/computations in series after the shared embedding layers. \n\nMethods\n\n1. I found the description of the gating mechanism quite hard to follow and had to read the section several times to understand what exactly is going on, even though the overall function is quite simple. The description should be improved for clarity. \n2. “The slope of the non-linearity s is progressively increased during training”, what schedule was used to do this? The experiments later also do not shed any light on how the slope was changed. \n\nExperiments\n\n1. Why use 64 bins while computing the STFT? The standard practice is to use 40 bins (in speech and other audio tasks). Do any of the tasks require extra resolution along the frequency axis? Lowering the number of frequency bins is a useful way of reducing the input size and therefore the number of computations performed in the first/input layer. \n2. “Note that the choice of the tasks used for the evaluation is consistent with the selected temporal granularity”. I’m not sure what temporal granularity means in this content. \n3. “As such, we do not consider speech recognition tasks, which generally require a much finer temporal granularity” I’m not exactly what is meant here. Temporal granularity in feature extraction? Or in the labels that are assigned to the input audio? This statement should also be clarified. \n4. “The number of parameters of the output softmax layer depends on the number of output classes.” What is the sum of parameters in all output layers combined? Surely this is non-trivial compared to the number of parameters in the encoder (65k and 125k)\n5. Figures 2b,c and 3b,c should be enlarged for clarity. The text says that the curves should not start at 0 since they consider the number of parameters in the task specific layers, but many points on curve 2b look like they start at 0. Also Figures 2 and 3 are quite difficult to read. Its not the most clear presentation of the argument. The tables are much more clear though. \n\nSummary\n\nThe paper presents a novel model  for performing multiple audio related tasks using joint/tied layers. The main novelty of the paper is to present an additive term in cost function and a gating mechanism that penalises large models. I think the multi-head baseline against which the results are presented is quite restrictive since it doesn’t have any task specific parameters and also has to share the same vectors for encoding all the information related to all the different tasks. Although the number of different tasks and experiments presented is commendable, the evaluation section does not present a convincing argument. The evaluation section also needs to be reworked in order to present the arguments and results more clearly. Finally, the authors should consider the relationship between the tasks considered and whether they expect all of them to be benefit from a joint/multi-task approach. \n"
        }
    ]
}