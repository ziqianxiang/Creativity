{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposed a method to estimate the instance-wise saliency map for image classification, for the purpose of improving the faithfulness of the explainer. Based on the U-net, two modifications are proposed in this work. While reviewer #3 is overall positive about this work, both Reviewer #1 and #2 rated weak reject and raised a number of concerns. The major concerns include the modifications either already exist or suffer potential issue. Reviewer #2 considered that the contributions are not enough for ICLR, and the performance improvement is marginal. The authors provided detailed responses to the reviewersâ€™ concerns, which help to make the paper stronger, but did not change the rating. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Paper Summary:\n\nThis paper proposed a method to produce instance-wise saliency map for image classification tasks. The proposed method develop a U-net-based generator for saliency mask, where the important modifications are (1) a skewness-inducing activation function for mask generation (i.e. controller), which is either a ReLU function or a scaled sigmoid function raised to a certain power. Authors argue that the proposed activation function leads to output saliency score to have right-skew distribution, which leads to more distinguished saliency map. (2) a smoothness-inducing mechanism where the saliency mask is generated at coarser scale then up-sampled with a bilinear operation so the generated map is smooth. Finally, the training is performed by minimizing solely on the cross-entropy loss with respect to the original model predictions, which is expected to improve faithfulness in mimicking the original black-box classifiers.  \n\nI'm leaning toward rejecting this paper in its current form. While I think this paper proposed an interesting strategy in improving the faithfulness of the explainer (i.e. training on the cross-entropy loss with respect to original classifier), the rest of the two modification either already exists in the literature (generate mapping at coarser resolution is an idea from Dabkowski & Gal (2017) / Du (2018) as pointed out by the author) or suffers potential technical issues that can benefit from further methodology improvement/empirical justification (please see Major Comments). Furthermore, empirical experiments seem to suggest that the proposed method sometimes fail to produce more smooth and more discriminative maps compared to its MGnet baseline (e.g. Figure 5, top right row), calling into the question of whether training solely on a cross-entropy loss is enough to ensure the generated saliency map is of high-quality. I still like the overall idea of this paper, and I think this work can be made more rigorous and informative by including a careful ablation study about the loss/gain of different regularization terms and loss functions (e.g. the effect of TV(M) term on smoothness, the effect of f(\\phi(I, 1-M)) term on discriminativeness, and the trade-off between cross-entropy v.s. the original negative log likelihood loss as used in Dabkowski & Gal (2017)).\n\n\nMajor Comments:\n\n(1) Custom Transformer\n\nThe derivation of the new activation function assumes that the distribution of the hidden-layer output $m^g_{ij}$ is strictly Gaussian (i.e. unimodal, symmetric, and with sufficiently light tail), and author proposed to build this Gaussian distribution from raw hidden-unit output through standardization (Section 3.2.2). However, this approach may not be valid in practice. This is because there is no guarantee that the distribution of $m^g_{ij}$ is strictly unimodal and symmetric. If the output is skewed/multimodal in anyway, the normalization operation won't produce a Gaussian distribution, since a linear transformation (i.e. normalization) of a skewed distribution is still a skewed distribution. As a result, I am worried that the hyperparameter derived in Section 3.2.3 may not always lead to right-skewed distribution as expected. If author would like to ensure the outcome distribution is right-skewed, it may be good to tune these activation function parameters for each instance so the empirical distribution of $m^d_{ij}$ satisfy certain skewness criteria (e.g., maybe tune the parameter in a way such that the pearson's coefficient of skewness of the empirical distribution is sufficiently small).\n\nIn addition, I find the author's claim about the benefit of right-skewed distribution (i.e. producing more discriminative maps) not yet well supported by the empirical evidence. For example, in Figure 4 the produced maps in the first three rows are relatively fuzzy, and the difference between the irrelevant features and the important features does not seem to be very high. The situation is only improved after the instance-wise finetuning. I think there are two possible reasons, either (a) the right-skewness of the actual distribution of the saliency map score in those images were not guaranteed (as mentioned earlier, I believe the current approach cannot guarantee the pre-activation units are Gaussian distributed, therefore the actual skewness of the post-activation values may vary depending on the input). (b) the right-skewness in the outcome distribution alone is not enough to guarantee good discrimininative behavior.\nTo clarify this (and provide more evidence for author's claim that skewness <-> better discrimination), author can consider visualizing the saliency map with varying degree of skewness (you can measure the skewness of the empirical distribution of the saliency-map score using pearson's coefficient of skewness), and show that the \"discriminativeness\" and skewness of the empirical distribution are correlated.\n\n(2) Smoothness of the produced map\n\nIn the original MGnet work, the authors used two mechanisms to guarantee smoothness of the saliency map: (1) generate map at coarser level and (2) TV penalty on loss function. In this work author used only (1). As a result, the produced map in some cases (e.g. Figure 5, top right row for BF After, also Figure 7, left fifth row for BF after) appear jagged and not smooth. I'm wondering if adding TV penalty back would help somewhat. Again, a careful ablation study would be beneficial in clarifying this.\n\nMinor Comments\n\n(1) There's some minor technical issue in the description for Equation (6). Assuming $z_{ij}$ is Gaussian, it is $\\eta$ AND sigmoid() COMBINED to \"approach the uniform distribution\", it is important to mention the sigmoid function since the uniform transformation used here is based on the Probability Integral Transform,   (i.e. if a random variable $z \\sim \\Phi(.)$ follows Gaussian distribution with CDF $\\Phi(.)$, then $\\Phi(z) \\sim Unif(0, 1)$, i.e. you are transforming a Gaussian random variable using Gaussian CDF to get uniform distribution), and equation (6) is using $\\eta$ AND sigmoid() combined together to approximate this Gaussian CDF [1].\n\n(2) Equation (14) on page 13. There should be an $z^2$ term within the exponential function.\n\nReference:\n[1] Gary R.Waissi, Donald F.Rossin (1996) A sigmoid approximation of the standard normal integral."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduced a new framework in the family of local perturbation-based explanations (saliency maps) for deep neural networks. The method is similar to existing methods in the literature (real-time saliency maps, etc) and seeks to tackle the difficulty of hyper-parameter selection for the complex optimization objective of these methods through assuming distributional preferences over the scores in generated saliency maps. The idea is that basically many of the terms in those objectives could be achieved by enforcing a right-skewed distribution on the importance scores (and also using the interpolation idea).\n\nI lean towards rejecting this paper for one main reason: the contributions are not enough for this venue. Both the introduced method and the metrics are slight modifications of what already exists and the experimental results do not convince me that the introduced method tackles an important problem with the existing methods.\n\nMore specifically, the paper seeks to solve the hyper-parameter selection problem with existing approaches (e.g. MGNet). None of the experiments seek to show how the method is helping with this problem. For instance, the average performance of MGNet over a set of randomly selected hyperparameters versus the model's performance, the computational cost of training a good MGNet mask generator versus the introduced method, etc.\n\nOne major motivation behind perturbation-based methods is their black-box nature. The authors refer to their method as being applied to black-box models. The introduced method clearly utilizes the inner layer activations of the network. This is not a black-box method.\n\nThe main contribution is a marginally improved performance compared to rival methods for the introduced metrics (which are slight modifications to already existing metrics.) The introduced Explainability metric is very well-justified and tackles the issues with the two older metrics. The advantage of the introduced methods is not very clear on the provided subjective examples.\n\nA few suggestions and questions:\n\n* The exact contribution of the work should be stated more clearly and experiments should be targeted towards it.\n* The paper is sometimes very, very difficult to grasp (Sec 3.2 and 3.3.)\n* It might be a wrong intuition but it seems like the fine-tuning step seems to make the output scores curated for the M_F metric which would make the results in Table 1 not quite fair. The same modification seems to be applicable to any given importance mask and a more fair comparison would include applying the same fine-tuning step in all of the methods.\n* The method's optimization process seems to be only focused on the SSR and neglect SDR. The text does not make it clear why this does not result in any loss of performance and if that's the case why it was necessary for methods like MGNet."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a mask predictor to bridge the information gap between the classification loss and the constraints introduced to obtain satisfying explainability. The predictor consists of a distribution controller and a mask generator to refine the mask towards the desired score distribution. The predictor is optimized solely under the classification loss without additional constraints, which therefore improves the faithfulness of mimicking target black-box models. This paper introduces two metrics to evaluate the proposed mask predictor and the experiments demonstrate the proposed method outperforms others in terms of faithfulness and explainability.\nOverall, this paper has some advantages:\n(1) the method could be a significant contribution to improve the local explanation for black box. The mask predictor can transform the output of the mask generator to establish the right-skewed distributions for relevance scores with the designed controller, which can decrease the difficulty of tuning the hyper-parameters. To solve the problem of the ad hoc constraints, the distribution controller models these above constraints and then is integrated with a neural network to directly guide the distribution of relevance scores. \n(2) the paper develops two metrics in terms of faithfulness and explainability to evaluate the effectiveness of the proposed method quantitatively. And compared with other baseline methods, it also generates saliency maps to visually demonstrate the explainability. All the experimental results demonstrate that the performance of the proposed method is better than others.\n(3) the paper states the motivation and the proposed algorithm clearly. The code is given.\nHowever, for the experiments, the following should be addressed.\n(1) It would be better to evaluate the proposed method with other networks as black boxes.\n(2) More detailed analysis should be added on the experimental results.\n(3) It should compare the explainability with some of the latest baseline methods.\n(4) It would be better to conduct the experiments on more datasets.\n\n Moreover, there are some minor comments:\n(1) Please state more details about the innovation of the method.\n(2)\tFigure 1 needs to be polished, for example the font. "
        }
    ]
}