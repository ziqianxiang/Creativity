{
    "Decision": {
        "decision": "Reject",
        "comment": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe proposed paper adapts Attentive Pooling Network (ATN) for graphs, by noting that the order of\nneighbors of any node in a graph does not matter unlike the neighboring words in a sentence (for\nwhich APN was developed). A simple modification of removing the layer which encodes higher order\nsequential properties in APN, eg n-gram like statistics, the APN is adapted to GAP. This allows\nencoding context from the neighboring nodes for a specific pair to be compared. Applications on Link\nPrediction and Node Clustering are demonstrated on three benchmark datasets.\n\nDetailed comments:\n- The paper is simple and easy to read and the modification of APN to GAP is also small while being\nwell motivated and empirically important.\n\n- There are some confusions in the writing: p4 first paragraph says that \"In principle one can learn\nusing all pairs of nodes, however that is not scalable, and hence we restrict learning between pairs\nin E.\" However from and around Eq2 it appears that r_s.r_t^- is also computed, where (s,r^-) is not\nan edge.\n\n- Also the statement below Eq2 \"The goal is to learn, in an unsupervised fashion...\" is not correct\n  specially for link prediction task, as having a training graph (with edges) is having annotations.\n        \n- Although, the removal of the `encoding' step from APN makes sense, it would also be possible that\n  the encoding operation is learned to be an identity function automatically if the training data is\n  presented appropriately. I would assume that learning in such a case might take longer, and might\n  not achieve the performance achieved by GAP. An ablation experiment could be reported to support\n  GAP further where encoder is kept in APN, but at the training same pair is presented multiple\n  times with the order of the neighborhoods randomized.      \n       \n- Similar to the experiment above, the order of the nodes could be arbitrarily fixed by some simple\n  logic. Eg order the neighboring nodes sorted by the feature similarity with the current node. This\n  can then be fed to APN and a baseline could be reported.\n\n- I do not directly work with Link Prediction or Node Clustering tasks, so am not familiar with the\n  literature and the baselines reported. However, it might be that many baselines do not use a\n  discriminative objective. Perhaps just using the discriminative objective gives a lot of boost and\n  using the context from neighbors is not very important? From Fig2 and Fig3a it seems that\n  neighborhood size does not make a big difference? This should also be ablated. One experiment\n  could be to use just the current feature and have a small MLP on it, and learn using the proposed\n  discriminative objective. If any other experiment can be designed in similar lines, it should be\n  included as well.\n\n- Why are run time comparison given on simulated data and not real data?\n\n- The experimental setup for training edge selection should be detailed more. In particular, I\n  would recommend that true generalization in terms of nodes and edges should be ensured, by having\n  no test node (i) appear in the train set and (ii) has an edge which connects it to a training\n  node. If this is not used then the alternative scheme used should be explained and argued for.\n\n\nI am not a direct expert in the area and felt that the paper was somewhat lacking. I am putting my initial rating\nto the conservative side. I am very open to revising the rating based on the author responses and the other \nreviewers' comments."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "- Minimal theoretical novelty: The paper is too focussed on the empirical advantage achieved on the datasets used in the experiments. \n\n- Regarding equation (2), what is the guarantee this modification depicted by the hard-margin loss will always lead to improved performance like what happened with the experimented datasets? or more practically what are the required conditions for it to perform well?\n\n- Writing really needs to improve. There are too many typos and grammatical mistakes. \nExamples include:\n -- p1: \"to learn representation of graphs\". \n -- p1: \"on other contexts its coupled with\".\n -- p1: \"NRL studies have shown a context-sensitive approach significantly outperform previous context-free SOTA methods in link-prediction task.\"\n -- p3: \"a more sophisticated neighborhood functions\".\n -- p5: \"reporeted\"\n -- p7: \"two other variant\"\n -- p8: \"and gain\"\n\n- The latter issue makes is sometimes tricky to follow the ideas presented. \nExample:\n -- last two lines in Section 3. \n\n- Last paragraph in Section 1 is pretty informative about the pros and cons of the method. It also rather admits the first issue mentioned here in the review.  \n\nMinor:\n- p4: \"Eg. \" --> eg. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposed a GAP (graph neighborhood attentive pooling) method to solve problems of node clustering and link prediction. The proposed idea is mostly inspired by the attentive pooling network approach (APN) that has been widely used in the NLP domains; indeed the detailed steps of the proposed method is almost a ``mirror‚Äù application of APN from question-answer pair ranking in NLP to the graph domains, and the only difference is that rather than using an RNN or LSTM to deal with the temporal orders in sentences (as in APN), the proposed method simply collect the first-order neighbors of the source node and target node and concatenate them without considering their orders. For the rest part, the authors used a hard margin loss function which is also the same as in APN.\n\nThe proposed method is very similar to the APN method and so its novelty is limited. Another concern is that although graph nodes do not have temporal orders as in word sentences, their relative connections manifested through the edges are important topological information that should be captured in contextualize a node. Unfortunately the authors almost totally ignored this information. The ignorance of the node orders appear to me not an advantage but instead a limitation, though it makes the computation and implementation much easier.\n\nThe graph attention network (GAT) is a very related method but I see no comparison with it in node clustering tasks. Also many recent methods on graph convolutional networks are not incorporated for comparison. \n\nBased on the concerns of novelty, lack of considering node topologies, and lack of comparison with strongly related methods, it is hard to recommend acceptance of this paper.\n"
        }
    ]
}