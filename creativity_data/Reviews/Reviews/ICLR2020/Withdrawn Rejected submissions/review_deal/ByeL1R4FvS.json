{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper shows that data augmentation methods work well for consistency training on unlabeled data in semi-supervised learning.\n\nReviewers and AC think that the reported experimental scores are interesting/strong, but scientific reasoning for convincing why the proposed method is valuable is limited. In particular, the authors are encouraged to justify novelty and hyper-parameters used in the paper. This is because I also think that it is not too surprising that more data augmentations in supervised learning are also effective in semi-supervised learning. It can be valuable if more scientific reasoning/justification is provided.\n\nHence, I recommend rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes to substitute simple noising operations with many data augmentation methods in consistency-based semi-supervised learning. The main idea is the same as previous work: constrain the model predictions of unlabeled examples to be invariant to different noise. The proposed UDA is evaluated on a wide range of language and vision tasks.\n\nOverall, the paper is well-written and clear. The most impressive point of this paper is its strong empirical results. However, it looks not surprising to me that more data augmentations found in supervised learning are also effective in semi-supervised learning. The paper fails to provide any theoretical insights but a thorough empirical evaluation.\n\nOne of my concerns is that the hyperparameters on vision tasks follow those of AutoAugment, which is carefully tuned on supervised tasks. Apparently, their hyperparameters are based on the whole labeled training dataset. In this case, the adopted hyperparameters include sort of information of the whole labeled dataset. Is it fair?\n\nAnother concern is how to control the strength of augmentations. For example, for digit images like SVHN, a \"6\" rotates by 180 degree is \"9\", whose prediction should change correspondingly. In this case, the assumption of invariance does not hold when the augmentation is too strong. \n\nI'm willing to increase my score if the authors address my concerns."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, their method brings substantial improvements across six language and three vision tasks under the same consistency training framework. I think the topic itself is interesting and I have the following concerns.\n(1) The first is about the contribution of this paper. In this paper, all the results, including the augmented methods are all well established approaches. The authors have just employed them in solving a new problem, without support about why they work. Thus, the results are only strategies, without theoretical guarantee or insights. It is difficult to convince the reviewers.\n(2) Although the authors have achieved seemingly promising results, I think it can not convince me since the authors have not answered the questions about why and when. I think this paper likes a technical report, not a research paper.\n(3) I have also noticed the discussions among the authors and other readers. It seems that the large improvement depends on the parameters heavily. So, why not to share the parameters directly? "
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper \"Unsupervised Data Augmentation for Consistency Training\" marries two recent ideas of \n1. \"Data Augmentation\" (DA) from supervised learning: The authors explore various methods for \"DA\" mostly inspired by much recent work such as Random image transformations, Backtranslation, and TF-IDF based word replacement.\n2. \"Consistency Training\" (CT) from semi-supervised learning: CT tries to minimize the divergence between the output distributions of the classifiers that are produced by adding noise to the input.\n\nThe key insight in this paper is that, data augmentation methods that work well during supervised training should also work equally well as the noise distribution for consistency training on unlabeled data. The authors support this claim empirically through the experiments in table 1 and 2. \n\nThe paper is well written and the authors present extensive comparative and ablation tests to demonstrate that their proposed method works well with both low and high amounts of labeled data.  This paper should be accepted into the conference."
        }
    ]
}