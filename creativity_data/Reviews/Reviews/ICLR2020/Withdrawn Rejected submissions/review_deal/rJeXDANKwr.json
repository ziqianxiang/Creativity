{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper introduces a neural architecture search method that is geared towards yielding good uncertainty estimates for out-of-distribution (OOD) samples.\n\nThe reviewers found that the OOD prediction results are strong, but criticized various points, including the presentation of the OOD results, novelty as a NAS paper, missing citations to some recent papers, and a lack of baselines with simpler ensembles.\nThe authors improved the presentation of their OOD results and provided new experiments, which causes one reviewer to increase his/her score from a weak reject to an accept. The other reviewers appreciated the rebuttal, but preferred not to change their scores from a weak reject and a reject, mostly due to lack of novelty as a NAS paper.\n\nI also read the paper, and my personal opinion is that it would definitely be very novel to have a good neural architecture search for handling uncertainty in deep learning; it is by no means the case that \"NAS for X\" is not interesting just because there are now a few papers for \"NAS for Y\". As long as X is relevant (which uncertainty in deep learning definitely is), and NAS finds a new state-of-the-art, I think this is great. For such an \"application\" paper of the NAS methodology, I do not find it necessary to introduce a novel NAS method, but just applying an existing one would be fine. The problem is more that the paper claims to introduce a new method, but that that method is too similar to existing ones, without a comparison; actually just using an existing NAS method would therefore make the contribution and the emphasis on the application domain clearer. \nI have one small question to the authors about a part that I did not understand: to optimize WAIC (Eq 1), why is it not optimal to just set the parameterization \\phi such that the variance is minimized, i.e., return a delta distribution p_\\phi that always returns the same architecture (one with a strong prediction)? Surely, that's not what the authors want, but wouldn't that minimize WAIC? I hope the authors will clarify this in a future version.\n\nIn the private discussion of reviewers and AC, the most positive reviewer emphasized that the OOD results are strong, but admitted that the mixed sentiment is understandable since people who do not follow OOD detection could miss the importance and context of the results, and that the paper could definitely improve its messaging. The other reviewers' scores remained at 1 and 3, but the reviewers indicated that they would be positive about a future version of the paper that fixed the identified issues. My recommendation is to reject the paper and encourage the authors to continue this work and resubmit an improved version to a future venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers the neural architecture search (NAS) problem under the out-of-distribution (OoD) environment. As the OoD problem is not visited in the current NAS literature, this paper proposes replacements for each of the three standard components in NAS, i.e., the proxy task, the search space, and the optimization algorithm; and each replacement is built upon an ensemble of existing techniques. Experiments are further verified on CelebA, CIFAR-10, SVHN, and MNIST.\n\nOverall, the novelty in this paper is limited and experiments are not very convincing. Please see the questions below:\n\nQ1. In the introduction, the authors write \"Machine learning systems often encounter OoD errors when dealing with testing data coming from a different distribution from the one used for training\".\n- What is the validation set used in this paper?\n- For the NAS problem, the architectural parameters must be guided by the validation set. So, does the validation set follows the same distribution as the training data set or the testing data set?\n- If the validation set and the testing set have the same distribution, is it still a meaningful OoD problem?\n\nQ2. \"For example, naively using data likelihood maximization as a proxy task would run into the issue pointed out by Nalisnick et al. (2019a), with models assigning higher likelihoods to OoD data\".\n- How can I see this point from the given experiments?\n- Is it better adding this into an ablation study?\n\nQ3. Except for WAIC, what other metrics can we consider? The authors should have a more comprehensive related work section, which includes discussion on this part.\n- Is it more meaningful to search a better metric than search architectures (which is just a standard applicaiton)?\n\nQ4. In Section 3.4:  \"CelebA (Liu et al.), CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and MNIST (LeCun)\" are used.\n- Could the authors explain more tasks and details on these data sets? Specifically, why they are OoD problems.\n- Based on the descriptions from the authors, these data sets seem to be standard ones.\n\nQ5. Variation is already considered in (1), I mean the second term there. \n- Why should we still consider an ensemble of models?\n- Is it better adding an ablation study on M about \" ROC and PR curve\"? (not Figure 13, 14 in the appendix)\n\nQ6. Is it better to have a comparison with standard NAS in the experiments?\n- While authors argue they are not applicable here, it is still good to demonstrate how not applicable they are.\n- If the validation set follows the same distribution as the testing set, gradient signals on architectural parameters perhaps can still be helpful.\n\nQ7. Can natural gradient descent be applied to (4)?\n- \"First, optimizing p(α), a probability over ..., each network’s optimal parameters would need to be individually\". The first problem is not a really challenging problem please have a check at \"Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search\".\n\nQ8. What is the searching time of the proposed method? How's it compared with recent NAS methods? e.g. DARTS (DARTS: Differentiable Architecture Search). "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors propose a neural architecture search (NAS) method to construct a Bayesian ensemble of deep learning models. This ensemble is then employed to detect out-of-distribution examples.\n\n\nThe authors propose to use a differentiable architecture search method which model the architectural parameters using a concrete distribution. This idea was originally proposed by Xie et al. (2019) but this work was not discussed. Similarly, the work by Chang et al. (2019) is not discussed. In my opinion the novelty with respect to NAS is the WAIC objective function and its application to out-of-distribution detection.\n\n\nThe idea of using ensemble to detect out-of-distribution examples is not new. The authors already refer to the works by Choi & Jang (2018) and Lakshminarayanan et al. (2017). I'd like to add MC-Dropout (Gal et al., 2016) to this list which was used e.g. to detect adversarial examples.\n\n\nThe experimental section is well-written and the proposed method is able to outperform the chosen baselines. Obvious baselines are missing. There is no experiment that proof that this way of searching architectures finds better suited ensembles. How about maximizing the cross-entropy and train the discovered architecture multiple times from scratch and use these models in an ensemble to detect out-of-distribution examples? How about any ensemble-based method mentioned in the previous paragraph?\n\n\nConcluding, the idea is nice but based on the current state of the paper it seems incremental. Experiments to back the usefulness of the described method are missing.\n\n\nSirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin: SNAS: stochastic neural architecture search. ICLR 2019\nJianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang, Chunhong Pan: Differentiable Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019)\nYarin Gal, Zoubin Ghahramani: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML 2016: 1050-1059"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper needs crucial, quick fixes.\n\nThis paper throws neural architecture search at the problem of out-of-distribution detection. Rather than searching over multi-class classifier architectures, and using the result for OOD detection, they instead search for generative model architectures.\nThe approach appears to work on some of their cherry-picked OOD datasets.\n\nI give this paper a 3 because they are quite possibly only showing their strongest results and not giving a complete picture, and there are numerous small errors throughout the paper.\nAfter a more thorough evaluation, the technique will likely not look strong on datasets such as CIFAR-10 vs CIFAR-100.\nThis is acceptable since they are comparing density estimators vs multi-class OOD detectors, the latter of which has been vastly superior for many years.\nTheir technique brings density estimators within striking distance of multi-class classifier perofrmance, but the paper must give a complete picture. If these issues are fixed, the paper is easily a 6 or an 8, depending on the results of currently unshown OOD datasets. It is acceptable if their technique gets and 55% AUROC for CIFAR-10 vs CIFAR-100 while multi-class classifiers with Outlier Exposure get 96%. It is OK because this technique appears superior to other generative models, which have lagged far behind. Currently the paper leaves the impression that this is not only leapfrogging past previous density estimators, but that it also is beating multi-class classifiers. This likely isn't true. We need to see performance on other OOD datasets.\nI am willing to increase my score from a 6 to an 8 even if the results are negative.\n\nThe CIFAR-10 model should have to detect OOD samples from CIFAR-100, Rademacher/Bernoulli noise, and Gaussian noise. In addition, they should train a model on CIFAR-100 since many OOD detection techniques exhibit much worse behavior on CIFAR-100, compared to CIFAR-10 or MNIST or SVHN.\n\nIn summary, I give this a 3 due to critical flaws, but if these are rectified, the paper will likely deserve a 6 or 8.\n\nMiscellaneous Points:\n\nPlease include CIFAR-10/100 code. Currently the code is for MNIST.\n\nThere are several errors in discussing related work.\n\n> Moreover, previous work on deep uncertainty quantification shows that a single model may not suffice to quantify uncertainty and detect OoD samples (Lakshminarayanan et al., 2017; Choi & Jang, 2018)\n\nEnsembles do not perform appreciably better at OOD detection under systematic OOD benchmarking when using multi-class classifiers. For generative models, ensembles can help. Ensembles mainly help multi-class classifier _calibration_ on in-distribution in-class data, but they have miniscule to nonexistent utility in classifier OOD detection. Please add appropriate qualifiers.\n\n> \"With no access to OoD data, unsupervised/self-supervised generative models which maximize the likelihood of in-distribution data become the primary tools for uncertainty quantification.\"\nI think they mean \"with access to labels.\" Multi-class classifiers are the most performant tool for OOD detection, and unsupervised generative models are around chance-levels. _Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty_ (NeurIPS) uses self-supervised learning for OOD detection and achieves good performance, but this work is not cited.\n\n> \"However, these models counter-intuitively assign high likelihoods to OoD data (Nalisnick et al., 2019a; Choi & Jang, 2018)\"\nThis was shown in previous work, such as Shafaei et al. 2019 and Hendrycks et al. 2019, and hence deserve mention.\n\n> \"Moreover, existing methods to calibrate model uncertainty estimates assume access to OoD data during training (Lee et al., 2018; Hendrycks et al., 2019). This is flawed when anomalous data is rare or not known ahead of time\"\nLee et al. does not assume access to OOD data during training; they synthesize their own. Hendrycks et al. does assume access to OOD data, but not OOD data seen during evaluation; in this way, they do not assume data is \"known ahead of time,\" which they reiterate throughout their paper. This sentence is painting with too broad a brush.\n\nThere is a smaller experimental problem. They compare to ODIN, but ODIN assumes access to OOD data from the test distribution. While this assumption is clearly questionable, their evaluation does not use the assumption they did, so their technique is not actually ODIN. I suggest just comparing against the Maximum Softmax Probability Baseline in Table 1 since the rest of this paper assumes OOD examples are not known ahead of time.\n\nUpdate: I have changed my score to an 8.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}