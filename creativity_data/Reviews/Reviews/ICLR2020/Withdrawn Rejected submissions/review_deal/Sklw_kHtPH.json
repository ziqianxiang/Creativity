{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "Overall I think there is a room for this paper to improve. I expect to see more comprehensive experiments for comparing Adam and AdamT. The authors applied AdamT to four different tasks and compared training and testing loss difference. Are the models used in these tasks state-of-the-art? I did not see a discussion about whether parameters (including learning rate) are tuned for the best results for each optimization method. Also it is hard to visualize the difference in a figure (e.g. Figure 3) to claim whether it is significant. I would suggest using more quantitative measures.\n\nMore details:\n- label all equations\n- second paragraph on page 3: \"are denoted as \\alpha and \\beta\" --> \\gamma and \\beta?\n- \"Equation 3.1\": which equation?\n- FIg. 1: Has the author tried different learning rates for Adam? I wonder whether the faster convergence speed by the proposed trend estimation can be achieved by a larger learning rate\n- Fig. 3: The testing loss for Adam+Dropout and AdamT+Dropout does not seem to be significant. Is the loss smoothed or averaged across different runs to remove randomness? Can the authors apply AdamT to the state-of-the-art model for CIFAR-10 and can still show advantages?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The experimental results of this paper are not convincing.  \ni) One should use much more powerful networks.\nii) Different variants of Adam should be considered, also with different learning rate schedules.\niii) One should study how newly introduced hyperparameters affect the results of AdamT. \nI find it hard to trust your conclusions like \"Empirical results demonstrate that our method, AdamT, works well in practice and constantly beats the baseline method Adam\" after inspecting Figure 3. Even much greater difference is performance can be due to a small change of hyperparameters, here you have several new hyperparameters. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposed a new type of Adam variant, namely AdamT. Instead of using  exponential weighted average, it used Holt’s linear method to compute the smoothed first order and second order momentum. The authors show the convergence of the proposed algorithm in convex regret bound setting and compare the experimental results with conventional Adam algorithm.\n\n- Contribution: the proposed method looks like a direct combination of Holt’s linear method (to replace exponential weighted averaging scheme) and vanilla Adam. The convergence is only established on the simple convex regret bound setting (following vanilla Adam) which is not quite interesting nowadays.\n\n- Clarity: the paper is easy to follow since the main idea is quite simple but lack of necessary demonstrative explanations to show the intuition of using Holt’s linear method, i.e., why Holt’s linear method is better than exponential averaging?\n\n- Soundness: There are a few things that may hurt the soundness of this paper. First, it seems that the proposed algorithm cannot guarantee the term v_t is strictly larger than 0 and therefore has to put an absolute value outside v_t. This is very strange as if v_t close to 0, the overall learning rate would be quite large and may cause non-convergence issue. Second, there is possible non-convergence issue of vanilla Adam algorithm, as pointed by Amsgrad paper. The fix by Amsgrad is to add an additional max operator for v_t term. Here the authors did not adopt this way, I wonder how the authors fix this? If not fixed, then the convergence proof is still flawed.\n\n- Experiments: The authors only conducted experiments on a few small size dataset/architecture and compared only with vanilla Adam algorithm. I would suggest the authors to add ResNet/DenseNet experiments on CIFAR10/IMAGENET and compare with other baselines mentioned in the paper in order to show the performance of the proposed algorithm. Furthermore, I would suggest the authors to also put the test error plot for better comparison as the current result looks far from optimal. For example, on CIFAR10, 40 epochs are far from enough to fully optimized the network and the training loss is largely above 0 currently.\n\nDetailed comments:\n\n- In proof of Lemma B.4, it is not immediately clear why the last step go though.\n"
        }
    ]
}