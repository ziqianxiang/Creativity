{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper addresses the problem of inferring dynamical systems from observations. It aims at calcium imaging data, but in fact only tests its proposed methods on two synthesized data sets.\n\nI am not really familiar with neuroscience, and found the the paper relatively clearly written, and interesting. The model seems to work, for the synthetic data. I appreciate the confluence of neuroscience and machine learning.\n\nI rate the paper Weak Reject, for two main reasons.\n\nFirst, the paper is, although readable on a high level, not accessible in detail to readers who are unfamiliar with VLAEs, like myself. I feel to be a good fit for a conference with a broad range of topics, some effort should be made to keep papers, at least to some degree, readable by the broad audience. Otherwise, a narrow conference, e.g. on calcium imaging, might be a better venue.\n\nSecond, the paper title promises \"an application to calcium imaging data\", but that application is not actually presented. I understand the two synthetic data sets are meant to assess the viability of the model for the task, but the title calls for an actual application to real data.\n\nSome more feedback:\n\nThere are a few things not immediately clear (which may or may not have to do with VLAEs). E.g. why does the model contain both an inference network and a generative network?\n\nThe conclusion had a few grammar errors, please grammar-check it."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This manuscript combines key ideas from two previous popular methods, Latent Factor Analysis for Dynamical Systems (LFADS) and DeepSpike to approach the problem of using LFADS on calcium image data in neuroscience.  The system is also extended to use a hierarchical structure based on the ladder network, motivated by theoretical ideas of how brain structures work.  Empirical results show that the system can potentially do better than a two-stage strategy where the popular package Oasis is first run on the calcium imaging data to extract spike trains, which is the typical import to LFADS.\n\nFirst, I disagree with the claim that OASIS+LFADS cannot reconstruct the calcium imaging trace.  In fact, OASIS is a deconvolutional model; if the spike train is predicted, then it is straightforward to predict the corresponding calcium trace based upon the autoregressive model using in OASIS.  Therefore, it is not clear that the Ladder LFADS that directly using fluorescence data is, in fact, better than this two stage procedure, nor does it really have a benefit on predicting all data types.  Can the authors please clarify if the proposed strategy works improves reconstruction of the calcium trace?\n\nSecond, I think that the experiments need significantly more exploration of uncertainty, reproducibility, and robustness.  The  R^2 values differ massively between methods; this needs to be explained more clearly as to why these large differences exist between relatively similar methods, and how these systems hold up to repeats.  Are the results in the table statistically significant?  What is the uncertainty? Why do these large differences exist?\n\nThe authors really should elaborate on why the ladder network is beneficial.  There is a nice motivation for using a hierarchical structure in the introduction, but it is unclear if there is real benefit to this structure and how it could be used to help answer scientific questions, as is ostensibly the goal of this manuscript.  Can the authors please describe how this modification will help answer a scientific question?\n\nFinally, the modeling details in this paper is not self-contained and are difficult to follow.  The model introduction section needs to be rewritten for clarity and actually introduce the methodology in a somewhat self-contained manner. A better description of LFADS and DeepSpike is necessary.\n\nWhile I think there are too many issues to support acceptance of this manuscript, especially on the significance and utility of the results and method, it would be exciting if the approach could actually be used to extract and utilize a broad variety of dendritic shapes."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper contributes to the space of hierarchical latent modeling with applications to neuroscience as follows:\n  1) A model combining variational ladder auto-encoders with RNNs is presented;\n  2) An inference method based on ELBO maximization is presented;\n  3) Results on synthetic different data sets show good performance although firing rates are reconstructed better by another approach on a key task (firing rate reconstruction).\n\nThe paper is fairly well written but it seems that the last paragraph and conclusion have been written with more haste (also a period is missing at the beginning of the last sentence of the first paragraph in the conclusion).\n\nI am a bit skeptical about whether to accept the paper.\nAlthough hierarchical latent modeling is interesting and producing models and training approaches that are modular and composable is valuable, it is not clear here that the model under consideration really provides strong improvements on the state of the art.\nFurthermore, it appears that the set of benchmarks being considered is narrowly focused on a subfield of neuroscience without providing clear wins on any task involving actual data.\nAs the theoretical contribution of the paper is rather weak and the empirical results are not very strong, I recommend a weak reject.\n\nI believe that a stronger experimental section could help the authors increase the impact of their paper"
        }
    ]
}