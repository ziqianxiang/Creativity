{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose to use numerical differentiation to approximate the Jacobian while estimating the parameters for a collection of Hidden Markov Models (HMMs). Two reviewers provided detailed and constructive comments, while unanimously rated weak rejection. Reviewer #1 likes the general idea of the work, and consider the contribution to be sound. However, he concerns the reproducibility of the work due to the niche database from e-commerce applications. Reviewer #2 concerns the poor presentation, especially section 3. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper is about a method for estimation of parameters of a collection of HMMs and the main contribution is the  combination of classical EM with a neural net. \n\n+ I like the idea of generally approximating gradients in more specific layers that are usually not easy to compute. They clearly formulate their message here and the technical parts of adapting work from prior literature looks solid. \n- At the same time I don’t know how much the computational constraints that are formulated are really constraints in practice. A couple of the related works that are cited don’t seem to have these issues. Naively O(T^2) doesn’t really sound like too much of a problem unless the sequences are really long. (In their practical example that isn’t applied to more general recommender systems, this doesn’t really seem to be the case. So it’s unclear )\n+ The technical contribution of the gradient estimation seems sound. While I didn’t really go through the proof of convergence, it at least looks rigorous. But I would have to spend a lot more time here to form a well informed opinion.\n+ The experiments on synthetic data are clear and further empirically motivate the authors’ work. \n\nThe paper is very well written in some parts and in other parts is difficult to understand.\n\n- I am not sure what the benefit of the “e-commerce” application is to the community. The dataset seems to be neither open-source, nor referenced and is insufficiently described. The comparison and conclusion with respect to e.g. GraphSage is hard to interpret as GraphSage is neither explained nor referenced properly (unless I missed it somehow). The authors repeatedly emphasize that their approach works well here but not in the “more general recommender systems scenario”. It would be good if the authors showed something that the rest of the community can directly relate to instead of something that is closed-source and by definition not reproducible. \n- I suppose “We apply DENNL in a clearly defined and fast-growing sector on OUR e-Commerce platform, namely Home Decoration” is technically a violation of the blind review if the authors were to now include a reference/link etc. to the dataset. On the other hand, if the dataset remains closed-source then blind review isn’t violated but results aren’t reproducible and hard to follow by the community with the current level of description. \n\nIf the authors can comment about the last few points above (especially about open dataset, reproducibility) then I will reconsider raising the rating. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose to use numerical differentiation (using random perturbation) to approximate the Jacobian of a particular update (essentially equations 5~7) which plays an important role in the estimation of HMMs.  To do so, the authors provide first a concise intro to HMM models (well known stuff in S2), presenting the iteration in detail, jump into their model (cryptically presented in my opinion in S3) and then propose a numerical approximation scheme using SPSA (building upon literature from the 90's, with Theo 1 being the main contribution), before moving onto experiments.\n\nI have found the paper poorly presented. Its general motivation stands on a shaky ground (as illustrated by the choice of words by the authors, see below). In terms of presentation, reminders on HMM are welcome, but unfortunately the authors have not kept the same standard for clarity of notation in Section 3, which makes reading and understanding what the authors are doing quite difficult. Not being a specialist in this field, I have struggled a bit to understand the model itself, and the practical motivation of adding a DNN in the middle of what is otherwise an unrolled back-and-forth between k steps of EM estimation of transition parameters and the addition of a DNN layer. Despite the complexity in the story, what the authors propose is essentially to apply a numerical approximation scheme for Jacobians of these EM updates instead of backprop. Since this is the crux of the contribution,  I feel some more numerical evidence that their approach works compared to baselines (e.g. Hinton et al 2018) is needed. For these reasons my assessment is a bit on the lower side. \n\n- parenthesis bug in b_j(... in Eq.4\n- in Eq. 5, index i appears both in numerator (as regular index) and denominator (as sum index)\n- what is \\Psi in Eq.8 ?\n- \"While HMM is arguably less prevalent in the era of deep learning\": odd way to start an intro. All papers cited date back to more than 2011, 2 in 2006, all the rest in 20th century. This is particularly strange given the few citations to papers >2015 in Section 5.\n- the observation sequence o_{t,1:T(u)} is \"weakly\" indexed by u (since T(u) is just a length)\n- What is the \\forall u notation below Eq. 9?\n- \"the number of nodes required to build the forward and backward\nprobability in the computation graph of an automatic differentiation engine is on the order of O(T^2). Empirically we found this leads to intractable computation cost.\" since this is critical, where is this empirical evidence? this seems to be a storage problem and cannot be a complexity issue. There are ways to mitigate this problem by only storing partially information, I feel this comparison would add a lot of value to the authors' claim.\n- Where is J^{(k)} defined (as opposed to \\hat{J}^{(k)}) defined in Eq.14?\n"
        }
    ]
}