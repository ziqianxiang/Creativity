{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper the authors aim to develop an intuition for why neural networks (NNs) generalize so well, despite often being overparametrized for the given problem.\n\nThe paper is well written exploring a very interesting issue. It provides a good discussion of why generalization is puzzling, related work on the attempts to improve generalization bounds for NNs, and qualitative discussion of possibly answers to this problem. Despite this, we feel that a much more experimentation should be done to address some of the hypothesis in the paper. Also, it would have been helpful, if some of the results and figures were presented much earlier in the paper. What follows are the pros and cons / discussion points / styling suggestions the authors could choose to address:\n\n[pros]\n-\tWell-written\n-\tA good survey of related literature and concepts\n-\tInteresting visualizations\n\n[cons / points / styling]\n-\tThe placement of Figure 1 seems arbitrary as you refer to it for the first time on Page 3. This would not be such a big problem; however, it divides the paragraph in two without need to. Could you perhaps place the figure and caption on the bottom of the page?\n-\tIs there a reference for “implicit regularization”?\n-\tOn page 4, it would have been helpful to provide examples of “linking the intuitive arguments back to theory”\n-\tIt would be helpful to provide more details on the filter-normalization scheme.\n-\tOn page 4, could you explain what you mean by “wide margin classification is replaced with an implicit regulation (regularization - ?) that promotes the closely related notion of ‘flatness’ “. How is “flatness” encouraged as a prior? Through the choice of an appropriate loss function?\n\nWould it be perhaps more suited for a longer format, e.g. Journal of Machine Leaning Research?\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nThe paper studies the generalization ability of neural networks. It is a timely topic with potentially large implications. The paper makes several interesting and intuitive claims about the link between optimization and generalization in deep networks. The central, intuitive, claim is that the optimization trajectory avoids multiple trivial bad minima because they have much lower volume in the overparametrized regime; this is because they correspond to low norm of the Hessian. However, this claim is not validated experimentally well enough. Therefore, my main issue with the paper is that the intuitions that paper proposes (\"Our goal here is to develop an intuitive understanding of neural network\") might be technically incorrect and misleading. On the other hand, many observations (e.g. in Figure 1) are not very novel and statements such as \"Miraculously, SGD always finds its way through a landscape full of bad minima\" are rather misleading. As such I am inclined towards rejecting the paper. \n\n*Detailed comments*\n\n1. The fact that there are many \"bad\" minima that corresponds to a network that output incorrect (random) labels on some held-out data is well known. See [1,2]; in particular, [2] have performed in Section 5 the same experiment (without the visualization). What could have been novel is examining proximity of the \"bad\" minima. However, this is examined only qualiatatively, and as such does not offer additional insights beyond what was explored by papers such as [1,2].\n\n2. There are implicit assumptions that have to be true if the claim is to be correct. However, these assumptions are not experimentally validated. More precisely:\n\n2a) Implicitely, the paper model the optimization process as a random search through all the minimas. Therefore, if there are only a few sharp minima, they will not be selected by this random process. However, why such an approximation of the learning dynamics is justified? For instance, [3,4] show that the optimization trajectory is rather selective for shape of the minima, and therefore is far from such a random sampling process.\n\n2b) It is implicitely assumed in the analysis that wide minima are good for generalization. However, it seems to me that recent evidence points towards the direction that width of the minima is a epiphenomenon, see the experimental data in [5,6,7,8]. Given how central to the paper is that wide minima are \"good\", these references should be discussed in more detail. These papers suggest that the link between curvature and generalization is only a correlation, and therefore the claim that the reason SGD avoids bad minima is because they are few, which in turns is due to their low norm of the Hessian, might not be correct. Additionally, I didn't understand the discussion on Dinh et al. Could you please clarify it?\n\n3. In Figure 5, how are the different minimizers found? Is it by starting from different random initializations? If this is using different random initializations, then this implicitely changes the effective learning rate (for the sake of the argument we can define it as learning_rate/||H(0)||, where H(0) is the Hessian at the initialization). In turn different learning rates tend to select for minima with different shape [3,4]. Hence, this plot could be measuring the effect of the learning rate, and the effect of curvature might be correlational in nature (see also 2b). Do you achieve the same result is the learning rate is normalized? \n\n4. The claim that flat minima correspond to wide margin, while not demonstrated yet convincingly in the literature, has been claimed by other papers. I think [9] (cited in the submission, but cited in another context) is particularly worth discussion in this context, given that it makes almost the same visualization in [9] in Fig.1.\n\n5. The experiments comparing a neural network to a linear model are not very illuminating. It is well known that there is a strong role of the architectural prior, compared to the implicit regularization induced by the choice of the optimizer. I think at least some prior work should be cited here.\n\n*References*\n\n[1] Arpit et al, A Closer Look at Memorization in Deep Networks, https://arxiv.org/abs/1706.05394\n[2] Wu et al, Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes, https://www.padl.ws/papers/Paper%2010.pdf\n[3] Jastrzebski, Kenton et al, Three Factors Influencing Minima in SGD, https://arxiv.org/abs/1711.04623\n[4] Wu et al, How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective, https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf\n[5] Golatkar et al, Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence, https://arxiv.org/abs/1905.13277\n[6] Yoshida et al, Spectral Norm Regularization for Improving the Generalizability of Deep Learning, https://arxiv.org/pdf/1705.10941.pdf\n[7] Jastrzebski et al, On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length, https://arxiv.org/abs/1807.05031\n[8] Guiroy et al, Towards Understanding Generalization in Gradient-Based Meta-Learning, https://arxiv.org/abs/1907"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper aims to provide an intuitive explanation for why neural networks generalize despite over-parameterization. They argue that neural networks generalize well because flat local minima generalize well since they are insensitive to small perturbations in the data, and they are wide so they tend to attract SGD more than sharp local minima. \n\nOverall, the paper is well-written and contains nice visualizations. However, I don't think the paper provides an explanation to why neural networks generalize for a few reasons:\n\n- First, I don't think the methodology used in the paper to identify bad local minima is a good sampling strategy of local minima in general. It could be the case that adding the second objective in Eq 2 will tend to generate sharp local minima  (since by construction it works well for a subset of the data and works poorly on another subset). In other words, the local minima identified by solving Eq 2 might be sharp by construction. Of course, these sharp minima do not generalize by construction as well. Using this as an explanation is not justifiable since there might be other bad minima that are flat but are never identified by solving Eq 2. \n\n- The explanation provided here does not explain why linear models do not generalize well (the ones that have been tried in Figure 2). Do linear models have sharp minima? It would be good to measure the curvature of the loss for both the optimal solution of the linear model and the local minima of neural networks. \n\n- The explanation that flat minima are wide and, hence, tend to attract SGD is incorrect since we don't know how many of the local minima are wide. What if the majority of local minima are sharp? There is no analysis in the paper of the ratio between the number of flat minima and sharp minima. \n\n- Regarding the discussion about high dimensionality, it is true that flat minima will be several order of magnitudes larger than sharp minima but there is also the generalization penalty of having higher dimensions. Are the generalization bounds that explain the value of flat minima dimension-independent? I don't think they are since the dimension often shows up indirectly in the bounds (such as in the condition number which tends to increase linearly with dimension in random matrices). \n\nIn summary, the paper is nice to read but I don't think it answers the question of why neural networks generalize in practice. \n\n"
        }
    ]
}