{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The basic idea of this paper is to relate attention over parameters for different possible dialogue skills, so as to compose different sets of skills. It is an interesting idea but the paper lacks motivation and clarity on the technical content. Too implementation details are discussed, making it hard to grasp the basic ideas and motivation for the work.\n\nI didn't understand the clear motivation for the problem setting considered in this paper. Why combine three datasets? What is real world scenario where it is desired to do both task oriented dialog and chit chat together? Are we arguing that dialog dataset from chit chat can help learn task driven dialogues and vice versa?\n\nSome of the baseline models considered in the paper are not really dialog models but neural networks employed in the proposed dialog modeling setting.\n\nI would like to see human evaluation for any dialog model. Otherwise, we are throwing arrows in the dark.\n\nI didn't understand a clear motivation for using SQL queries, and authors emphasize upon it at multiple places.\n\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "1. Summary: The authors of this paper proposed an attention over parameters based model for dialogue response generation. They assume that different tasks or topics require different models to generate responses. To model this idea, they design a model that can generate an attention vector based on historical dialogue history. This attention vector can then be used to get a combination of parameters that focus on the right topic/task to generate the next response.\n2. Overall assessment: This idea looks very interesting to me. It's simple but effective. However, this paper can still be improved in several aspects and it is not qualified yet to be published in a very competitive conference such as ICLR. I hope the authors can keep improving this paper and makes it a stronger publication.\n3. Comments:\n3.1 It seems not necessary to model the whole dialogue history. The next response usually mostly depends on the most recent user queries. Meanwhile, with a long sequence of words to encode, the model may not be very efficient. How well does the model work if only a subset of the dialogue history, such as the most recent one or two rounds, is modeled?\n3.2 Before formula (2), $Y_{:k-1} = \\{<SOS>, y_1, \\cdots, y_k \\}$, is it $y_k$ or $y_{k-1}$?\n3.3 How do the authors combine the three training datasets? This is very important information for other researchers to replicate this work.\n3.4 How to get the prior knowledge vector? It is given in the training data? What if we need to work on a dataset without such information? Will the model still work well? What if we remove the supervision over the attention vector in formula (8)? I suggest the authors conduct some experiments and analysis to answer these questions.\n3.5 From an engineering angle, how efficient is it to derive different decoder parameters for different instances and do the inference? As we know, having constant parameters at inference time enables us to do a lot of optimization and parallelizations.  How is the efficiency of the proposed model compared with existing ones?"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this work, the authors tackle the problem of building dialogue systems that can handle diverse domains and exhibit both chit-chat as well as goal-oriented behavior. The authors describe these different functionalities as different skills. Their work proposes a dialogue model inspired by the Mixture of Experts architecture (Jacobs et. al.), which learns different skills through distinct parameters which are then mixed together in a soft fashion through learned weights. On a combined dataset consisting of MultiWOZ (Budzianowski et. al.), In-Car Assistant (Eric et. al.), and the Persona Chat (Zhang et. al.), the proposed Attention Over Parameters (AoP) model outperforms other baselines on F1, BLEU, SQL query accuracy, and Booking statement accuracy. On a Persona chat evaluation, the AoP also outperforms baselines on a consistency metric calculated via a natural language inference scoring model trained from a dialogue NLI corpus (Sean et. al.)\n\nWhile the problem the authors were tackling is an important one (namely combining chit-chat and goal-oriented dialogue systems), I had several problems with the proposed approach. First off, it is still not clear to me how different the AoP model truly is from a mixture-of-experts model, and in particular the differences presented in Figure 1 are not well-explained. \n\nMoreover, the AoP model does not seem like a novel or significant enough contribution. From my understanding, it simply feels like trying to learn separate decoders per skill and then just softly combining them through an attention weight. Why is having explicit decoders strictly necessary, rather than playing with the multiheads of the self-attention operation?\n\nIn addition, I had issues with the evaluation presented. While the model seemed to outperform some simplistic baselines (transformer and seq2seq architectures) on automatic evaluation, there was no human evaluation done. It is difficult to assess how good the generated outputs are from automatic evaluation alone. \n\nFinally, the authors claim that the model learns fine-grained skill decomposition, and they demonstrate this by fixing their attention weights and showing that they can generate very different output sequences. The only example of this claim is a (potentially) cherry-picked example, and without any larger scale quantitative analysis (with some sort of human eval), it is difficult to know whether the model is actually learning skill decomposition in its separate decoders."
        }
    ]
}