{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper introduces a new method for 3d point cloud generation based upon auto encoders and GANs.\n\nTwo reviewers voted for accept and one reviewer for outright reject. Both authors and reviewers posted thorough responses. Based upon these it is judged best to not accept the paper in the present. The authors should take the feedback into account in a an updated version of the paper.\n\nRejection is recommended.  ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a new dual generation model for learning representation by combining GAN and Autoencoder for 3D point cloud data. The idea is to add a regularization term over the GAN loss, and the regularization term measures the distance between the output of the encoder of Autoencoder and that of GAN by two point cloud metrics: Earth Movers's distance (EMD) and Chamfer pseudo-distance (CD). The experiment shows better representation are learnt by this dual generation model. The paper is in a good writing, and easy to read.\n\n``My suggestions:\n\n1) Although this paper is about 3D-point cloud, the idea itself is also feasible for 2D data. In the experiment, a small section is on 2D image data. In order not to confuse readers, I would suggest writing this paper as a general framework on both 2D and 3D data. Is there any particular reason for this work focusing on 3D point cloud data?\n\n2) In 2D data, how to measure the point-cloud distance?still using EMD and CD?\n\n3) In Figure 2, are you trying to minimize the distance between intermedia representation z and \\bar{z} from encoder and GAN respectively as well? I do not find corresponding step in Algorithm 1.\n\n4) How is the lambda changing the results? The regularization is the main idea for this paper, and thus it would be interesting to see how the regularization trades off with the loss.\n\n5) how is EMD's performance compared with CD?"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary:\n\nThe paper focuses on designing a generative framework for 3-D point data clouds. These point clouds correspond to objects shapes in 3-dimensions. According to the paper, previous approaches for generating such 3-D point clouds involved autoencoder and GANs used separately. The authors propose a framework combining both autoencoder and GAN in a single network. The authors claim that part of the network learns effective latent space embedding for 3-d point clouds corresponding to different objects and thus the entire network is more effective in generating 3-d point cloud. Experimental results are presented to support the claims for efficient embeddings, and better generation of 3-d point clouds.\n\nDecision\n\nThe paper has some lacking in explanation. I am particularly interested in the answers to the following questions:\n1.\tThe encoder module (denoted as E in the paper) uses a loss function that apparently does not involve the encoder output (z in the paper). How the module weights can be updated this way is unclear.\n2.\tG1 is updated twice in each iteration, according to algorithm 1 in paper (once during update of ùúΩG1 and ùúΩG). How the generator G1‚Äôs output manages to converge to output z from encoder E is unclear.\n3.\tThe authors claim that the proposed model has both better performance and efficiency. Although experimental results are provided to support claim for better performance, none have been offered for efficiency claim. (calculation of EMD distance seems to be very expensive computationally)\n\nFeedback\n\nFor section 5.5, a quantitative analysis might make the claim for portability of the framework stronger. In paragraph 1 of introduction, disadvantage of 3d point cloud data can be better explained.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper propose a dual generator approach for learning to generate point clouds.  The proposed algorithm is a adversarial autoencoder with learned prior generation of point clouds. From this point of view, the novelty of the paper is limited, also, the authors failed to discuss all the related works properly. \n\n1. The problem of the point cloud generation of Achlioptas et al., (2018) is it can't generate arbitrarily many number of points. This issue has been discussed and addressed in those works \n\n* Li et al., Point Cloud GAN, ICLR workshop, 2019.\n* Yang et al., FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation, CVPR, 2018\n* Groueix et al., AtlasNet: A Papier-M√¢ch√© Approach to Learning 3D Surface Generation, CVPR 2018\n* Yang et al., PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows, CVPR 2019\n\nWith stronger prior in the generation process is even studied\n* Groueix et al., 3d-coded: 3d correspondences by deep deformation, ECCV, 2018\n* Li et al., LBS Autoencoder: Self-supervised Fitting of Articulated Meshes to Point Clouds, CVPR 2019\n\n2. The two generator approach is nothing new.  The first generator g_1 is the same as the \"learned prior\". There are many related works,  e.g.\n* Tomczak et al., VAE with a VampPrior, AISTATS 2018\n* Zhao et al., Adversarially Regularized Autoencoders, ICML 2018\n\n3. The paper is not well written, there are several wrong formats of citations and some typos, especially on math symbols, which hinders the reading.  (e.g. G is undefined in Algorithm 1)"
        }
    ]
}