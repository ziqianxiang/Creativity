{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a parallelization approach for speeding up scheduled sampling, and show significant improvement over the original.  The approach is simple and a clear improvement over vanilla schedule sampling.  However, the reviewers point out that there are more recent methods to compare against or combine with, and that the paper is a bit thin on content and could have addressed this.  The proposed approach may well combine well with newer techniques, but I tend to agree that this should be tested.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Scheduled Sampling aims to overcome the problems that come with the discrepancy between train and test\nwhen training sequence generation models using teacher-forcing. However one of the drawbacks scheduled sampling is that is it is hard to parallelize this is simply because some of the decode input tokens aren't known beforehand and need to be inferred i.e. loss of some tokens are dependent on some previous generations. \nAuthors introduce a method to perform parallel scheduled sampling by iteratively train using teacher forcing however after each iteration the gold references are mixed with the model predictions with a probability p, this allows parallelization as all decoder input tokens can be known beforehand. The final loss is calculated as before, conditioning on the final mixture.\n\nIn theorem 2.1 Authors proof that parallel scheduled sampling can converge to scheduled sampling when P is set to 1 and the number of iterations is larger than the sentence length. \n\nIn experiments over summarization, dialogue response generation and image generation authors show that the parallel scheduled sampling can achieve comparable performance to scheduled sampling with high-performance gains reaching 300 times faster and very comparable to teacher forcing. \n\npros:\nThe work presents a simple idea that is presented neatly with sufficient experiments. One of the outcomes of this method, that might not been stressed enough in the paper, is a neat way of doing scheduled sampling for the transformers architecture which wasn't straight forward before. \nCurrent proposals include some architecture modifications such as in Mihaylova et al. 2019 https://arxiv.org/pdf/1906.07651.pdf\n\ncons:\nThe only drawback I can find in this paper is it is lacking content. while the idea is interesting and supported by experiments, I find the content is slightly below the amount of content in average ICLR papers. \n\nThe explanation of the proposed scheduled sampling can be much simplified. The idea is quite simple however for example I find the pseudo code in Algorithm 2 is quite hard to grasp maybe due to some typos\n\nQuestions to authors: \n- We only see performance comparison in dialogue response generation experiments. There are other factors that can affect performance or make parallelization effective.  I wonder what are the performance gains of parallel scheduled sampling on normal scheduled sampling with respect to avg. number of tokens / sentence or batch size. \n\n- I had a hard time grasping Algorithm 2 although I understood the corresponding text could you please verify there aren't any typos. What it says is in the first iteration (k=1) scheduled sampling will be performed which doesn't make sense. \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a parallelized version of Scheduled Sampling, which is proposed initially to mitigate Exposure Bias. Exposure Bias occurs when sequence generation models are trained by teacher-forcing, where the t-th token, $y_t$, is forced to be correctly estimated given ground truths, $y_1, ..., y_{t-1}$. Scheduled Sampling randomly selects the previous sequence from generated one or ground truth. Since sequential Scheduled Sampling requires a long time to train, the authors propose a way to parallelize it without degrading the performance. Experimental results for dialog response generation, summarization, and image generation (as a grammar model on images) demonstrate that the proposed method is faster than the original Scheduled Sampling. Additionally, the performance gain is shown in comparison to baselines trained by teacher-forcing.\n\nMy first concern is that several papers are not cited, although they also address Exposure Bias:\n- Venkatraman et al., Improving multi-step prediction of learned time series models. AAAI, 2015.\n- Ranzato et al., Sequence Level Training with Recurrent Neural Networks. ICLR, 2016.\n- Zhang et al., Bridging the Gap between Training and Inference for Neural Machine Translation. ACL, 2019.\nThe second paper proposes MIXER, which is a method based on reinforcement learning. The training step can be performed without sequential sampling from the ground truth and predicted tokens. The third paper also randomly selects from a predicted sequence and the ground truth, but the selection is performed in a sentence-wise manner. \n\nRelated to the first concern, secondly, Scheduled Sampling itself is no longer the state-of-the-art method to solve Exposure Bias. It is unclear if the proposed method is competitive with the methods above. In addition, the empirical benefit in terms of training time seems to be small in comparison to them. The authors evaluated the proposed method using diverse tasks. Although it is good to see if Parallelized Scheduled Sampling is versatile, lack of comparisons to the existing methods except for the original Scheduled Sampling remains the superiority of Parallelized Scheduled Sampling unclear.\n\nFinally, the technical contribution is not so fascinating. Although parallelizing the original Scheduled Sampling is practically essential, the proposed solution seems to be a straightforward way. Theoretical arguments and proof also look obvious.\n\nNow I lean to reject this paper because of the concerns above. Since Exposure Bias is a fundamental problem for sequence generation problems as described in this paper, I would like the authors to revise their paper and submit it to another conference."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a technique for scheduled sampling that can be parallelized. At a high level, the method is to do teacher forcing, followed by passes of sampling from the model. During each round, the output of each pass are mixed with some probability. The time complexity then scales with the number of passes as opposed to the length of the sequence. The authors show results on dialogue generation, image generation, and machine translation. The proposed method generally obtains performance on par with scheduled sampling, but is much faster. The paper is well-written and the method clearly explained. I am not super excited about the technical contribution of this work, so my score is for weak acceptance.\n\nSome feedback\n- It would be nice to put the task in the caption for Table 1\n- Figure 1 is underwhelming and doesn't really help illustrate how the method works, though its caption does.\n- Because the main selling point of this is efficiency, I would like to see how it compares to work on non-autoregressive sequence generation (e.g. https://arxiv.org/pdf/1902.01370.pdf), which should probably be in the related works as well."
        }
    ]
}