{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this paper, a simple decomposition for transformer models is proposed to speed up the computation for a specific type of problems, where the input has paired, such as a question and an answer for QA tasks. The basic idea is to encode the question and the answer with self-attention alone, and then apply the co-attention on top of them, so that the complexity is reduced from O(p+q)^2 to O(p^2+q^2). Such a decomposed transformer is trained by distilling from a full transformer, where the knowledge distillation loss, layerwise similarity loss and task loss are adopted as the objective functions. Empirical studies show that the proposed method can be up to 3.5 times faster than the full transformer (BERT) model, while only losing a small amount of accuracy.\n\n1. The paper lacks a large body of related and representative works in machine reading comprehension, where such a decomposition is widely used, such as BIDAF (Seo et al.), QANet (Yu et al.), R-Net (Wang et al.) among many others. If those works are properly acknowledged, the novelty in this paper will become slim.\n\n2. The improvement from O(p+q)^2 to O(p^2+q^2) is negligible. On one hand, they are of the same order, so there is no theoretical contribution. On the other, as the author acknowledges, the method can only work on pair-input tasks like QA. However, most of the questions in QA are quite short, so the terms related to q can be ignored and the only bottleneck is p^2, making the difference very minor.\n\n3. Since the method is essentially a distillation, the full transformer model is still needed for supervision. That is to say, it cannot work without the original large model.\n\n4. There are many ways for distillation. For example, one can just use smaller hidden size or a shallower network, or their combinations. Those are straightforward baselines but they are not mentioned or compared with in the paper. \n\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a way of reducing the computation of pre-trained Transformer models by splitting the lower layers into two parts: one corresponding to a question and one corresponding a passage. Auxiliary losses are added during the fine-tuning stage to recover the lost performance due to the split. While this method alone cannot bring more than 2x speed-up, the authors show that caching of question part when it has to be compared to multiple passages can bring additional saving in computation time. \n\nThe paper is well written and it was easy to understand. It has thorough experiments showing the benefit of the proposed method.\n\nHowever, I am doubtful of the impact of this paper. The original problem domain is quite limited and it’s not clear if the method would be useful in other scenarios. The method is only relevant when one is using a pre-trained transformer model on two part inputs, and also one part is often have to be compared to multiple other parts. I think the paper can be made stronger by showing the method can be generalized and used in more diverse scenarios."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper, the authors proposed an opinion that it may not be necessary to apply the sequence-wide self-attention over all layers in the BERT model. They proposed a decomposition to a pre-trained Transformer that allows the lower layers to process segments of the input independently, enabling parallelism and caching. The experimental results show that decomposition enables faster inference, significant memory reduction while retaining most of the original performance. \n\nI think the study problem in this paper is very meaningful. However, this paper has some syntax errors. I hope the author can read their paper carefully and correct grammatical mistakes. \n\nQuestions:\nIn Figure 2, the authors show the average variance of passage representations when paired with different questions at different layers. But I don’t understand why the variance of the lower layer is smaller than, the higher one. Would the authors like to provide some explanations?\nThe authors mentioned that the information loss in the lower layers can be potentially compensated for by the higher layers. Would the authors like to provide some theoretical proof and experimental proof?\nThe authors used the Bayesian Optimization to tune the hyperparameters \\gamma, \\alpha, and \\beta, however, they don’t post the results of the \\gamma, \\alpha, and \\beta in the experimental results. \nIn section 4.2, the authors said the search range is [0.1, 2.0] for the 3 hyper-parameters, which confuses me?\nIn Table 1, would the authors like to explain why the model performs differently in different datasets? Why in the QQP, the model achieves 2.0x Inference Speedup, while it achieves higher Inference Speedup in other datasets?"
        }
    ]
}