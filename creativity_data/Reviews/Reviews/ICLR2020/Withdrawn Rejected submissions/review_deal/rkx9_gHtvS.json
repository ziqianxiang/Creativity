{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an interactive approach to enable the machine inquire missing information before it finally makes classification over users’ incomplete queries. Based on their assumption of independence between interaction turns, they use a simple approach to croudsource the initial user queries and interactive qa pairs for two existing datasets (i.e. FAQ document suggestion and bird species identification). To model the interaction, they employ information gain to select question at each turn, and proposes to use a policy controller to decide when to stop the interaction. Their experiments on the two datasets show that their approach outperforms several non-interactive and interactive baselines by large margin.\n\nStrengths:\n1. The interactive approach to do classification is well-motivated and looks interesting in real-world settings where computers can interact with users.\n2. Good formulation of the task and detailed mathematical derivations from the original problem to their final model based on their assumptions.\n3. Great improvement over the baseline methods, and promising human evaluation.\n\nWeaknesses:\n1. I am not sure whether the assumption of independence between turns in the interaction is valid or not. Basically, following this assumption, we can ignore the order of the interactions, and the problem can be reduced to finding the most supportive attributes of the label and then do classification.\n2. The FAQ and BIRD datasets using in this paper are not very popular datasets in NLP and CV, and both of them are not originally designed for interactive classification. So, I am wondering why the author chose these datasets. As the author mentioned, there are many existing works on interactive classification. Why not just use their datasets?\n3. The NLP model used in this paper is too simple. They just use recurrent neural network (SRU) without attention or state-of-the-art encoders.\n4. The author claims great improvement in the introduction over non-interaction baseline. However, the non-interaction baseline doesn’t make sense for their task. Because they encourage workers to provide incomplete information intentionally during the data collection.\n5. The interactive baselines they compared might be too easy, or old. Maybe the author should consider better retrieval models. I also don’t understand why is the STATIC INTERACT baseline not conditioned on the initial query."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a mechanism for interactive search, where users issue an initial query, then the system decides whether to ask further questions (and what questions to ask) or to give a search result. The search is done entirely in the text space. The method consists of a RL-based controller (which chooses whether to ask a question or give a result) and an information-theoretic criterion (for which question to ask). The proposed method is compared to several no-interaction baselines, and two basic interaction methods (static interaction which does not use the initial query, and randomly chosen questions). The method outperforms the baselines, in both a simulated and live scenario. Unfortunately, information-theoretic search is not new (e.g. see Ferecatu and Geman, ICCV 2007, \"Interactive search for image categories by mental matching\"; Kovashka and Grauman, ICCV 2013, \"Attribute Pivots for Guiding Relevance Feedback in Image Search\", etc.) This means the method isn't particularly novel, and importantly, the proposed work needs to be compared to stronger baselines from recent literature."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors show that a query classification system can be improved by providing additional information to the system. In particular, additional ‘tags’ are converted to question/answer pairs and the system can choose to ‘ask’ these questions to obtain the ‘answers’ before finalizing its decision. They use a deterministic function (based on information gain) to choose the next question, and train a reinforcement-learned controller to either ask another question or stop. The system is trained without actual interaction, relying on crowdsourced annotation tasks.\n\nThe authors apply their approach to two different domains: FAQ document suggestion and bird species identification, which seem to be well suited to test their approach. In both cases the authors use a heuristic approach to turn a collection of tags into questions for which annotators then provide answers.\n\nThe paper is generally well written and organized, and the few minor typos I noticed can be fixed with a spell-checker.\n\n\nI believe the paper is an interesting contribution and I’m leaning towards acceptance.\n\n\nI think that the general question the authors are trying to answer is very interesting: can a system be improved by obtaining additional information from the user. The potential need to disambiguate the user query, and potential underspecification motivate this well.\n\nHowever, the way they set this up the task and created the data is limited though: the system can not dynamically identify an information need and try to fulfill it. It can only add knowledge about pre-defined ‘tags’ to the decision making process. For example, it is not clear to me if a similar improvement could be achieved by identifying a number of relevant tags beforehand and providing these in a single step. So while I am not entirely convinced that the annotation task is a close enough approximation of the real task the authors are trying to solve, I still think it is an interesting result. It might be an interesting additional baseline to train a classifier that has both the query and the relevant tags as input.\n\nIn addition, ideally I would have liked to see a more thorough analysis of the results. In particular, I would have liked to understand better how the additional information contributes to the classification. \n\nI would also like to better understand what value the policy controller provides over choosing a fixed number of questions. For example, how does the performance of the system differ for different numbers of fixed steps? What is the variance in the number of steps the controller chooses?\n\nMoreover, the description of the neural model is very short and could be expanded.\n\nIt would have been interesting to see an analysis of the kind of question the current approach chooses. In addition to that, other heuristics for choosing questions (e.g. increasing diversity) could have been evaluated in addition to information gain.\n\nLastly, the authors say that the code, data and experimental setup will be made public. Are there already concrete plans for doing this and when can it be expected?\n"
        }
    ]
}