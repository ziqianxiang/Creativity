{
    "Decision": {
        "decision": "Reject",
        "comment": "Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.  Indeed, if all reviewers and the area chair do not interpret the paper well, the authors' best response would be to rewrite the papers rather than disagree with all reviewers.\n\nIn the area chair's opinion, the current form the paper does not merit publication.  The authors are advised to address the reviewers' concerns, rework the paper, and submit to a conference again.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper argues that Artificial Neural Network (ANN) lack in biological plausibility because of the back-propagation process. Therefore, the authors provide an alternative approach, named neural net evolution (NNE) that follows evolutionary theory. This approach uses a large number of genotypes (in the form of vector with binary logits) that will evolve overtime during training. It does not require to calculate the gradient explicitly. The authors have conducted some experiments on MNIST using ANN with only one hidden layer. The experimental results show that the NNE can learn the classification task reasonably well considering that no explicit back propagation is used. \n\nI think overall the motivation to combine ANN with evolutionary theory is very interesting. The reviewer is not very familiar with evolutionary theory. So I judge this paper in the perspective of machine learning, from which I think the current approach is a week variant of back-propagation that still relies on gradient (see detailed comments below). Based on this, I give my rating. \n\nThe approach is formulated as NNE_x(y) = (x^T)*(W^T)*y. In traditional linear regression, W is the weight to be learnt. In this paper's formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d. with certain probabilities. The parameters to be optimized is x, which is named as a genotype that is viewed as a vector x \\in {0, 1}^n. So first of all, as W is fixed so the formulation is very similar to a traditional linear regression with an additional linear transform. The difference is that x is a binary vector with probabilities. These probabilities are optimized over time. \n\nFrom the Equations 1), 2) and 3), the probabilities are updated in a way to minimize the loss. This is kind of similar to back-propagation. Then the probabilities are updated and thus x is changed as well. In my understanding, this is still gradient-based optimization. I do not see it fundamental different to back-propagation. This is my main concern about this work. \n\nI did not check the details of Theorem 1. Could the authors please comment what is the purpose of Theorem 1 before proving it? This part is unclear to me in this paper. \n\nOne more question, for the W matrix,  the authors choice beta = 0.0025 in the experiment. Is there any particular reason for this choice? Or does it matter what value to choice as it is fixed anyway? \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this paper the authors propose a method for training neural networks using evolutionary methods. The aim of developing this method is to provide a biological alternative to back-propagation. The authors prove that their method converges and with high probability succeeds in learning linear classification problems. Another method is also proposed which is linked to dopaminergic neurons.\n\nIn terms of presentation, the paper is generally clear and well-written. I was not able to assess the importance of the theoretical contributions of the work as my research is not in this area, so my comments are limited to the other aspects.\n\nWith regard to the biological plausibility of the method, it is unclear to me how the evolutionary method proposed here can enable learning in typical scenarios such as conditioning experiments in animals. The learning processes in animals typically occurs in short time spans (for example a few training sessions for conditioning to stimuli predicting food/no food) and therefore I don’t find it plausible to suggest evolutionary methods across generations are behind such forms of learning. Perhaps what the authors have in mind applies more to other forms of behaviour such as innate and involuntary responses in animals formed across generations rather than ongoing updates in synaptic plasticity as an animal adjusts its behaviour using environmental feedback. But then in this case the biological plausibility of the method seems fairly limited and not really an alternative to methods such as back-propagation.\n\nThe other biological aspect of the proposed work is the connection to dopamine and using the sign of gradients for updating the weights. I think connecting the current learning rule to the activity of dopamine neurons requires quantitative comparisons with experimental data, otherwise although I agree that the method is biologically inspired, but whether it is biologically plausible is not clear.\n\nBased on the above comments, I think the work will benefit from further developments before being ready for publication."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "First of all, I must confess that my knowledge is quite limited to read this paper. Perhap the authors present something that I can not catch up at the present.\n\nI conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. \nThe paper is somewhat cumbersome in the introduction that makes the reader (here is myself) can not understand the main idea. At first the authors introduce about evolution in genetics and genomics that is a bit different from what I known. Then the authors claim that they can show that the brain circuits can evolved in their model.\nThere are so many mistake and/or typos in sentences and in mathematical formulation. These make me can not finish reading the paper. I have to stop reading at the end of Section 2.\nHere are my concerns and questions:\n1). What is \"STDP\" in the 2nd papragraph in Introduction?\n2). in the 3rd papragraph in Introduction: \"Suppose that the brain circuitry for a particular classiﬁcation task, such as “food/not food”,is encoded in the animal’s genes, assuming each gene to have two alleles 0 and 1\". This is realistics, the allels in animal is 0 1 or 2 if encoded.\n3). The authors use the words \"gene, genotype, phenotype\" in a special way that is different to what I known in genomics (in GWAS).\n4).  in the 3rd papragraph in Introduction: \"At each generation, a gene is an independent binary variable with ﬁxed probability of 1\". What do you mean by fixed probability of 1? I can not understand in anysense that I know.\n5). In Section 2, What is n? the authors start the mathematical formulation but I can not find out what is n? Is it the sample size?\n6). In Section 2, paragraph 2, you define y~ \\mathcal{D}, BUT then in all formulas later you denote y ~ D. What is D??? I can not understand.\n7).  In Section 2, paragraph 2, you define a label of y as \\ell(y), BUT then in the 1st sentence of the 3rd paragraph in Section 2 you wrote    L(NNE_x(t), l(y)). What is l(.) here ???\n8). In the equations (1) and (2), what is p^t  ???  You have NOT defined it.\n9). Right after equations (1) and (2), What is \\epsilon ???? Can NOt understand.\n10). The sentence right after the equation (3): \"This is the standard update rule in population genetics under the weak selection assumption.\" This is NOT trivial to me, and even the machine learning comunity, we do not know this rule, it is not obvious. PLEASE provide exact reference.\n11). the first equation in the PROOF of LEMMA 1 wrong  \\mathcal{L} (p^t)  should be equal to E_{x~p^t}  NOT p.\n12). in the PROOF of Theorem 1, I can NOT find out where \\gamma has been defined?\n13). What is d  in Theorem 2? is it the dimension? I make too many guesses !!!\n"
        }
    ]
}