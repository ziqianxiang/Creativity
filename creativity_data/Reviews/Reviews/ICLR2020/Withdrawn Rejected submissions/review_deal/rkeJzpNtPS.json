{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Interesting paper! The paper is well organized, well written and easy to read.\nThe main idea proposed in the paper is to modify the activation function $\\sigma(x)$ in a neural network to $\\sigma(ax)$ where $a$ is a learnable parameter. Every neuron could have it's own $a$, or $a$ could be shared across all neurons in a layer or could be shared across all neurons in a network giving rise to different versions. In addition to this, a new term is added to the cost function which favors larger values of $a$. They demonstrate through empirical experiments that this helps for some particular problems in physics and in general for image recognition tasks in datasets with low resolution images. In addition to this, they theoretically argue that, introducing this new parameterization makes gradient descent not attracted to local minima. \n\n1. Results in Fig 4 are evaluated on the same data as training and similarly for Fig 5 and 6, it is just the train loss that is reported. Does this coincide with an increase in generalization performance? Can you report your train and test accuracy for all these cases?\n\n2. How do the Layer and Neuron based activations work when you've convolutional filters? For the neuron one, do you have as many parameters as the number of pixels for every channel? For the layer case do you share only across channel or do you share across the whole layer?\n\n3. Does your conclusions in Fig 5 and 6 hold if you use deeper networks (Resnet50) ? Does it hold on ImageNet (i.e, on larger resolution images)?\n\n4. Why is parameterizing the slope term as $na$ necessary? Does it have anything to do with the way your slope recovery term is defined? In addition to this, do you have projection steps to ensure that $a$ is always positive?\n\nWhile I can convince myself that having an extra degree of freedom in terms of slope may help with activations functions like tanh and sigmoid, I'm extremely surprised that the authors observe improvements in case of ReLU as well. Can the authors offer some reasoning/explanation for understanding this? Also, is it just fitting the training data better or are we seeing better generalization performance (same as point 1). \n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "In this paper, two approaches of locally adaptive activation functions are proposed to improve the training of deep and physical-informed neural networks. The so-called adaptation is realized by multiplying the output of the previous layer by a learnable parameter before passing to the activation function. If the parameter is a scalar and applied to the whole layer, it is called layer-wise locally adaptive activation functions (L-LAAF); if the parameter is a scalar and each neuron has a unique one, it is called neuron-wise locally adaptive activation functions (N-LAAF). The motivation of the adding this adaptive parameter is to increase the slope of of the activation function, so that the convergence is faster.\n\nI am not convinced why this approach will work.\n\nFirstly, the parameter a^k will be absorbed into the w^k and b^k, making it equivalent to learning a scaled version of w^k and b^k. For example, if L-LAAF is applied to eq (1) and we redefine w^k := a^kw^k and b^k := a^kb^k, then the new form is the same of the original one. The same rationale applies to N-LAAF.\n\nSecondly, I don't understand why adding this parameter will increase the slope of the activation function, unless the authors impose the parameter to be a big positive number. But we cannot make this assumption as the parameter is trainable rather than a constant. To keep or enlarge the magnitude of the gradient, there are more principled algorithms such as normalized gradient (https://arxiv.org/abs/1707.04822) and signSGD (https://arxiv.org/abs/1802.04434), but the author fails to mention or compare. \n\nTherefore, I am afraid the proposed method is equivalent to doing nothing to the activation, as the added parameter will be just absorbed to other learnable parameters.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper introduces a parameter to scale the input to an activation function and a loss function to encourage the scaling parameter to be large. The authors test it to learn a discontinuous function as well as several image datasets.\n\nThe concept of learnable activation functions is not new and this work is fairly incremental. However, for the image dataset results it seems the authors are using ReLU for their activation functions. Having a scaling parameter before the input to ReLU and encouraging that scaling parameter to be large is similar to increasing the learning rate. There are no experiments to verify whether or not this is the case. Also, the authors only report loss and not accuracy on these datasets."
        }
    ]
}