{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper demonstrated that the simple k-NN baseline approach provides an effective defense against clean-label poisoning attacks with minimal degradation in model performance. The k-NN defense mechanism identifies virtually all poisons from two state-of-the-art clean label data poisoning attacks, while only filtering a small percentage of non-poisons. The k-NN defense outperforms other simple baselines against the existing attacks\n\n1.  I am not at all familiar with clean-label attack methods, therefore no knowledge of relevant related works. \nThese papers seem to follow a pattern that : one propose an attack, and another paper proposes a defense. They do point out the potential issues on non-natural samples, however, only having results from CIFAR-10 certainly weakened the significance or the contribution of this paper. \n\n2. It does not provide good intuitions or theoretical reasons why such attacks exist and why a simple kNN based method can defend against those attacks. \n\n3. I hope the authors can provide a more thorough survey of related works. \n\n4. The experiments seems quite messy, with unfinished analysis and ill-positioned figures. \n\n5. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose a simple and effective method to address the problem of targeted clean-label poisoning where the adversary injects a few samples into the training data thus causing the deployed model to misclassify a particular test sample during inference. Experiments demonstrate that the proposed defenses are able to detect over 99% of poisoning examples. The paper is clearly written by addressing an important problem but I still have several concerns:\n1.\tIn general, the Knn is sensitive to the parameter k, which is not robust in some cases. The authors are expected to clarify how to set the proper parameters especially for the complex tasks. \n2.\tThe authors are expected to make more comprehensive evaluations to demonstrate their advantages compared with alternative methods, and the effectiveness to other poisoning attacks. \n3.\tIt is supposed that the performance highly relies on the feature extraction and the authors are expected to further justify how the performance would be if the features are not reliable due to the adversarial attacks. \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposed a strong baseline defense, the K-nearest neighbor (KNN) based data filtering against clean-label poisoning attack, which aims to add small noises to (a subset of) training data but maintain the original training labels such that some targeted images at inference will be misclassified. The authors provide geometric interpretations of why K-NN is a good defense and also compare the defense performance against several baselines, including L2 Defense, Robust Feature Extractor, One-Class SVM, and Random Defense. The proposed defense outperforms the compared defense methods by a large factor.\n\nIn general, this paper is well-motivated. The defense is simple yet quite effective. However, based on the current presentation, I have the following concerns.\n\n1. The scope of this paper can be limited since it only focuses on defending against one type of attack (clean-label poisoning attack) in the family of data poisoning attacks. It will be great if the authors can further motivate how the KNN defense can be used in a broader context.\n\n2. It is good to see simple methods like KNN turns out to be a strong defense in this context. However, I am wondering how easy it is to bypass this defense. For example, when crafting clean-label poisoning attack, it should be possible to consider a more insidious setting that the added perturbations will make the K-nearest neighbors of the perturbed training samples belong to the target class label (similar to how targeted adversarial examples work). In this case, this defense may be in vain. There are some prior works that study how robust KNN algorithms are when used as a defense, such as \"On the Robustness of Deep K-Nearest Neighbors\" by Sitawarin and Wagner. Since the proposed defense is \"simple\" enough so that the attacker may use it as a baseline defense to propose stronger attacks, I believe it's necessary to verify how \"easy\" it is to be bypassed. Otherwise, technical insights can be limited.\n\n3. How does the proposed defense compare to the data filtering approach used in\"Spectral Signatures in Backdoor Attacks\" by Tran et al?  Can it be also used as a baseline defense?\n "
        }
    ]
}