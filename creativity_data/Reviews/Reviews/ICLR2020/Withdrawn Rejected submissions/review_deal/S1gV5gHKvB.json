{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "In this paper, the authors present a method and analysis for generating an artificial datatset to fool pre-trained language models into classifying the sentences, which are random strings of words, as being grammatical. They test their method on BERT and RoBERTa. They take (Ro)BERT(a) that is fine-tuned on CoLA and use it to select \"sentences\" that fool the model. The sentences they use are random strings of words. The method is evolutionary: the top 100 of the first n generations of sentences if selected to spawn off 10 new, slightly altered, sentences, where the sentences are ranked according to the probability of them being classified as grammatical. \n\nI would recommend this paper be rejected. Here are some of the issues with this paper,\n- The results section provides no hard numbers. The paper only descriptions of the results like \"obtaining a result similar to the original score\" and \"the models are always fooled.\" This alone is a disqualifying factor\n- The presented algorithm adversarially selects random strings of words that fool (Ro)BERT(a). From my reading, it seems like this is used as partial evidence that (Ro)BERT(a) is incapable of distinguishing between grammatical sentences and random sentences. The authors claim that since the sentences that are generated from the algorithm are ungrammatical strings of random words, the models can not distinguish \"correct sentences and selected random collections of words\" (section 6.2). I don't think this is true. The algorithm is explicitly designed to pick out these random strings. The fact that an algorithm that does random word selection and substitution doesn't land on grammatically sound sentences is not evidence that the models used in loop are failing at learning concepts of grammaticality. (I'd like to be clear here and say I'm not claiming that these models fully understand grammaticality, I just don't believe this is good evidence that they don't.)\n- The sentences in Table 2 don't appear grammatical to me, though the authors say that they are. Example from table 2, \"NY restaurant student Shelly dove lack.\" These results are used to claim that sometimes the algorithm does converge on grammatical sentences, these conclusion seems unfounded to me. \n- In section 6.4 the authors claim that if a BERT model fine-tuned with some of the algorithm's generated sentences is not \"fooled,\" i.e. if the algorithm with this model converges on grammatical sentences, then the \"dataset was not built to deal with random sentences.\" I think that this is trivially true. CoLA has no random sentences in it, so random sentences are naturally outside the dataset distribution.\n- In section 6.5.2 it is unclear if there is a held out test set of \"fooling\" sentences.\n\nThe paper concludes that RoBERTa is more robust to adversarial attacks than BERT. This would be a neat conclusion but I'm afraid there is no concrete evidence of that presented in the paper. The biggest issue with the paper is the lack of empirical results, we are given no numbers to demonstrate performance making the results impossible to interpret.\n\nSmaller issues:\n- Figure 1 is unnecessary as it doesn't offer additional clarification of the algorithm.\n- There is some odd phrasing through the paper. I'd recommend getting some edits from another native English speaker to help polish the paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This is an analysis paper that aims to find weaknesses in pretrained language models by constructing nonsense strings of words that will nonetheless be classified as \"real sentences\" according the model. The paper focuses specifically on BERT and RoBERTa, two related but different pretrained LMs. The authors use an evolutionary algorithm to generate the adversarial examples. They show a handful of examples of nonsense strings that receive high acceptability scores according to BERT and RoBERTa, and report distributions of the models' scores for various sets of generated adversarial examples. The authors do some manual labeling to address the fact that the evolutionary algo might end up generating sentences that are in fact valid, though the details on how they do this manual labeling and what the results are are scant.\n\nEvaluation: I don't think the paper is above the bar for ICLR as is. I think the idea of finding such vulnerabilities in pretrained models is interesting, but this paper does not offer much insight on the topic. The authors' purported goal is to \"discover if and why it is possible to fool [language] models\" like BERT and RoBERTa. I think prior work has established the \"if\"--we know that BERT and similar models can be made to fail using various types of input perturbations (see [1,2,3] below). The particular type of weakness explored here--misclassifying random input as \"acceptable\"--has not been done, as far as I know, but I don't think the results are particularly surprising as they are consistent with what we know pretty well: these models are thrown off by inputs that are outside the distribution of their training data. Thus, I think the paper needs to offer a bit more on the \"why\" part of the question. There is no clear quantitative or even systematic qualitative evaluation of the types of examples that are leading the models to fail, and thus its hard to learn much from the results. More specific questions for the authors below.\n\n* You say you manually split the generated sentences into grammatical/not grammatical--how did you do this? What was the criteria and agreement? Looking at your examples of grammatical sentences, I don't agree that they are grammatical.\n* In Figure 2 vs. 3 you show distributions of scores for cases when the genetic algo converged to generating grammatical vs. ungrammatical sentences. What does this mean? Does this mean that *all* of the sentences that the genetic algo converged to were grammatical? Or just some? Also, you make claims about the bimodality of the distributions in these figures--is it actually the case that all the high-scoring sentences were grammatical and all the low-scoring sentences were not? How did you verify this?\n* Can you comment on the fact that even when the genetic algo. ends up producing \"good\" (i.e. ungrammatical) adversarial examples, the distribution is quite flat. This does not convince me that there is some reliable vulnerability in the LM. I'd expect that if there was some reliable vulnerability, the genetic algo should be able to exploit it and produce a set of examples such that nearly all of them receive high scores.\n* I am concerned about the lack of controllability of the genetic algo. I think it is hard to use this to make any useful general claims. What can we conclude from the fact that a run, or even a handful of runs, failed to find examples that fooled the model? This does not mean the model is unfoolable. Similarly, if the examples found fool BERT and not RoBERTa, that doesn't mean BERT is more foolable in general, only that we happened to find some examples in which that was the case. I think there needs to be more clarity about assumptions and experimental design to help us make useful conclusions on the basis of the genetic algo's output.\n* You use the word \"universal\" to talk about properties that are true for both BERT and RoBERTa. However, it is worth emphasizing that these models are actually remarkably similar in architecture and thus may behave more similarly to one another than two completely unrelated LMs might. Similarly, I appreciate that you run some experiments using different initializations at finetuning, but we are limited in what we can conclude from these experiments too, since all share the same pretrained BERT base and thus are very likely to share similar vulnerabilities.\n* Worth having the paper proofread by a native speaker, some wording is a bit unnatural\n\n[1] https://www.aclweb.org/anthology/P19-1334.pdf\n[2] https://cocolab.stanford.edu/papers/DasguptaEtAl2018-Cogsci.pdf\n[3] https://www.aclweb.org/anthology/P18-2103.pdf\n \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nSummary:\nThe authors propose using evolutionary algorithm to generate sentences in order to fool pretrained language models.\n\nDecision:\nOverall, I found the importance of finding robish sentences is not convincing. Hence, I recommend for a reject.\n\n\nSupporting argument:\n1. Compared to https://arxiv.org/abs/1907.11932, I found the use of evolutionary approach is ill-motivated. Different from the perturbations, which is bounded by some constraints, in images, the perturbations in natural language is distinguishable to human and do not have valid semantic or syntactic structures. \n2. I think the study of fooling attack is less interesting compared to other semantic, syntactic preserving attacks.\n3. In Figures 2, 3, 4, what’s the reason that the maximum value exceeds 1?\n4. There is no quantitative measure in Section 6.7.\n5. Section 6.3, it is basically adversarial training.\n\n\nAdditional feedback:\n1. P1, Q2: fooling images -> fooling sentences\n2. P2, ‘in other to’-> ‘in order to’\n"
        }
    ]
}