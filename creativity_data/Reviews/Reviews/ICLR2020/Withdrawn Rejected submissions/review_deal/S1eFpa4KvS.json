{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this study, the authors propose a regularizer for the dense layers of a deep network that consists in penalizing pairs of neurons that share similar connectivity with previous layers but have different responses. The study shows that the regularizer has three effects:\n(i) it promote the emergence of cell assemblies with similar connectivity and firing patterns\n(ii) it promotes target-class selectivity of individual neurons \n(iii) it substantially improves performance of few-shot learning on MNIST and Fashion-MNIST compared to vanilla networks and an alternative few-shot learning method (imitation networks, Kimura et al. 2018)\n\nAlthough this study is intriguing (especially (iii)), I would tend not to recommend acceptance  at this time because the presentation has issues, and the test cases of the method are too limited (MNIST and Fashion-MNIST)\n\nPros:\n- interesting result on few-shot learning.\n- intriguing connection to biology\n\nCons:\n\n1) typos, bad english wording and incorrect grammar makes the article hard to read, e.g., \"Efficient connectivity presents topologically in most BNNs due to the physical restrictions of dendrites and axons (Rivera-Alba et al. (2014)) and for energy efficiency.\" (no verb, this is just one example among many ill-constructed sentences)\n\n2) The section describing backpropagation and the chain rule is not necessary.\n\n3) The importance of the first two results is overemphasized. It is expected that this regularizer would promote cell assemblies and class selectivity and does not constitute an important result for ML.\n\n4) The correspondence to biology of class selectivity and cell assembly is not convincingly referenced:  \nTononi & Sporns (2003) is a model and does not present experimental evidence for cell assemblies in the visual cortex\nPeyrache et al. (2010) find cell assemblies in the prefrontal cortex, which is different than the visual system (decision center).\n\n5) It would be important to test that this regularizer also enhances few-shot learning on some more complicated tasks such as CIFAR-10.\n\n\nAdditional Feedback:\n\n- It would be interesting to develop a theory explaining why this regularizer is helpful for few shot learning.\n\n\n\n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a new regularization technique for neural networks which the authors call “Biologically Enhanced Artificial Neuronal Assembly Regularization” (or just BEAN). The authors call it this because they think this technique bears relation to how real brains work. BEAN is really best summarized as the addition of a regularization term in the loss function that encourages neurons in a given layer to form functional clusters that are highly correlated with one another. The authors demonstrate that this leads to clusters in neural activity. They also show that it leads to apparently better performance on MNIST and Fashion MNIST than other regularization techniques after only a few training examples.\n\nUltimately, the idea of this new regularizer is interesting, and the final data suggests that it may be a very good regularizer for classification tasks. But, unfortunately, this paper is a mess, and should not be accepted to ICLR. Here are the major issues:\n\n1) The claims of biological plausibility and relation to how real brains work are risible. Hebbian assemblies are ultimately still a supposition in the literature, and there is no evidence for the strict clustering that the BEAN regularizer induces. Indeed, there is actually good evidence that when the neocortex is actively processing sensory stimuli neurons *decorrelate*, presumably for better information transmission (see e.g. Harris, Kenneth D., and Alexander Thiele. \"Cortical state and attention.\" Nature reviews neuroscience 12, no. 9 (2011): 509). As someone who has actually done many recordings in animal’s brains, I can tell you that the activity looks nothing like the BEAN profiles shown in Figs. 2 & 3, and actually looks a lot more like the MLP and ResNet data. Also, the best evidence that does exist for Hebbian assemblies lies in the mnemonic circuits of the brain, like the hippocampus and amygdala (see e.g. Josselyn, Sheena A., and Paul W. Frankland. \"Memory allocation: mechanisms and function.\" Annual review of neuroscience 41 (2018): 389-413.). These Hebbian assemblies are likely the results of recurrent connections between neurons and Hebbian plasticity on those connections, not feedforward connectivity with a regularizer that enforces stimulus driven correlations. Thus, the BEAN regularizer is both biologically implausible, and it results in representations that do not look like the representations found in real brains. The claim that this has anything to do with biology is just plain misguided.\n\n2) What BEAN really does is force clusters on the network. That is explicit in the regularization term. As such, all of the figures showing clusters in the BEAN networks is really just confirmation that the regularizer does what it is designed to. Now, given that this is not biologically relevant/plausible, is this at least maybe a good thing to do for machine learning purposes? The few-shot data is interesting, and suggests it might be. But, I have a sneaking suspicion that BEAN regularization will only work well this way on categorization tasks where the enforcement of clusters is likely to be beneficial. How would this do with RL, or more standard regression? Not so well, I bet. Moreover, I wonder what the performance of the BEAN networks is after more training. Is it better or worse than the other regularizers? That no data is shown that way is suspicious, and leads me to suspect that BEAN sacrifices end of training performance for rapid learning in categorization tasks, due to the enforcement of hard category clusters early in training.\n\n3) The interpretability claims in this paper are hard to swallow. Let’s clarify what has actually been demonstrated. The authors show that: i) BEAN forms clusters in the last layers that correspond to specific categories, ii) lesioning these clusters has a major impact on the associated category performance. In other words, enforcing clusters with a regularization term leads to clusters. We already knew that. That has nothing to do with interpretability, per se. Moreover, to return to the question of biological connections, the authors seem completely unaware that real neural activity is in large part uninterpretable and untuned!!! (See for example: Olshausen, Bruno A., and David J. Field. \"What is the other 85 percent of V1 doing.\" L. van Hemmen, & T. Sejnowski (Eds.) 23 (2006): 182-211. or Zylberberg, Joel. \"Untuned But Not Irrelevant: The Role of Untuned Neurons In Sensory Information Coding.\" BioRxiv (2017): 134379.). Thus, far from being a demonstration that BEAN has used a biological principle to inform the design of more interpretable neural networks, this data essentially shows that BEAN is clearly nothing like what occurs in the brain, and is really all about clusters (which again, we already knew).\n\n4) The inclusion of the graph theory and BIG-ADO rule commentary is completely out of nowhere and has no bearing on the rest of the paper. I do not see why it was included.\n\nAltogether, this paper is not suitable for publication. I recommend strongly to the authors that for future submissions they drop the pretense of any biological realism, and instead just focus on the question of whether the cluster enforcing regularizer actually improves few shot learning in a manner that has practical implications for ML."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors investigate a novel form of regularizer for feedforward neural networks, that adds a penalty to the cost function for pairs of neurons with: a) similar connection weights to the next (downstream) layer, and b) dissimilar activations. This regularizer pushes the networks to form cell assemblies: groups of neurons that receive similar inputs (hence, having similar activations), and make similar outputs. Experiments indicate that the regularizer works as intended: such assemblies do in fact form.\n\nMoreover, on few shot learning from scratch tasks, networks with the BEAN regularizer(s) outperform ones with other regularizers.\n\nI think this is a potentially useful idea, although there are several factors that make me skeptical, and reduce my enthusiasm. Those are outlined below.\n\n1) I need more information before I can evaluate the performance comparison and clustering comparison (Table 1 and Fig. 2). Specifically, what were the regularizer weights (vs. the cross-entropy weight) in the loss functions? (How) were these hyper-parameters optimized for each of the regularizers? Is it possible, for example, that the authors are comparing networks with the optimal BEAN regularizer weights, to networks with sub-optimal dropout keep rates, in Table 1?\n\n2) It's not clear to me how BEAN-2 is meaningfully different from BEAN-1. BEAN-2 appears to be an element-wise squaring of BEAN-1: in the extreme case of binary variables (0 or 1), then, BEAN-1 and BEAN-2 should be identical. Perhaps I've missed something, but these really look the same.\n\n3) It seems like the most important thing the BEAN regularizer does is to reduce entropy in the unit activations, by making groups of units do more-or-less the same thing.  While that does seem to be a benefit for underconstrained tasks, like few show learning from scratch, I wonder if the same (or similar) benefits could be derived from simply having fewer hidden units? A more careful comparison across network sizes would help resolve this question."
        }
    ]
}