{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a hybrid RL algorithm that uses model based gradients from a differentiable simulator to accelerate learning of a model-free policy.  While the method seems sound, the reviewers raised concerns about the experimental evaluation, particularly lack of comparisons to prior works, and that the experiments do not show a clear improvement over the base algorithms that do not make use of the differentiable dynamics. I recommend rejecting this paper, since it is not obvious from the results that the increased complexity of the method can be justified by a better performance, particularly since the method requires access to a simulator, which is not available for real world experiments where sample complexity matters more.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "==Summary==\n\nDDPG is a popular RL method for continuous control problems. It is more widely applicable than traditional model-based approaches like MPC, since it doesn't require differentiable models of the dynamics. However, in many environments, dynamics are differentiable. This paper proposes a method for extending DDPG to exploit simulator gradients. In particular, the Bellman error objective (which is defined in terms of critic values) used for training the critic is augmented with additional terms defined in terms of gradients of the critic. This leads to faster convergence in practice on a range of benchmarks.\n\n==Overall Assessment==\n\nI recommend acceptance. The paper's contribution is well-motivated, works reasonably well, and is relatively easy to implement.\n\n==Comments==\n\nIt would be good to add an argument explaining to readers that accurately estimating Q using Q_\\phi does not mean that the gradients of Q_\\phi will be good approximations of the true gradients of Q. I found Fig 1 of arxiv.org/pdf/1705.07107.pdf informative.\n\nCan you justify the choice of euclidean norm in line 10? In terms of the critic helping teach the actor, the direction of the gradient may be more important than the norm. What if you used cosine sim?\n\nYou argue that DRL is better than MPC because DRL explores better. Could you use the simulator gradients somehow to improve exploration?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper shows how the derivatives from a differentiable\nenvironment can be used to improve the convergence rate of\nthe actor and critic in DDPG.\nThis is useful information to use as most physics simulators\nhave derivative information available that would be useful\nto leverage when training models.\nThe empirical results show that their method of adding\nthis information (D3PG) slightly improves DDPG's\nperformance in the tasks they consider.\nAs the contribution of this work is empirical is nature,\nI think a very promising future direction fo work is to\nadd derivative information to and evaluate similar\nvariants of some of the newer actor-critic methods\nsuch as TD3 and SAC.\n\nI have two minor questions:\n1) Figure 2(a) shows the convenrgence of regularizing states,\n   actions, and both states and actions and the text\n   describing the figure states that this is\n   \"expected to boost the convergence of Q.\"\n   However the figure shows that regularizing both states and\n   actions results in a slower convergence than doing\n   them separately. Why is this?\n2) How should I interpret the visualization of the\n   learned Q surface in Figure 2(f) in comparison to\n   the true Q function in Figure 2(g)?\n   It does not look like a good approximation."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper studies optimal control problems where a physical simulator of the system is available, which outputs the gradient of the dynamics. Using the gradients proposed by the model, the authors propose to add two additional terms in the loss function for critic training in DDPG, where these to terms corresponding to the prediction error of $\\nabla_{a} Q(s,a)$ and $\\nabla_b Q(s,a)$, respectively. However, my main concern is that the form of gradient given in equation (2) might contains an error.\n\n1. Equation (2). Note that in DDPG, the action is given by a deterministic policy. Thus, we have $a_t = \\pi(s_t)$ for all $t\\geq 0$. For critic estimation, it seems you are basing on the Bellman equation \n$ Q(s,a) = r(s,a) + Q(s', \\pi(s'))$, where $s'$ is the next state following $(s,a)$. Then, it seems that Equation (2) is obtained by taking gradient with respect to $(s,a)$. However, I cannot understand what $\\nabla_{\\pi} Q$ stands for. If it is $\\nabla_a Q(s_{i+1}, a_{i+1}) \\cdot \\nabla_s \\pi(s_{i+1}) $, then that makes sense. \n\n2. Based on the experiments, it seems that the proposed method does not always outperform MPC or DDPG, even in a small-scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG. \n\n3. Here the model-based gradient in equation (2) is defined by only unroll one-step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\\lambda$).\n\n4. Missing reference: Differential Temporal Difference Learning https://arxiv.org/abs/1812.11137",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}