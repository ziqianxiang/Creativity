{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper presents a technique for gender-debiasing word embeddings. The technique consists of applying PCA and k means clustering to word embeddings, in order to reduce the effect of gender biased semantics.\n\nThe presentation is clear in the sense that the reader can understand the design of the technique and of the experiments; but less clear in the sense that the term bias is overloaded: early on (introduction) the authors state that removing the gender dimension is equivalent to removing gender bias (this is attributed to Bolukbasi et al. 2016 and Prost et al. 2019). This has not been preceded by a definition of gender bias; so in effect, gender bias is defined as the presence of gender in the embeddings. Further down in the introduction, the authors posit that frequency \"contaminates the gender dimensions\" (this is not further elaborated; as it stands, it is unclear). Even further down, the authors state their intuition that there is another type of bias in word embeddings that is closely intertwined with gender bias: this other type of bias is that words with similar frequencies in training tend to be closer in the vector space even if they do not have similar meanings. So now the core frequentist nature of embeddings is also presented as another type of bias that affects gender bias. After this line of reasoning, the authors state that their technique manages to reduce gender bias and is capable of preserving distributional semantics. But they have just spent two paragraphs defining distributional semantics as bias (in a fuzzy way). \n\nAnother problem is the lack of several SOTA gender-debiasing baselines in the experimental evaluation. The only gender-debiasing baseline used is the Hard method, but this is the very method that the authors extend with their technique. This is a limitation.\n\nThe topic of gender bias is important. The authors are encouraged to continue working on this area, and to focus on clarifying their definitions so that they are expressed in precise language. Thorough experimental evaluation against several SOTA baselines  is also important."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a post-processing recipe for gender-neutralizing word embeddings.\nThe proposed method is a combination of existing Debias Algorithm (Bolukbasi et al, 2016) and all-but-the-top model (Mu and Viswanath, 2018). The former tries to identify gender component and eliminate the effect of that and the latter tries to reduce the effect of frequencies in the quality of embedding. The proposed method, Double-hard Debias tries to reduce gender bias from embedding, in a way that embeddings remain effective.\n\nThe paper is readable and addresses an important question.  However, the method proposed is incremental, missing extensive technical or intuitive exploration of its effect and has limited  experimental evaluation. \n- Debiasing experiments are limited and the task in Section 4.1 seems a  toy task  to me.\n- Showing neighborhood of problematic words, before and after the preprocessing is useful to see.\n- The performance of the processed embeddings in downstream NLP tasks could be reported.  \n\nDecision: I vote for a weak reject."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nSummary:\nThe authors propose Double Hard Debias, which downweights both gender and frequency components for debiasing word embedding for gender.\n\nDecision:\nOverall, the experimental results seem promising on some benchmarks. However, I am inclined for weak reject as it seems to be a simple extension of previous work (Mu and Viswanath 2018) with Hard Debias (Bolukbasi et al 2016). \n\nSupporting argument:\n1. The experiments seem solid compared with previous vanilla embedding and Hard Debias work. However, due to the commonly used contextual embeddings and subword units. It wonder whether the improvement in this direction is significant. The proposed technique cannot generalized to those cases.\n2. Through among the fairness benchmark, the proposed approach seems promising. I wonder how does it change some downstream tasks that uses word embeddings.\n3. As mentioned earlier, the proposed approach is a simple extension of Mu and Viswanath 2018 and Hard Debias. \n\n\nAdditional feedback:\n1. Footnote 1 should be after the period.\n2. In Section 4.1, ‘a better Debias result’ -> `a better debias result’.\n"
        }
    ]
}