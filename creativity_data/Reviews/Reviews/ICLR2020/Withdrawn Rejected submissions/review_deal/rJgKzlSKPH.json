{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a regularization strategy for training a neural network classifier using full weight precision that can then be quantized to a low-bit fixed-point representation. The main claimed contributions are\n\n- A solution to the problem of the soft quantization of batch normalization layers, which revolves around regularizing the (folded) shifting coefficient values towards their (post-training) fixed-point quantization.\n- An exponential schedule for the regularization hyperparameters.\n- An overall model that outperforms other quantization approaches.\n\nMy assessment is that the paper is slightly below the acceptance bar. As an outsider to the low-precision DNN literature, the proposed soft quantization strategy for batch normalization layers looks interesting, but the submission suffers from clarity issues (see clarity-related comments), and the evaluation is insufficient to support some claimed contributions (see evaluation-related comments).\n\nClarity-related comments:\n\n- The spacing between paragraphs has been removed to satisfy the soft 8-page limit. This has a negative impact on the submission’s readability.\n- “Since BN layers operate on different statistics after training [...]” The use of the term “statistics” in this statement is imprecise. Unless there’s an input distribution shift between training and evaluation, shouldn’t the “statistics” BN layers operate on be the same? Maybe the authors are trying to convey that the way in which these statistics are approximated (using a mini-batch during training vs. using a running average during evaluation) is different?\n- The submission claims to accommodate pure fixed-point models, but the soft quantization of biases is swept aside in the presentation: “Since additions play a subordinate role for complexity, we focus on weight multiplications.” Can the authors clarify whether and how soft quantization is applied to biases?\n- Soft quantization of multiplicative factors is handled in two distinct ways in the model: weights are quantized using a symmetric uniform function, and batch normalization scaling factors are quantized using a logarithmic function. Why are they handled differently?\n- “Furthermore, we determine the layer-wise step-size on pre-trained weights [...]. In the next chapter, however, we see that the actual step-sizes are learned within the batch normalization component.” I have trouble reconciling these two statements. They give the impression that the step size is both determined post-training *and* learned during training. In fact, Equation 12 suggests that the step size is learned. Can the authors clarify?\n- “This also prevents the regularization parameter from being too high.” I don’t understand what “regularization parameter” refers to. Is it the lambda values (Equation 13), or is it the R values (Equation 13)?\n\nEvaluation-related comments:\n\n- I don’t understand the reasoning behind the bolding of entries in Table 2. In the MNIST section, why is the Add-Net entry bolded but not the TWN and SGM entries, even though they share the same error rate? Is the 0.04% error rate difference between Fix-Net and SGM significant, considering that this corresponds to four additional misclassified examples on the test set?\n- More generally, the presentation in Table 2 doesn’t group together comparable approaches, which leads to incongruities like Add-Net and Fix-Net entries being both bolded in each section despite Fix-Net systematically outperforming Add-Net. \n- As far as I can tell there is no empirical evidence to support the asserted benefits of the exponential regularization schedule.\n- I’m also doubtful of the significance of the claim that the submission achieves state-of-the-art quantization results, given that in most settings bit-sizes do not directly compare to those of related methods.\n\nAdditional comments:\n\n- I don’t believe L2-regularizing weight values towards the closest quantized weight value has a direct probabilistic interpretation, because the way regularization is applied depends on the weight values themselves. If I’m mistaken, what unconditional PDF over weight values would that correspond to?\n- “Due to their lower number of redundancies, DenseNet and ResNet20 are considered as difficult to quantize.” Is this common knowledge? Can the authors point to related work supporting this assertion?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents Fix-Net, soft quantization approach to train deep neural networks. The proposed approach is evaluated using three common benchmarks, and the results are promising. The paper is well-written and easy to read in general, but there are some unclear things (outline below).\n\nIt is unclear what you mean by \"soft quantization\". I've read the paper several times but have not found a definition. How does it differ from standard quantization?\n\nThe performance of your fixed-point model with 4-bit weights and 4-bit activation is sometimes better than a full floating-point model. This is surprising. Why? Can explain or elaboration on that?\n\nRegarding batch normalization, it is unclear to me if it is quantized during training, or after training? (See Eq (8), (9), and the surrounding text)\n\nThe results in Table 2 look selective, e.g., why is CIFAR-100 only compared with two other approaches? \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this paper, a soft quantization approach was proposed to train DNNs that can be evaluated using pure fixed-point arithmetic. Fixed-point quantization constraints were applied for batch normalization and ReLU as well. In the experimental analyses, employment of fixed-point calculations reduce computational costs (memory and running time) compared to floating-point arithmetic.\n\nIn general, the paper is well written, and initial experimental results are promising.\n\nMajor and minor issues:\n\n- What is the motivation of using three different quantization functions for three different algorithms (convolution, BN and relu)? How do the results change when you apply the same quantization function for all of the algorithms?\n\n- The work proposes an approach for training a general class of DNNs using fixed point arithmetic. In order to support their strong claim, I suggest authors to perform the following additional analyses:\n\n-- Experiments using larger networks (e.g. larger Resnets etc.)\n\n-- Using larger scale datasets, such as ImageNet etc. This is particularly important since the results of state-of-the-art methods change crucially when they are employed for training quantized DNNs using Cifar-10/100 vs Imagenet, Open Images etc.\n\n-- If you would like to analyze the proposed methods for vision tasks, then I suggest you to apply the proposed methods for training, at least, detection and segmentation models in addition to image classification models.. \n\n\n"
        }
    ]
}