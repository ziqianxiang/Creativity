{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose an approach CWAE to recover reward function in IRL problems. The proposed CWAE employs Conditional Variational Auto-encoder (CVAE) and Wasserstein loss function. However, I have a few key concerns below, that prevent me from giving a direct acceptance.\n\n•\t1. There lacks for theoretical analysis to support the idea of using VAE for solving IRL problem. Especially, GAIL [1], establishes rigorous theoretical connection between IRL and GAN, which solves the same problem. It is unclear of the motivation and the strong reason of employing VAE.\n\n•\t2. Experimental results are. The performances of CWEA-IRL as shown in the paper are not promising, when compared baseline approaches, e.g. deep maximum entropy IRL. The statement of “Deep Maximum Entropy tends to give negative rewards to state spaces which are not well traversed in the example trajectories” is not clearly explained.\n\n•\t3. No comparison with GAIL.\n\n[1] GAIL https://arxiv.org/abs/1606.03476\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to solve the inverse reinforcement learning (IRL) problem via supervised learning. This is done by formulating the IRL problem as one of variational inference using a Conditional-VAE. The proposed approach seeks to learn an encoder that takes (s,a,s') tuples and encodes them into a reward signal. The decoder then seeks to recover s'. \n\nThis is a very interesting idea, but it is not ready for publication. The paper is lacking experiments involving RL. When evaluating IRL algorithms you need to run RL on the learned reward to really verify if it works. Just getting a reward that is \"close\" to the true reward doesn't mean that the learned policy will be anything like the optimal policy. \n\nThe results in Figure 2 seem to show that the method is not working as well as the paper claims. As mentioned above, the real test of reward learning is what policy results from RL. Similarly, in Figure 3 the mean looks good, but the variance appears very high and it is unclear if the network has really learned the right reward function without comparing policy performances on the ground truth reward. \n\nThis paper also overlooks two recent papers that also convert IRL into a supervised learning problem and do not require knowledge of MDP dynamics or data collection for reward inference:\nWang, et al. \"Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation.\" ICML, 2019.\nBrown, et al. \"Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations.\" ICML, 2019.\n\nIn general, I would recommend spending less time on deriving the preliminaries and use the extra space for experiments involving policy optimization on the learned reward function. Given successful experiments this would become a nice solid paper."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The submission considers the inverse reinforcement learning, and follows the framework of BIRL. Further, the authors propose to use the CVAE to solve Bayesian model of BIRL. Since I am not familiar with line of research of CVAE, I would like to focus on the IRL part. Specifically, I have two questions regarding the identifiability (or the entanglement) of the model:\n\n1. The definitions of Q^* and Z' are not provided, which is very confusing. In the framework of MaxEnt methods, the target is not exactly the reward function $R$, but a reward shaped by the dynamics of the MDP (as suggested in Fu et al. (2017)). I am wondering whether $R$ is identifiable in the formulation of the submission? In other words, given the dataset, is it guaranteed that the reward function maximizing the objective function is unique or only has a constant difference? Right now, it is unclear in the submission. It is also fine if the identifiability argument is not true. But we definitely need more discussions regarding this issue.  \n\n2. So, is the MaxEnt method considered in the experiments the method in Fu et al. (2017), which can estimate a disentangled reward function? Since MaxEnt method aims to estimate the reward shaped by the dynamics, it may be unfair to compare with other MaxEnt methods. \n\nDue to the ambiguous of the formulation, and the lack of discussions regarding the identifiablitiy issues, I tend to reject, now. \n"
        }
    ]
}