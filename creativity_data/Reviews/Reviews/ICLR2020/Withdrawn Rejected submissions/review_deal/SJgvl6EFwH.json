{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents a conditional CNF based on the InfoGAN structure to improve ODE solvers. Reviewers appreciate that the approach shows improved performances over the baseline models. \n\nReviewers all note, however, that this paper is weak in clearly defining the problem and explaining the approach and the results. While the authors have addressed some of the reviewers concerns through their rebuttal, reviewers still remain concerned about the clarity of the paper.\n\nI thank the authors for submitting to ICLR and hope to see a revised paper at a future venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a conditioning approach for CNF and explore speed-up by tuning error tolerance of the ODE solvers.\n \nOverall, this is a mediocre paper which directly use a similar structure introduced in InfoGAN but absolutely lose beautiful insights in the construction of the loss function in InfoGAN, i.e., the mutual information between the generated image and codes. In other way, this paper is just an incremental paper with even less insights than the seminal paper. \nAt least in the loss function Eq. (4), I didn't see any mutual information regularization used here. Instead, the authors use a GMM but I am totally not sure why a GMM is better than the mutual information regularization. At the same time, in equation (4), I didn't see the specific definition of L_xent and L_NLL and thus I am not even able to verify that the use of the loss function is correct. For the current version, I am not be able to gain any insight from the loss function. It seems to be an ensemble of several existing works and definitely not innovative. \n\nFor the tuning error of the ODE solvers, I didn't even see the problem statement.  Is it possible to make the problem more clear? It seems that the first time when the authors mentioned error tolerance is in contribution 2, but I didn't see the definition of the error tolerance. I am not sure whether it is a good idea to introduce two problems in one paper. At the same time, I do not know why the problem should be formulated in the form of the reinforcement learning problem. I didn't see any advantage.  Intuitively, learning a generative model can be time consuming  and solving a reinforcement learning problem is also hard. I do not understand why combining them together would be beneficial and even time efficient? For me, it seems that authors are just trying to make the problem unnecessarily more complicated and thus they can use fancy tools to solve it.  \n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "This paper suggests a methodology of partitioning latent code to a set of class-specific codes, to solve the inefficiency from the large size of the latent code in conditional normalizing flow. This inefficiency problem has been a weak point in the existing conditional normalizing flow models. Also, this work addresses the increase of the number of function evaluations which is caused by code partition, by optimizing the error tolerance with a reinforcement learning approach.\n\nThis paper has a novel contribution, outperforms the baseline (CCNF: CNF (Chen et al.) + GMM / auxiliary classifier (Kingma & Dhariwal, 2018)) on several evaluation metrics. I agree to accept this paper, but I vote for ‘weak accept’ because of the following weaknesses:\n\n1. The (three) contributions described by the authors seem to be somewhat exaggerated.\n- For the second contribution, the authors’ way is quite similar to Wang et al. (SkipNet). For both studies, the purpose is the efficiency of certain neural architectures and training scheme is hybrid reinforcement learning with REINFORCE (but with different reward design).\n- For the last contribution, I think it is a minor contribution comparing to two first bullets and overlapping with the second one.\n\n2. There is a lack of explanation to support the efficiency of the proposed model.\n- The authors claim that InfoCNF needs fewer parameters than the baseline. Then, why didn't you show the actual number of parameters? The only line that I found was “... InfoCNF requires 4% less parameters than CCNF. (Section 2)”.\n- Also, it would be better if there were a direct computation and comparison between the size of InfoCNF and CCNF.\n- Finally, is there any constraint on the length of merged latent code z? Since InfoCNF is also an invertible model, it should have the same size as the input, but I cannot find descriptions about it.\n\n3. It is skeptical that automatic tuning of error tolerance has achieved its original purpose.\n- For me, it is questionable whether automatic tuning has achieved its original purpose: reducing NFEs for better speed (and performance).\n- In figure 3c and 3f, we can find NFE of InfoCNF learned tolerances eventually exceeds the NFE of InfoCNF fixed tolerance. It looks like learning tolerance increases NFE in large epochs, and the timing seems to depend on the batch size. If so, the batch size is a really important hyper-parameter in this framework, how can we determine the appropriate size?\n- In section 4.4 (Automatic Tuning vs. Manual Tuning), the authors state that “our automatic approach learns the tolerances which outperform the manually-tuned ones in both classification and density estimation while being only slightly worse in term of NFEs.”. But as abstract says, is reducing NFEs the original goal of automatic tuning?\n- Lastly, how can we know/confirm “our automatic approach via reinforcement learning requires much less time and computational budget”. I cannot see any explanation about this claim.\n\n4. Minor things.\n- What is the stage stated in Figure 1?\n- The abbreviation of the CCNF first appears in section 1 but its full name first appears in section 2.2.\n- In figure 3a and 3b (and table 1), why is test NLLs of CCNF lower than InfoCNF’s where test error of CCNF is larger than InfoCNF’s with high margin? Is there any possible explanation?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a conditional CNF based on a similar intuition of the InfoGAN that partitions the latent space into a class-specific supervised code and an unsupervised code shared among all classes. To improve speed, the paper further proposed to employ gating networks to learn the error tolerance of its ODE solver. The experiments are performed on the CIFAR-10 dataset and synthetic time-series data.\n\nThe paper has addressed an important issue of investigating efficient conditional CNF. The general idea of the paper is clear, but I found certain parts can be improved, such as the formulation of InfoCNF. It seems the authors assume readers know InfoGAN well enough, which might not be the case. \n\nMy main concern is the limited evaluation as all the experiments are performed on the CIFAR-10 and synthetic data. Since the paper address efficient conditional CNF, it would make the claim much stronger if more experiments could be performed on larger images: if not the original imagenet, maybe imagenet-64 or imagenet-128 or image benchmarks with higher resolutions.\n\nWhy does InfoCNF achieve slightly worse NLL in small batch training, while it outperforms CCNF in all the other metrics? Do you have any explanations?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper examines the problem of extending continuous normalizing flows (CNFs) to conditional modeling. The authors propose a model, InfoCNF, which models a latent code split into two partitions: one that is unique to a class, and another that is more general. InfoCNF relies on the accuracy of ODE solvers, so the paper also proposes a method that learns optimal error tolerances of these solvers. They perform experiments on CIFAR10, showing that InfoCNF outperforms a baseline in accuracy and negative log-likelihood. They also ablate through experiments that demonstrate the utility of learning the ODE error tolerances. \n\nCNFs are an exciting tool, and it's an important problem to devise methodology that applies CNFs to conditional modeling tasks. The idea of splitting latent codes into two separate components -- one supervised, the other more general -- is interesting. And the approach to learning error tolerances is a good idea.\n\nThe main drawback of this paper is the lack of clarity. It is poorly written and the presented model is not clearly motivated. Even after reading through the paper multiple times, I find it difficult to understand various aspects of InfoCNF. Below are some examples of this lack of clarity:\n\n- When motivating Conditional CNF (CCNF), the details for training the model are unclear. What is the loss function, and how does it balance between modeling x and learning the auxiliary distribution? Although these may seem like small details, since InfoCNF builds off on CCNF, it is crucial to solidify an understanding of the training procedure and how the auxiliary task relates to modeling x. Moreover, the cited reference (Kingma and Dhariwal, 2018) does not contain these details (and there is no mention of object classification in the paper, contrary to the claim on page 3). It would be helpful to cite other references when mentioning that this approach is widely used. \n\n- The definition of InfoCNF is unclear. The variable y has been used to denote the image label, so why are there now L latent variables y_1, ..., y_L? The following terms of equation 4 are undefined: L_{NLL}, L_{Xent}, and y_hat. Although some readers would be able to understand these definitions from context (flow-based NLL, cross entropy loss, and the logits provided by the auxiliary distribution q), they are never explicitly defined and result in confusion and ambiguity. Most importantly, p(x|z,y) is never made explicit; although one can infer from context that  is transformed to x using CNFs, it is never made explicit in the definition (and that these flows only appear in L_{NLL}). Overall, the motivation for splitting the latent code into two pieces is not clearly explained, and the paper should spend more time arguing for this.\n\nThe paper compares InfoCNF to a single baseline (CCNF). I understand that the paper is proposing the first method that combines CNFs with conditional modeling, but there are plenty of non-CNF flow-based conditional approaches that could've been compared. The paper goes over these approaches in Section 5 and discusses their differences with InfoCNF, but these are never experimented with. It seems possible that these models could be extended in a straightforward manner to use CNFs instead of other normalizing flows. Even comparing with these models without CNFs would have been interesting. I think that using a single baseline, instead of applying a more complete set of comparisons, hurts the persuasiveness of their method.\n\nIn addition to comparing to a single baseline, the paper only compares for a single dataset (CIFAR10). A more convincing experiments section would compare against more models on more than a single dataset.\n\nThe paper proposes a promising idea to an important problem. Due to the lack of clarity throughout the paper and incomplete comparisons, I would argue to (weakly) reject.  "
        }
    ]
}