{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors propose an RNN-based variational autoencoder model with one encoder and multiple decoders. The authors utilize multiple decoders to model different dynamical systems, aiming at addressing variations in time series such as phase shift, length variation, stretch and compression and so on. EUNN-based RNNs are used in the proposed model. The authors derive the ELBO of the proposed model and describe the EM training steps. The idea brought by this paper has some merits, but given the discussions and evaluations in the current manuscript, it has not been supported and validated that the proposed model does address those a variety of problems/properties in time series comprehensively and systematically.\n\nLots of related work fields are missing. For example, time series clustering should be included. Recent models on extracting hierarchical / multi-scale temporal features should be discussed as well. Distance- and density-based methods are also relevant.\n\nOnly RNN-AE are compared in the experiments. At least the authors should compare baselines such as LSTM/GRU-based models and other VAE-based RNNs.\n\nThe datasets and tasks are relatively simple. On the synthetic dataset, the models are supposed to distinguish time series with different numbers of periods. Results on the driving behavior dataset (Figure 8 and Table 2) are not strong. What is the metric shown in Table 2? Are the results from 5 runs of 10-fold cross-validation? If the accuracy is shown, ~0.5 for 3-class classification is not strong.\n\nHow about the training and inference efficiency of the proposed model, especially compared with recent RNN baselines? This may prevent the model from being applied to large datasets in practice. \n\nAdditionally, the authors could better organize contents in the main paper and the supplementary. The main paper should be self-contained and include, for example, how to calculate r_{nk}, Algorithm 2, explanations of Fig 4, and what the baseline RNN-AE exactly is."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a new method to extend an RNN autoencoder for multi-decoder setting. The presented algorithm and derivations are not novel, although the presented idea is a reasonable extension for sequential data which should be processed by multi-decoders. \n\nIn principle, the main idea of multi decoder is to use separate decoders since one decoder is not applicable to express various types of sequential data (decoding part) when a single encoder is enough to handle input sequences. \n\nHowever, in my personal opinion, it is not motivated enough why multi-decoder is necessary (except for an empirical evidences in Table 2) when many people use a RNN model as a good but approximate function approximator.\n\nAuthors may argue that multi-decoders are necessary to process some sequential data. Unfortunately, experimental results are not promising enough for me, the proposed algorithm/method is compared only with an existing method."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a variation of clustering algorithm for feature extraction, by employing the variational Bayes method, on time series data that’s prone to modulations in frequency, amplitude and phase. This approach includes the proposal of a new architecture, multiple decoder RNN-AE (MDRA), in order to capture the dynamical system characteristics. The proposed contribution has been validated by its performance in a classification task on a synthetically modulated time series data as well as driving behavior data.\n\nMain Argument\nThe paper does not do a good job of demonstrating if the problem being addressed is a significant one. The authors incorrectly attribute RNN-AE to Srivastava et al 2015 to establish their motivation and that RNN-AE is indeed a problem which they are addressing by including multiple decoders. But, the architecture presented in Srivastava et al. 2015 is not an RNN-AE but an LSTM-AE. This leaves us wondering what could be the overall impact of the current proposed architecture to the literature. For example, the authors could have instead tried multiple decoders on the LSTM-AE. Also, the use of EUNN (Jing et al. 2016) to avoid vanishing gradient is rather naive, given that sophisticated training techniques for RNNs are available in the literature. In fact, GORU has been proposed by the same authors (Jing et al 2017) of EUNN (Jing et al. 2016) and has shown to be superior to EUNN. This choice of EUNN over GORU or even the use of RNN over LSTM leaves us with little about the impact of this work. The authors could have indeed focused on comparing their proposed method with the current state-of-the-art architectures which include LSTM-VAE/GMVAE and many others. The experiments, although look fine, lack the robustness/generalizability often desired of such complex architectures by trying it on more complex sequence data. Specifically, a section on experimental settings (initialization scheme, hyperparameter optimization, etc.) haven’t been touched upon.\n\nFurther supporting comments:\n1. The MDRA architecture was proposed based on the premise of capturing the dynamical system structure. The interpretation of RNN as describing the dynamical system structure aka h, although good, is particularly serving no specific purpose in this work as the authors themselves point out the emphasis on the transformation rule, in Section 4 Paragraph 1 Sentence 5. Equating the U’s, V’s and W’s to the dynamical system features is a very shallow reasoning and is too limited in explaining anything about the actual dynamics of the system under study.\n2. In the last sentence of Section 3 the authors indicate that they adopted the method, EUNN, proposed by Jing et al (2017). However,  the paper does not justify the choice. Another method, GORU was proposed by Jing et al (2019) and it seems to perform better than EUNN. Why not this one, for example, or anything else?\n3. In fact, given the incredible success of LSTMs in many application areas and the gamut of innovations that have been proposed for superior training of LSTMs, the paper may need to justify the use of a more simpler and problematic RNN-AE in the face of a more successful LSTM-AE (Srivastava et al. 2015). Maybe, a comparison to a similar but LSTM-based multi-decoder architecture would demonstrate the reasoning.\n4. Do the authors have a reason for not trying RNN-VAE/LSTM-VAE with multiple decoders instead of MDRA? What are the distinct benefits of MDRA, in terms of latent representations, over a VAE/GM-VAE/Normalizing flows?\n5. As the discussion of the work relates to RNN, I believe this work should have a wide range of applications beyond just time series data i.e. it should generalize well to any sequence data (speech recognition, sentence classification), The current experiments although decent could have been extended to include more complex datasets and thereby demonstrate the generalizability of this architecture.\n\n\nOther comments:\n1. Section 2 last paragraph, Pascanu et al is out of place. Arjovsky et al should have been cited in the last sentence of the previous paragraph. \n2. In Section 1 Paragraph 2 Sentence 2, the authors incorrectly attribute RNN-AE to Srivastava et al when in fact they proposed LSTM-AE.\n3. Section 1 Paragraph 2 may benefit from a rewrite as it is really vague in building-up the motivation behind the proposed work.\n4. Some use of terminology is unclear\n   1. Example: In Section 1, Paragraph 1, Sentence 3 I doubt if phase shift, compress/stretch, and length variation are outcomes of time series variation or if they are sources of time series variability. \n   2. Example: Section 3, Paragraph 1, Sentence 2: The architecture of the main unit is called cell. Instead, the primary component of an RNN architecture is called a cell (hidden node in other words).\n5. Typos\n1. Section 4 Heading: DERIVATION not DEVIATION\n2. Section 4.1 Paragraph 2. Variables not valuables\n3. Section 6 Heading: DISCUSSION instead of DISUCUSSION\n4. Appendix C Heading: EXAMPLE not EXAPMPLE\n5. Appendix D: quiet not quite\n6. Figure 11 box 2: output not outout\n7. Figure 11 caption: Fully not Full\n8. Section 5.2 Line 1: drivers’ not driver’s\n9. Appendix A3. In the expectation of log_beta with respect to a Gamma distribution, ‘x’ should have been ‘beta’ instead.\n"
        }
    ]
}