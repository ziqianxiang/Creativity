{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper extends work on dilated convolutional nets to Pixelwise\nAdaptive Dilated Nets (PAD-Nets) by allowing for spatially varying, pixel and feature dependent dilations. Dilation as a function of input at individual pixels is achieved through sampling  probability distributions that are conditioned by pixel-wise input features (Gumbel-Softmax is used to approximate the sampling process in order to ensure end-to end differentiability).  The paper makes the assumption that dilation values are related to inter-layer\npatterns between convolution layers due to their hierarchical nature and the RF size at each\nlocation is adjusted based on information flows between corresponding inter-layer pixels during\nforward propagation.\n\nPROS:  Addresses adaptation of local context and feature dependent modification of receptive field size,  Experiments are shown to improve semantic segmentation performance in a number of datasets.  \nCons:  Incremental evolution of existing ideas,  Too much claim (especially on discussion section) on how the design leads to more interpretability.  The claims are still largely empirical without theoretical grounding.  Only a few images are shown illustrating performance.  The paper should also show examples where there are fundamental limitations. Thus, one does not see what challenges (if any) remain.  If the authors address these above concerns in the rebuttal, I will be inclined to reexamine the rating."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThe premise of this paper is to propose “adaptive dilation” this allows convolutional kernels to “flexibly adjust based on different contents.” What precisely this mean should be clearer even at the abstract.\n\nThe authors are motivated by the observation that \" two obvious problems, which universally reside in most of existing dilated CNN structures, need to be properly tackled”\n\nThese are \n1. “All weights” share a single dilution value across all pixels”\n2. Dilution selecton is data-independent\n\nThe first problem is unclear.  The latter is not exactly true, the dilutions are hyperparameter values chosen based on datasets just as much as any other ML architecture choices.\n\nThe authors propose to incorporate dilation selection into a “unified data-driven framework”.\nThe premise here is to learn a distribution over dilation values. \nThese are then sampled from during training from properties \nthat rely on \"content-related hidden priors”.\nThe sampling approach relies on the popular Gumbel softmax.\n\nThe paper makes many fuzzy claims, for example:\n“dilation inference grants a manageable way to partially understand the feature hierarchy”\n\nThe paper has a number of qualitative experiments aimed at interpretation \nof the new receptive fields in the hopes that they via weak supervision\nreveal some significant structure in the images.\n\nThe experiments show some improvements on semsantic segmentation tasks\nover vanillar versions of various models \nincluding ResNet101, Xception, VGG, and DRN-D.\n\nThe authors also report some very slight improvements on image classification,\nbut these are built on weak models, the strongest of which is a ResNet50\nand all results are far from passable numbers on the task today.\nThe bes result here has 76.9% accuracy, while ResNext gets 84.5 %.\nEven the vanilla ResNet-101 from the original paper in 2014 gets 78% accuracy,\nbetter than the best reported  number here.\n\nIn the end I think this is an interesting heuristic but it’s not clear what is gained\nand the results are promising but I am not quite convinced that a significant contribution\nhas been presented.\n\nMinor:\n\n“The power of prestigious Convolutional Neural Nets (CNN)”\n>>>\tWrong adjective. \n\n“Gumbol-softmax”  ==> “Gumbel-softmax”\n>>> \tMisspelled throughout the paper\n\n\"To model inter-layer pattern smarter,”\n>>>\t“smarter” ==> “more effectively”\n\n“3.3 discussion”\n>>>\tIt’s poor style to have a “Discussion” section that is not the actual discussion section\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose letting a classification network learn the value of its dilation parameter at a per-pixel-location level. They experiment with different network architectures on dense classification and whole image classification tasks on several standard canonical datasets.\nStrengths:\n- Conditioning dilation on learnable weights is a sound idea, which intuitively makes sense as to why it could offer improvements\n- There is a strong variety of experiments performed\n- PAD kernels are tried on several different architectures, which is good\n- Reporting the negative results at the end of section 4.2 is a strong truthful admission that is rarely done, and we commend the authors for doing this.\nWeaknesses:\n- The approximation of the dilation determining function with a gumbel softmax seems weak. There should not be stochasticity in the dilation value given a fixed pixel input. The choice appears to be made solely because it is differentiable, not because it fits the goal.\n- The assumption of dilation independence within a layer (intralayer as opposed to interlayer) cannot possibly hold. The simplest argument against this: if detecting an OR function of A and B across spatial distance, if the dilation around A increases to include B in its receptive field, B does not need to increase its dilation to include A.\n- The discussion of interpreting feature space through dilation parameters is vague and not rigorously substantiated.\n- The claim about PAD kernels increasing receptive fields for conv5 as opposed to conv3 is confusing, because conv5 has a larger receptive field than conv3 period, regardless of the effect of PAD kernels.\n"
        }
    ]
}