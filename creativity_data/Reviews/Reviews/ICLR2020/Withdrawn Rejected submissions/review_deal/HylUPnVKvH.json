{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents Mix&Match, a formalization of mixed image sizes training for conv nets. One core idea is to set a computational budget which can then be \"spent\" on either training with higher image sizes or training with larger batches / more duplicates. The paper also presents a gradient smoothing technique along with a batch norm calibration procedure to deal with issues related to mixed size training.\n\nGenerally speaking, I do like the ideas presented in this paper. The insights used to justify the Mix&Match approach are not groundbreaking, but worth reading. The experiments are also comprehensive -- assuming we are ok with considering only image classification. However, there seem to be a general lack of details/explanations or some unsupported claims in some crucial parts of the paper. I provide six examples here:\n\n1) I am not convinced by the third contribution (\"We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy\"). As pointed out by the authors themselves in Sec. 2.1, Tan and Le 2019 already shows this. Can the authors point out more clearly what is the basis of novelty for this contribution?\n\n2) The benefits of gradient smoothing are not well described. First, ablation study in Appendix B (Fig.5) only shows a slight advantage (to take the authors' own words) to gradient smoothing. Is the \"slight advantage\" significant? Moreover, this advantage only stands for test error, which would suggest that gradient smoothing is more a regularizer, and this is not discussed in the analysis. Perhaps the benefits of gradient smoothing would be more obvious if a larger set of sizes S would be chosen (on another problem than CIFAR-10, of course)? Finally, the paper states how \"gradient smoothing is designed to adapt globally\", which I actually consider as a weakness. In particular, it thus assumes that the variance ratios are the same for all layers. Is it the case? Any difference would even be amplified by the L2 norm applied on the gradients (instead of L1 for instance)...\n\n3) The batch norm calibration seems to be of little use in practice, where the size of images are more likely to be completely arbitrary instead of a few discrete values. As a result, one would either have to perform the calibration for every size or assume a \"close enough\" calibration using (for instance) the nearest power of 2 to be sufficient. The former would require a lot of computation time while the latter was not discussed in the paper. Can you expand on the benefits of batch-norm calibration for practical problems?\n\n4) CIFAR-10 results in Fig.2 and Table 2 look dissimilar. The ResNet-44 baseline accuracy (92.84%) does correspond to the ~7.1% error in Fig.2. However, the 94.46% accuracy for D+ regime should translate to a <5.5% error, which is not what we see on Fig.2 (about 5.9%). Similarly for B+ regime (94.3% accuracy in Table 2 vs. ~6.7% error in Fig.2).\nSure, the difference is small, but so are the differences between baseline and MixSize approaches. Is there something I overlooked here?\n\n5) ImageNet experiments (Table 2, Fig.3-4, and the related explanations) are unclear to me. The paper states:\n> While the original training regime consisted of images of size 224×224, our proposed regime makes for an average image size of S̄ × S̄ = 144 × 144\nBy using a smaller image size, batch size or batch duplicates can be increased. This is then compared to a baseline network using 224x224 fixed size images. However, considering the paper advocates for mixed sizes training against fixed size, the baseline should, to be fair, be the most efficient fixed size. Looking at Fig.1, we see that training on fixed 160x160 size results not only in a slightly better evaluation accuracy at 224x224 (about 1%, if I had to guess solely with the figure) but also in a much lower computational burden.\nAs a result, the difference between \"Baseline\" and \"D+\" approach would decrease, but above all, the D+ computation budget would have to be computed using 160^2/S^2 instead of 224^2/S^2, which would lead to a decrease in performance. Similarly, B+ regime would not be 2.7x faster, but 2.7 / (224^2 / 160^2) = 1.38x faster. Admittedly, this is still faster, but by a much lower amount.\nLooking at Fig.4, the same issue arises. Fig.4b does present all Fixed S approaches, but not Fig.4a which is the crucial one since it directly compares to Fig.1. \"Mixed S=144\" would probably still fare better, but not by the same margin.\nOverall, just by looking at these results, it is unclear if the best approach should be mixed size training with D+ regime or simply fixed size training with S=160. Could these results and analysis be updated to provide a fair comparison with fixed size approaches?\n\n6) The \"profound impact on the practice of training convolutional networks\" is not obvious. First, in practice, pre-trained networks probably lead to a much faster training anyway. And if a pretrained network is not available (say, a custom architecture), then the practitioner has to cross-validate the stochastic regime over several alternatives, as the authors did on ImageNet (Sec. 5.1) to take advantage of MixSize. Computational time budget for training is also unlikely to be a huge issue in most cases -- sure, there may be specific applications where it is, but then we are not talking about general impact. What is the actual use case in practice for MixSize?\n\nIn summary, the paper limited contributions are nice, but not enough to compensate for all these issues. In themselves, each of them is not a deal breaker, but all put together, it makes it hard for me to advocate for acceptation. I would be more than pleased to change my rating, however, if the claims and experiments were revised and better explained."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a mixsize regime which uses different sizes of input in training classification networks. The method also utilizes batch augmentation and varying batch sizes, plus gradient smoothing and BN calibration. The paper obtains better results than baselines and more robustness when testing under different resolutions.\n\nThe method is well-motivated in section 3, seems quite reasonable and the result improvement over baseline is impressive in Table 2. But I do have several important concerns.\n\n1. In computer vision, typically only in image classification the network takes in a fix-sized input. In object detection and semantic segmentation, all state-of-the-art models naturally accept images of different resolutions. In detection this is typically by the use of some form of pooling and in segmentation the output size naturally scales with input size. So I suspect the novelty of the paper is not enough: it basically brings a common practice (varying input size) used in detection/segmentation into classification.\n\nAlso, I wonder whether transfering the model trained with mixsize could improve the results when the classificatio model is used as a backbone in object detection/segmentation, given the later two already vary the input sizes. However, the evaluations are only done in a classification setting.\n\n2. The mixsize regime seems to complicate itself by incorporating batch augmentation. Why is this needed? If the main point of the paper is to argue training with multiple sizes of input helps, then it should isolate other factors like batch augmentation, and keep the batch size in each iteration fixed instead (in my understanding this means removing D and keeping B fixed). If B and D are to keep the computation budget fixed, I'd rather see this achieved by extending the training iterations, because B and D can influence the results by themselves. This was not clearly explained in the paper. \n\nIf B and D are crucial for the improved convergence speed/accuracy, then the main point of the paper should not be only using different sizes of input but use them together with the B and D, and should compare itself with the counterpart that use B and D but not with different input sizes. This is important in helping readers understand what is actually useful here.\n\nThe use of gradient smoothing and BN calibration is more understandable since they explicitly address the resolution inconsistency issue.\n\n3. The ImageNet performance in Table 2 improves quite a lot over the plain baseline, but comparisons with other more advanced methods (e.g., [1,2]) seem to be missing. \n\nOther questions:\n1. Is the correlation calculated for p(x^32, x^24) using the same model or different models (i.e., one model is trained on 32, the other is trained on 24)?\n2. Why is gradient smoothing only used in B+ but not D+?\n\nGiven the rather complicated regime coupled with B and D (2), what actually is useful is not clear, and the experiments may not be a faithful reflection of the abstract/introduction. Also considering the novelty issue (1), I'm leaning to reject the paper.\n\n[1] mixup: Beyond Empirical Risk Minimization.\n[2] Fixing the train-test resolution discrepancy. \"Using a lower train resolution offers better classification at test time\". \n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper introduces new mix-size regime for training CNNs. Several image sizes are mixed, and the input size can be changed for each mini-batch. As a result, trained model generalizes better to different image sizes at test-time. In two suggested training regimes, the model either is training faster or achieves better accuracy at test-time on both fixed and varying size testing dataset. Modifying the image size leads to several difficulties in training the model. To overcome these difficulties the paper suggests a couple of methods: gradient smoothing to decrease the gradients’ variance and batch-norm calibration to correctly collect mean and variance statistics in batch-norm layers.\n\nOverall, the paper addresses an important problem and provides a significant methodological contribution. The problem is clearly demonstrated by a figure showing poor generalization to different test time image sizes. The table with gradient correlation between different spatial size of images convincingly motivates the mix-size idea. Possible severities of training in mix-size regime are considered. The experiment section provides results on CIFAR10, CIFAR100 and ImageNet with different network architectures. Suggested methods outperform the baseline and provide a trade-off between the training time and final accuracy.\n\nThere are, however, several questions/drawbacks, which I list below.\n1.\tExperiments in Section 5 lack the comparison with not only baseline, but also with one of the previous attempts to adapt training with mixed image sizes. Is it possible to compare, for example, either with (Touvron et al., 2019), or with one of the works on progressive resizing? It seems that Appendix C is somewhat close to this comparison, can you add it to experiments in Section 5.1? It can be added even on the same Figure 2.\n2.\tThe Table 1 suggests a conclusion, that faster learning could be achieved using some regime with the mean input size increasing over iteration. However, in main experiments some balanced stochastic regime without mean input size increasing was chosen. The experiment in the Appendix C evidences that two regimes with random (but increasing) image sizes works better than non-random increasing. However, these two regimes are not compared with the models trained without size increasing. On the Figure 6(b) from Appendix, it seems that these two models are indeed slightly better: final error 5.3-5.4% vs 6%. However, it’s not clear, whether the experiment in Appendix C and Figure 6(b) in particular is also CIFAR10, ResNet44. Aren’t these two regimes a further improvement of the approach? Moreover, as they are inspired by an observation in Table 1, they should be in the main part of the paper. Also, is it D+ or B+ regime at Figure 6(b)?\n3.\tFor the full self-sufficiency of the paper, a brief description of batch augmentation technique, used in the experiments, is required. Although there is a reference, batch augmentation is not just related work but an important element of one of the two suggested training regimes (D+). As it’s not very commonly used approach yet, I believe it requires some description.\n4.\tFor two significant changes in training procedure, namely gradient smoothing and batch-norm calibration, the paper provides an experimental assessment of gradient smoothing in the appendix. Is it possible to experimentally assess the influence of batch-norm calibration for varying image sizes? For example, to compare with fine-tuning from (Touvron et al., 2019).\n5.\tIn Experiment 5.1, there is no description of the baseline. The sentence is probably needed, that baseline is a model trained with fixed image size.\n"
        }
    ]
}