{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a domain-adaptive filter decomposition method via separating domain-specific and cross-domain features, towards learning invariant representations for unsupervised domain adaptation.\n\nOverall, this well-written paper is well motivated with a better technique for learning invariant representations using convolutional filters. Nonetheless, reviewers still have major concerns: 1) the novelty of the paper may be marginal given the significant line of recent work on learning domain-invariant representations; 2) when the label distributions differ, learning invariant representations can only lead to worse target generalizations; 3) the provided theory has an unclear connection to the presented filter decomposition method. The paper can be strengthened by further discussions on how to mitigate the aforementioned negative results. \n\nHence I recommend rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summarize what the paper claims to do/contribute.\n* The paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as \"domain-adaptive filter decomposition\". Each domain contributes its own share of features to be combined by a subsequent common layer. The method was benchmarked against competing methods on simple classification tasks and a hard semantic segmentation task.\n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\nWeak Accept.\n* I think the paper was very well written, the explanations were clear and the technical contributions seem sound.\n* The experiments were satisfying for the most part. I would have wanted to see MNIST->SVHN for the unsupervised case as well, as this is a particularly hard one.\n\nProvide supporting arguments for the reasons for the decision.\n* Please do not use the Office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it's hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets.\n* For GTA->Cityscapes you are missing a few works eg, the CYCADA work. Also please use citations in the tables if you did not yourselves run experiments (as to make it clear that experimental protocols also might be slightly different etc). \n\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n* Tables 1 & 2: It'd be better to not refer to methods as A1,2,3 but rahter with some specific names or explicitly describe what these abbreviations mean in the caption of the table."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a way to decompose features for better domain adaptation via learning domain-invariant representations. The main advantage of the proposed approach is that by only introducing a few model parameters, the proposed approach could quickly adapt to new domains. Similar idea has been proposed in [1], where the authors propose to use domain-specific encoders to extract domain-invariant features. Compared with [1], the main novelty of this paper lies in a new feature decomposition of the traditional convolution layer.\n\nMy main concern is that this paper seems to miss a significant line of recent work on learning domain-invariant representations [2-3]. Basically, it has been shown that invariant representations provably hurt generalization on the target domain when the marginal label distributions are different between the source and target domains. Note that such result also holds when different feature extractors are used in source and target domains, so the model proposed in this manuscript is also subject to such inherent tradeoffs. \n\n\n[1].    Domain Separation Networks, NIPS 2016.\n[2].    On Learning Invariant Representations for Domain Adaptation, ICML 2019.\n[3].    Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment, ICML 2019.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. The approach is similar to methods that use multi-stream networks (a stream for each domain), but using the filer decomposition scheme, the authors avoid the issue of excessive increase in the number of parameters typical in fully multi-stream architectures. \n\nThis is achieved by learning a separate basis for convolutional filters for each domain while sharing the basis coefficients across domains. This encourages shared semantics across domains while maintaining a balance between the network expressiveness and the computational complexity.\n\nThe authors argue that the basis learned for each domain can be understood as correction/alignment mappings to bring together the representations of each domain. A toy example presented is convincing and shows the correction basis learned in a simple synthetic case. Theoretical arguments are provided to show that the proposed correction scheme covers a large range of possible domain shifts.\n\nThe authors also show that plugging the decomposition scheme into existing CNN based unsupervised domain adaptation algorithms results in consistent improvements across methods and datasets. \n\nHow does the method compare to approaches that learn to adapt the representations using conditional/adaptive batch norm [1,2]? \n\nOverall, the paper was well motivated and easy to read. The methods appear to be a useful addition to tools available for domain invariant learning.\n\n[1] Chang, Woong-Gi, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. \"Domain-Specific Batch Normalization for Unsupervised Domain Adaptation.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7354-7362. 2019.\n[2] Kumar, Abhishek, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, and Gregory Wornell. \"Co-regularized alignment for unsupervised domain adaptation.\" In Advances in Neural Information Processing Systems, pp. 9345-9356. 2018.\n\n"
        }
    ]
}