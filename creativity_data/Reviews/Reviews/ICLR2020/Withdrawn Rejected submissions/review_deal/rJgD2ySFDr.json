{
    "Decision": {
        "decision": "Reject",
        "comment": "There was some support for this paper, but it was on the borderline and significant concerns were raised. It did not compare to the exiting related literature on communications, compression, and coding. There were significant issues with clarity.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "\nPaper Summary:\n\nThe paper proposes to use ML methods, specifically neural networks, to learn source and/or channel coding systems, either jointly or separately. Specifically, they investigate these systems under the bandwidth-limited channel.  They investigate their models applied to the task of transferring images across the channel.\n\nPros:\n\n- The paper is in a difficult area, and needs to communicate ideas about communications and information as well as various deep learning models. The initial exposition does this well.\n\n- The paper discusses ML-based communications models applied to the bandwidth limited channel and performs experiments to investigate joint vs. separate coding. \n\nTwo main things of concern:\n\n- This area (communications, compression, coding) is highly developed, yet there are no comparisons to practical, state of the art techniques in this paper. The experiments investigate using the authors' models to send images across a noisy channel. So shouldn't we see a comparison to an equivalent non-ML-based system that currently accomplishes this task, if only for perspective? \n\n- The paper contains too much exposition and the structure makes it difficult to read.  The authors' work seems to mostly be focused in sections 4 and 6, but prior work is summarized in section 5.  The authors want to communicate a lot of ideas which the audience might not be familiar with, and this is a difficult task. However, the paper could be reorganized such that the ideas are clearer. Rate distortion is delegated to an appendix, but two pages are spent on basic communications. Overall it could be much more focused. \n\nSome smaller concerns:\n\n- Pg. 3 \"hypothesis spaces can be searched increasingly quickly in an automated fashion, allowing researchers to search...\" \nI think this statement may be over-reaching. The authors also frequently use the term \"flexible function approximation\". Neural networks do have interesting approximation properties. But this is not a complete picture of deep learning, and the situation is much more nuanced than this. Where is the role of optimization and data? In order to transmit data with the author's algorithms, do we need to go out and collect a large body of examples in the specific domain, like CelebA? Because you don't need to do that to e.g. compress any image with JPEG, code the data using LDPC to send across a channel. Is this comparing apples and oranges?\n\n- Pg. 6 \"Note, that a simple additive white Gaussian noise.... LDPC. However, in more general scenarios they do not perform as well and can be beaten by neural network architectures.....\"\n\nI think the claims in this paragraph need to be toned down a bit. LDPC does not perform well in regards to what? Can be beaten under what conditions? Decode efficiently with regard to what block length? I don't think the picture is as clear as painted here.\n\n- Pg. 4 The statements here regarding the bandwidth-limited channel appear to be the focus of the author's work. This section should be expanded and explained. Reading the first paragraph then going to the second paragraph (\"To summarize...\"), there's sort of a disconnect. How do we know these two things are equivalent? What exactly is novel that was introduced?\n\nSmall typos:\n- Pg. 2 final paragraph, some  norm is used for the distortion, but this is not defined (either on pg. 2 or in App. B)\n- Pg. 6 white is misspelled \"withe\"\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper is out of my research area. I could understand that the paper studies the message transformation with bandwidth-limited channels. It seems naturally the message transformation could be represented as a autoencoder model. The paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example VAE, etc. Technically, what's new of this paper? Was it the auxiliary variable decoders? Is it that this class of algorithms/models firstly applied to this problem domain? To be honest the paper mentioned most of the terminologies in ML and seems that the paper wanted to connect to them, for example, ELBO, VAE, GAN, re-parameterization, etc. The paper provides experimental results on the designed model for bandwidth-limited channel."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper focuses on transmitting messages reliably by learning joint coding with the bandwidth-limited channel. The authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators. Their experiments show the advantage of their design decisions via improved distoration and FID scores.\n\nPros:\n\n1. This paper is clearly written and well-structured in logic. For example, the authors use Figures 1 and 2 assist readers to catch the difference between joint communication system and separate communication system.\n\n2. This paper gives a reliazation of joint source-channel coding, especially to give auxilary latent variable decoders.\n\n3. This paper has been verified in both Gaussian channel and bandwidth-limited channel. The empirical results show the advantage of joint coding.\n\nCons:\n\n1. Intuitively, you Section 4.3 should be better than Section 4.2. However, I don't see any difference or major items to justify this kind of benefits. Could you please explain why techniques in Section 4.3 can outperform these in Section 4.2.\n\n2. Although the authors verified their work on CelebA, it seems that the proposed method has very limited applications. If possible, the authors should do more datasets to verify their proposed method, which will be more useful to boarder readers."
        }
    ]
}