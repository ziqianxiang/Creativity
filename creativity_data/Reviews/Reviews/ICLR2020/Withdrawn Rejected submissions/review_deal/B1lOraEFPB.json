{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper builds a transition-based dependency parser for Amharic, first predicting transitions and then dependency labels. The model is poorly motivated, and poorly described. The experiments have serious problems with their train/test splits and lack of baseline. The reviewers all convincingly argue for reject. The authors have not responded. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #4",
            "review": "The paper describes a dependency parser for Amharic text trained on the Yimam et al. 2018 treebank.\n\nThe proposed method is an unlabelled arc-eager transition-based dependency parser followed by a dependency label classifier. Both models are based on LSTM architectures. Unfortunately there is no clear description of these models, which makes it difficult to understand their structure.\n\nThe experimental section contains a questionable selection on the train/test split (which should have been done blindly before looking at the results) and lacks any baseline comparison with previous approaches. The authors argue that performing arc prediction and arc labeling in two separate stages is beneficial, but they don't compare with a method that perform both actions at the same time.\n\nThe motivation of the paper is also questionable: the introduction section describes some of the peculiarities of the Amharic language, such as being morphologically rich, but the proposed method seems completely generic and does not address or exploits any of these peculiarities. If the authors want to argue that Amharic would benefit from specialized parser architecture, the methods and results reported in this paper fail to provide evidence for this position.\n\nThe citation format is also broken.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "* overview \n- This paper describes a model for transition-based dependency parser, and tested on the Amharic treebank. It proposes a modification that is different from the common practice of combining the transition with dependency relation, namely to predict the transitions first, then the dependency relations.\n    \n* strengthens\n - Unfortunately, I can't find any strength in this paper. It is more like a course project than a research paper for ICLR.\n\n* weaknesses\n- The main idea of first unlabeled parsing then predicting labels is hardly novel, and it generally won't help, since the predicted labels could be useful features for determining the transitions. There is also no experiment to compare the proposed model to normal parsing model with 2n+2 predictions to support the author's claim.\n- Generally, the architecture of the model is poorly explained. For example, it seems that the \"embeddings\" for the words are just one-hot index vectors (since the number of dimensions are the same as the vocabulary size), which is not what we mean by embeddings. It is also unclear what the dot product of the embeddings means. The attention layer is again not explained at all, e.g. what are the input and output.\n- The splitting of train/test set does not make sense, the selection should not be influenced by the results at all. There is no reason not to use the standard split in UD. \n- I don't see what is the challenge in the discussion section, since UAS is by definition lower than LAS.\n- There numerous typos and grammar mistakes, and the citation format is broken.\n- In many places, the quoted presumably Amharic words are not shown."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper claims to build a transition-based dependency parser for Amharic. To produce a parse tree, the paper takes a two-stage approach, first producing the structure of the tree, followed by annotating each individual arcs on the tree. The paper also explores attention and finds it useful for improving the performance.\n\nI am giving a score 1, because the paper lacks precision and the contribution is not significant.\n\nThe major problem of the paper is precision. There is a lack of formal descriptions of the parser, e.g., the representation of the input and output, what each individual block in the figures are doing, how the arc-eager transition parser works. The text description is certainly helpful, but without formal descriptions, it is impossible to reproduce the results in the paper. Besides, without setting up a precise language/notation, the paper is not easy to follow.\n\nIn terms of contribution, the paper begins with the motivation that Amharic has certain language characteristics that are absent in languages, such as English, where most research effort has been spent. However, the paper still uses a parser for English, and does not utilize the special features of Amharic. Generic architectural modification, such as adding attention, has been explored, but this has little contribution on its own and has little to do with Amharic.\n\nIn addition to the above, other minor weaknesses of the paper includes typos, poor experimental practices, missing citations and related work, and other wording issues.\n\nIn particular, choosing the split of the data set that achieves the best result, as quoted below, is concerning.\n\n\"We experimented on 60/40, 70/30 and 80/20 train-test splitting ratios ... Therefore, we select 70/30 train-test splitting ratio throughout the experiments.\""
        }
    ]
}