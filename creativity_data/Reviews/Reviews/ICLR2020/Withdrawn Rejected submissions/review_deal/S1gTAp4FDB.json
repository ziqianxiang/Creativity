{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes 1) using neural-guided Monte-Carlo Tree Search to search for expressions that match a dataset and 2) Augments the loss to match the asymptotics of the true function when these are given.\n\nThe use of MCTS sounds more sensible than standard evolutionary search.  The augmented loss could make sense but seems extremely niche, requiring specific side information about the problem being solved.\n\nOverall, the task is so niche that I don't think it'll be of wide interest.  It's not clear that it's solving a real problem.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\n\nThis paper introduces the use of asymptotic constraints to find\nprune the search space of mathematical expressions for symbolic\nregression.  This is done by training a neural network to\ngenerate production rules conditioned on being given the\nasymptotic constraints and previously generated production\nrules. This neural network is then itself also used to guide a\nMCTS to generate mathematical expressions.\n\nThe algorithms is compared against a reasonable set of baselines\nand related algorithms.\n\nFeedback:\n\nThis is a very clear and well-written paper. It was\nstraightforward to understand and very easy to see how it fits in\nwith the broader literature. The insight about using asymptotic\nconstraints makes the result a bit limited to only generating\nmathematical expressions, and it would have been a bit nicer if\nthere was something more generically applicable to program\nsynthesis in general. It's not really clear to me how the\nexisting work extends to programs.\n\nThe evaluation was very thorough and the appropriate algorithms\nwere compared against the work. I came away with a good\nunderstanding of how well the model generalizes to larger\nexpressions compared to existing work.\n\nMinor notes:\n\nThe abstract is a bit inaccurate as the NN generates production rules\nand not expressions."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\nThe authors of this paper propose a novel approach for symbolic regression. The simulation results demonstrate that the proposed approach can find a better function g producing similar results as desired function f by providing the additional information – the leading power of function f.\n\nPaper strength:\n1.\tThe proposed NG-MCTS is elegant to solve the problem of symbolic regression.\n2.\tExperiment results illustrate the superiority of the proposed approach\n\nPaper weakness:\n\n1.\tI can follow most of the mathematics in the paper. But the most confusing part for me is why you feed the random partial sequence for training. Besides, how you do inference to generate a sequence of the production rules.\n2.\tWhat is the final objective function? If I do not misunderstand, it could be the cross-entropy loss between the output of GRU and the next target production rule, RMSE and the error on leading power. Then how you optimize it? Please describe more details about this.\n3.\tThe authors of this paper introduce more information – leading power of desired function for symbolic regression but they incorporate the additional information by introducing a simple loss function term. How about the performance of baseline approaches with those kinds of information?\n4.\tThe whole systems seem like very complicate and it would be more interesting if the authors provide sufficient ablations to decompose the complex algorithm.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper considers a task of symbolic regression (SR) when additional information called 'asymptotic constraints' on the target expression is given. For example, when the groundtruth expression is 3 x^2 + 5 x, it behaves like x^2 in the limit x -> infinity, thus the power '2' is given as additional information 'asymptotic constraint'. In the paper's setting, for SR with univariate groundtruth functions f(x), 'asymptotic constraints' for x-> infinity and x -> 0 are given. For this situation, the paper proposes a method called NG-MCTS with an RNN-based generator and MCTS guided by it to consider asymptotic constraints. In SR, a learner is asked to acquire an explicit symbolic expression \\hat{f}(x) from a given set of datapoints {x_i, f(x_i)}, but unlike the parameter optimization aspect of a standard supervised ML setting, the problem is essentially combinatorial optimization over exponentially large space of symbolic expressions of a given context-free grammar (CFG). For a given symbolic space with a prespecified CFG, extensive experimental evaluations are performed and demonstrated significant performance gain over existing alternatives based on EA. Also, quantitative evaluations about extrapolative performance and detailed evaluation of the RNN generator are also reported.\n\nThough it includes a lot of quite interesting methods and results, the paper has two major issues: \n\n(1) the proposed method NG-MCTS explicitly uses the fact that the target expressions are generated from a CFG, but this assumption sounds unnatural if the target problem is SR (unlike the case of GVAE). Apparently, all results except a toy case study in 5.2 depend on artificial datasets from a CFG, and any practical advantages or impacts are still unclear because the experimental settings are very artificial. \n\n(2) the most important claim of this paper would be the proposal to use 'asymptotic constraints', but the availability of this information sounds too strong and again artificial in practical situations. A one-line motivation saying that 'this property is known a priori for some physical systems before the detailed law is derived' is not convincing enough."
        }
    ]
}