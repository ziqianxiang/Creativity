{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Successor Representation/Features, decomposing the value function into a policy dependent state density function and a weighting over states is an old and powerful idea that has attracted renewed interest recently.\n\nHowever, a weakness of successor features has been that that the policy dependency means that this decomposition only transfers well to tasks which are similar to the training tasks.\n\nHere the author's propose a ``state-independent'' approach to learning features by sampling trajectories under a random policy and learning a state embedding (in a similar way to node2vec) such that discounted state density is predicted by the embeddings (eq 15). These state embeddings are then used as features to linearly predict the value in subsequent tasks.\n\nThe primary weakness of this work is lack of a significant contribution. The features are not really ``policy independent'', rather they are features learned under a random policy (which can be a good way of recovering a representation which maps the state topology well). SFs under random policies are not a new idea (e.g. [1]) and the approach used here is very similar to Madjiheurem & Toni (2019) which is cited and, as noted in the paper, node2vec. The addition of discounting the states is not a significant addition.\n\nThis lack of novelty along with the relatively small scale environments that is tested on lead me to conclude that this work may be of only limited interest.\n\nThere is a number of errors and notational issues in page 4/5 which make it hard to follow the method they are using. The notation $w$ is both used to denote a vector, and also to index over previous policies which is confusing.\n\nEquation 8 should say $w'$ for the final $w$ and the $max_w$, indexing over prior policies, is confusing notation. This notational confusion follows to eq 11 and eq 12 and makes this section difficult to understand. Why is there a maximisation over $\\theta_{w'}$, without constraints on $\\theta_{w'}$ wouldn't this be unbounded? Later, it appears (and makes more sense) that $\\theta_{w'}$ is being fitted to the state-action value (eq 16). Why is $\\psi(s,a)$ defined in terms of an expectation over tasks (what does this even mean), later $\\psi$ is trained in a task independent way. Essentially, section 3.1 seems much more understandable, and I struggled to understand the preceding section 3.\n\nMinor issues:\n\nThe paper would be improved by careful copy-editing (including node2vec being misspelled in the abstract).\n\n\nPage 2 the claim that ``this is of obvious interest as this formalism potentially model[s] real life application'' does not seem well-justified. How ``real world'' the successor feature assumption is non-obvious.\n\n[1] Stachenfeld, Kimberly L., Matthew M. Botvinick, and Samuel J. Gershman. \"The hippocampus as a predictive map.\" Nature neuroscience 20.11 (2017): 1643."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The author's attempt to define off-policy successor features, but end up rediscovering successor features during a fixed (uniform?) policy. They demonstrate the utility of this on the four rooms domain, outperforming the unrelated node2vec algorithm.\n\nThis paper should be rejected.\n\nThe algorithm isn't really novel, it is a nearly equivalent to the successor features under a fixed policy (discounted expected feature occupancy), but with the strange choice of being recursively defined in terms of /Psi rather than /Phi.\n\nThere are no relevant baselines. SF under a uniform policy at minimum, UFSA with all greedy training policies ideally.\n\nThe four rooms environment is also far too trivial for being the sole experiment in a theory-less paper. Why not use the environments discussed in all of the other SF literature?"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper tackles the problem of off-policy successor feature learning, with the aim of using these successor features to quickly learn new tasks.  The paper proposes a method and evaluates it on a simple gridworld domain.\n\nAs is, the method proposed by the paper is unclear and the experiments insufficient to merit publication.\n\nI found Section 3 especially hard to follow:\n-- How do you take an expectation of policies from \\mathcal{M}?  Where are you getting these policies from?\n-- The method is stated as off-policy, but Eq 13 seems to be on-policy w.r.t. pi. \n-- Equations 13/14/15 don't make much sense. For example, Eq 15 does not describe a normalized probability distribution.\n-- A pseudocode would be helpful.\n\nIn addition, the experiments should be extended to more interesting domains. On the gridworld that is evaluated, many simple techniques can work very well."
        }
    ]
}