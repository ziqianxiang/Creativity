{
    "Decision": {
        "decision": "Reject",
        "comment": "The reviewers agree that the technical innovations presented in this paper are not great enough to justify acceptance.  The authors correctly point out to the reviewers that the ICLR CFP states that the topics of \"implementation issues, parallelization, software platforms, hardware‚Äù are acceptable.  I would point out that most papers in these spaces describe *technical innovations* that enable improvements in \"parallelization, software platforms, hardware\" rather than implementations of these improvements.   However, it is certainly true that a software package is an acceptable (although less common) basis for a publication, provided is it sufficiently unique and impactful.  After pointing this out to the reviewers and collecting opinions, the reviewers do not feel the combined technical and software contributions of this paper are enough to justify acceptance. \n ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe authors present a PyTorch based framework for performing second-order\nreverse mode autodiff for meta-learning.\n\nFirst, the authors present a formalization of a general prototypical\nmeta-learning setting.\nThey then provide an algorithm that solves this problem via gradient based\noptimization.\nFinally, perhaps the main contribution is a specific PyTorch implementation\nof said algorithm.\n\nThe type of meta-learning setting the authors consider is one where a gradient\nbased inner loop optimizer finds $\\theta^\\star$ by performing a finite number of steps.\nThe inner loop optimizer is parameterized through $\\varphi$ that consists of\ntwo parts $\\varphi^\\text{loss}$ and $\\varphi^\\text{opt}$.\nThe parameters $\\varphi^\\text{loss}$ are somehow part of the loss used for\ntraining in the inner loop. Example: Regularization paramter.\nThe parameters $\\varphi^\\text{opt}$ do not occur in the loss but in the\noptimizer step. Example: Learning rate.\n\nExample of an inner loop step:\n$\\theta^{k+1} := \\theta^k - \\alpha (\\nabla L(\\theta^k) + \\lambda \\nabla R(\\theta^k))$\nwhere $\\theta$ are the parameters of a neural network, $L$ is the training loss,\n$R$ is the regularizer, $\\alpha$ is the learning rate, $\\lambda$ is the regularization parameter.\nIn this example we would have $\\varphi = (\\alpha \\lambda)^T$.\n\nThe authos assume $\\theta^K$, the output of the inner loop after $K$ steps to\nbe differentiable wrt $\\varphi$.\nFurthermore, the meta-learning loss is assumed to be differentiable wrt $\\theta$\nso that a gradient of the meta-learning loss wrt to $\\varphi$ can be computed.\nThe authors also assume the meta-learning loss to be sufficient smooth in $\\varphi$\nsuch that a gradient based optimization can even be used for meta learning to\na local optimum.\n\nThe authors explicitly write down the reverse mode auto differentiation of\nthe inner loop and show how to, in that way, compute the gradient of the\nmeta-learning loss wrt to $\\varphi$.\n\nThe reverse (adjoint) mode auto differentiation of the above example inner loop step\nis the following step (iterated over in reverse down from $k = K$ to $k = 1$:\n$\\bar \\theta^k := (I - \\alpha (\\nabla^2 L(\\theta^k) + \\lambda \\nabla^2 R(\\theta^k)))^T \\bar \\theta^{k+1}$\n$\\bar \\alpha := \\bar \\alpha - (\\nabla l(\\theta^k) + \\lambda \\nabla R(\\theta^k))^T \\bar \\theta^{k+1}$\n$\\bar \\lambda := \\bar \\lambda - \\alpha \\nabla R(\\theta^k)^T \\bar \\theta^{k+1}$\nwhere $\\bar \\alpha$ accumulates the gradient of $\\theta$ wrt $\\alpha$ and\n$\\bar \\lambda$ accumulates the gradient of $\\theta$ wrt $\\lambda$.\n\nThe authors give some implementation details specific to some frameworks necessary\nfor implementing such \"gradient of an inner loop\".\n\nThe authors present experiments where they show how to meta-learn learning rates\nwith their framework.\nThey also how their framework can be used to quickly implement a MAML type\nmeta-learning optimizer ablation study comparing various combinations of\narchitecture, optimizer and inner loop steps etc..\n\nRecommendation:\nI propose to reject the paper.\nIn my eyes the only contribution is the implementation of a meta-learner in\nPyTorch based on well known methods.\nThe provided unifying formalization is theoretically inaccurate (see below)\nand to me come across as merely a motivation for their framework\n(but no value added compared to existing literature).\nThere is no new insight provided on the software engineering level either as\nfar as I can see.\n\nDetailed comments:\n- Page 1: ...provides tooling for analysing the provable requirements...\n\tseems like a complicated way of saying something that could be said simple\n\n- Page 2: Without loss of generality, ...\n\tYou are assuming a parametric model. Not sure what generality this phrase\n\trefers to.\n\n- Page 2: A formalization as $\\theta^\\star = argmin(\\theta, L(\\theta, \\varphi))$\n\tis inaccurate in the sense that it does not acknowledge the existing of\n\tmultiple optima.\n\tI would recommend not to use the $argmin$ operator here, since $argmin$ for\n\tsomething like a neural network would for example either return a global\n\toptimum (which no optimizer used in practice finds, and is not ment here)\n\tor would take on a set value with multiple local minima for example.\n\n\tIn the same context, the authors should mention the issues about uniqueness\n\tof optima (we are not even really finding optima when training neural networks),\n\timplicit functions / implicit differentiation\n\n\tIn the context of using stochastic optimizers one should also at least\n\tmention something about the differentiability of outputs of such optimizers\n\tand how they potentially depend on randomness of mini-batches\n\t(what if different randomness is used with the same or a perturbed\n\thyper parameter?)\n\n- Page 3: You mention the potential statefulness of the optimizer.\n\tWhy not explicitly carry it in the math notation?\n\tProbably things would get cluttered but saying it should be covered within\n\t$\\varphi$ does not seem reasonable to me.\n\n- Page 3: While this may seem like a fairly trivial formalization...\n\tYes, but also nesting this in an outer loop is fairly trivial in the sense\n\tthat it is a well known approach.\n\n- Page 4: there exist continuous hyperparam...\n\tIf they are not continuous then they should not even occur in this\n\tformalization so saying there exist... does not make much sense to me here\n\n\t$\\alpha \\subseteq \\varphi^\\text{opt}$ implies that $\\varphi^\\text{opt}$ is\n\ta set from notation although we are treating it as a vector everywhere else\n\n- Page 4: All of section 2.4 seems somewhat trivial to me, but I guess that is\n\thighly subjective.\n\n- Page 5: in the definition of stop operator perhaps use $:\\Leftrightarrow$\n\n- Page 5: Perhaps explicitly mention how your approach differs from a reverse\n\tmode differentiation of training or if it does not differ, say this.\n\n- Page 14: When talking about _S_GD (instead of just GD) perhaps mention\n\tsomething about non-existence of mini-batch randomness / being deterministic\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This work presented a general formulation of a wide class of existing meta-learning approaches, and proved the requirements that must be satisfied for such approaches to be possible.\n\nHalf of the work is focused on describing the unnamedlib library, which extends PyTorch to enable the easy\nand natural implementation of such meta-learning approaches.\n\nThe early sections are interesting, especially section 2, which gives some great insights to the existing inner loop pattern in meta-learning. However, from section 3, the paper has turned to examples and related works, where I was hoping the author would give more detailed analysis of the pattern. My concern is the authors have spent too much space on the unnamedlib library. So http://www.jmlr.org/mloss/ might be a more suitable place for publication."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The authors propose the general formulation of recent meta-learning methods and propose a good library to use.\n\nPros:\n1. The general formulation of recent meta-learning methods is reasonable.\n2. The proposed library is easy to use.\n\nCons:\n\nThe paper lacks technical novelty. I understand the goal of this paper is to build a library. However, the paper only describes a general formulation for recent meta-learning methods (e.g., MAML) and implement the formulation. It is better to clarify and some key engineering challenges and do the corresponding experiments.\n\nIn addition, in the experiment parts, the authors only compare the results with MAML++. It will be more convincing if the authors can analyze other popular meta-learning methods (e.g.. Prototypical network [1], meta-LSTM [2]). \n\nAnother suggestion is that the authors can give some examples to connect current meta-learning models with the proposed general formulation. For example, the meaning of \\phi_i^opt, \\phi_i^loss in MAML, Prototype, Reptile, etc.\n\nIt is better to explain the meaning of different colors in Figure 3.\n\n[1] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" Advances in Neural Information Processing Systems. 2017.\n[2] Ravi, Sachin, and Hugo Larochelle. \"Optimization as a model for few-shot learning.\" ICLR (2016).\n\n\n\nDecision after rebuttal: I have read the authors' responses. Like review 2, I also think the \"generalization\" is overclaimed, it only provides a general formulation. Thus, I finally decide to keep my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}