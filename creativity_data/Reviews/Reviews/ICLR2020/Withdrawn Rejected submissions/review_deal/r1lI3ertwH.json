{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper is focused on simplifying the use of larger datasets (via pretraining models) for the purpose of transfer learning onto smaller domains/datasets. An alternative view of this paper is that it is focused on a more privacy-friendly manner of doing data selection in a client-server manner.\n\nIn particular the paper proposes an interesting client-server architecture which allows for servers to potentially hold on to large datasets and have pretrained models. On the other hand, clients can leverage these models while sending minimal information to the server so as to get the server to return a subset of the data -- which the client can in turn use for pre-training / joint training. \n\nIn this case the proposed technique is composed of a few steps: On the client side, the data is partitioned into a few clusters from which pretrained models are trained. Next these models are shared with the client, and then used to \"adapt\" the model (e.g. one additional layer on top of the pretrained model) to figure out the best performing (pretrained) model (and thus effectively the best server data clusters). Lastly data from these good-performing clusters can be appropriately sampled and sent to the client.\n\nOn the plus side I liked this vision of a server-client manner of interacting and pulling datasets. The basic skeleton of the overall infrastructure also makes sense to me.\n\nThat said I had a few concerns which make me believe that the paper could do with more work and experiments before it can realize its potential impact. In no specific order:\n\n- In general I would have wanted a far more nuanced understanding of the efficacy of the proposed transfer learning / data selection methodology. There have been numerous works in this domain (not just restricted to vision) and it felt like there wasn't really any comparison with any of the more common approaches to the problem.\n\nFor example: One common family of approaches to performing such data selection is to run some clustering or PCA-like dimensionality reduction and then find clusters in the larger dataset closer to the clusters / basis vectors of the target set.\nAnother set of techniques directly work in a common embedding space to find similar data points\n\nWhy wasn't any such approach considered / discussed?\n\nThere are also many interesting \"active\" learning style approaches to the problem which allow for you to iteratively select data based on performance on some available target dataset. Those too would be valid in this setup and fair comparison points right?\n\n- The evaluations also felt discombobulated from the motivation/exposition of the approach. The paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task -- the two datasets weren't even combined! To me the ability to combine different datasets was something that held significant appeal about the problem and the paper and I really would have wanted to see that showcased / some positive evidence towards the same.\n\n- On a somewhat related note, the paper motivated by saying that some of these datasets are so large that clients cannot afford to download them or pretrain on them. If that is the case is 20-40% really going to be that different? Again given the motivation in the paper I would have liked to see some deeper analysis on this.\n\n- Significance testing is a key empirical practice and one I would request the authors to add.\n\n- It also felt that the paper's exposition and techniques were somewhat unclear -- they seemed to be focused on classification tasks (e.g. the superclass partitioning) but then were trying to generalize to non-classification problems without satisfactory explanations of how these approaches would generlize\n\n- On a more minor note, I felt the discussion in the paper is very specific to vision tasks since language understanding tasks have very different trends and techniques (e.g. BERT -- where more pretraining data only helps the model). I would actually try to clarify this scope accordingly earlier in the paper.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors propose a novel method that aims to optimally select subsets of data in various transfer learning scenarios and demonstrate state-of-the-art performance. \n\nI thought this paper tackles an interesting problem but the current version needs to be improved.\n\nHow does the proposed method handle rare classes/instances in target domain or client dataserver?\n\nAuthors mentioned sensitive data set such a hospital record. Biomedical dataset often show variations, for instance, histology images have intensity variations across institutions or scanner. How does this approach work when you have unwanted variation between a centralized database and dataserver-client?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper focuses on transfer learning from large source server data to target client data. The server does not have access to the target data due to privacy and the proposed method finds a useful subset of source data to pretrain a network as initialization for subsequent target training. Since the source and target data may have different label spaces, self-supervision by classifying image rotations is used as a proxy for target set performance. The problem is interesting and important, but the current version is not satisfactory, especially the writing, which can be greatly improved.\n\nFor the partitioning schemes described in Sec. 3.2.1, it is not clear how the input images with different sizes can form the feature vectors of the same length. As specified in the text, the source dataset contains images from various domains and thus may have different image sizes and label space.\n\nAccording to Sec.3.2.1, the accuracies z_i is between 0 and 1 already, does this mean the normalization step in Sec.3.2.2 is not necessary? The temperature is set as T=0.1, why? How sensitive the system to this parameter?\n\nWhy Eq.(1) is equivalent to Eq.(8)? How does the assumption help? A rigorous derivation would be helpful.\n\nExperiment\n- Fig.3 is not very convincing. Only one target dataset does not mean a lot about their correlation. What is the correlation coefficient?\n- How many experts are used for each setting (the k in k-means)?\n- The work of Cui et al. (2018) is mentioned in the Related Work but not compared in the experiment. It would be more informative if the empirical comparison is provided.\n\nWriting and presentation\nGrammar\n- The very first sentence in the main text has grammar errors: In (the) recent years, we have seen (an) explosive growth in both the number and variety of A.I. applications.\n- Commas are excessively used to a point that parsing can be difficult at places. For example, in Section 3.1, \"Generally, we will assume that multiple tasks, each â€¦, are available, and denote this by Y\". The tasks are denoted by Y?\n- We follow (Gidaris et al., 2018) which *is* demonstrated.\n- We define its corresponding label \\hat{y} by XXX. Without looking at Gidaris et al. (2018), it is not clear what the label is.\nPresentation\n- The gating function in Eq.(2) is not accurate: there is a missing index i.\n- y_j in Eq.(4) is not defined. The \\mathcal{L} takes two arguments in Eq.(3) but only one argument in Eq.(4).\n- Eq.(5) is also confusing. What is the output of the expert network, i.e., e(r)? Is it a scalar indicating the rotation, or a four-dimensional vector showing the probabilities of being each angle? Assuming it is a scalar, does {e(r(x,j))}_{j=0}^3 mean the set of outputs of four images? Why taking argmin over images? Is [] regular bracket or indicator function? j is abused too much here.\n- Sec.3.3.2, z_i is a scalar so should not be bold (also w_i); p is a scalar in (6).\n"
        }
    ]
}