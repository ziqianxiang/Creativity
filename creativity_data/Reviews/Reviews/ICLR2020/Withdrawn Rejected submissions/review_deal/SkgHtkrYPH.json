{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper aims to make inference of convNets on mobile (and other such lower power) devices a) faster,  b) use less flops, and c) with reduced model size, d) and yet with higher overall classification accuracy. It focuses on improving improving three existing models namely MobileNet V1, MobileNet V2 and EfficientNet although the same approach can be used more generally on other models. It sparsifies the networks using the approach of (Zhu & Gupta, ICLR 2018) with a few additional details, tricks (which layers to sparsify, etc). The main new contribution is the implementation of new kernels for sparse matrix (times) dense matrix multiplication and libraries for running sparse models training with their model pruning library in TensorFlow. \n\nUnfortunately while they briefly allude to important ideas for the sparse matrix*dense matrix multiplication, much of the detail necessary to understand and reproduce this is missing from the main paper (they promise code). I suggest reducing the introduction and related work sections in order to expand on the description of their main contributions in much more details. In effect I am currently unable to comment in detail on that contribution, its novelty etc. To me, this is the main area where the paper needs improvement, but it is so critical that unfortunately it significantly impacts my scores\n\nExperimental results seem quite interesting. They show significant reduction in flops (3x) and model parameter counts (2x). Inference time also improves but to a much lesser extent (1.1-2x). While this work focuses on mobile phone or similar processing environment it would also be useful to understand and explicitly highlight impact when processing on a GPU (if nothing else, to highlight that the work does not aim to address this use case). \n\nThe experimental set up itself is quite sufficient for ICLR and shows a good amount of data and analysis to convince at least this reader about the value in their specific use setting on mobile phones. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper empirically shows that using weight sparsity to accelerate state-of-the-art convolutional networks is possible by providing fast kernels ARM and x86 processors. Overall the intuitions are reasonable and the experiments show that the methods and rules of the kernel operations are providing some advantages. This ideas used in the paper doesn't seem to be novel, however, I am not knowledgeable enough to rebut or provide existing work references. The paper doesn't compare the sparse results with other sparsity methods."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Training energy-efficient deep neural networks is of critical importance to improve inference efficiency, especially on mobile and edge devices where the resource budget is highly limited. This paper presents an efficient implementation of sparse 1x1 conv kernels that leads to significant improvements in various real-world hardware settings. I believe the proposed tool would definitely encourage more research in developing more fine-grained network pruning approaches.\n \nFirst of all, I feel like the merits of this paper would be better appreciated for a computer architecture related venue. \nItâ€™s kind of intuitive to me that fine-grained weight-matrices sparsity would lead to energy-efficiency boost with dedicated accelerators. As an outsider from a more machine-learning like community, I personally would be most interested in techniques for training block sparsity-induced networks, which, however, are largely based on previously published work.\n \nAdditional related references you might find interesting:\nYao et al., Balanced Sparsity for Efficient DNN Inference on GPU. AAAI. 2019.\nCao et al., Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity. FPGA. 2019.\n \nAgain, I feel like the results are super exciting. However, I am leaning to vote rejection as the paper lacks of ML-related contribution. \n"
        }
    ]
}