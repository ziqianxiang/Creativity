{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper discussed adversarial training for robust model against strong evasion attack such as PGD. The main idea is to combine single-step adversarial example generation FGSM with dropout in multiple layers, and the authors find this strategy can train models as robust as PGD adversarial training.\n\nI have some comments\nFirst, the paper is unnecessarily long. The method is rather simple, and there is no rigorous theoretical analysis. For example, section 2 seems can be shortened and merged to other sections.\n\nSecond, the claim “...learns to prevent the generalization of single-step adversaries...” on page 2 looks rather confusing. What does “generalization of single-step adversaries” mean? In figure 1, is  the same method (FGSM) used to generate adversarial examples for training and validation? The FGSM adversarially trained robust model should be able to defend against FGSM attacks. How could you explain the big gap in Fig 1 last column?\n\nOn page 6, the hyper-parameters P_d and r_d seems to be well-tuned. What is the insights from these various parameters for different settings?\n\nIn almost all the experiments, the proposed method is actually worse than PGD adversarial training under the strong PGD attack.\n\nPlease discuss the following related work\nAdversarial Training for Free. https://arxiv.org/abs/1904.12843\nYou only propagate once. https://arxiv.org/abs/1905.00877\n\nDiscussing a concurrent submission will be a plus \nFast is better than free https://openreview.net/forum?id=BJx040EFvH"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper first analyzes the gradient mask effect caused by naive single-step adversarial training. Then it proposes the SADS method to improve the robustness of the single-step adversarial training by adding the dropout mechanism. The evaluation in the experiments is appropriate and reasonable.\n\nHowever, the critical point that prevents me from accepting this paper is that there is a severe degradation on clean accuracy when applying SADS. For example, in Table 3, the clean accuracy of SADS on CIFAR-10 is only 75%. So I doubt that if SADS can scale to a larger dataset like ImageNet, since the degradation on the clean accuracy may be more serious.\n\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary\n========\nThis paper investigates why single-step adversarial training leads to gradient-masking, as shown in \"Ensemble Adversarial Training\" by Tramer et al. The authors suggest that this is due to overfitting, and propose a new type of single-step adversarial training method coupled with dropout, which appears to achieve comparable robustness to more expensive multi-step training methods.\nOverall, I find the findings over prior work to be somewhat limited, and the experimental evaluation is also not the most convincing. I thus lean towards rejection.\n\nComments\n=========\nInvestigating why FGSM adversarial training leads to gradient masking is an interesting question.\nHowever, I'm not convinced that the experiments conducted by the authors give fundamentally new insights into this question. In particular, I'm not sure that \"overfitting\" is really the right characterization for what's happening.\n\nThe fact that the metric R_\\eps proposed by the authors satisfies R_\\eps < 1 was already hinted at in the work of Tramer et al. Specifically, Tramer et al. observe that for a model trained with single-step methods, the FGSM update step is less adversarial than a fully random step.\nThe interesting observation here (Figure 1) is that the FGSM adversarial examples are initially good, but then perform worse as the model trains. However, I don't think that characterizing this effect as overfitting is necessarily correct here, as the training data and validation data do not come from the same distribution anyways.\nThe authors' experiment with the validation loss shows that as training progresses, the model becomes more vulnerable to adversarial examples transferred from another model. This high transferability was also observed by Tramer et al., and seems to have a simpler explanation: after the model \"learns\" to mask gradients, the FGSM examples it produces are not too different from the clean examples. So once gradient-masking kicks in, the adversarially trained model is trained similarly to the model that is the source of the transferred examples, which explains the increase in transferability.\n\nThe combination of FGSM adversarial training with full-layer dropout is still interesting to consider.\nYet, I do find some of the experimental results a bit hard to believe:\n\nFor example, Figure 3 seems to suggest that FGSM + standard dropout (FAT-TS) achieves about 30% accuracy against PGD on MNIST. This seems clearly wrong. E.g., one of the models in the \"Ensemble Adversarial Training\" paper was trained in this way and did not achieve meaningful robustness against PGD.\nThe step-size and number of steps chosen for PGD seem problematic here. With 40 steps of size 0.01, you can at most make a per-pixel change of size 0.4. Yet, because of the random start, every pixel could potentially be 0.6 away from its optimal value.\n\nSome numbers in Tables 1,2,3 are also suspicious:\n- In Table 1, EAT models A and B achieve non-trivial robustness to PGD and IFGSM. This suggests that the attacks are too weak. These models are not robust to iterative attacks. The same holds for models B & C in Table 3.\n- In Table 2, FAT and EAT model D also achieve non-trivial PGD robustness which is surprising.\n\nWhile the evaluation is fairly thorough, many of the evaluated attacks are redundant. E.g., FGSM can be dropped altogether, while IFGSM, MI-FGSM, and PGD are essentially all variants of each other. Adding a gradient-free / hard-label attack would be good practice.\n\nFor the results in Table 6, were these obtained with models trained against l2 attacks?\n\nI don't understand what Figure 6 is supposed to demonstrate that can't already be inferred from Figure 5. Isn't Figure 6 just a zoomed-in and inverted portion of the graph in Figure 5?\n\nFinally, with regards to time complexity (Section 4.5), the authors should consider recent works that show that multi-step adversarial training can be accelerated significantly by recycling gradient information.\nE.g., \"Adversarial Training for Free!\" by Shafahi et al. or \" You Only Propagate Once\" by Zhang et al."
        }
    ]
}