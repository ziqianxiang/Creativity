{
    "Decision": "",
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This study introduces a simple regularisation scheme that enforces an alignment between the logit distribution of two samples from the same class and/or augmented samples. The results demonstrate substantial gains in accuracy across several data sets compared to other output-based regularisation schemes.\n\nThe manuscript is well written, the story is clear and straight-forward, and the amount of experiments is extensive. I only have a few comments:\n\n* Please reference the logit-pairing paper [1] which uses a similar idea (encouraging logits from the same class to be similar) but uses a slightly different formulation and only evaluates it in the realm of adversarial robustness. Nonetheless, I believe this is an important related work to cite.\n* I did not find any comparison to simple L2 weight decay. I understand that weight decay is not an output regularisation, but most readers would still be interested (maybe add it to Table 2?).\n* How important is the regularisation on augmented samples relative to the regularisation based purely on the different samples from the same class?\n* It is unclear which networks you have used in table 2 and 3.\n* Assuming that you used ResNet-18 in table 2, I am wondering about a discrepancy between the values you report for mixup: in the original paper mixup decreases the error of ResNet-18 on CIFAR-100 from 25.6% to 21.1%, while you see a decrease only to 23.28%. Could you please explain the difference?\n\n[1] Adversarial Logit Pairing, Harini Kannan, Alexey Kurakin, Ian Goodfellow"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to regularize deep neural networks for image classification using a consistency loss (KL distillation) between images of the same class and between augmentations of the same image. The intuition is simple: the output distribution over classes for images should roughly match when they are (i) in the same class and (ii) two augmented versions of the same image (random rotation / crop / color jittered) etc. The experimental results seem to confirm the effectiveness of the proposed method wrt to recent baselines.\n\nThis is a well written paper but, in its current form, it isn't above the acceptance bar: (i) the idea of using consistency terms as regularizer is not novel and related works on consistency losses have not been adequately cited; (ii) the experiments are in general well executed but are not quite comparable with the state-of-the-art.\n\n1) About consistency as regularizer:\n1.1) Consistency loss as a regularizer dates at least back to https://arxiv.org/pdf/1412.4864.pdf (NeurIPS '14, not cited) and https://arxiv.org/pdf/1703.01780.pdf (NeurIPS '17, not cited). Consistency across corrupted / augmented views has been used in a plethora of domains, not only in vision but also NLP (https://nlp.stanford.edu/pubs/clark2018semi.pdf, not cited).\n1.2) Sampling images from the same class as a form of corruption seems novel to me but I feel like it could be dataset specific ? What are the limits of the approach ? That could give more depth to the paper.\n\n2) About the experiments:\n2.1) For CIFAR-100, mixup reports an error of 21.1%, (Figure 3 in https://arxiv.org/pdf/1710.09412.pdf) while you report 23.28%. Why didn't you use the same architecture ?\n2.2) Again for CIFAR-100, there exists another method manifold-mixup which reports 20.3, (Table 1(b) in http://proceedings.mlr.press/v97/verma19a/verma19a.pdf), while your best score is 21.51. Do your gains transfer to comparable architectures as per the mentioned paper ?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\n\nThis paper proposes a regularization method that forces two samples within a class to have the same output distribution.  The paper also adds sample-wise consistency by augmenting an input data and forcing the output distributions of  original and augment data to be similar.  The idea is to minimize a KL-divergence between two distributions.  The proposed method applies the cross-entropy loss with these two additional regularization terms.  Experiments show that the proposed regularization method works better for many image datasets compared with AdaCos, Virtual-softmax, Maximum-entropy, mixup, and the simple cross-entropy baseline.  The proposed method has the nice property that the output is confidence calibrated compared to other methods, which is also shown in the experiments.\n\n\nPros:\n\nThe proposed method is extremely simple and easy to understand.  The experimental results demonstrate that the proposed ideas are meaningful.\n\n\nCons:\n\nIt would be better to explain the motivation behind forcing the output distributions to be similar among samples within the same class.  This somehow seems to add a strong assumption on the data structure.  For example, if two classes p(x|y=1) and p(|y=2) have some distribution overlap, samples closer to the decision boundary should have a lower underlying temperature but samples far from decision boundaries should have a higher underlying temperature.  The proposed method forces these two samples to have similar \\hat{p}(y|x) even though the true p(y|x) for these two samples are not similar.\n\nThe confidence calibration property of the proposed method is nice.  It would be interesting to have some discussions on why this happens.\n\nCurrently the paper fixes hyper-parameters related to the proposed method in experiments, e.g.,T=4 and \\lambda_sam = 0 for conventional tasks.  Since this is the first paper to propose class-wise self-knowledge distillations, it would be better to have an ablation study, e.g., different temperatures and different hyper-parameter setups, and when only having one regularization term at a time instead of having both, so that readers can have a better understanding of the behavior of CS-KD.\n\n\nOther very minor comments:\n\nIn 2nd paragraph of Section 4, citation link is missing for Srinivas & Fleuret (2018) and the last citation in this Section is also missing a link.\n\nIn Reference section, Pereyra et al. 2017 is cited as an ICLR paper, but is this a workshop paper?\n\nIn Reference section, citation for the VAT paper combines two authors into one, and the order of authors seems to be different.\n\n"
        }
    ]
}