{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Motivation: The authors propose the use of polynomials as the choice of activation function.\n\nClarity: The authors does not give a very clear introduction why using polynomial functions as activation functions is a good idea. Rather they say, \", we believe that the nonlinearity learned by the deep networks can provide more insight\non how they can be designed.\" a bit more intuition in the beginning of the introduction would make the paper a bit easier to read. \n\n\"We demonstrate the stability of polynomial of orders 2 to 9 by introducing scaling functions and initialization scheme\nthat approximates well known activation functions\"\n\nGenerally, a good technique is to motivate your design decisions before actually stating what those design decisions are. Atleast to me, it was not clear, why you need to introduce scaling functions and initialization scheme in the beginning of the introduction.\n\nScaling Functions: Authors note that the polynomials of order n can  explode, to circumvent this authors introduce scaling functions to dynamically scale the inputs.\n\n\"The purpose of our proposal is to explore newer nonlinearities by allowing the network to\nchoose an appropriate nonlinearity for each layer and eventually, design better activation functions\non the gathered intuitions\" \n\nHaving this somewhere in the introduction could help.\n\nIn general, the paper does not motivate well why use of polynomial activation functions is a good idea."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors introduce learnable activation functions that are parameterized by polynomial functions. The authors learn the coefficients of the polynomials through backpropagation. In order to account for the large values that occur as the input goes to the extremes, the authors scale the output of the input to polynomial function by the maximum input value. The authors try a variety of initialization schemes for the polynomial functions and test their activation function on MNIST, CIFAR10, and CIFAR100. The results obtained show that their function is, at best, slightly better than ReLUs, however, they usually obtain about the same performance.\n\nThe authors do not obtain activation functions that consistently improve performance and do not offer any insight into learned activation functions. They show plots of what the learned polynomials look like, but there is no consistent pattern in their shape and the authors provide no explanation as to why the final shapes might be interesting in the first place.\n\nFurthermore, the paper is poorly written. For example:\n\nIn section 3, the authors use x_i to indicate an output of neuron i in a layer, however, x_i is bold. Is x_i a vector or a scalar? It is being used as input to a polynomial function, so it must be a scalar.\n\nIn section 4, \"On observation, Swish approximations are relatively more accurate when compared to ReLU and TanH.\" By what metric? Visual observation?\n\nIn section 4 they say \"we train each possible combinations (41^3 = 68921) of initialization for an epoch. We use same initialization for all the three activations.\" I am assuming 41 comes from a variety of coefficient values and \"for all three activations\" is actually all three coefficients of the polynomial.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes using parameterized polynomials as activation functions. A regularisation is proposed to stabilize training Experiments are performed and it is shown that polynomials perform well.\n--------------------------\nUnfortunately, I only had limited time reviewing this paper next to the other 6 I had to review and unfortunately i won't find more time before the submission deadline.\n--------------------------\nI am very conflicted with the premise of the paper. Using parameterized polynomial activation functions is hardly new and a very quick search revealed papers going back to the 90s. The paper does not seem to reference any of this prior work or the theoretical results that have been given (e.g. the unsurprising result that polynomial networks with finite depth are not universal approximators). The reason i don't mention a concrete reference is that I do not know which of the old references is the most important. \n\nThe paper does not seem to motivate why looking at parameterized polynomials should be important. In general, they are harder to use and have all kinds of degenerate cases. The paper proposes a way to bound the polynomial range of values and derivatives to alleviate the training problem, yet i miss why this should be important or better than any of the already developed solutions. Most polynomials learned seem to be quadratic or with very low higher order components, so it is not that the learned shapes are interesting.\n\nI did not have the time to assess the experimental results.\n\n\n"
        }
    ]
}