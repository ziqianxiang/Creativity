{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "With the recently growth in interest in adversarial examples for natural language, this paper takes an important step back in asking 'do the adversarial examples we generate actually respect invariances of meaning'. The authors first give a list of constraints which attacks against NLP systems might have to satisfy, and then evaluate 2 recently proposed attacks on NLP classifiers to examine whether they satisfy some of the proposed constraints. The authors show that the attacks actually make more errors of a type that are detectable by automatic grammar checkers, and that humans do not reliably confirm that the proposed attacks preserve the semantics of the original sentence. Further, humans achieve higher than chance performance when telling perturbed text from original text. Finally, the authors show that it is important to control how much tolerance is allowed when considering attack success.\n\nWhile I am very sympathetic to the aims of this paper, and I feel like it makes an important message (that there is no such thing as an imperceptible manipulation of text), I feel the execution of the paper is somewhat lacking, and the experiments need a lot of tightening up before the paper is ready to be accepted. Further, the writing of the paper is very imprecise at times which is an issue especially if this paper is setting out to standardise terminology. In addition, I feel like the authors are too willing to abandon hope of semi-automated measurement of constraint satisfaction. While it is true that human evaluation is the gold standard, many of the constraints the authors propose could be amenable to automation, and I feel like this deserves further discussion. For this reason, I feel like this paper needs one revision cycle before resubmission, but with tighter writing would be a good addition to the adversarial text literature. \n\nWe will start with the imprecise writing: this mainly concerns section 3:\n\nThe whole of section 3.1 uses the word 'morphology' in a very loose sense. The authors seem to mainly use 'morphology' as a synonym for 'surface distance', but in linguistics, morphology has a strict meaning; importantly, small surface changes to a word can dramatically change its morphological categories, or even its root word. To say, therefore, that small surface distances are 'morphology-preserving' is highly inaccurate.  I highly recommend not using the word 'morphology' for this kind of constraint. Further, the first sentence of this section: 'the attacker is willing to chance the semantics of the input as long as all changed sentences read the same to a human', is difficult to understand. By definition, if the semantics is noticeably different, the sentences no longer 'read the same' to a human. Perhaps the intended meaning is that humans are able to repair surface errors using context? \n\nEvaluating whether generated text fulfills some semantic specification is well-studied problem in NLG. The crafters of the message know exactly what semantic content they wish to convey. Depending on the form of the semantic representation, one can attempt to parse the output message back into a semantic representation, and compare this with the desired input semantic representation. Or, if the attacker has examples of output with the intended semantics, one can use well-beloved overlap based metrics like BLEU, ROUGE or METEOR to evaluate whether the generated text has the intended semantics. While all automatic metrics have their problems, they are used too widely to be ignored completely.\n\nThe non-suspicious constraint can also similarly be automatically measured. For instance, recent work on detecting computer-generated news articles have used large pretrained language models to discrminate between human-written and computer-generated articles [1], which is a technique which shows some promise. Therefore, claiming that 'evaluation of the non-suspicious constraint must be done by humans' seems wide of the mark.\n\nI feel the experimental section could do with a lot more examples. For instance, examples of each category of grammatical error for both models should be included, and examples of sentences which annotators judged to consistent have changed meaning. In addition, in Section 4.2, satisficing behaviour is well known in crowdsourcing surveys, and not including check questions to prevent this kind of behaviour can throw doubt on the conclusions that can be drawn. Also, one notes that when the intent of the question is reversed, the average human response is roughly 5-x, where x is the original human response. This, to me, shows that exactly how one asks the question is somewhat irrelevant. \n\n[1] Defending Against Neural Fake News, Zellers et al. 2019\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper discusses the many shortcomings of existing adversarial attacks for NLP models. It is extending the work of (Gilmer et al., 2018) --mostly about attacks for CV models-- to NLP. It proposes 5 types of constraints that should be made explicit by algorithms generating adversarial examples: morphology-preserving (e.g., character substitutions), semantics-preserving (anything as long as semantics are unchanged), semantics-constrained (anything as long as the semantics chosen by the attacker are expressed), syntactically-constrained (anything that is grammatically correct) and non-suspicious (appearing human written). It goes on to show that two recent algorithms relying on synonym-substitution, namely (Alzanto et al., 2018) and (Jin et al., 2019), fall short on 3 of these constraints, sometimes despite claims of the contrary.\n\nOverall, the paper introduces interesting ideas, with a noble goal: making the categorization and evaluation of adversarial generation algorithms both more rigorous, more fair. But I feel the organization of the paper makes its exact contribution somewhat unclear and drawing conclusions is difficult.\n\nSection 3: Constraints\n- It would be helpful to relate the constraints with the existing adversarial NLP literature. What is new? How much of it is just a review of the existing body of work? Are you uncovering blind spots in the classification of attacks, that researchers have not previously considered? Could you make explicit where the novelty lies?\n- It would be interesting to discuss how comprehensive the provided list of constraints is (or is expected to be), especially since the conclusion hints at future work that may \"expand [the] hierarchy\".\n\nSection 4: Experiments\n- Why not do a more comprehensive evaluation of the literature? This section only chooses two similar algorithms. A general framework is introduced but only a narrow slice of the literature is evaluated.\n- Only three constraints are studied, because some are considered mutually exclusive. It would be nice to state this explicitly in this section, as well as keep the terminology consistent (e.g., syntactic constraint vs grammatical correctness).\n- I don't find the evaluation methods very convincing in the way they are presented:\n      - How good is LanguageTool to evaluate grammaticality? Could it have biases that make it more likely to detect errors in adversarial examples? Are there other tools not relying on hand-curated rules available? This could be discussed.\n      - On preservation of semantics, what fraction of ratings are \"Not sure\"? Does the average score change if \"Not sure\" ratings are discarded? Neutral, noncommittal ratings can be an issue with human evals. Also, why not use the \\epsilon=2 rule advocated for in Section 3.2?\n\nOverall this paper raises important issues, but its current organization makes it feel incomplete. Both section 3 and 4 could further expanded with additional analyses to make a much stronger statement. Perhaps it could even be worth splitting this paper into two?\n\nI would suggest the authors make more explicit or consider:\n- How do they want this paper to impact the field? What conclusions should the readers get to?\n- Could they provide a set of tools that benefits the evaluation of algorithms generating adversarial examples? Without such tools, what prevents future researchers to come up with evaluation methods that have the same shortcomings that you try to address?\n- What research is needed to further automate the evaluation process and ensure consistent evaluations? Can this paper encourage such research?\n\nOther minor details:\n- Why are the first three constraints mutually exclusive? One and two don't seem to be, as one can modify a few characters while maintaining semantics. Making these adversarial might be hard, but wouldn't these also be some of the most interesting adversarial examples?\n- Page 1: typo - vulnerable\n- Page 6: typo - far more often\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nSummary\nThe authors survey and categorize recent work in adversarial examples in natural language. The authors further quantitatively disprove previous synonym-based substitutions preserve semantic and grammatical correctness.\n\nDecision\nOverall, the paper is well written and formally categorize many recently work in adversarial examples in natural language. The quantitative study using LanguageTool and human eval is quite important for the community.\n\nSupporting argument\n1. The paper is clearly organized and formally categorize the perturbation in natural language. \n2. The findings that adversarial examples diverge in semantic meaning or do not appear to be human-written are important.\n3. It might be good to show some qualitative examples for the grammatically, semantically incorrect cases.\n\n\nAdditional feedback:\n1. Missing reference: https://arxiv.org/abs/1904.12004v1 is another relevant adversarial attack to machine translation, image caption models. It might not fit into the current categories. \n2. Huang et al. 2019 uses IBP not only for synonym attacks, but also for morphology preserving perturbations (typos). \n3. I disagree with the problematic statement for training IBP. The drop of nominal test accuracy is not limited IBP, but is related to the tradeoff between robustness and nominal test accuracy (https://arxiv.org/abs/1805.12152)\n"
        }
    ]
}