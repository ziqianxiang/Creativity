{
    "Decision": {
        "decision": "Reject",
        "comment": "This article studies the inductive bias in a simple binary perceptron without bias, showing that if the weight vector has a symmetric distribution, then the cardinality of the support of the represented function is uniform on 0,...,2^n-1. Since the number of possible functions with support of extreme cardinality values is smaller, the result is interpreted as a bias towards such functions. Further results and experiments are presented. The reviewers found this work interesting and mentioned that it contributes to the understanding of neural networks. However, they also expressed concerns about the contribution relying crucially on 0/1 variables, and that for example with -1/1 the effect would disappear, implying that the result might not be capturing a significant aspect of neural networks. Another concern was whether the results could be generalised to other architectures. The authors agreed that this is indeed a crucial part of the analysis, and for the moment pointed at empirical evidence for the appearance of this effect in other cases. The reviewers also mentioned that the motivation was not very clear, that some of the derivations were difficult to follow (with many results presented in the appendix), and that the interpretation and implications were not sufficiently discussed (in particular, in relation to generalization, missing a more detailed discussion of training). This is a good contribution and the revision made important improvements on the points mentioned above, but not quite reaching the bar. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "The topic of the paper is the inductive bias of neural networks. The authors study a simple model, namely a perceptron with no bias term viewed as a mapping from {0,1}^n->{0,1}. They show that initializing weights with a distribution that is symmetric under coordinate sign flips corresponds to an initialization in function space that is biased towards low-entropy functions. They also exhibit empirical evidence that by adding a bias term, or by using multiple layers, this tendency towards low entropy appears to increase. They also prove a bound on the minimal size of a network in order for it to represent all boolean functions. Finally, they prove a result that suggests that for ReLU networks with infinite widths the bias towards low-entropy function does indeed increase with depth. \n\nMy main concern regarding this paper is that the claim in the title and the statement of Theorem 4.1 seem to rely crucially on the fact that the functions are viewed with input as {0,1}^n. The origin of the \"simplicity\", in the basic case that the authors address (perceptrons with no bias) appears to be a consequence that hyperplanes through the origin are quite likely to classify input points in {0,1}^n similarly. If one switches to a symmetric domain, for example {-1,1}, the effect in this setting completely disappears. The authors actually mention this in Section 5, noting that the expressivity of the perceptron is much lower for centered inputs. However, this to me suggests that Theorem 4.1 is not capturing any significant aspect of neural networks (in fact, the statement is a property of how linear hyperplanes to separate {0,1}^n, not neural networks). I may be mistaken, but I would like the authors to clarify this point.\n\nAnother concern is related to Theorem 5.5. This might be a more substantial result, but it is difficult to interpret and its implications are not discussed. Understanding the effect of depth on the \"simplicity bias\" seems to me an important problem, but for some reason Theorem 5.5 (which deals with deep neural networks rather than linear perceptrons) is emphasized much less than Theorem 4.1. Why is this the case?\n\nThe paper is well-written but not always very clear. In particular, notation is not always defined and the authors use on notions from complexity theory that are not introduced (e.g., Lepel-Ziv complexity).\n\nOther comments: \n\n* Is the set F_t is defined as set of all functions with assigned \\mathcal T, but later this seems to be restricted to the functions expressible by a network/perceptron.\n* Definition 3.5: this defined the entropy H(f) of a function but then write H(p). It should probably be H(f) = -plog p - (1-p)log(1-p) where p = \\mathcal T(f), right?\n* Definition 3.6: some context or references for this definition could be useful.\n* Regarding the fact that functions in F_t are not uniform, shouldn't the distribution be the same for isotropic weight distributions? Assuming the distribution of w/|w| is uniform on the sphere, a more precise description of  P(f) seems possible\n* Section 4.3: what is the \"rank\" in this setting? Isn't the parameter a vector w in R^n?\n* Some typos: Definition 3.1 w_l \\in R^{n_{l+1}}, \",.\" in the beginning of Section 5, several in the last paragraph of Section 5.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors study the behavior of simple neural networks at initializations. Particularly, the authors show that at initializations, neural networks tend to be functions with high class imbalance.\nFurther, the authors show that how such conclusion would be reached with or without a bias term, with different number of hidden layers, with change of activation functions. \nThe work is fairly interesting, yet the motivation is less clear.\nThe conclusion that study of such initializations can help understand the generalization power is not convincing.\nDespite that neural networks at initializations are biased towards low entropy functions, it’s not clear with training on a dataset with an optimizer, how much we can conclude about the generalization power.\nOverall the paper is well written.\n\nBelow are some more detailed comments:\n1) In the Introduction and the first paragraph of Section 2, the authors motivate by describing how important it is to understand the inductive biases. Yet the study is about the behavior of a random initialization. It would be nice to tie these two better; or motivate from another angle, other than the inductive bias. To my reading, the sentence “what the inductive biases … are, and how they arise” is not well supported because I still don’t follow what the inductive biases are from reading the paper except that a random initialization likely be low entropy functions.\n\n2) In Figure 3(a), 4(b,c,d), there is a spike at the mid-point of t. Though not as high as the extreme points, this is contradictory to the main conclusion. It would nice to add discussions.\n\n3) It would be nice to add experiments to study how such bias at initializations would impact the model training.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the a-priori bias of a feed-forward neural network when the weights are initialised uniformly at random and independent of the network architecture. The paper claims that this initialisation leads to biases towards low entropy functions when the input and output are binary values. \n\nThe paper starts with a single layer perceptron without the bias term, and generalises the analysis to networks with multiple hidden layers and ReLU activations. The proposed approach seems rigorous, but I have a hard time to follow the paper as many of the important results are presented in appendix. In addition, the analysis is based on a feed-forward neural network with binary inputs and a single binary output, it is not clear whether these results can be generalised to architectures of practical importance such as convolutional/recurrent neural networks. Overall an interesting piece of work that contributes to the understanding of deep neural networks.\n\n\nMinor comments:\n\nSection 4.2: \n\"as as predicted by Eq. 1\" -> remove duplicated \"as\"\n\nSection 5.2: \n\"some interesting recent work\" -> \"Some ...\"\n\"produce.At first sight\" -> add a space before \"At\"\n\"If there is not bias\" -> \"If there is no bias\""
        }
    ]
}