{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper provides a generalization error bound, which extends the results from PU learning, for the problem of knowledge graph completion. The authors assume a missing at random setting, and provide bounds on the triples (two nodes and an edge) that could be mistakes. Then the paper provides a maximum likelihood interpretation, as well as relations to existing knowledge graph completion methods. The problem setting is interesting, and the writing clear.\n\nThis discussion was extensive, with reviewers and authors following the spirit of ICLR and having a constructive discussion which resulted in improvements to the paper. However, there seems to be still some remaining improvements to be made in terms of clarity of presentation, as well as precision of the theoretical arguments.\n\nUnfortunately, there are many strong submissions, and the paper as it currently stands does not satisfy the quality threshold of ICLR.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work conducts a theoretical study of the generalization bounds for the number of wrongly predicted triples of KG embedding methods. Under a \"``missing completely at random'' assumption, the authors model the distributions of sampled KG and ground-truth KG, and derivate the bounds as a function of the KL-divergence by leveraging Pinsker's inequality. Preliminary discussions on how existing KG embedding methods fit into this theoretical study have been conducted.\n\nPros:\n- This work raises a novel problem, i.e., analyzing the theoretical generalization ability of existing KG embedding methods. \n- The paper is well motivated with clear writing and technically sound with rigorous formula derivation.\n\nCons:\n1) For KG construction, people usually collect/mine in a batch/incremental fashion in which each batch focuses on certain aspects. For example, one can collect some bio facts about Persons from Wikipedia, then collect some entertainment related facts from news. So in practice, \"``missing completely at random\" assumption may not hold for KG.\n\n2) The derivation is suitable for a class of KG models that maximize log-likelihood losses, while many KG embedding methods use margin-based loss functions. Although the authors mentioned \"``log-loss can in principle be also used and it was observed by Trouillon & Nickel (2017) that the margin-based loss functions, used by many knowledge graph embedding methods, are more prone to overfitting compared to log-likelihood'', more rigorous theoretical analysis is suggested to verify (or refine) the applicability of the proposed analysis \n\n3) It would be better to see more examples that applying the proposed analysis to different KG embedding methods, corresponding comparisons help to get a deeper understanding for existing KG embedding methods, and may shed further light on how to design a KG embedding method that achieves a good enough generalization with fewer model parameters.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary\n\nThis paper studied the knowledge completion problem from the viewpoint of statistical learning theory. More specifically, it formalized a knowledge graph completion problem as an estimation of the optimal embedding by assuming the canonical distribution derived from the embedding and derived the upper bound of misclassification rate of entities under the missing-completely-at-random assumption.\n\n\nDecision\n\nAlthough the formalization of knowledge graph completion problems is novel (to the best of my knowledge), there is much room for improvement in the organization of the paper. Therefore, I would recommend to reject the paper this time and to ask authors to revise the paper so that the paper is more accessible to researchers and engineers.\nThere are numerous existing works about the knowledge graph embedding problem and the knowledge graph completion problem using embeddings. However, few works have been done to justify these methods theoretically. This paper attempted to answer this question via statistical learning theorem perspectives. The theorems gave sufficient conditions in terms of the size of a knowledge graph and properties of embedding functions under which the misclassification rate goes to $0$. In this aspect, this paper gave insights into how we can give justifications to existing embedding methods.\nHowever,  I think it is hard for those who are not familiar with this field to follow the logic of the paper, as I write in detail in the following sections. I understand some people argue that the paper organization is not essential for how a paper contributes to science and technology. However, the accessibility to the paper is vital to promote technical communications in different fields. Also, I believe it is beneficial for the paper to maximize its value. Thus, I would ask the authors to polish the paper.\n\n\nSuggestions\n\n- Introduction \n\t- Please explain what knowledge graphs are in the introduction before explaining how knowledge graphs represent knowledge, or what problems of practically available knowledge graphs have.\n\t- I think it is a bit too casual and conversational to use wordings like \"Please bare in mind\" in papers.\n\t- I think it is helpful if the authors add a summary of theorems and its implication in the introduction to grasp the theoretical contributions the paper has made.\n- Section 6, Theorem 1\n\t- Please describe what is the probability of $P$ and with respect to which probability distribution $\\mathbb{E}$ takes the expectation.\n- Section 7\n\t- The authors claim that Pinsker's inequality plays a central role in deriving the upper bound for the ratio of wrongly predicted triples. However, we cannot know how the authors used the inequality unless we see the proof of Theorem 3, which is available in the appendix.\n\t- In Theorem 3, the definition of \"wrongly predicted triple\" is missing. I only find a description related to it in the proof of Theorem 3 in the appendix. Could you add the definition in the main part?\n- Section 8, Theorem 4\n\t- The statement of Theorem 4 says that $\\mathbb{X}$ is a learned representation. But it is not available in the main article how we obtain it. We know that the authors used the maximum-likelihood estimator if we read the proof of the theorem. The author should clarify it in the statement of the theorem.\n- Section 10\n\t- What does \"this\" mean in the title of the section?\n\t- In the initial reading, I could not understand the main point the authors want to address in the paragraph starting with Loss. I think authors can clarify the point by restructuring the paragraph.\n\n\nMinor comments\n\n- Section 2 and Section 6\n\t- The authors treated $\\mathbb{X}$, which is the set of vector representations of objects and relations, as a lookup table in Section 2.2 and reinterpreted it as the subset of $\\mathcal{X}^{|\\mathcal{O}|+|\\mathcal{R}|}$ in Section 6. I think we can simplify the description if we treat $\\mathbb{X}$ as the subset of $\\mathcal{X}^{\\mathcal{O} \\sqcup \\mathcal{R}} := \\{f: \\mathcal{O} \\sqcup \\mathcal{R} \\to \\mathcal{X} \\}$ and denote $\\mathbb{X}(o)$ as $x_o$ in short hand for a representation $\\mathbb{X}$ and an object $o\\in \\mathcal{O}$ (and similarly for $x_r$).\n- Section 4\n\t- It would be helpful to make the correspondence explicit between each sentence and theorem later in the last paragraph. For example, \"We prove a generalization bound for log-likelihood from which a bound on Kullback-Leibler divergence follows (Theorem 3)\" or something like that.â€¨- Section 6, Theorem 1\n\t- The definitions of $\\mathbb{X}_o$ and $\\mathbb{X}_r$ are missing, if I do not miss something.\n\t- It is better to add the assumption that $\\sup_{h, r, t} |\\psi(x_h, x_r, x_t)|$ is finite.\n- Section 12\n\t- \"simplistic\" means too simple by itself. So, \"too simplistic\" should be \"too simple\" or \"simplistic\".\n\n\nQuestions\n\n- Section 4\n\t- I could not fully understand the intuition of the proof in the second paragraph. The authors think that the situation is desirable if multiple knowledge graphs are available. However, in the first approach, they concatenated the graphs into one (I interpreted the union of graphs as $\\widehat{\\mathcal{G}}_1 \\cup \\widehat{\\mathcal{G}}_2 \\cup \\cdots \\widehat{\\mathcal{G}}_n$. Correct me if I am wrong). Since this operation would reduce the situation to the single-graph case, we cannot take advantage of multiple graphs.\n- Section 7, Theorem 4\n\t- The upper bound is in terms of the estimator which maximizes the expected log-likelihood. I am wondering whether this estimator can achieve the best possible misclassification rate (i.e., $|\\mathcal{F}/|\\mathcal{O}^2|/|\\mathcal{R}||$). I understand that it minimizes the discrepancy between the inferred distribution and the data generating distribution. But I am not sure it implies the least misclassification rate.\n\t- The small terms in the upper bound (i.e., first and second terms) depend on the number of objects $|\\mathcal{O}|$ and not on relations $|\\mathcal{R}|$. Since the role of $\\mathcal{O}^2$ and $\\mathcal{R}$ is symmetric mathematically (if I understand correctly), I feel it is weird that the additional terms do not necessarily go to zero when $\\mathcal{R}\\to \\infty$. I particular, I expected that the additonal small terms depend solely on $|\\mathcal{G}|=|\\mathcal{O}|^2|\\mathcal{R}|$."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The paper presents a study on bound of the number of the expected triples wrongly predicted by general graph embedding methods. The paper considers methods that follow the \"completely at random\" assumption and aim to maximize the likelihood (or minimize the log-loss).\n\nThe study considers the learning from positive and unlabelled data problem and theoretically demonstrates the correctness of the bounds on the number of triples that embedding methods can add during the completion of the knowledge graph.\n\nThe paper is well written, theorems and proofs seem to me to be mathematically sound and correct. To the best of my knowledge, I agree with the authors when they claim to be the first to inspect the problem to give bounds under the setting they have considered. However, I have to admit that I am not an expert on the problem, I am more aware of the practical part of this field and less aware of the theoretical part.\n\nHowever, I have carefully checked the validity of the analysis and I have not found any flawless or critical errors in the proofs. I would suggest removing aliases for |O|, |R|, etc. from the proofs as they do not simplify the formulas significantly.\n\nOther corrections:\n- on page 5 the word \"the\" is written twice in the sentence just before Theorem 2.\n- on page 7 C is defined as constant but C should be C_1. Also C_2 should be defined.\n- on page 7, in \"Lipschitz Continuity\" after the sentence \"Our result requires Lipschitz continuity\" there must be a full stop or the word Most must begin with a lowercase m.\n- on page 8 there is inside the Figure 1 (on the red line, near 1*10^4 number of objects in the domain |O|) a link to page 6 that should be removed."
        }
    ]
}