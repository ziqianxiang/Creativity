{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThis paper proposes a method for training GANs on target domains with a small number of samples, by adapting a single or multiple high-quality GANs pretrained on larger source domains (e.g. faces or imagenet). The method is based on training a small network M to transform a mixture-of-Gaussian samples to a Gaussian, which is fed to the pretrained GAN generator G (which can be kept fixed or finetuned). The paper distinguishes two transfer cases: on-manifold, in which the target domain is a subset of the source domain (e.g., celebA and FFHQ women), and off-manifold, in which the target domain is different from the source (e.g., celebA and FFHQ children). Experiments show that the proposed method can perform better than training from scratch or simply finetuning on the target domain.\n\nComments:\n-  The usage of the term \"knowledge transfer\" in the paper is a little confusing. I don't think the knowledge distillation of Hinton et al 2014 and Romero et al 2015 is relevant here, yet the paper references them in the sense of adapting/transferring a pretrained network to small data regimes.\n-  In addition, the notion of transferring/adapting a network trained on a large set to a subset of it doesn't seem very useful to me. It is certainly different from transferring a generic representation in supervised learning (e.g. from imagenet) to small data regimes. That being said, the paper discusses off-manifold transfer, which could be more interesting from a practical point of view.\n-  The method is simple, which is not particularly a bad thing, but it's not really clear what is the advantage of it over simply finetuning the whole network. I am not convinced that finetuning will result with mode-collapse (as mentioned in Sec. 4) while this method won't. I believe the authors mean overfitting here, which is probably why we see much better results in for their method (MineGAN) compared to finetuning (TransferGAN) in Fig. 3.\n-  Regarding Fig. 3, training from scratch seems of very low resolution. I believe more effort should be done on creating stronger baselines.\n\nIn conclusion, I don't think there's enough evidence (neither theoretical nor empirical) that the proposed approach is significantly better than simple finetuning which undermines the novelty and contribution of this work.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposed a method to transfer knowledge for GANs. The model utilized a network to identifies which part of the generative distribution of the pre-trained GAN close to the target domain. The experiments are conducted on several datasets with different architectures. \n\nFor me, the only novelty of the method comes from the miner network, which is limited. It could be difficult to generalize to the target domain with more/diverse classes. It could also fall when the source and target domain are quite different. \n\nI think the motivation of this work is similar to previous work on domain adaptation for generative models, which has been explored in several other papers. However, the paper did not mention them. \na. Unsupervised Image-to-Image Translation Networks, NIPS17.\nb. CyCADA: Cycle-Consistent Adversarial Domain Adaptation, ICML18. \nc. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification, CVPR18.\n\nRegarding the experiments, I do not think the current setting is reasonable. All the splits are within the same domain (even the same dataset), digit-digit, face-face, object-object. There is not much domain gap there to evaluate the effectiveness of the knowledge transfer. The baseline is also pretty simple. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThe authors propose a new GAN model to transfer the knowledge from source domains to the target domain. The experiments empirically show the generation results, which show the power of the proposed GAN model.\n\nStrength:\n1. The problem is interesting. Sometimes it is real and reasonable to apply GAN on other domains with limited data. \n\n2. The motivation for multiple generators is interesting and useful. In real-world, transferring one single domain to another may cause unstable transfer results and transferring from multiple source domains can alleviate the problem.\n\n3. Empirically, the authors show the power of the proposed model.\n\nWeakness:\n\n1. For single GAN mining, the motivation of Miner M is a litter weird to me. I understand the goal for Miner M is to shift the distribution. I am curious about the power of M if the distribution in the target domain is quite similar to the source domain. It will be more interesting to investigate performance v.s. M design in the experiment parts and give more insights.\n\n2. The technical contribution is not enough. The key problem is how to use the pre-trained model. The model is too intuitive. It will be better to discuss the contributions in the rebuttal period.\n\n3. In the experiments, it will be more convincing if the authors can show more analysis on the probability p in multiple generators. {Car, Bus}->red vehicle with different combination ratios seems simple. I hope to see more complex case studies. "
        }
    ]
}