{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a two-stage adversarial training approach for learning a disentangled representation of style and content of anime images. Unlike the previous style transfer work, here style is defined as the identity of a particular anime artist, rather than a set of uninterpretable style features. This allows the trained network to generate new anime images which have a particular content and are drawn in the style of a particular artist. While the approach works well, the reviewers voiced concerns about the method (overly complicated and somewhat incremental) and the quality of the experimental section (lack of good baselines and quantitative comparisons at least in terms of the disentanglement quality). It was also mentioned that releasing the code and the dataset would strengthen the appeal of the paper. While the authors have addressed some of the reviewersâ€™ concerns, unfortunately it was not enough to persuade the reviewers to change their marks. Hence, I have to recommend a rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This work introduces a Generative Adversarial Disentangling Network based on two stages training the first aims at learning a style independent content encoder and then content and style conditional GANs is used for synthesis.\nAt stage 1 training to prevent the encoder from encoding authors introduce a gan style training in which an adversarial classifier that tries to predict the corresponding artist from the encoded image.\nAt stage 2 is training a style/content conditional gan. To condition on the style (artist) authors introduce an extra adversarial classifier so the generator tries to generate samples that would be classified as the artist that it is conditioned\non. While to condition on the input content another loss is ensuring that the generated image is encoded back to its content input.\n\nAuthors compare the proposed method against the original neural style transfer and StarGAN over various styles within the context of anime illustrations and the NIST Dataset where styles are being represented by artist name. \n\nWhile the work tackles some of the problems by conditioning only on artist names other than style features that might be hard to have annotations for. The proposed modifications are quite incremental. Additionally, the experiments section is quite weak, evaluation is only done quantitatively over some cherry-picked examples, although some extra ablation study in the appendix is provided. \n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Contributions:\n1. This paper proposes a method to learn disentangled style (artist) and content representations.\n2. By carefully designing two-stage training objectives, the method learns a style-independent content-encoding E at the first stage and the style encoder S and generator G both from the first and the second stage.\n3. Empirical results justify the validity of the method.\n\nI think this paper makes a good contribution to disentangle style and content in anime. My main concern is the complicated learning procedure design may affect the reproducibility of this method. Moreover, I will suggest several points to the authors to clarify in the main text.\n\n1. I encourage the authors to release their code when published.\n\n2. In stage 1 (Style Independent Content-Encoding), the purpose of the classifier C, to my understanding, is to try to classify the generated example G(E(x), S(a')) as the \"ground-truth\" style (a). That is, the classifier C tries to disregard the S(a') when making a decision. As an adversarial player, E, G, S will try to fool C by making E(x) to be non-informative regarding the style. However, since you are still optimizing G and S, how do you make sure that it is safe to hold E fixed while still changing G and S in the second stage? Or more specifically, how do you make sure the style encoding network S preserves a good one in the second stage? Aside from that, are you using the trained G, S from the first stage to initialize G, S in the second stage?\n\n3. There are eight different terms in stage 2, so it worth checking the necessity for those terms. E.g. what happens if you drop the L_cont term? The term L_cont seems to guarantee the validity of E, but E is fixed in step 2."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes an image generation method with the focus on generating anime faces from various artists. The proposed method which is a combination of conditional GANs and conditional VAEs manages to generate high fidelity anime images with various styles.\n\nThe paper is well-written and easy to follow and understand. The goals are clearly stated and the background (which is more of history), as well as related work, is comprehensive. The decision behind every design decision has been mentioned in detail which makes the paper stronger. The main writing flaw of the paper is in the figures where more annotation and caption is required to make them easy to understand. For example:\n- The significance of colors in Figure 1 (architecture of the model) is not annotated at all and the letters are not clear either (although they are described in the text itself a figure should be comprehensive by itself).\n- Figure 2 and Figure 4 are really hard to understand with very limited annotation and caption. I had to read the text multiple time to Figure out what is what in these figures which is not a good sign for clarity.\n\nIn terms of experiments, I think where the paper suffers the most is in comparison with other conditional methods. In Section 5 it has been clearly mentioned that \"this result can be expected from a class-conditional GAN and the focus in on Disentanglement\" however very little evidence has been provided for superior disentanglement. More experiments are required to demonstrate the capabilities of the model compared to other conditional methods (which is currently only limited to StarGAN) as well as its capability of disentanglement. I agree with the authors that quantitative evaluation of generated anime faces is not easy (although it is possible with a carefully designed human study), however, the disentanglement (which is the focus of the paper) is easy to evaluate quantitatively. This demands for more experiments on disentanglement datasets with known generative factors. Although the current ablation study in the Appendix provides more details for architectural decisions, a more qualitative and quantitative comprehensive ablation study (by actually ablating the final model) can help to demonstrate these decisions.\n\nIn conclusion, the paper has great results. We all know a big part of writing this kind of paper is to make the model \"work\" and authors truly demonstrate that they worked hard. However, the impact of the paper (in the current form) is not clear. With the focus on disentanglement, little evidence has been provided to justify the capability of the proposed method. I believe by addressing my comments on the experiments the paper can be easily pushed above the acceptance bar. Also releasing the code dataset should increase the impact of the paper.\n"
        }
    ]
}