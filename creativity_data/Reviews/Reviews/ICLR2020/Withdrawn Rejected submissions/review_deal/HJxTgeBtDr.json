{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper diligently setup and conducted multiple experiments to validate their approach - bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. However, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper -NER. While manuscript proposes this approach as ‘general’, reviewers failed to seem this point. Another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix (17). There were some confusions around writings as well, pointed out by some reviewers. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper discusses a methodology to interpret models and model outputs for Named Entity Recognition (NER) based on assigned attributes. The key idea is to bucketize the test data based on characteristics of attributes and then comment on effect of the attribute on the model, the task itself or the dataset bias. \n\nThe empirical evaluation is impressive. The authors have constructed a series of experiments to make their case. The paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. While the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily (e.g. using domain knowledge). Furthermore, there is only one problem setting considered (i.e. NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. \n\nThe bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very  interesting and useful to the community as a whole. The strongest part of this paper is the empirical evaluation that allows drawing interesting conclusions, and suggests a methodology to reach that conclusion. While some of the claims made (e.g. regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "TOWARDS INTERPRETABLE EVALUATIONS A CASE STUDY OF NAMED ENTITY RECOGNITION\n\n\n\nThe authors propose an evaluation methodology to study the relations between datasets and machine learning models. This methodology introduces the notion of attributes which describes different aspects of the samples and buckets which group samples according to the attributes. The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4.\n\nThe article is very dense and the author chose to present the method from an abstract and generic point of view which makes the reading of the article difficult. In the end, the proposition is a formalisation of the simple error analysis which is commonly done when trying to improve a machine learning system. The advantage of the method could be to introduce some metrics to make the error analysis more automatic. These metrics are given in section 4 but here again only from a formal point of view : it is very difficult for the reader to understand how to interpret them and how to use them for a practical case. \n\nThe paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper. The aspects of the paper related to learning relations is no put forward enough. \n\n2. Related work\n\n2.1 :\n -supplementary exam : unclear\n\n2.2 : \n- methodological perspective : a bit a repetition of introduction\n- task perspective : not very clear, is the main message  \"it important to understand what in the dataset make the model work ?\"\n\n3 Task\n\nSection is too small to be a level 1 title\n\n4 Attributes\n\nfigure 2 : where are the links to levels ?\n\n4.2 :\n\n  * familiarity : test/train distribution should be the same. Fk computer on train set because it is bigger ? it allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ?\n\n  * multi attribute familiarity : risk of metric explosion ? how to select the attributes ?\n\n  * eq 3 : spearman not defined\n\n* 4.3\n\n  * metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them\n\n  * \"Usually where a, b represent two different models and usually model a has a higher performance (by dataset-level metric)\" : unclear\n\n5 Experimental setting\n\nTable3 : \nthe encoding of the model name is not clear\na metric on all the dataset for each model could be computed to decide which one is the best overall\nhow did you choose the tested combinaisons ?\n\n6\\.2\n\nanalysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ?\n\ntable 4 : spearman\\**r*\\* ?\n\n6\\.4\n\n* CRF vs MLP : \"... a major factor for the choices of CRF and MLP: **if** a dataset with higher ζMF−et, in which longer entities can benefit more from CRF-based models.\" > missing words ?\n\nWriting :\n\n* \"Concretely\" isn't very natural at the beginning of sentences, same thing with \"Formally\", 'Intuitively' …\n\n* in 4.1 : \"We refer to E, P, K as the sets of entities (i.e. New York), entity attributes (i.e. entity length) and attributes values (i.e. 2).\" => \"We refer to the sets of entities (i.e. New York) as E, entity attributes (i.e. entity length) as P and attributes values (i.e. 2) as K\" would be better\n\n* same thing in 4.3 \"we refer to M = m1,··· ,m|M| as a set of **models** and P = p1,··· ,p|P| as a set of **attributes**\" doesn't really work, \"M = m1,··· ,m|M| is a set of **models** and P = p1,··· ,p|P| is a set of **attributes**\" maybe\n\n* in 4.2 page 5 : \"the familiarity Fk (p1 , p2 ) is a measure with intriguing explanation …\" : not clear\n\n* 6.3 (3) \"Only using character-level CNN is apt to overfit the feature of capital letters.\" **apt** doesnt work here"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nThe manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The method considers a set of methods addressing the task of Named Entity Recognition (NER) as case study. In addition, it proposes a set of attribute-based criteria, i.e. bucketization strategies, under which the dataset can be divided and analyzed in order to highlight different properties of the evaluated methods.\n\nAs said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The characteristic of being able to provided deeper insights on strength/weaknesses and relevant factors on the inner-workings of a given method is \nsomething very desirable for every evaluation. As such, in my opinion, the \"interpretable\" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the \"interpretable\" tag and stress the contribution of this manuscript as an evaluation protocol. \n\nIn Section 4.2, for the R-Bucket strategy it is stated as having the requirement of discrete and finite attributes. Based on the equations of the other two strategies (R-bucket and F-bucket), it seems that they also have the requirement of having discrete attributes. Is this indeed the case? if so, it should be explicitly indicated. \nHaving said that, this raises another question: Is this protocol exclusive to tasks/problems with explicit discrete attributes?\n\nThe goal of this manuscript is to propose a general evaluation protocol for NLP tasks.\nHowever, it seems to be somewhat tailored to the NER task. My question is: How well the proposed method generalizes to other NLP tasks without attributes? Similarly, how well the proposed bucketization strategies generalize beyond the NER task? Perhaps the generalization characteristics and limitations of the proposed evaluation methodology should be explicitly discussed in the manuscript.\n\nLast paragraph of Section 4.2 summarizes ideas that were just presented. It feels somewhat redundant. I suggest removing in in favor of extending the existing discussions and analysis.\n\nI may consider upgrading my initial rating based on on the feedback given to my questions/doubts.\n"
        }
    ]
}