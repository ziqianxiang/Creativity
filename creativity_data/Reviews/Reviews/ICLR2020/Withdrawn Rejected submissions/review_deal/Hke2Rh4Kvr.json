{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a theoretical framework that supports the estimation of hierarchical generative models in a greedy layer-wise fashion. They propose a \"cascaded boosting\" model that basically consists in training a cascade of (latent variable) generative models (the top generative model is trained to maximize the log-likelihood of the marginal posterior distribution over the latent variables induced by the lower model). This boosting method overcomes some of the computational complexity of a previously proposed generative boosting model of (Grover and Ermon, 2018). The experiments suggests that piling up generative models in this way can lead to better performance (log-likelihood on a test set) which will plateau at some point.\n\nEstimating hierarchical generative models is well-known in the literature (HVAE, e.g. https://arxiv.org/pdf/1702.08396.pdf and ref. therein). I guess that one of the contributions of this paper relies on showing that, under the assumption of a factorized posterior, training models in a greedy layer-wise fashion is a reasonable thing to do to maximize likelihood of the data (under the assumption that these models do a good job at capturing the marginals).\n\nI am not comfortable enough with this field to carefully assess this paper but I don't think this paper is above the acceptance bar due to (i) relatively weak novelty of the proposed approach and no discussion on its shortcomings and limitations; (ii) experiments with architectures well below the state of the art.\n\n1) About the novelty and clarity:\n1.1) This paper it doesn't seem to me a sufficiently novel or impactful contribution that can open the path for better models in the future. Previous work on hierarchical latent variable models is dense (https://arxiv.org/pdf/1702.08396.pdf as an entry point) and the contribution wrt these works doesn't seem clear enough to me.\n1.2) What's the limitation of optimizing the bound in a greedy layer-wise fashion ?\n1.3) The connection to boosting seems unintuitive: one of the characteristics of boosting is training weak learners on re-weighted data samples (this is basically what multiplicative boosting generative model also do (Grover and Ermon, 2018)), here one of the assumptions for improving the bound is that the learners are powerful enough. Why then is this called \"boosting\" ?\n1.4) What's the conclusion of this paper ? Should I really use a collection of simpler models trained greedily for my generative task at hand?\n\n2) About the experiments:\n2.1) Every result in this paper is solely on MNIST (apart from Table 1 which reports some results on CELEBA but doesn't compare to other models).\n2.2) Section 4.4 is rather weak, the authors seem to suggest that their proposed method (boosted vae) is \"better\" than merely having a \"larger\" VAE. However, the lower bound for the wider VAE (-87.74) is higher than the bound for the boosted VAE (-100). From a general point of view, every result in this paper is quite far from the MNIST SOTA which is -78.5 (https://arxiv.org/pdf/1612.04739.pdf). A stronger result would be to show that a collection of weak cascaded boosted models could achieve similar results to a near SOTA architecture.\n2.3) Are Table 3 results on MNIST too ?\n\nMinor points:\n- Put a space between log and the argument\n- It’s confusing that conditioning has variable notation (, instead of |)\n- Theorem 1 , what’s “n” ? Should be “k” ? why p_k(x, h_1, …, h_k) "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a novel approach of cascaded boosting for boosting generative models. \nThe authors derive a decomposable variational lower bound of the boosted model, which allows each meta-model to be trained separately and greedily. All in all, I like the idea of cascading two model for better performance and the VB lower bound derivation fuse the model training nicely. Detail comments are as following.\n\n1.\tWould a global correction step be possible after greedily learned each weak learner? \nWill this step help improve the model performance?\n2.\tVB is notoriously for it local minimal. More experiment details such as how to initialize the model and how will it affect the model performance need to be provide.\n3.\t Like section 4.4 a lot. It will be better to provide metrics other than lower bound for the experiment part.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nThe authors interpret the standard stacked training of deep generative models as a boosting algorithm.  They provide some theorems about the likelihood of the resulting model, including identifying conditions under which adding layers increases a lower bound on the likelihood.  They perform some experiments demonstrating that the bound increases in practice, and showing some improvements in the likelihood obtained by stacking a Gaussian mixture model on top of recent sophisticated VAEs, comparing this with a number of other algorithms.\n\nThe fundamental novelty in this paper appears limited to me.  The training algorithm abstracts the training of a deep belief network in an obvious way.  The theoretical treatment seems closely related to [1].   The authors of [1] also commented that this method could be viewed as a boosting algorithm.  \n\nThe improvements found by through their application of stacked training with a particular combination of models appear to be small.  Also, if I understand correctly, only an approximation of the test log-likelihood was used in the quantitative results.\n\n\n\n\n\n[1] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"A fast learning algorithm for deep belief nets.\" Neural computation 18.7 (2006): 1527-1554."
        }
    ]
}