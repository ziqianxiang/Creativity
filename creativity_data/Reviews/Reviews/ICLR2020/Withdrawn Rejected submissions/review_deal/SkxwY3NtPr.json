{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThe manuscript proposes an evaluation methodology to assess objectively methods for model explanation without the need of ground truth. This methodology is based on four metrics evaluating different aspects of the generated explanation. These four metrics evaluated the bias of the explanation map at the pixel level, quantify the unexplainable features components, robustness of the explanation to spatial masking, and mutual verification of different explanation methods.\n\nExperiments on the CIFAR10 an Pascal VOC 2012 considering a good set of representative explanation methods show the strenghts and weaknesses of the proposed method. \n\nGiven the recent proposal of a significant number of works aiming a model explanation, the manuscript touches the critical point of assessing objectively the performance of these methods. \nOverall the manuscript has a good presentation, and its content, to great extent, is clear and easy to follow and good formal preentation of the proposed method is given. I appreciate that the experiments section covers two datasets and various landmark methods for model explanation.\n\nMy main concerns with the manuscript are as follows:\n\nIn several parts of the manuscript they were sentences refering to some explanation methods as ables to reflect/diagnose the \"logic of a CNN/model/network\". I could not help but all the time that I encountered the expression \"logic\" ideas related to reasoning or internal decision-making process came to my mind. This really affected the flow of the content, and with all due respect, none of the explanations generated by existent methods is able to faithfully reflect the decision-making process of the models being explained. The majority are oriented towards generating approximations and indicating relevant regions of the input data.\n\nAlong the direction of objective evaluation of explanation methods, Oramas et al. ICLR'19 and more recently \nYang and Kim, arXiv:1907.09701 proposed methodologies aimed at quantitative evaluations of explanation methods. \nGiven the common objectives that these works have with the manuscript, it would be strenghten the manuscript to positition itself wrt. these methods in the related work section.\n\nIn Sec.3.3 the proposed method quantifies the amount of unexplainable regions, i.e. regions with low absolute attribution, of different methods. However, unless I missed something, it seems that these unexplainable regions are native of the explanation methods but native by the masking step introduced by the proposed metric.\nIn addition, this metric relies on a pre-defined threshold which determines the number of pixels to be considered. Could you motivate how this threshold value was selected? what would be the effect of selecting different values?\n\nThe metric proposed in Sec. 3.4 works under the assumption that a explanation method which is robust to spatial masking, can be considered more convincing. More specifically, it assumes that explanations from occluded versions of the same input should be similar. Do this metric takes into account the prediction $y=F(\\hat{I})$ produced by the model to be explained? if these predictions differ, assumming that the explanations should be similar might not be correct. Could you comment on this?\n\nThe multual verification metric proposed in Sec. 3.5: assumes that methods for explanation are more reliable if they produce similar heatmaps. I think this assumption might be incorrect. For instance, Adebayo et al., NIPS'18 showed that several methods were able to produce very similar explanations. Yet, all these explanations were wrong since they were all biases towards edge-like structures of the input data and were not faithful to the decision-making process of the model being explained. Under the proposed mutual verification metric, these biased methods would have been flagged as objectively reliable.\n\nIn the evaluation, when conducting experiments on PascalVOC 2012, could you motivate why cropping the images is necessary? Isn't this removing some background clutter that could pose an interesting scenario on which the performance of explanation methods could be analyzed?\n\nThe manuscript had a good detailed and motivated beginning. In comparison to its beginning its conclusion was somewhat abrupt. Discussions presented in the evaluation section related to the four proposed metrics were very shallow and most of the time pointed towards the supplementary material. In this regard, it is not clear how much of an insight we got from the evaluated explanation methods. Moreover, having this relevant part in the supplementary, gives the impression that the manuscript is not self-contained. Perhaps some re-organization of the content might be needed.\n\nFinally, the manuscript claims the proposed method to be sufficiently general to be applied to different method. However, as was evidenced in the experiments, there were some methods, e.g. CAM, grad-CAM, Pert, where some of the proposed metrics were not applicable. Therefore the claims related to the generality of the proposed method should be relaxed."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThe paper proposes four metrics to evaluate explanation methods, namely 1) bias of the attribution map based on Shapley value, 2) unexplainable feature components, 3) robustness of explanations, and 4) mutual verification (whether two explanation methods can verify each other). These metrics focus on explanation methods that give attribution maps as outputs (such as a pixel-level attention or saliency map). Besides, it also evaluates nine widely used explanation approaches with these metrics.\n\nI would recommend for acceptance. Several explanation methods have been proposed to generate attribution maps. However, it is shown that it is hard to inspect the quality of these explanation methods visually (e.g., as mentioned in http://arxiv.org/abs/1810.03292). I believe the proposed metrics provide valuable insights on quantifying the performance of explanation methods. The evaluation results on nine existing approaches are also helpful to future work on explainable AI.\n\nHowever, I would also like to note that while the four metrics proposed in the paper reflect the desired properties of explanation methods, they should not be the only criteria. The desired criteria depend on the application, and methods without these properties could also be potentially useful or even better in some cases. For example, instead of giving an unbiased attribution map, it would be sometimes more helpful to provide a discrete binary mask of regions of interest (e.g., for downstream image processing)."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper considers the important problem of evaluating explanation methods for neural models without recourse to `ground-truth' explanations. \n\nThe authors rightly highlight that plausibility (to humans) does not imply faithfulness to the underlying models. They then define a suite of metrics, largely drawing upon known techniques, to evaluate explanations. For instance, the authors propose evaluating the agreement between model attributions and Shapley values. The latter are approximated in two ways: Only pixels with high (or low) attribution values are considered; and sampling is used to make estimation tractable.\n\nOverall, this is a nice paper that investigates an important problem --- I particularly appreciated the comparison of multiple attribution methods across these suite of metrics. However, the results are something of a mixed bag, and I was left without a clear takeaway. I am not sure the ICLR audience will get much out of this submission. It would be nice if the authors could provide additional guidance concerning the interpretation of the various metrics they propose, and for which scenarios they view these as appropriate. \n\nStrengths\n---\n+ The topic of explainability is of increasing importance, and the authors focus on an important question regarding this: How can we evaluate attribution methods? \n+ The approach of defining a suite of metrics, rather than only one, is welcome given that different applications will call for different types of attribution. \n+ The comparison of existing methods for attribution across a range of metrics is a nice contribution. That said, the results are sort of a mixed bag and it is not obvious what one should take away from this. \n\nWeaknesses\n---\n- Much of this work (and the proposed metrics) seems to derive from prior efforts. I'm not sure how much novelty there is here, although there is a value in bringing these metrics together.\n- To the latter point, however, the authors do not really speak to the pros, cons, and applicability or relevance of the different metrics considered. A user of this suite of metrics would therefore be left with four separate measures, and it is not immediately clear how to interpret these, or what the relative performance of explainability methods with respect to these metrics tells us. \n- I am not sure I followed the unexplainable feature component metric (3.3). I found the notation and terminology here difficult to parse; can the authors please clarify what they mean by \"unexplainable feature components\", and also clarify what $f_I$ represent (I think induced feature representations of the image I but again am not certain). From what I gather this is somehow measuring the peakiness (or lack thereof) of attribution values, but it is not obvious to me that this is an inherently desirable property.\n- The robustness metric seems to make the assumption that the model does not capture interactions, i.e., a pixel j may be important only when seen together with some other pixel i. I'm not sure this assumption is warranted; perhaps the authors can elaborate to provide further intuition.\n\n"
        }
    ]
}