{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents an analysis of the languages that can be accepted by a counter machine, motivated by recent work that suggests that counter machines might be a good formal model from which to approach the analysis of LSTM representations.\n\nThis is one of the trickiest papers in my batch. Reviewers agree that it represents an interesting and provocative direction, and I suspect that it could yield valuable discussion at the conference. However, reviewers were not convinced that the claims made (or implied) _about LSTMs_ are motivated, given the imperfect analogy between them and counter machines. The authors promise some empirical evidence that might mitigate these concerns to some extent, but the paper has not yet been updated, so I cannot take that into account. \n\nAs a very secondary point, which is only relevant because this paper is borderline, LSTMs are no longer widely used for language tasks, so discussion about the capacity of LSTMs _for language_ seems like an imperfect fit for an machine learning conference with a fairly applied bent.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #6",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary\n-------\n\nThe authors investigate (subclasses of) generalized counter machines with respect to their weak generative capacity, their ability to represent structure, and several closure properties. This is motivated by recent indications that LSTMs have comparable expressivity to counter machines, so that the formal properties of these machines might provide indirect insights into the linguistic suitability of LSTMs.\n\n\nEvaluation\n----------\n\nI also reviewed this paper for SCiL a few months ago.\nWhile I had major reservations back then, I am happy to provide a more positive evaluation this time as the authors have done some revisions that clear up many points of confusion.\nI have to add two caveats, though.\nFirst, I am a bit disheartened that the authors chose not to adopt many of the excellent changes suggested by another SCiL reviewer (who went way beyond the call of duty with their multi-page review).\nSecond, I did not have sufficient time to check all proofs for their correctness.\nIn many cases the strategies strike me as intuitively sound, but my intuition tends to miss edge cases.\nNonetheless, I think that this paper, albeit a bit of a gamble, would make for an interesting addition to the program.\n\n\n1) Weakness: Link to neural networks still unclear\n\nThe central weakness of the paper is still the link between neural networks and counter automata.\nBased on what is said in the paper, this is merely a conjecture at this point, not a well-established fact. \nWithout this link, the value of the paper is unclear.\nIf, however, this conjecture should turn out to be true, the paper would mark a very strong starting point for further exploration.\nThis makes it a gamble worth taking.\n\n\n2) Strong results, but lack of examples\n\nThe results are not trivial and provide deep insights into the inner workings of counter machines.\nIn particular the fact that counter machines cannot correctly represent Boolean expressions reveals key limitations on their representational power.\nThe semilinearity result is less impressive because of how limited the machines are that it applies to, and I'm not sure that the proof provides a good basis for generalization to more complex machines.\nThe authors might consider removing this part to clear some space for examples, which are sorely needed.\nThe formalism is abstract and unfamiliar to most readers, and a few concrete examples would greatly strengthen the readers' intuition.\n\n\n3) No investigation of linguistically important string languages\n\nAs the authors make claims about linguistic adequacy, it is surprising that there is no discussion of TALs, MCFLs or PMCLFs.\nThe grammar formalism of GPSG was abandoned because it was limited to context-free languages and could not handle those more complex language classes.\nSo if counter machines fail here, the issue of their linguistic adequacy is already decided without further probing semilinearity or representational power.\nAs far as I can tell, real-time counter machines cannot generate the PMCLF a^{2^n}, which is an abstract model of unbounded copying constructions in natural language (see Radzisnky on Chinese number names, Michaelis & Kracht on Old Georgian case stacking, and Kobele on Yoruba).\nNor is it obvious to me that counter machines can handle the copy language {ww | w \\in \\Sigma^*}, a model of crossing dependencies, although they can handle a^n b^n c^n (a TAL).\nIt should also be possible to generate the linguistically undesirable MIX language, which is a 2-MCFL but not a TAL.\n\n\nMinor comments\n--------------\n\n- As noted in my SCiL review, your definitions still differ from those of Fischer et al. 1968. What is the reason for this?\n\n- Theorem 3.1: \\subsetneq would be clearer than \\subset\n\n- p4, typo: the the\n\n- Proof of Theorem 3.2: Unless I misunderstand your modulo construction, your ICL only has resolution up to mod n. For instance, with mod 2 it can distinguish 2 from 3, but not 2 from 4. The CL can do that. Don't you need a second counter c_i' for each c_i, then, to keep track of how often you have wrapped around modulo n in c_i? That would still be incremental as you can never wrap around by more than 1 in any given update.\n\n- Sec 6.1: in all those definitions, if should be iff\n\n\nReferences\n----------\n\n@ARTICLE{Radzinski91,\n  author = {Radzinski, Daniel},\n  title = {Chinese Number Names, Tree Adjoining Languages, and Mild Context\n\tSensitivity},\n  year = {1991},\n  journal = {Computational Linguistics},\n  volume = {17},\n  pages = {277--300},\n  url = {http://ucrel.lancs.ac.uk/acl/J/J91/J91-3002.pdf}\n}\n\n@INPROCEEDINGS{MichaelisKracht97,\n  author = {Michaelis, Jens and Kracht, Marcus},\n  title = {Semilinearity as a Syntactic Invariant},\n  year = {1997},\n  booktitle = {Logical Aspects of Computational Linguistics},\n  pages = {329--345},\n  editor = {Retor{\\'e}, Christian},\n  volume = {1328},\n  series = {Lecture Notes in Artifical Intelligence},\n  publisher = {Springer},\n  doi = {10.1007/BFb0052165},\n  url = {http://dx.doi.org/10.1007/BFb0052165}\n}\n\n@PHDTHESIS{Kobele06,\n  author = {Kobele, Gregory M.},\n  title = {Generating Copies: {A}n Investigation into Structural Identity in\n\tLanguage and Grammar},\n  year = {2006},\n  school = {UCLA},\n  url = {http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf}\n}"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proof properties of counter machines that have in recent work be suggest that LSTMs can model those. \n\nThe papers starts to mentions related work that relates automaton's and counter machines with LSTMs. These related work papers do some correlational experiments partly restricted in size, layers and architecture. They provide mostly empirical evidence that some behaviour is related to performance seen in LSTMs, GRU, etc. and similar to those in counter automata. \n\nThe paper makes then the point to take  the counter machine as a simplified formal model of the LSTM. However, I would read Merrill 2019 that counter machines could be model via LSTMs but are not limited or they are not an upper bound what they can compute.  The authors does some proves on counter automata and hopes to gain insights into the properties of LSTMs used for NLP or semantic analysis and this would provide insights for the use in NLP.  It seems to me that the paper claims that counter automatons are an upper bound for the computation power of LSTMs. In the way I read this seems at least not well formulated or too strong.\n\nI would not follow the conclusion that 'A general take-away\nfrom our results is that just because a counter machine (or LSTM) is sensitive to surface patterns in\nlinguistic data does not mean it can build correct semantic representations'. \nThe argumentation is flawed as counter machines are not an upper limit of expressiveness of LSTMs nor do they describe well what they do. That one can use LSTMs to compute languages that counter automata can do too means not that they could do more. The property of Counter automata are useful for instance to build phrase structures meaning they can be use to express scope and keep track of. However, deeper layered networks are widely used to put structure over the scopes (arguments) to connected them in a higher order fashion.   \n\nThere are many paper which show empirical how to build semantic or syntactic structures using LSTMs - also in already quite well in seq2seq fashion. \n\nThe more theoretical part looks fine to me and could be of value to readers. Nevertheless, the authors could considere to revise  their claims as they are not well supported by the evidence provided in the paper nor pervious literature cited. \n\n   "
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "Motivated by a link between LSTMs and counter machines (suggested by recent work, e.g. Merrill, 2019 et al.), this paper studies the formal properties of counter machines (and LSTMs by extension) as grammars, in hopes of discovering why LSTMs perform particularly well in language tasks despite having no obvious hierarchical structure.\n\nIt makes the following contributions. It shows that: (1) many variants of counter machine converge to the same formal language, (2) the counter languages are closed under common set operations (e.g. intersection, union, and complement), (3) counter machines are incapable of evaluating boolean expressions, and (4) only a weak subclass of CLs are sublinear (and most are not).\n\nWhile this paper gives thorough proofs, I would have liked to see more connection to practical NLP with some experiments. Also, I would have liked to see more concrete takeaways from this paper: if correctly detecting surface patterns doesn't mean that LSTMs build correct semantic representations, what can ensure that LSTMS do have a correct semantic representation? \n\nAs this paper is far from my area of expertise, I'm willing to change my score based on my co-reviewers."
        }
    ]
}