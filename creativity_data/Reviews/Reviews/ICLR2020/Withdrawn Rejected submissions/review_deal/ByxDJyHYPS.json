{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a model that learns to extract a chain of sentences as evidence for answering multi-hop questions. The chains are annotated via heuristics (summarized below). Then they train a model to identify such sentences and then pass a set of chains to a reader model which reasons with them to find the answer.\n\nTreating each sentence as a node, they create a graph by connecting two sentences if they share an entity (as determined by running a tagger). Two sentences also share an edge if they are in the same paragraph. After creating this graph via the above heuristics, they exhaustively traverse the graph to create chains that lead to the answer entity. Finally the chains of shortest length and the one with highest question overlap (computed with Rouge) are preserved and are used as training data for the chain extraction model.\n\nNext, a paragraph is encoded by concatenating it with the question and passing it through BERT. Sentence representations are created by max pooling over the sentence span. After that, they train a pointer network style model that learns to select the sentence conditioned on the previously selected sentence. At test time, a beam of chains are selected. \n\nThey test their method on two datasets (Wikihop and HotpotQA) in a closed domain setting. They achieve favorable results in both outperforming baselines. Ablation studies are also performed showing the efficacy of the chain extraction model. Finally, they perform a small human study where crowd workers label whether the retrieved chains make sense.\n\nStrengths:\n* The paper is written fairly well and the ablations are well done\n\nWeaknesses:\n1. I am a bit concerned if the heuristic method of obtaining chains of reasoning would work in the more realistic open-domain setting of HotpotQA. The graph will be much larger and the number of chains that would be recovered by doing question overlap would be quite high. It is important to test whether the heuristic method would generalize in that setting (such as the more ad-hoc ones such as removing entities that occur more then 5 times). Moreover, it has been shown, that in the distractor (closed-domain) setting, all questions don’t really need multi-hop reasoning and also noted in the paper, the results of beam search often have same sentences. Therefore, I think experimentation on open-domain setting is really important in this case.\n2. I am also not very surprised by the fact that the heuristic based annotation of chains is actually quite effective in this setting. Because, if you think about it, while creating the HotpotQA setting, the data set creates and crowd workers also followed a similar paradigm of finding a bridge entity and asking a question for which the answer is present in the bridge entities document. Therefore, the process of connecting sentences that share the same entities replicate the data collection process which might be a result of the improved performance. The authors should include a discussion regarding the same.\n3. Missing citations: Another way of doing multi-hop retrieval which do not necessarily follow the data generation process is via query reformulation. Two recent work — Multi-step retriever reader interaction (Das et al., 2019) and Multi-Hop Paragraph Retrieval for Open-Domain Question Answering (Feldman and El-Yaniv, 2019) do something similar. It would be nice to include citations and discussions regarding this approach. Recently Godbole et al 2019 — (Multi-Step entity centric Information Retrieval for Multi-hop Question Answering) proposed an approach to generate chains but in the more practical open-domain settings. The paper should also cite that work. However, given that this work is very new, I am not penalizing for missing the citation.\n\nOverall, I am a little skeptical about how the method proposed by the paper would generalize to an open-domain setting and I think it is important to include the results for the same."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper is tacking the problem of multi-hop machine reading.\nThe method is evaluated on hotpotqa and wikihop datasets.\nThe model consists of two steps: \nFirst, the model learns to retrieve supporting facts.\nTo do so, the authors propose a heuristic method to retrieval \"reasoning graph\" based on entities present in the question and the text.\nIn the method, The nodes are basically the sentences, the edges are the adjacency of sentences and the presence of shared entities.\nEach reasoning path starts from the question and ends at the reach of the answer.\nFinally, 2 strategies are used to select the path 'shortest' and 'ROUGE'.\nFrom this dataset, an RNN based extractor is trained.\nA first question is why not using the supporting facts training signal which is present in both datasets.\nIt would have been useful to compare this heuristic extraction method for the training dataset extraction to the sole use of the supporting facts as this first step of the method is considered as the originality.\nThen, based on the supporting fact extraction, an SoA BertQA model is used to perform the extraction.\nAs a second step of the method, the method seems to produce a quite marginal improvement compared to the current state of the art."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper proposes to solve question answering (QA) tasks using a two step procedure. 1) extract sequences of sentences with the relevant facts. 2) Use these as inputs to a \"standard\" model, e.g. BERT.\nThe sentence sequences are extracted using a pointer network over the entire document collections and beam search. \nThe sentence extractor network is trained using heuristically generated \"gold\" sentence sequences. \nThe heuristic uses Named Entity Resolution (NER) and coreference resolution to create a graph of sentences by linking sentences which contain the same named entity. Additionally sentences are linked to the sentence following it in the same paragraph. Given the graph the heuristic enumerates all paths from the question (also part of the graph using NER+coreference resolution) to the answer.\nThe \"best\" path is picked using a scoring function.\nGiven a trained sentence sequence extractor the \"standard\" QA model is trained on the extracted sentence sequences to output the answer.\n\nThe paper reports state-of-the-art (SOTA) results on WikiQA and \"strong\" results on HotpotQA.\n\nI've chosen to not recommend this paper at ICLR, but I'm not very confident in this recommendation. On one hand, the model is bit a kludge and there's not a lot of truly novel ideas presented. On the other hand SOTA results are always worth at least understanding. Some of my uncertainty is because I'm not familiar enough with these two datasets to judge how significant the improvements are.\n\nThe results presented in table 2 show the proposed method having the best score on HotpotQA, but in the body of the text SOTA is not claimed. If this is indeed not SOTA, then please include SOTA in the table such that the reader can understand how well your model performs.\n\nWhy train the sentence extractor in the first place as opposed to using the heuristic directly? If I understand correctly the heuristic could be run at test time, since it doesn't require any labels or anything only present in the training set. Does the authors think that the trained sentence extractor can become \"better\" than the training data? \n\nIs the first step of extracting sentences a good idea in its own right or is it mostly a computational concern? Would the authors still recommend it if they had ~infinite computational resources, e.g. if they could fit BERT on the entire document corpus in memory? If yes, why? What information or bias does this step add or encode?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}