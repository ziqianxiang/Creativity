{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper received mixed scores: Weak Reject (R1 and R2) and Accept (R3). AC has closely read the reviews/comments/rebuttal and examined the paper. After the rebuttal, R2's concerns still remain. AC sides with R2 and feels that the generated interpretations are not convincing, and that the conclusions drawn are not fully supported. Thus the paper just falls below the acceptance threshold, unfortunately. The work has merits however and the authors should revise their paper to incorporate the constructive feedback.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a variation on the generator of GANs. The authors modify the generator by adding a concept of \"blocks\" which are randomly activated based on part of the random input vector. It is similar to adding random dropout in the generator, except that the dropout would apply to larger sets of activations instead of single component.\n\nThe authors also add a diversity term to force the different blocks to have different blocks. This is a term based on the L2 distance between the weights of different blocks. Although this term is ad-hoc and could probably be refined into something more grounded in theory, it should indeed provide some diversity.\n\nThis block structure allows for more understanding of what each layer of the generator does, since it is easy to change the discrete variables that switch blocks. The paper presents an empirical evaluation of what each switch does, and show that concepts are well disentangled between layers (for instance, one layer changes the background whereas another one changes the color of the foreground).\n\nThey also show that blocks can be added after training is done which is a nice property for incremental training.\n\nThey also show that this framework can train a generator without non-linearities (except for the block switching), which could potentially simplify the analysis of such networks.\n\nGenerated samples are presented up to 128x128 pixels which, although far from state of the art, proves that the concept works."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper addresses the issue of interpretability of GAN generation through an alternative approach to the introduction of variability. To seed the generation, instead of providing a random input vector (typically sampled from a standard Gaussian distribution), the authors instead modify the generator architecture so as to allow for randomization in the routing: each layer is replaced by a bucket consisting of several blocks, and in forward propagation only through randomly chosen blocks. In this case, the input vector is chosen to be a constant - the only source of randomization\nprovided to the generator is in the choice of blocks through which to propagate. The explanability derives from the tendency of blocks to associate with a common interpretation after training. Their use of blocks necessitates the introduction of a block diversity loss, to discourage mode collapse. The scheme is referred to as RPGAN, for \"Random Path GAN\".\n\nThe main strengths of the paper:\n\n(1) Their proposed approach is highly flexible. In principle, any underlying GAN architecture can be adapted by assigning each layer to a distinct bucket, and then replicating the layer across the blocks. \n\n(2) Experimental results do show that different block sequences are associated with common image characteristics after training, especially for the initial and final layers.\n\n(3) The use of non-standard ways of introducing stochasticity to GAN generation is an interesting idea in itself.\n\nThe main weaknesses:\n\n(1) Although the authors provide experimental examples showing images associated with various paths through the architecture, is not clear how interpretations can be associated with these paths. In the examples presented, there seems to be a tendency for greater interpretability at the initial and final layers, with the explanation given for the intermediate layers being less convincing.\n\n(2) The number of experimental examples is low, yet the authors draw rather firm conclusions (end of Section 4.1) regarding interpretability across layers. I am not sure that their conclusions adequately capture what is going on here, nor am I convinced that they generalize to other situations.\n\n(3) The number of buckets limits the numbers of explanations. Essentially, the method has the same difficulties as in clustering, where specifying too many or too few groups can profoundly influence the nature and quality of the result. Although the authors do discuss an approach by which the number of buckets can be incrementally increased (thereby allowing for variation in the number of explanations generated), the experimental evidence is insufficient.\n\n(4) Presumably, the replication of a layer across the blocks assigned to its bucket would require more training data and/or greater training times. What is the relationship in both time and quality between the original GAN network and its RPGAN versions?\n\n(5) There are many presentational problems with this paper, in grammar, vocabulary and terminology, sentence structure, etc.\n\nOverall, in its current state (not least due to presentational issues) the paper appears to be below the acceptance threshold.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes the Random Path Generative Adversarial Network (RP-GAN) to serve as a tool for generative model analysis.  The main idea is to have several different buckets in each block of the generator and then train the generator with random paths. To interpret the features captured by each block, the authors unfreeze one block and show the variance of the generated images via different buckets.\n\nThe contribution of the paper is limited. Most of the observations proposed in this paper are just to confirm the findings in the recent paper Bau et al (2019). And the authors did not clarify why their methods are better than the previous work. In addition, this work changes some standard ways of generating images. For example,  they use a fixed input vector Z rather than a random vector Z following Gaussian distribution. Then when they claim that their findings are also valid to standard GAN generators, e.g., SN-ResNet, they add the noise to the weights in SN-ResNet and claim that we can conclude similar findings as RP-GAN. Therefore, my question is if adding noise to the weights to a generator is sufficient for the interpretation of the generator, why do we still need this work for further interpretation?\n\nMinor:\nIn Figure 3, there are 10 images in each line but the number of buckets in each block is 40 for CIFAR10, as you claimed in Section 4. It should be clarified how you get those 10 images from 40 blocks in the unfreezed bucket.\n\nIn conclusion, I vote for a weak reject for this work. "
        }
    ]
}