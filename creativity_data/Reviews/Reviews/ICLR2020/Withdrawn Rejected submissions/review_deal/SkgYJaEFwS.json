{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors claim to have obtained a pac-bayes bound on generalization error of unseen tasks. They then propose a learning algorithm that empirically seems to be performing well and making better-calibrated predictions.\n\nThe experimental results seem promising. However, the theorem statement seems trivial (see detailed comments below). Also, the abstract and introduction overstates the contributions. Therefore, I cannot recommend acceptance at this point. If the authors derive some non-trivial theoretical results, I would be happy to change my decision. \n\nDETAILED COMMENTS.\n\nIn the introduction, the authors highlight two problems that exist in current meta-learning approaches. One of the problems that the authors mention and claim to have addressed in this paper is that most of the state-of-the-art algorithms come with no generalization guarantees. However, I do not see how the proposed algorithm comes with any sort of guarantees. I would be grateful if the authors could justify and explain.\n\nQuestions about Theorem 2:\nIs the population for each task assumed to be finite? Throughout the paper, I only see the loss evaluated on draws from the validation or training set, and the risk term (expected loss) never appears. In which case, how can there even be a generalization bound proved? Is the paper about transduction? If so, that has to be made clear in the abstract, introduction and the rest of the paper.\n\\mathcal{Y}_i^v is a validation set (of labels). What does the first expectation in Eq. (1,4 and others) mean? Can I replace it with an average over \\mathcal{Y}_i^v ‘s?\nI cannot see how the first term in Equation (5) ( \\hat \\mathcal{L}_i^v ) is different from  the term stated in Equation (4) ( \\mathcal{L}_i^v ). If it is the same, than Corollary 1 seems to be trivial. Obviously, if one has two non-negative terms A and B, then A<=A+B. I do not see how a PAC-Bayes bound is needed there at all.\nSimilarly, Thm (2) has a trivial equivalent.\nLet’s assume that Theorem (2) holds and perhaps there are some typos that make it trivial. Why is it interesting? It is bounding the cross entropy loss on the validation set (so it has nothing to do with generalization, maybe at best transductive bounds if there is some kind of mistake on the right hand side) in terms of quantities that depend on the same validation set. Where does the training set appear? How about expected loss?\n\nRemark 1. I don’t think that McAllister 1999 has results on unbounded loss functions, and definitely not in Section 5 (conclusions). Germain et al. 2016 (PAC-Bayes meets bayesian inference) does.\n\nThe quality of the work could be improved if all of the assumptions and approximations were stated clearly and numbered. \n\nWhy the approximation in Equation (12) is reasonable? What are the conditions under which the approximation is good? What if the model is misspecified (which it is)? “We expect under this modelling approach, the discriminator model is approximately correctly-specified.” - I see no reason to expect that. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary\n-------\nThis paper addresses the problem of few shot learning where the objective is to learn a model for T parallel tasks, each of these having few learning samples. The first contribution of the authors is to propose a PAC-Bayes bound over the cumulated risk over the T task. As expected with PAC-Bayesian theory, the bound involves a KL divergence between prior distribution over the set of hypothesis and a posterior that is obtained by learning. The prior is supposed to be shared against the tasks and the posterior is task specific. Prior and posterior distributions are trained by variational inference. An experimental evaluation is made on a synthetic data for regression and on an existing benchmark for classification.\n\nEvaluation\n--------\nMy main issue come rom the fact that the PAC-Bayes seems not valid since the prior is obtained after having seen the training data which is not allowed in the setting considered by the authors. For the variational inference, the authors seem to combine many existing idea to perform the inference efficiently. It is not clear if there is a particular contribution here, but at least combining the ideas in an appropriate way is already an interesting contribution. Evaluation is ok but restricted to only one benchmark for classification, the evaluation can maybe be extended. \n\nOther comments\n------------\n\n-In classic PAC-Bayesian theory, and in particular with McAllester's result, the priori must be chosen before having seen the data. This is required in the proof at some step to be able to exchange expectations over prior and learning sample respectively. As far as I understand the approach, the parameter vector \\theta that parameterizes the prior is trained iteratively over all the training instances of all the tasks which violated the assumption of the theorem.\n\nTo have data-dependent priors, the authors could consider the following papers:\n*E. Parrado-Hernandez, A. Ambroladze, J. Shawe-Taylor and S. Sun. PAC-Bayes Bounds with Data Dependent Priors. JMLR, vol 13, 2012. \n*G. Lever, F. Laviolette, and J. Shawe-Taylor. Distribution-dependent PAC-Bayes priors. ALT, pages 119–133. Springer, 2010.\n*G Dziugaite, D. Roy. Data-dependent PAC-Bayes priors via differential privacy, NIPS 2018.\n*O. Rivasplata, E. Parrado-Hernandez, J. Shawe-Taylor, S. Sun, C. Szepesvari. PAC-Bayes bounds for stable algorithms with instance-dependent priors, NeurIPS 2019.\n\n-The paper integrates PAC-Bayes theory and Bayesian inference, note the following reference that can help to position the paper with respect to these two fields:\n*P. Germain, F. Bach, A. Lacoste, S. Lacoste-Julien. PAC-Bayesian Theory Meets Bayesian Inference, NIPS 2016.\n\n-For the variational part, as far as I understand, the authors combine many existing ideas that are related to Bayesian inference. I may guess that the way the authors combine the different things is somewhat novel, but it could be informative if the authors can more clearly identify what is novel here.\n\n-In the experimental evaluation on classification, the baselines used for 1-shot and 5-shot learning are different. This probably because the authors have selected the best competitors for each problem, but I think it would be interesting to evaluate the performance of all baselines on the two types of problems as for Simba.\nIt is no clear in the setup if the other competitors use the same base classifiers, otherwise it is important to precise it.\n\nI also wonder to what extend the results for 1-shot learning is dependent on the 4-layer CNN network used, having the result for other architecture would be interesting. \n\n-One interesting aspect of PAC-Bayesian theory is the fact that it can produce informative bounds. When the theory is corrected, it could be interesting to complete the experimental evaluation by providing some bound values to check the information provided by theory.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper derives a novel meta-learning algorithm that uses the PAC-Bayes framework. Starting from the generalization error bound of vanilla theoretical PAC-Bayes results in single tasks, the authors extend it to the few-shot learning setting. Compared to the amortized Meta-Learner, the authors propose a more expressive way that implicitly models the shared prior and task-specific posteriors. Moreover, they use MAML algorithms to meta-learn the parameters of the discriminator. Experimental results demonstrate that the proposed method is competitive for the state-of-the-art.\n\nOverall, this paper is well written and easy to follow. The proposed method avoids estimating the correct weighting factor of KL divergence and has better representability than variational function-based algorithms, which might be significant to the community. In particular, the proposed method has a strong theoretical guarantee. One limitation is that the experimental results have no obvious improvement than existing methods. \n\nThough I'm not familiar with this research area, I think it is a good submission."
        }
    ]
}