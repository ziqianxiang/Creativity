{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a mechanism to construct an automatic text generator to produce biased text. The biased generator has such an effect that it would incorrectly associate a particular label-agnostic feature with the biased target label. The author also conducted experiments on existing SOTA models (BERT etc): namely training these models with the biased dataset to see the impact of learning from biased data.\n\nThis paper requires some clarifications and hopefully additional experiment results. I am leaning towards rejecting the paper for below reasons.\n\nThere are apparently two parts of this paper. The first part introduced a text generator (CARA). The second part consumes the produced text from CARA. Before consuming the output from this newly introduced text generator, should we evaluate the text generator itself by some metrics? We know that text generator evaluation is a challenging topic, but I expect some common criteria like \"fluency or readability\" and \"accuracy or relevance\" are measured somehow either by human judgement or automatic evaluation metrics. I understand that evaluation of the machine generated sentences is a difficult task , but without any evaluation the quality and correctness of the generated text cannot be justified. Furthermore, since this text generator is supposed to generate a biased dataset, should we also define \"bias\" and measure quantitively how well it accomplish this task (the amount of biasedness in the generated text)? \n\nIf the author could show that the text generator as a stand-alone module could approve its own value using some evaluation on the above two criteria i) basic text generation quality 2) biasedness evaluation , the proposed method would be well justified.\n\ntypos, nitpick\n* last paragraph in section 1 \"Our approach paves the wave for ...\". I guess you mean \"way\" instead of \"wave\".\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers the timely and important problem of 'bias' in datasets, and how these affect neural NLP models. The authors point out that at present it is difficult to undertake systematic study of biases and the degree to which models exploit these. To address this, they propose a method for generating (synthetic) \"biased\" datasets in a controllable fashion, via a generative model. As far as I know, this --- the idea of learning to generate 'biased' data to benchmark models resistance to them --- is a novel idea, and one with a fair amount of potential if the aim can be realized. However, I am not convinced that the specific method the authors have proposed here encodes or instantiates 'bias' in a meaningful way. \n\nIn particular, the authors do not actually define 'bias' in the first place. A formal definition of this seems necessary to advance the agenda of generating biased datasets, but the nearest thing provided is the Eq in Section 3; but this relies strongly on the assumption that $\\delta$ is a label-agnostic feature. Perhaps I am missing something here, but as I understand the method entails manipulating a subset of hidden representations for instances $z$ via $T$ (yielding $z'$) and associating these with the bias target class. Consequently, the decoder will generate a reconstruction $x'_b$. \n\nThe authors report that if we do not add the reconstructed (synthetic) train instances to the train set, the model classifies them `correctly', i.e., as per the original designation. However, if we do train on these, the model will learn to associate the $\\delta$ signature (however it is instantiated by the decoder) with the label samples have been changed to. Should we find this surprising? It seems to me both obvious and even desirable that a model would pick up on such a strong cue, if present (and it is indeed strong by construction, as per 4.1). The other result --- that models \"correctly\" classify inscribed examples if they are not seen during training --- likely reflects the smoothness objective imposed by CARA, so is also not terribly surprising. \n\nThese issues aside: Do we have any reason to believe that the $\\delta$-inscribed texts resemble the kinds of biases present in real data? Looking at Tables 1 and 2 do not obviously suggest so. What I would guess is that the $T$ manipulation in the latent space yields systematic patterns that the authors then explicitly and strongly associate with labels; again I am left wondering what it is the models \"should\" do in such cases? It seems that it would only be reasonable to hope the model would avoid using such signatures if it \"knew\" that they reflect sensitive attributes; otherwise, it is simply doing what the objective says to do --- minimize error. I do think it is interesting that model size does not seem to directly affect the propensity for finding these cues. \n\nMore generally, my feeling is that this is a truly interesting direction, but I think for it to be a compelling approach the authors need to formally define bias (the sort they are trying to simulate) and then convincingly show that generated samples reflect the sort of biases that plague real-world datasets. I realize this is no small ask, but with the current setup I am just not sure what I can take away from this other than: If you systematically introduce input patterns that are strongly correlated with target labels, neural networks will pick up on these. Without further assumptions or evidence, I don't know that this tells us much about 'bias' specifically.\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary\n========\nThis paper proposes a method to generated biased datasets for NLP. The method relies on a conditional adversarially regularized autoencoder (CARA), which is similar to the original adversarially-regularized auto-encoder (ARAE), but in addition is conditioned either on a label (in text classification) or on a label and another sentence (in natural language inference). The latent variable in this CARA model is also transformed by adding to it a label-specific value that represents the bias in the latent space. Then the generated sentences are presumably biased. \nThe generated texts are then used for training classifiers based on several pre-trained model, and it is shown that the learned classifiers are highly biased, while classifiers trained on the original datasets are much less biased. \n\n\nThe paper presents an interesting approach for studying the problem of bias in NLP datasets by generating biased datasets. However, the empirical evaluation is lacking in terms of text generation quality and classification quality. There are also unclear points in the methodology. Finally, the motivation may be more clearly explained. Please see the main comments below. \n\n\nMain comments:\n==============\n1. Motivation: the motivation to the work is not entirely clear to me. I agree that studying model robustness against bias is importnat. However, I'm not convinced by the desideratum for generating \"objectively biased samples that explicitly associate features to labels, regardless of label semantics.\" This may be far from the actual bias scenarios that occur in practice, and thus less practical. On the other hand, there may be an ethical concern with generating datasets that are biased w.r.t sensitive attributes. So there's a bit of a conundrum here, which is worth thinking more about. \n2. Generation quality: in work on language generation, the quality of the generated language needs to be properly evaluated. This typically means human evaluation for criteria like coherence, diversity, or faithfullness/consistency, depending on the scenario. At the very least, some proxy measures of quality should be reported, for instance perplexity for coherence and some similarity measure for consistency. This limitation is especially concerning given that the ARAE often produce text that is not of high quality. \n3. Bias triggers: I was not able to understand the bias trigger synthesis in sec 4 and 4.1. It's reasonable to transform z to contain label-specific information, so I can understand if z were to change to be far from P_base and closer to P_target. But then, 4.1 maximizes the distance from examples x ~ P_target, which means that delta is actually farther away from P_target. \n4. In general, I found it hard to follow the methodology in sections 3+4. On the one hand, there's redundancy between themse sections and subsections. On the other hand, the references to the algorithms are incomplete and misplaced, only referring to certain lines and after having described other parts. \n5. Classification quality: the quantitative results show how biased classifiers are, but not how good they perform. This is partly related to point (2) from above, but also can be assessed separately. What is the accuracy of classifiers trained on the generated/clean, when evaluated on generated/clean data? \n\n\nOther comments: \n===============\n- Related work: there is in fact at least one other work on a conditional ARAE:\nGu et al., 2019. DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\n- As a matter of presentation, it might be better to present the model with the simpler text classification case, and later explain how to extend to the sentence-pair case. \n- The various text examples should be described in more detail, to explain what they mean to convey.  \n- Do you have any explanation for the differences between the behavior of different models noted in the end of 5.1. \n\n\nPhrasing, grammar, etc.\n=====================\n- The abstract is very vague and generic, and can be made more concrete. \n- Abstract: myriad ways -> a myriad ways \n- 1: consider (Sap et al., 2019) -> consider Sap et al. (2019)\n- 3: may leads -> may lead; holdout -> heldout; is controllable -> is a controllable\n- 4: Algorithm 2 show -> shows\n- When referring to tables, figures, etc. in the appendix from the main text, indicate that they are in the appendix. \n"
        }
    ]
}