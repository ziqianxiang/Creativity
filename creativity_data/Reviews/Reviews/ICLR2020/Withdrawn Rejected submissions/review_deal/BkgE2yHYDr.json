{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper describes a data augmentation approach whereby noise is added to features (activations) late in a prediction network.\n\nThe value of this is based on the relationship between high feature weight and impact on classification decision.  By using this information to drive the augmentation noise, the augmentation can introduce more noise to more \"important\" features.\n\nThe performance is generally strong, but is somewhat worse than VAT+EntMin.  Additional discussion comparing these two approaches would be helpful. \n\nThere is some inconsistency in notation in the noise equation on page three. \\theta_{fc} is defined to be 128 x C, but \\bar{f} is R^128.  while f(\\bar{f} + noise) is used later.  It is unclear how R^128 is added to 128 x C without broadcasting or some other operation.  Detail should be provided here.\n\nOn page 5, the dropout probability is set to 0.8 \"because feature-based augmentation inherrently has its own regularization\".  I suspect this should be 0.2, not 0.8.  0.8 implied more dropout that the baseline of 0.5, and more regularization not less."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper describes a new approach for semi-supervised learning, in which uniform noise is added to the last layer of a DNN, scaled according to the magnitude of the entries/features of this layer; this is phrased as 'data augmentation at the feature level'.  Several other loss terms are also used---a parallel ensembling step, a \"consistency\" loss term, and a \"noise consistency\" loss term.  The result is a semi-supervised algorithm that is empirically compared with several other recent state-of-the-art semi-supervised algorithms, and outperforms them on most benchmarks.\n\nThis paper achieves slightly better performances on some state-of-the-art baselines; but the overall presentation and writing of the paper is very confusing, and more importantly, it is very unclear to the reader (a) what exactly is done, (b) why, and (c) which parts are actually important.\n\nTo start, the paper motivates the idea in ways that largely ignore a lot of prior work- for example, ignoring past work that has applied feature-level data augmentation or noising, attributing the idea of adding noise to prevent overfitting (a very old one) to Goodfellow 2016, etc.  Overall, the idea of 'feature-level data augmentation' is not novel, though the particular approach used here may indeed be unique (it is slightly unclear).\n\nMore importantly: the actual approach used is unclear due to inconsistent and incomplete terminology and explanations.  For example, g is defined as the augmentation function, but then elsewhere \"Aug(g(...))\" is written; confusing set notation is used; etc.  More importantly, in Sec. 2.1, the feature augmentation & ensembling are introduced (again, both of which have been done before), but then several more terms are introduced in 2.2 and it is unclear what they are motivated by.  For example:\n- L_consistency is presented as the MSE of the sum of the summed feature vectors of the labeled and unlabeled datasets- what is this motivated by?  And surely it is at least normalized given different dataset sizes?\n\nMore importantly than somewhat unclear presentation, is the fact that there is no ablation study done to reveal exactly what each component contributes.  Which ones are actually necessary?  Since many of these individual terms have already been proposed, which one / what aspect of them are actually causing the results to be better?  Without this kind of analysis, it's hard to take any kind of conclusions away from the paper or understand the results.  In this case, the paper must be accepted purely because it beats some SOTA results, which seems hard to do."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary of the paper:\nThis paper proposes a method for feature augmentation, whose aim is to prevent overfitting and improve generalization.\nThe feature augmentation is done using the last feature vector of the CNN, which is the input to the fully connected layer. The method applies noise to the weights of the fully connected layer that contributes the most to the output.\nIt presents results on two datasets: SVHN and CIFAR-10.\n\nDecision: Reject\n\nArguments for the decision:\nThe paper is missing references to some important recent works in the field of semi-supervised learning. \nFor example, the following papers present results on the same datasets, and could be compared with: Label Propagation for Deep Semi-supervised Learning, Iscen et al., CVPR 2019; Tangent-Normal Adversarial Regularization for Semi-supervised Learning, Yu et al., CVPR 2019; Mutual Learning of Complementary Networks via Residual Correction for Improving Semi-Supervised Classification, Wu et al., CVPR 2019; \n\nMore importantly, in [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms, A. Oliver et al. (NeurIPS 2018)], the authors raise a lot of questions on the current state of the art of the field and present a testbed to address the issues. These issues are related to the application of semi-supervised methods to realistic scenarios and could be taken into account in the next top-level publications in the field.\n\nIn addition, the method is not introduced to the reader in the most clear way.\nFor example, in the introduction, the authors talk about “stable targets”, but have never talked about “targets” before. The objective function is not discussed conveniently, for example, the loss noise consistency is not well motivated. The ablation study should also assess the contribution of the different losses. \n\nRegarding the experiments, besides the lack of comparison to SOTA results, and comments on important questions raised by Oliver et al., there are important details missing, especially in the semi-supervised field. For example, how is the validation set built, and how are the optimal hyperparameters found?\n\nQuestions to authors:\nAt some point, the authors write “Augmentation in the latent space is suitable for regularization as it can produce new data point that is more plausible and comprehensive.” Why is it so? What means “plausible and comprehensive” in this sentence?\n\n\nAdditional feedback related to things that did not impact the score, typos, etc:\nOn abstract, typo: Filp - flip\nThe figure could use the same symbols used in the algorithm, so to facilitate the reading of both.\n\nOverall, I would recommend the authors to follow the recommendations of Oliver et al., mentioned above, in order to improve the paper and be in line with state-of-the-art works."
        }
    ]
}