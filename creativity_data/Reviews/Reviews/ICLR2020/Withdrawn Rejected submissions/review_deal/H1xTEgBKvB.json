{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper introduces a novel way of augmenting a seq2seq model by an external memory module. The obtained system achieves promising results on the PubMed datasets.\n\nAlthough the paper is clear-written and the idea is well-received, I found the idea proposed is relatively incremental, given the advances brought by the end-to-end memory network. The experimental improvement also seems a bit marginal, compared to the chosen baseline model.\n\nThe ablation study is necessary for architecture like this. The result of the regularization term is interesting.\n\nIn theory, the architecture can be plugged in any seq2seq system. Would this technique help to enhance other tasks, such as machine translation or other?\n\nWith that being said, I'd recommend trying adding the memory module to other architectures and experiment it on tasks with more variety.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a neural model, Mem2Mem, to summarize long texts. The main contribution is to extend the typical RNN Encoder/Decoder architecture with a memory module used both for selection (from the encoder) and generation (for the decoder): essentially, the encoder transforms a document into L vectors (one per sentence) and uses an r-head attention mechanism to extract r sentences to store in the memory module. The decoder performs text generation by reading a subset of the memory and updates the memory bank during summary generation.\n\nMy main concerns about this paper are the followings:\n- Novelty is a little limited: most architectural pieces were already present: either in Lin et al. for the regularization loss to promote diversity and sparsity for the memory bank choices, or in Benmalek et al. for the memory updates.\n- The experimental section is limited to a single dataset.\n\nOn the positive side, the results are promising (especially in terms of ROUGE-L), even if limited to a single dataset, and previous summarization MAED (memory-augmented encoder/decoder) models have mostly focused on short documents or extractive summarization. I also appreciated the author presenting all the necessary details for training (in the Appendix).\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2478",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "<Strengths> \n+ This paper proposes the Mem2Mem mechanism that is applicable to general seq2seq models for summarization. The proposed approach improves the HRED base model for abstractive summarization of PubMed dataset. \n+ The paper reads very well.\n\n<Weakness>\n1. The technical novelty may need to be further justified.\n- The memory network-based model has been already proposed for the task of abstractive summarization (Kim et al. 2018), although it may not have read/write mechanism unlike Mem2Mem. This distinction may need to be clearly stated in introduction.\n- The key novelty of this work is Mem2Mem mechanism, which consists of encoder memory (section 3.1) and read/write decoder memory (section 3.2). \n- The encoder memory is rather standard with attention regularization, which is adopted from Lin et al (2017). Also, the extractive nature in the encoder memory is common standard in most memory network models.\n- The proposed decoder memory may be also incremental over the scratchpad (Benmalek et al. 2019), which also proposes memory read/write mechanism for general seq2seq models. As far as I know,  the scratchpad was not applied to abstractive summarization tasks. Is it only big update from scratchpad to Mem2Mem? The difference assertion below in Eq.(20) is not convincing but still looks incremental improvement.\n\n2. More critical weakness of this work is limited experiments. \n- First of all, the proposed approach is evaluated only on PubMed (Cohan et al 2018). I have rarely seen previous work in text summarization that is only evaluated on a single benchmark dataset. A couple of more datasets, for example newsroom dataset, should be tested to verify the success of the proposed algorithm. \n- The performance improvement by Mem2Mem is too marginal over the baseline HRED. As shown in Table 1-2, each key component of Mem2Mem only slightly enhances the base model. Especially, the effectiveness of Mem Transfer may need to be further justified empirically.\n- In my opinion, such small performance gap in a single dataset hardly convinces that the improvement by the proposed method is statistically meaningful.\n"
        }
    ]
}