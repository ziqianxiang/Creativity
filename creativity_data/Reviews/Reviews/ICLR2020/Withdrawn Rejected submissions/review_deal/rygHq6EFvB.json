{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a new method for Multi-Source Domain Adaptation (MSDA) based on generative adversarial networks (GANs). The proposed method is called TriGAN. The authors assume that the appearance of an image depends on the content, the domain, and the style. Specifically, the proposed method progressively removes the style and domain-specific statistics from the source data. Then it re-projects obtained invariant representation onto the pixel space using the target domain and style.\n\nThis paper should be rejected because (1) the proposed method seems ad-hoc, and (2) the performance gain over another competitive method is marginal.\n\nThis paper assumes that the appearance of an image depends on the content, the domain, and the style. It is not clear whether this assumption is general or not. Evaluations on two datasets (Digits-Five and Office-Caltech10) is not enough to validate this assumption. More evaluations on diverse datasets are required.\n\nIt is unclear what is the unique and intrinsic problems for multi-source domain adaptation (MSDA). The authors should explain why the proposed method can solve those problems. For example, it would be better to mention why three factors are able to reduce the number of learned translations by theory or practice.\n\nIn table 1, the average accuracy of TriGAN is better than M^3SDA. However, M^3SDA wins two categories, whereas TriGAN wins three categories among five categories. In Table 2, the gain of accuracy (TriGAN:97.0%, M^3SDA: 96.4%) is marginal. From these results, it can be said TriGAN is competitive to the SOTA, but it is hard to conclude that TriGAN is better."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces a model for multi-source domain adaptation. All the domains share the same label set. Given an image in domain A, the model learns to generate another image with the same content, but with characteristics that depend on another domain, say B, and on a conditioning image from domain B. A classifier is then trained using this transformed image. The generative mapping relies on parametrized transformations which perform some normalization of their input followed by a shift-scaling transform, generalizing the batch-norm idea. The intuition presented by the authors is that they first remove domain and style information from the input image so as to keep only the content information and then inject domain and style information of an image from another domain. Training is performed via an adversarial criterion. Experiments are performed on two multi-source datasets : digits and Office Caltech.\n\nThe model combines several already existing ideas, but overall propose an original  system for the MS domain adaptation task. The general idea is well motivated and the description is relatively clear. I found however the distinction between domain and style not so convincing, and in the qualitative evaluations I cannot see what distinguishes them. Also it is not clear, what is the role of the whitening operation in the decoder – why normalizing the “domain independent” representation with the new domain statistics. The “style” paragraph in section 3.2.3 could be improved, as such it is not easy to follow the justification of the different operations.\nThe experimental part in this version of the paper is however not so convincing and many details are lacking so that it is difficult to appreciate the performance of the model. In table 1, where different models are compared, there is no indication on where the numbers come from. Did you rerun all these models or did you pick the numbers from other publications? How do you make sure that these numbers are comparable: in domain adaptation, the performance heavily relies on the choices made for the NN architectures for example so that comparisons with generators of different complexity make no sense. How did you adapt the different models to the multi-source context? Why no confidence intervals? Given the instability of GAN training, the variance might be quite high. How did you perform model selection for your model and the others in the unsupervised context? I understand that you have been using one multisource configuration for selecting the hyperparameters, which corresponds to some form of supervision, what about the  other baselines, did they use the same setting?\nFinally, the ablation study shows that the performance indeed varies when removing or changing some component, but in quite small proportions compared to the gain provided by the proposed model w.r.t. the baselines. If none of the individual components alone is responsible for the performance increase w.r.t. the baselines, where does the improvement come from?\nOverall I found the model an interesting contribution, but the evaluation raises too many questions.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\n-------\nThe paper addresses the problem of Multi-Source Domain Adaptation for a specific application on image classification. Each image is assumed to depend on 3 factors: the domain, the style (low level feature variations) and the content. The approach proposes to use a generative model constituted of generator G that can take as input an image of any domain, and G contain 4 blocks each having a particular objective: (1) address the style discrepancy, (2) address the domain discrepancy, (3) impose a specific domain (4) impose a specific style. The output is an image with the same content as input, with a chosen style and a target domain. The content should help to learn an invariant representation across multiple source domains. The generative part can be used to generate \"target instances\" from which a classifier is then learned.\n\nEvaluation\n--------\nThe proposed method makes use of 2 blocks existing in the literature and the authors propose the use of 2 novel blocks. The methodology appears to be interesting in the context of image classification and the results presented are good. There is no theoretical foundation for the proposed model, but the interest of the components are experimentally assessed by an ablation study. While the method is specifically designed to image classification - which in a way reduces a bit the scope the possible applications but this is a minor remark - I find the experimental evaluation a bit limited in the sense that authors use mainly small or relatively easy datasets. Some parts of the method certainly deserves more justification.\n\nOther comments\n------------\n\n-The two datasets Digits-Five and Office-Caltech are relatively easy and low-resolution datasets. I think larger experiments on more recent datasets such as Office-Home (http://hemanthdv.org/OfficeHome-Dataset/) or Visda are important here. In particular, an more complex analysis on the possible style transfer and domain characterizations on large domain will reinforce the study of the paper.\n\n-I may have missed, but I did not really understand in the experimental setup how many additional target instances are used to train the classifier because I guess that the random aspect of the generator can help to generate multiple additional data but in this context the quality of the model can dramatically decrease. \nSo I would like more discussion on the real applicability of the generative part of the method classification.\n\nAnother point unclear for me in the setup is the use of target labelled information: are they use for learning the classifier? If so a comparison with a classifier only trained with target labelled data should be added.\nOtherwise, the approach is strange since target domain is included as one of the domains and the number of target labelled data seems important in the experiments.\n\n-I have a question about Hyperparameter tuning, in particular wrt the digit classification experiment: only one configuration is used to find the parameters for the others. 1st this requires to have enough target labels, second it assumes in some way that similar information is shared among all possible configurations making all the adaptation tasks similar which can explain why only one tuning can work.\nComparisons on other datasets would probably not allowed such a strategy.\n\n-The ablation study is interesting, but I do not find the difference in terms of result very significant, one can wonder if it is not possible to achieve the target avg. accuracy by a finer hyperparameter tuning.\n\n-Training generative models including GAN often leads to optimization issues and instability. The proposed architecture is rather complex and I would appreciate if the authors can discuss the difficulty encountered for training their model (note that this can be in part related to the second item).\n\n-If possible, I would like to have a characterization of the style block: what are the style changes that can be done and what are those that cannot.\n\n-The authors consider that learning invariant representations is an holy grail for domain adaptation but it must be noticed that actually it may lead to undesirable situations are reported in the following recent paper:\nHan Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant representation for domain adaptation. ICML 2019.\n\n-A suggestion: As far as I understand, the role of blocks (3) and (4) is only to produce a target image in order to apply loss functions to train G, but they do not extract any more useful information of the input image. In other words, all useful information G seems to have from the input is only captured by blocks (1) and (2). I would be interested to see the result of the following strategy: Train G, then use the output after block (2) (Domain invariant representation) as the input of the classifier to train it. I think it would be interesting to compare this idea to TriGAN. \n"
        }
    ]
}