{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors propose a simple and highly parametrically efficient module named TAM, recursively encouraging neighbouring channels to collaborate in order to produce a spatial attention map as an output. \n\nThe paper offers a good read and diagrams are informative.\n\nBelow are comments for improvement and clarification.\n- The authors claim the effectiveness of capturing inter-channel relations based on performance but it would be great if they can illustrate inter-channel relations.\n\n- Also, since the authors mentioned other works of attention modules (NLNet, GCNet, BAM, CBAM), it would be good to compare with them (although they compared with CBAM)\n\n- In Figure 6, on ResNet-50, TAM shows faster convergence and lower Top-1 Error but on ResNext-50, it is hard to see the difference. Any comment on this?\n\n- It would be interesting to see any segmentation task and performance comparison by imposing nonlinearities between neighboring channels and considering inter-channel relations. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Paper Summary:\n\nThis paper proposes to aggregate local information across channels in a convnet with the objective to emphasize / de-emphasize location in the feature map with a local gating for each position, shared across all channels.\n\nReview Summary:\n\nThe paper reads well and is technically sound. Experiments are performed over a few datasets. The approach could be related to other gating strategies and the results would be more convincing if the authors would take setup and accuracy from prior work. The use of the word \"attention\" is slightly misleading given it does not refer to the specific attention module which has become standard in recent years.\n\nDetailed Review:\n\nTo me, the contribution relies on three ideas:\ni. introducing gating maps that gates each feature map, i.e. sigma(g) * f\nii. sharing the gating pattern across channels. sigma(g) is the same for all channels.\niii. computing sigma(g) through recursive three computation, which allows very low parameter counts.\n\nI feel it is necessary to evaluate these ideas gradually with ablations and refers to related work on gating. For gating units, one can refer to GLUs introduced by Dauphin et al 2016. It would make sense to verify if ii. is a good idea, i.e. verify if GLU with sharing across channels perform better than without and later introduce recursive trees. Also verifying that computing gating potentials from point-wise convolutions rather than with larger kernels need to be done.\n\nI would strongly advise against using \"attention\" for anything which as no normalization across competing locations, \"gating\" is much more appropriate to describe your proposal.\n\nFor the experimental evaluation, the presented results are below the original wide resnet paper (BMVC-16) for Cifar10 (wrn err=4.81), Cifar100 (wrn 16-8, err=22.07) and SVHN (wrn16-8, err=1.54%) and none of the presented variant outperform this result. This poor performance casts doubts on the validity of the resnet experiments. Given that resnet/resnext networks are mostly compared on imagenet, I would suggest to also report at least one model architecture with the four variants on this set, with the baseline taken from another paper. Tiny imagenet also has a decent set of available results with a wide variety of architecture, with a lesser training cost.\n\n\nDetails: \n\nFor Figure 1. You could clarify how the module is connected to the whole network.\nDefine m in Eq. 1. \nDefine point-wise convolution at first mention.\nDefine PRelu and introduce equation.\n\nMissing references:\n\nLanguage Modeling with Gated Convolutional Networks, Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier, 2016.\n\nParsing natural scenes and natural language with recursive neural networks\nR Socher, CC Lin, C Manning, AY Ng, 2011."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes an attention mechanism that is designed to capture inter-channel relations. This is in contrast to other recent works (and the usual motivation for attention in computer vision) which focus on capturing long-range spatial dependencies. The approach at hand is based on recursive application of one-by-one group convolutions, which introduces only few extra parameters.  Experiments on CIFAR and SVHN show gains over a baseline without attention as well as two previously published attention methods. Several ablations are performed with the aim to provide insight into the attention method's behavior.\n\nOverall, I think that in its current state this paper should be rejected. Besides several issues with the presentation (which could potentially be improved in a future revision), my main criticism is that the approach is not well-motivated and not sufficiently justified experimentally: the reported results are on datasets consisting of 32x32 pixel (i.e. small) images, and results on ImageNet, the most standard classification benchmark, are missing.\n\nThe motivation given in this paper is that both inter-channel relations and long-range dependencies are important (I had trouble spotting this claim in the Cao et al.  (2019) paper which is cited in the introduction; it would be nice if the authors could elaborate on this?). Then, the authors focus only on the former without further exploring the latter. Intuitively, one would assume that long-range spatial context becomes more important as the input dimensionality increases?\n\nRegarding the presentation, I took issue with the following aspects:\n- The related work section is folded into the introduction and consists of a mere listing of current works on attention in computer vision. Some concepts like non-local networks are not introduced, which makes the listing hard to understand for readers that aren't already familiar with the literature.\n- The motivation regarding why one should forget about spatial dependencies is not clear. What I got out of it was that using inter-channel relations only produces good results, too -- but it's not clear to me why it should be, in principle, superior to exploiting both inter-channel and inter-location ones.\n- After reading section 2 multiple times, I still do not fully understand how the method works in practice. What I found missing was how C_0^m is applied back to the activations X that f is applied to. The text mentions \"recalibrate the input feature map by element-wise multiplication\", but I don't know what \"recalibrate\" should mean? In the end, are all channels simply scaled by a C^0_m? I think in addition to clarifying the textual description, it would be good to add X, Y, and the input to the following layer to Figure 1.\n- The result tables sometimes contain surplus information, e.g. the FLOPS column in table 1 and 2. It would be sufficient to say that the number of FLOPS does increases only by X% in the text?\n- I found Section 4.1 somewhat confusing. Based on my current understanding, if C_0^m is a 1xHxW tensor, presumably with different entries for HxW, how can the variance be 0? If it's the case that the method is bound to produce zero variance, then I would suggest not plotting the variance at all and merging the four graphs in figure 3 into a single one.\n- Section 4.2 was not clear to me either. Are the PReLU weights removed only after the network was trained? If not, figure 5(a) suggests that the weight in PReLU does not matter, which begs the question why this activation function is being used in the first place. \n- Figure 6 does not seem to support the claim of faster convergence speed."
        }
    ]
}