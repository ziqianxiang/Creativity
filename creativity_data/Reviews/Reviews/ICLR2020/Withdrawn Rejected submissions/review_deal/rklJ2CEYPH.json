{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposed to use  normalizing flow to model point processes. However, the reviews find that the paper is incremental. There have been several works using deep generative models to temporal data, and the proposed method is a simple combination of well-established existing works without problem-specific adaptation. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a new intensity-free model for temporal point processes based on continuous normalizing flows and VAEs. Intensity-free methods are an interesting alternative to standard approaches for TPPs and fit well into ICLR.\n\nThe paper is written well and is mostly good to follow (although it would be good to integrate Appendix A.1 into the main text). The paper proposes interesting ideas to learn non-parametric distributions over event sequences using CNFs and the initial experimental results are indeed promising. However, I found the presentation of the new framework and the associated contributions somewhat insufficient.\n\nThe proposed approach seems to consist mostly of applications of existing techniques and of only few technical contributions. There is also no real theoretical analysis of the advantages of the new approach beyond general statements. In addition, the experimental analysis is missing comparisons to\n- other intensity-free methods (e.g., [1, 2]) \n- other NeuralODE based methods (e.g, [3, 4]) \nand would also benefit from a closer analysis of the models advantages and/or additional tasks. While each of these points on its own would not be very severe, I found that the combination of all of them is problematic in the current version of the paper. I hope that the authors can address this in their response or future revision.\n\nFurther comments:\nThe results on Breakfast of the competing methods seem quite lower than the results published in (Mehrasa 2019). What is the cause for the differences here? For instance, APP-VAE in (Mehsara 2019) would outperform the results of PPF-P both in terms of LL and MAE (142.7 vs 204.9)?\n\n[1] Xiao et al: Wasserstein Learning of deep generative point process models, 2017. \n[2] Xiao et al: Learning conditional generative models of temporal points processes, 2018. \n[3] Chen et al: Neural Ordinary Differential Equations. \n[4] Jia et al: Neural Jump Stochastic Differential Equations"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a method for learning models for discrete events happening in continuous time by modelling the process as a temporal point process. Instead of learning the conditional intensity for the point process, as is usually the case, the authors instead propose an elegant method based on Normalizing Flows to directly learn the probability distribution of the next time step. To further increase the expressive power of the normalizing flow, they propose using a VAE to learn the underlying input to the \"Flow Module\". They show by means of extensive experiments on real as well as synthetic data that their approach is able to attain and often surpass state of the art predictive models which rely on parametric modelling of the intensity function. The writers have put their contributions in context well and the presentation of the paper itself is very clear.\n\nThough the final proof is in the pudding, and the addition of the VAE to model the base distribution yields promising results, the only justification for it in the paper is to create a more \"expressive\" model. There are multiple ways of increasing the expressiveness of the underlying distribution: moving from RNNs to GRU or LSTMs, increasing the hierarchical depth of the recurrence by stacking the layers, increasing the size of the hidden state, more layers before the output layer, etc. A convincing justification behind using a VAE for the task seems to be missing. Also, using the VAE for a predictive task is a little unusual.\n  \nAnother, relatively small point which the authors glance over is the matter of efficient training. The Neural Hawkes model suffers from slow training because of the inclusion of a sampling step in the likelihood calculation. I believe that since the model proposed by the authors allows easy back-propagation, their model ought to be easy and fast to train as well. Including the training time for the baselines, as well as the method proposed by the authors, will help settle the point. \n\nMinor point:\n\n - The extension of the method to Marked Temporal Point Processes in the Evaluation section seems out of place, esp. after setting up the expectation that the marks will not be modelled initially, up till footnote 2 on page 7."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a model for point processes using normalizing flows. The conditional distributions of next inter-arrival times are modeled as normalizing flows with base distributions conditioned on the history. Continuous-time flow based on neural ODE was employed. \n\nOverall I find this paper incremental. There have been several works using deep generative models to temporal data, and the proposed method is a simple combination of well-established existing works without problem-specific adaptation. \n\nI don’t get the point of using VAE in the likelihood estimation. The model (PPF-D) already defines a flexible conditional distribution. The only reason I can imagine to introduce VAE type model is when the dimension of the latent variables (which should be strictly the same as the observed variable) is too large to model directly. In such case one may choose to optimize a lower bound computed from the variational distribution defined over the lower-dimensional latent spaces. Hence in case of temporal point processes where the dimension is one, I see no point of doing VAE. The authors stated that VAE based model (PPF-P) performs better than ordinary flow model (PPF-D) because PPF-P has more flexible base distribution. My guess is that PPF-P has one more stochastic layer so the architecture itself is more expressive than PPF-D. PPF-D with more complex structure (e.g., more layers in base distribution parameter network) may result in a similar performance.\n\nThe authors stated in coclusion section that “The proposed PPF can be optimized by maximizing the exact likelihood”, which is not true for PPF-P optimizing the lower bound. "
        }
    ]
}