{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper augments transformer encoder-decoder networks architecture with k nearest neighbors to fetch knowledge or information related to the previous conversation, and demonstrates improvements through manual and automated evaluation. Reviewers note the fact that the approach is simple and clean and results in significant improvements, however, the approach is incremental over the previous work (including https://arxiv.org/pdf/1708.07863.pdf). Furthermore, although the authors improved the article in the light of reviewer suggestions (i.e., rushed analysis, not so clear descriptions) and some reviewers increased their scores, none of them actually marked the paper as an accept or a strong accept.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "### Summary\n​\nThis paper provides a framework to augment dialogue generation with external data sources using K-Nearest Neighbors in the embedding space. The idea seems simple and intuitive, and the results show improvements over prior work in dialogue generation and retrieval.\n​\n​\n### Strengths\n- The paper shows quantitative improvement over some prior works on dialogue agents (however this needs to be correctly validated).\n- The reviewer appreciates the human study and conversation example provided in the paper to qualitatively evaluate their model.\n- The paper provides ablation studies of the various training tricks for each dataset they have proposed.\n​\n### Weaknesses\n- There are certain changes in the training pipeline that makes the comparison with prior work difficult (Tables 1 and 2), and find the real contribution of data-augmentation caused by KNN-based information fetching (KIF). e.g., In Wizard of Wikipedia experiment, most recent dialogue utterance and turn number are used as salient features. Similarly, the paper utilizes a \"personality\" feature in ImageChat dataset. It seems the results taken from prior papers have different training settings. Could the authors verify that the extra assumptions made for their model are equivalently applied to other generative baselines? If not, the reviewer recommends to provide some experiments with same conditions.\n- Could the authors provide the embedding dimensions and other training details, to assess the contribution of different components of the input's embeddings.\n​\n​\n#### Minor:\n- Is there a sound reason behind using concatenation of input representation and fetched representations? This design choice makes the architecture inflexible to the number of data sources. Another way to combine embeddings - e.g. addition or inner product, can also be tried to see if they provide performance improvements.\n- It is mentioned that attention based modules scale poorly with large sized datasets. If the authors conducted a quantitative evaluation to test this, it would be valuable to add it to the paper.\n- The title and abstract should be limited to generative dialogue modeling, instead of transformers, since the contributions proposed are more suited for this particular application and are quite engineered. Hence, it is not correct to make this claim for transformers in general.\n​\n​\n### Score\nWeak Reject (Leaning towards Accept if appropriate experiements can be provided)",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper uses KNN-based retrieval method for extracting relevant information from different sources for the task of generating response in a dialogue task. They utilized different sources of information from Wikipedia, YFCC image set and dialogue utterances. The method is very straight forward and using nearest neighbors from external information sets as auxiliary information and encoding that information via same neural net to produce the encoded external information. The concatenation of encoded input and auxiliary information is concatenated to produce the output in the dialogue via decoder network. The novelty of the proposed method is incremental over Dinan et al. (2018). The KNN-based retrieval module can produce some drift in dialogue from the actual context of the input which can result in irrelevant response in dialogue. Some failure cases can show the quality changes with those drifts in dialogues.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n- The paper proposes augmenting transformer neural networks with KNN-based information fetching modules that can access relevant external knowledge, combine knowledge from different sources, and integrate the information into seq-to-seq architectures. The authors apply their proposal to generative dialog modeling, and apply it to two dialog datasets.\n\nStrengths:\n- The paper is well-written and well-motivated.\n- The authors evaluate their approach on 2 publicly available datasets and compare it to existing approaches, showing improvements in terms of F1.\nThe authors conduct a human evaluation to compare their approach against other approaches.\n\nWeaknesses:\n- There are some details that are missing from the paper, for example details about the mapping operator, and the specific representations of E_i for the datasets used.\n- Parts of the analysis are rushed (e.g., sections 6.2 and 6.3).\n\nQuestions/Comments:\n- In the human evaluation, is there a difference between the ratings for seen and unseen topics for the Wizard of Wikipedia dataset?\n- Section 6.3 (especially the part on the effect of gating) can be improved with additional information/analysis."
        }
    ]
}