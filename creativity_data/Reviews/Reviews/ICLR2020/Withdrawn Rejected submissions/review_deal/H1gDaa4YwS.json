{
    "Decision": {
        "decision": "Reject",
        "comment": "Both reviewers (we apologize for the lack of a 3rd review) did not feel the paper should be accepted. The rebuttal offered did not change the reviewer scores. So the paper cannot be accepted unfortunately. But the authors should use the feedback to improve their paper and resubmit.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a scheme for training layered feedforward neural networks with backwards accumulation of gradients. In the proposed scheme, the intermediate activations during a forward pass are constrained, using an L2 norm penalty, to be close to the activations of a model that inverts the operations of each layer and transforms the data in reverese order (from output to input). The second, inverse, network shares all parameters with the feedforward network.\n\nThe paper hypothesizes that such training, which they call racecar training, results in features that are transferable between tasks; i.e. using racecar training to learn the features, then applying regular training on a novel task. To support this hypothesis, the paper provides an empirical analysis of the mutual information between inputs and intermediate features, and between intermediate features and outputs (as proposed in the information bottleneck literature). Under this analysis, the paper shows that using racecar training the intermediate layers contain less information about the outputs than standard feedforward training. The paper states that this makes the features learned by racecar training better for transfer: features that enable the network to achieve high predictive accuracy on a particular tasks but that carry little information about the output distribution, thus less specialized. The paper continues this analysis to the transfer setting, first to the same initial task, and then the a new task. In both cases one of the variants of the proposed method appears to produce higher accuracy than standard pre-training.\n\nI'm inclining to reject this paper given that the results on the main hypothesis (i.e. transferability of features) seem to provide only marginal improvement, and we have no idea about the repeatability of the results ( how many times did the authors run the experiments for figures 3,4,6,7,9? What's the spread of the results? Are these results significant?). The results on style transfer and super resolution are promising, but these are only illustrative examples: these results do not provide insights on how well the method works in general, or when does it fail.\n\n\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper attempts to learn general and reusable features for transfer learning tasks. The authors propose a training paradigm called Racecar Training. The core idea of it is to operate a reverse pass for the network. The authors use mutual information to analyze the network for its improved generalizing capabilities. They also conduct experiments on classification, regression and stylization to validate their method’s effectiveness.\n\nPros:\nThe reverse pass idea is similar to auto-encoder paradigm that discards redundant information and only save the essential low dimensional one by comparing the original data and the decoding data. The general feature the authors mentioned is like the essential low dimensional information, which is reasonable.\n\nCons:\n1.\tI think the main drawbacks of this paper is that the authors make a poor presentation. The authors talk about learning general features with which the model can use on new task in the title, introduction and even the whole paper. However, there are no explicit general features learned during the learning procedure. They only perform the reverse pass when learning in the original task. Even the structure of networks does not change at all. The general and reusable feature is only an explanation of the improved performance. I think the authors should change this explanation to a more convincing one. For example, the network may learn general and reusable weights or parameters since it is the model learned that will be applied to new tasks instead of the features. \n2.\tThe analysis by mutual information makes the paper hard to follow. The figures such as figure 2 are so confusing. There are so many points and lines in each picture. What do they mean? What are the x-axis and y-axis? The authors also do not explain what the meaning of different mutual information are. \n3.\tThe symbols are chaotic. The authors explain “RR^3” means n=1 in equation 1. Then what does n equal to in RR^1? The authors explain “AB” means the model was trained for task A during phase I, and is then trained for task B as transfer in phase II. Then what does AA/AB in “Std_{AA/AB}” mean? \n4.\tIn the last paragraph of page 3, the authors mention orange color. However, there are only green, blue and yellow color in Figure 1.\n5.\tIn experiments, I do not see obvious advantage of the proposed method. For example, in Figure 6, the test accuracy of Std_{AA} is 0.9842 while that of RR^3_{AA} is only 0.9859. In Figure 7, the test accuracy of Std_{AA} is 0.8377 while that of RR^13_{AA} is 0.8711. The transfer tasks (referred as AB) also show minor advantages. With minor advantages but increased requirements for memory and additional computations (e.g., 61.13% slower per epoch for the MNIST tests mentioned by the authors), the proposed method shows very limited values.\n\n\n\n"
        }
    ]
}