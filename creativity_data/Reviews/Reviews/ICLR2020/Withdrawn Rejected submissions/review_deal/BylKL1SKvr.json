{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pre-trained on some dataset and subsequently fine-tuned on the target dataset. The authors theoretically analyse two-layer fully connected networks and provide an extensive empirical evaluation arguing that the loss landscape of  appropriately pre-trained networks is easier to optimise (improved Lipschitzness). \nUnderstanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. However, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. As a result, I will recommend rejecting this paper in the current form. \nNevertheless, as the problem is extremely important I encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "I thank the authors for the clarifications and the modifications to the paper. However, I still lean towards rejection. While the authors provided detailed explanations on some of my points here, most of these are still not in the paper, so the reader would still probably be confused. There are new figures in the appendix, but they are not referred to from the text. The paper feels like a hastily compiled collection of weakly related, somewhat anecdotal and not necessarily clearly explained and motivated experiments. While I believe there is interesting content, with the current presentation style it is really difficult to digest. As an advice, I think it may be better to not squeeze in all the results into one paper, but rather focus on some aspects and analyze them really well, with clear explanation, motivation, and preferably with demonstrating practical gains that result from the analysis - not just hypothetical, but supported by experiments.\n\n---\n\nThe paper studies transfer of representations learned by deep networks across datasets and tasks. Namely, the paper analzes the standard setup with pre-training on one dataset and further fine-tuning on a different  dataset. Results include both experiments and theory. On the experimental side, the focus is mainly on TODO. On the theory side, an analysis of transfer in two-layer fully connected networks, based on the ICML-2019 work of Arora et al., is proposed. \n\nI lean towards rejecting the paper, since the presentation and the technical quality are somewhat substandard. This is mainly based on evaluation of the experimental results, since I am not an expert in this subfield of theory and was therefore not able to check the statements and the proofs thoroughly. On the presentation side, many details are missing, which often makes understanding difficult and will likely lead to the results not being reproducible. On the experiments side, the issue is that they are quite anecdotal and have multiple confusing or questionable details, as specified below.\n\nPros:\n1) An interesting topic of trying to understand generalization and transfer in deep learning\n2) Multiple types of experiments, including visualizations of loss landscapes at convregence and at initialization, plots of Hessian eigenvalues, measuring the deviation of the weights from their initial values, measuring the variance of the gradients of the weights, measuring the transfer between different datasets, measuring the transfer performance depending on the durarion of pre-training.\n3) Theoretical analysis. As mentioned, I cannot really judge about the quality of the results.\n\nCons:\n1) The presentation is quite confusing. \n1a) The paper includes many different experiments as well as theory, and it is not very clear how these all come together and what message do they give to the reader. The paper states at one point that it \"may stimulate further algorithmic advances\", and it would be great if there was a bit more elaboration on this. \n1b) Experimental methodology is not presented in the main paper and not referred to. Some of it is described in the appendix, but also not too detailed, for instance the duration of ResNet training is not clear, the details of loss landscape visualization are confusing (for instance, the phrase \"... i.e. 0.1× gradient and a total of 200 × 200 grids\"), \n1c) The paper is over 9 pages, which is more than the recommended length of 8 pages.\n1d) Scaling in Figure 8(b) is quite suboptimal, it is impossible to read the test accuracy results.\n1e) Minor issues:\n - No color-code in Figure 3 (unclear what values do the colors correspond to) and it does not seem to be referred to in the text. \n - Page 5: \"predictive\" -> \"predictable\"?\n - Page 6 \"While pretraining on sufficiently large datasets...\" - I do not think the experiments tell anything about the dependence of the effect on the size of the dataset, so this phrasing is not justified\n\n2) I found many of the experiments confusing or unconvincing. This is partially affected by the aforementioned issues with presentation.\n2a) In Table 1 and Figure 2, it is unclear if difference in generalization between the dataesets is due to similarity to ImageNet (by the way, ImageNet is only mentioned in the caption of Table 1, but not in the text) or due to the inherent properties of the datasets (perhaps some are naturally more difficult or prone to overfitting). I think numbers for training from scratch would be helpful for disambiguating the two cases.\n2b) It is unclear why is the norm difference normalized by the square root of the number of target examples. This choice is not justified and it seems it can very noticeably affect the results in counter-intuitive way. For instance, if I understand right, if one duplicates each example in the training set, these values will change, which seems somewhat counter-intuitive. Would it make more sense to normalize by the initial norm of the weights?\n2c) In Figure 4, it is unclear if the permutations of the weights are accounted for. The output of a deep network is invariant under a wide variety of weight permutations, and it is natural that networks trained from different random initializations may converge to weights permuted in different ways. In order to meaningfully compare the weight vectors of these networks, one would have to first \"align\" them, which is a non-trivial task. Another issue I have with the results on Figure 4 is that I don't find them all that counter-intuitive: it seems natural that the weights stay relatively close to the pre-trained network when fine-tuned for that (paritally because the aforementioned permutation symmetry is already largely broken during pre-trianing).\n2d) It is unclear which downstream task is analyzed in Figure 6. Moreover, the plot seems to mix two factors: magnitue of the gradient and the smoothness of the loss landscape. Would it not make more sense to make fixed-size steps in the direction of the gradient? Moreover, I am wondering if the difference between the loss values is simply because the overall loss magnitude is larger for the randomly initialized network?\n2e) A few parts of subsection 5.1 are confusing. It is not clear where does the interpretation of Figure 8 follow from (perhaps partially because figure 8(b) is partially difficult to read). \\psi and W are not defined. What does it move that the weight matrices \"show no improvement\"? Where are the results on Caltech-101 and Webcam? Why is Food-101 mentioned in the text, but CUB-200 shown in the plots?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper gives an extensive empirical and somewhat limited theoretical analysis for the transferability in DNNs. It is shown that transferred models tend to have flatter minima with improved Lipschitzness in the loss function when good choices of pretraining are made.\n\n- The paper is well written and well-organized. Notations and claims are clear.\n\n- This paper presents an interesting line of research, that in my opinion, would be interesting to many researchers in the field, and could attract many follow up works. \n\n- Empirical analysis in sections 3 and 4 are interesting, and give good sense of generalization and optimization landscape. Analyzing the Frobenius norm of the deviation between fine-tuned network and fixed network is reasonable. \n\n- The theoretical analysis seems like a good start, but it is not sufficient in general. There seems to be gap between the network architectures used in empirical evaluations and the theoretical results. However, analyzing transferability is an important topic that needs to be evaluated more. This paper presents interesting new steps towards that goal. That being said, I would be interested to see theoretical results for more general cases alongside experiments on different types of applications.\n\n- Overall, the paper presents a novel approach for evaluating transferability, that I think would be interesting to many researchers in this field. The theoretical results are still limited, and should be investigated more. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "N.B Since this paper exceeds 8 pages, it was held to a higher standard as per the instructions. \n\nThe main goal of this paper is to concretely demonstrate some key properties of transfer in Deep Neural Networks through experiment and theory. By transfer, here they specifically mean the ability for pre-trained networks to be 'updated' to new, similar datasets either completely (all parameters are updated while being initialised by the pre-trained network parameters) or partially (all but the last few layers are kept constant at pre-trained parameter values). There are of course other ways of carrying out transfer learning, but this paper focusses on these methods.They attempt to assess the viability of such a process in its improvements to generalisation and improvements to the loss landscape. In addition, the authors attempt to assess when this type of transfer is viable. The majority of the paper focus on experimental results, while the final 2 pages present some theoretical work that explains those results. \n\nI believe that this work is well motivated. As the paper suggests, there have been several advances in the use of transfer learning that showcase its benefits. That being said, there is a lack of work that systematically tries to explain why these benefits are seen, and how we can better make use of them. This paper tries to fill that gap. \n\nHowever, while it goes a way in trying to do so, I am not convinced that this paper sufficiently addresses what it sets out to do. This is why I recommend a weak reject, and a summary of my reasons for this are as follows:\n\n1) Section 3: In this section, the authors try to show that transferred networks tend to have better generalisation when the dataset being transferred to is similar to that the network was pre-trained on (ImageNet dataset). The results are shown in Table 1, and they assess this by showing that for new datasets that are more visually similar to the ImageNet dataset, the generalisation error is lower (e.g the Webcam dataset shows lower gen. error than the Stanford Car dataset). I believe that the authors are saying that they are 'visually' similar. However, the ImageNet dataset has subsets that are similar to the Webcam dataset (geological formation), and the Cars dataset (wheeled vehicle). As such, while the goal of the experiment is interesting, it is not clear how interpretable the results are, nor the validity of the conclusions raised. \n\nIn addition to this, I would have liked to see the generalisation error of a randomly initialised network for each of the datasets. This would have been an interesting control to see whether the pre-training does indeed improve generalisation. \n\nFurther, the authors use the Frobenius norm between the original pre-trained parameters and the final parameters as a measure of how much knowledge is preserved. I am not convinced that this is a sufficiently representative measure of this. I think the extent to which knowledge is preserved is indicated by how well the new, transferred network performs at old tasks from the original dataset. Simply measuring a distance between the parameters doesn't show this. Also consider the fact that it isn't true that networks with parameters a fixed distance away from the original parameters will have similar behaviour. \n\nFigure 3 isn't mentioned anywhere in the text!\n\n2) Section 4: Here, the authors show that pretrained networks lead to flatter, smoother loss landscapes when compared to randomly initialising. This is shown in Figure 5 and Figure 6 mainly. Figure 5 depicts the loss landscapes, and directly shows what this section is claiming. Figure 6 further solidifies this claim by showing that the change in loss at each gradient update is smaller when compared to a randomly initialised network. That being said, the experimental details of this setup is quite sketchy; what dataset is Figure 5 and 6 transferring to, having been pre-trained with ImageNet? Has this been tested between multiple different datasets, multiple times, to show that the conclusions are consistent? Further, it looks like Figure 5a was taken from another paper, why?\n\n3) Section 5: This section tries to answer the question of when transfer learning (as defined by this paper) is viable. Section 5.1 was quite difficult for me to read because I could not understand how the experimental setup described in the text. For example, Section 5.1 says that the network was pretrained on the MNIST dataset and transferred to the SVHN dataset, whereas Figure 8b states the complete opposite. If I assume that the text is correct, the generalisation error in Figure 8b is very difficult to read. In addition to this, I am not sure what the norm of phi-phi(0) is. I also still have my reservations to the use of the norm between parameters, as mentioned above. \n\nIn the the section 'Varying Labels with fixed input', the authors mention the use of Caltech-101 and Webcam data, but this isn't mentioned in Figure 8, instead, it mentioned CUB-200, which isn't mentioned in the text. They also mention conclusions from experiments using the Food-101 and Places datasets, but don't show these results anywhere. \n\nSection 5.1 asks important questions, but the authors haven't shown results that can properly answer them. \n\nThat being said, Section 5.2 shows the very interesting result that pre-training after a certain number of epochs starts showing diminishing returns in terms of performance of the transferred network. \n"
        }
    ]
}