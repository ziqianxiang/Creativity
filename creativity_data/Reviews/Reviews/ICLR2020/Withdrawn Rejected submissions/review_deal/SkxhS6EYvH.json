{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The information bottleneck Lagrangian corresponds to the optimization problem max I(Y;T) - \\beta I(X;T) over all T satisfying the Markov chain Y-X-T. Recent literature has shown that different values of \\beta may correspond to the same value of I(X;T) at the optimum, or the same \\beta may have multiple optimum values of I(X;T). This has motivated alternate versions of the Lagrangian where I(X;T) is replaced by a function of I(X;T). Building up on a recent suggestion to replace I(X;T) by I(X;T)^2, this paper looks at h(I(X;T)) for a strictly convex function h. The main result is that each \\beta corresponds to a unique I(X;T) in this setting, thus generalizing the squared Lagrangian formulation.\n\nThe paper is well-motivated and the author's have cited relevant literature. The paper's contributions, although correct, are incremental in my opinion. The main observation of this paper is essentially contained in the squared IB result, and I don't think the jump from squaring to using arbitrary strictly convex functions is sufficiently novel. For this reason, I recommend this paper be rejected. \n\nMinor comments: I have some comments regarding the technical presentation. These have not influenced my decision, but may be useful in future iterations of the paper.\n\n- Throughout the paper, the authors appear to assume random variables with finite, discrete supports. This is evidenced by their use of entropy as opposed to differential entropy throughout the paper. However, when I read the simulations section, the authors appear to use a continuous representation T. Is the theory valid for continuous representations T? \n- Related to above: What is the set \\Delta in equation (1)? If it is as stated \"the set of all random variables satisfying Y--X--T$, then there is no reason to expect a maximizer to exist. Is is more appropriate to use \\sup in (1) since the existence of an optimizing T is not guaranteed.\n- The notation used to depict distributions of random variables is archaic. For example, the authors use p(x) and p(y) for the pdfs of X and Y. This creates ambiguity: What is p(1)?  The authors should use more modern notation with pdfs denoted by subscripts of the r.v., such as p_X(x), p_Y(y), p_{Y|X}(y|x), and so on.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors introduced the convex information bottleneck Lagrangian. They want to provide a solution to the problem that when Y is a deterministic function of X, traditional IB Lagrangian can not explore the whole IB curve. The authors introduce a general family of convex IB Lagrangian, and prove that there exists a bijective mapping between the IB curve and IB Lagrangian multiplier. Several experiments are conducted to prove the performance of power IB Lagrangian and exponential Lagrangian.\n\nThe problem that IB Lagrangian multiplier can not explore the IB curve is worth understanding and fixing. However, the convex IB Lagrangian is not very different from the square IB Lagragians. Firstly, the authors didn't point out any deficiencies for using square IB Lagrangian, so the motivation of using other convex IB Lagrangian are not strong. Also, the theories about convex IB Lagrangian seem to be a simple generalization of the theories of square IB Lagrangian. Finally, the experiment results for square IB Lagrangian and exponential IB Lagrangian are quite similar. \n\nTherefore, the whole paper seem to be too incremental, compared to square IB Lagrangian. I think the contribution of this paper is not enough to be accepted.   "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "In recent years, the information bottleneck has emerged as a useful framework for understanding deep networks.\nIt provides a framework for theoretically motivating the choice of a intermediate representation T when learning to predict Y given X.  The constrained optimization required by the information bottleneck is often pursued using Lagrange multipliers.  Previous work by Kolchinsky, et al pointed out that, when Y is a deterministic function of X, the Lagrange multiplier technique breaks down, and also that solutions to the information bottleneck criterion are degenerate.  They proposed a simple modification to the objective arising from application of Lagrange multipliers that, in a sense, always \"works\", even in the case that Y is a deterministic function of X.  \n\nThis paper proves that a more general family of modifications of the Lagrange multipliers, in a sense, \"work\".   They also provide solid experiments supporting this theory.  They also show how the appropriate Lagrange multiplier depends on the amount of \"compression\" from X to T desired. \n\nThe paper is very clear and very well-written.  The description of previous work is extensive, and interesting.  In a sense, proving that a more general family of methods work makes it clear why the method of Kolchinsky, et al worked.  Having a mapping from the desired compression level to the required Lagrange multiplier seems potentially valuable.\n\nGenerally, the contribution feels a little incremental and technical to me.  I would not argue against accepting the paper if others like it.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}