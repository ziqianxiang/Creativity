{
    "Decision": {
        "decision": "Reject",
        "comment": "This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal. The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims. The reviewers acknowledge the authors' efforts to resolve the main issues but find these efforts insufficient. Thus, this paper cannot be accepted to ICLR2020.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper was extremely hard to read or comprehend. It’s riddled with typos, inaccurate notations and undefined variables (see below for a sampling). The authors will need to significantly polish and improve the presentation of the paper. \n\nAfter a few forward and backward passes through the paper, I was able to gather the following high level ideas about the paper:\n(1) This paper is somewhat related to the Defferard et. al, 2016 in that the authors want to define a propagation filter for graph neural networks.\n2) This proposed filter known as “ballistic filter” should have the property of allowing fast diffusion through the network. \n(3) The authors claim that the ballistic kernel diffuses @ O(k) as compared to O(\\sqrt k) when compared to traditional GCNs, where k is the number of propagation steps.\n(4) The authors additionally claim that their approach needs one-third the number of parameters.\n(5) The authors provide some plots to visualize the linear diffusion rate of their proposed filter.\n\n--- Issues and clarifications ---\n- Sec 3, Eq 1 seems to have been taken from Eq 1 in Defferard et. al, however there’s no reference to it and the terms g, U, etc. are not defined.\n - Sec 4, Algo 1 contains the main core of the proposed algorithm, but it’s only defined for the 2D grid case. The notation therein is extremely unclear. What is H_space, H_c? How does one sample \\hat{O}_coin.? The net result is that algorithm is undefined. Without a clear definition of the algorithm, it’s completely unclear what the proposed method does.\n - Sec 4.2 is completely unparseable. What is problem setting? What is the metric? How have the baselines been implemented? How has data been split for training/testing?\n - Section 5 mentions that one-third params are used to get 97% but no details are provided as to how less params are consumed.\n - How is figure 7 generated?\n - Sec 8, feel totally unrelated to the paper. There are a whole bunch of random, unmotivated diffusion equations  Eq 6, mentions “.. \\hat{g}(f) decreases as f increases and thus can be seen as a low pass filter…” . This is not true from the formula.\n \n\n --- A sampling of typos ---\nSec 4.1, .. consisits …\nSec 5 “REVISIT” -> “REVISITING”\nFigure 6, text, “cassical”\nSec 6.2 title, “SUMMAY”\nSec 8  “aggreated”\nSec 8  t=\\-tau to -\\tau\nSeveral typos with Hardmard, Hadmard instead of Hadamard.\n\nOverall, the major criticisms of this paper:\n - The proposed algorithm is not clear. \n - The authors need much more experimentation to bolster their claims in the paper. It’s completely unclear if fast diffusion even if it were possible will help GNNs perform better on a diverse set of tasks.\n - The paper needs a lot more polish and proof reading to make this paper presentable.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "The paper \"Beyond Classical Diffusion: Ballistic Graph Neural Network\" tackles the problem of graph vertices representation. While most existing works rely on classical random walks on the graph, the paper proposes to cope with the \"speed of diffusion\" problem by introducing ballistic walk.\n\nI noticed the comment of the authors that gives a correction for the introduction. But even with it the paper remains very cryptic, with very few pointers to help the reader in understanding the contribution. The introduction (even corrected) is very abrupt and it is very difficult to understand the problem that the authors propose to attack. The problem is that authors start with mathematical discussions before presenting the manipulated concepts and formalizing the adressed problem. I only understood the adressed problem after seing which are the baselines the proposal is compared with in section 4.2. Also, the introduction does not introduce the proposal at all. \n\nA symptomatic example of the lack of paper positioning is the Related Works section which does not even give a single reference !  A related work section with no related works in it appears to have a limited interest to me...  This section should at least introduce other works in the field of graph embedding, such as those reported as baselines. It would also greatly help to understand the contribution of the paper. Also, the ballistic concept is not introduced at all in section 4. Where does this term comes from ? The proposed approach is completely cryptic, with clearly not enough definition of the notations the algorithm deals with. A global view of the approach, from the input graph to the final representation, would also be required to help the reader to understand the proposal. If the contribution is only a new kind of random walk on a graph, is ICLR the good targeted venue ? If authors think so, they should present their contribution in a representation learning perspective, which would highlight the importance of this new walk for the graph representation learning process. \n\n\nFrom my point of view, without a full re-writting of the paper, this work cannot be published in a conference like ICLR.   ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposed a new diffusion operation for the graph neural network. Specifically, the ballistic graph neural network does not require to calculate any eigenvalue and can propagate exponentially faster comparing to traditional graph neural network. Extensive experiments have been conducted to verify the performance of the proposed method.\n\n1. The motivation of this method is to accelerate the diffusion speed in a graph. However, as we know, a very severe issue of graph neural network is the over-smoothness issue. The reason is that, in the high layer, the node feature is diffused to far neighbours.  When using the proposed ballistic filter, node features diffuse much faster than the regular GNN. Thus, the over-smoothness will appear in the shallow layer very fast. As a result, we cannot use many layers so that the non-linearity of deep neural networks cannot be fully utilized. Thus, is it necessary to accelerate the diffusion speed for graph neural network?\n\n2. There is only one dataset for  the comparison of the performance of different graph neural networks. More datasets are needed to thoroughly verify the performance of the proposed ballistic graph neural network.\n\n3. Is it possible to slow down the diffusion speed with the proposed ballistic filter?\n\n\n"
        }
    ]
}