{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents a method for out-of-distribution detection under the condition of access to only a few positive labeled samples. The main contribution as summarized by reviewers and authors is the new proposed benchmark and problem statement. \n\nAll reviewers are in agreement that this paper is not ready for publication in its current form. The main concern is around the validity of the problem statement. The reviewers seek more clarity motivating the proposed scenario. Though the authors argue that as few-shot recognition is very difficult and may benefit from strategies like active learning, it is not directly clear how out of distribution detection is the best approach. In addition, R3 seeks clarification on the similarity to existing work. \n\nConsidering the unanimous opinions of the reviewers and all author rebuttal text, the AC does not recommend acceptance of this work. We encourage the authors to focus their revisions on the explanation and motivation of this new benchmark and submit to a future venue. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a method for out-of-distribution (OOD) sample detection for cases that only a few samples are available from positive classes. The authors bridge the gap between novelty detection and few-shot classification models. The paper addresses an interesting task in machine learning but in point of view of the out-of-distribution detection task, the proposed method seems to be an incremental task and far from the state-of-the-art.\n\n--The authors claim that the combination of one-class classification and few-shot classification has not been studied before, while this task is implicitly mentioned in previous work. For instance, these topics are explored in “Out-of-Distribution Detection for Generalized Zero-Shot Action Recognition” for a specific task.\n--The authors divided the out-of-dataset (OOS) into three categories, while the most famous state-of-the-art methods for one class classification task especially those that are devised based on reconstruction error or adversarial training schemes do not fall in any of these categorized.\n--Detecting the OOS samples task is very similar to recognizing the membership attack task, but the authors do not mention anything about them.\n--The proposed model has access to the negative samples (OOD samples) during training. First, this assumption is not realistic, as in all such applications, the irregular (OOD) samples are either poorly sampled or not sampled at all. Second, most of the previous works on OOD assume that OOD samples are not available during training. This makes them not directly comparable to the proposed methods that its settings are more-or-less advantaged. \n--Page 4 – what is D_c’ or C’ in Eqs. (3) and (4)?\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This work investigates a new problem setting that combines few-shot image classification and out-of-distribution detection. The main procedure for the task still follows a standard few-shot classification task, but in each episode, the data from a different distribution may be presented together with the query images. The evaluation focuses on the performance of out-of-distribution detection, which relies on a scoring function to assign a high score for the normal query images while having a low score for the out-of-distribution images. The paper evaluates with three different scoring functions on four few-shot classification datasets and nine out-of-distribution datasets.\n\nWe recommend a weak rejection, although the proposed new problem setting might be interesting to the community, due to three major concerns. The first is insufficient description for reproducing the evaluation. For example, the text mentions “All results are evaluated using 1000 test episodes” without information of how the in-distribution data and out-of-distribution data (especially the OOS data) is chosen in each episode. Second, the experiments and discussions do not provide enough insights for readers to understand the impact of combining the two problems. Some questions should be addressed and are listed below. The last is the writing style which has a noticeable fraction of content not directly related to the proposed problem setting. Such a style can confuse readers and require additional passes of reading to understand. \n\nSome questions to be addressed for the second concern:\n1. What’s the impact of doing out-of-distribution under the few-shot setting? Is it harder than a normal out-of-distribution detection setting? How does the “N-way X-shot” setting affect the difficulty of the problem?\n2. The paper proposes to use -MinDist and LCBO for the scoring functions instead of the methods that are commonly seen in a standard out-of-distribution detection paper. Why not use those previous methods (ex: ODIN, Mahalanobis, ensemble strategies, etc.) for the evaluation? If those previous methods do not work well with the proposed new setting, what are the possible causes?\n\nExamples of the third concern include: (1) The MAML in Section 2 has a whole paragraph that could be summarized in a few sentences. (2) Figure 3 draws the schematics of out-of-distribution detection, but its connection to the proposed setting is not clearly described. (3) The introduction mentioned semi-supervised learning and continual learning, which does not strengthen the argument of why few-shot learning and out-of-distribution detection should be combined. (4) Lastly, the FS-SSL paragraph in the experiment section has no conclusion.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary\n\nThe paper investigates both Out of Distribution (OOS) and Out of Episode (OOE) tasks. The distinction is that OOE samples are from same dataset but come from classes not represented by the support set. The paper shows that existing confidence scores developed in the supervised setting are not suitable when used with popular few-shot classifiers. The paper proposes two new confidence scores, -MinDist and LCBO. \n\nStrengths\n\nThe paper proposes benchmark datasets for out-of-distribution detection of few-shot classification. These datasets are Omniglot, CIFAR100, miniImageNet, and tieredImageNet.\n\nThe paper presents baseline results for two popular few-shot classifiers — Prototypical Networks, and MAML.\n\nThe paper shows that a simple distance metric-based approach improves the performance on both tasks.\n\nIt also proposes a parametric, class-conditional confidence score that takes a query x and a class c, and yields a score indicating whether x belongs to class c.\n\nWeaknesses\n\nThe paper is very specific to two few shot learning baselines. The question is how representative these two baselines are. Will the distance metric help for other few-shot classifiers?\n\nWhat is the motivation to detect OOS and OOE in the few shot setting given the accuracy is already low?\n\nThe contribution is mainly the metrics. \n\nOverall, the paper does not have enough interesting results for acceptance.\n"
        }
    ]
}