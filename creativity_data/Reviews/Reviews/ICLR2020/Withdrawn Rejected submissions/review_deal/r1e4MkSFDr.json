{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents a continuous CNN model that can handle nonuniform time series data. It learns the interpolation kernel and convolutional architectures in an end-to-end manner, which is shown to achieve higher performance compared to naïve baselines. \nAll reviewers scored Weak Reject and there was no strong opinion to support the paper during discussion. Although I felt some of the reviewers’ comments are missing the points, I generally agree that the novelty of the method is rather straightforward and incremental, and that the experimental evaluation is not convincing enough. Particularly, comparison with more recent state-of-the-art point process methods should be included. For example, [1-3] claim better performance than RMTPP. Considering that the contribution of the paper is more on empirical side and CCNN is not only the solution for handing nonuniform time series data, I think this point should be properly addressed and discussed. Based on these reasons, I’d like to recommend rejection. \n\n[1] Xiao et al., Modeling the Intensity Function of Point Process via Recurrent Neural Networkss, AAAI 2017.\n[2] Li et al., Learning Temporal Point Processes via Reinforcement Learning, NIPS 2018.\n[3] Turkmen et al, FastPoint: Scalable Deep Point Processes, ECML-PKDD 2019.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "1.\tThe motivation of continuous convolution is not very clear, can the authors please motivate? To my understanding this is just to handle inputs with unequal time steps, but that can be handled multiple ways, why not just naively resample?\n2.\tThe proposed network was defined as continuous convolution followed by the standard convolution. Why not just stack multiple continuous convolutions?\n3.\tContinuous convolution should be a general case for standard convolution, can authors explicitly show it?\n4.\tAnother way to handle unequal timesteps is by using dilated convolution, can authors please comment how they differ, pros and cons etc.?\n5.\tTwo hot encoding seems another way to discretize, no?\n6.\tThe experiments section is rather weak, CCNN seems to have a lot of spikes in prediction, e.g., in Fig. 5.\n7.\tIt’s very strange why two hot encoding does not perform that well, while reading the method section, it seems very obvious to take two ends of an interval, in that way two hot encoding seems logical.\n\nOverall it seems like an easy extension with a lot of parts not well-justified. Also I don't clearly have a well-grounded motivation for a continuous convolution.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a continuous CNN model to accommodate the nonuniform time series data. The model learns the interpolation and the convolution kernel functions in an end-to-end manner, so that it can capture the signal patterns and be flexible. A layer has three networks, which learn a kernel function to represent the combination of interpolation and convolution, a bias function to represent the error correction with convolution, and then produce the output based on them. The authors introduce two assumptions and a two-hot encoding scheme for the input to control the model complexity. The paper also introduces an application of the proposed CCNN by combing with temporal point process. Experiments on simulated data compares the proposed method with some degenerative baselines show the advantage of learning the interpolation and the two-hot encoding configuration. The authors compare the performance on time interval prediction task based on real world dataset to show the model produces a better history embedding for the task.\nOverall, the paper has some incremental improvements on the existing methods that dealing with the nonuniform time series data. Instead of using preset interpolation kernels, the proposed model can learn it with the convolution in a data-driven manner.\nThe paper includes clear explanation on module structure and detailed experiment settings.\nThe experiments of signal value prediction support the claims of the advantages of the proposed model.\nThe notation in the caption of figure 1 is a little confusing: is x(t_i’) the same as hat x(t) in the algorithm?\nIt is good that the related works section mentioned the adapted RNNs that are used as baselines in the real-world dataset experiment, and the differences between the proposed model and the related SNNs are introduced.\nHowever, this section and the introduction can be better organized to distinguish the novelty and the contribution of the work.\nIn page 7, the purpose of the reference in the sentence “The time information is either two-hot encoded (Adams et al., 2010) (CCNN-th), or not encoded (CCNN).” is not very clear.\nIt will be better if there are a little more analysis of the experiment on predicting time intervals to next event.\nThe upper plots in the figure may be not convincing enough to support the claims.\nThe advantage of two-hot encoding seems subtle in the figure 5. Is there any reason for the significantly higher deviation of CCNN-th in StackOverflow?\nSometimes the usage of “CCNN” is not clear, for example, the experiment on speech interpolation compares the “CCNN-th” method with baselines, but uses “CCNN” in the analysis. Also, it could be better to show the “CCNN-th” result in the upper plots of figure 5 instead of “CCNN”.\nMinor comment:\nThere are some typos in the paper, for example, missing the right parenthesis in page3 “(refer to Appendix A.1”, in page4 section 4.1 “According to Eq.(4), the input is …”.\n“The left plot shows” in the last line of the caption of figure 3 should be “right”."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "A method that was proposed by authors deals with a problem of non-uniform data in time series. One of ways to deal with this problem is interpolate input signal between data points. In signal processing a standard way to interpolate is to apply a convolution with a kernel. This operation, by itself, is a non-trivial, since we lack any information about signal's spectrum (and do not know optimal kernel). Thus, the authors propose to search for a kernel in a form neural network. To this term they also add bias term which is also a neural network. \n\nBasic idea of the paper seems promising, but reported results are only partial. Since a paper is experimental, i.e. no theory at all, then the main judgement should be based on experimental results. They are not convincing, as CCNN is compared with methods that can not be called state-of-the-art."
        }
    ]
}