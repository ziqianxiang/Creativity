{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors apply neural architecture search techniques to the problem of physics based learning. It is interesting because it cleverly tackles the challenge of manually designing priors and network architectures. The results are also impressive as the proposed method surpasses all the considered baselines. Despite of the above upsides, I have the following questions/concerns.\n1. There is limited technical novelty as the entire method is mainly based on previous work on neural architecture search. Nevertheless, it might be helpful to have some ablation study to show the improvement of the task-specific adaptations presented in the paper, with which I believe this could be a good paper on the application side.\n2. I'm curious about the performance of the baseline methods given the same amount of computation. For example, is it possible to perform intensive hyperparameter tuning for the baselines to also obtain improvement. It seems that the authors did not discuss the computational costs and whether different methods are compared given the same cost."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes PhysicsNAS, which proposes a method to automatically design architectures that\nincorporate domain expertise from phyiscs-based models while also accounting for potential\nmismatch between the model and real world due to unaccountable factors. While existing work seem\nto incorporate such information via one of 4 standard ways (given on page 2), the proposed work\nattempts to meld them so as to find the optimal combination for the problem at hand and the data\navailable.\n\nWhile I don't see anything fundamentally wrong with the paper, I do not feel that the technical\ncontributions are substantial enough to warrant acceptance at ICLR.\nMore specifically, the methodological novelty is limited and the experimental evaluation only\nevaluates the method on two fairly simple problems. \n\nOn a positive note, the authors have done a good job of illustrating the idea and have compared it\nto most natural baselines. I also thought that the illustrations of the architectures found\nfor different sample sizes (in the Appendix) quite insightful.\n\nI encourage the authors to pursue this line of work, but test this on more complex prediction tasks\nwhere entirely model-based approaches are unreliable, and entirely black-box estimators are sample\ninefficient. It also seems that the approach need not be confined to physics per se - in many\nproblems in chemistry, materials science etc. scientists are looking for ways to incorporate domain\nexpertise while accounting for model-mismatch. \n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "=== Overall comments ===\nThis paper proposes to generalize approaches to physics-based learning (PBL) by performing network architecture search (NAS) over elements from PBL models found in the literature. This entails including physical inputs to the network and the incorporation of new operations to the NAS. I think the idea has merit and rather like it. However, there are several aspects of the work that could be improved. The technical novelty is small, as the extension of the  existing NAS models to handle physical inputs and a few new operators is relatively straightforward. The experiments, while well designed, only explore uninteresting toy problems. While I appreciate the necessity to explore the methods performance in a more controlled setting, a more impactful testbed would be more convincing. Another drawback of the evaluation is the lack of a proper statistical analysis of the results, given the small data and model sizes.\n\n\n=== Relevance & Prior Work ===\n+ The related work gives a good summary and categorization of prior work in physics-based learning\n+ The problem (physics-based learning) is interesting and relevant to the community\n\n\n=== Novelty & Approach===\n+ application of NAS to physics based learning\n+ incorporation of physics solutions as inputs into differentiable NAS\n+ creation of physics-informed operation sets to merge physical models into network\n- technical steps to merge NAS and PBL are relatively straightforward\n\n\n=== Evaluation ===\nTwo representative physical simulations were chosen for evaluation, where elements of the physics model are intentionally omitted,  1) estimating trajectory of a ball in presence of wind and air resistance, and 2) a collision speed simulation where two objects collide, where sliding friction is not accounted for in the physics model.\n\nThe baselines consist of: a 3-layer MLP (data-driven), a 3-layer MLP with Physical Regularization, a 3-layer MLP with residual connection to the physics prediction, an MLP with two input branches, on for the data and one for the physics predictions (Physical Fusion), and the Embedded Physics model which estimates parameters for the physics modelu using a 3-layer MLP.\n\nPhysicsNAS can combine elements of the baseline models, but the total number of nodes is limited to 5.\n\n+ Experiments testing the dependence of the model on the numbers of samples and the strength of the physical inconsistencies were conducted. In both cases, PhysicsNAS outperformed the best specialized physics models.\n\n- The chosen testbed tasks are toy problems. While these types of experiments are necessary to understand the performance of the model, it would have been interesting to see PhysicsNAS applied to a more impactful task\n\n- Given the size of the networks and the training data, there is no reason why a more sophisticated statistical analysis of the results wasn’t performed (confidence intervals, t-test, p-value). Similarly, a more complete set of experiments with more sample amounts could be provided with little effort.\n\n\n=== Clarity & Other Comments ===\n- “precious nodes” -> previous nodes\n"
        }
    ]
}