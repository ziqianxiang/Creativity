{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies exposure bias in neural language models and proposes a method to quantify it. The method evaluates the discrepancy between the conditional distributions of the next token--given a history---of the data distribution and the model distribution. The authors conclude that exposure bias seems to play a fairly minor role especially for \"short generations\" (e.g., short sentences). \n\nThis paper studies exposure bias a seemingly important issue which has remained elusive. Exposure bias has fueled research on text GANs (and other non-MLE-based frameworks), as such better understanding it and/or how it affect MLE-trained models is important and could be impactful. I find that this paper makes a step in that direction. If the results are to be believed, the exposure bias problem may in fact not be as important as once thought. This results also goes in the same direction as recent research where the role of exposure bias never seemed to have materialized. Overall, while I think this kind of analysis if worthwhile, I am not sure that the contribution is significant enough to warrant a publication at this venue. \n\nDetailed comments/suggestions:\n- In Section 5.2, the synthetic data is generated using an LSTM LM. Another LSTM LM is then used to fit this data to quantify exposure bias. I am wondering if using this model could change the results since in this case the trained model could indeed recover the generating distribution (i.e., the model is perfect)? If anything, it seems like it could lessen the impact of exposure bias.\n\n- There are methods such as professor forcing (Lamb et al., NIPS'16) aimed at combating the exposure bias and seem to outperform LMs trained using teacher forcing. Could you briefly discuss these and hypothesize as to where their gains come from? This is in contrast with text GANs that---as you point out---may not be better than MLE-trained models. \n\n- I am a little puzzled by the second paragraph of the Related Works Section (6). If I understand correctly, your claim is that previous work implied that exposure bias may not be a serious problem in MLE training.  As far as I understand, these works do not make such a claim but rather come to the same conclusion as you do (i.e., it's also possible that text GANs simply do not solve the problem well enough). \n\n- In Table 1, why show the results using transformers since in the sequel you use an LSTM LM (and transformer results are in the appendix).\n\n\n\nMinor comments:\n\n- STOA -> SOTA\n\n- In Eq. 5 the position of the parameters of MGD are not the same as in Example 1."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper studies whether exposure bias (the fact that a test time, when  a model is making a sequence of predictions, its history is its own predictions, as opposed to during training, in MLE setting, where its history is ground truth predictions) is a real problem for generation systems. The goal is to find a way to really quantify whether there is an issue empirically. The paper presents one result which argues that it is not (by comparing p_model(x | ground truth history) vs p_model(x | predicted history) for a number of datasets and show it is not a problem. It then argues that this is a bad measure (which I somewhat agree with, although it is a deeply intuitive one), and then tries to compare p_gtdistribution(x | predicted history) vs p_model( x | predicted history). This is where the paper loses me. p_gtdistibution makes no sense from the point of view of real data because we just have samples, and assigning probability is the introduction of a model. The paper tries to continue by using a synthesized distribution here (an LSTM) and then somewhat off-handly declares that in this setting exposure bias is small so its not really a problem. \n\nI'm very torn about this paper. I found it very interesting, but I also don't think using a synthetic distribution makes sense but I do not have any proposals for how operationalize the paper's proposed definition for measuring exposure bias with real data. The issue is that I don't care if its a problem or not of a synthetic distribution, only real world data. As such, the results toward the end of the paper do not seem valid to me but aren't really overstated (because of the writing, its hard to take the result very seriously, and the authors seem to know). \n\nI am not sure if the ICLR audience would be as generally interested as I was in the paper, so I am tentatively marking weak reject, although if there is other interest among reviewers I would upgrade.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the seriousness of the exposure bias problem in language modeling. Many NLP researchers believe that the fact that an auto-regressive language model (LM) is conditioned on its own prediction at test time whereas it was conditioned only on training data at training time induces exposure biases. In this work, based on the observation that conditioning on even random inputs or model's past history will still generate valid continuations, questions whether exposure bias is a serious problem. \n\nThe first experiment in this paper is based on measuring the distance between the marginal distribution of the model's predicted word at a certain position to the ground truth marginals, where the model is either fed with ground truth prefix or its own predictions. While this metric shows around 10% difference between ground truth prefix and prediction prefix as the length of the generation goes to around 20, the authors argued that there might be multiple causes since the model's prediction prefix might have a different distribution to the ground truth prefix, leading to different marginals.\n\nTo separate out the effect, this work considers measuring the distance between conditionals instead of marginals at a certain position, where conditionals are either conditioned on ground truth prefix or language model prefix. However, since in real data we do not get access to the true conditionals (especially when prefix is generated from the model), this work did a synthetic experiment, using an LSTM language model to generate data as the true data distribution, and use other LSTM models to fit. On this synthetic task, 1) the difference in distances either conditioned on ground truth prefix or predicted prefix is within 3%, and 2) scheduled sampling does not help, but seqGAN helps a bit.\n\nPros:\n1. This paper raises a very interesting question of quantifying exposure bias and proposes a metric to quantify the phenomenon of exposure bias.\n2. The experiment on the synthetic task is convincing, showing that exposure bias is not a huge issue there.\n\nCons:\n1. In terms of motivation, the observation here is that auto-regressive LMs are able to self-correct, and authors found similar results for transformers. However, as shown by Holtzmann et al 2019, when directly beam search from the LM, it will produce repetitions that would be further reinforced by the model itself. Those two observations seem contradictory.\n2. In the synthetic experiment, the data generator is also of an LSTM architecture, so this setting is similar to knowledge distillation where the student and teacher networks have similar structures. It is well established that the data generated from an LSTM LM is biased (smoother and simpler) compared to the data distribution it's trained on. It is thus not very surprising that the distribution of student's samples is similar to the \"true\" data distribution (teacher's samples), whereas this is not the case in the real world. One way to evaluate this is to repeat the first (\"wrong\") experiment on this synthetic dataset, and I would expect the measured differences to be also smaller here. Another way would be using a much more powerful generator and a less powerful LM (what if we use transformer LM to generate but use RNN w/o attention to learn?)\n3. What does 3% difference in TV/JS mean? It is a token level metric anyway. Would it mean a less than 3% difference in sequence-level metrics such as BLEU? Does it mean 3% wouldn't compound over time steps? I don't think we can draw any conclusions by looking at this number. \n\nOverall, I think this paper asks a really interesting question, but this paper doesn't provide a convincing answer, and I am still not sure if exposure bias is a serious problem or not. I am inclined to reject this work unless I can get a convincing assessment of the exposure bias problem.\n\nReferences:\n1. Holtzmann et al 2019. The Curious Case of Neural Text Degeneration. https://arxiv.org/abs/1904.09751"
        }
    ]
}