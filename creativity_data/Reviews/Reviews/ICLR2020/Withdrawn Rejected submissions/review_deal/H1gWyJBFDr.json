{
    "Decision": {
        "decision": "Reject",
        "comment": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduced a novel parametrized graph operation called bipartite graph convolution (BGC). The proposed bipartite graph convolution layer functions as a regular graph convolution followed by a graph pooling layer, but it uses less memory. Also, the BGC layer can be used to aggregate multiple different graphs with various number of nodes. This paper further discussed the possibility of extending it to construct bipartite graph U-net structure with skip connections. Experimental evaluations have been focused on (1) comparing BGC against regular graph convolution layer followed by graph pooling layer in terms of classification accuracy and memory cost; and (2) comparing the regular graph-AE with the graph U-Net built on the proposed BGC layer with the unsupervised feature learning task.\n\nOverall, reviewer is very positive about the technical novelty of the paper. However, the experimental results seem not very strong. \n\n(1) The ECC model (Simonovsky and Komodakis, 2017) is no longer the state-of-the-art one on ModelNet. Please consider more recent papers such as the following one. Besides that, the performance delta seems very incremental.\n\n-- Dynamic Graph CNN for Learning on Point Clouds. Wang et al. In ACM Transactions on Graphics, 2019. \n\n(2) The current results are not very convincing as only one network structure is compared for each of the experiment. The ablation studies on graph structure (e.g., number of layers) are currently missing (Figure 4 and Table 1). \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new graph neural network model named BiGraphNet, which introduces a parameterized bipartite graph convolution operation to perform transformation between input and output graphs. The proposed method is claimed to have advantages over existing deep hierarchical GCN architectures mainly in terms of being able to construct analogous building blocks employed by modern lattice CNN architectures and the reduced computational and memory cost. The main weaknesses of this paper are listed as follows:\n\n1) The motivation is relatively weak, which is to bring in the analogous building blocks in CNN architectures. Although GNN is closely related to CNN and RNN, the graph learning tasks may not have the same property as in computer vision or natural language processing. It would be better to convince the readers from the GNN itself and carefully argue the necessity of the proposed method.\n\n2) The experiments in this paper are rather weak and not convincing. First there is no performance comparison to state-of-the-art GNN models, such as DGCNN, DIFFPOOL and GIN, etc. At least on the D&D dataset, many existing models report graph classification accuracy over 78.0, but the baseline method used in this paper only achieves 72.5. Thus it is not fair to claim the proposed method can retain or improve the performance of existing GNN models.\n\n3) The related work comparison is not sufficient. For example, some existing works have already explored to apply skip connections to the graph neural networks, such as [1], which is not mentioned and compared in this paper.\n\nBased on the above arguments, I would like to recommend a reject for this paper.\n\n\n[1] Xu, Keyulu, et al. \"Representation learning on graphs with jumping knowledge networks.\" arXiv preprint arXiv:1806.03536 (2018)."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes BiGraphNet, which proposes to replace the graph convolution and pooling with a single bipartite graph convolution. Its motivation comes from using stride(>1) convolution to replace pooling in CNN. The authors claim that the computation and memory can be reduced with the proposed bipartite graph convolution, because the pooling layers are removed. The authors also conduct experiments about graph skip connection and graph encoder-decoder to show that their method's flexibility.\n\nCons:\n1. If I understand it correctly, the bipartite graph convolution still needs a cluster algorithm to determine the output graph, which is identical to cluster-based pooling methods like DiffPool. In addition, previous pooling methods like DiffPool, gPool are NOT non-parametric as suggested by Figure 1. Therefore, the advantage of the proposed method is vague.\n2. The idea of bipartite graph convolution seems different from that of stride convolution. The connection should be better explained.\n3. The experiments of this paper are not very convincing. Comparison with more baselines and ablation study are needed to demonstrate the effectiveness of this method. On graph classification tasks, many other methods (GCN with pooling) are worth comparing with, like DiffPool, SAGPool, gPool, etc. More datasets should be included. In addition, it will be more convincing to do ablation study, e.g. single layer replacement."
        }
    ]
}