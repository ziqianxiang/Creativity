{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Here is my summary of the paper:\ni) a latent representation needs be learned in a self-supervised way, due to the shortage of labels; \nii) this latent representation should nevertheless efficiently support supervised learning;\niii) this is hardly the case in mainstream approaches as the latent representation aims at compression, not making any difference between informative and uninformative parts of the example (image).\niv) assume that one can determine the informative and uninformative parts;\nv) enforce the latent representation to focus on the reconstruction of the informative parts; \nvi) validation: this latent representation is more efficient, classification-wise\n\nA very interesting research programme, indeed. \n\nUnfortunately the devil is in the details: I incline to think that the paper is premature for publication at ICLR:\n* Assumption iv) is the weakest one; the argumentation (repeated parts; but not the background - though the background is repetitive in general) is inconsistent.\nI find it hard to consider that a part is informative/uninformative per se; (e.g. even the background can be informative if the goal is to discriminate natural from interior scenes.\n\nSuggestion: It might be a good idea to use the notion of saliency to define the informative parts (e.g. to define the loss weights involved in ${\\cal L}_w$; see for instance \"What do different evaluation metrics tell us about saliency models?\" (ArXiv 2017) and references therein.  \n\n* Step v) is not very clear. I understand that a grid of cells is defined on the image, and to each cell is associated the \"prototypical (repeated) patterns\" that appear in the cell (boolean presence/absence; continuous code). But a simpler alternative would be to break the full image into cells; to learn a vanilla latent code on these cells; the repeated ones might be better encoded just due to their higher number; (it is true that the background also would matter); or you use the saliency score as weight on the loss. \n\nDetail: I would omit the comparison between infant cognition and the NN training, where the former one needs incomparably less examples; the comparison suggests that the infant is an independent entity, which is debatable -- think of a feral child. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a VAE architecture that tries to represent only interesting parts of an input in its latent space. These interesting parts of an image supposedly resemble patches in the observation space. The proposed model makes it also necessary to sort of redefine the reconstruction loss. The goal of the model is to find latent representations that are more useful (compared to standard VAE models) for downstream tasks.\n\nThe paper suggest to learn a binary mask in latent space that indicates 'interstingness'. The concept can be generalized to multiple 'interesting things', the number of which is upper bounded by the user (see ablation study). These binary indicators not only identify interestingness, but also 'sameness', i.e. 'interestingness' is resembling the fact that an identified concept appears repeatedly.\n\nAs a first comment it seems that the paper is clearly tied to the problem of learning representations of visual data -- it seems to be not easily applicable to arbitrary domains?\n\nI think this is overall a nice idea. I also like that the ultimate goal of the architecture is 'downstream' usability (classification results in this case).\n\nNonetheless, the model is formulated in a generative so what's its generative capacity? I'm missing this aspect in the paper (or Appendix at least). Also, in terms of comparing it with alternative approaches, why is the betaVAE choosen? Conceptually a VQ-VAE seems to be much more closer to the described modelling assumptions (this may potentially not directly obvious). Finally, I'm uncertain about the actual impact of your model, compared to the additionally introduced weighting scheme of the reconstruction loss in section 3.4. This seems to have an important effect, make your model and betaVAE look very similar performancewise. So what does your model actually bring to the table? I'm missing investigations along that question.\n\nSpecific remarks:\n* Why use a NN for the downstream classification task. Why not something much simpler, like a linear classifier?\n* What's the structure of this NNclassifier? Section 4 mentions fully-connected layers, but 4.2 talks about 'added conv layers'?\n* You repeatedly miss the article \"the\"."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces an algorithm based on the VAE for training a feature map in an unsupervised way, and later assesses the quality of said feature map in supervised classification tasks. The trained feature map is a fully-convolutional neural network, which thus extracts features that relate each to some spatial path of the image. The latent formulation forces the model to cluster these spatial patches into groups. Patches assigned to the same group see their features averaged and only copies of this average are given to the decoder. The rationale is that this mechanism guides the model into focusing on patches that display some kind of repetition across the dataset, which are more likely to contain useful information for classification. Here the VAE is used as a proxy for training the feature map, rather than as a generative model. Several experiments are provided comparing the classification performance when using the feature map trained with the proposed model PatchVAE and its variants compared to the more traditional beta-VAE.\n\nWhile the paper presents an interesting construction on the VAE loss, I see a few issues with it which in my opinion should block its acceptance, mostly concerning the experimental results. I however believe they can be addressed and I will revise my decision if they are.\n\nFirst of all, the tables of results (1, 2, 3, 4 & 5) all present extensive accuracy result, but lack any indication of uncertainty or confidence intervals. Where they obtained by a single run each? Are they an average over several runs with different random seeds? This should be clearly detailed in the text of the paper, and if possible indication of the variance of the results should be displayed. As this paper stands, I cannot say if the results of PatchVAE are indeed significantly different from those of beta-VAE, or if the difference is actually within the margin of error of the experiments.\n\nSecondly, the experiments show that the feature maps learned by PathVAE and beta-VAE are both significantly outperformed by a supervised training, but there is no commentary about that. In which context could PatchVAE be used in practice? If it is not yet good enough to be used in production, do you have any suggestions about where to direct future research to further improve it? Also, how does PatchVAE compare to the self-supervised tasks that are in Section 2?\n\nThird, your ablation study notably does not talk about beta_app. How was its value chosen for the experiment? Does it have a lot of impact on the results? This seems to be an important parameter to get right.\n\nFinally, is a paragraph in the paper that needs to be clarified: part 3.2, paragraph \"training\": I don't think this is the posterior Q^A that is assumed to be a Normal distribution N(0; I), but rather its associated prior. Similarly for Q^V."
        }
    ]
}