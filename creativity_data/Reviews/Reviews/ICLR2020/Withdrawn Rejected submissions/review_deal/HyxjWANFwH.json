{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper tackles the problem of accelerating the linear average consensus algorithm over networks and proposes a data driven approach by formulating it as deep neural network training. Specifically, the authors unfold the algorithm as a feedforward network where each layer represents a specific time and nodes are states at that time, and weights represent connecting weights (edges) in the network. Then the training was done by stochastic gradient descent. In the experimental validation, the proposed approach outperforms the baseline (Xiao & Boyd (2004) for deterministic and random synthetic networks. The proposed approach seems technically sound and interesting, but the paper may need to improve the following issues. First, it may need to show justification why the proposed approach achieved the better results and works well. Second, it needs to have more thorough experimental validation. Specifically, the baseline seems too old and not working well since the proposed approach is more than ten times better in errors, so more recent and appealing baseline (such as Ito et al. (2019)) would be required for better demonstration of the effectiveness of the proposed approach. Finally, the presentation of this paper might need more detail (see my comments below). \n\nDetailed comments:\n- It was not clear to me how training data were generated.\n- In incremental learning, what are the initial values of weights when adding another layer?\n- Why do you choose mini-bath size of 1? Have you tried larger sizes?\n- Why does only WS network have the error increase at k=5?  \n\nTypo:\npage 3: deep leaning techniques -> deep learning techniques\n\nTo AC: \nI do not know much about this area, but tried to my best to understand and assess the paper and this paper does not look  ready yet to publish in ICLR.  However, it is still possible that I did not understand the paper well.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper presents a deep-learning based variant of the linear average consensus algorithm over networks.\n\nAt a high level, starting from an initial set of states (scalars associated to each node) the goal is for all states to converge to their average via local exchange of information. Linear average consensus achieves this by (iteratively) updating each state variable by a linear combination of each node's state with those of its neighbors. The idea proposed in this paper is to learn -- in a data-driven manner -- the *weights* used in synthesizing this linear combination so as to achieve quicker convergence than those used in traditional methods (where the weights are pre-chosen and kept static throughout) for a given budget of iterations (say T). \n\nSuch a learning can be achieved by recognizing that the dynamics of the state updates can be \"unrolled\" to form a feedforward T-layer linear neural network; therefore, the weights of such a network can be learned using standard learning methods, given enough training samples of initial states (and their averages). The authors show improved performance of such methods over a baseline method proposed by Xiao and Boyd (2004).\n\nWhile the problem/research direction is interesting, I have several concerns about the setup, proposed method, empirical evaluations, and theoretical contributions:\n\n* The basic setup is not well-posed in my opinion.  Specifically, the authors never reflect upon the choice of window length (T), and in their experiments somewhat arbitrarily fix T=10 without justification. But why would one want to compute the average in exactly T rounds (no more, no less)? \n\nFor example, if the network is well connected (and the initial distribution is unimodal and peaky) then the mixing time should be very small -- so if a large and inefficient T is chosen, one would presumably converge much more slowly using this method. Likewise for the reverse case: if the network is very poorly connected (e.g. there is a bottleneck somewhere) the mixing time should be very large, and for an incorrect T, no matter how well the weights are trained one would never get good answers.\n\n* The algorithm used to train the network is puzzling. The \"unrolled\" dynamical system resembles an RNN (not surprising, since one can unroll many iterative methods into an RNN), but the authors train this incrementally layer by layer. Why not use other, more common RNN training methods (e.g., plain backprop with or without weight-sharing)? \n\n* The experimental results are in general not easy to understand. Figure 2 suggests that the learned weights jump around quite a bit from layer to layer without much structure (likewise with Figure 5). There are strange bumps in the convergence plots which suggest that the consensus error actually *increases* in some intermediate layer before decreasing. This again suggests that the method is not robust. Moreover, the test examples are synthetic, small networks and it is unclear whether the proposed method would scale at all to realistic networks.\n\n* Finally, the theoretical part is a bit confusing. I understand how the upper bound on r^T_asym is derived (basically, Holder's inequality repeated used) but don't know where the lower bound (and hence the claimed tightness) comes from. \n\nFor all these reasons, I recommend a reject."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Distributed averaging algorithms (a.k.a. average consensus) have been thoroughly studied in the control and distributed systems communities. This paper proposes to learn a sequence of weight matrices for distributed averaging, by leveraging differentiable programming and stochastic gradient descent. The idea is natural to investigate, and the findings suggest that the proposed approach can indeed learn useful sequences of weight matrices, converging faster than repeatedly applying the same matrix (the more common \"static\" approach).\n\n1. That allowing to use a sequence of weight matrices leads, in general, to overall faster convergence is not so surprising. After all, the additional matrices provide additional degrees of freedom. The real challenge is how to design a sequence of weight matrices which, as rightly pointed in the paper, does not have a clean, convex formulation. However, there are two papers that have explored related a question:\n\nA Sandryhaila, S Kar, and JMF Moura, \"Finite-time distributed consensus through graph filters,\" ICASSP 2014.\n\nS Segarra, AG Marques, and A Ribeiro, \"Optimal graph-filter design and applications to distributed linear network operators,\" IEEE Trans Signal Processing, 2017.\n\nIt would be good to mention the relationship to these and possibly to also compare to the design approaches proposed there.\n\n\n2. There are some aspects of the experimental evaluation that could be improved.\n\n2a. Experiments are reported for Zachary's Karate network, a 2-d grid, and two families of random graphs, Barabasi-Albert and Watts-Strogatz. It would be interesting to try learning weights for graphs that are known to be harder for consensus. (See, e.g., A Olshevsky and JN Tsitsiklis, \"Convergence speed in distributed consensus and averaging,\" for examples)\n\n2b. Training and testing with the case where nodes are initialized with iid signals is not very interesting for this application. In a sense, all nodes are initialized close to the average. It also follows from the law of large numbers that each node only needs to average with a few number of neighbors to have a good estimate of the average. In the case of a \"static\" weight matrix, the worst-case initialization is given by the eigenvector associated to the second largest eigenvalue (in magnitude) of W. Have you tried using an initialization more like this, for training and/or testing? More generally, what happens if the initialization at test time is different from that used for training?\n\n\n3. I was happy to see that periodic extensions are investigated in Sec 3. Have you studied the impact of T? For the networks considered, I would guess that T=10 is not far from the diameter of the network (maybe it's even larger). What happens if larger or smaller T is used?\n\n\nBased on the above concerns, I recommend that this paper be rejected. The initial results are promising, but additional work is needed to strengthen the results. Even more compelling would be to \n\n\n\nMinor: \n- The paper mentions that \"It was recently shown by Kempton et al. (2018)...\". I believe the well-cited paper on \"Randomized gossip algorithms\" by Boyd, Ghosh, Prabhakar, and Shah also proposes a decentralized algorithm for solving the semi-definite program formulation of optimal weight design.\n\n- Why constrain/force the network weights to be symmetric? Other work has shown that using non-symmetric mixing weights can lead to faster consensus (but the design problem also becomes non-convex, and thus more challenging).\n\n- Why train in the incremental, layer-wise manner described in Sec 2.2? Did end-to-end training not work as well?\n\n- Why add the Frobenius norm regularizer? Why is any regularization needed at all here? Also, is the optimization process constrained to learn stochastic matrices?\n\n- Sec 3 mentions that \"the number of data-set per learning is set to 10000\". Is this the number of optimizer steps taken? Or are are multiple epochs performed over this data set? Is this the number of steps per layer, or total?\n\n- How are the weights W initialized in the experiments? Have you examined if the proposed approach finds very different solutions if started from different initializations, or if trained with different random seeds?\n\n- What value of \\lambda (regularization parameter) was used in the experiments? How sensitive is the performance to this choice?\n\n\n"
        }
    ]
}