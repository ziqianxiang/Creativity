{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Authors consider the attacks where there is 1 edit in the middle characters of words. They propose a mapping strategy for the most frequent words. The point of the mapping is to map all adversarial versions of a word to be mapped to the same word. In essence the mapper is a typo corrector. They design the mapper by first connecting all the words that have a common edit 1 typo. In the basic version they map all the words in a  connected components to the most frequent word of that component. In the more advanced version they define a criteria based on the distance of a words embedding from its mapped words embedding and greedily merge clusters. Using this preprocessing technique drops the accuracy of the model by 10-20%. But the model is almost 100% robust. In the face of an attack the accuracy does not drop more than 5%. Whereas other methods suffer much higher drops in the face of an attack.\n\nI vote for rejecting this method since 10-20% drop in the accuracy is only acceptable if a system is under constant attack and all the sentences are adversarially perturbed.\n\nOne downside of their method is that their mapper maps each word individually and context independent. There is no reason to have a constant word mapper. A context dependent mapper can identify whether in this sentence “aboupt” is a typo of “about” or “abrupt” since their use cases and part of speeches are quite different. If one augments the words with their part of speech at the very least one can still have stable mappings. There would be no need for such high sacrifices in the fidelity."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper explores adversarially robust neural encodings for text that ensure robustness to text typos. The key idea is to balance two criteria, namely (i) the fidelity of a model, i.e., accuracy for a task, and (ii) stability, that captures the robustness of the model to perturbations. The paper proposes two methods that balance these criteria: (i) using connected components  over text features and their perturbations, and ii) agglomerative clustering of the encodings. Experiments are provided on six GLUE tasks and show some promise against baselines, when using the BERT encoding scheme.\n\nPros:\n1. The paper, while a bit too verbose, has a tutorial flow to it. However, I would think it could be significantly trimmed, as there is an \"oversimplification\" of obvious details. \n2. The key idea is straightforward, and make sense. \n3. The results show promise.\n\nCons:\n1. The proposed methods (connected components and agglomerative clustering) seem very basic ideas in this context. The paper proposes hand-crafted schemes to decide the connected components. Can't we have more principled end-to-end learning for achieving robustness? I am not an expert in NLP so cannot judge the significance of the ideas or the results, however from what I read, the contributions look basic/very incremental and heuristic.  \n\n2. The experiments show some promise, however the edit distance of perturbations is limited to one. While prior works have also reported in this setting, it appears to be a very constrained setup. Further, the baseline accuracy of the model is substantially lower (standard in Table 1) against the perturbed performance. Is this justifiable? \n\nOverall, the paper uses classical techniques in a heuristic way to combat sensitivity to adversarial perturbations in the text domain. Experiments show promise, however, the novelty and significance of the contribution is questionable."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper explores a method for correcting typos in a \"worst-case\" way, in which worst case means against, for example, all mistakes of edit distance k for specified k. To me, this is an interesting problem because 1) typos do not work for word vectors, which is the most common input in downstream NLP tasks, 2) minimizing edit distance is a combinatorially hard problem, so all brute force methods are infeasible at scale. \n\nThe paper provides a nice discussion of these issues, and proposes a graph-based method in which all words + their typo versions form a graph, and either a word is corrected by a representative word which is the most popular amongst connected components, or a softer version of this is used. The paper also describes a tradeoff between stability and fidelity (which I think is better understood as performance degradation) and discusses how this approach that really is task agnostic. \n\nOverall, I think the scheme is simple to understand and intuitive as to why it works well. Table 1 also looks like a nice collection of tasks, and the baselines seem reasonable. The main concern I have is I don't see how the combinatorial nature is reduced. If you are given a corpus and someone writes a typo, what is the procedure? Is that typo already stored in a database, in which case the database is exponentially large in memory? Or do you need to compute edit distances in order to create this graph to begin with, having the combinatorial issue there? If the combinatorial issue is not resolved then I don't see why this is better than just a nearest neighbor lookup where the distance is edit distance, which otherwise I would suggest as another important baseline. (If the authors can show this is a misunderstanding on my part, and/or include a much more in-depth discussion of the computational complexity of each scheme, I will increase my score.) \n\nMy other main critique is that some things are a bit vague. For instance, I don't understand how the sentence embedding is created, though it is discussed--the exact procedure is vague. Also, I don't really understand equation (5). What is the std taken over? What is the random variable? It seems like the first term should be conditional on the accuracy of a typo-free scheme, since the task can be arbitrarily hard. \n"
        }
    ]
}