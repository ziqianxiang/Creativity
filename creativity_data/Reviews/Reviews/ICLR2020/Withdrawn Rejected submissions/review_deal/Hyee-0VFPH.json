{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes a modification to momentum based optimization algorithms motivated from theory of geodesic optimization algorithms on manifolds. The modification suggested by the paper is to use the normalized inner product between the previous update and the present gradient in the momentum multiplier in standard momentum algorithms like Polyak's Heavy Ball or Nesterov's momentum. They propose two DIFFERENT algorithms for strongly convex vs mini-batch non-convex optimization. In essence for STRONGLY CONVEX, the momentum multiplier is INCREASED proportionately when the present negative gradient aligns with the previous update. On the other hand for NON-CONVEX mini batch optimization the update is the exact opposite, the momentum multiplier is INCREASED proportionately when the present negative gradient DOES NOT align with the previous update (essentially treating the newly seen update as an outlier). Experiments are performed with strongly convex objectives and Deep auto encoders. \n\nI propose a rating of Weak Reject for the paper. The primary reason for this rating is that while the idea looks interesting and potentially helpful, the treatment of the idea presented in the paper is unfortunately quite PRELIMINARY and leaves much to be desired. The following points detail what I find missing from the paper - \n\n1. More discussion about the intuition wrt Geodesic Optimization - Even though the paper presents itself as a close relative of the Geodesic Optimization, the connection between the presented algorithm and the geodesic optimization seems a little superficial. It would have been great to see a derivation for the algorithm as a special case of the Geodesic optimization on R^d or something to that effect. In absence of such scientific connection - the present motivation seems merely visual. \n\n2. Theoretically grounding at least the convex case - I would have loved to see some analysis of the convex case of the algorithm. Of course one cannot beat the Nesterov acceleration rate in general. In that light, some analysis/discussion as to in which kind of functions one might expect to see a benefit of such a term even on quadratic functions would help to build intuition. At the very least establishing a convergence rate of the same order as Nesterov momentum would be extremely helpful. The purpose of such analysis is no to merely provide a proof but rather to build intuition as well as understand that there are no failure modes for the algorithm. \n\n3. Proposing completely opposite updates for strongly convex and non-convex belie the lack of intuition even more. The authors seem to point to the fact that momentum seems to play different roles for mini batch potentially non-convex optimization and full strongly convex optimization. This is a good understanding to have, one that is still in the process of being better understood in theory. Nevertheless, proposing opposite updates with little discussion seems very preliminary. Maybe a good question to ask here is suppose I wish to do mini-bath convex optimization, which of the two opposite versions would the authors recommend in that scenario?  \n\n4. The experiments also are preliminary. They are complete in the full batch strongly convex case, but even in convex cases mini-batch versions seem to be missing and these are important considerations. For non-convex deep learning based experiments - the authors claim that the autoencoders considered in the paper are benchmarks for deep learning optimization but that is unfortunately an outdated viewpoint. While they are definitely the first important cases to verify, to seriously verify the efficacy of the method (especially in the light of no theoretical treatment) it would be good to experiments on larger vision/language models. \n\nOverall while I believe that the idea is promising, the treatment of the idea seems preliminary and hence my rating. The authors can certainly improve the paper by adding some analysis/discussion of the algorithm even in simple cases or performing large scale modern experiments to establish practical efficacy. \n\nAlso I would like to note that the idea of including the inner product of the previous gradient and the present gradient in training dynamics is not completely new even in deep learning. Consider this paper for instance - https://arxiv.org/pdf/1703.04782.pdf-  which i think should be cited/compared to. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThis work provides novel algorithm called SGeO to do the geodesic optimization. SGeO combines the advantages of both first-order algorithms and second-order algorithms. The authors provided two versions of SGeO for convex and nonconvex cases. Experiment results backup their theory.\n\nPros:\nThe writing of this paper is clear. The authors provided the new algorithm SGeO and showed some intuition behind this algorithm. The experiment results are convincing. \n\nCons:\n- The title of this paper overclaims. It seems that SGeO is a general optimization algorithm which does not utilize the properties of neural networks. Thus, I suggest the authors to revise the title and introduction part to emphasize that their algorithm works for general optimization problem. \n- The importance of this work is limited. First, there is no theoretical result for SGeO. Second, the authors do not compare their algorithm with previous baseline algorithms in detail. Thus, it is hard to convince me that this work is significant. I suggest the authors add theoretical analysis and detailed comparison with other geodesic optimization algorithm for both convex and nonconvex cases. \n- The explanation about the change from convex case to nonconvex case is not convincible enough.  I do not understand why it is necessary to make such a change from Algorithm 1 to Algorithm 2. The authors can make more comments in section 3.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors proposed a new optimization algorithm called Stochastic Geodesic Optimization based on a geodesic optimization algorithm. It can also be viewed as an adaptive momentum methods. It may be some kinds of useful tools, but I feel the intuition behind the algorithms a little strange.\n\nSpecifically, I cannot relate the geodesic optimization with the newly proposed algorithms. Please see the detailed comments. Also, the authors do not provide some theoretical analysis of their algorithms, which let me doubt the generality of their algorithms. I know the analysis of adaptive momentum methods can be very hard, even with strongly convex and smoothness assumption. But the current formulation cannot convince me well.\n\nDetailed Comments:\n1. The notation is somewhat confusing. It maybe better to use g_t as the gradient and d_t as the previous update throughout the paper.\n2. What is the adaptive coefficient in Eq.(2) mentioned before Eq.(10)? If it is the inner product of v_t and gradient, the authors should mention is in Sec 2.1.\n3. Still, how to derive the algorithm in Sec 3.1? The quadratic approximate geodesic equation is theta_{t+1} = theta_t + d_t delta_t + 0.5 g_t delta_t^2 - <g_t, d_t>d_t delta_t^2, and the update of the proposed Geodesic Optimization is theta_{t+1} = theta_t + alpha (1-<\\bar{g}_t, \\bar{d}_t>) d_t - \\delta_t g_t. There are so many differences between this two formulas. For example, the approximate geodesic have the quadratic term delta_t^2, while the proposed update do not have. Also, there is no need for the approximate geodesic equation to normalize the d_t and g_t. The authors additionally introduce a tunable alpha, which makes me confusing. If it directly follows from geodesic approximation, why should the authors introduce this alpha? If there is only little connection with geodesic approximation, why should the authors mention it?\n4. As far as I can say, there are some differences between momentum and geodesic approximation. For example, in momentum methods, the coefficient of momentum can be set close to 1, but for geodesic approximation, [1] says that the step size (similar to the coefficient of momentum) should scale with inverse of the norm of gradient. I am not sure how the authors deal with this issue and connect these two methods.\n5. I think the transformation between (11) and (13) is so intuitive without any further theoretical support. And using the normalized gradient and previous update also violates the claim of geodesic optimization. I can hardly say what does the proposed SGeO really do. It may be useful, but I feel the intuition of it really weird.\n6. There are also several other second-order methods as well as optimization on Riemannian manifold, e.g. [2][3]. Although they may use higher-order information, I think the authors should still have some discussion over that, if the authors want to claim they are geodesic optimization.\n7. The experiment results are strong. However, I prefer much more tasks with much more dataset and much more network structures, at least classification with CIFAR10 and CIFAR100 is doable.\n\nTo sum up, I feel the proposed methods have little correlation with geodesic optimization. It is more likely to be some adaptive momentum methods, but with no theoretical analysis. The experiment results are somewhat strong, but I wonder how it can generalize to different tasks, different network structures and different dataset. Both of the theoretical analysis and more elaborated experiment results are necessary for this paper to meet the standard of publication.\n\n[1] Ricky Fok, Aijun An, and Xiaogong Wang. Geodesic and contour optimization using conformal mapping. Journal of Global Optimization, 69(1):23–44, 2017.\n[2] Zhang, H. & Sra, S.. (2016). First-order Methods for Geodesically Convex Optimization. 29th Annual Conference on Learning Theory, in PMLR 49:1617-1638\n[3] Song, Yang, Jiaming Song, and Stefano Ermon. \"Accelerating Natural Gradient with Higher-Order Invariance.\" International Conference on Machine Learning. 2018."
        }
    ]
}