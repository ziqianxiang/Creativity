{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1889",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nMain contribution of the paper\n- The paper proposes a pruning technique targeting Depthwise separable convolution (DWC), which is not yet importantly treated in this field.\n- The proposed method achieved meaningful improvements on CIFAR 10 and SVHN.\n\nMethods\n- Clustering the kernels having similar pruning patterns\n- Training and filter pruning are conducted simultaneously.\n- In the pruning step, the filters are pruned among the cluster.\n\nQuestions\n- The clustering of the kernel seems to be critical for the performance. Is the clustering derived by the constraint of DWC as explained in 3.2? \nIf so, it can be the strong point of the work, and it would be better to add a more detailed explanation in section 3.3.\n\nStrongpoints\n- As far as the reviewer knows, it is a novel attempt to prune lightweight DWC based networks, MobileNet v1 and v2, in this field.\n- They propose a pruning technique considering the constraints of widely used DWC structures.\n- The proposed method achieved meaningful improvements on CIFAR 10 and SVHN.\n\nConcerns\n- The main concern the reviewer has is that the experiment section should include more items.\n- Practically, many works stem from MobileNet v1 and v2 reports ImageNet (or at least Cifar 100) classification results. We concern that the results from CIFAR 10 and SVHN are not enough to verify the proposed method. Also, recent pruning methods (Mostafa.et.al, Dettmers.et.al, Han.et.al 2) report ImageNet result, or at least Mini-imageNet result (Lee.et.al).\nSince the proposed method targets MobileNet V1&V2, the verification on the larger dataset (ImageNet - at least Cifar 100 or Tiny-imageNet) is required.\n- The other concern is the lack of comparison to the other method. The author argues that the existing pruning methods are not adequate for DWC pruning, but we cannot find sufficient comparsion results regarding the argument. The reviewer thinks that it is possible to apply the existing pruning methods (Han.et.al 1),(Of course, we cannot guarantee the performance.) So the author must compare the proposed method to other pruning methods and show that the proposed method performs well in DWC pruning; MobileNet V1&V2, than the existing pruning method.\n\nConclusion\n- The author proposed a new pruning technique for DWC (MobileNet v1&V2) which would be novel, but the experiments should include more items to verify the method.\n\n\nInquiries\n- Testing on larger datasets (ImageNet, or at least CIFAR 100 and Tiny imageNet)\n- Comparison to the other pruning method\n- (supp) More explanation for the clustering part.\n\nReference\n[1] Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, 2019.\n[2] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.\n[3] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pp. 1135â€“1143, 2015.\n[4] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J Dally. Dsd: regularizing deep neural networks with dense-sparse-dense training flow. In ICLR, 2017.\n[5] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. In ICLR, 2019."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a technique to gradually prune the depthwise separable convolution networks, such as MobileNet, for further improving the speed. By imposing more structural constraints and using multi-stage iterative pruning, the proposed pruning algorithm can achieve roughly 2x speedup with little accuracy drop on standard benchmarks.\n\nIn my opinion, this paper is a borderline paper because it lacks novelty in terms of the algorithm itself. Particularly, multi-stage gradual pruning has long been used in network pruning for better performance. Nevertheless, applying the technique of network pruning to lightweight architectures (so as to handle depthwise separable convolution) seems to be new and promising. Given that, I've given a score of 3 and I'm willing to increase the score if the authors can resolve my concerns below.\n\nConcerns:\n- All experiments are done with MobileNet (v1 and v2), I wonder if the algorithm works well on other architectures. I suggest the authors to conduct an extra set of experiments with a new architecture.\n- The scope of the paper seems to be a little narrow. I wonder if the authors can include a few other lightweight operators.\n\nMinor Comments:\n- I think a recent paper [1] is quite relevant, though they focused on pruning standard convolution kernel. In particular, the paper utilizes filter pruning to get depthwise separable convolution from standard convolution kernel. I wonder if the technique in this paper can been applied to depthwise separable convolution operator to further reduce parameters.\nFor example, you can first reparameterize the original depthwise separable convolution with three consecutive layers with the first and third layers 1x1 convolution (the first and third layers serve as eigenbasis to whiten the middle layer).\n\nReference:\n[1] EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis. ICML, 2019."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper study the structured filter pruning of depthwise separable convolution weights. The main idea is to remove filters while not introducing intermediate channels not having either incoming weights or outgoing weights. The authors present experimental results for MobileNet V1 and MobileNet V2.\n\nThe main weakness is the novelty of the proposed algorithm. I think that the main idea is quite straight forward, which I believe a simple extension of ordinary convolutional filter pruning. Crucially, the experimental results are also weak. The proposed algorithm was not compared with any other pruning algorithm which is definitely not convincing to verify the effectiveness of the proposed algorithm. \n\nIn terms of the presentation, the paper is relatively well written. Especially, I appreciate the clear explanation of preliminaries (Section 3.1). There are some undefined notations in Section 3 (e.g., \\mathbf{F}, cluster)."
        }
    ]
}