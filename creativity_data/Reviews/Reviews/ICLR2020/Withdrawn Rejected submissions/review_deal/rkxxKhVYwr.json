{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper addresses the task of evaluation of point-based architectures for deep learning, aiming at establishing the contribution of various architectural blocks (e.g., local density reweighting) to a number of desirable properties of the resulting models (such as adversarial robustness, rotation robustness, etc.). To assess the influence of such blocks on the resulting models, authors formulate and test a variety hypotheses while measuring model performance using a number of existing measures, such as information discarding/concentration, etc. \n\nI believe the paper cannot be accepted in its present form, as (1) no clear motivation for the work is presented, (2) the problem statement and the research questions are very blurry, and consequently (3) the results do not provide significant new knowledge related to the point-based neural architectures.\n\nWhile the specific architectural choices do influence the characteristics of commonly used models and a systematic evaluation of the former is indeed useful for practice, the paper does not indicate a particular application context, which I believe is crucial to the entire following narrative. Do the authors aim at evaluating in the recognition/labeling context? In the synthesis context (e.g., novel shape synthesis, completion, or upsampling)? In the geometry processing context (normal estimation, patch decomposition, or denoising)? Would the claimed hypothesis, let alone the results, even be formulated the same way, once one switches to a different class of applications? If yes, it would be hard to understand from the paper. If no, this could potentially diminish the value of the entire work significantly. \n\nIf the paper targeted a particular class of applications or simply a specific task, the hypotheses might have looked more natural; however, currently I find the motivation for these hypotheses rather vague. New insights into neural architectures or other models could emerge from (empirical and/or theoretical) regularities revealed by a purposeful experiment; however, the authors perform a seemingly random evaluation of point-based architectures which is of a fairly limited value per se. The reader might wonder: what is the exact problem that the study aims to address? Thus, I believe, a clarification of the problem statement and the basic research questions for the entire work is needed. \n\nThe paper does a good job of thoroughly presenting results of numerous computational experiments and values of the selected measures. However, as stated before, the results fail to provide significant insights into point-based neural architectures due to (1) lack of specific applications the authors are trying to solve and (2) arbitrariness of the chosen experiments lead to equally arbitrary conclusions. If “architecture of We et al., which uses local density information, improves adversarial robustness”, what difference would it make? Would it open novel opportunities for point-based deep nets, and if yes, which exactly?\n\nOne minor additions is that since the paper focuses on empirical evaluation of machine learning models, significance measures such a statistical confidence intervals would be very useful to present in the results.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary\n\nThe paper compares four recent deep architectures for 3D point cloud in terms of rotation robustness, adversarial robustness, and neighborhood inconsistency. \n\nThe paper verifies Architecture 2 and Architecture 4 mainly improve the rotation robustness; Architecture 1 and Architecture 3 have positive effects on adversarial robustness; Architecture 3 usually alleviates the neighborhood inconsistency.\n\nStrengths \n\nThe comparison is very useful and timely. \n\nWeaknesses\n\nHowever, it is not clear why these four architectures. What about LaserNet, Pixor, FuseNet from Waymo?\n\nThere are not much insights in the findings. \n\n\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "First I must state that this paper is not within my area of expertise. I'm leaning towards recommending accepting the paper, but this recommendation comes with high variance.\n\nThe paper investigate different neural network architectures for 3D point cloud processing. A set of published architectures are studied in terms of robustness towards adversarial attacks, rotational robustness and neighborhood consistency. To do this, the paper propose a set of metrics for measuring such robustness and compute these under various datasets.\n\nOverall, I am happy that the authors provide a set of hypotheses and conduct experiments verify/debunk these. Not being within this field I cannot determine the potential impact of the gained insights. Are there any surprise conclusions from this study or does it verify what was already known?\n\nIt is not entirely clear to me how I can operationalize the findings of the paper. For example, if I want to design an architecture that is robust to both rotations and adversarial attacks, does the paper tell me which building blocks to use in my architecture? This aspect was not clear to me.\n\nRegarding the applied metrics, then I would like to hear how they are different from the ones proposed by Ma et al. (the authors name this as their main source of inspiration).\n\nIn metric 2, I did not understand the notions of \"foreground\" and \"background\". Should I merely think of \"background\" as being outliers? Sometime background information carry relevant information on the foreground; sometimes the background information is not present. How does this work?\n\nWhen taking the expectation of the Jensen-Shannon divergence, is this done by sampling? If so, are there practicalities that should be discussed in the paper?"
        }
    ]
}