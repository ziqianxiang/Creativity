{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper describes how they extend a previous phrase-based neural machine translation model to incorporate external dictionaries. The reviewers mention the small scale of the experiments, and the lack of clarity in the writing, and missing discussion on computational complexity. Even though the method seems to have the potential to impact the field, the paper is currently not strong enough for publication. The authors have not engaged in the discussion at all. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposed an end-to-end phrase-to-phrase NMT model (NP2MT). I think the contribution of this paper is incremental and the idea is of less novelty. In general, the model is largely based on the NPMT model, where modification is the introduce of phrases in the source sentences. Then the author proposed the memory module strategy. In the experiments, the performance improves significant when using out of domain dictionary, but less significant for in-domain dictionary. I also have a concerns about the experiments. The dataset used in this paper seems not convincing to me. By my own experience, the performance on small dataset for either LSTM or Transformer is not stable. The authors just tested the model performance on WMT test set. I think at least the WMT training data should be used for training as well. Another question is the details about the training time and decoding time, since the dynamic programming is used.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This submission belongs to the field of machine translation. In particular, it looks at the problem of phrase-to-phrase translation (previously used state of the art approach) using neural network approaches. The main idea behind this paper is to use a segmental (analogue of phrases) form of neural networks on the source side and attention mechanism to align those segments with segments generated on the target size. This submission additionally describes how an external dictionary can be incorporated using a heuristic approach. I believe this submission could be of wide interest to the machine learning community. I find experimental validation to be satisfactory whilst presentation unsatisfactory for the following reasons:\n\n1) notation\n\nGiven that you are dealing with two sets of sequences (source, target side), segments on both sides, attention linking both sides I find it strange that the notation used is not carefully introduced and clearly explained. What is ${\\bf g}_{{\\bf z}<z_k}$ using precise mathematical language? How it is different from ${\\bf g}_{{\\bf z}<k}$ and what that is? The same for ${\\bf y}_{<t}^{z_k}$, a*, d* and all other variables. In order to help the reader understand your approach it is fundamental to be precise and not ambiguous about each (!) symbol you are using. \n\n2) Algorithm 1, 2 and Figure 1\n\nThe algorithmic description was meant to help the reader to understand the process. Unfortunately I have to disagree that is has accomplished this purpose. Please make sure you have unambiguously explained every single term, explicitly say what you are running argmax over, etc. Please also make sure you are discussing/describing the algorithms/figures in your submission. Given the non-trivial nature, lack of proper introduction into the notation used, you cannot simply point the reader to it and not discuss it. \n\nMinor comments:\n\nPlease refrain from using \"vanilla model\" unless you can cite a publication defining exactly what that is.\nPlease explain how did you derive dictionary. \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a phrase-based encoder-decoder model for machine translation. The encoder considers all possible phrases (i.e. word sequences) up to a certain length and compute phrase representations using bidirectional LSTMs from contextual word embeddings computed with another bidirectional LSTM layer. The decoder also considers possible segmentations and computes contextual representations for the previously generated segments. Each word in the current segment is generated by a Transformer model by attending to all phrases in the source sentence. The authors present a dynamic programming method for considering all possible segmentations in decoding. They also present a method for incorporating a phrase-to-phrase dictionary built by Moses into the decoding process.\n\nI like the idea of phrase-to-phrase translation and the relatively simple architecture proposed in the paper. At the moment, however, I am not quite sure how practical their approach is. One reason is the experimental setting. Both of the datasets used in the experiments are quite small and it is not clear how the proposed model performs when several millions of sentence pairs are available for training. \n\nAnother reason is that the computational cost of the proposed model is not really clear. The authors state that it is much more efficient than NPMT but it is not clear how it compares to the standard Transformer approach. It seems to me that the computational cost of their model is highly dependent on the value of P (maximum length of phrases). \n\nAt first, I thought the decoder was implemented with LSTMs, but I realized that it was actually implemented with a Transformer by reading the appendix. I think this should be explained in the main body of the paper. I am also wondering how the authorsâ€™ model compares to a standard seq-to-seq model whose decoder is implemented with a Transformer.\n\nThe equation in section 2.2 seems to suggest that the model prefers segmentations with small numbers of segments. I am wondering if there is any negative effect on the translation quality.\n\nHere are some minor comments:\n\np.2 valid of -> valid\np.4 lookup -> look up?\np.4 forr -> for\np.4 indict -> indicate?\np.5 Table 1 -> Table 1"
        }
    ]
}