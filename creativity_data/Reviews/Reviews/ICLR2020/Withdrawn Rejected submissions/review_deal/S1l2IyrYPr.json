{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents two methods to debias the neural language models trained from large datasets. Concretely, one method is based on semantic similarity, i.e. two sentences describing the same content but with different control variables should have similar representation. The other method is based on sentiment similarity, i.e. regularizing on the predicted sentiments for sentences constructed with different control variables. There is no doubt that debiasing LM is important. The proposed methods also seem to be straightforward. My major concern is whether the resulting sentiment distribution associated with the prompts are desired and the generalizability of the method to broader categories of bias.\n\nDetailed comments:\n- Since the fairness metrics are defined based on sentiment prediction, it's surprising to see from Table 2 that the semantic similarity-based method is more effective than the sentiment similarity-based one.\n\n- From Fig 1, the distribution seems to become narrower, i.e. the predictions after debiasing seem to be more neural and less extreme. I'm not sure if this is what's desired for a debiasing method. \n\n- Other types of bias, which are beyond sentiment bias, exist in the language models too, e.g. the implicit association between general/race/nationality and properties. How to generalize the proposed method to other types of bias? \n\n- grammar: \"we can measure the the distance via cosine similarity.\""
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes to evaluate bias in pretrained language models by using a fixed sentiment system. The core of the idea is that if you generate with a given prefix from a language model the distribution of scores assigned by a sentiment system should be the same if the prefix is drawn for the same equivalence class (i.e. if the prefix includes different country names, or different occupations or person names). Several different prefix templates are tested and an objective is proposed for reducing this notion of bias (essentially applying transformations on unsupervised data corresponding to the equivalence classes that will be tested and then trying to match hidden states between transformed and untransformed instances). \n\nOverall, I have concerns about this paper on (a) methodological and (b) experimental grounds. \n\nMethodological: I can't understand the motivation behind the mixing of a generation system and a sentiment analysis system. While I recognize one could generate with an lm and then analyze its output with a sentiment system, why would one do this? This just doesn't correspond to any real usage of pretrained lm in nlp, or how any sentiment system would work. Pretrained lm are used to provide representations on human authored text and then processed for downstream tasks.  Why not just follow the methodology of Kiritchenko & Mohammad ( https://www.saifmohammad.com/WebDocs/EEC/ethics-StarSem-final_with_appendix.pdf ) and just compute difference in accuracy among templates where sentiment is known?\n\nExperimental: It seems like you are using the same augmentations for the training routine that will be testing your system on. Isn't it obvious that if you train your model to have equivalence for the small wordlists you constructed at training and then test with them again later, you will get improvement? I could not confirm any practical train/test separation between how the test was constructed and your proposed solution.  Furthermore, a simple missing baseline is just applying the augmentations and training with the LM objective and not doing any hidden representation matching.\n\nOverall I really like the direction of constructing equivalence sets and thinking about model behavior counterfactually but I feel this particular work has significant mythological and experimental flaws. \n\n\n\n\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary\n-------------\nThis paper first measures sentiment bias in language models as reflected by text generated by the models. The models are conditioned on prefix texts that have certain attributes (e.g., gender), and then generate continuations. Then a sentiment classifier is used to evaluate whether the generated text exhibits bias. The classifier are used for calculating certain fairness metrics. To reduce bias, the paper adds other objective terms to the usual language modeling objective, where the new term encourages similarity between two texts generated by conditioning on attributes from different groups. This similarity may be employed at either the language model hidden representations or after a projection in the sentiment classifier. The proposed new models show reduced bias according to the fairness measures, while obtaining similar perplexity to the baseline model and slightly reduced semantic similarity of the generated text to the conditioning context. \n\nThe paper does a very good job of motivating and situating the work in light of recent research. The problem is important and timely. The adaptation of metrics from fair ML may be useful for other scenarios handling bias in NLP models. The proposed regularization techniques are interesting and may also be useful in other contexts. \nThe experimental setup makes some assumptions that are not completely justified in my opinion. In particular: (1) using a sentiment classifier to assess bias may be problemantic; (2) the templates may lead to mostly neutral texts; (3) the semantic-similarity measure may not be very informative; Please see detailed remarks below.  \n\nAccording to the ICLR call for papers, papers exceeding 8 pages will be held to a higher standard. At present, I think the paper needs to be improved to meet this expectation. I am willing to reconsider my evaluation depending on the author response.  \n\n\nMain comments\n----------------------\n1. The sentiment in generated texts is assessed via a sentiment classifier. The authors acknowledge that the classifier \"is not perfect and might exhibit some biases\" and \"leave investigation of an unbiased evaluator to future work\". I think this is a reasonable approximation, but would like to propose at least a small-scale human evaluation of sentiment in the generated texts.\n2. Besides the question of whether the classifier is biased, one concern that I have with the methodology is that most generated texts may not have a very strong sentiment signal in either direction. The templates and examples given in the paper and appendix suggest that most generated text might just be neutral in its sentiment. The histograms in figures 2+3 also hint at that. The fact that the histogram in figure 2 shows bias which is less evident in figure 3 is a good evidence to the issues discussed in the paper. However, if indeed most sentiment is neutral, the magnitude of bias might be quite small. I'm not sure how to interpret the magnitude of the various fairness metrics, but the numbers in tables 2+3 appear quite small. \n3. It makes sense to evaluate perplexity to judge the quality of the generated texts and the tradeoff with fairness. It is also important to evaluate whether the generated texts is faithfull/consistent with the prompt, which seems to be what the semantic similarity gets at. However I'm not sure it makes much sense to measure cosine similarity of a sentence representation and one word embedding. I am also not sure whether the semantic similarities are reasonable. The comment on how empirically irrelevant sentences happen when there is a >20% drop in semantic similarity is also made in passing. It may be good to re-consider how to measure semantic similarity, as well as provide more examples/statistics on what makes a large difference. In this context, a human evaluation may be very helpful. \n\nOther comments\n-----------------------\n1. The related work is comprehensive and sets the background well for the paper problem setup and contributions. \n2. The point made about specifying the fairness measure based on the distributions distance, rather than individual predictions, seems quite important. It does make sense to me. However, the proposed bias reduction methods then operate at an instance level, rather than the distribution level. Would it make sense to consider alternatives that work at the global level, such as posterior regularization?  \n3. Does the methodology extend to multiple attribute groups? That is, a sentence may have more than one kind of attribute, such as both gender and race. \n4. What is P^*_S? Is this the sentiment score distribution over the union of all subgroups a \\in A? Or something else? \n5. What are \\alpha's in section 4, bottom of page 5? Also, the features end up being the average of the top two layers, based on the intuition that sentiment is a high-level property that should be represented at the top layers. This seems reasonable, but have you tried other layers as well? \n6. The concern of over-regularizing with the embedding similarity sounds plausible. However, giving a weight \\lambda to the fairness objective, as is done, should be able to control that. I'm not sure this is a good motivation for the sentiment similarity, as that too may lead to over-regularization. In this context, do you back-prop through the sentiment classifier into the language model? \n7. The details on the sentiment similarity regularization are a bit blurry. Is it the sentiment score that is used or the projection? What projection exactly is used? I assume this is something from the 3-layered MLP mentioned later.\n8. What are the baseline models w.r.t to the proposed regularized models? Given the curriculum training, it sounds like the regularized models are in fact the baseline models that are continuing to be trained with the regularization terms. This means that they were trained for longer than the baseline models, which may make the results less comparable (although perplexity values are similar). \n9. Have you evaluated the importance of curriculum training? What happens if models are trained from scratch with the regularization? For example, one could perform embedding similarity or even get the sentiment score/projection from an off-the-shelf sentiment classifier for the sentiment similarity regularization. \n10. I am not convinced that \"[t]he simple counting-based method .. is less prone to giving biased judgements\". Is there any evidence or further argument for that? \n11. I do not understand this sentence from page 9: \"When fairness scores are similar, sentiment-similarity regularization achieves better fairness scores.\" \n12. Section 5, last paragraph: looks like the words \"in Wikitext-103\" are missing, to explain that the bias in Wikitext-103 is smaller than WMT-19. \n13. Comparing tables 2 and 3 (end of section 5) is tricky, because both datasets and models change between them. Thus it isn't possible to disentangle the two.  \n14. Table 4 with the examples is mentioned but not discussed at all. Do we learn something from it? \n15. Kirichenko and Mohammad's paper appears twice in the references. \n16. Appendix B, last sentence isn't clear: \"We sample with template of 1.0.\" \n\n\n\n "
        }
    ]
}