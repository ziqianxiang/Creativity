{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper presents a timely method for intuitive physics simulations that expand on the HTRN model, and tested in several physicals systems with rigid and deformable objects as well as other results later in the review. \n\nReviewer 3 was positive about the paper, and suggested improving the exposition to make it more self-contained. Reviewer 1 raised questions about the complexity of tasks and a concerns of limited advancement provided by the paper. Reviewer 2, had a similar concerns about limited clarity as to how the changes contribute to the results, and missing baselines. The authors provided detailed responses in all cases, providing some additional results with various other videos. After discussion and reviewing the additional results, the role of the stochastic elements of the model and its contributions to performance remained and the reviewers chose not to adjust their ratings.\n\nThe paper is interesting, timely and addresses important questions, but questions remain. We hope the review has provided useful information for their ongoing research. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors describe a neural architecture with dropout randomizer with the aim of producing an ensemble of physical trajectories that seem plausible for a human eye. The question is relevant and interesting.\n\nThe authors bring in two improvements to the HRN model by (Mrowca et al., 2018). They drop the hierarchy from the shape loss, which is decipeted in Figure 3, and provide a change in the recurrent training decipited in Figure 4.  Figure 4 is only showing the modified training, not the initial HRN training, although Figure 3 contains both. For the sake of consitency and good read the Figure 4 should contain the original training image.\n\nThe description of the system is very verbose, but a concrete description of the system is only in the references, which make the manuscript hard to read and it does not fit well in a conference where one cannot assume that the audience is not an expert of this particular subfield.\n\nAs a remedy, I would suggest, a simple one-hieararcy level architecture with hyperparameters to show what is really happening. One could use the system in Figure 1 as a more thorough example.\n\nThe results looks good and valuable, altough the images provided are quite small.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary\n\nThere has been work on deep learning based forward dynamics model to learn the dynamics of physical systems. In particular, the hierarchical relation network (HRN) as proposed by Mrowca et al. (2018). However, HRN is deterministic. This is problematic for long-term predictions because of the uncertainty of physical world. This paper builds on top of HRN. It proposes a Monte Carlo sampling based graph-convolutional dropout method that can sample multiple plausible trajectories for an initial state given a neural-network based forward dynamics predictor. It also introduces a shape preservation loss and trains the dynamics model recurrently to better stabilize long-term predictions. It demonstrates the two techniques improve the efficiency and performance of model-free reinforcement learning agents on several physical manipulation tasks.\n\nStrengths\n\nLearning physics models which accounts for the multi-modal nature of the problem is very important. Graph-convolutional dropout is one method to deal with the multi-modal nature of the problem. It is a very good contribution. \n\n The tasks evaluated are not very sophisticated. \n\nWeaknesses\n\nThe paper's contribution is very incremental. \n\nThere should be a discussion on the different type of methods to account for uncertainties, e.g. bayesian neural networks and how they differ in terms of multi-modal predictions.\n\nBecause the tasks are not very complicated, it is not clear how good the whole neural physics predictor is.  \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "Overview:\nThis paper introduces a method for physical dynamics prediction, which is a version of hierarchical relation network (Mrowca ‘18). HRNs work on top of hierarchical particle-based representations of objects and the corresponding physics (e.g. forces between different parts), and are essentially are graph-convolutional neural networks.\nUnlike the original work, the proposed method introduces several improvements: 1. an updated loss function, that adds distance constraints between *all* the particles of the object. 2. recurrent training, when the predictions are fed to the inputs. 3. adding dropout.  \n\nWriting:\nThe paper is relatively well-written and easy to follow.\n\nEvaluation:\nAuthors compare their model on a dynamics prediction task and seem to outperform the original HRN, especially on longer-term sequences. In addition they report results for trajectory sampling (qualitative) and model-free RL, where using their model as a stochastic simulator seems to have positive impact on agent training.\n\nDecision:\nAlthough the proposed improvements upon HRN generally make sense, it is not clear if those are very significant on their own: adding dropout and recurrent training do not seem particularly novel and, since there is no ablation study, it is hard to see what exactly contributes to the reported improvements. \nAs for the experimental evaluation, it seems like important baselines are missing, and the model seems to be very sensitive to hyperparameters (see questions). Thus, I am currently leaning more towards a rejection, hence the “weak reject” rating.\n\nVarious questions / concerns:\n\n* It is not clear why authors do not provide a comparison to DPI-Nets (Li ICLR‘19). This model seems to be outperforming HRN, and from what it looks like is publicly available: https://github.com/YunzhuLi/DPI-Net. I would encourage authors to provide comparison to this baseline, and potentially on similar sets of experiments, or explain why this comparison would not be possible (which seems unlikely).\n\n* Authors seem to acknowledge that the model is sensitive to the hyperparameter choice (dropout rates), however, there is no numerical evaluation that would help readers understand how critical this choice is for the final performance. Judging from very specific settings in different experiments, this could be a serious concern.\n\n* I find it a bit strange that results in Fig. 7-8 are for random seeds. Is it not possible to just plot an average for e.g. 10 runs?\n\nUpdate:\nI would like to thank the authors for a detailed response!\nIt seems like there is a common concern about the novelty among reviewers: improvements over HRN are quite incremental. Although authors provide a verbal justification for not comparing to another strong baselines, I do not see why would it not be possible to compare methods in the similar settings, even though that baseline might be more limited.\nGenerally, if the main contribution is actually only the \"stochastic\" part and not improved performance, then just adding dropout does not seem like a particularly novel approach to me: whether the convolutions are on graphs or on euclidean domains, this does not change the way dropout is done.\n\n\n\n\n\n\n\n\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}