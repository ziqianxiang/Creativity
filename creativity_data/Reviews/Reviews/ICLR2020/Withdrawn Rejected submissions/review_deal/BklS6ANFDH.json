{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a semi-supervised learning model for semantic dependency parsing using CRF Autoencoders. \nThe encoder adopts the model of Dozat & Manning (2018), which formulates the semantic dependency parsing task as independently labeling each arc in a directed complete graph.\nThe decoder is inspired from Corro & Titov (2019) and propose to generate sentences for multiple times (each time conditioned on a different head) and combine to yield the decoding probability.\n\nThe paper is written clearly, but there are some weak points, which hurt the quality of this paper.\n\n1. Novelty. \nThe idea of CRF Autoencoder is not new. Further, the encoder in this paper is not a CRF, so it is confusing to say \"using CRF autoencoder\". The main novelty seems to come from the design of the decoder.\n\n2. Experiments.\nFor semi-supervised learning, showing the proposed model achieves improvement over the supervised baseline is usually not enough. Comparing to self-training (a classic and straightforward method also capable of learning from both labeled and unlabeled data), at least, is needed.\n\nNoting that the main novelty comes from the decoder design, the readers usually expect to see how the new decoder design improve over a baseline decoder, e.g. consisting of simple class conditional distributions, as in the original paper of CRF Autoencoder ( Zhang et al. (2017) )."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper considers the task of semantic dependency parsing (where the parses look similar to dependency parses but are DAGs instead of trees). The paper focuses on semi-supervised learning using auto-encoding loss: given a sentence s, the encoder defines a distribution p(Y | s) where Y is an adjacency matrix, and the decoder defines a (pseudo-)distribution p(\\hat s | Y, s) (Note the conditioning on s, which is missing in the paper). Both models can be factorized based on the arcs y_{ij}, so training and inference are both efficient. On standard benchmarks, the proposed method shows improvements when there are more unlabeled data available.\n\nThe method is similar to other works on autoencoder for semi-supervised learning: maximize p(\\hat s, Y | s) for labeled data and the marginalized p(\\hat s | s) for unlabeled data. Unsupervised learning improves the F1 scores by about 0.5 to 1.5 depending on the amount of data, which is decent for the SDP task.\n\nThe derivation is mostly sound, but there are some possible improvements to make it more rigorous:\n\n- The decoder is technically p(\\hat s | Y, s), with conditioning on s. So the factorization should be p(\\hat s, Y | s) = p(Y | s) p(\\hat s | Y, s). The prediction should be maximizing p(Y | s, \\hat s), which turns out to be equivalent to maximizing p(\\hat s, Y | s).\n\n- Page 3: h^head W h^dep should have a transpose on h_head.\n\n- Page 4: Not sure why GCN is mentioned here. My intuition for the reconstruction probability is that it measures whether the head x_k is helpful for predicting the next word in the language modeling task. The assumption here is that the head is useful for predicting the dependent words, but distracting for predicting non-dependent words.\n\n- The mention of LSTM on Page 4 caused me to think that the model is not arc-factored. However, since the LSTM only depends on s and the *gold* output sequence \\hat s_{1:i-1} = s_{1:i-1} (not a sample from the softmax), the model is arc-factored.\n\n- Equation 4 could be written as U(m_k^head \\odot m_{i-1}^pre), where \\odot is elementwise product.\n\n- The loss L(s) could just be a sum of two summations, one for labeled data and one for unlabeled data.\n\n- Appendix A is a bit unnecessary since the models are arc-factored. The third expression in the appendix should have \\prod_ij in the parenthesis.\n\nOther comments:\n\n- After reading the derivations a few times, I am convinced that the models are indeed arc-factored. However, this means that the model does not use CRF anywhere. (The CRF autoencoder paper uses CRF for the encoder model, while in this work the encoder is a directed graph.) It might be better to remove the mentions of CRF throughout the paper.\n\n- Is there a mechanism to prevent non-DAG graphs?\n\n- The work on semantic parsing (e.g., Poon & Domingos 09, etc.) are unrelated to SDP despite the name similarity.\n\n- One interesting experiment to run is to use all the labeled data + a large amount of external unlabeled data.\n\n- The paper briefly mentions some work on structured VAEs [https://www.aclweb.org/anthology/D16-1116/ | https://www.aclweb.org/anthology/P18-1070/ | https://arxiv.org/pdf/1807.09875.pdf]. The paper should draw a stronger connection to them. For instance, highlight that the arc-factored model makes it possible to avoid high-variance RL-based training."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1393",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper focuses on semi-supervised semantic dependency parsing, using the CRF-autoencoder for integrating unlabeled data to train the model in a semi-supervised style. The paper has obtained a certain improvement in the results, indicating the effectiveness of semi-supervised training on low resource labeled data tasks. \n\nMy comments are given as follows.\nWriting:\n1.1. The author claims CRF-autoencoders in the title, which is not introduced in the text, especially the encoder part, even without any manifestation, important details are ignored.\n\nModeling\n2.1. From the model description part of the paper, Encoder and Decoder seem to have nothing to do with each other and can be regarded as two separate models.\n2.2 For the output Y of the Encoder is enough to obtain a semantic directed acyclic graph through the search algorithm. It is not clear why the author should choose a decoder for word generation. More importantly, the decoder does not guarantee the generation in the input words, how to locate the position of the generated word in the original sentence, use copy-generator? The author needs clarification.\n2.3 .In 2.2 DECODER, dependency graph is into m sub-graphs w.r.t each word, in which each corresponding word plays as root. I understand $s_k$ is only regarded as head, namely, the out-degree of each word, wonder why not consider $s_k$ as dependent (any computational difficulty occurs?). Expect to see experiments or necessary explanations for this concern.\n \nEvaluation\n3.1. For the use of unlabeled data, the pre-trained language models may be a good choice too. The reported results show that the LM target is superior to autoencoder in natural language understanding. Thus I expect to see using the pre-trained LM instead of using CRF-autoencoder for exploiting unlabeled data. Furthermore, is it possible to conduct experiments to see if the supposed pre-trained LM can work together with the proposed CRF-autoencoder?\n3.2. From the comparison between the supervised method and the baseline, the improvement of the results is not obvious enough and thus requires significant test. \n3.3. D&M baseline is not well explained. The results that the paper reports lack of comparison with those from related work.\n\nSuggestions:\n4.1. I notice the Decoder generators adopts LSTM, why not BiLSTM? If so, then each word may sense both head -> dependent and dependent -> head.\n4.2. For both encoder and decoder, it is suggested to replace LSTM RNN by Transformer. The authors may present additional experiments with Transformer or explain why LSTM setting has been good enough.\n4.3. Figure 1 is not so intuitive, especially for (b), in which Illustration of the decoder does not show the procedure of sentence generation.\n"
        }
    ]
}