{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "(emergency review)\n\nThe authors propose function-specific embeddings that use a multidirectional representation learning method for word-level semantics. In specific, vocabularies can be divided into groups (S, V, O, +iO) with overlapping allowed, then the word vector representations are trained synchronously by SGNS-like manner. Empirical experiments show that the model outperforms task-specific comparison models with a reduced number of parameters by sharing.\n\nStrength:\n- A simple but effective model for learning (function-specific) word-level representations.\n- The paper is easy to follow with extensive literature reviews.\n- Experiments show performance improvements with less number of trainable parameters.\n\nWeakness:\n- Word-level representations are usually used as an input layer. It might be better to show more about the improvement of performance in terms of downstream tasks.\n- Word-level representations could be dynamically changed with contexts, so it seems that those function-specific word vectors could be also constructed through contextualization. I would like to see the results of \"contextualized\" word vector representations models such as ELMo or language model based embedding techniques.\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a neural network to learn the so-called function-specific work representations and demonstrated its advantage over alternatives. The motivation was clearly presented. The proposed method seems to be comparable to alternatives, according to the empirical results. The presentation of the proposed methodology seems to be at some rather high level and, as a consequence, I got the idea but the technical side is not totally clear to me. Personally, I am wondering if the empirical results are convincing enough. This paper is unfortunately beyond my expertise and the final decision has to be rely on other reviews.\n\nMinor point:\n- Page 4: \"another\" should be \"the other\"?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper proposed a method for training function-specific word vectors, in which each word is represented with three vectors each in a different category (Subject-Verb-Object). The evaluation is conducted on tasks where the directionality encoded in the vector representations is helpful as claimed in the paper. I have several questions and concerns and hopefully the authors could clarify those for me.\n\n\n(I) The distributional semantics was proposed in around 1956-1957 as a philosophical hypothesis of the meaning of languages, which is complementary to the previously prevalent denotational semantics. It has been applied for many years as a guideline for text analysis given the contextual information, and it indeed took off after Latent Semantic Analysis/Indexing was proposed. \n\nWe all understand that recent advances in learning vector representations of concepts including both images and languages heavily rely on the superficial contextual information where individual concepts exist. However, it is a relatively superficial application of the distributional hypothesis. I'd argue that the method proposed in this paper is also based on distributioned hypothesis as the functionality of words also depends on the context, so it would be great if the authors could clarify this point in the second paragraph.\n\n\n(II) There are simpler ways to enforce directionality into the learning paradigm. \n\nLearning matrix representations [1,2] rather than vector ones is a relatively simpler and cheap way of injecting directionality into the representations as matrix multiplication is not commutative. Therefore, \\mS \\times \\mV is indeed different from \\mV \\times \\mS which resolves the issues brought up by using vector representations easily. The paper argued that the tensor representations proposed by previous papers are too expensive to learn and store, then I'd have a try on the matrix representations with the exact same loss function as in skipgram as a suitable baseline.\n\nFrom another perspective, if the learnt/publicly available vector representations derived from Skipgram or CBOW are not able to capture the the directionality, again, an easy fix would be to use outer product between a given word vector (likely from word2vec/glove/fasttext) and its function-specific vector to form a matrix representation and then learn the function-specfic vectors using the same objective function as defined in SGNS. This idea can be derived from Tensor Product Representations (TPRs, [3]).\n\n\n(III) Some results from baseline methods and the proposed method are missing.\n\nThe paper argued that verbs are the most informative constituents in terms of making predictions in the chosen evaluation tasks, then there should be a baseline that only uses the vector representations of verbs from word2vec/glove/fasttext, and another baseline should be from an additive composition of word vectors. Also, there is a result of the proposed method with the network composition function on KS108.\n\n\n(IV) I really appreciate the diversity of the research topics submitted to ICLR, and I hope people who are in the CompLing area keep submitting their good work. However, I have to say that after reading this paper, I am not sure what precisely I have learnt from this paper in either CompLing or MachineLearning perspective. \n\n\n\n[1] Socher, Richard et al. “Semantic Compositionality through Recursive Matrix-Vector Spaces.” EMNLP-CoNLL (2012).\n[2] Mai, Florian, Lukas Galke, and Ansgar Scherp. \"CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model.\" ICLR (2019).\n[3] Smolensky, Paul. \"Tensor product variable binding and the representation of symbolic structures in connectionist systems.\" Artificial intelligence 46.1-2 (1990): 159-216.\n \n"
        }
    ]
}