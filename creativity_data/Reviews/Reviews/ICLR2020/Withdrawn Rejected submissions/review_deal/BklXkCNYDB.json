{
    "Decision": {
        "decision": "Reject",
        "comment": "While there was some interest in the ideas presented, this paper was on the borderline, and was ultimately not able to be accepted for publication at ICLR.\n\nReviewers raised concerns as to the novelty, generality, and practicality of the approach, which could have been better demonstrated via experiments.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method to train graph neural networks on dense hardware such as TPUs. The method is motivated by an observation that connections in graphs have locality in some datasets.  Experiments show significant improvements in training speed compared to single-GPU training.\n\nThe overall score of this paper is slightly positive. There is a certain demand to perform training on hardware targeted to dense computations. Even though the applications of the proposed method is limited to data with low-bandwidth, the paper shows there are real applications of the method. The effectiveness of the proposed method is well-supported by the experiments.\n\nMajor comments:\nComparisons with single-GPU training can be unfair. The method in Ma et al. (2018) is indeed not easy to scale many GPUs because their target is processing extremely large graphs in parallel. Since the experiments in the submitted paper use relatively small graphs that fit in a single GPU memory, it will not be so challenging to scale many GPUs. At least, it is recommended to compare the results with training on several GPUs using data-parallel execution implemented in TensorFlow (or any other suitable frameworks). If it is difficult, please provide more specific reasons why it is challenging to perform multi-GPU training."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "The authors propose a method to speed-up the time to validation accuracy for a particular class of graph neural networks:  Gated graph sequence neural networks (GGSNNs).\n\nThe paper is interesting in that it describes several operations and engineering considerations to speed up the processing of a GGSNN on TPUs. It is essentially a collection of engineering steps that improve the time to validation accuracy.\n\nWhile I'm not an expert in (G)NN acceleration on TPUs, I have experience with GNNs and approaches to accelerate CNNs in GPUs. My assessment is that the scope of this work is far too narrow. It is specific to GGSNNs which is a small family of GNNs not widely used. It is also specific to TPUs and lacks evaluations of the proposed approach on other type of hardware.\n\nIt is for these reasons that I think the paper is not appropriate for ICLR. The scope has to be broadened both in terms of the NN models and the hardware types. \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The authors present a framework to implement graph neural networks training\nefficiently ---an inherently sparse task--- using \"custom dense hardware\", here\nTensor Processing Units (TPUs V2). The key steps are: (1) reordering the labels\nto reduce the bandwidth of the adjacency matrix, (2) (Sometimes approximate)\nDecomposition using block matrices for efficient storage and computations, and\n(3) memory layout optimization. They evaluate their framework on the VarMisuse\ndataset and compare the performance against a GPU implementation running on\nNvidia Tesla V100. In the best configuration, they were able to reach 78% test\naccuracy in 13 minutes vs 19h for the baseline.\n\nThe paper is well structured and thorough. I was able to understand the\nchallenges and the solutions proposed, even without prior knowledge in the\narchitecture this work focuses on. I think it is sufficiently detailed to enable\nan independent implementation without referring to the authors source code.\n\nHowever, I feel that it might lack novelty. Indeed, as described in the Related\nWork section, each component of the pipeline is well known and used very\nfrequently in the HPC community. Sometimes, knowing how arrange common\nprimitives is very powerful, and looking at the results of the experimental\nsection, this is enough to improve performance by almost two orders of\nmagnitude. However, I think it might be more due to some intrinsic properties of\nthe dataset than the method itself. As made clear by the title of section 5.1,\nit is the data itself that has low bandwidth. If we consider a dataset that does\nnot satisfy this requirement, stage (1) has no effect, and it is impossible to\nperform the decomposition done in (2). In that situation, the contributions of\nthis paper would be nullified.  It would be perfectly reasonable to think that\nmost datasets have low bandwidth, but this not a claim that made the authors.\nThis work would be considerably more impactful if it measured the bandwidth of\nmore well recognized datasets. My small exposition to these problem does not\nallow me to make any meaningful suggestion.\n\nFinally, I am puzzled by the \"dense hardware\"/GPU distinction. From my\nexperience, GPU devices *are* designed for and extremely efficient at dense linear\nalgebra. Sparse operations are historically performed on CPUs. While they are\npossible on GPUs, they are usually much slower. For example CUSparse, the sparse\nmatrices library, part of NVidia CUDA toolkit, was only introduced in its\nversion 4. It's a clear indication that sparse operations are not a strength of\nGPUs. According to my experience writing GPU code, I feel that this approach\nwould actually perform extremely well on GPUs as it does on TPUs. I think it is\nthus important to compare this framework on GPUs too. Since these\noptimizations are not TPU specific and have not been applied in the GPU based\nGNN libraries referenced in this paper reinforce my concerns that they are\nproblem-specific.\n\nEven though the performance gains demonstrated are sizeable, the fact the\napproach does not seem TPU specific and is potentially problem specific makes me\nlean towards rejection.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}