{
    "Decision": {
        "decision": "Reject",
        "comment": "In this paper, the authors showed that for differentially private convex optimization, the utility guarantee of both DP-GD and  DP-SGD is determined by the expected curvature rather than the worst-case minimum curvature. Based on this motivation, the authors justified the advantage of gradient perturbation over other perturbation methods. This is a borderline paper, and has been discussed after author response. The main concerns of this paper include (1) the authors failed to show any loss function that can satisfy the expected curvature inequality; (2) the contribution of this paper is limited, since all the proofs in the paper are just small tweak of existing proofs; (3) this paper does not really improve any existing gradient perturbation based differentially private methods. Due to the above concerns, I have to recommend reject.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a quantity called expected curvature to analyze the convergence of gradient perturbation based methods that achieves differential privacy. Comparing to minimum curvature, which was used in previous convergence analyses, expected curvature better captures the properties of the optimization problem, and thus offers an explanation for the advantage of gradient perturbation based methods over objective perturbation and output perturbation.\nUsing expected curvature is a pretty interesting idea, and having more refined convergence bound is useful. I have the following questions.\n1. It seems to me that the convergence bound is similar to previous bound, except that \\mu is replaced by \\nu and one log(n) disappears. Could you explain more intuitively how hard the new analysis is? Is it similar to just replacing any \\mu by \\nu in the previous analysis? And why do they differ by log(n)?\n2. How are the experiments (in terms of setup and results) differ from those in Iyengar et al? It seems to me the paper is proposing a method for convergence analysis and the DP algorithms remain the same. I feel like in Iyengar et al, there is no clear difference between gradient perturbation and objective perturbation. Maybe I was wrong about that, but could you elaborate more?\n3. I agree that the expected curvature better captures the convergence of gradient-based DP methods. Yet I don’t see clearly how this can be used to show that they have more advantages than objective perturbation. Is it possible that the analyses of objective perturbation can also be improved (maybe with other techniques)? Since all we have are upper bounds, I feel like it is a bit early to conclude that it is less powerful. You mentioned that “That is because DP makes the worst-case assumption on query function and output/objective perturbation treat the whole learning algorithm as a single query to private dataset. ” I didn’t follow this part. I feel like DP is always making a worst-case assumption; even for gradient perturbation, you need to add noise to protect the worst case. Could you elaborate more on that?\n4. My understanding is that for different dataset the curvatures would be different. I think it might be interesting if you plot something similar to Figure 1 for other datasets and compare how they match with the training curve. Do you expect them to look very different on different dataset / optimization problem?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the problem of differentially private optimization in the (strongly) convex setting. The authors focus on the gradient perturbation methods, i.e., DP-GD and DP-SGD, and provide the utility guarantees of DP-GD and DP-SGD under the so called expected curvature assumption. However, it is very hard to verify the expected curvature assumption, and thus the theoretical results may be invalid. I summarize my main concerns as follows:\n\n1. All the theoretical results provided in this paper are based on the expected curvature assumption (Definition 3). However, it is unclear what kind of loss functions will satisfy this assumption. If the authors can prove that this assumption can hold for some specific loss functions, such as logistic loss or square loss, the contributions of the current paper will be much stronger.\n2. Again it is unclear how large $\\nu$ will be compared to $\\mu$, and thus the theoretical results can be useless.\n3. Since the authors use the approximation form of the gradient in equation (2), why the first inequality in equation (2) holds according to Definition 3?\n4. How do you get the average and minimum curvatures in Figure 1?\n5. I don’t think the argument in the last paragraph in section 3.1 is sound. Because for the restricted strongly convex function, we will ensure the loss curvatures stay in certain directions during the training process.\n6. The contribution of the current paper is very incremental. All the proofs are just replacing the strongly convex condition with the expected curvature condition.\n7. There are several gradient perturbation based DP algorithms [1,2] for solving high-dimensional problems are missing in the related work.\n\nReference:\n[1].Talwar, Kunal, Abhradeep Guha Thakurta, and Li Zhang. \"Nearly optimal private lasso.\" Advances in Neural Information Processing Systems. 2015.\n[2].Wang, Lingxiao, and Quanquan Gu. \"Differentially Private Iterative Gradient Hard Thresholding for Sparse Learning.\" 28th International Joint Conference on Artificial Intelligence. 2019."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Differential privacy (DP) can be achieved by perturbing the objective function, the output or the gradient. In this paper, the authors consider gradient perturbation and claim that it is more advantageous than other methods. To prove this claim, they present a novel utility analysis by taking the noise into account. The previous papers (like Bassily et. al) present utility guarantees in DP context, but their analysis follow the same steps used for non-noisy setting. In non-noisy setting, the analysis is based on strong convexity parameter \\mu which is the minimum curvature. However, in this study they present ”expected curvature” which is computed by considering the noise variance and based on averaging the curvatures along the number of iterations. The order of utility is given for both convex and strongly convex objectives and it has become smaller than the previous studies. Since other perturbation methods does not add noise at intermediate steps, expected curvature is the same with \\mu and the utility advantage is not valid.\n\nComments (Positive)\n\n- The paper is well-written and easy to follow. I didn’t see typos or mistakes (I didn’t check the last proof).\n\n- They claim that they are the first study showing the advantage of gradient perturbation theoretically (I haven’t seen such a study either).\n\n- Since they remove the dependency to minimum curvature \\mu, they present utility order for both convex and strongly convex objective for DP-GD and DP-SGD.\n\n- With the help of privacy noise, they obtain a better utility which is an interesting contribution.\n\nComments (Negative)\n\n- In the numerical experiments, number of iterations are taken as 20, 200 and 800. It might be checked for more iterations. The chosen privacy levels are tight enough (0.1 - 1).\n\n- The learning rate of DP-SGD is divided by 2 at the middle of training. The reason and whether it is applied to other SGD method is not clear.\n\n\nOverall, this type of utility analysis exists in the DP literature, but their novelty comes from the idea of averaging the curvature."
        }
    ]
}