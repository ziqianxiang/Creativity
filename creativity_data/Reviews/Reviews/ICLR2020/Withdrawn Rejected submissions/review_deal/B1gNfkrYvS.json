{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a new architecture that takes inspiration from capsule networks and CNNs. Specifically the authors propose a layer called the capsule layer which takes a rank 3 tensor as input and outputs another rank 3 tensor. \n\nThe authors also discuss routing procedures but ultimately there proposal does not have any routing procedure, and their architecture can be thought in a more straightforward manner as a generalization of a CNN. The fundamenta lconjecture in the paper is that \" the strong modeling ability of CapsNets come from this tensor to tensor mapping between adjacent capsule layers\" and the authors demonstrate this empirically on CIFAR and MNIST dataset.\n\nThe basic way in which the authors generalize CNNs is by using elementwise-multiplication and addition between 3D tensors to create an algebra (strictly speaking a ring, but nothing stops the authors from using elementwise division as well) over these elements. After this step the 3D tensors can be treated simply as scalars. In essence this ties the weights that can act on each 3D tensor and results in parameter reduction. \n\n\nThe strong point of the paper is the great empirical performance of the proposed neural network architecture on the CIFAR and MNIST datasets. Unfortunately, it seems to me that this is not nearly enough work to conclusively show the importance/utility of this idea. There are many other datasets/benchmarks such as Fashion-MNIST, CIFAR-100, and larger datasets such as ImageNet that are now considered essential for acceptance of a purely empirical contribution such as a new neural network architecture.  \n\nBecause of the above reasons I cannot recommend acceptance of the paper in its current state.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to improve the Capsule Networks by making three empirical modifications to them:\n1. Without using routing procedure from CapsNets\n2. Without using convolutional layers\n3. Using higher-rank tensors to save parameters\n\nWith these modifications, they achieve comparable performance with much fewer parameters. In the experiments, they also show some interesting insight on the initialization of CapsNets. Overall, this paper is well written, easy to understand. However, I have the following concerns:\n\n1. The P-CapsNet outperforms the original one. Which one of the three modification contributes the most to the improvement? A comprehensive ablation study should be done in this work.\n\n2. The P-CapsNets shows competitive without requiring the expensive routing procedure. However, the generalization ability is not the only way to evaluate capsule networks. It is not clear whether other properties of CapsNets are affected by the removal of the routing procedure.\n\n3. In P-CapsNets, the basic unit of input, output, and capsules are all rank-3 tensors. The Equation 4 is obtained by extending Equation 3 to the 3D case and incorporating the convolution operation. What is the relationship between this work can DeepCaps[1] where they apply 3D convolution based dynamic routing algorithm?\n\n[1] Rajasegaran, Jathushan, et al. \"DeepCaps: Going Deeper with Capsule Networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n\n4. In Section 7, the adversarial robustness of CapsNets is evaluated. The paper claims that P-CapsNets also suffer this issue, even more seriously than CNN models. Do the experiments with the Basic Iterative Method (BIM) show the same result? The experiment should be done as in Hinton et al. (2018).\n\n5. The universal adversary perturbation (Moosavi-Dezfooli et al. (2016)) is applied to evaluate the resistance to white-box attack. Why this work chooses this one instead of a popular white-box attack method (e.g., FGSM as in Hinton et al. (2018))?"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Authors argue that the routing mechanism is unnecessary in a capsnet and one can just simply aggregate. They scale the matrix product introduced in Hinton 2018 to rank 3 tensors to decrease the number of parameters. Furthermore, they eliminate the free-form initial convolution layers.\n\nThe motivational derivation on equation 3 is simply wrong. c_ij is not a scalar or a learnt parameter. It is a function of c_ik at previous iteration for all other k. equation 3 would have been correct for a scalar or a learnt parameter but not a function of other parameters that just multiplication with W will not bring into the equation. Removing c_ij all together reduces capsule networks into simple CNNs, because there is no notion of agreement (cluster finding, attention) anymore. It is just a CNN with more weight sharing now. Also arguing that since number of parameters involved is jut 7 percent it is not important is just nonsense. The importance of a parameter is not based on the quantity. Several routing iterations may indeed be unnecessary, but a grouping mechanism need to still validate if the votes agree or not or it will be simple pattern matching like normal CNNs. \n\nCapsNets where proposed as a network that have comparable classification accuracy with CNNs with the extra benefit of the transformation generalization. If the classification accuracy was the primary goal one would have just used ResNets or etc. There is complete ignorance to the main point of CapsNets (viewpoint generalization) throughout the paper. No experiments showing any viewpoint generalizability of the current proposed network. \n\nAs for Adversarial Robustness of EMCapsNet, they also have fewer parameters incompare to the baseline CNN. I would argue the white box rebustness exactly comes from the aggregation method (here essentially an xor function is replaced with addition). "
        }
    ]
}