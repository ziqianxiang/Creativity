{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The main idea of the paper can  be totally described in one line: perform adversarial\nperturbation of the query image rather than the 'k-shot' example\nimages. (Algorithm 3 of their paper.)\nThe paper first nicely motivates the need for robustness in the few-shot learning setup and why existing robust training methods are inadequate.  In the context of four existing meta-learning algorithms, they present evaluation of adversarial fine-tuning on robustness.  They conclude that adversarially perturbing only the input images used during fine-tuning is not effective as perturbing the query images used in the outer meta-learning loop.  Experiments all throughout are on two datasets Omniglot and mini-imagenet.\n\nStrengths:\n      - Systematically layout of  experiments on what kind of perturbations are most relevant for robustness\n      - One of the first papers to take on the problem of robustness in few-shot learning.\n      -  Clear writing.\n\nWeakness:\n      - The depth and extent of contribution falls short of the requirements of the main ICLR conference track.  The layout of the paper so that adversarial querying is brought out as a contribution of the paper was a bit surprising.   The natural 'porting' of adversarial training to meta-learning would include adversarially perturbing all the input instances: the ones used for fine-tuning and the ones used in the outer loop of a meta-learning algorithm.  The paper differentiates these, and calls the later  as query images.   Subsequently, it first decides to dwell on methods that do not perturb the inputs of the outer-loop.  I find that an artificial distinction.  \n\n     -  For a paper that is primarily about empirical comparison of different options,  more ablations across dataset, architectures, and training sizes are required.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThe authors of this paper propose a novel approach for training a robust few-shot model. Adversarial querying is a simple method by changing the query set of meta-learning to adversarial examples, however, it gets very promising results to improve the robustness of the resulting model. \n\n\nPaper strength:\n1.\tThey provide a thorough empirical analysis of the robustness of meta-learning. Specifically, they study the robustness of several variances of meta-learner against well-known strong adversarial attacks and they also study different defenses like adversarial training and transfer learning.\n2.\tCompared with adversarial training with heavy computation cost, the proposed approach is simple and efficient to improve the robustness of meta-learning against a strong attack. \n3.\tSufficient experiment results provide a lot of insights and illustrate the superiority of proposed adversarial querying.\n\nPaper weakness:\n\n1.\tThe organization of the paper needs to be adjusted since I am confused about the adversarial querying in table 3 and section 3.2 before I read section 4. Why not put ‘5-shot Mini-ImageNet-AQ’ in table 3?\n2.\tI would like to see more comparison results for black-box attacks [1,2] since it has shown to be as strong as white-box attacks (e.g. PGD).\n3.\tIt would be more interesting if the authors provide more theoretical analysis about the insights of replacing query set to its adversarial counterparts during meta-training. \n\n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization-based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017.\n\n[2] Li, Yandong, et al. \"NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.\" arXiv preprint arXiv:1905.00441 (2019).\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nSummary: This paper presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm (e.g. MAML). Extensive experimental results are reported to demonstrate the robustness of the proposed approach on benchmarks. \n\nStrong points:\n\n-S1. The problem of robust meta-learning is interesting. The proposed solution is well motivated and extensively evaluated with promise.\n\n-S2. The paper is clearly presented and easy to follow in general.  \n\nWeak points:\n\n-W1. The degree of novelty in methodology is limited. In my opinion, the proposed method is an adaptation of PGD to optimization-based meta-learning. Although the idea is somewhat interesting and it comes up with a thorough numerical study, the value-added beyond a direct combination of traditional adversarial learning techniques and meta-learning seems not sufficient enough to represent a significant contribution to the addressed topic. \n\n-W2. The method is (over) extensively evaluated but short of justification in principle. The core idea of the proposal is introducing adversarial attack in the query data for task-level training. However, it is not clear why introducing adversarial attack on the query data should work better than on the support data or both query and support data? The principle/intuition behind the idea needs to be more clearly explained in theory and/or experiment.  \n\n-W3. Numerical study is misleading in some places. The goal of this paper is to show the effectiveness of introducing the adversarial attack during query steps to enhance the robustness of few-shot learning. However, some reported experimental results are less relevant to this theme. For an instance, Table 6 shows that by replacing the loss, AQ-MAML can achieve better performance. These results seem not very interesting for the core method evaluation; they just demonstrate that changing the loss function can effectively improve the performance. The experiment reported in section 4.1 also gives the reader a similar impression that the results are weakly related to what the paper is really about. To make the contents in Section 4 more concentrated, I suggest moving some of the less relevant experiments presented in Section 4 to Section 3 as the motivation of adversarial meta-learning, or to the appendix. Overall, the main Section 4 should be more focused on the experimental/theoretical comparison against prior approaches, rather than exploring the effects of different model structures or loss functions.\n\nMinor issues: \n\n- M1. The concepts of ``AQ MAML’’ (at the end of Section 3.1) and ``robust meta-learning’’ (above Table 4) are not defined or referenced in the context. \n\n- M2. On the table citation in section 4.2, the Table 8 and Table 9 are wrongly cited as Table 4.2.\n"
        }
    ]
}