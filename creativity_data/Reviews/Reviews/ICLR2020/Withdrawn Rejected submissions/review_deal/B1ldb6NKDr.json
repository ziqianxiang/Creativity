{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents an approach combining multi-agent with hierarchical RL in a custom-made simulated humanoid robotics setting.\n\nAlthough it is an interesting premise and has a compelling motivation (multi-agent, real-world interaction, humanoid robotics), the reviewers had some trouble pinpointing what the significant contributions are. Partly this is due to lack of clarity in the presentation, such as with overlong sections (eg 5.2), unclear descriptions, mistakes in the text, etc. Reviewers also remarked that this paper might be trying to do too much, without performing the necessary experiments/comparisons and analyses needed to interpret the contributions of each component. \n\nThis work is definitely promising and has the potential to make a nice contribution, given some additional care (experiments, analyses) and rewriting/polishing. As it is, it’s probably a bit premature for publication at ICLR.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a multi-agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi-agent settings (e.g. avoid collisions, collaboration, chase and escape) in a physically simulated environment. The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots. This is the main reason of using the hierarchical RL. \n\nIn general, I like this paper. It is an important step towards multi-agent learning in complex physical environments. The results look appealing, too. However, I voted for \"Weak Reject\" for two reasons. First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel. The combination of these two methods seems straightforward. Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works. I do not understand the \"deep integration of MARL and HRL\" that is claimed in the Introduction. I also do not agree with another claim that \"We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2\". In most of the simulators that I am familiar with, such as Mujoco, Bullet, DART, it is straightforward to add multiple simulated robots.\n\nSecond, the writing can be greatly improved. Almost half of the technical details are buried in \"8. Supplementary material\". Since it is not fair to use \"Supplementary material\" as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7. In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting. I think that these are important details and may also be the contributions of this paper. Most of these should be moved to the main text. \n\nHere are some more suggestions on writing:\n1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2. \n2) It would be great if the paper can clearly define the experiments: \"waypoint\", \"oncoming\", \"mall\", and \"bottleneck\".\n3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,\npromiss->promise\nweek signal->weak signal\nmissing citation [?] in page 3\nreuse the same symbol v_{com} for agent's velocity and desired speed in eq(3)\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The submission proposes a method for hierarchical RL in multiagent settings. In particular it proposes to explicitly decouple training of a high-level and low-level controller with grounded the controller interface as goals in the environment to reach for the low-level controller. The model is trained via PPO with GAE and evaluated on a small set of multi agent locomotion tasks.\n\nThe paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case. Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG. To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission. \n\nA large part of making the MA system work well is based on reward shaping which nearly fills all of page 5. This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms. \n\nThe experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).\nRegarding the challenges (and focus on learning simple tasks), reference [3] might be of interest to the authors.\n\nMinor\n- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.\n- Self-referential sentences in the supplementary materials (i.e. referral to itself)\n- Missing references on page 3\n- The egocentric velocity field is not described (section 5)\n- Section 3.1: maximize\n- The wording new paradigm in MARL might be unsuited given existing work on complex domains.\n‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.\n- Text on experiment figures is much too small.\n\n[1] Andrew Levy, Robert Platt, and Kate Saenko. Learning Multi-Level Hierarchies with Hindsight. In International Conference on Learning Representations, 2019.\n\n[2] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient Hierarchical Reinforcement Learning. In Advances in Neural Information Processing Systems, pp. 3303–3313, 2018.\n\n[3] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning Tom Schaul, Diana Borsa, Joseph Modayil and Razvan Pascanu\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "Summary:\nThis paper looks at the MARL problem in high-dimensional continuous control settings. To improve learning in this multi-agent setting, they propose to pre-train a lower-level policy that takes as input foot-step goals and is executed for a fixed number of timestep, thereby simplifying both the learning and exploration. \n\nI'm a bit unsure of how to evaluate this paper. On the one hand, I believe it has several contributions:\n- Proposing a new MARL - continuous control environment\n- Proposing a new lower-level policy for high-demensional continuous control environments, including how to learn it\n- Using it to perform MARL in this environment\n\nOn the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:\nClearly, a main part of the paper is the work done to construct the hierarchical setup, including goal space, observation space and reward functions. However, this work, as far as I can tell, is separate from the MARL problem. Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.\n\nOn the other hand, there is the application of the hierarchical setup to the MARL problem. However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem. Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL. \nI believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph: \n- Why does temporal correlation reduce the non-stationarity of the MARL problem?\n- Why does structured exploration reduce the number of network parameters that need to be learned?\n- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?\n\n\nIn summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.\n\nEdit:\nThank you for your response.\n\nUnfortunately, I don't feel like it sufficiently addresses my questions and concerns. \nI do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions. \n\nRegarding my questions: I understand where the temporal correlation is coming from in an HRL setting. However, what was not clear to me is how this reduces the non-stationarity of MARL. \nI also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.\nAnd lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier. \nI feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help. \n\nI want to re-iterate that I think that the submitted work by the authors is impressive and can provide valuable insights, but I believe it requires more work and more relevant baselines.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}