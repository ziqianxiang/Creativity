{
    "Decision": {
        "decision": "Reject",
        "comment": "The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state-of-the-art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose a neural network approach to variable unification and\nreasoning by example as a way to mimic the human ability to identify invariant\npatterns in examples and then apply them more generally in practice.\nThis general idea of identifying invariates and mapping new instances to\nthem is well motivated by the authors, citing work in philosophy of mind,\ncognitive science, and developmental psychology.\n\nThe authors go on to propose MLP, CNN and Memory Network models\nof unification for sequence, grid, and story reasoning tasks respectively.\nExperiments on the sequence and grid datasets demonstrate the data efficiency\nof this approach. MLP and CNN models with unification achieve near perfect\nperformance in fewer iterations (an order of magnitude fewer in the MLP case!)\n than their non-unification enabled counter parts.\nUnification enabled models also demonstrate high performance in a reduced\ntraining set setting (using only 50 training examples).\nWhile this is encouraging, these are very simple toy tasks.\n\nI also am in doubt as to whether the representation of these problems\ncauses some issues. In the sequence task, one question the models are\ntrying to solve is what symbol is the head or tail of the sequence.\nModeling variables over the sequence of symbols here is, in a sense, the\nwrong object of study. The position of the symbols would need to be\nrepresented, e.g.\n\na b c d\n1 4 3 1\n\nwhere I've represented positions as a-d, and the learned invariant about\nhead questions would be:\n\nX:a b c d\nY:1 4 3 1\n\nAs is, by mapping symbols and not positions to variables, one cannot,\nat the variable level distinguish between the two 1s in the sequence above.\nMy guess is that in practice the bi-GRU model that produces embedding\nfeatures of the symbols in sequence is implicitly representing head/tail\npositioning.\n\nSimilar arguments could be made about the grid example.\n\nI don't find the experiments/analysis on the bAbI dataset very convincing.\nFor instance, in the example given in Figure 4b (reproduced below)\nis shown as an example of\ntemporal reasoning, where a symbol Z is mapped to the\nword morning (a symbol distinguishing a time), and the question asked is\nwhere was Bill before school.\nIf logical reasoning is being used to solve this question, surely the\nsymbol 'before' must also be represented as a variable. Its possible that\nthe model is instead learning a trick about mutual exclusivity, i.e. that\nY:school is the only location symbol not mentioned in question but this\ncould fail as a general strategy.\n\n\nthis Z:morning X:bill went to the Y:school\nyesterday X:bill journeyed to the A:park\nwhere was X:bill before the Y:school\nA:park\n\nFigure 4b\n\nIt would make for a much more interesting paper if the authors took\nexamples such as these and formed counter-factuals to probe the way\nthe models are answering the questions. E.g., transforming the question\nin 4b to \"where was X:bill today\" or \"where was X:bill after school.\"\n\nBecause the authors use soft unification, interpretability is difficult\nto assess. Interpretability is crucial here because to claim that unification and reasoning by logical induction\nis being used to solve tasks, it becomes important to show how the neural networks\nmake their decisions. Given the instances of extra variables and one to many\nmappings on the bAbI dataset it seems very likely that the models are not\nsolving many tasks\nusing unification as it would be possible to learn to use the symbols directly\nto learn to answer. As such, I think these issues are not addressed in the\npaper sufficiently to warrant acceptance.\n\n\nMinor Notes\n\n- In definition 1, the definition of Variable is a little confusing because there are two different senses of the word in use. I understand them to be (1) Variable (X) in the logical template that is intended to be learned and used in problem solving, and\n(2) variable (x) in the neural network model that is a soft asignment of\nthe Variable to a default symbol s. It would be nice if this distinction could\nbe noted or made clearer.\n\n- In the definition 2, in the phrase \"is the invariant example such as a tokenized story\" it might be worth stating that the tokens are the symbols in S.\n\n\n- My understanding is that each unique symbol in the invariate is a potential\nvariable. Does this mean there are no co-referent symbols in the invariate?\nWould be helpful to state whether babi contains co-referent expressions\nand how these might affect the model.\n\n\n- It might be interesting to see how model architecture affects variable\nlearning. For example, does a CNN result in more sensible variable\nassignments  than the mlp on a flattened representation of the grid problem?\n\n\n- What is the strongly supervised case? These are token level annotations I think (at least for babi) but it might be good to specify in more detail what\nthey  are.\n\n- The figure and explanation of the UMN are not very clear. From the figure\nis does not seem that the variables interact with the memory at all. More\nspace could be devoted to this section.\n\nPossibly Relevant Related Work\n\nBrenden Lake. Compositional generalization through metasequence-to-sequence learning. NeurIPS 2019.\n\n\n\n\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a novel approach for learning invariants that can capture underlying patterns in the tasks through Unification Networks. This effectively allows the machine to learn the notion of `variable`, which is a symbol that can take on different values. \n\nPros:\nThe authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification. \nThey further performed analysis on the learned invariants, and verified the sensibility. \nThe paper overall is well written and structured.\n\nCons:\nDespite its superiority over plain baseline, the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks.\n\nSome of the technical details regarding the choice of hyperparameters are missing. For example:\nIn section 6, whatâ€™s the rationale of setting $t$ differently for bAbI solely?\nIn Equation 5, how is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task? A bit more discussion on these choices would be helpful. \n\nOverall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other state-of-the-art methods. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper explores a very interesting idea: can a model learn what variables are and how to use them? Unfortunately, the paper doesn't seem quite ready: the model description was very hard to follow and it's not clear the approach has found a compelling use case.\n\nI read the paper carefully three times, and try as I might, I simply can't get my head around the entire architecture. The modeling section jumps straight into a series of definitions, without trying to build intuition or provide a worked example. There is an example in Figure 2, but it isn't really explained and I didn't find it helpful. Unification seems to be implemented as a form of attention (or self-attention) where the model can control the degree to which a symbol acts as variable. But the relationship between soft unification and attention isn't really spelled out -- what's the same, what's different? Ultimately it's not clear to me what the model is attending over during soft unification.\n\nThere are various other aspects of the paper that aren't clear:\n- strong vs. weak supervision\n- comparison models DMN and IMA are not introduced at all, and include no references\n- the logical reasoning experiment is not clearly described\n- there is only a cursory conclusion\n\nI am not sure the model has found a compelling use case. On bAbi with weak supervision, the model is worse than the comparison models. It only slightly beats out memory networks with strong supervision. For logical reasoning, it's not clear what it is compared against or if the comparison is fair. The clearest win over standard networks is on the simple synthetic experiments.\n\nFinally, the authors mention the paper has a cognitive science motivation, in that \"Humans learn what variables are and how to use then at a young age\" or that \"symbolic thought with variables is learned...\", taking a strong \"nurture\" stance on the origin of variables. But variables could very well be innate and simply early emerging. Any discussion of the origin of variables in the mind requires more nuance.\n\nI am excited about this research direction, and it could ultimately be a very nice contribution as the work matures. I don't think the paper is ready in its current form.\n"
        }
    ]
}