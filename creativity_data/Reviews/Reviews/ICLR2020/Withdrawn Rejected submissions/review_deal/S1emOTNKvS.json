{
    "Decision": {
        "decision": "Reject",
        "comment": "This submission proposes a graph sparsification mechanism that can be used when training GNNs.\n\nStrengths:\n-The paper is easy to follow.\n-The proposed method is sound and effective.\n\nWeaknesses:\n-The novelty is limited.\n\nGiven the limited novelty and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a trainable graph sparsification mechanism that can be used in conjunction with GNNs. This process is parameterized using a neural network and can be trained end-to-end (by using the Gumbel softmax reparameterization trick) with the loss given by the task at hand. Experimental results on node classification tasks are presented.\n\nThe paper is well written and easy to follow. I think that overall the method is sound and well executed. \n\nEmpirical results are convincing as the method consistently improves performance (compared to not applying the sparsification) over several GNNs. I feel that the claim of the improvement could be softened a bit. Please correct me if I'm reading Table 2 wrongly, with the exception of the Transaction dataset, most differences are below 3%. I am not familiar with these datasets so I cannot judge the significance of the improvement. In any case, the reduction in computation with the improved performance is a strong result.\n\nThe baselines that include unsupervised graph sparsification as a pre-processing make the results worse (with respect to not applying it) in all cases. This shows that for this problem, a task driven specification is crucial for maintaining performance. \n\nNeural Sparse model has more parameters than the version that does not use a sparsifier. Do you think that this could influence the performance? Would it be possible to compare using similar number of trainable parameters? I assume that using more parameters would not imply better performance for the baseline, but it would be good to clarify.\n\nI can understand that the performance of the model depends critically on k, and that k might vary significantly over datasets. In my view, it would be informative to include (maybe in the supplementary material) the performance variations on the corresponding validation sets as one changes k for the different datasets (as done in Figure 3 (c)).\n\nIn Algorithm 2 it would be better to use a different letter for the edge set (as currently looks like the real numbers)."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors argue that existing GCN-based approaches may pose non-trival overfitting risk during the training phase, especially when high-dimensional features and high-degree entities are observed in the graphs. To address the issue, the authors integrate graph sparsification with conventional graph neural nets. Experimental results show the efficacy of the proposed model in a series of benchmark datasets. In general, the paper is easy-to-follow and well-organized. My main concern is there lack some insightful discussion regarding the problem motivation and the proposed algorithm. In particular,\n(1) It is unclear why existing GCN-based approaches can not handle the cases shown in Fig. 1. Is there any evidence (either theoretical or empirical) or reference to support this argument? \n(2) The motivation example shown in Fig. 1 is confusing. Conventionally, graph sparsification aims to find smaller subgraphs from the input graphs that preserve the key structures. However, in Fig. 1 (b), the sparsified subgraph seems only downsampling the edges while preserving all the nodes as the original graph. The authors may want to clarify whether the sparisified subgraph has the identical size as the input graph.\n(3) Some notations are not formally defined before using them. In Eq. 2, what do Q_\\theta and Q_\\phi denote? \n(4) The statement of \"trade-off between model accuracy and graph complexity by tuning the hyperparameter k\" is vulnerable. If the overfitting exists, larger k may result in lower accuracy in the testing phase. \n(5) What is the complexity of f_\\Phi()?  \n(6) The complexity (i.e., f(km)) of the proposed model is problematic. As stated at the beginning of this paper, the paper targets the graph with \"complex local neighborhood\", where each node is described by rich features and neighbors. In other words, the target graph is not sparse. In this case,  the complexity of the proposed algorithm can be intractable, especially when k is large and m is close to n^2. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The authors propose a supervised graph sparsification technique that \"mitigates the overfitting risk by reducing the\ncomplexity of input graphs.\"\n\nThe idea is as follows: there is a sparsification network which samples subgraphs (adjacency matrices) by computing a probability distribution for each edge and drawing the existence of each edge from this distribution. The sparsified graph is then fed to a GNN that computes a classification loss. Since the authors use the Gumbel-softmax trick the method is end-to-end differentiable and the output consists of a node classifier and a graph generative model that can be used to sample sparsified graphs. \n\nThe paper covers an interesting topic and results in some good numbers on standard benchmark datasets. I also like the idea to use the Gumbel-softmax trick to make the entire model differentiable. \n\nUnfortunately, the authors miss to cite and discuss highly related work [1] (ICML 2019). In this work, the authors also maintain a graph generative model (also by parameterizing edges but with iid Bernoulli RVs), also sampling graphs from this generative model, and also using these sampled graphs to train a GNN for semi-supervised node classification. The resulting model is also end-to-end differentiable. Instead of using a Gumbel-softmax to keep the method differentiable (given that we have discrete random variables) the authors propose a novel way to compute gradients for the parameters of the Bernoulli RVs by posing the problem as a bilevel optimization problem. In [1] the graph cannot only be sparsified but also enriched with edges that might be beneficial for the classification accuracy. Indeed, it was shown that adding edges is more beneficial than removing edges. The results on Cora and Citeseer are better than the results reported by the authors in this submission. At the very least, the authors should familiarize themselves, discuss, and compare empirically to [1]. \n\nThe existence of this previous work also reduces the novelty of the proposed approach. However, if a  comparison to [1] would be added, the submission could be seen as an alternative instance of the framework presented in [1]. I would be willing to increase my score to accept, if the authors provide such a comparison in an updated version. \n\n\n[1] https://arxiv.org/abs/1903.11960\n\n\n-----\n\nI've read the rebuttal and while I don't agree necessarily with all statements made by the authors, I am happy to increase my score based on the discussion of previous work. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}