{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The paper proposes the idea of having a two-stage time selection and classification pipeline. For the first one they use a light network followed by a gating network (which learns N anchor kernels - or concepts) to choose the most representative time steps. For the second one, they use a heavy network such as I3D to classify the entire video based on the most representative time steps.\nWhile the idea is clear, simple and intuitive, it is not novel enough to be considered for publication in ICLR. This work is mostly a combination of different networks based on the idea of frame pruning, than a concrete network architecture that addresses the issue fundamentally (e.g. a temporal attentive video classification network).\nMoreover, based on Figure 1, the entire frames are fed to the heavynet and then the features are pruned. This adds to the importance of having a single concrete network that addresses the issue more fundamentally.\nAlso, the experiments are not enough to support the actual benefit of the work. This work should be directly compared to other works addressing the same problem (e.g. VideoBert), not just comparing it with its own baseline (I3D+Ours vs. I3D).\nThe paper also mentions the idea of concept kernel but it does not provide a detailed explanation on the concepts and/or any visualization."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents a frame-selection method for activity recognition in videos. By using a small CNN to process all frames to select a subset, this can greatly improve runtime performance. The proposed approach is straightforward, combining two CNNs: one to select frames and one to classify them. Overall, the paper is well organized (though missing some details) and the results show some initial promise. However, due to some major issues with the experimental results (details below), my initial rating is to reject.\n\nQuestions:\n\n1) How is the input to the HeavyNet formed? Mostly for the 3D networks, is it a concatenation of all the frames where a > 0.5? It seems that it could be difficult for 3D networks to learn good representations when the inputs are not temporally evenly distributed. Did you notice any negative or strange effects of using these inputs to a 3D CNN?\n\n2) Related - how evenly distributed are the selected frames?\n\n3) \"Latent concepts\" - how are these implemented? It is a bit unclear in the current paper. It seems like they are a set of vectors that are learned. It would be helpful to clarify k_i in the paper.\n\n4) Figure 6, right - Is the y-axis also the percent of selected frames or is it accuracy?\n\n5) Section 4.4 \"We observe a drop in performance when using this variant of the Timestep Selector.\" Which timestampy selector is leading to a drop in performance? Is it the frame (blue) or context (orange)? If the context, could this be due to  selecting fewer frames?\n\n6) Section 4.4, \"We conclude that the cost of selecting timestep using LightNet is marginal to that of the HeavyNet and classifier.\" This conclusion is unclear and seems different from what this section is about. Which selector is better/used in the paper? Or does it not matter?\n\n\nExperimental Weaknesses:\n\na) There are claims in the paper: \"report state-of-the-art results on two datasets for long-range action recognition: Charades and Breakfast Actions\" However, there is no comparison to any other work, of which there are many. Section 4.6, which has an experiment on Charades, only provides a graph comparing two models. Of these two, the top performance is only around 26. \n\nThe graph also has the y-axis labeled as % accuracy, but the standard evaluation metric on Charades is mean-average precision as it is a multi-label dataset. Are the reported numbers accuracy (if so, how is accuracy computed for the multi-label setting?) or is it mAP? If it is mAP, the state-of-the-art is significantly higher than 26 (many papers in the 30s, 40s and even 50s now). Thus the state-of-the-art claim is invalid for Charades.\n\nFor these two datasets, there should be a table comparing this papers results to previously reported results in order to claim state-of-the-art. Without that, the claim is unsupported.\n\nb) The proposed \"latent concept kernels\" are not experimentally evaluated at all. Does removing them hurt/help? How many are selected? What is the effect of using more/fewer kernels?\n\nc) This work is quite similar to \"End-to-end learning of action detection from frame glimpses in videos.\" Though that work has a possible advantage of not needed to process every frame, even with a light network. It dynamically selects frames by temporally jumping around the video. It would be good to compare the proposed approach to that previous method.\n\n\n\nOther feedback:\n\nFor Figure 4, it would be helpful to add a baseline using all timesteps for the HeavyNet. This would show the gap between the expensive, full network and the performance using various selected frames and give an expected upper bound to the performance.\n\nFigure 7 could be improved to better show the speed performance gains of the approach. The x-axis is not to scale, making it hard to visually see the difference between the approaches. "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This submission proposes an efficient method to do video classification for long-range actions. The authors propose a two-stage method, first selecting salient clips from a long video and then performing inference only on the selected salient clips to achieve good trade-off between accuracy and speed. The framework can be end-to-end trained. \n\nI give an initial rating of reject because (1) paper is not well written, many details missing (2) dataset are not representative (3) performance is not good, and the authors do not compare with state-of-the-art. My detailed comments are as below.\n\n1. The idea of the paper is to select salient clips for efficient video classification. It should be a follow-up work of SCsampler, as stated by authors as well. This submission moves one step further to make it end-to-end, which is straightforward. The contribution is limited.\n\n2. Paper is poorly written, many details are missing. For example, \n    - What are the concept kernels? \n    - How do they learned? \n    - What is the network architecture being used to learn them? \n    - What is K and N in the experiments? \n    - What does the learned concept look like (some visualizations)? \n    - Any ablation studies on with or without concept kernels? \n    - How do you train the network? like learning rate, epochs, etc.\n    - How do you evaluate the model? like input clip length, fusion strategy, etc.\n    - How is the comparison between gating and context gating module? \n\n3. The datasets used in this submission are not representative. The breakfast action dataset is small and outdated. There are  few literature working on that dataset. Charades dataset is ok, but charades is a mutli-labeled video dataset, which is not appropriate to showcase clip selection. I think using a large-scale popular dataset is a must, e.g., sports-1m, kinetics400, HACS dataset, etc. \n\n4. The performance of the proposed technique is not promising. For example, the state-of-the-art method on Charades is already 55+ mAP. However, this submission only achieves 20+ mAP.  I understand this is not a fair comparison. But there are so many state-of-the-methods, such as I3D (CVPR 2017), non-local (CVPR 2018), R(2+1)D (CVPR 2018) and slowfast (ICCV 2019), the authors need at least compare to one of them. Otherwise, your claims are not well supported. \n\n5. SCsampler use a clip of motion and residual images as input to train the sampler, while this submission only use a single RGB frame. It would be better to make a comparison. And this maybe the reason why the performance is not promising. "
        }
    ]
}