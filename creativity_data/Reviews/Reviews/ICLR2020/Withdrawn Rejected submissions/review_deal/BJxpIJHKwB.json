{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new model for few-shot classification. Their meta-learning approach falls into the category of weight generation models that, given a new classification task, generate the weights of a linear layer on top of learned embeddings that can be used for classifying query examples of that task. The weights of their model are meta-learned via a sequence of training tasks.\nMore concretely, in each training task, the support and query examples are first embedded using into a d-dimensional space. Then, the support set together with a given query example are processed to form a hidden vector, via two “paths”. This hidden vector, is fed into a weight generator which outputs the linear weights that will be used for classifying the query example. Both processing paths contain a projection layer, followed by self-attention on the support set, and the second path additionally performs cross-attention for the query example on the processed support set. They observed empirically that when training this system simply using the cross-entropy query loss as usual (with the loss computed using the generated weights for each query) did not lead to significant gains. They therefore added terms in their loss function to maximize the mutual information between the generated weights and the support and query sets. The resulting objective is comprised by terms that maximize the cross-entropy on the support and the query sets and reconstruction terms for the support and query images. \n\nPros\n[+] Interesting new objective for training a few-shot classification model\n[+] Thorough ablation analysis\n[+] Seems to perform well on standard benchmarks\n\nCons\n[-] Lacking sufficient details on LEO. Specifically, the paper largely emphasizes the comparison of their method with LEO. For example, certain of their ablations are described based on their relationship to LEO. Given this, it would be useful to give more background on that method. While it is described in related work, I found that description too high-level to understand LEO’s training objective, its mechanism for conditioning on the support set, etc.\n[-] Some inconsistencies. For example, the last paragraph of section 3.3 suggests that LEO is similar to using only the first two terms of Equation 13 (ie. lambda_2 = lambda_3 = 0). However, later in the paper, they say that the ablation called “Generator conditioned on S with IM” (which includes the cross-entropy on the support set and reconstruction of support set terms only) is “equivalent to LEO to some extent”. This, however, entails using the second and third terms of Equation 13, seemingly contradicting the previous claim of which version of their method corresponds to LEO. It would be good to clear up this confusion.\n[-] I found the names of the ablations hard to understand. For example, “Generator conditioned on S only” is the same as “Generator conditioned on S only with IM” except that the reconstruction term for the support set is omitted? Or is the full objective used, but with the weights w having been generated using only the contextual path? It seems that which path(s) are used to generate the weights is orthogonal to which terms of the loss function are used for optimization. So it would be really useful to clarify exactly what each of the ablations is doing.\n\nAdditional comments / questions:\n- Is the embedding network pre-trained as in LEO (and held fixed during the meta-training?), or are all the weights meta-learned end-to-end?\n- Do the rest of the models that are being compared against also re-train on the union of the meta-training and meta-validation sets? This procedure does not seem standard and I’m wondering if the comparison is fair. \n- A recent related work: “Cross attention networks for few-shot classification. Hou et al. NeurIPS 2019.” Their method also modifies the feature extraction of the support set differently for classifying each given query example, motivated by the need to possibly highlight different regions of the support examples depending on the given query. They also use attention to accomplish this. Given these similarities, it would be useful to compare to them.\n- This paper largely emphasizes comparisons with LEO. Along those lines, can a version of the proposed approach also tackle few-shot regression as LEO does? \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper suggests a method for generating classifier weights for few-shot learning. A multi head self attention module is used on both the support-set and the union of the support-set and a specific query to extract features for the generator. A loss term is added ensuring the support set and the query can be reconstructed from the generated weights (information maximisation).\n\nMajor points:\n1. Missing details and/or reference for the multi-head attention. Even together with the appendix it’s not self contained.\n2. I suspect that the learned model is not really adapting to the specific query as claimed, for the following reasons:\n    a. The “random shuffle in class” experiment (given the typical accuracy STDs) suggests that the generated weights are not depended on the query.\n    b. lambda3 is 3-order-of-magnitude lower than other loss terms.\n    c. Also, according to the ablation study (lambda3=0) its contribution seems to be marginal at best.\n3.  MetaOptNet is compared to only for tiered-imagenet where it is outperformed by AWGIM, but is not compared to for mini-imagenet where MetaOptNet achieves better performance.\n4. Why the attentive path alone is not enough? It takes into account both the context (support-set) and the query.\n5. It is not clear why X^cp and x^ap (The support-set and query after the attention modules) are reconstructed and not the original X and x_hat."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This submission proposes AWGIM, a few-shot learning method derived from a mutual information perspective. An architecture is proposed that, conditioned on a support set and a query example, produces a set of weights that should make accurate predictions for the query. Additional terms to the learning objective are added, derived from an information maximization perspective. These terms correspond to reconstruction terms of the (features of) the support and query inputs, as well as a prediction of the support set labels. Results on miniImageNet and tieredImageNet are presented, with comparisons to prior work and an ablation study.\n\nThough the results of AWGIM are pretty good, I have several concerns about this submission:\n\n1. The results on miniImageNet are actually worse than those of MetaOptNet (64.09% and 80.00% for 1 and 5 shot, results which somehow are missing from Table 1). So the claim of state-of-the-art performance is inaccurate.\n\n2. I find the motivation of the method to be lacking. The authors claim that \"estimating the exact and universal classification weights from very few labeled data in the support set is difficult and sometimes impossible\". First, not much more is provided to support this statement. Second, I would instead argue that producing a set of weights *per query* is even more difficult. Indeed, having different weights for different samples suggests more capacity than having a single set of weights, which would suggests an even more (not less) difficult overfitting situation to address. \n\n3. The mathematical foundation of the method is equally unconvincing. At a high level, the authors could better motivate why the mutual information perspective is the right way of approaching the problem. In particular, the end result is simply the addition of reconstruction terms of inputs from the support and query sets and the label of the support set. Moreover, these terms are weighted by hyper-parameters, making the method even less directly related to the original information maximisation objective. Overall, I find this narrative to add unnecessary (I'm tempted to write, inflated?) complexity to the contribution.\n\n4. The writing is not great, and I've find several typos, which a more thorough proofreading could have caught (I mention some I found at the end of this review, but I stopped taking such notes after section 3.2.1).\n\nThis paper is not without value. Putting aside the motivation which I personally find lacking, the final learning objective is interesting, and seeing that the ablation study find that the reconstruction terms are useful is a surprising and interesting result. Assuming a complete rewrite of the paper, it's possible that this work would become much more convincing. On the other hand, I do have some potential concerns about the experiments, mainly that 1) the confidence intervals of Tables 1 and 2 are suspiciously small for AWGIM (an explanation for why that is would be valuable) and 2) I'm wondering whether hyper-parameters were tuned separately for each models in the ablation study (given that there are many variants compared, I doubt they were, though that would be a fairer ablation).\n\nBecause I think a thorough rewrite would be required for this submission to be strong enough, I unfortunately doubt that I'll be changing my score (though I'll obviously read and consider any rebuttal from the authors).\n\nFinally, here are some minor comments:\n\n- \"data receives\" => data has received\n- \"Most of successful\" => Most successful\n- \"as weights generation method\" => as a weights generation method\n- \"need inner update\" => \"need inner updates\n- \"casted to a\" => cast as a\n- \"refer attention to\" => refer to attention as\n- \"of stochastic process\" => of a stochastic process\n- \"achieved be attention\" => rephrase\n- \"product of marginal distribution\" => product of marginal distributions\n- \"that we need to predict\" => and we need to predict\n- \"is show in\" => is shown in\n- \"support set to is\" => rephrase\n"
        }
    ]
}