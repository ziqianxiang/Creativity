{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper focuses on analyzing and visualizing trained QA neural models. The main idea is to quantize latent representations during training, making the model more interpretable eventually.\n\nThe general idea is understandable, but the details are very unclear and I found it extremely difficult to follow.\nAs far as I understand, one of the layers (in which kind of a network?) is replaced with a quantizing layer. This quantized layer is different than the original layer only in its forward computation; during backprop - the gradients from the next layer are copied backwards. The network is then trained (this is not said explicitly, until this point I thought that the paper discusses analyzing already-trained models) using this new layer, with 3 losses: QA loss, VQ loss and commitment loss. These losses are not discussed nor defined further. Finally, the quantized vectors are clustered using K-means.\nOverall, I think that this paper should be rejected. It is very unclear, the proposed technique is not compared to any existing technique, and the interpretation of the network - which is the main goal of the paper - does not produce any really useful insights.\n\nFor another example of vagueness, the paper refers to a figure by \"Each color represents a codeword\" (Page 2, para2). What is a codeword? What exactly do the authors refer to here? How did they choose the number of colors and how did they color each word?\nAnother example - \"a neural network can use only 20 or so vectors to cover almost all information\" (page 2, para3). Which vectors? The reader doesn't even know which model architecture is discussed at this point.\n\nThe experiments are based on extracting the quantized vectors from the trained model and running K-means with different values of K. What are the metrics used here? Table 1 shows F1 score, but I don't understand how precision/recall are computed? Are these the original metrics used in the benchmarks? what is exactly the experiment here? Table 2 is not referenced nor explained.\n\nI did not understand Section 5.2 - \"Meaning of Nonsense words\" at all. What are \"nonsense words\"? How they are chosen? Please define these more accurately. Since I didn't understand the settings, I did not understand the results in Table 3 (metrics/units are not mentioned there as well). \nI did not understand Section 5.3 as well (\"Position and Portion of Nonsense words\"). If I understand correctly, the main point here is that \"the higher a codeword’s percentage, the less meaning it preserves\", but I don't understand how is this observed from Figure 3, and how does the paper measure \"preserving of meaning\".\n\nRelated work: \nThe paper misses a discussion and comparison with recent network-interpretation approaches, for example [1][2][3] below.\nAdditionally, some of the existing discussion is very lacking. For example - Doshi-Velez (2017) and Montavon (2018) - I have not read these papers, but why \"only those deep learning researchers are able to understand and utilize the results\"? This is a vague remark regarding these papers. Didn't these papers show do how they interpret their results? Did they release their code? Are these papers too complicated mathematically? \n\nMore questions to authors:\n1. What are codewords and codebooks? These are not defined nor explained.\n2. The main idea in the paper is quantizing some vectors in order to interpret them better. But how interpreting discrete vectors is better than interpreting continuous vectors in this case? If the main applied technique is K-means clustering, why can't we just cluster continuous vectors?\n\nMinor:\n* I personally don't like the use of phrases that humanize machines like \"the machine thinks\" and \"Models should know\". Treating neural models with human characteristics is inaccurate and misleading.\n\n[1] \"What Does BERT Look At? An Analysis of BERT’s Attention\", Clark et al, 2019\n[2] \"Are Sixteen Heads Really Better than One?\" Michel et al, 2019\n[3] \"How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations\" Aken et al, 2019"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to add an extra vector quantized layer (VQ layer) to the existing QA models to help with the explainability. To be specific, for a given layer that needs to analyze, they map each continuous vector in the layer to a quantized vector through a codebook. These quantized vectors are further fed to more layers leading to answer prediction. After training converges, those quantized vectors can be used to probe the model’s behavior. The experiments are conducted with two QA models, namely, FlowQA and SDNet on two different datasets QuAC and CoQA. The results demonstrate: 1) With only a small codebook size, the QA models can still maintain good performance. 2) The explainability can be achieved by calculating the statistics of the quantized vector. One interesting thing they found is that less frequent quantized vectors often denote more important tokens, which are responsible for making the boundary of answer spans.    \n\n\nStrength:\n1. The idea of using quantized vectors to explain the QA models’ behavior is novel. \n2. The experiments are well-designed and clearly written.\n3. The results show the effectiveness of the proposed method.\n\n\nWeakness:  \n1.  This paper tries to unveil the ‘’reasoning process’’ using the proposed vector quantized layer. However, the ‘’reasoning process’’ here is not well-defined. The colored quantized vectors seem like another version of attention heat map to me — it just shows which part of the context is important regarding the given question, though the authors do find some interesting properties of those quantized vectors. Those vectors do not actually explain the exact ‘’reasoning process’’ of a QA model. \n\n2.  The authors try to establish that the colored quantized vectors are useful to explain the models’ behavior. However, there are no experiments showing how good those explanations are compared to other methods. I would like to see a comparison between the proposed method with other methods, e.g. gradient-based methods, attention-based methods (Sarthak and Wallace 2019), and how those explanations agree with human judgments. \n\n\n\nJain, Sarthak, and Byron C. Wallace. \"Attention is not Explanation.\" Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a way to understand a neural Question Answering model by first quantizing a layer of latent representations into a small number of codebook vectors, and then visualizing the frequency of such vectors being active as each question is sent into the model for an answer.  The vector quantization procedure uses k-means clustering with random initialization.  In experiments with two standard QA models (FlowQA and SDNet) with various quantization parameters, it is found that only 16 (or less) codebook vectors are needed to capture essentially all the information needed for answering (i.e., with negligible degradation in accuracy).  Additional observations are made on the forward-shifting of codeword activation as the question section progresses, and on the masking or activation of different codewords for non-semantic information like POS tags or starting and ending positions.\n\nWhile the work sets out to produce some intuitive understanding of neural QA models,  what gets achieved seems to be more like an illustration of the effects of some potential simplification.   It is hard to argue that seeing different words being activated for the relevant questions is a form of understanding; at least, such understanding is still very preliminary.  For example, it does not explain why the various other layers are needed,  how training may have affected the weights and attention weights, and how the words in the input questions trigger different words for the answer.\n\nGiven the way the context passages and the questions are arranged, the correlation (e.g. positions and question turns) between the questions and the words needed for answering them is rather obvious, and is hardly surprising.   Also, the questions are highly tailored to the passages, and the passages do not contain much of confusing or redundant text to challenge the models' capabilities.   Making too much of a claim that this produces understanding seems to be unjustified.\n\nIt could be that a better way to position this work is to go after the compression effect that the vector quantization process can offer, and to pursue a more systematic algorithm to generate compact and efficient representation of the models instead.\n\nThe paper is targeted to only readers who are familiar with neural QA models as it assumes much knowledge of such from the readers.\n\nQuestions:\n\nP.2, what exactly is the QA model being analyzed in Figure 1?   Is this your own or some public model you downloaded from somewhere?   What do you define as a context?   Such details should be given upfront, even  as a forward reference to the later sections of hte paper.\n\nP.7, colorful segments that may contain answers shift forward as time goes on.  Is this more a property of the way the used text is written, or the way the questions are ordered?  If you shuffle the questions and ask some later ones first,  will you break this trend?"
        }
    ]
}