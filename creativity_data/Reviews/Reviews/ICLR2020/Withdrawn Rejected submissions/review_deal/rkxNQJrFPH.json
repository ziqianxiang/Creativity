{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new approach to zero-shot learning using natural language descriptions of class. In particular, the paper introduces a method, N^3, that regress model parameters from textual descriptions of a seen class. The regressed parameters are combined with parameters of a  pre-trained classifier and fine-tuned on seen classes. The text description is first embedded using BERT method and then used as an input to a novel module, called parameter adaptation module, PEM. The proposed method shows good classification improvement on zero-shot class of four major datasets. \n\n+ves:\n+ The idea of providing a newer approach to zero-shot classification using natural language descriptions is interesting. Such approaches have been used in generative models (GANs, for e.g.), but it is novel in the zero-shot setting.\n+ The introduction of the PEM module seems interesting.\n+ The paper is well-written, and the results show good promise.\n\nConcerns:\n- The primary concern is that the paper does not evaluate the proposed method against state-of-the-art zero-shot classification methods (e.g. CADA-VAE: Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders, CVPR 2019), but only against other methods that use natural language descriptions (or adaptations of related methods). This seems limiting in the usefulness of this work (and the key reason for my decision on this paper). It would have been nice to at least see results of what happens if the proposed method is induced into an existing SOTA method, and if its performance improves.\n\n- It is not clear why we need an additional parameter adaptation module (PAM) module. It seems that a combined version of Semantic Encoding Module (SEM) and the PAM module is possible to regress parameters.\n\n- Table 3 shows different values of \\Gamma and \\Mu. From that study, the dominance of the pre-trained network, i.e. \\Gamma = 0.99, is evident. When we ignore the pre-trained network, i.e. \\Gamma = 0.01, the accuracy is bad. So, are the PAM model parameters actually contributing much? What if we use some other regularization on weight space of the pre-trained network and ignore the PAM module? (by doing so, we can mitigate the extra overhead of training PAM network and then fine-tuning the pre-trained network). Please mention the end values of \\Mu.\n\n- Can we replace the pre-trained model parameters with the generated PAM model parameters? What if we do not use the pre-trained model parameters and use only the regressed/generated parameters from PAM to classify and make an end-to-end network? (In this case, the PAM generated parameter space must match the pre-trained network parameters space). In this way, we can understand the exact contribution of the PAM module.\n\n- Tables 3 and 4:  The Accuracy @ 1 on which dataset? Table 4: using BERT, the Accuracy @ 1 is 17.6%; however, in Table 3 we can see the Accuracy @ 1 is 18.5% (almost 1% improvement). Are we using the same BERT model, \\Gamma = 0.99 and initial \\Mu = 0.01 for these tables?\n\n- Please clarify in the paper if the term “Task description”  actually implies “Class description”, and not referring to the “Task” definition given in work such as Taskonomy, Zamir et al., CVPR 2018. It would be nice to associate task as a more general concept than just a class. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper approaches zero-shot learning (ZSL) by directly generating neural network weights from natural language descriptions. The main motivation is to resolve the limitation of existing methods which rely on meta-data to get feature embeddings for text descriptions.  To this end, it employs a semantic encoding module to obtain text feature embeddings directly from descriptions by incorporating BERT and Transformer. From the text feature embeddings, it generates delta weights for every deep neural network layers, which is different from existing approaches that generate weights only for the last classification layer.  The delta weights are added to the pretrained weights in the base model (i.e., ResNet101) to adapt it for zero-shot classification. Experimental evaluations on four text-based zero-shot learning datasets demonstrates its superior to previous SOTA methods.\n\nPros\n\nThe paper is clearly written overall with helpful schematic illustrations\n\nGenerating the weights for the whole classification neural network rather than only the last classification layer is interesting.\n\nThe semantic encoding module is technically sound. Leveraging the powerful BERT model for class embedding is  better than TFIDF feature as used in previous work.\n\n\nCons\n\n1) For the parameter adaptation module, the approach to finetuning the base model guided by class embedding seems interesting but the motivation is lacking. The delta weights are produced and added to the weights of the base model. A more straightforward approach is to directly update the weights of the base model based on class embedding as some previous works did in Ref. [1, 2].  The motivations of choosing this way of updating weights should be clearly provided and the performance comparisons of these two approaches are expected.\n\n\n2) The performance comparison with MEGAZSL is unfair. In the proposed model, the feature extracted network (i.e. ResNet101) is finetuned with zero-shot classification objective while MEGAZSL uses the pretrained networked without finetuning. I believe that  finetuning plasy a very significant role since it pushes the model to distinguish the particular classes in each dataset, specially for the fine-grained datasets, such as CUB, NAB, and Flowers, in which subtle differences among classes can hardly be captured without finetuning. For a more fair comparison,  the base model should be first fine-tuned with a conventional classification objective on the training set, and the features are extracted by the fine-tuned model for zero-shot learning.\n\n3) Missing training details: It is unclear which are the learnable variables of the proposed model, and how they are learned -- which parameters need load weights from pretrained models and which do not; the information about the learning rate, optimizer, training epochs etc. are also missing.\n\n4) The compared methods seem to be dated. More comparisons with recent ZSL methods will strengthen the experiments.\n\n\n[1]  Morgado et al. Semantically consistent regularization for zero-shot recognition. In CVPR, 2017\n\n[2] Li et al. Discriminative learning of latent features for zero-shot recognition. CVPR 2018\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary: \nThis paper proposes a meta-model called Neural Networks from Natural language (N^3) for zero-shot learning. This model maps a set of class object descriptions to neural network parameters to create zero-shot classifiers, which contains two modules: (1) SEM encodes class descriptions into corresponding embeddings; (2) PAM fine-tunes the generated parameters in the SEM. The authors conduct experiments on 4 popular benchmark datasets and achieve best results compared with baselines.\n\nStrengths:\n(1) This paper proposes a good Parameter Adaptation Module (PAM) to generate parameter adaptations to fine-tune all layers in the pretrained network based on the class embeddings, to classify unseen classes.\n(2) This paper achieves good results compared with baselines.\nWeaknesses:\n(1) I am not sure the effectiveness of the N^3 according to the results in Table 3, 4, because the BERT module may play a major role.\n(2) Although PDCNN (2015) and GAZSL (2017) are very strong baselines, this paper does not compare the work published in 2018 or 2019.\n(3) There are many grammatical mistakes in this paper.\n\nI have some following major concerns about the paper:\n(1) This paper conducts extensive experiments to demonstrate the performance of N^3 in Table 2, but why not compare N^3 with the work published in 2018 or 2019? \n(2) In Table 3, this paper studies what extent does the superior performance of N^3 depend on \\gamma and \\mu, but why not show the results with gamma =1, mu=0 to demonstrate the effectiveness of PAM or fine-tuning?\n(3) In Table 4, I can come to a conclusion that BERT plays a great role in your model. To better demonstrate the effectiveness of N^3, I think you should show the accuracy of the baselines and N^3 with ELMo and Glove embeddings separately in Table 2.\n(4) This paper does not explain the results clearly such as Figure 5 (a) and (b).\n(5) There are many grammatical mistakes in this paper, such as the sentence “we also perform ...... and demonstrated ......” in Section 1.\n"
        }
    ]
}