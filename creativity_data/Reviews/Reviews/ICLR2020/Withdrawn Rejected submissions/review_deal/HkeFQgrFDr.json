{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work introduces a new method for stochastic multi-domain image translation called GMM-UNIT, which bridges a gap in the existing literature. Namely, the authors claim that existing approaches are limited to either deterministic translation between multiple domains, or stochastic multi-modal translation between only two domains.\n\nGMM-UNIT is an autoencoder-like model that allows stochastic multi-modal translation between multiple domains. To this end, images are described by two stochastic latent variables that represent the content and domain (or style) of the image. Importantly, the method uses a GMM prior for the style variables. By using the ground-truth domain knowledge, the authors train it such that each mode of the GMM represents a separate domain. Meanwhile, the encoders of GMM-UNIT take the role of inference networks that model the posterior distribution of (content, style) given an image. Having extracted a content variable, it is now possible to sample style from any of the GMM modes to translate that image to a different domain. One can also interpolate between modes to interpolate between domains. Additionally, one-shot domain transfer (style transfer) is also possible, since we can extract a style from a single image, possibly from a novel domain.\n\n The model is very complicated, and its training requires optimization of 8 (sic!) different loss terms (with corresponding weight hyperparameters), among which are two reconstruction errors, two GAN-style image generation losses, and a cycle-consistency loss. The model is compared against SOTA baselines on one-to-one domain translation of sketch->shoe, many-to-many domain translation of digits of different styles, and on translation between different attributes on CelebA. Additionally, style-transfer (as one-shot translation) results are provided, and authors investigate the effect of some of the loss terms in an ablation study.\n\nThe proposed model is very complicated, requires detailed analysis and clear descriptions if it is to be understood by the community. Meanwhile, this paper is poorly written, the approach is badly (and incorrectly) motivated, and there is not enough model analysis to warrant publication. For this reason, I recommend REJECTing this paper despite the impressive empirical results. I provide more detailed comments below and hope that this article can be improved and published at a future conference.\n\n1. The name GMM-UNIT is not explained beyond its first component, \"GMM\". What does UNIT mean/stand for?\n\n2. The authors claim that representing a domain with a continuous vector is novel, and contrary to discrete one-hot representations used in the literature, it 1) does not grow linearity with the number of domains, and 2) allows interpolation between different domains. I think this claim is wrong. The fact that a one-hot representation grows is an implementation detail; one could easily use integers and a learned constant-dimensional lookup table of domain embeddings. Similarly, even if one-hot representation is used, multiplying it with a matrix converts it into a vector. Since these one-hot vectors are typically fed into a neural network, one can easily interpolate between the results of matrix multiplication with such a vector. The fact that this has not been tried does not mean that it is impossible, or that it is a fundamental flow of prior art. Interestingly, the authors do say it themselves at the beginning of section 3, where they show how to derive prior art from the GMM-UNIT framework. Since these two claims are central to the paper and are both wrong, I think that the approach should be motivated differently.\n\n3. Figure 2 is very difficult to read, and the caption does not help. Please simplify the graphics and make the caption easier to follow. For example, what does \"train the network to fit the GMM\" mean?\n\n4. I don't understand why the \"domain classification\" losses are necessary, and there is no explanation given. Please give a detailed description and add an additional ablation study without those losses.\n\n5. I do not understand how non-exclusive binary attributes in CelebA can be modeled as different Gaussian components in a GMM other than having different modes enumerate the possible configurations of the attribute vector. I think this should be thoroughly explained.\n\n6. Some results (e.g., in table 2) are reported as number+-number, but it is unclear what those two numbers represent. Are they mean and standard deviation, respectively? Many runs were performed to get these numbers? Also, it seems that \"the best\" result is highlighted. However, in this case, all the best results (with no statistically significant difference) should be highlighted, which is not the case. Please fix that.\n\n7. The authors modify the StarGAN method to make it stochastic and say that this does not improve sample diversity. However, there are no details on this modification, which means that the readers cannot assess its sensibility by themselves. Please add the details.\n\n8. The appendix contains a figure with TSNE embeddings of inferred style vectors and samples from the GMM for the CelebA experiment, color-coded according to the domain. One can see that there is a substantial mismatch between the sampled values and the inferred ones, which indicates that the model did not do a good job at aligning those two distributions. Why is that? This should be discussed."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a unifying algorithm for multi-modal multi-domain image translation with either better performance or more functionality than its predecessors. It uses a GMM distribution to model the variations both within the same domain and cross-domain differences and an encoder to extract context information invariant across domains. The setup is able to degenerate into many of its predecessors (e.g. StarGAN, pix2pix, etc.) if we choose specific sets of hyperparameters. \n\nOverall, the paper did a good survey over the literature and comparisons across multiple data sets and baselines. There are a few caveats that made me hesitant to accept this paper. \na) There is a plethora of literature in image translation in recent years that works on improving image quality and diversity, avoiding model collapse, expanding to multi-domain translation, etc. Although it is worthwhile combining all those works into an overarching framework, I expect the sum to be more than the individuals all bundled together, which is unclear from this paper (see numbers in Table 2). MUNIT is a great example where the idea of AdaIN is not new, but its learned version applied to image translation yields great results and lots of future directions to explore.\nb) It would be better to have a clearer future direction of improvement for this paper. The authors mentioned this in the last sentence, but it would be great to have something to inspire future researchers to tackle the ill-defined unsupervised image-to-image translation problem. \n\nQuestions that are left unanswered:\n1. If multi-modal output is desirable, is there a tradeoff or could we get diversity for free?\n2. From eq.15, such a multi-term loss function may not scale to higher resolution output where model collapse occurs more often, or it could become extremely expensive to train, which hinders the speed to iterate through ideas. Has GMM-UNIT been trained at 256x256 or higher? How long did it take to train at 128x128?\n3. In image translation or style transfer domain, one of the often asked questions is \"what counts as a domain/style as opposed to the content?\" In this work, the content is defined as the encodable features that stays constant after translation to different domains. The style is defined as the Z modeled by the domain dependent GMM distribution. From fig. 6, it looks like local changes or simple changes like eye makeup and the overall color of the image are considered as parts of the style or noise that can vary within a domain. On the other hand, more complicated changes like hair style, hair length, jewelry, are treated as the content. What causes the network choosing to classify a feature as style versus as content? Does that change if we train from scratch again?\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes an unsupervised method for multi-domain image-to-image translation. Incorporating GMM as a latent representation, the proposed methods enjoys multi-modal generated images. \n\nI am voting for a paper to be accepted. \n\nThe main arguments for this decision:\nThe paper addresses an actively studied (i.e. interesting to the community) problem and the proposed method appears to be novel. The simple (in a good sense) idea of incorporating different domains as components of a GMM surprisingly provide great results. \n\nThe paper well written and easy to follow. The method is thoroughly studied from different angles. It is compared with relevant state of the art methods.\n\nSome major comments though where the paper has limitations and where it can be improved:\n1. Missing relevant work that challenges the claim that “we are the first ones being able to translate images in previously unseen domains”: Chen et al. \"Homomorphic Latent Space Interpolation for Unpaired Image-to-image Translation.\" CVPR 2019. Although in the referred paper there is no direct study on that, it would be better to include at least discussion on comparison of the proposed and referred methods since they both exploit continuous latent representation\n2. Some kind of ablation study or at least discussion on importance of disentanglement is missing. In general, the idea of disentanglement being one of the main features in the proposed work is not well presented in the paper. It is not clear from the paper what is the role of disentanglement in the proposed work and in the image-to-image translation literature in general. How important disentanglement representation in the proposed method? \nI believe it may greatly improve the paper if disentanglement would be considered on the level as multi-domain and multi-modal properties. For example, in the Related work section, there is a review of work on multi-domain and multi-model translation, as well as a nice summary of that review in Table 1, it would be great to have that for the disentanglement property as well. \nIn terms of the review of disentanglement in image translation, I could suggest some works currently missing in the paper: \na. Gonzalez-Garcia et al. \"Image-to-image translation for cross-domain disentanglement.\" NeurIPS 2018 for disentanglement similar to the proposed method: content-domain attributes \nb. Wu, Wayne, et al. \"Transgaga: Geometry-aware unsupervised image-to-image translation.\" CVPR 2019 for different type of disentanglement: appearance-geometry \n3. Regarding the claim that GMM-UNIT is a generalisation of the existing methods, do the special settings in GMM-UNIT described in the last paragraph on page 3 recover the exact versions of the existing methods? It seems it may depend on the implementation details such as specific losses, or architectures. It would be good to see empirical justification that GMM-UNIT with the special settings can recover the existing methods\n4. On the note of implementation details, for reproducibility, it is important to specify implementation details of the baseline methods in addition to the details of the proposed method. Are they coming from the original papers without any modifications? If the authors use public implementations of the baselines the references should be added \n5. Missing relevant work: Liu et al. \"Few-shot unsupervised image-to-image translation.\" arXiv preprint arXiv:1905.01723 (2019). Maybe of less importance because this work seems not to be published yet, but given the emphasis of the paper on ability of the proposed method to do few-shot translation, it seems that comparison with this work would be beneficial\nI am willing to increase the score if (some of) the above is addressed in the rebuttal\n\nOther lesser comments/suggestions:\n1.     Second paragraph in Introduction. Technically speaking, the text has not provided the information that image-to-image translation methods are usually based on GANs or VAEs. It would be helpful to include this introduction to GANs and VAEs explicitly, otherwise, from the text perspective it is unclear what GANs and VAEs have to do with image-to-image translation\n2.\t“thus naming the method: GMM-UNIT” – it explains only the GMM part of the name but not the UNIT part\n3.\t“Altogether, the models in the literature are either multi-modal or multi-domain <…>  DRIT++ (Lee et al. (2019)) also proposed a multi-modal and multi-domain model” – These two sentence contradict each other\n4.\t“Existing multi-domain models such as Choi et al. (2018) or Pumarola et al. (2018)” – this would much better work if Pumarola et al work was introduced in the paper\n5.\t“by separating the latent space from the domain code” – what is domain code?\n6.\t“its latent attribute z_n is assumed to follow the n-th Gaussian component of the GMM, z_n \\in N (\\mu^n,\\Sigma^n), thus K = N”, given that N is never defined, the statement about its equality to K is a bit off. It is unclear what is the purpose to introduce another variable. It seems and indexing domains with k rather than n would be a more straightforward choice since it is earlier specified that components of a GMM would correspond to domains\n7.\tEq. (2) and (3) – it is unclear why exactly those two properties are required for ensuring disentanglement of content and attribute extractors. \n8.\t“When traveling through the network” – what does it mean?\n9.\tDifference between “Consistency” and “Realism” properties is unclear from the first paragraph in Section 3.2\n10.\tMotivation of the need to use the isometry loss (when it is introduced before the ablation study) in the paper would be much appreciated as well as a verbal description of this loss\n11.\tAny motivation of using l_1 norm in eq. (4) – (8) is encouraged\n12.\tNDB, JSD, and HED – full names before introducing acronyms are encouraged\n13.\t“To estimate the IS” – IS was not defined\n14.\tLast paragraph on page 7 – is it supposed to be \\Sigma_k rather than \\sigma_k? If not, what is \\sigma_k? (the definition should be in the main text, not only in Appendix)\n15.\t“D: the number of domains, N: the number of output channels, K: kernel size” – mix in notation. N was used before for the number of domains and K was used for the number of components in GMM\n16.\t“Appendix B.1 shows instead the quantitative results of paired image translation.”: “Appendix B.1” -> “Table 7”?\n17.\tAppendix B.2, first sentence. It is better to add that these are results for the Digits experiment\n18.\tMore elaborated description in Appendix C would be appreciated\n\nMinor:\n1.\t“a phenomenon known as model collapse.” -> mode collapse\n2.\t“(i.e. a day scene ↔ night scene in different seasons)”: “i.e.” -> “e.g.”?\n3.\t“or map in the same model multiple domains”: “model” -> “mode”\n4.\t“We willshow that our model” – missing blank\n5.\t“Thus, our GMM-UNIT is a generalization of the existing state of the art. the In the” – redundant “the” before “In”\n6.\t“In the textbfconsistency term” -> \\textbf{consistency}\n7.\t“The value of most of these parameters” -> The valueS\n8.\t“the quantitative results on the CelebA datset” -> datAset\n9.\t“GMM-UNIT will be applied” – inconsistent tense. Better leave it present as in the other sentences\n\n\n\n"
        }
    ]
}