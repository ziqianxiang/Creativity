{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the problem of translating natural language questions into SQL queries to answer questions from databases. \n\nStrengths of the paper:\n1. The research problem is of great value in applications, as users may not have expertise on writing a good SQL queries. The proposed algorithm can help to parse the input arbitrary queries into SQL standard queries.\n2. The experiments were conducted on publicly available datasets, i.e., the Spider dataset that has been investigated in the previous work.\n\nWeaknesses of the paper:\n1. Many details of the equations and techniques are not clear. For instance, what do the notations $c_{i,0}^{fwd}$, $c_{i,0}^{rev}$ stand for in section 3.3?; what is $d_z$ and how to set $H$ in section 3.4? what does $\\alpha_{i,j}^{(h)}$ work for in section 3.4?\n2. The motivations of some techniques used in the paper are not clear. For instance, why do the authors apply BiLSTM but not other recurrent neural networks to obtain representations of the nodes in the graphs?  Are there other techniques to obtain embeddings of nodes in a network? There are a number of network embedding techniques having been proposed, for instance:\n(a) A. Grover et al., Node2vec: scalable feature learning for networks, KDD 2016.\n(b) Z. Meng et al., Co-embedding attributed networks, WSDM 2019.\n3. The contributions of the paper seem not significant. The paper sounds like a technical report on the combination of some existing encoding and decoding techniques to the task, although the task is interesting. In terms of technical contributions, the authors are encouraged to state them explicitly and more clear.\n4. Is the code of the proposed model publicly available? \n\nIn sum, the task of the research is very interesting. But at the current version, the paper needs to be revised to make all the notations and techniques be more clear, and the contributions of the paper seems marginal.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes an algorithm to translate natural language questions into SQL queries. First, a question, columns, tables are encoded using bi-LSTMs. Then, relation-aware self-attention with a stack of layers is used. Scheme encoding and scheme linking is conducted to model the column/table references. Then, decoder by LSTM is used to output a sequence of actions. Experiments are executed using Spider dataset. The proposed algorithm is compared to several existing algorithms including ones with BERT, and shown to outperform ones without BERT.\nMy simple question is why the authors do not use BERT? If it brings much gain on performance, just try RAT-SQL+BERT, which will clarify the contribution of the paper more apparently. Otherwise, there should be some merit without BERT. Second question is that by looking at Table 2(c), a large portion of performance gain is brought by scheme linking. However, scheme linking part described in Sec. 3.6 seems somewhat heuristic. If this is the important part, more elaborate analysis is needed (including possibility to apply this part to other existing methods.)\nOverall, although I think this research is important, it can not be accepted as a strong ICLR paper yet."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper deals with the problem of translating natural language questions to SQL queries by incorporating relational structures in database schema and the question to query. \n\nSpecifically, the authors proposed that existing methods of translating natural language to SQL queries cannot generalize well to unseen schemas. They then proposed their methods based on self-attention modules and graph representation of schemas. In addition, they exploited the heterogeneity of the schema-induced graph and designed various relations and corresponding relation-aware embeddings. Finally, the authors introduced a schema linking task that helps identify references in questions and the columns/tables in DB. \n\nStrengths: \nS1: The paper investigates an important and interesting problem of translating natural language to SQL sentences, which can potentially be significant in the applications of DB. \nS2: The paper clearly identifies the challenges and shortcomings of existing works, which is easy to follow. The analysis of related work is sufficient. \nS3: The idea of inferring soft relations between questions and words is novel and interesting.\nS4: In the experiment part the authors made a visualization of how the alignment between question words and elements in the DB schema looks like, which clearly illustrates the effectiveness of schema linking, one main contribution of the paper. \n\nWeaknesses: \nW1: I think the main weakness of the paper lies within the organization and description of the Sec. 3 “RAT-SQL”. Specifically, the authors made many forward references that make it hard for readers to follow. \ne.g. \na. On page 3, in Sec 3.1 “Problem Definition”, the authors referred the readers to Section 3.6, which I think is unnecessary as Section 3.6 introduces detailed solutions instead of introducing problem. \nb. On page 3, in Sec 3.2 the authors referred the readers to Table 1, which actually lies on Page 5. \n\nIn addition, in Sec 3.7, the authors did not mention the objective function that is used for learning the decoder. Since the decoder outputs a syntax tree instead of ordinary sequences (e.g. in machine translation), I think it is necessary that the authors make clearer descriptions on how the objective function is chosen and how the decoder is learned. \n\nW2: The empirical evaluations of the method are somehow limited. For example, in “Related Work”, the authors mentioned that there are two publicly available datasets. So why didn’t the authors use both instead of one?"
        }
    ]
}