{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose a method for automatic tuning of learning rates. The reviewers liked the idea but felt that there are much more extensive experiments to be done especially better baselines. Also, clarifying what aspect is automated is important, because no method can be truly automatic: they all have some hyperparameters. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nThe authors propose an automatic learning rate schedule based on an explore (always increase LR initially) and then exploit (more typical patience based decay) strategy. The strategy seems to factor in recent understanding of deep learning optimization, and I was very much convinced by the overall idea, even if not fully convinced by the motivation. The main issues with the paper I have is that (1) I does not compare to a strong enough baseline (one cycle), and (2) the schedule does not feel automatic enough to call it \"automatic\"; see detailed comments. Finally, I found the remarks and analysis regarding width of the minima, and the stipulation that there are more narrow minimas, not substantiated and even contradicatory to some of the results in the literature. Based on this, I am currently leaning towards rejecting the paper. I am willing to raise my score if issues with the experimental setting are addressed.\n\nDetailed comments:\n\n1. I am not convinced that the baseline is strong enough.\n\nWhile I appreciate the extensive range of architecture and datasets, I am not convinced that the baseline schedule tested against in each setting is actually \"state of the art\" as stated in the abstract. To the best of my knowledge, the state-of-the-art learning rate schedule, which uses a similar computational budget as the proposed method, is one cycle. In particular, one cycle uses LR range test to select the appropriate starting learning rate and then warmups to it gradually; see [1] for more details (see also [2]). \n\nNote that one cycle includes warming up the learning rate, which is another motivation for including it. As seen for instance in Fig. 4 the proposed AutoLR can increase learning rate initially, while the hand tuned schedules don't. This does not seem to be a fair comparison, given that the state of the art schedules in vision models very often do warm-up the learning rate.\n\n2. The learning rate schedule requires feeding in \"seed\" learning rate. Is it an automatic learning rate schedule then? I am a bit confused by this claim. Could you please clarify what is the claim about the method?\n\n3. To sum up point 1 and 2, while I like what the paper set out to do, I think that it is key to either (1) demonstrate that the automatic schedule is substantially better than the state of the art schedule, or (2) demonstrate that initial seed learning rate is not an important hyperparameter. If both are not true, then how would you convince the reader to use the method in practice?\n\n4. It is implicitely assumed in the analysis that wide minima are good for generalization. It seems to me that recent evidence points towards the direction that width of the minima is a epiphenomenon, see [3,4,5,6] experiments. I think at least a discussion of (some) of these papers is very important. Also it is worth noting that [5] studied how a high learning rate (or a small batch size) enters wide regions of the loss surface early.  \n\n5. \"In that respect, an interesting intuitive observation is that a large learning rate can escape narrow minima “valleys” easily (as the optimizer can jump out of them with large steps), however once it reaches a wide minima “valley”, it is likely to get stuck in it (if the “width” of the wide valley is large compared to the step size).\". I think [5,8] should be cited here. Both papers studied how learning rate selects minima shape.\n\n6. At the same time, I am not convinced by the experimental data for \"We hypothesize that for deep neural networks, narrow minimas far outnumber the wide minimas.\":\n\n6a) Taking on the face value that the experiment shows that a low learnring rate can find a wide minima, this doesn't prove the hypothesis. At best (i.e. assuming the indeed there is a strong correlation between width of the low lr minima, and generalization) it proves that low learning rate can find wide minima. It doesn't establish any bound/estimation on the relative number of narrow to wide minima. What if low learning rate by default is driven to narrow minima, and high learning rate is driven to wide minima, but the two are equal in number? I think a similar experimental data would be observed in such a null world.\n\n6b) More importantly, it is not demonstrated that the one experiment that worked better had lower curvature. It seems implicitely assumed that if it worked better then it for sure ended up in a wider minima. I do not think this is a valid reasoning given the weak data for a causal relationship between curvature and generalization (see 2). Could you please report curvature explicitely, rather than making this assumption?\n\nReferences:\n\n[1] Fast.ai documentation on one cycle method, https://docs.fast.ai/callbacks.one_cycle.html\n[2] Arpit et al, Walk with SGD, https://arxiv.org/abs/1802.08770\n[3] Golatkar et al, Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence, https://arxiv.org/abs/1905.13277\n[4] Yoshida et al, Spectral Norm Regularization for Improving the Generalizability of Deep Learning, https://arxiv.org/pdf/1705.10941.pdf\n[5] Jastrzebski et al, On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length, https://arxiv.org/abs/1807.05031\n[6] Guiroy et al, Towards Understanding Generalization in Gradient-Based Meta-Learning, https://arxiv.org/abs/1907.07287\n[7] Wang et al, Identifying generalization properties in neural networks, https://arxiv.org/pdf/1809.07402.pdf\n[8] Wu et al, How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective, https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "The paper proposes an automatic tuning scheme for learning rate while training neural networks. Since learning rate is the most sensitive and important hyperparameter during training, an automatic versatile method for choosing learning rate for various workloads would be of great importance. \n\nThe proposed method works for various optimizers (SGD/Momentum/Adam shown in the paper) and shown to perform as good or better than typical baseline learning rate schedule practitioners use to obtain competitive performance. There are a good number of empirical checks on both image and language tasks.\n\nWhile I’m not fully familiar with literature on automatic learning rates, the authors claim that this is the first auto learning rate tuning scheme to achieve SOTA performance. \n\nTwo main components of the scheme are based on observation that initial high learning rate seemingly without making any improvements is essential for obtaining good final performance. Therefore initially there is an `explore’ phase and then in exploit phase quadratic local approximation around high learning rate update is used. Novelty in the exploit phase is expansion respect to perturbation around some potentially large step size instead of assuming step size is small. \n\nOne main concern of the proposed method is the choice of seed learning rate and duration of explore phase is still somewhat arbitrary and requires tuning for specific model/dataset. For example, in CIFAR/ResNet there are experiments showing that duration of `explore’ phase is important and choose 50 epochs. In the case of BERT for SQUAD fine tuning 2500 explore steps(half epoch) are chosen. It seems choice needs to be tuned to get good performance and with this the proposed scheme is semi-auto tuning at best. Similar points for seed learning rate could be made. \n\nIn order for the proposed method to be fully successful, either authors need to show insensitivity to general choices of seed learning rate and explore phase, or an automatic method to choose good values. Without that  I do not see significant improvement beyond tuning learning rate schedule via linear or cosine decay schemes (which is also known to perform comparatively to multiple drop schemes). \n\nIn that spirit I think an interesting baseline to compare is tuning learning rate schedule. \n\nThe proposed explore-exploit method has great potential in terms of generality and strong performance on various tasks. I would happily raise my score if the main concern is addressed, however at this point I slightly lean toward rejection. \n\nUPDATE POST AUTHOR RESPONSE:\nI thank the authors for carefully responding to the concerns raised in the initial review. I do suggest that refraining calling current method as 'auto' since it could be misleading. I think full automatic learning rates, if it can be done, will have great impact in neural network optimization. I have raised my score as author's response cleared some of issues (sensitivity to seed / comparison with other common tuned schedules).  ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}