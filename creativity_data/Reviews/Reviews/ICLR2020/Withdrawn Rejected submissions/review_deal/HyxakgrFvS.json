{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies continual learning. Fine-tuning from previous task will make the network forget previous knowledge, but directly using features from previous tasksâ€™ network will cause underfitting. This paper proposes a method that is in the middle of fine-tuning and using fixed feature. A new small network is added to the architecture to help the training of new task. The method is evaluated on several datasets. This paper proposes interesting insight but the contribution of the proposed method is very limited.\n\nComments\n1.The setting in this paper is not a rigorous continual learning setting. Continual learning setting require model not to incrementally grow with task number. The novelty of keep all previous model and merely add new network for every task is very limited. \n2.The novelty of the proposed model is very limited. There are several works that proposed more advanced idea than this work such as [A] and [B]. The latter also proposed a method that avoid keeping all previous model in evaluation.\n3.The paper argue that the proposed method needs to train only smaller amounts of parameters for each task, but there are several works on this subject as [C]. Please conclude these methods in related works and compare with them.\n4.If the architectures of base model and side model are different, how to combine the features from these two networks? \n5.What is Side-tune (R) in Table 5? Why it outperforms Side-tune for a large margin?\n6.What is the relationship between the scale of side networks and the difficulties of tasks? How to make sure the architecture that to be used for a certain task? \n7.There is no need to arrange many unchanged images in Figure 5 that takes up much space in the main body of the paper.\nFrom above reasons, I vote for rejecting this paper.\n\n[A] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n[B] Flennerhag, Sebastian, et al. \"Transferring knowledge across learning processes.\" arXiv preprint arXiv:1812.01054 (2018).\n[C] Rosenfeld, Amir, and John K. Tsotsos. \"Incremental learning through deep adaptation.\" IEEE transactions on pattern analysis and machine intelligence (2018).\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposed a novel lifelong learning approach, called side-tuning. The idea of side-tuning is that they firstly have a fixed base network and when a new task arrived, they learn the task with a smaller network which transferred the adaptive knowledge from large base network while freezing  it. It can get fine-tune strength without catastrophic forgetting problem.\n\nThe approach looks practical that they only train a smaller side-network for new tasks. But the proposed model might have several crucial weakness. \n\n- It is extremely dependent to the base (pretrained) model. When the base knowledge will be largely different to the arriving tasks, it looks hard to appropriately train the model.\n\n- There is no knowledge transfer among arriving tasks. N side networks can not communicate among them which means they can not manage the redundancy problem among side networks.\n\n- There is a few of ablation study. Why does the model requires to element-wise combination among base parameters and side parameters? It seems that the architectural choice of the model and hyperparameter selection procedure is heuristic.\n\n- Even the model requires less number of 'trainable' parameters, the machine inevitably requires to memorize not only side-network parameters but also base network parameters. Then, in my understanding, the authors are required to compute the total capacity as addition of them, not only with trainable ones.\n\n- Also, the idea looks quite similar to piggyback, or Progressive neural networks that both approach kept the parameters of base network intact. The authors also require to compare with them.\n\nMallya, Arun, Dillon Davis, and Svetlana Lazebnik. \"Piggyback: Adapting a single network to multiple tasks by learning to mask weights.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes to tackle many tasks including among others lifelong learning or transfer learning. To do so, they propose to perform at the feature level an alpha blending between a fixed pretrained representation and a learnable (possibly smaller) network. Such a design allows to avoid catastrophic forgetting by construction and represents a tweakable middle point between a feature extractor and a fully finetuneable network.\n\nI am strongly leaning towards rejection. This paper provides numerous experiments but this should not replace novelty or presenting as novel something already present several times in the literature. For example, the proposed method does not differ much from \"Progressive Neural Networks\" by Rusu et al. if you switch off their lateral connections. \n\nFurthermore, several network adaptations papers are missing from the references and also present themselves as tweakable middle points between feature extractor and  a fully finetuneable network. Among others: \"Universal representations: The missing link between faces, text, planktons, and cat breeds\" by Bilen et al., \"Incremental Learning Through Deep Adaptation\" by Rosenfeld et al. or \"Efficient parametrization of multi-domain deep neural networks\" by Rebuffi et al. The latter also proposing an additive adaptation all along the network which preserve the original filters for the original task."
        }
    ]
}