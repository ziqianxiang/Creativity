{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes to improve the faithfulness of data-to-text generation models, through an attention-based confidence measure and a variational approach for learning the model.  There is some reviewer disagreement on this paper.  All agree that the problem is important and ideas interesting, while some reviewers feel that the methods are insufficiently justified and/or the results unconvincing.  In addition, there is not much technical novelty here from a machine learning perspective; the contribution is to a specific task.  Overall I think this paper would fit in much better in an NLP conference/journal.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors propose several approaches to making a data-to-text generation system more precise, that is, less prone to hallucination.  In particular, they propose an attention score, which attempts to measure to what degree the model is relying on its attention mechanism in making a prediction. This attention score is used to weight a mixture distribution (a \"confidence score\") over the generation model's next-word distribution and the next-word distribution of an unconditional language model. The learned conditional distribution can then be calibrated to the confidence score. The authors also propose a variational-inference inspired objective, which attempts to allow the model to ignore certain tokens it isn't confident about. The authors evaluate their approach on the WikiBio dataset, and find that their approaches make their system more precise, at the cost of some coverage.\n\nThis paper is well motivated, timely, and it presents several interesting ideas. However, I think parts of the proposed approach need to be better justified. In particular:\n\n-  What justifies defining the attention score A_t in this way? First, is there an argument (empirical or otherwise) for using the magnitude of the attention vector (rather than some other statistic)? Is it obvious that if the attention vector has a high magnitude then it ought to be trusted? Note that this might be a reasonable assumption in the case of a pointer-generator style model, where a single attention vector is used both for attending and for copying, but in a model where attention isn't constrained in this way the magnitude of the attention vector may be misleading.\n\n- The variational objective seems difficult to justify. First, I don't understand what it means for p(y | z, x) to be assumed to 1. Is this for any z (in which case y is independent of z)? Otherwise, how can it be removed from the objective? (Put another way: Equation (17) is not in general true; it neglects an expected log likelihood term). I'm also not entirely clear on how Equation (12) is modeled: do the z's really only rely on the other sampled z's? Doesn't that require a different model than the one that calculates P^{\\kappa}?\n\n- Somewhat minor: the claim that optimizing the joint objective needn't hurt perplexity relies on kappa being 0; have you confirmed empirically that when it isn't zero the perplexity improves over the baseline model?\n\n- Finally, I'm not sure I understand why there needs to be a stop-gradient in equation (4). It would be nice to also verify empirically that this is important.\n\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a method for conditional text generation that has higher factual precision, minimizing hallucination of facts. The method involves predicting confidence of generation at each time step and using this confidence measure to skip tokens during generation and calibrate output probabilities in test time. Their method achieves SoTA performance on automatically measured precision and human evaluated \"faithfulness.\" However their method does see a drop in recall (automatic metric and human evaluation).\n\nComments and issues,\n- The intuitive explanation for the confidence score is a little confusing. In Section 4, page 3, you say that \"If a token is likely a content word (i.e. when its generation probability by the encoder-decoder is much higher than the unconditioned language model), but the attention score is low, then the token might not be predicted based on the source, and could be hallucination.\" However, this doesn't seem like an airtight conclusion. Isn't it possible that the base-LM and enc-dec model have similar probabilities for a content word with the enc-dec attention being low? This seems possible given your observation that low attention to the source is what may be causing content hallucination. This same thing is essentially restated in section 4.1 \"we expect P(y_t |y_<t, x) to be higher than P(y_t | y_<t) for content words so the confidence score will largely depend on the attention score\", which seems more tangled up since P(y_t |y_<t, x) inherently depends on the attention score. This is all clarified when you explain the alteration made to the base-LM. I would recommend rewording/rearranging some of the earlier explanation for the efficacy of the confidence score since it seems that the alteration to the base-LM is an essential part of the explanation. \n- Need some explanation for Equation 6. I don't really get the intuition behind it.\n- The presented results are pretty good! However, I would like to see some numbers on average score across a few runs.\n- It would also be good to see results on one more dataset like E2E.\n- Provide a little more detail on human evaluation, you don't even mention if the evaluation was done with crowd-workers or another pool of people like grad students. How many annotators? What is the inter-annotator agreement? What was the prompt/structure? Human evaluation of models is notoriously difficult, more details would give some more weight to the results.\n\nI think this is a well written paper with thought out experiments. I recommend it be accepted to ICLR. I'd also be curious to see some future work that improves, or at least maintains recall, while keeping the higher precision.\n\nMinor requests/recommendations: \n- Include more examples of generations. Could be an appendix.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper aims to solve the unfaithful generation problem for a specific data-to-text generation task, i.e. wikibio dataset. The wikibio dataset has a specific feature, where the output doesn't often reflect the input info box. This will cause the traditional seq2seq-style neural generation models to hallucinate frequently since the training objective is often based on word likelihood.\n\nThe paper thus design a confidence scorer that estimates whether a word should be generated according to the source information. This score is used in both training and testing. In training, it helps avoid learn to generate the low confidence words. In testing, it is used to adjust output probabilities.\n\nOverall, I think this is an interesting idea. However, the design of confidence score highly rely on the attentions calculates from the generation process, and whether attentions can be reliably estimated is questionable. Maybe it would be useful to show some statistics (not just manually picked examples) on the hallucinated words, and see what's the portion of them are due to \"flattened\" attentions.\n\nFurthermore, the experimental results are not convincing. The generations of the proposed models are significantly shorter (might be the result of training, see my comment below about 4.3), the results are mixed, both coverage and fluency are worse. Wrt results, since the dataset is from Wiki, BLEU should be pretty indicative of the generation quality. And we do see significant drop of the proposed model.\n\n\nMore comments:\n- Eq 6 needs to be better explained. I don't know if this is the common way to calculate attentions, or I misunderstood the equation. \n\n- In 4.3, I'm not sure if I can understand it correctly. When the authors say \"minimize the negative log-likelihood on the confidence sub-sequence\", does it mean words not in the subsequences are ignored? Won't this hurt the language modeling part? I.e. cause the ungrammaticality? Is this why the fluency scores are low in Table 2?\n\n- If the authors want to show their model improve faithfulness, sample outputs should be shown."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the problem of data-to-text generation so that the generated text stays truthful to the data source. The idea of the paper is use a learned confidence score as to how much the the encoder-decoder is paying attention to the source. The paper includes several components, 1. unconditioned language model to incorporate the confidence score, 2. use calibration techniques to adjust the output probability; 3. variational bayes objective to learn the confidence score. \n\nThe paper has good motivations and is quite well-written. The problem is of great pragmatic interest. In the experimental part, the authors demonstrate the effectiveness of the proposed algorithm.\n\n1. For training part, regarding the language model and  variational bayes objective being trained jointly, does it have convergence problem? What is the motivation of not training them jointly?\n2. Will the code be released and the human evaluation be published?\n3. There are some importance baseline missing, such as [1], [2], [3]\n\n[1] Marcheggiani, Diego, and Laura Perez-Beltrachini. \"Deep graph convolutional encoders for structured data to text generation.\" arXiv preprint arXiv:1810.09995 (2018).\n[2] Ratish Puduppully, Li Dong, and Mirella Lapata. \"Data-to-text Generation with Entity Modeling.\" arXiv preprint arXiv:1906.03221 (2019).\n[3] Ma, Shuming, et al. \"Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation.\" arXiv preprint arXiv:1908.03067 (2019).\n"
        }
    ]
}