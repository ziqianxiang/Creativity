{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper treats the important problem of when a model should be re-trained from scratch or incrementally trained as new data becomes available. Unfortunately, it is very hard to extract any useful conclusion out of this 6-pages paper, and I think considerable work is required before it can be useful for the community.\n\nThe problem tackled is very important, but it is not very well formulated (e.g. their notation keeps changing along the paper, the assumptions are never properly stated, etc.) and their proposed solution is quite confusing, specially the “Reservoir sampling” part. Furthermore, the experimental section is extremely poor: the experiments performed are not well explained, the reported results are very weakly linked to their proposed method, there are no error bars what so ever in any of their single-curve plots, and even if we make an effort to understand which of the curves is their method, there doesn’t seem to be any substantial difference with respect to the other (baseline?).\n\nThere are considerable typos throughout the paper, some of them considerably obscuring the meaning of their sentences. Just to report a few:\n- Fig 2 legend (which by the way is not referred anywhere in the text…) “A one-shot SGD settings as a function of ratio of initial to incremental data.”\n- Are “N” and “n” used interchangably in the notation? As all their notation, we encourage them to fix it and define it clearly.\n- “The reservoir of samples is inferred before a training epoch”. Do they mean sampled, or that they run the inference of their model on all the data-points in the reservoir?\n\nOther misc comments:\n- Instead of saying that your model is characterized by a weight matrix W, you should say “the weights of our model” or the “parameters of the model”. In modern Machine Learning this might be much more than a single weight matrix.\n- It is very hard to understand the utility of the reservoir sampling, and there should be a comparison to other simpler ways of sampling from the available data (like uniformly sampling n elements). No ablation studies what so ever are performed. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This study proposed a heuristic approach to decide whether to use incremental training or full training of a pre-trained model with the availability of new data. The idea is that if the new data is consistent with the previous model (therefore, incremental learning achieves a bounded error), one can resort to incremental learning; otherwise, a full re-training is required. To assess this need, the authors proposed to use reservoir sampling to select data for incremental training.\n\nThe idea is intuitive, but it is not supported well by the authors, as no theoretical proof was presented, the results don't show a clear advantage of this method, and it is contradictory with previous results (e.g., that of Golmant et al., 2017). Also, the paper requires more experiments to support the claims in the abstract:\n1- Speed up for different tasks should be shown. Also, an analysis of the worst-case scenario (e.g., where the distribution of the samples changes by the time (non-stationary situation) and full training is required for each scenario) is required.\n2- Catastrophic forgetting was claimed to be avoided in the abstract, while the authors later stated that they try to reduce it using reservoir sampling. Also, it should be measured using metrics introduced in Kemker et al. 2018 to show that whether the idea is really achieving its goal or not.\n3- The threshold for having a bounded error also plays a significant role in the performance of the system. An experiment is required to investigate the effect of this parameter on the speed of the system, and the results should be analyzed and discussed.\nFurthermore, since the results are somehow contradictory to the previous task, more experiments on other tasks (e.g., NER, sentiment analysis) or domains (computer vision, time series such as stock data) should be conducted to draw a meaningful conclusion from the paper.\n\nAs a side note, the authors are encouraged to proofread and rewrite some parts of the paper for the next submission (e.g. \"Approaches to saving on training...\", \"we discuss our approach to informing...\"), better use the \\cite{} and \\citep{} commands of Latex, and present more readable figures (e.g. by including meaningful legends)."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an approach for deciding when to incrementally vs. fully retrain a model, motivated by the overall setting of iterative model development in real-world slot filling (e.g. info extraction) tasks.  The proposed approach, \"Parameterized Incremental Training\", maintains a reservoir of ground truth-labeled data samples- at each incremental training epoch, performance is tested against these examples, and if accuracy is below a user-specified threshold, the model is fully retrained.\n\nThis setting / motivation is a very relevant one, and the method seems like a sensible start, but neither the proposed approach nor the experimental treatment are substantial or novel enough to warrant acceptance; however hopefully something on this topic and approach- with a more thorough empirical and/or theoretical treatment- will appear later on from these authors!"
        }
    ]
}