{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions.\n\nOverall, the paper is relatively well written and fairly clear.  The reviewers agree that the theoretical contribution of the paper could be improved (tighten bounds) and that the experiments can be improved as well. In the context of other papers submitted to ICLR I therefore recommend to reject the paper.\n\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies loss landscape of Non-negative matrix factorization (NMF) when the matrix is very large. It shows that with high probability, the landscape is quasi-convex under some conditions. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. Implications on deep networks are also discussed. \n\nThe NMF problem is known to be NP-hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U’,V’, the MSE loss of NMF is convex with high probability.  The extra assumption is that the rank of U and V should also be large enough. Section 3 is devoted to prove this. It seems to me there are some typos which are quite serious and make the equation (3) incorrect. However, the main result (Theorem 1) still seems to hold. The equation (3) should replace W_2 with 2 W_2. The reason is in the appendix D.2, the definition of W_2 has missed this constant 2, which is the hat X’’(lambda) at lambda=0. Therefore, all the constants in the equation (4) need to be modified accordingly. In D.2, to derive the equation (9), it seems to me the McLaurin series should give 2 l’’’’(0) l’’(0) >= (l’’’(0))^2, isn’t it? The whole proof is quite long to check. In Fact 1, is the mu the mean of z? In Lemma 1, what is lemma 9? Fact 10 has a constant 2 which seems to be forgotten. Therefore significant modification is needed to correct all the errors. \n\nRegarding experiments, some data-set in Table 1 does not seem to me relevant to the paper (Assumption 1), in particular those with r < 10. Figure 6 shows that the gradient flow is close to a straightly line, suggesting that the gradient descent algorithm follows a convex landscape. The Figure 6(b) seems to me have not converged yet, as at step 10,000, the cosine is not as flat as the others. This means that maybe the gradient flow does not converge to the local minima (U^*,V^*). Further explanation about this is needed in the paper. Regarding optimization efficiency, it is not that convincing since even in the over-parameterized regime: the landscape become more convex, but there can be a lot of local minima which are not as good as the global minima. Therefore from an optimization perspective, finding global minima still remain challenging. I think it would be better to mention this somewhere in the paper. \n\nMinor typo includes: \nequation (5), write ||W_2||_F^2. \nEquation (12), hat 1 should be 1\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "1.\tTypos: \n(1)\t“We will consider slighly weaker notation of…” in page 3 should be “We will consider slightly weaker notation of…”.\n(2)\t“The same holds along the line line…” in page 3 should be “The same holds along the line…”.\n(3)\tA comma should be added between $U^*$ and $V_1$ in the Subsection ‘Proof sketch of unobserved Data’ in page 5.\n2.\tFigure 3 presents the loss surface of NMF on straight paths connecting two random points for 8 real-world datasets. From this figure, we can see that the minima is obtained around lambda equals to 0.5. Whether the authors can explain this phenomenon? Besides, the authors should annotate the differences among each curve in each subfigure in Figure 3 and Figure 4.\n3.\tExperiments utilize 8 datasets to demonstrate the good performance of the proposed planted model. The decomposition rank are given in the previously literature. However, in practice, the rank of a new dataset is unavilable. How to handel this situation?\n4.\tThere are unknow elements in Goodbooks, Movielens and Netflix datasets. These can be processed by the given sparsity. However, there is no items in the proposed planted model that can handle the sparsity.\n5.     Open codes about this paper.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions. The star-convexity property is also shown to hold to some degree on real world datasets. The paper argues that these results explain the good performance that usual gradient descent procedures achieve in practice. The paper also puts forward a conjecture that more parameters make the loss function easier to optimize by making it more likely that star convexity holds, and that a similar conclusion could hold for DNNs.\n\nThe paper is rather well written, although there are many small typos or notation errors (of which I mention a few below). In addition, I have a few issues with the presentation of both the theoretical and experimental results. Although the results relating to star convexity seem compelling and interesting to understand the good practical performance of the usual simple NMF algorithms, I find the conjecture on concentration of measure a bit hand-wavy.\n\nIn particular, I have some questions regarding the main theorem:\n\n\t- \"r grows as $O(n^\\gamma)$ and m as $O(n)$\": Big-O means asymptotically bounded, i.e., that r is roughly smaller than n^\\gamma (which is a tautology for $\\gamma=1$ anyway), and that m is roughly smaller than n. Is that really what you mean? Or do you mean $\\Omega/\\Theta$ instead? Fact 2 uses \"recall our assumption that $r = c n^\\gamma$\" which seems to imply you mean \"r = \\Omega(n^\\gamma)\", which can also be informally stated as \"r grows as n^\\gamma\", without Big-O notation. Please clarify this notation.\n\n\t- Also, re $\\gamma \\geq 0.5$, I do not quite see why such a strong assumption is made. Wouldn't $r = \\Omega(n^{1/3 + \\varepsilon})$ be enough with respect to Fact 2 to get an asymptotically vanishing deviation probability? i.e., wouldn't the final result work with $\\gamma > 1/3`$, which is more general?\n\nGenerally, this begs the question of whether $r = \\Omega(n^0.5)$ is realistic. Can this be put in perspective with respect to other work?\n\n\t- What is Lemma 9 mentioned in the proof of Lemma 1? Is that in the paper? I could not find it. In particular, I am unclear on \"we can do a polynomial number of union bounds\": polynomial with respect to which variable?\n\nI also have the following additional questions on the experimental findings and the proposed conjecture.\n\n- Section 4.2 and Figure 5 uses the 'relative deviation', which is the standard deviation normalized by the mean. I am not sure what conclusion to draw from this, however. In particular:\n  - Why normalize by dividing by the mean?\n  - What is the evolution of the mean itself? If the mean is always greater than a few standard deviations, then the fact that curvature is getting more concentrated may not matter. How about showing instead the fraction of trials where the curvature was non-negative? (i.e., where we had convexity along the line)\n\n- I struggle a bit to make sense of the conjecture. In particular, 'concentration of measure' is related to randomness, but it isn't quite clear which 'measure' we're talking about here. Table 2 is also not very convincing: although the legend indicates 'increased width makes the loss surface increasingly locally convex', it only seems to hold for the fourth row (epoch=300). In addition, these means are reported without standard deviations, making it hard to judge whether to trust the ordering of these few numbers.\n\nTypos/unclear:\n\nintroduction: \"strictly non-negative factors\", what do you mean by strictly?\n\"randomly chosen or the global minimizer\" is a bit unclear - what is the 'or' over?\n\"Convex along straight paths towards the optima x*\": x* -> $x^*$ to be consistent with Definition 1.\n\"similar to Dvoretzky's\": I fail to see the similarity and how this is related to the present paper.\n\"we will assume that there is a planted optimal solution (U*, V*)\": U*, V* should be in bold to be consistent with further notation.\n\"how r and m depends on n\": depends -> depend\n\"as the size of the problem increaseS\"\n\"loss function *of* equation 2\"\nTheorem 1: \"at least \\geq\" is redundant\nTheorem 1: \"but with exponent -c r^{1/3}\" is unclear: which exponent is this? Write the statement in full instead.\n\"it's second derivative\": its?\n\"for unobserved Data\": why is 'data' capitalized?\nD3: \"clearly, equation 9 holds in this case\": not sure what is meant here - equation (10) does not imply (9) or reciprocally. Did you mean that equation 10 holds instead?\nD3: \"3 evenly spaceD \\lambda\"\nD6: Definition 2: \"iff there *exists* positive constants...\"\nD6: \"one can easily verify that independenT Gaussian variables\"\nD6: \"i=j, j=l, and so on\": what is 'and so on'? all indexes are completely arbitrary?\nD6: \"z = P(X_i, X_j, X_k, X_l)\": clarify notation $P$. In the main text, you've used the notation $p$ instead.\nD6: Equation (12) and (13): should the RHS be exp(-X) instead of exp(X)?\nD6: $- \\mu$ notation is a bit confusing: the text doesn't explain what it is, and lack of bold face hints that this is a scalar and not a matrix.\nD6: \"Lemma 9 says that each one can be expressED\"\nE1: End of proof of fact 3: do not include an equal sign if there is no left-hand side.\nE1: \"trace and linearity are linear operators\": linearity is linear?\n"
        }
    ]
}