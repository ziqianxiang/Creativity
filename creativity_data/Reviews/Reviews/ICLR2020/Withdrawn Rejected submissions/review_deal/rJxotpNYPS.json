{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper addresses the problem of domain generalization. The proposed solution, DIVA, introduces a domain invariant variational autoencoder. The latent space can be decomposed into three components: category specific, domain specific, and residual. The authors argue that each component is necessary to capture all relevant information while keeping the latent space interpretable.\n\nThis work received mixed scores. Two reviewers recommended weak reject while one reviewer recommended weak accept. There was extensive discussion between the reviewers and authors as well as amongst the reviewers. All reviewers agreed this is an important problem statement and that this work offers a compelling initial approach and experiments for domain generalization. There was disagreement as to whether the contributions as is was sufficient for acceptance. Some reviewers were concerned over similarity to [ref1], this work appears close to the time of ICLR submission and is therefore considered concurrent. However, despite this, there was significant confusion over the proposed solution and whether it is uniquely useful for domain generalization or for other areas like adaptation or transfer learning with reviewers arguing that experiments in these other settings would have helped showcase the benefits of the proposed approach. In addition, there was inconclusive evidence as to whether the two latent components were necessary. \n\nConsidering all discussions, reviews, and rebuttals the AC does not recommend this work for acceptance. The contribution and proposed solution needed substantial clarification and the experiments need additional analysis to explain under what conditions each latent component is needed either to improve performance or for interpretability.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work proposes to solve domain generalization problem in a Bayesian way. The idea is relatively simple: use three hidden variables to encode the domain-related, label-related and the residual information from the original signal. \n\nSome questions:\n-\tMy major concern is the intuition for the proposed algorithm. Though the author uses z_d, z_x and z_y to encode different information from x, it is unclear why they are disentangled. It is also possible that all of them share the same information. In other words, why Figure 2 can be derived from this setting is not well explained. I am also confused on why the model can generalize to unseen target. Only using the learned encoder/decoder from training is hard to generalize.\n-\tFor Figure 2, the first column shows z_d. As a common sense, 30 and 45 degree should be more similar than 30 and 60. However, it seems that the cluster center between 30 and 60 is closer than 30 and 45. Is there any justification? My further concern is whether z_d is meaningful at all. What does z_d look like when applying the model on the unseen testing domain?  (Figure 6 should plot in the context with training domains.)\n-\t A single k-means method can cluster on rotated MNIST by labels. So I think this property should be kept in the feature without any supervised information. However, z_d seems to remove all label information away. I’m not sure why this can happen.\n-\t(Minor) The datasets used for comparison are not discriminative. Maybe the encoder structure is more important than the domain generalization method itself. More challenging datasets are expected. \n\nI would like to improve my score if the author can give a reasonable intuition on why the model can generalize on new domains.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this work, the authors propose a domain invariant variational autoencoder for domain generalization problem. Specifically, the data is assumed to be constructed from three independent variables, one for the domain, one for the class and one for the residual variations. The method can be used for both unsupervised and semi-supervised cases. Experimental studies on rotated MNIST dataset and a malaria cell images dataset verify the effectiveness of the proposed method.\nThe paper is well-written and easy to follow. The proposed generative model is simple and technically sound. However, I have the following concerns.\n(1)\tIt is not clear how the problem setting, i.e., domain generalization, matters in DIVA. In another words, the proposed DIVA is not specific to domain generalization problem, but can be used for domain adaptation, multiple source transfer learning etc. Actually, I find that the authors compare with DA, which is a conventional domain adaptation method, in the experiment. Moreover, the experimental setups in section 4.1.3 is a multi-source transfer setting, and DIVA can be well be applied. In this sense, I am not very convinced on the claim of the contribution that DIVA is proposed for domain generalization. For me, DIVA is a more general method.\n(2)\tWith point 1, more related works on VAE for domain adaptation need to be discussed.\n(3)\tThe idea of constructing data from disentangled latent variables is not new, see a latest work [ref1]. The main difference of DIVA from [ref1] is the residual variation variable. Two baselines are necessary for comparisons: (1) [ref1], and (2) DIVA without the residual variation variable. Actually, the semantic meaning of z_x is not well discussed. Even in the experimental studies figures 2 and 3, it is hard to tell what Z_x actually represents. \n(4)\tRegarding the 4.1.2, why DA is selected as a baseline? How does DA deal with the multiple domains? Can any other domain adaptation methods, e.g., [ref2], or multiple source transfer methods, e.g. [ref3], be compared?\n(5)\tIn the right side of Table 1, the improvements seem to be very marginal considering the variance. This makes the ability of DIVA to use unlabelled data less convincing. \n(6)\tRegarding 4.1.3, it seems that the domain similarity plays an important role in the performance, comparing the results of M_{30} with M_{60}. Without the labelled M_{60}, which is very similar to the target M_{75}, the performance degenerates dramatically. The current DIVA treats all the domains equally, is it possible to have a weighted form of DIVA that distinguishes the contributions of different domains? \n(7)\tWhat is the task of the malaria cell images experiments? Is it to classify the parasitized and uninfected cells? For a given patient, it makes more sense that all the cells belongs to one category, either infected or healthy. How is the class distribution for a person (in this case a domain)? Is it very unbalanced? \n(8)\tFor figure 3, It is hard to judge the cell images parasitized or uninfected without domain knowledge, can you give the label for each image? Again, the semantic meaning of Z-x is hard to tell. I am not convinced by the shape of the cell for Z-x.\n(9)\tFor 4.2.2, why and how DA is compared? As far as I know, it is for unsupervised domain adaptation. Moreover, the improvements are quite marginal. \nSome minor comments:\n(1)\tPage1, first para, 3rd line, “present” -> “presented”.\n(2)\tPage2, first para, 2nd line, “Y” - > “Y denotes”\n(3)\tSome references lack of page information\nThe paper should be self-contained. I would suggest the authors move some paragraphs in appendix to the paper, for instance, 5.1.1, 5.2.2, and 5.2.3.\nOverall, the paper is presented with extensive empirical evaluations, but less theoretical justification. The significance of the paper is moderate as the key idea of learning disentangled latent variables has been studied, and the paper lacks of evidence to show the pure benefits of introducing Z_x as well as the comparison with the related work [ref1]. \n[ref1] Learning Disentangled Semantic Representation for Domain Adaptation\n[ref2] Conditional Adversarial Domain Adaptation\n[ref3] Multiple Source Domain Adaptation with Adversarial Learning\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper introduces a VAE that can be used in problems in which domain information is available at training time to increase the classification performances on unseen domains. The model can also be used in a semi-supervised setting if unlabelled data is available (even from unseen domains).\n\nAs the authors state, this is a quite common scenario in many interesting applications such as medical imaging. As such, I found the paper interesting to read (the paper is also written quite well).\n\nTo allow the classification of data from any domain, in the inference network the labels d are not used to infer the latent states, but only as an auxiliary loss that forces z_d to capture domain-specific information. However, this makes me wonder if z_d is needed at all? I wouldn't be surprised if the model performed equally well if domain information d was not passed to the model even during training, in which case z_x would also capture domain-specific information. To understand if this is the case, it would be very helpful to add a baseline model in which you use a version of DIVA in which z_d and d are not present both in the generative model and in the inference network.\n\nSince from the technical point of view the novelty of the model is limited, I would have liked to see a stronger experimental section to show the real-life applicability of the model:\n- The MNIST experiments are visually helpful to understand the disentanglement in the model. However you solve a quite simple task. What are the performances of a baseline classifier on this?\n- The malaria cell experiment is definitely more realistic and therefore interesting, and the results are quite convincing (despite being on quite low-resolution images). Since I consider the MNIST experiment a \"toy task\", I think the paper would greatly improve if a new real-world experiment was performed (e.g. other medical imaging datasets).\n\nOverall I liked the paper and I think it is relevant for the ICLR community, therefore I am voting towards acceptance. However, a more convincing experimental section is needed for me to increase the score.  \n\n\nSmall comments:\n- you should make clearer from the abstract that the goal of this paper is to build a domain-invariant classifier (as opposed to solving more VAE-like tasks)\n- in the beginning of section 2.1 you write twice p(x|z_d, z_x, z_d), without z_y\n"
        }
    ]
}