{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "What is the paper about ?\n*The paper proposes a new approach to Automatic Essay Scoring (AES) by pairwise comparison of essays using Siamese Network.\n*The method claims to mitigate the data requirement of existing deep learning based techniques for AES.\n\nWhat I like about this paper ?\n*Few shot results and ablation studies adds weight to the paper and makes the approach attractive.\n*Section 4.5.1 explicitly states the benefits over regression.\n\nWhat needs improvement ?\n*While application to AES is novel, the method itself is not very novel and to me seems like a derivative of BERTScore [1].\n*I am not very familiar with AES in general but I think that related work section needs improvement and the authors have missed out on few works that I found are closely related to their work. One such example is [2]. \n*Figure 1 doesn’t explain the method very clearly and can be improved. What does text embeddings in the blue box mean ? Is that sentence level representation ? Is there more than one reference essay ? While it can be understood by reading the paper, all this is not clear from the diagram and defeats the purpose of having one.\n*Table 1 doesn’t highlight which methods are actually proposed in the paper. The numbers in the table corresponding to other approaches are they taken from a paper or are from authors’ own implementation of that method ?\n*Overall readability of the paper is low and has room for vast improvements. It is difficult to follow at times, especially the “Scoring” section 3.3. The writing quality can also be improved. Lots of grammatical errors spread out across the paper.\n*What appears to be a simple method in the beginning turns out to be quite complicated approach considering the different dropping rates for training data and lots of other strategies to make it work. Improvement over existing methods like Skipflow or 10×(CNN+LSTM) is also not very significant.\n*Methods like Skipflow are missing from Table 4. How do those perform in low resource setting ?\n\nNits\n*Section 4.2 Line 2 It is not “surprising”.\n\nQuestions for Authors ?\n*Did the authors use BERTScore as an inspiration or was this a concurrent work ? Since its relevant to this work, can BERTScore be cited in the paper ?\n*Section 3.2.2 From my understanding the training data is generated in such a way that one essay is always better than the other as same score essays are not paired. In this case won’t the model be biased to always predict a given essay as better or worse than the reference essay even though the quality is actually the same ? Is this taken care of by penalizing the distance of $p_i$ from 0.5 ?\n*Section 3.2.2 mentions “Therefore, it is foreseeable that requiring RefNet to categorize identically rated essays will frustrate the model during training and impair the performance.” Did you see this in practice ?\n*In Table 4, Was RefNet pre-trained ? If yes did you only use 10% and 25% of the data for pre-training ? \n\n[1] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text Generation with BERT. arXiv preprint arXiv:1904.09675.\n[2] Ronan Cummins, Meng Zhang, and Ted Briscoe. 2016. Constrained multi-task learning for automated essay scoring. Association for Computational Linguistics.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "# Summary #\nThis paper works on essay scoring. Instead of treating it as a regression problem, the authors proposed to model it by pairwise comparison (i.e., who should be scored higher). They then introduced a way to infer the final score by comparing the test essay to multiple reference essays. The experiments result demonstrated the improvement against the baseline, especially when the training data is limited.\n\n# Strength #\nS1. The paper is in general well-written and can be easily followed. The ablation study is well-designed to show improvement.\nS2. The proposed idea seems to be novel for essay scoring and works well in both the limited and sufficient data cases, which may become the essential building block for future work.\n\n# Weakness or comments #\nW1. Beyond the issue of insufficient data, I feel that in general, essay scoring should not be treated as a regression problem. Indeed, the score may not follows a good metric: the difference between score 6 and 4 is not equal to score 4 and 2. This is similar to the problem of facial age estimation: the degree of appearance change between ages 1 and 3 is different from ages 61 and 63. The essay scoring problem might naturally be better modeled as an \"ordinal regression problem\" (or ranking) or classification problem, rather than regression. What the authors proposed can indeed be seen as ordinal regression by pairwise ranking, and the authors should thus discuss the literature. The followings are examples.\n\nZ. Cao et al., \"Learning to rank: from pairwise approach to listwise approach,\" ICML 2009\nL. Lin et al., \"Ordinal regression by extended binary classification,\" NIPS 2007\n\nW2. The related work can be much strengthened. The authors could discuss problems where learning pairwise ranking/similarity is beneficial, for example:\n\nF. Sung et al., \"Learning to Compare: Relation Network for Few-Shot Learning,\" CVPR 2018\nD. Parikh et al., \"Relative attributes,\" ICCV 2011\n\nThe final decision is made by aggregating the scores from references (based on the pairwise comparison or similarity), which is essentially a case of non-parametric models, which are known to work well in insufficient data. The authors thus may discuss techniques like support vector machines (regression), Parzen-window methods, etc. For example, Changpinyo et al. showed that with limited data, support vector regressions work much better than MLP for regression.\n\nChangpinyo et al., \"Predicting visual exemplars of unseen classes for zero-shot learning,\" ICCV 2017\n\nW3. The method is not learned end-to-end directly for score prediction. The authors should discuss about this.\n\nW4. [About testing/ evaluation] The authors do not include formulas for evaluation. Also, how many reference (known) essays the authors consider in making a decision for a test essay (e.g., Eq. (3)). Since there are multiple prompts, during testing, do the authors only select those known essays that are from the same prompt of the test essay to aggregate the score? Is this a fair setting comparing to other baselines (i.e., by knowing the prompt where the test essay is from)?\n\nW5. The explanation of Sect. 4.2 is not clear. How can a model overfit when using even more data? I think it might relate to imbalanced training, where there are more pairs with large score differences than those that have small score differences.\n\n# For rebuttal #\n1. Please discuss W1-W5.\n\n2. Besides using transfer learning (i.e., pre-train and fine-tune), the authors should compare to multi-task learning: i.e., directly learning the two objectives --- pairwise comparison and regression --- together.\n\n3. What is 10x in Table 3? Also, do the existing methods also use BERT for text embeddings? If not, the comparison might be unfair."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method of automatic essay scoring (a document regression problem) by using pairwise comparisons. The paper proposes using siamese networks to compare the candidate essay with a set of anchor essays for every possible score interval to determine which score it is closest to.\n\nWhile the paper is well laid out I think there are some issues for the paper. The main issue is the limited novelty: using pairwise approaches to compare documents has an extensive history in document similarity problems and this can easily be used for score similarity. Other than using Bert pretrained embeddings and performing pretraining with the regression task (approaches which are known to work for text classification) there doesn't seem to be additional novelty in the paper. Important experimental details are also missing, such as how many anchors were used at test time, the sizes of the models,  the hyperparameters used and the loss used when training for regression.\n\nThere are also some peculiarities with the experimental setup. Why are only one layer of RNN/LSTMs explored? By softmax I assume the paper means a logistic layer or a 2-class softmax as their output layer for the pairwise model. The ablation results in table 5 also seem to imply the pretraining and other techniques have little impact on the model improvement which appears to mostly come from switching from regression to pairwise comparisons. Section 3.2.3 is commonly called pre-training rather than transfer learning. Section 4.3 is also incorrectly labelled few shot learning when instead it's an experiment with a reduced training set rather than truly giving only a few examples of each score notch. In the conclusion, the paper states that the pairwise model uses mutual information but that isn't what mutual information is usually taken to mean.\n\nFinally, there are numerous typos and grammatical issues in the paper.\n\nOther minor comments:\nP1: \"Computers also prevail\" -> \"Computers also prevail over\"\nP2: \"several problems remind\" -> \"several problems remain\"\nP2: \"power the of\" -> \"power of\"\nP3: \"same time semantically\" -> \"same time being semantically\"\nBelow eq3: \"in specific\" -> specifically\nEnd of P5: \"the the\"\nStart of section 4.2: \"It is not supervision\" -> Is is not supervision\nStart of section 4.4: \"from starch\" -> \"from start\""
        }
    ]
}