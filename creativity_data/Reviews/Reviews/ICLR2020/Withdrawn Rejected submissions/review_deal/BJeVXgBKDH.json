{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses the problem of question generation from paragraphs. It introduces two models that incorporate learning of \"hierarchical\" representations -- one LSTM-based, and one Transformer-based. The paper shows that these models yield higher performance than their \"flat counterparts\".\n\nThe basic idea of leveraging hierarchical structure in language is of course sensible, but it's not clear to me how well-motivated or original this particular instantiation of hierarchical representation is, or what we have learned based on the comparisons reported.\n\nThe overall motivation, both for the problem in general and for this particular approach, should be improved. The fact that question generation is popular is not adequate argument for its importance -- the paper should argue for why this problem matters. As for this particular approach, by certain definitions of \"hierarchical\", one could say that many or most models these days incorporate hierarchical representations, so this paper needs to be much clearer about how its instantiation of hierarchy is different. Part of the problem here is that the exact methodology is obscured by the non-optimal structure of the writing. Even after a couple of passes, I'm fuzzy on the precise model details. In what precise way is this model's instantiation of hierarchical structure different from the general hierarchical structure of, for instance, processing text starting at the word or character level and producing token-level representations as well as sentence-level representations? \n\nEven more problematic is the lack of detail about the baselines to which the proposed models are being compared. The paper's claims about the value of this hierarchical approach hinge entirely on the comparison with these baselines (notably, there is no comparison to the current state of the art, presumably because these methods do not compare favorably). So the value here would be in what we learn about how a particular instantiation of hierarchy can improve over the lack thereof. But because I'm not clear on the precise differences between the baselines and the proposed models, I'm unable to assess whether this comparison is in fact a comparison of the presence of hierarchical representations, or something else -- for instance, is it possible that the \"hierarchical\" models simply have more parameters? Or have access to richer features? It's not clear what factors have been controlled for and what have not. There is also no analysis or discussion to assist us in understanding what we have learned.\n\nAnother area without adequate methodological description: what was the nature of the human evaluation? Was this crowdsourced, or simply performed by the authors? On how many questions was this human evaluation performed, and how were they selected? If the humans were not the authors, how were they asked to do the ratings?\n\nOverall, I think the paper needs to improve its clarity with respect to motivation and methodology, before it can be ready for publication."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes two hierarchical models for the challenging task of question generation from paragraphs. One is the hierarchical BiLSTM model with selective attention, which learns to attend to important sentences and words from the paragraph. The other is the hierarchical Transformer architecture. The two hierarchical models both learn the hierarchical representations of paragraphs. Extensive experimental evaluation on the SQuAD and MS MARCO datasets shows that the hierarchical representations to be overall much more effective than their flat counterparts. \n\nThe paper is reasonably well written; one can easily follow and understand most technical aspects. However, I have some following major concerns about the paper:\n(1)\tThe idea is not novel, because the use of hierarchical structure to model long text is straightforward and has been widely studied in natural language processing tasks, such as dialogue [1,2] and document summarization [3,4]. The authors should explore the differences among these tasks and maybe provide a related work for the utilization of hierarchical structure in NLP [1,2,3,4].\n(2)\tThe experiment is not sufficient, the authors should compare their model to the SOTA baselines, such as the paragraph-level methods [5].\n(3)\tHow much data and how many volunteers did you use for human evaluation? The authors should give a detailed description for how to conduct your human evaluation.\n\n[1] Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.\n[2] Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma. Hierarchical Recurrent Attention Network for Response Generation\n[3] Yang Liu, Mirella Lapata. Hierarchical Transformers for Multi-Document Summarization.\n[4] Xingxing Zhang, Furu Wei, Ming Zhou. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization.\n[5] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks.\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a method for question generation given long paragraphs. The authors propose to encode the paragraph with a hierarchical architecture first encoding sentences in terms of constituent words, then paragraph in terms of constituent sentences. They experiment with both a bi-lstm and a transformer based architecture. The question decoder is a soft-attention model and the attention mechanism is specifically tailored to take into account the hierarchical nature of the encoder. The paper's main contribution is to propose two hierarchical architectures as well as a novel way of attending over the inferred hierarchy of representations during decoding. The empirical results suggest that the hierarchical versions of the architectures outperform the \"flat\" ones.\n\nThe main weaknesses are (1) the novelty of the proposed methods is low (2) results are hard to compare with previous work and some experimental details are omitted. Together, they put the paper below the acceptance bar.\n\n(1) Hierarchical encoders have been used under different architectural twists in a plethora of different seq2seq settings. The proposed hierarchical attention mechanism consists in re-weighting the word-level attention probability by the sentence-level attention probability, hence the hierarchical nature of the attention. Simply put, the probability of attending over word w given decoder state h_t, is just p(w|s, h_t) p(s|h_t), where s is the sentence containing word w and p(s|h_t) and p(w |Â s, h_t) are just standard soft-attention. Something close has already been proposed in https://arxiv.org/pdf/1602.06023.pdf, but it's not cited in the paper.\n\n(2) Experimental results do not report any previous published baselines, but just baselines proposed by the authors. I'd love to see experimental results by https://www.aclweb.org/anthology/D18-1424.pdf. I don't know if their results are directly comparable. There's no information about how the human experiments have been performed, how many annotators have been used and what are the instruction given to the human annotators to judge syntax, semantics and relevance.\n\nThis paper could be improved by strengthening the experimental evaluation and reporting the relevant references to previous work:\n\n1) add reference to https://arxiv.org/pdf/1602.06023.pdf, and to the plethora of hierarchical attention models in the literature not cited in the paper.\n\n2) compare the scores of the model with https://www.aclweb.org/anthology/D18-1424.pdf. Especially report the results under different splits of Squad corresponding to previous work.\n\n3) Add more details on the human evaluation.  \n\n"
        }
    ]
}