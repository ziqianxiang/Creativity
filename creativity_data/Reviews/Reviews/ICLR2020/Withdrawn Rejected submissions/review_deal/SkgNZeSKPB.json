{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this paper, the authors investigated the gradient exploding/vanishing issue in RNNs. \nThey analyze the gradient flow along the temporal dimension as well as the depth dimension. \nA new recurrent architecture called STAR is proposed to resolve the gradient vanishing/exploding issue.\n\nAlthough I think the designing of the new recurrent cell is novel, this paper does not distinguish itself from a  \nlarge number of papers trying to address the RNN gradient vanishing/exploding issue. \n\nOn one hand, the gradient analysis in Section 3.1 is not novel. The two-dimension gradient flow analysis seems to be a trivial extension of \nRNN gradient analysis along the temporal dimension, which appears in many related pieces of literature [1][2][3].\nOn the other hand, both the gradient analysis and the toy example hold under a very special condition ( h=0, W being orthogonal). \nIn practice, this is rarely the case unless spectral constraints are applied to weight matrices [3][4]. \n\nEmpirical results show that STAR can achieve better performance with a deeper structure. \nHowever, it seems for most experiments stacking more layers does not improve the model performance. \nSome time more layers make it worse even for STAR. It would be more convincing if the authors could provide empirical \nresults showing deeper RNNs have better performance than shallower ones.\n\nIn general, I think this is an interesting paper but with limited contribution.  I vote for rejection.\n\n\n[1] Arjovsky, M., Shah, A. and Bengio, Y., Unitary evolution recurrent neural networks. In ICML 2016 (pp. 1120-1128).\n\n[2] Zhang, J., Lin, Y., Song, Z. and Dhillon, I., Learning Long Term Dependencies via Fourier Recurrent Units. \nIn ICML 2018 (pp. 5810-5818).\n\n[3] Mhammedi, Z., Hellicar, A., Rahman, A. and Bailey, J., Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. \nIn ICML 2017 (pp. 2401-2409).\n\n[4] Zhang, J., Lei, Q. and Dhillon, I., Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. \nIn ICML 2018 (pp. 5801-5809)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper investigates the magnitude of gradients when training deep RNNs, and presents a new type of gated cell named the STAR unit to address the gradients vanishing/exploding problem. The STAR unit is shown to ease the training difficulty and thus enable stacking more RNN layers.\n\n1. The major contribution of this work is to extensively study the gradient vanishing/exploding problem of multi-layer RNNs. However, it is not clear on what dataset the authors did the experiments in Figure 2. Besides, I encourage the authors to validate these results over more datasets for more compelling evidence.\n\n2. As mentioned by the authors, the GFRNN addresses the training difficulty of multi-layer RNNs via vertical skip connections. Please compare the STAR net with this model in experiments.\n\n3. As far as I can tell, the STAR unit is very similar to GRU (both removing the cell state and the output gate). Could the authors shed more light on the originality of the STAR unit over GRU?\n\n4. In Table 1, the authors did not include the performance of some state-of-the-art methods, e.g. tLSTM [He et al. 2017], BN-LSTM [Cooijmans et al. 2016], and sTANH-RNN [Zhang et al. 2016]. All of them yield better accuracy than STAR on the pMNIST dataset. A minor problem is that Table 1 is not discussed in the text.\n[He et al. 2017] Wider and deeper, cheaper and faster: Tensorized LSTMs for sequence learning.\n[Cooijmans et al. 2016] Recurrent batch normalization. \n[Zhang et al. 2016] Architectural complexity measures of recurrent neural networks.\n\n5. On the gesture dataset, the authors might also compare the STAR net with [Zhang et al. 2018]."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces a new structure to help gradients propagate through the multilayer recurrent neural network.  The authors demonstrate the problems of backpropagation in RNN and LSTM analytically and numerically. Finally, the authors showcase the power of the their new structures on three datasets: pixel MNIST, TUM and Jester.\n\nThe paper should be rejected because: (1) The novelty of the paper is limited. The final recurrent unit looks very similar to GRU but the authors do not compare it with GRU in any of the experiments. (2) The calculation of the gradient norm is ambiguous. The weights are shared in different timesteps in recurrent neural network. But it is unclear that the authors show the gradient of loss function with respect to parameters on only one timestep or the sum of all time steps. (3) It is unclear why the depth is an important problem in recurrent neural network. Backpropagating through the layers of the unit should be similar to backpropagating through the timesteps.\n\nMain arguments:\n1. The authors start from the LSTM cell and analysis each part of it. Finally, the authors come up with a new structure (equations (10) (11) (12)). This new structure looks extremely similar to GRU (Gated recurrent units). The only difference is that the authors use another tanh outside the gated update in equation (12). However, the authors never compare it with GRU in all the experiments or even mention it in the main text. The only place to mention GRU is the appendix. From figure 6, GRU performed similarly as STAR in norm of gradients. But the authors never mention it in the main text.\n \n2. RNN parameters are shared across all the timesteps. If you calculate the gradient, g_w (equation 3), it should be a sum of all the timesteps \\sum_t(dh_t^l/dw)g_{h_t^l}. Even though the vanishing gradient problem only cares about one term in the sum above, it is hard to tell if the authors notice the difference. Moreover, it is hard to tell if the authors plot the norm of one term in the above sum or the whole sum. This should be clarified.\n\n3. The vanishing gradient problem is that gradient decreases exponentially with the timestep but the authors seem to misinterpret it as the gradient is small, which is incorrect. If the authors still want to show the vanishing gradient/exploding gradient, the authors would better show the log scale norms of the gradients in figure 2. From the current plot, I cannot see \"The gradient decreases exponentially with the timestep/number of layers\". They are all roughly in the same log scale.\n\n4. It is unclear why the depth is, in particular, a problem in recurrent neural network. Backpropagating through the layers of the unit should be similar to backpropagating through the timesteps. The maximum backpropagating length will be depth+length, which is just a longer sequence.\n\n5. Some parts of the analysis are unclear. Like:\na. In the last paragraph of page 4, \"sigmoid function causes...drop to 0.25\". I do not know why this sentence is important since there are multiple terms in the sum in equation (8). \"0.25\" may not make the whole gradient smaller.\nb. Not sure why the two Jacobians now has singular values equal to 0.5 in the sentence below equation (14).\n\nMinors:\n1. The last paragraph \"We note that...\" in the related work seems to be unrelated to the paper. I would remove it.\n\n"
        }
    ]
}