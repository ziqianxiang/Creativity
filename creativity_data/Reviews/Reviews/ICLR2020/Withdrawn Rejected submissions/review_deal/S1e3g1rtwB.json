{
    "Decision": {
        "decision": "Reject",
        "comment": "This manuscript investigates and characterizes the tradeoff between fairness and accuracy in neural network models. The primary empirical contribution is to investigate this tradeoff for a variety of datasets.\n\nThe reviewers and AC agree that the problem studied is timely and interesting. However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty of the results. IN particular, it is not clear that the idea of a fairness/performance tradeoff is a new one. In reviews and discussion, the reviewers also noted issues with clarity of the presentation. In the opinion of the AC, the manuscript is not appropriate for publication in its current state. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "General:\nThe paper proposed to use a causal fairness metric, then tries to identify the Pareto optimal front for the vectorized output, [accuracy, fairness]. While the proposed method makes sense, I am not sure what exactly their contribution is. It is kind of clear that Pareto optimal exists, and what they did is to run the experiments multiple times with multiple \\lambda values for the Chebyshev method and plot the Pareto optimal front. \n\nPro:\nRan multiple experiments and drew the Pareto optimal front for the considered dataset. \n\nCon & Question:\nThe so-called causal fairness metric does not seem to be any more fundametal than the other proposed metrics. It seems like they worked with another metric. \nAfter defining the fairness metric, everything else seems straightforward. Just use test (validation) set to estimate the accuracy & fairness, then plot the results on the 2d plane. \nCan we identify the Pareto optimal front without running all 1500 experiments? What happens when running a model takes long to train? Then, the proposed method cannot be practical. \nFigure fonts are very small and hard to see. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a novel joint optimisation framework that attempts to optimally trade-off between accuracy and fairness objectives, since in its general formal counterfactual fairness is at odds with classical accuracy objective. To this end, the authors propose optimising a Pareto front of the fairness-accuracy trade-off space and show that their resulting method outperforms an adversarial approach in finding non-dominated pairs.\n\nAs main contributions, the paper provides:\n* A Pareto objective formulation of the accuracy fairness trade-off\n* A new causal fairness objective based on the existing Weighted Average Treatment Effect (WATE) and Average Treatment Effect for the Overlap Population (ATO)\n\nOverall, I think the paper makes an interesting contribution to the field of fairness and that the resulting method seems quite attractive for a real-world practitioners. However, I found the writing / notation imprecise at times and the experimental section too small (lacking an extensive set of baselines, and only on two datasets). For these reasons, I give it a Weak Accept.\n\nSome feedback on notation / writing:\n* Typo on page 2, the loss L should be defined on X x Y and not Y x Y\n* In page 5, h is being used without being introduced first \n* the justification for using ATO in the internal layers of the network is a bit insufficient\n\nIn terms of suggestions, I think the experimental section needs to be extended and that the various modelling choices need to be explored and/or be further justified."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a method to approximate the \"fairness-accuracy landscape\" of classifiers based on neural networks.\nThe key idea is to set up a multi-dimensional objective, where one dimension is about prediction accuracy and another \nabout fairness. The fairness component relies on a  definition of fairness based on causal inference, relying on the \nidea that a sensitive attribute should not causally affect model predictions.\n\nI found the causal idea intriguing, since it makes sense that we don't want a sensitive attribute to have a causal effect.\nHowever, there may be several problems with this approach:\n\n1) For a causal estimate to be valid we need several assumptions. For example, we need A (the sensitive attribute) \nto be independent of potential outcomes conditional on X --- the so-called \"unconfoundedness assumption\" in causal inference.\nWe also need \"positivity\", i.e., that 0< P(A=1|X) <1. \nThese assumptions are not discussed in the paper. Furthermore, the particular context of the paper, where the treatment is actually an immutable characteristic, makes such discussion much more subtle. \nWhat will we do, for instance, if there are no A=1 in the sample when X = ...?\n\n\n2) The authors seem to assume that the propensity score model is well specified. This can be tested, e.g., using [1].\nWhat do we do when this fails? \n\n\n3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified.\nIn particular, its relation to \"fairness\" is never fleshed out, but just assumed to be so.\nThis can be problematic when, say, we are missing certain important X that are important for A. \nThen, there will be a measurable causal effect of A on h(). \n\nSome other problems:\n- What is the reason for focusing on 'neural classifiers'? There is nothing specific in the method or analysis \nthat relates to neural networks, except for the use of the causal estimand in a 'hidden layer'.\n\n- In the Introduction, the authors could cite the works of Amartya Sea, etc., on fairness. \nCertainly the study of fairness problems did not start in 2016.\n\n- What exactly is a \"sensitive attribute\"? If we don't want to bias our predictions, then why include it in the analysis?\n\n- It is unclear what is new and what is related work in page 3.\n\n- Sec. 4: The claim that \"causal inference is about situations where manipulation is impossible\" discards \nvoluminous work in causal inference through randomized experiments. In fact, many scientists would \nagree that causal inference is impossible without manipulation.\n\n- As mentioned above, why this particular estimand leads to more \"fairness\" is never explained.\n\n- Do we need a square or abs value in Eq (5)?\n\n- The experimental section is weak and does not illustrate the claims well. \n   It would be important to explain the choice of the particular causal estimand, the choice of the hidden layer to put the estimand in, to explore the choice of the objective, and so on. Currently, none of these choices/design aspects are being investigated.\n\n\n\n[1] \"A specification test for the propensity score using its distribution conditional\non participation\" (Shaikh et al, 2009)"
        }
    ]
}