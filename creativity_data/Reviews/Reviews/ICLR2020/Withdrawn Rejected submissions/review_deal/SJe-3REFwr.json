{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #4",
            "review": "The paper describes a variation of the Transformer sequence2sequence architecture for neural machine translation.\n\nThe proposed innovations are:\n1) moving the token-wise feed-forward blocks of the transformer to be in parallel with the self-attention blocks rather than interleaved between them as in the original architecture.\n\n2) combining in parallel the self-attention and FFN blocks with dynamic convolution blocks.\n\nThe methods are evaluated on WMT14 En->Fr, IWSLT2014 De->En and IWSLT2015 En->Vi.\n\nFor 1) a small improvement of 0.5 BLEU over the original Transformer is reported on a single task, with no significance analysis. Given that the improvement is small and results from a single experiment, it's not possible to draw strong conclusions from it.\n\nFor 2) the model shows strong improvements over the original Tansformer, but it's on pair with the Dynamic convolution Transformer of Wu et al. 2019 from which this work is based, despite having more parameters.\n\nOverall it's not clear from the reported evidence that the methods proposed in this paper represent an improvement over Wu et al. 2019.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The proposes a transformer layer that performs self-attention, feed-forward and dynamic-convolution all in parallel. The dynamic convolution consists of several different kernel sizes, which are then combined by learned weights. Experiments are done on three NMT tasks. \n\nIn terms of novelty, there is very little. The model essentially combines DynamicConv from Wu et.al with self-attention. More importantly, it's not clear why that's better. The only direct comparison to Wu et.al. is the EN-FR translation task, but the proposed model doesn't not outperform DynamicConv, even though having more parameters. \n\nAnother novelty in the paper is placing feedforward in parallel to self-attention, which seems to improve the performance. This is probably why the model is more stable compared to Transformer because it has essentially 2x less depth, while maintaining the same capacity. \n\nAs I read the paper, it felt bit rushed. For example, page 2 top says \"language modeling\", but I can't find any language modeling experiment. Figure 3 is never mentioned in the text. Also, the equation numbers in \"weight tying\" paragraph doesn't match, and Eq8 is missing an operator (probably ReLU).  In page6, the authors say \"DynamicConv diverges\", but I can't find that result. Also it said \"The small version of MUSE still beats Transformer by a large margin.\", but where is that result? From table 3, it looks like they match in performance.\n\nOther comments:\n- The authors claim their architecture allow the feed-forward module better access to token features, but I think that's only true for the first layer. Besides, Transformer has skip-connection that can directly feed token features to all layers.\n- Why W_in is tied to W^V? Why not W^K for example? \n- Why \\alpha is initialized to 1/n as if they are probabilities? When you pass \\alpha through softmax, any constant offset will have no effect on the output. \n- What are the kernel sizes used? \n- Type: \"... dataset, We ...\"\n- Figure 4 caption says \"dynamically\", but my understanding is that the kernel weights are just learned (adaptive) and doesn't dynamically change with inputs."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces a Multi-Scale attention model. The results are strong/competitive but the overall idea is highly incremental. In the author’s own words, the key contribution of this paper is “a concatenation of a self-attention network, a feed-forward network and a dynamic convolution network together”.  \n\nThe multi-scale here refers to global context and local context modeled by self-attention and convolution respectively.\n\nWhile I appreciate the engineering efforts in this paper to glue several recent neural components together, I really think there is a serious lacking of novelty in this work, e.g., gluing several known components together. The performance gains are also not particularly interesting since more parameters are introduced. Notably, this model performances on par with other models (vanilla Dynamic Conv) on WMT En-Fr and no results are reported on En-De.\n\nI think this work would make a good technical report and/or workshop paper. But the concatenating of several papers for a marginal performance gain probably will not warrant acceptance in a top DL conference. Overall, I am voting for a weak reject largely because of the lack of novelty.\n\nThat said, it is nice to see how dynamic convolutions and self-attention can work hand in hand for better performance. I am also curious about why the need for \"token-level\" features with position-wise FFNs, since Transformers are already composed of these.\n\nThe claim of trying to answer the question if \"is attention all you need\" is a bit of a stretch. I think opening with such a line and not really fulfilling the question can be seen as risky. I am not a fan of how the authors opened the paper.\n\nI appreciate the author’s honesty and actually not over-selling the work but at this juncture, I think the authors may want to investigate the possibility of both conv and attention modules working together with better synergy.  This might help nudge this paper over the current hurdle of lacking novelty."
        }
    ]
}