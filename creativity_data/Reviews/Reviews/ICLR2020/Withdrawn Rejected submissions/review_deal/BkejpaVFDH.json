{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduces the Affine Self Convolution that combines local connectivity, relative positional embeddings, and a convolutional filter. This form of local self-attention is spatially equivariant and is further extended to include rotational symmetries. Finally, a generalized form of Squeeze and Excite is derived that can be implied in a setting where the symmetries of some group need to be preserved. The experiments demonstrate that networks using affine self convolution use less parameters while attaining similar accuracies compared to using convolutional models.\n\nThe paper describes how to merge self attention with (depthwise) convolutions. This merge is executed in a sensible way and the derivations seems correct to me. The theory justifies that combined convolutions and attention could be beneficial and the experiments do verify that it is possible to train networks with affine self convolution to a reasonable accuracy. \n\nHowever, I recommend to reject this paper because:\n1) The paper does not clearly explain its contributions.\n2) The experiments do not conclusively demonstrate a significant practical advantage that justify the additional complexity.\n3) A significant part of the paper is dedicated to generalizing spatial equivariance to arbitrary groups, which seems unrelated to the contributions.\n\nMain argument\n\n1) The paper itself is not enough to understand how the proposed method compares to other local self-attention methods or adaptive filter methods. In the related work section various self-attention and data dependent filter methods are mentioned. It is unclear how the performance of those methods compares to the attention mechanism proposed in this paper.\n\nThe convolutional operation that is merged into the attention has a special form of sparsity because it is factorized in the feature dimension (depthwise convolutions). In convolutional models, depthwise convolutions have been used to increase performance while reducing parameter count. The paper only compares attention models with depthwise convolutions to CNN models with normal convolutions. It is therefore unclear what the increased parameter efficiently should be attributed to and whether the attention adds any benefit on top of the depthwise convolutions.\n\nFinally, it is unclear what the contribution is in the section on Group Squeeze and Excite (Group SE). It seems obvious that if an average over all spatial dimensions is taken as an input, the function will be invariant to any transformation of the spatial coordinate like translation and rotation. Therefore, the Group SE does not seem to differ in any significant way from normal SE. \n\n2) The reported accuracies on CIFAR are on par with the convolutional baseline. The authors argue that affine self convolution is more efficient in terms of parameters. However, as mentioned before this is not an apples to apples comparison because a method with sparse convolutions is compared to a model with dense convolutions. Other metrics that could give more information about the potential efficiency  of the method (flops, wall-clock time, etc.)  are lacking. There is also no details about how the affine self convolution can be implemented on modern accelerators which seems non-trivial.\n\n3) Due to the structure of the paper it is difficult recognize the contributions of the authors.  Local self-attention is derived within the context of an arbitrary group rather than just spatial transformations. However, it is unclear how generalizing the derivation to groups leads to any new insights. In fact, it seems obvious that any function that acts solely on a patch that is extracted using either spatial or rotational transformations will be invariant to those transformations. There is no special structure in the proposed attention mechanism that creates this symmetry or makes it easier to compute the function at every patch.\n\nPossible improvements:\n1) Re-organize the paper such that all group equivariance material is in its own section.\n2) Explain how the group equivariance affects the algorithm apart from operating on a different set of patches.\n3) Perform an ablation study. This might be somewhat complicated by the fact that two different methods are merged into one. Both the ablation towards vanilla self-attention as well as depthwise convolutions would be valuable.\n4) Provide more detailed information about the practical implementation of the attention mechanism: How efficiently can this be implemented on modern accelerators? How fast could it be in theory? What is the observed wall-clock time compared to CNNs?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose an operation that combines convolution and self-attention, which they call the Affine Self Convolution (ASC). This operation is used to replace the spatial convolutions in CNN architectures. The authors show that the proposed operation is translation equivariant and further introduce its roto-translational equivariant version. They additionally develop a group equivariant version of Squeeze-and-Excite. They conduct experiments on CIFAR10/CIFAR100 image classification and show some improvements compared to fully convolutional and fully self-attentional models (higher accuracy at a similar model size)\n\nI take several issues with the current draft.\n\n1) How is ASC different than MHA(Conv1x1(Q), Conv3x3(K), Conv3x3(V)) (where MHA is the multi-head attention function)? \nIt seems that the proposed operation is simply a convolution followed by self-attention (without batch norm or non linearity in between), since the affine map is simply a convolution with biases. This is obfuscated in the text.\n\n2) The experimental section is quite weak: \n- The authors do not compare against [1] which shares motivation with their work.\n- There is no mention of FLOPS/running times so it is hard to know the significance of the improvements, especially given the equivalence of ASC to MHA(Conv1x1(Q), Conv3x3(K), Conv3x3(V).\n- The authors only use the CIFAR dataset, which usually favors high regularization. This makes it hard to evaluate the significance of the roto-translational results.\n- The used architectures are far from SOTA.\n\n3) There are several issues with the claims made by the paper about related work:\n- \"We focus on self-attention in vision models, and we combine it with convolution, which as far as we know, are the first to do [...] While this is applied differently at each spatial location, we show that it is translation equivariant\". This is quite literally the focus of [1].\n- \"While there is work towards using attention in CNNs, the current models use them independently, sequentially, or in parallel.\" Given that ASC consists in applying convolutions and self-attention sequentially, I don't think this claim is valid.\n- \"Ramachandran et al. (2019), α(x, y) = softmax_y(score(f(x), f(y)+β(y−x))). It can also be added to the neighbors score, after computing the score function, as done in Bello et al. (2019); Hu et al. (2019), α(x, y) = softmax_y(score(f(x), f(y)) + β(y − x))\". Actually, Bello et al. introduced the former formulation which was subsequently used in Ramachandran et al.\n- \"In parallel, Ramachandran et al. (2019) also show improvements using local self-attention with local relative positional embeddings over Bello et al. (2019) which use global self-attention with global positional embeddings\". What improvements specifically? Bello et al introduce relative self-attention over images with content-to-position interactions and show that this can work in combination with convolutions or by itself. Ramachandran et al uses similar content-to-position interactions with local self-attention and focus on fully attentional models.\n\nIn summary, the proposed method is of limited novelty, the writing obfuscates the nature of the method, the experimental section is unconvincing and some related work is characterized wrongly.\n\nMisc\n- \"Regardless, they are faster than the attention based models.\" What does 'they' refer to?\n\n[1]: Attention Augmented Convolutional Networks, Bello et al. ICCV2019\n[2]: Stand-Alone Self-Attention in vision models. Ramachandran et al NeurIPS 2019"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\nThe paper introduces a novel layer type that fuses self-attention with convolutions. The mechanism (per pixel x and neighbouring pixels y in neighbourhood k) can be summarized as: 1) apply a different linear projection for each pixel y around x (there is an analogy to convolution) to get relative feature vectors for y wrt x; 2) add positional embeddings for each of the k elements in the kernel window (this is just a bias term for projection of 1), 3) apply local attention in kernel window around x. The only difference to pure strided, local self-attention is the fact that (K, V) feature vectors for all y are computed by a specific linear projection relative to x. This almost exact idea has been explored and is known as relative self-attention, see for instance [5]. The difference is that also V is subject to a relative linear projection. Local self-attention has also been introduced in various papers, e.g., [6]. The mathematical derivation of the idea is interesting, but at the same time suggests that the authors are doing something completely novel. I also feel that the complexity of their derivation is not warranted by the complexity of the actual mechanism. This might be misleading and hence makes it harder to establish the mentioned connections to prior work (e.g., relative self-attention). Finally, the presented results are not convincing at all. The approach is only tested on cifar and compared to a fairly poor baseline. There are no comparisons to empirical results from the literature, either. The proposed approach performs only marginally better at best.\n\nGiven that my assessment of the methodology is correct (please correct me if I am wrong) leading to a lack of novelty and the lack of strong empirical results, I vote to reject this paper.\n\n\nMajor issues:\n- Rephrase the claim to be the first to combine convolutions and self-attention as there have been multiple works on this subject before, e.g. [1], [2], [3], just to name a few. Maybe the first to fuse ideas of convolutions and self-attention into a single layer.\n- The derivation of their mechanism is introduced in a much more complicated way than necessary.\n- The authors basically introduce local, relative sel-attention \n- There is a complete lack of acknowledging prior works, e.g. [1-6]\n- Only marginal improvements over a single, not very storng baseline.\n- No empirical comparison to literature.\n\n\n[1] Attention Augmented Convolutional Networks. Bello et al 2019\n[2] Convolutional self-attention network. Yang et al 2018\n[3] QAnet: Combining local convolution with global selfattention for reading comprehension. Yu et al 2018\n[4] Pay Less Attention with Lightweight and Dynamic Convolutions. Wu et al 2019\n[5] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.  Dai et al. 2018\n[6] Image Transformer. Parmar et al. 2018\n"
        }
    ]
}