{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper considers the problem of reinforcement learning with goal-conditioned agents where the agents do not have access to the ground truth state.  The paper builds on the ideas in hindsight experience replay (HER), a method that relabels past trajectories with a goal set in hindsight.  This hindsight mechanism enables indicator reward functions to be useful even with image inputs.  Two technical contributions are reward balancing (balancing positive and negative experience) and reward filtering (a heuristic for removing false negatives).  The method is tested on multiple tasks including a novel RopePush task in simulation. \n\nThe reviewers discussed strengths and limitations of the paper.  One strength was that the writing was clear for the reviewers. One limitation was the paper's novelty, as most of these ideas are already present in HER with the exception of reward filtering.  Another major concern was that the experiments were not sufficiently informative.  The simulation tasks did not adequately distinguish the proposed method from the baseline (in two of the three tasks) and the third task (RopePush) was simplified substantially (using invisible robot arms).  The real world task did not require the pixel observations.  The analysis of the method was also found to be somewhat limited by the reviewers, though this was partially addressed by the authors.\n\nThis paper is not yet ready for publication since the proposed method has insufficient supporting evidence.  A more thorough experiment could provide stronger evidence by showing a regime where the proposed method performs better than alternatives.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "### Summary ###\n\nIn this paper, the authors focus on the problem of goal conditioned reinforcement learning. Specifically, the authors consider the setting where the agent only observes vision as input and the ground truth state is not observable by the agent. In this setting, it is hard to specify a reward function since the reward function has to compute rewards from images.\n\nThe authors first consider the setting of using a proxy reward function, and argue that false positive errors in the proxy reward function hurt the policy training significantly more than false negative errors. The authors demonstrate this empirically in a simple simulated robot arm reaching setting. Then the authors propose to use an indicator reward function that eliminate all false positive errors. The authors combine the indicator reward function with a mixture of goal relabeling schemes and a heuristic way of filtering out data with false negative rewards.\n\nThe authors evaluated the proposed method on 3 simulated robotic manipulation environments and one real robot environment. The results presented by the authors suggest that the proposed method performs better than baseline methods.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in learning goal conditioned policies from vision. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\n\nPro:\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative explanations and empirical evidence to support the crucial assumption that false positive rewards are worse than false negative rewards. The results for the main experiments are also easy to understand.\n\n2. The paper includes a fairly comprehensive set of ablation studies for each part of the proposed method in the appendix. The ablation study clearly illustrated the effects of balancing different goal relabeling schemes and filtering transitions.\n\n\nCon:\n\n1. I’m not convinced about the magnitude of novelty in this paper. The indicator reward has already been used in HER[1], and the balancing of relabeling schemes seems like a direct extension of the various relabeling schemes proposed in HER. It seems to me that the only novelty of this paper comes from the filtering techniques for false negative rewards, which I do not think is enough for this venue.\n\n2. The experiment results are not very strong for the proposed method. In two of the three simulated robotics environments, the proposed method performs similarly to the indicator + balance configuration, which in my opinion is only a slight variation of HER. Therefore, I do not find the claimed advantage of the proposed method to be convincing.\n\n\nThe idea in the paper is well presented and carefully investigated. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement. Therefore, I would not recommend acceptance before these problems are addressed. \n\n\n\nReferences\n[1] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Advances in Neural Information Processing Systems. 2017.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper tackles the problem of self-supervised reinforcement learning through the lens of goal-conditioned RL, which is in line with recent work (Nair et al, Wade-Farley et al, Florensa et al, Yu et al.). The proposed approach is a simple one - it uses the relabeling trick from (Kaelbling, 1993; Andrychowicz et al., 2017) to assign binary rewards to the collected trajectories. They apply two simple tricks on top of relabeling:\n\n1. Reward balancing: Balancing the number of 0/1 rewards used for training the policy.\n2. Reward filtering: A heuristic that rejects certain negative-reward transitions for learning if the q value for the transition is greater than some threshold q_0.\n\nWhile I like the simplicity of the proposed approach when compared to competing methods, my overall recommendation is reject based on the current state of the paper, because of the following reasons:\n\n1. The technical novelty is quite limited - the paper mostly uses the framework from Andrychowicz et al., 2017 with a specific choice of epsilon (=0) for giving positive rewards. The method does reward balancing, but similar goal-sampling have been used in prior work like Nair et al., 2018, and is not essential to obtaining good results (Appendix C). The main technically novel component is likely the reward filtering mechanism, but I find it to be somewhat ad-hoc since it assumes that the Q-values learned by the Q-network to be reasonably good during training time, which is not the case for most modern Q-learning based methods [1, 2]. \n2. The provided analysis is not particularly illuminating, see my detailed notes below.\n3. The experiments are underwhelming, see my detailed notes below. \n\nI would be willing to overlook 1 or 2 if the authors did a more thorough experimental evaluation which showed the method working well when compared to alternatives, but that is not the case right now. \n\nNote on Section 6 (analysis) \nThe authors provide a simple analysis in Section 6 to bound the suboptimality of the learned policy. Unless I’m missing something, the resulting bound of t_3 <= t_1 + d is trivially true, since d is defined to be the diameter of O_{+}(o_g), and t_1 is the number of timesteps taken by an optimal policy to go from o_t to O_{+}(o_g). As a result, I don’t find this analysis illuminating or interesting - perhaps the author can provide counter arguments here to change my mind. \n\nNote on Section 7 (experiments) \nFor sim experiments, two of the tasks are extremely simple (free space reaching in 2D and 3D, respectively) where essentially everything works - the proposed method, baselines and ablations. The third task of rope manipulation is fairly interesting at a first look - but it appears to have been greatly simplified. The authors consider a high-level action space and an episode of only three timesteps. Further, the authors make the simulated robot arm invisible in the supplementary videos, which greatly simplifies the problem visually. Since the entire motivation is about learning well in real world settings, I feel this is a bit underwhelming. Figure 1 is misleading, since it shows a visible robot arm in front of a rope. This also appears to hint that the method did not work well with realistic visuals, highlighting a major limitation of the proposed approach. I think it would be valuable to include such failures (and discussions around them) in future submissions. \n\nFor the real world experiments, the task being considered is extremely simple (free space reaching), and does not even require pixel observations. Even for this simple task, the error achieved by the method is 10cm (starting error was 20cm), which is quite poor - robotic arms like the Sawyer should be able to achieve much lower errors. Even the oracle reward achieves an error of 10cm, which might indicate a bug in the author’s real world robotic setup. In comparison, prior work such as Nair et al. is able to tackle harder problems in the real world (like non-prehensile pushing). \n\nMinor points\n- Section 4 contains a nice discussion on false positives and false negatives when using non-oracle reward functions for reinforcement learning, where they also perform a simple experiment to show how false positives can negatively impact learning much more severely than false negatives. This does a good job of motivating the method (i.e. avoiding false positives), but also undermines the motivation behind reward filtering, which is perhaps the main technically novel component of the proposed approach. \n- Section 2.3 (i.e. related work on deformable object manipulation) states that \"Our approach applies directly to high-dimensional observations of the deformable object and does not require a prior model of the object being manipulated.”, and only cites prior work that assumes access to deformable object models. However, there is recent work that enable similar manipulation skills without access to such models. For example, Singh et al. [3] are able to learn to manipulate a deformable object (i.e. a piece of cloth) directly from high-dimensional observations using deep RL in the real world, and do not require object models (or ground truth state), but do require other forms of sparse supervision.\n- Typo: On page 7, “is kept the same a other approaches.” -> “is kept the same as other approaches.”\n\n[1]: Diagnosing Bottlenecks in Deep Q-learning Algorithms. Fu et al., ICML 2019\n[2]: Double Q-learning. V. Hasselt. NIPS 2010\n[3]: End-to-End Robotic Reinforcement Learning without Reward Engineering. Singh et al., RSS 2019.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors propose to apply HER to image-based domain, assigning rewards based only on exact equality (and not using an epsilon-ball). The authors also propose to (1) filter transitions that are likely to cause false negative rewards and (2) balance the goal relabeling so that the number of positive and negative rewards are equal. The authors demonstrate that these two additions result in faster and better learning on a simulated 2d and 3d reaching, as well as a rope task. The authors also show that the method works on training a real-world robot to reach different positions from images.\n\nOverall, the paper is a fairly straightforward and simple extension of HER, and the proposed changes seem to result in consistent improvements. Studying the importance of false positive vs false negative rewards is an interesting problem, but some of the details of the analysis are missing. Most of the writing of the paper is relatively clear, but there are a couple of assumptions that should be discussed more clearly. The analysis section seemed particularly confusing, as it introduces new notation and concepts (D without a subscript, goal observation set diameter) without explaining their significance. The experiments are relatively simple, making it difficult to judge whether or not the method will work on more challenging domains.\n\nIn more details:\n1. The related works section feels unnecessarily long. \n\n2. The analysis section is quite confusing to me and I do not understand its significance. It seems to say that reaching og rather than O+(og) is not very bad, if O+(og) is small. While this makes sense, it's not clear how the derivations of the paper add to this intuition. In particular, the paper derives an upper bound for t3/t1, which equals\n\n time to reach og / time to reach O+(Og)\n\nWhy is this \"an upper bound on the suboptimality of the policy\"?\n\n3. Can details of Figure 2 be given? This seems like a very important experiment, so it is a shame that it is not discussed or analyzed in detail.\n\n4. An assumption that is critical to this method's applicability is the assumption that no two states will receive the same observation. Without this assumption, the statement, \"the reward is positive only if ot+1 = og, which implies that f(ot+1) = f(og), or equivalently, st+1 = sg\" is false, and so therefore the statement, \"It should be clear that this reward function will have no false positives\" is also false. I do think that this is a reasonable assumption to make for some domains, but the authors should make that explicit.\n\n5. For the \"Indicator\" baseline, what is the probability used for relabeling with a future state? I assume p1=0, but what are p2 and p3?\n\n6. The ablations (Section C, Figure 5) indicates that only the filtering matters. I don't think this detracts from the author's submission, but it would be good for the authors to highlight the main paper. Currently, the statements, \"These experiments show that **both** Balance and Filter are important for optimal performance across the environments tested.\" seems unsubstantiated by the experiments--only filtering seems important.\n\n7. The authors should clarify the difference between the \"true reward\" and the \"original reward.\" What is the relationship? The last paragraph of Section 4 discusses false positive rates and says \"true reward is defined by O+(og) = {o | ||o − og||2 < epsilon}.\" However, two paragraphs earlier, the paragraph also discuses false positive rewards and says, \"under the original reward function r(st, sg).\" The experiment section then says \"Oracle uses the ground-truth, state-based reward function.\" Is the assumption that the \"true\" reward (defined in image space) is a perfect representation for the \"original\" reward (defined in state space)? Does this method work if this assumption is violated?\n\n8. (very important) How sensitive is the method to q0, and how was it chosen for the experiments?\n\n9. Figure 6 in the Appendix is very difficult to read.\n\nI would be inclined to raise my score if the authors\na. clarified some questions above\nb. conducted ablations to study the importance of q0, the threshold\nc. added experiments on more challenging domains (e.g. cube pushing, pickup) that demonstrated the effectiveness of the method on more domains"
        }
    ]
}