{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The submission is proposing an importance weighting based transfer learning algorithm in which the importance weights are learned using reinforcement learning. The reward function used in the work is the evaluation accuracy. Moreover, the learned parameters are class-based importance weights.\n\n* Novelty *: The paper is somewhat novel. The idea of re-weighting source dataset for a better transfer has been known, and used in the literature of domain adaptation well over a decade (eg. Daume&Marcu JAIR 2006). The main contribution of the paper is learning these weights using reinforcement learning. However, a similar approach of learning to re-weight examples using validation rewards has been published by (Ren et al, ICML 2018). Hence, only novel contribution is extending these meta-learning the weights idea into transfer learning setting. In other words, submission combines existing ideas in a sensible way. This is definitely interesting. On the other hand, it is also important to note that the novelty is very-limited. \n\n* Experimental Setup *: Authors perform very extensive collection of experiments. Unfortunately, the results are neither conclusive nor complete due to following issues:\n\n1)  It is a widely known fact that reinforcement learning algorithms have huge variance. Especially, the REINFORCE method has no internal variance reduction mechanism. Considering this potential very high variance, all tables and figures need sensitivity analysis. Authors state that the experiments are run with 3 random seeds. Not reporting variances and min/max values of obtained results is clearly unacceptable. Considering the fact that all of the margins are very low, I do not find any of the results conclusive without a sensitivity analysis. More importantly, a separate sensitivity analysis to the hyper-parameters would also be very beneficial since RL is typically very hyper-parameter dependent. \n\n2) Baselines are incomplete. The paper is performing various experiments on both transfer learning and few-shot learning. The experiments do not include any few-shot learning baselines. Instead, it only compares with fine-tuning. Hence, the conclusion stated in the paper \"...The results show that L2TL can yield significant improvements in real-world tasks where the number of training examples are limited...\" is not really warranted. Transfer learning experiments are also missing various baselines. A simple baseline would be metric learning as demonstrated by Scott et al in NeurIPS 2018 (Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning). They show that fine-tuned metric learning is a strong transfer learning baseline. Comparing only with basic fine-tuning is definitely incomplete empirical design.\n\n* Is RL really needed? * To me, using RL for learning the weights is the real contribution of the paper. However, it is not really evaluated. There are various ways to pose this meta-learning problem and RL is only one of the ways. There are various existing methods which the paper can compare with in order to justify the RL decision. A comparison with (Ren et al., ICML 2018) is the most obvious one and can be applied in a straight-forward manner. One can replace the validation set (which is used for meta-training) with the target validation set and simply apply their method. Similarly, (Deep Bilevel Learning by Jenni et al, ECCV 2018) can be applied in a similar fashion.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary & Pros\n- This paper proposes a transfer learning approach based on fine-tuning and reinforcement learning. This approach adaptively computes the importance weights of classes in the source dataset, for maximizing the transfer effect.\n- The authors consider the importance weight as an action, and evaluation on the validation set as a reward. Using this formulation, they learn a policy network (i.e., importance weight) via policy gradient. This scheme does not require explicit similarity metrics between source and target samples.\n- The proposed method outperforms fine-tuning and DATL in several settings.\n\nConcerns #1: Verification of RL-based approach\n- Policy gradient algorithms should update policy networks to maximize the Q-value function. However, the proposed method considers maximizing only an instant reward at the current iteration in a greedy manner. Also, a number of episodes are typically required for learning while the proposed method considers only one episode. Thus it is hard to guarantee that policy gradient can find a good policy.\n- I think this method is more related to gradient approximation of objective (R) than RL because the advantage A_t can be considered as R(theta_t+1)-R(theta_t), so the gradient might be approximated as A_t*dP(lambda)/dtheta_t.\n- In Algorithm 1, computing lambda is deterministic. However, REINFORCE is based on stochastic policy. So I wonder how to apply REINFORCE into the proposed framework more precisely.\n- Detailed ablation studies on RL-based importance weights should be provided to demonstrate the effectiveness of RL, e.g., weight distribution while training, or using multi-step rewards instead of instant (or one-step) rewards.\n\nConcerns #2: Novelty of the proposed method\n- I think the novelty seems to be limited because the proposed method just trains class-wise importance weights using the validation set.\n- For training the weights, the proposed method uses REINFORCE without any modification to specialize this problem, thus I think this paper has a limited contribution.\n\nConcerns #3: Incremental improvements\n- The performance gains of L2TL over fine-tuning seem to be incremental even many samples in the source dataset are irrelevant to the target task (Table 1 & 4). I think ImageNet has many irrelevant samples to the fine-grained tasks, e.g., ImageNet has a small number of bird-related class.\n- For each target task, the proposed method requires all samples (>100GB) in the source dataset while fine-tuning just needs pre-trained models (< 100MB). Despite the high complexity, the performance gain is relatively small.\n- When fine-tuning, a regularization technique can also provide meaningful improvements on such fine-grained datasets, e.g., MaxEnt [1].\n- Overall, to prove the effectiveness of the proposed method, I think it should provide more improvements.\n\n[1] Dubey, Abhimanyu, et al. \"Maximum-Entropy Fine Grained Classification.\" Advances in Neural Information Processing Systems. 2018."
        }
    ]
}