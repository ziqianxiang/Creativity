{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes and evaluates using graph convolutional networks for semi-supervised learning of probability distributions (histograms). The paper was reviewed by three experts, all of whom gave a Weak Reject rating. The reviewers acknowledged the strengths of the paper, but also had several important concerns including quality of writing and significance of the contribution, in addition to several more specific technical questions. The authors submitted a response that addressed these concerns to some extent. However, in post-rebuttal discussions, the reviewers chose not to change their ratings, feeling that quality of writing still needed to be improved and that overall a significant revision and another round of peer review would be needed. In light of these reviews, we are not able to recommend accepting the paper, but hope the authors will find the suggestions of the reviewers helpful in preparing a revision for another venue. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose a soft semi-supervised learning approach on a hypergraph. On the one hand, the vertex labels should be not only numerical or categorical variables but also probability distributions. On the other hand, hypergraphs provide a much more flexible mean to encode the real-world complicated relationship compared to the essential pairwise association. Specifically, the authors carefully obtain the generalization error bounds for a one-layer graph neural network. The experiments support the theory and provide the empirical verification of the proposed method. The appendix offers plenty of details that are helpful for the reader to understand both the theoretical and practical perspectives of this paper. Also, the code looks good to me. The structure of the provided codebase is clear and well documented.\n\nI have some questions and suggestions for the authors:\n1. In the method section 3.3, the authors said that “A key idea of our approach is to treat each hyperedge as a vertex of the graph” After this transformation, the graph could be super sparse. So I’d like to know more about the efficiency of the proposed method. Because the datasets in this paper could not be considered as a giant graph. The efficiency could be much more critical for large-scale applications. Besides, such conversion could be one of the most significant technical novelty in this paper, which makes me worry about the methodology contribution of this submission.\n\n2. In the theoretical analysis section 4, there are lots of references for the existing lemmas and theorems. It could be much better if the authors could briefly review and summarize these equations before applying them. By the way, I appreciate the detailed appendix at the same time. But such a summary could complete the paper in a more self-contained way. Also, the authors should improve the writing of paper at the same time.\n\n3. In the experiments section 5, I wonder if the authors would consider some more challenging datasets with the larger graph to evaluate the proposed method. Also, the theoretical analysis is about one-layer networks, which looks technical sound to me. However, in practice, we could not only use the one-layer network for graph classification, even for a small graph. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Contributions:\n1. This paper proposes a semi-supervised graph neural network method for hypergraphs.\n2. A generalization error bound was proposed adapted to the semi-supervised setting with Wasserstein's loss.\n3. Empirical results demonstrate the effectiveness of the proposed method.\n\nThe algorithmic contribution of this paper is clear: it proposes a new network architecture that (1) initialize latent features H^{(0)}_E for hyperedges from a \"hyperedge graph\" and (2) learn latent features for each node and hyperedge using a GCN type network. Since the latent features are formulated as discrete distributions, a Wasserstein distance can be applied for training with the sinkhorn approximation algorithm.\n\nI think the weakness of this paper is two folds, which makes it not ready to publish. First, the paper claims that the performance gain results from the exploitation of directed hyperedges. This is reasonable if I barely see the results for Soft-DHN. However, I find the design of the hyperedge GNN to be fragile regarding the number of layers from the result in Table 4. Normally it is reasonable to use a two-layer GNN, but the result is very bad when doing so (see Soft-DHN 2 layers result in Sec. A.4.1). Also, the result of the proposed model is still good when not applying a hypergraph GNN. So I'm confused about where the performance gain comes from.\n\nAnother weakness is the paper writing. Below are a few comments:\n[Page 3, Sec. 3.1] By saying \"t\\neq\\Phi\", do you mean \"t\\neq\\emptyset\"?\n[Page 3, Sec. 3.2] Please introduce the notation (M, C) right after it first appears in the third line of Sec. 3.2.\n[Page 3, Sec. 3.2] The notation n,m are confusing. Do you mean m=|E|?\n[Page 3, Sec. 3.2] I cannot see how Z=h(\\mathcal{H},X_V,X_E) maps each vertex to a probability distribution. Do you mean each row of Z is a probability distribution for each vertex?\n[Page 4, Sec. 3.2] Please define E_D. Is E_D the same as E_d?\n[Page 4, Sec. 3.3] Be explicit by saying \"approximate the hypergraph by a suitable graph\", what do you mean by \"suitable graph\"? Does it mean use cliques in place of hyperedges?\n[Page 4, Sec. 3.3] t=0,...,\\tau-1\n\nSince the writing significantly affects the paper readability, and the core contribution of the paper seems incremental, I will vote for a reject to this paper."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This work explores hypergraph-based SSL of histograms. DHN enables soft SSL learning based on the existing tools from optimal transportation. The idea of treating hyperedges as vertexes of another graph is novel and the theoretical analysis is sound.\n\nHowever, the paper has the following issues:\n\n1) The writing is poor, especially about the reference, and the description of how and why DHN works (section 3.3).\n\n2) The novelty is restricted. It seems that the only innovation is introducing the information from the hyperedges into $H_E^{(0)}$.\n\n3) Though the experiments on Cora and DBLP have revealed the superiority of DHN, the authors still need a more thorough empirical evaluation on some challenging benchmarks to draw the conclusion.\n\nI'm willing to increase my score if the concerns are addressed.\n"
        }
    ]
}