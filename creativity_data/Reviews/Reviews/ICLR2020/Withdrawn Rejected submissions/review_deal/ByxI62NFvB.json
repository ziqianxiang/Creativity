{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors introduce an interactive attention learning framework called Neural Attention Process (NAP) which produces weight predictions over the input variables in addition to the usual target predictions. The motivation behind this type of model is to obtain an interpretable explanation of the model’s output which can be shaped by a small set of human expert annotations. At a higher level NAP seems to be an extension of a previously introduced model called RETAIN with an additional component inspired by Neural Processes (NPs) which incorporates the annotations of the experts. The authors test their model on five different tasks from 3 datasets (health records, fitness and real estate) and compare to a number of related baselines.\n\nWhile I think the idea is interesting, in my opinion this paper is not ready for publication and I would argue for it to be rejected, for the following reasons:\n\n* Presentation: The paper is over badly written, sloppy and not well structured.\n        * Badly written: there are lots of grammatical errors in the paper to the point that it severely affects its clarity (for example, the subject of a sentence often changes from plural to singular or vice versa within the same sentence and it’s unclear what the authors are referring to). \n        * Sloppy: many variables are not defined in the paper, and some are called different names depending on the figure or part of the text (i.e. the use of ‘s’ or ‘t’ interchangeably to refer to the iterations). At some point even the name of the model (referred to as NAP for the majority of the paper) is just referred to as NP-Selective in the text and some figures (compare nomenclature in table 1 vs figure 5). This lack of attention to detail (although the name of the main model should hardly count as a detail) makes me worry about the scientific rigorousness of the work. \n        * Not well structured: The paper is not very easy to follow as there are jumps between different models (RETAIN, NPs, NAPs) which are not always evident. On that note, I would have liked to see an objective function to know what exactly is being used to optimise theta.\n\n* Significance: Overall the results don’t seem incredibly significant (e.g. figure 6). If one of the main claims is that the number or annotated examples is smaller with NAP the authors should provide experimental evidence that this is the case (as this would be one of the most significant benefits of the model).\n\n* Motivation: a key motivation for this paper, as stated by the authors, is the desire to obtain interpretable representations, which in itself is an important research problem. However, I am not sure that the suggested solution is addressing this issue. If I understand correctly, what the model learns is, in essence, a weight mask over the inputs that determines how much each of the dimensions of the input contribute to the output. This could be achieved with any attention model in and is in itself not novel. I think a more interesting motivation for this model might be its ability to incorporate some human annotations that teach the model where to attend. However if this was the main story I think the authors should focus on making this a stronger argument and proving that narrowing down the attention with said annotations actually leads to better/faster/more robust learning, rather than focussing on the interpretability narrative.\n\nSome more specific comments and recommendations for the authors (I think there are some positive aspects of this paper and I hope that I can give a few points of advice to help improve its quality.) I will go through them in the order of the paper:\n\n1) The name Neural attention process might be a bit confusing given that there are already Attentive Neural Processes. I would strongly recommend changing the name.\n2) Check the language and make sure it’s correct, the introduction is particularly full of errors and you don’t want to lose the reader from the beginning.\n3) If you are talking about interpretability you would have to mention as well all the work on interpretable representations (e.g. disentangled representations etc) and if you talk about attention, it is probably worth mentioning transformers.\n4) Algorithm 1: this is not very clear. I would define the variables at the top that are not self-explanatory (such as S). Also, from the look of the algorithm theta is not updated in the else loop.\n5) The change in notation is very confusing. It’s not always clear when you use ‘v’ vs ‘l’ or ‘s’ vs ‘t’. Sometimes it seems it depends on which of the different models you are talking about, but either way you should try to make it all uniform.\n6) I would strongly recommend to specify the loss function and the regularisation term as this will make the task much clearer to the reader.\n7) I would make the difference to RETAIN very clear and argue why you think your addition improves on the original (from the text at the moment it’s not even clear when you are describing RETAIN and when you talk about your model, maybe making a subsection would help?)\n8) Figure 3 is too small.\n9) You claim to have 5 datasets, but the way I would interpret your setup, you have 5 tasks from 3 datasets.\n10) I would say a sentence or two about why you chose the baselines you chose and why you are hoping that they prove your different points.\n11) Figure 6 has no axis labels on the plot itself, which are definitely necessary. I also recommend not using red and green as the two colours as it will be hard for colour blind to distinguish between those two.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "1. Summary: In this work, the authors proposed a coset-effective interactive machine learning framework that leverages the attention mechanism and neural process. The proposed framework can reduce the amount of time used by human annotation and avoid retraining of neural the neural network. They conduct experiments on three datasets from different domains to demonstrate the effectiveness of their model. Extensive analysis from different angles is also included in this work.\n2. Overall: It's an interesting paper to read. It's well written and easy to follow. However, the idea seems to be pretty incremental as it stacks multiple existing techniques together without many innovations. Meanwhile, more experiments over large and more complicated datasets should be added to strengthen the experiments of this work.\n3. Comments:\n3.1 Figure 4 needs some more explanations. Do you sum up the attention of each feature belonging to different types of instances(false positive, false negative, etc)?\n3.2 It seems to me that the Fitness data and Realstate sales transactions dataset do not naturally have very clear attention distribution as the prediction is affected by multiple factors in a complicated way. How do the annotators assign annotations to these datasets?\n3.3 More description of the human annotation part needs to be added. How many annotators do you use? How do you deal with disagreement? How consistent are human annotations?\n3.4 Why focus on the time series prediction problem? It seems to me that this framework can be applied to any kind of problems. Is there any specifically reason the authors only focus on the series prediction problem? Meanwhile, it looks these three datasets do not have very rich sequence information as they can be very sparse and data points can be far away from each other in terms of time. For the realstate sales transaction dataset, do many houses have transactions every year, which seems not so realistic?\n3.5 Only selective and randomness are compared in Figure 6. How about other strategies? One relevant problem is that not many other interactive/active learning strategies are compared in this work."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "# Summary\nThe paper presented an interesting workflow, where an attention-based model, once initially trained from a dataset, is to be successively iteratively updated based on explicit \"corrections\" on the attention map received from domain experts, and where such expert annotations (in regards to them being directed at which samples and which attention positions) are also to be selected interactively based on the current model. The overarching goal here is to 1) make efficient use of the (potentially) sparse expert annotations (on attention maps) to continuously improve the end model; 2) learn a meaningful attention map to help with increasing model interpretability; and 3) achieving 1) and 2) while minimizing annotation costs.\n\n# Conclusion\nOverall, this is an interesting setting and the technical solutions presented in the paper look reasonable in principle. However I find it a bit hard to advocate acceptance just yet, mainly because of two concerns that a) this paper is largely about stitching multiple existing techniques (e.g. Neural Processes, Influence functions, Monte Carlo dropout, etc.) together and may hence be a little bit lacking in its own originality, and b) both the presentation quality and the empirical studies still leave quite a bit to be desired (details to follow).\n\n# Detailed comments\n1. On using influence functions for annotation candidate selection (Eq.6) - why taking the \"absolute value\" of the influences rather than simply sticking with the raw values? I would have thought the sign of the influence to be meaningful and you would actually like to focus on those that have the most negative influences (as per the text)?\n\n2. I find it a bit strange that, across the entire paper including the supplementary materials, nowhere is it explained what exactly is the loss function being used in this paper. And to add to the confusion, the critical NAP process itself is also not given a proper introduction, e.g. what's the ELBO in this case and how is it related to the original loss function L? Questions like such left unanswered, it makes one further wonder why would samples selected using influence functions (based on the original loss L) even be helpful during the iterative refinement stage (which supposedly follows NAP and uses a different loss).\n\n3. What's the rationale behind selecting \"top P\" validation points rather than just using all of them?\n\n4. In the text it says \"- an embedding network ... embeds x into v\", but in Figure 2.A-1, it seems like it's mapping x to \"l\"?\n\n5. In the caption of Table 1, it says \"four electronic health records datasets\" but the number seems to be three everywhere else?\n\n6. \"s^{valid}\" looks like a single point, maybe use curly fonts for sets like this?\n\n7. I probably wouldn't call the performance difference in Table 1 and 2 \"significant improvement\".\n\n8. I also find the naming of the methods and baselines in the Experiments section a bit confusing. It's very hard for me not only to remember what each method is about from their names, but also to contrast them in a meaningful way for an implicit ablation study.\n\n# Typos (an incomplete list)\nretrainig, tihs, proces, native (should be naive), depeding, veriables"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Cost effictive attention\n\nThe paper introduces an approach to effectively exploit human annotated attention maps using neural processes and select the most useful training instances to collect such annotations for using importance functions. Experiments show that the methods indeed lead to improved quantitative and qualitative results on 3 datasets.\n\nOverall, I like the ideas presented and the high level approach (importance functions and neural processes for active learning). However, I am a bit troubled by the datasets the methods are applied on. These seem overly simplistic to me. The input domain consists of a few variable classes (<100). I believe for the given tasks, the model has to mostly focus on a specific type of variable (e.g., smoking or blood pressure for classifying CVD) without really considering the other variables. A few expert annotations would be enough for the model to figure that out. E.g., one simple baseline would have been to use annotations of random examples apriori to select only the most annotated variables for each dataset and then train the model only on those. I think this willl lead to similar if not more improvements as it removes most confounders completely apriori. Finally, given the rather marginal improvements I am not sure the approach has a really big impact, other than eliminating some of the confounders.\n\nI am also a bit confused by the choice of architecture (attentional LSTMs) for non-time series data and the lack of explanation how and why these are used for the different datasets. I am also not sure about the practicality of the approach given the computation of the Hessians for the importance functions. I would like this to be discussed by the authors.\n\nAlthough the paper definitely has some interesting contributions, given the aforementioned considerations especially wrt. the experiments (datasets) I am leaning towards rejecting this paper in its current form.\n\n\nStrengths:\n- General methodology of using importance functions to select examples for labeling is promising.\n- Application of neural processes to avoid costly retraining.\n- Interesting results suggesting that importance weighting works reasonably well for the tested datasets, and that the suggested neural process indeed gives the model additional performance improvements.\n\nWeaknesses:\n- Datasets are very uncommon. The input domain is typically very simplistic (e few types of variables), as opposed to more general data types such as text, images, etc.\n- There are no baselines from the literature to ensure that the models used are somewhat competitive to state-of-the-art.\n- The results are not very convincing. Improvements are rather small, given the increased complexity of the overall approach.\n- Practicality: Computing Hessians for typical models  with millions of parameters is not practical. No solution or discussion about practicality is done.\n\nComments:\n- \\alpha_{var}  --> I would not call it var but state\n- Figure 1 could benefit from a description in the caption.\n- Use \\appendix before starting the appendix.\n- There should not be references after the appendix.\n\nQuestions:\n- Why do we do attention over the in RNN inputs (v) and not their outputs, as it is typically done because those contain contextual information?\n- What's g and h from the LSTM? The output and the internal state of the LSTM? If yes, typically only the output of the LSTM is used and not also the internal state.\n- Isn't the computation of the Hessian too expensive?\n- How is the LSTM applied to the different inputs of the very different types of datasets? The datasets are not time series, so why was a LSTM applied in the first place?"
        }
    ]
}