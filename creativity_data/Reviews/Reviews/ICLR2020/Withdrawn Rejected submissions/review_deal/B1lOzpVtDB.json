{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a method for using a knowledge graph for response generation (e.g., for dialogue-type tasks). The model maps words from the input utterance to the concepts/nodes in the knowledge graph. The model then uses an attention mechanism to condition on relevant parts of the graph (e.g., nodes in the vicinity of the concepts identified in the input) and the input utterance to generate a response.\n\nI found the problem tackled in this paper to be very interesting.  Incorporating external knowledge seems essential for tasks such as dialog and Q&A. The model also seems to perform fairly well compared to the proposed baselines. However, I found that the ideas in the paper could be better exposed. In particular, I found the description of the model to be quite difficult to follow (please see detailed comments below). I also found that seemingly relevant work was not discussed or not discussed in great depth. For example, very recent work learn how to build a graph from text (https://arxiv.org/pdf/1910.08435.pdf). I would also like the authors to better explain how their work differs from other recent similar work.  \n\nOverall, I find it difficult to evaluate the quality of the contribution with the current manuscript. \n\n\nDetailed comments: \n- I found that several terms were used without being properly defined.  E.g., common sense knowledge graph, grounded knowledge. I think that defining these early would strengthen the introduction. \n- Figure 1 is useful but I suggest to expand the caption to explain how the figure. I also found that it would be useful to include G^0, G^1, and G^2 in it. I would also suggest adding a second figure that shows the actual model (e.g., including its layers). The current Figure 1 only contains the attention weight/scores. \n- In Tables 1, 2, and especially 3, could you provide the variability of the estimators?\n- In Figure 2, perhaps these would be better illustrated as bar plots?  Also, I'm not sure why you connect the performance of the different models. \n- In the Ablation study, what is the number of random examples in the Gold method and how did you choose it? It seems a bit arbitrary and it seems like this value will greatly affect the results of this method. If I understand correctly I would suggest removing altogether this method. \n- In related work, there is a single paragraph (3rd) on work that explores similar tasks. I think it would be nice to survey work on graph construction from data (e.g., OpenIE) as well as text generation from graphs. \n- The title of the paper is perhaps somewhat misleading since this model is really meant for response generation and not multi-turn conversations (the latter would likely require to keep the previous grounded concepts around). \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to make use of commonsense knowledge base in conversation generation. \n\nThe general idea of introducing such KB is not wrong. \n\nThe main approach of the proposed method is in Secs 2.2.2 and 2.2.3. Unfortuantely, the paper reads like code comments, rather than a scientific paper. Most of the model components are not well motivated. \n\nFor example, the attention score of concepts is computed by softmax(.) * PageRank. The authors fail to justify why this formula is approximate and better than alternatives. It doesn't look good to me. Suppose the prompt is about a rare concepts, whose neighbors are also rare. Then the PageRank scores are low for all neighbors, resulting in very low attention weights. A better approach seems to be post-normalization. \n\nThe ablation test compares the model with random and golden concepts. However, it is unclear how each proposed model component (e.g., Central flow encoding and outer flow encoding) plays a role in the entire model. Also, it's unclear how the proposed model is compared with an obvious and seemingly better treatment, for example, softmax(.) * PageRank vs. softmax(. + log PageRank). \n\nBesides, the paper does not tackle the key issue of multi-hop reasoning. One of the key problems is that the number of neighbors grows exponentially wrt the number of hops. Therefore, I would expect some routing techniques (for example, trained by RL) that maintain a list of tractable paths, but with more steps of hops. The paper instead proposes some esoteric attention mechanisms on one-hop/two-hop neighbors.\n\nIn short, the proposed model is neither well motivated nor well tested. The technique is also pretty superficial.\n\n\nMinor:\n\nThere're some formatting issues. For example e_j -> e_i is better formatted as e_j \\rightarrow e_i."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a system for generating a single-turn response to a posted utterance in an open-domain dialog setting.  It follows but improves on a previous work that leverages commonsense in generating the responses.  The specific proposal is to extract keywords from the incoming utterance, match them to concepts in ConceptNet,  compute the one-hop and two-hop neighbors, and use them to influence the sequence generation process using attention weights.  The two-hop neighbors are prefiltered in a preliminary training step to control the excessive noise.  The novelty in this proposal is to use the notion of a \"flow\", i.e., diffusion into the neighbors of the grounded concepts.\n\nIn an experiment using a Reddit-based dataset, the proposed method wins over several baselines in 7 metrics that measure relevance between the generated and ground-truth responses.   The proposed method does not win on diversity but has some minor advantage on novelty.  Advantages on both appropriateness and informativeness are also observed in human evaluation.\n\nThe proposal is technically sound and the use of concept flow is intuitive appealing. The experimental results are convincing of the method's effectiveness.\n\nThe evaluation presented in Figure 2 requires some clarification. In particular, the \"Gold\" option needs some explanation: what are the negatives there, why are they used, and how are they used?   Also, what is this option supposed to demonstrate?   How does ConceptFlow select its negatives and how are they used?\n\nThe example shown in Figure 3 presents a more puzzling picture,  which needs additional clarification:\n\nWhy \"fan\" and \"fans\" show up as two distinct concepts?  Are they supposed to be the same concept?  \n\nIs there an explanation for why the response retains the concept \"championship\" directly from the post?  Can it be explained by that this is a relatively rare word in the dialog collection?  Why is\nthe attention assigned to it not very high?  \n\nFor what reason many of the very high attention concepts like \"huge\" are filtered out?   The loss of the word \"win\" in this filtering step seems to be especially puzzling -- intuitively one may think that \"win\" should connect well to the concept of championship.  It is not clear whether this example is to demonstrate success or a breaking mode.\n\nThere are minor errors in the English text that should be fixed in a revision."
        }
    ]
}