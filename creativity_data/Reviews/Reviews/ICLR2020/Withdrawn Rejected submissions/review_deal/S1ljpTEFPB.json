{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a guided attention-based strategy for sparsifying deep neural network models. The goal is to achieve both model compression and minimal loss in accuracy. The novelty of the technique seems to be the use of a combination of (group-)sparsity penalties and a variance promoting regularizer while training the neural network. Experimental results on MNIST and CIFAR (10/100) are presented.\n\nUnfortunately, the current manuscript is confusing to read, makes limited contributions, and is incomplete in many places. I include an incomplete list of caveats below, but it should be clear that the paper cannot be accepted without a major rehaul.\n\n* The authors use the \"attention\" nomenclature throughout the paper but it is unclear to this reviewer why this choice is made. As far as I can tell, the main formulation (1) is just regular cross-entropy minimization with a couple of regularizers thrown in -- no attention-type mechanism is used here -- so the basic formulation is of unclear novelty.\n\n* The groups in the group-sparsity penalty are not clearly defined anywhere. \n\n* The entirety of Section 4 (which seems to be a main contribution of the paper) is rather confusing. What is the vector V and what does it have to do with the main formulation (1)? What does \"Update gradient\" in Algorithm 1 referring to? What is the purpose of Section 4.3?\n\n* What does a \"sparsity level of 60%\" mean in the experiments? Are the authors referring to compression ratio here? Unclear since never defined..\n\n* The main (only) theoretical statement is Proposition 1, whose detailed proof is claimed to be in an \"Appendix\" but no such appendix is provided. Moreover, Proposition 1 does not seem to be a mathematically sound statement.\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "# Summary\n\nIn this paper, the authors propose an approach to induce (structured) sparsity in deep neural networks during training. \n\nThe proposed approach builds heavily on Structured Sparsity Learning (SSL) [1], which in a nutshell pioneered the use of a group-Lasso regularizer [2] to induce sparsity at the level of filters, input channels, layers and filter shape in deep neural networks. The main contribution presented in the manuscript is the inclusion of an additional regularizer in the SSL framework (Eq. 3) which, for each layer, encourages the collection of $L_2$ norms of each group’s weights to have a large variance, with each group thus acting as a “sampling unit” in the calculation of the variance to be maximized for each layer.\n\nExperimental results are provided for MNIST, using an MLP and LeNet-5-Caffe as models, and CIFAR-10/CIFAR-100, using VGG-16 as the model.\n\n# High-level assessment\n\nFrom a methodological perspective, the manuscript’s main contribution relative to prior work such as SSL [1] is the inclusion of the regularizer described in Eq. (3), which aims to penalize solutions for each group-wise $L_{2}$ norms have a small variance. The addition of this regularizer is however largely heuristic, lacking a theoretical motivation in the manuscript. \n\nRegarding the empirical assessment of the contribution’s performance, I believe that the experimental results presented in the current version of the manuscript do not provide sufficiently clear evidence that the proposed approach significantly outperforms existing baselines. Most importantly, the reported results for some baseline approaches appear to be in contradiction with those found in their respective original publications, though the reasons for this disagreement are not discussed in the manuscript.\n\nFinally, I found the paper to be severely lacking in clarity, to the extent that I unfortunately felt it got in the way of properly understanding some of the low-level details of the proposed approach. Sections 3 and 4, where the method is introduced, often make use of imprecise mathematical notation and vague textual descriptions that make it difficult to assess the soundness of the proposed approach. In particular, it is for me unclear how exactly Section 4 relates to Eq. (3), what the role of Proposition 1 is precisely and what its statement exactly refers to.\n\nBecause of these reasons, I cannot support acceptance of the submission in its current form. However, I would encourage the authors to thoroughly revise their manuscript to address those issues.\n\n# Major points\n\n1. In Section 3, the authors refer to [3] to justify the inclusion of their variance-inducing regularizer (Eq. 3). In particular, the authors claim that:\n\n“The power of the variance as a regularizer has been investigated in [3].”\n\nHowever, unfortunately I am unable to see the relationship between [3], which aims to minimize the (unknown) risk by minimising instead a surrogate upper bound of the risk that explicitly takes the bias/variance trade-off into consideration, and the approach proposed in this manuscript.\n\nIn particular, in [3] the authors propose **minimizing** (not maximizing) the variance of the **loss** (not of the parameters) with respect to the **empirical data distribution** (not parameter groups) in the context of convex optimization (not non-convex models like deep neural networks).\n\nTo this end, I would encourage the authors to clarify exactly how the results of [3] support their proposed regularization scheme.\n\n2. In Section 3.1, it is claimed that by reparametrizing $\\lambda_{v}$ as $\\lambda_{v} = \\alpha \\lambda_{s}$, the number of hyperparameters is reduced by, which seems to imply tuning $\\alpha$ is unnnecessary. However, this is not immediately obvious to me, given that the results in Table 4 suggest that the performance of the proposed approach can vary substantially with respect to $\\alpha$.\n\n3. In my opinion, the exposition of the proposed approach throughout the manuscript is, at times, confusing.\n\nFirstly, the paper frequently refers to the proposed approach as an instance of “attention”, including the manuscript’s title. However, I must admit I fail to see the connection to attention, in the sense of the methods referred to as related work in Section 2. If the authors feel their paper is indeed related to attention, I would encourage them to elaborate on this connection further, clarifying how exactly their method is related to those approaches cited in Section 2.\n\nSecondly, Section 3 could be improved by more clearly separating original contributions from prior work. Related to this, SSL [3] appears to be cited “out of place”, as I believe it would be more appropriate under the “group sparsity” subsection. Also, given the emphasis on structure, I believe the choice of groups should be discussed in greater detail since, at the moment, only one choice is used, with no justification, and it is only discussed as part of the experimental setup in Section 5.\n\n4. In my opinion, the current version of the manuscript suffers from a severe lack of clarity.\n\nSome of these issues, prevalent throughout Section 3, are somewhat minor, in that with some effort they can be “second-guessed” from context. Examples include:\n\na. The function $G$ in Eqs. (1-3) is never defined properly. Likewise, the function $H$ in Eqs. (1, 3) is also never defined, though a footnote claims it’s the identity function. However, in that case, what is the role of $H$ in the exposition?\n\nb. The authors refer to the regularizer in Eq. (1) as the inverse of $\\Psi$, whereas I believe they meant the reciprocal.\n\nc. Eqs. (2, 3) use a normalization factor $\\vert G(W^{l}) \\vert$ which is never defined. It is claimed that “... is a normalizer factor which is in essence the number of groups for the $l_{th}$ layer …”. Does this mean it is equal to $M$ and, if so, why use two different terms for the number of groups in a layer?\n\nd. The notation $( )_{l}$ used in Eq. (2) is not defined properly. Instead, I would recommend using the notation in [3], explicitly indexing the weights and number of groups per layer.\n\ne. The function $\\Psi$ as defined in Eq. (3) does not match its textual description. Eq. (3) describes the sum of variances (across groups in a layer) of the group-wise $L_{2}$ norms of the weights. Instead, the authors refer to this regularizer as “... simply the variance of the group values for each layer, normalized by a factor ...”. \n\nHowever, the lack of clarity is, in my opinion, more problematic for Section 4. In brief, I fail to see the connection between Section 4 and minimization of Eq. (1), because of the following issues.\n\nFirstly, I find the definition of $V(\\theta)$ in the first line of Section 4.1 puzzling. As I understand it, it would appear that $V(\\theta) = \\theta$. However, in this case it is unclear to me: (i) what the purpose of defining $V(\\theta)$ is and, most importantly, (ii) what is the relation between maximizing the variance of $V(\\theta)$ and minimizing (maximizing the reciprocal) of Eq. (3). In particular, regarding (ii), Eq. (3) appears to concern the variance of $L_{2}$ norms, whereas $V(\\theta)$ seems to involve the model weights directly. As a minor point, the authors should probably define $\\vert \\theta \\vert$ explicitly, though I take this to mean the number of parameters at some unspecified level of granularity (group?/layer?/model?).\n\nSecondly, I do not find the statement of Proposition 1 to be sufficiently precise to allow me to verify its correctness. In particular, I believe the statement “... does not enforce any upper bound on the variance vector $\\hat{V}(\\theta)$” should be rephrased in mathematically precise terms. This imprecision, linked to my confusion regarding what $V(\\theta)$ is meant to be exactly, makes it difficult to evaluate the proof of Proposition 1 as it stands now.\n\nThirdly, the proof of Proposition 1 refers to an appendix that, to the best of my knowledge, has not been provided for review. It would also be helpful to clarify what parts of the proof are an original contribution, seems to the best of my knowledge the derivations shown appear to significantly overlap with [4].\n\nFinally, I do not fully understand the relevance of Proposition 1 relative to the rest of the manuscript, though this is likely caused by all of the issues regarding clarity in Section 4 described above.\n\n5. Some aspects of Algorithm 1 remain unclear/unjustified, which is likely caused (at least in part) by the lack of clarity throughout Section 4 as argued in “major point 4” above.\n\nFirstly, it is not immediately clear to me how to estimate the mean of $V^{r}(\\theta)$ in practice in Algorithm 1 and, relatedly, what the assumed source of stochasticity for $V(\\theta)$ is. Secondly, the if-else condition in Algorithm 1 does not appear to be largely heuristic. Finally, I would encourage the authors to prove precisely how Algorithm 1 contributes to minimizing Eq. (1) and its relation to the regularizer described by Eq. (3).\n\n6. The experimental results do not convey clearly the benefit of the proposed approach in terms of accuracy nor sparsity. \n\nIn particular, results across the three reported experiments (Tables 1 and 2) appear to be largely mixed, with competing approaches often outperforming the proposed approach. Most importantly, the authors report no error bars to illustrate the effect of random variability due to factors such as random seeds, making it difficult to assess the statistical significance and reproducibility of the otherwise apparently small relative differences reported between approaches.\n\nFinally, the results regarding the trade-off between sparsity and accuracy reported in Table 3 are largely incomplete, incurring risk of “cherry-picking”. I would strongly encourage the authors to complete these results by reporting results for the other two experiments (MNIST/CIFAR-10) as well as for a much wider regime of sparsity values, ranging from 0% to 100%.\n\n7. Some of the reported results for competing approaches appear to be, at first glance, substantially worse than in their respective publications.\n\nFor example, [5, Table 1] reports an error of 0.75% on LeNet-5-Caffe trained on MNIST, with sparsities per layer of 67% - 98% - 99.8% - 95%, for Sparse Variational Dropout. Instead, this submission reports the same error, but much lower sparsity rates of 66%-36%-59%-75%, respectively. Similarly, [6, Table 2] reports that Bayesian Compression prunes more than 99% of all parameters while maintaining an error of approximately 1%, which again seems at odds with the reported sparsities per layer of 60%-74%-89%-97% in Table 1 of this manuscript. While not as directly comparable due to small differences in the experimental setup, similar discrepancies in attained sparsity rates appear to also be present for CIFAR-10 and CIFAR-100 (e.g. [5, Figure 3]) as well as other baselines (e.g. [7, Table 1 and Figure 3]).\n\nI believe these apparent discrepancies should be discussed and justified, adjusting the experimental setup to match the original publications if necessary.\n\n# Minor points\n\n1. The submission would benefit from being proof-read for English style and grammar issues.\n\n2. The authors should clarify what exactly is meant by “speedup” throughout Section 5.\n\n3. Given the focus of the manuscript on structured sparsity, more choices for the definition of groups should be explored in the experiments, following [1].\n\n# Proposed improvements\n\nI would encourage the authors to address the major points listed above. While they might be numerous enough to constitute a major revision, I nonetheless believe they would substantially strengthen the manuscript.\n\nIn particular, I encourage the authors to rewrite the manuscript with a focus on clarity, to better motivate their method both theoretically and in relation to prior work, to carry out additional experiments to highlight the benefits of the proposed approach and to clearly discuss the sources of discrepancies between the reported results for the baselines and the original publications.\n\n# References\n[1] Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" *Advances in neural information processing systems*. 2016.\n[2] Yuan, Ming, and Yi Lin. \"Model selection and estimation in regression with grouped variables.\" *Journal of the Royal Statistical Society: Series B (Statistical Methodology)* 68.1 (2006): 49-67.\n[3] Namkoong, Hongseok, and John C. Duchi. \"Variance-based regularization with convex objectives.\" *Advances in Neural Information Processing Systems*. 2017.\n[4] Wang, Chong, et al. \"Variance reduction for stochastic gradient optimization.\" *Advances in Neural Information Processing Systems*. 2013.\n[5] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational dropout sparsifies deep neural networks.\" *Proceedings of the 34th International Conference on Machine Learning*-Volume 70. JMLR. org, 2017.\n[6] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" *Advances in Neural Information Processing Systems*. 2017.\n[7] Louizos, Christos, Max Welling, and Diederik P. Kingma. \"Learning Sparse Neural Networks through $ L_0 $ Regularization.\" *International Conference on Learning Representations*. 2018."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposed Guided Attention for Sparsity Learning (GASL) to learn sparse model in neural networks. The authors demonstrate through experiments that their methods achieve state-of-the-art sparsity level and speedup with competitive accuracy.\n\nQuestions:\n1) In the abstract, the authors claim “Our work is aimed at providing a framework based on an interpretable attention mechanisms...”. How is your method interpretable? I think the paper needs more justifications on this point.\n\n2) The results of this paper would be more convincing if the authors could include experiments on ImageNet datasets, as in [1], [2]. \n\n3) In section 4.2, why is “ the choice of V^r should be highly correlated with V”?\n\n4) In section 3.2, why is variance loss related to attention mechanism? From my view, you simply add a loss function to penalize the variance of weights in a group. This part is confusing.\n\n5) Also you said that the detailed mathematical proof for proposition 1 is put in the appendix. But where is the appendix?\n\n6) For the experimental results, your method fall short of Sparse Variational Dropout in terms of clean accuracy. It seems to me that your method may not be so effective.\n\n7) There seems to be some typos in the current version:\nSection 4.1 last line, “from now one” should be “from now on”\nSection 4.1 something is wrong with the last line in equation (5). \n\n\nIn general, the paper is not clearly presented. Many details are needed to clarify the contributions. The paper writing needs to be improved, as there exist many typos in the current version and the presentation is confusing. Therefore I recommend weak reject for the current version of the paper.\n\n\n[1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015.\n[2] Learning Structured Sparsity in Deep Neural Networks. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li. NIPS 2016.\n"
        }
    ]
}