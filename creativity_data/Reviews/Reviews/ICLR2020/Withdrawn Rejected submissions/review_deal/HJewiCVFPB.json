{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents a method for improving optimization in multi-task learning settings by minimizing the interference of gradients belonging to different tasks. \n\nWhile the idea is simple and well-motivated, the reviewers felt that the problem is still not studied adequately. The proofs are useful, but there is still a gap when it comes to practicality.\n\nThe rebuttal clarified some of the concerns, but still there is a feeling that (a) the main assumptions for the method need to be demonstrated in a more convincing way, e.g. by boosting the experiments as suggested with other MTL methods (b) by placing the paper better in the current literature and minimizing the gap between proofs/underlying assumptions and practical usefulness. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes as solution to manage the case where gradients are conflicting in gradient-based Multi-Task Learning (MTL), pointing to different directions. They propose a simple “gradient surgery” technique that alters the gradients by projecting a conflicting gradient on the normal vector of the other one, in order to mitigate the effect. The method is generic in the sense that it can be directly applied to various gradient-based architectures easily.\n\nThe paper is well written and easy to follow. However, the whole proposal relies on the assumption that conflicting gradients are common and harmful for MTL. For simple convex models, like the model used in the theorem, I get that this can be an issue. But using a convex model for MTL seems no that common. And as far as I know, MTL is not used commonly with simple models, it is rather common is with deep neural networks, where a common part of the network (i.e. representation) is shared among the tasks, while we have a distinct head, of one or few layers, for each task. With such a setting, we expect to have enough capacity to model the various tasks, such that the neural network model should be able to model both tasks independently if they are in complete contradiction. In such case, over the training, conflicting gradients over some neurons with be a transient phenomenon, with the neurons specializing on one or the other task, or just be disabled. In practice, common elements will be shared, while contradictory elements will be in the task-specific part of the network. Said otherwise, I think that conflicting gradients may appear on some neurons over some data, but will be mostly a stochastic phenomenon which is not necessarily harmful on the long run, much like the stochasticity of picking a sequence of data from different classes in SGD.\n\nI may be not totally right in speculating in such way on what is going on with conflicting gradients in MTL. My point is to show that the whole paper is based on assumptions that are not verified. I would need to be convinced that we are tackling a real problem, not such an idea of something that may happen but in practice is quite rare or not necessarily that harmful. The experiments over the toy problem on trashing gradient (Sec. 3.1 and Fig. 1) is interesting to illustrate the problem, but show the issue in a 2D setting, where the model has very little degree of freedom. With deep networks, I don’t think that such constrained search space is common.\n\nAnother issue with the paper is the lack of comparison with other approaches for MTL. A conjecture I have on the performance of the approach is that it may dampen the gradient for the loss of the different tasks, and their amplitude, to get to an effect similar to what GradNorm is doing. In fact, all results reported are on a base method and one with PCGrad. Comparison with other methods to handle MTL is required in my opinion, in particular with GradNorm, which may have similar effects than the current one. If PCGrad and GradNorm achieve similar results, my guess is that the issue is not with conflicting gradients, but rather bad scaling of the losses when they are put together.\n\n** Update ** : After reading the authors' comments and other reviews, I would maintain a \"weak reject\", although if there was a \"borderline\" choice, I would have used it. The authors provide reasonable answer to my request, although I think the paper could have been stronger in term of experimentations. Also, the other reviews and comments on existing work make it clear to me that the positioning to other related work was incomplete. I think the proposal and paper is correct and interesting, so still below the acceptance threshold for ICLR.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes a simple rule to ignore the so-called conflicting gradients in addressing multi-task learning problems. The underlying idea is very straight-forward if two gradients are contradictory (the angle between them is > \\pi) then one should not be considered.\n\nI have some questions here. Let's assume the gradients passing to PCGrad are g1, g2, and g3 (in that order).\n\n1- In PCGrad, we start with g1 and will keep g2, or g3 if their direction complies with g1, isn't it better to keep the gradient that minimizes the loss more (or say has a bigger norm) instead? Maybe one should first pick which gradient is more important and then use that to start PCGrad.\n\n2- One may argue that if <g1,g2> <= 0, <g1,g3> <= 0, PCGrad will ignore both g2 and g3, hence, intuitively it should converge slowly and probably not generalizable. Can you comment on why I should not worry about this?\n\n3- Is it possible to generalize PCGrad to work with more than two gradient vectors? One imagines that if it can work with more than 2 gradients, maybe a better and more robust algorithm can be achieved.\n\nBased on the above, I believe that the paper in its current form has not completely studied the problem and hence I am giving the paper a weak reject score at this stage.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents a method to boost multi-task learning performance by editing gradient to remove conflicts between tasks. The main idea is to use cosine similarity to 1) determine if two task gradients conflict and 2) to project one conflicting gradient to the normal plane of the other, thereby removing the conflict at the expense of disturbing the other gradient to some extent. Experiments are presented for classification and other computer vision tasks along with reinforcement learning problems.\n\nOverall, I really liked this paper. The explications are clear, the visualizations provided help the understanding (especially Fig.1), and results are compelling. I definitely value the way the method is straightforwardly presented: the underlying idea is simple yet strong. There are, however, a few elements precluding me to pick a higher rating, which I describe in details here.\n\nFirst, there are a lot of similarities with the MTL method of Sener and Koltun 2018. In particular, I do not agree with the statement that \"[this] work, in contrast to many of these optimization schemes [incl. Sener and Koltun], suggests that the challenge in multi-task learning can be attributed to the problem of gradient trashing, which we address directly by introducing a practical algorithm that de-conflicts gradients from different tasks.\" MTL has the concept of \"common descent direction\" and, as Fig.1 of relevant paper suggests, it does \"de-conflict\" the gradients. Sure, the wording is not the same, but the idea is there nonetheless.\nTo be clear, this is not to say that PCGrad has no merits. I do find it simpler and more elegant (although the latter is a subjective assessment). But I think the similarities should have been discussed in greater details, and the performance compared with Sener and Koltun on at least one problem.\n\nSecond, the experiments make it difficult to see the performances of PCGrad alone. Indeed, it is always combined with another multi-task approach/algorithm (MTAN, WPL, SAC+PA, etc.). Providing these results is not incorrect in itself, but it makes it difficult grasp what PCGrad can do alone. Is it worth using only in conjunction with other approaches, or could someone consider using it in a standalone manner? Is PCGrad more a \"gradient fine-tuner\" than a comprehensive solution for multi-task leearning? The experiments presented, although significant, do not answer these questions.\n\nThird, I am unsure about the value of Theorem 1 and its proof in Sec. 3.2. It assumes too much (e.g., 2 convex tasks) to be of any use in practice. Also, leading to a minimizer of L does not forcibly mean leading to a good solution, depending on the definition of L1 and L2.\n\nThere is also one element I'm unsure about. Looking at Algorithm 1, it looks like the order of the tasks may have an effect. Indeed, since it is g_i that is modified at line 8 and not g_j, the last element of B will always remain unaltered (because all other task gradients will have previously been modified to avoid gradient conflict with it). The second to last element of B will be altered, but only due to potential conflicts with the last, and so on, up to the first which can potentially be altered by all others. As an effect, some gradients will always be significantly more altered than others, which can have an impact (at least theoretical) on the learning process. Algorithm 1 thus looks like a greedy approach. Nothing bad in itself (and some ad hoc adjustments, like shuffling B at each update, could very well fix this), but I think this deserves more discussion.\n\nFinally, a more generic comment: the naming of the method should remain the same through the paper. The abstract/intro/Fig.1 refer to the technique as \"gradient surgery\", while the explanations and experiments talk about PCGrad. Also, in the introduction \"plateuas\" -> \"plateaus\".\n\nIn summary, I think this is a good paper, presenting a straightforward and useful idea for multi-task learning. However, the related work is not always well described, the experiments lack important comparisons, and the practical effects of PCGrad should be explained in more details instead of focusing on a proof for a convex case.\n\n"
        }
    ]
}