{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "*What is this paper about?*\n\nThis paper proposes a new unsupervised (or self-supervised) learning method to get general purpose audio representations. It proposes three main self supervised tasks, two of them analogous to word2vec’s SkipGram and CBOW. The third task is related to predicting the distance between two audio segments. The authors then test this representations in a range of downstream tasks, showing an improvement when compared to untrained representations.\n\nShort review:\n\nThe techniques proposed in the work are simple and straightforward, which in itself is neither good nor bad. The fact that they present satisfactory results, though, is interesting and it is nice that the authors test them in a range of 6 different tasks. There are a few simple extra experiments which would have increased the impact of this work though. The first of these is that the authors didn’t run any experiments integrating the multiple loss functions. More details bellow.\n\nContributions:\n\nThis work proposes a new learning method to get representations for audio sequences with no labels. He then tests these methods in a range of 6 different downstream tasks.\n\n*What strengths does this paper have?*\n\nThe paper is very well written and easy to follow. \nAuthors propose simple techniques and run a nice range of experiments to evaluate them. \nThey also compare to meaningful baselines, making it easy to asses their method’s performance. \n\n*What weaknesses does this paper have?*\n\nMy main criticism would be that the authors did not run a few cheap and straightforward experiments. They mention in both the main results and conclusion sessions that it would be nice to test merging representations. Simple concatenation of the embeddings would be a very simple experiment to run which would fit nicely in this paper (and maybe not in a future one). They could also have trained the representations with a hybrid loss function combining their already implemented tasks (audio2vec, temporal gap and triplet loss).\n\n*Detailed comments:*\n\nThe reference to “Representation Learning with Contrastive Predictive Coding” is wrong. Probably the authors names are not correctly split in the bibfile. Also, the paper is from 2018, and not 2019.\n\nThe text in Figure 1 is very small and hard to read.\n\nIn the Temporal gap explanation the authors say “we ask the model to estimate the absolute value of the distance in time between two slices sampled at random from the same audio clip”, but then they define \\delta as a normalized distance. \n\nIn Figures 2 and 3, it is very hard to see the lines for Spectrogram and MultiHead.\n\nDo the authors have an intuition for why the finetuned Audio2Vec representations does not fully bridge the gap with respect to the Supervised model?\n\nDo the authors have an intuition for why the AutoEncoder benefits more from the non linear experiment in Table 5?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents two self-supervised algorithms for representation learning from audio, with artificially created supervised tasks. In particular, audio2vec borrows the ideas of CBoW and SkipGram from word2vec, and TemporalGap predicts the temporal distance between two segments. There is an ongoing theme of self-supervised learning in the machine learning community, with the goal of learning general features from large amount of unsupervised data with auxiliary tasks that may correlate with downstream tasks, so as to reduce the sample complexity for downstream supervised learning. The topic of this paper is quite relevant to the community. \n\nThe intuitions behind the algorithms do seem plausible to me; after all, word2vec was very successful for NLP. My concerns of this submission are on the execution of the ideas, and the empirical analysis.\n\n1. While the intuition behind representation learning for audio is to capture the temporal structure/contextual information, the proposed algorithms are not demonstrated on downstream tasks that require such information the most, e.g., ASR. The authors argued that \"Note that the choice of the tasks used for the evaluation is consistent with the selected temporal granularity. As such, we do not consider automatic speech recognition tasks, which generally require process much shorter temporal slices.\" But this argument is not convincing, as the large receptive fields of the architecture (T=975ms) is not an issue,  you just need to reduce the hop size (to sth like 10ms) to get per-frame features for ASR. The potential impact of the work could be higher if demonstrated on more significant audio tasks. \n\n2. For the downstream supervised tasks presented in the paper, the baselines seem a bit weak. There was not much details on the \"naive-Bayes\" classifiers for aggregating segment (with duration 975ms)  predictions to sample-level prediction, and how this choice is justified. I could imagine using a LSTM-based aggregator. Furthermore, if only an utterance-level feature is needed for the downstream tasks, one could even use the features learned by the method of Jansen et al, ICASSP 2018 (in fact their features are learned from AUDIOSET as in this submission).\n\nI am not sure if all tasks/datasets presented here are standard benchmarks; the authors shall try to provide the state-of-the-art performance for them. Ideally, the results shall be close to state-of-the-art with reasonable model architectures. Additionally, some of the tasks seems not very challenging: baseline linear classifier can achieve almost perfect performance on LSP?\n\nAlso, for encoder fine-tuning, it appears fine-tuning the top layers could significantly improve the performance, without increasing model complexity. Then why do not we always fine-tune all layers for downstream tasks, instead of using just linear or shallow networks on top of the features (freezing the parameters of the feature extraction networks)? In fact, in a pre-training + fine-tuning scheme, we would hope representation learning improves the generalization performance of downstream tasks. The experimental results of \"retrain_last_3\" does not show such improvement yet.\n\n3. Some of the experimental results contradicts with my intuition and perhaps more analysis is required to explain why. For example, while I can see that Librispeech as the feature learning dataset is inferior for non-speech audio event based tasks, it is not clear why it does not help on more human-speech related tasks (SPC and LSP, and in fact representation learning on Librispeech does not help LSP?)\n\n4. The authors argued in a few places the choices are suited for \"on device\" federated learning. I think it should be made clearer at the beginning of the paper the problem setup, and how it constrain learning (smaller model size, low computational budget, or limited communication bandwidth etc al). In any case, the justifications seemed somewhat contrived to me. If the goal is to learn generally useful features suited for multiple downstream tasks, I do not expect this can be achieved with a very small model. Besides, there are other ways of making models small, e.g., knowledge distillation or model quatization etc. \n\n5. Impact of encoder architecture size: when you increase the model complexity, should not the \"supervised\" baseline also improve? Are the results in Table 4 comparing against the stronger \"supervised\" baseline or the one in Table 2?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes three self-supervised tasks for learning general audio representations. Inspired by word2vec, the paper introduces one task that predicts neighboring segments given the center segment and another that predicts the center segment given the neighboring segments. The third task is to predict the time difference between two segments. The model is based on a 5-layer convolutional neural network. Experiments are done on several general audio tasks, including keyword spotting, speaker classification, language identification, and audio classification, and pre-training is done on AudioSet. The results are positive, better than Mel spectrograms and worse than the supervised counterparts.\n\nI am giving the paper a score 3. The paper includes a comprehensive comparison, and the baselines are all solid. The weakness of the paper is the lack of novelty, and reasons are given below.\n\nThe tasks, except the one that predicts the time difference, are similar to (Milde and Biemann, 2018). To be fair, (Milde and Biemann, 2018) evaluates their models on speech tasks rather than general audio tasks. However, the novelty is limited if the paper simply ports these models from one set of tasks to another. The results, showing that pre-training helps for many downstream tasks and that fine-tuning helps further, are not surprising. What is missing are why this is the case and what the models are actually learning from different tasks. It will be great if the paper can analyze/discuss what is being learned by the self-supervised tasks and why.\n\nA more concerning problem in the paper is the choice of tasks. As can be seen from Table 2, LSP, MUS, and BSD are probably too simple for evaluation, because there are little to no gaps between the baselines and the supervised networks. In particular, LibriSpeech is not even a suitable data set for speaker identification.\n\nAnother thing worth mentioning is that, though the results are all strong in Table 4, they have not surpassed the supervised results. It seems to suggest that despite all the effort, there is no reason to prefer pre-training on more data.\n\nOne quick question:\n\nEncoder fine-tuning: ...\n--> How do you decide which epoch to stop pre-training, where to start fine-tuning, what optimizer, how the optimizer is initialized, and what learning rate to use?\n\nUnspeech: unsupervised speech context embeddings\nBenjamin Milde, Chris Biemann\nInterspeech, 2018"
        }
    ]
}