{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper develops a RL attack algorithm named CopyCAT, which can lure a policy into a desired behavior  with a finite set of additive masks. The experiments on Atari games validates the effectiveness of the proposed approach.\n\nDespite the previous works in RL attack, I feel CopyCAT is impressive in the potential of running in real time and taking control of a neural policy. However, I still want to see some comparisons with previous works in the experiments. With real examples we can understand the solutions better. \n\nDetailed comments:\n- Figure 1: I cannot see which curve stands for CopyCAT\n- Figure 6: hard to read"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper tackles the problem of generating more realistic adversarial attacks for neural network policies, that can be generated in real-time and only require altering the current environment observation, not the agent’s state (e.g., saved previous observations). To meet this goal, the proposed approach CopyCAT optimizes for a universal (i.e., input-independent) attack per action, that will lead the attacked policy to output that action.\n\nThe evaluations show that CopyCat’s universal attacks are able to measurably alter agents’ behaviour, compared to FGSM attacks. The paper does not compare to more complex, state-of-the-art approaches for generating adversarial examples, because it argues that these approaches take too long to create adversarial examples, which makes them infeasible for real-time attacks. The method used for evaluation is creative -- a policy is attacked to make its actions match those of a *better* policy, and the behaviour of the agent is considered successfully changed if it obtains higher reward than without the attacks.\n\nI found this paper to be well-motivated, and there is a clear contribution here, with the introduction of a universal attack that changes the behavior of neural network policies. However, I have concerns regarding the evaluation, as described below. The writing also needs editing; there are quite a few grammatical errors and confusing / unclear phrasing (e.g., \"In the latter\" is used incorrectly in a couple places).\n\nRegarding the evaluation, my first concern is that the experiments are all on Atari, in which the visual variation is extremely limited, compared to real-world observations. For instance, most Atari games typically only have a handful of colors. Because of this, I’m not convinced that this approach of generating universal attacks would work on more realistic images, which limits the relevance of this paper.\n\nIn addition, it is too strong to claim that showing this approach is effective in the white-box setting means that it will also effective in black-box settings. These universal attacks are computed based on the training observations of the agent. When two agents are trained with deep RL, even if it’s with the same algorithm but with different random initializations of the network, it’s very likely that they will encounter different distributions of states during training. Thus, I would like to see additional experiments that evaluate how this attack performs in the black-box setting.\n\nFinally, there are other approaches that only require a forward pass through a neural network like CopyCAT does, for instance training a generative model to create adversarial attacks (e.g., AdvGAN [1]). Although previous work on this has largely focused on image classification, in essence a policy is just a classifier for which the output is images, so this line of work is worth mentioning and potentially comparing to.\n\nMinor comments / questions:\n- It's strange to refer to stacked observations as the \"state\" of the agent, since typically \"state\" is instead used to refer to either state features or a learned latent state. So claiming that prior approaches require changing the state of the agent is misleading.\n- Why are attacks with smaller l2 norm harder to detect? Is there a citation for this? It seems to me that limiting l-infinity norm is enough to make adversarial examples difficult to detect, and there's no need to limit both l2 and l-infinity. This is an important point, because the evaluations compare algorithms with respect to l2 norm.\n- Section 4 (Related Work) should be placed earlier in the paper, ideally right after the Introduction.\n- Regarding reproducibility -- where were the DQN and Rainbow agents obtained from? If they were trained in-house, what architecture and learning hyperparameters were used?\n\n[1] C. Xiao, B. Li, J. Zhu, W. He, M. Liu, D. Song. Generating Adversarial Examples with Adversarial Networks. IJCAI 2018."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a targeted attack on reinforcement learning agents. The goal of the attacker is to add small perturbations masks to an agent’s observations in order to encourage the agent to follow a specific (targeted) set of actions. Each mask corresponds to one specific action, and thus can be thought of as a universal perturbation to induce the agent into taking that specific action. The attack, named CopyCAT, is experimentally compared with current attacks in RL such as the FGSM attack of Huang et al.\n\nI would argue to reject the paper for two key reasons. First, the threat model is not well-motivated or clearly explained. Second, I am not convinced by the soundness of the experimental results because the FGSM baseline seems much worse than it should be. I am also concerned by the lack of ablation studies for the L2 regularization term in the author’s loss function.\n\nThe threat model the authors propose is that the attacker should only have read-access to the agent that it wants to attack, and not write-access. In RL terms, this means that the attacker can only attack the current observation, and not the internal state of the agent. However, in the actual experiments, the authors use networks like DQN which take as input 4 frames at a time, and then the authors only apply the mask to the final (4th) state. This seems like a very strange threat model - if an attacker can already attack every 4th state by applying a mask to it, why can’t the attacker just attack every state? In my opinion, the authors should reconsider the threat model, and perhaps focus more on the fact that they are generating universal perturbations for RL agents via action-specific masks, which does seem interesting.\n\nNext, I am worried about the soundness of the experimental results, especially the FGSM baseline. First, in Fig. 4, I do not think the L2 norm of the FGSM-L_inf attack should be on the x-axis - I would not expect the FGSM-L_inf attack to have low L2 norm at all, so it seems unfair to compare something that you explicitly regularize to have low L2 norm (CopyCAT) with something that has no L2 norm constraint.\n\nSecond, I do think that it is fair to have FGSM-L2 (I did not see this explicitly discussed anywhere in the text, which should also be addressed, but I assume this is an FGSM attack which is projected to a bounded L2 norm ball) in such a plot. However, it is worrying that the FGSM-L2 attack gets worse as the L2 norm of the allowed perturbation increases. Intuitively, allowing a larger norm attack should make it easier to force the targeted action; as a result, I am worried that the implementation is simply incorrect.\n\nThird, I would like to see ablation studies on the L2 norm regularization term added to CopyCAT. The intuitive explanation for why it was added (we should minimize energy) doesn’t really make much sense to me - why do we want masks of low energy? Couldn’t we just have a separate version of CopyCAT that just does L2 ball projections? Also, in Fig 4., it seems that the attacks with extremely low weight on the L2 norm regularization term perform better, so why have the term at all?\n\nFinally, I am confused why FGSM is so much worse. It seems to me that CopyCAT is very similar to FGSM, but it is computed over an entire training dataset as opposed to for one image. However, I would expect that FGSM on a single datapoint x to result in lower loss on that datapoint than the mask (which maximizes the loss over the whole dataset, not just x). I don’t think it’s a problem if CopyCAT performs slightly worse than FGSM, since the universality of the masks is the interesting feature; I almost think it’s a problem if CopyCAT performs better. If it does indeed perform better, it’s important to figure out which element of CopyCAT makes it perform better.\n\nSome additional feedback:\n- In the abstract, you should say what your contributions and results are.\n- I think having a section that very clearly outlines the differences (maybe by writing the two algorithms side-by-side) between FGSM and CopyCAT would be immensely helpful for understanding what your contributions are.\n- I think having more plots about how the masks transfer to out-of-distribution inputs would be really interesting. For example, you could explicitly start the agent off at states not seen in the data used for training CopyCAT and check if you can still force a specific action using the mask.\n- I would also add an additional baseline beyond just DQN vs Rainbow; while the two are different, they are trained to do similar things. I would also consider doing something like Randomly Initialized DQN vs trained DQN, since those have very different goals.\n"
        }
    ]
}