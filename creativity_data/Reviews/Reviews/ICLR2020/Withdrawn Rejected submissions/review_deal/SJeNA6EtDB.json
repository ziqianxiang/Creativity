{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes to parametrize each layer of a deep neural network, before training, with a low-rank matrix decomposition, namely SVD. The convolutions are accordingly replaced by two consecutive convolutions (channel-wise or spatial-wise depending to the shape of the matrix the weights are reshaped to). The decomposed method is then trained with an additional l2 orthogonality regularization on the factors of the decomposition, as well as an l1 (Hoyer) regularization on the eigenvalues of the SVD to control the rank of each decomposition.                                                                  \n\nThe idea is simple and appealing but not novel. Decomposing neural networks is already widely studied, both before training and post-hoc, with various regularizations. In addition, the proposed methodology is suboptimal due to the (arbitrary) reshaping of the convolutional kernels to matrices. By contrast, tensor based methods can apply decomposition directly to the tensor (e.g. Higher-order SVD), resulting in much less parameters.\n\n* The reviewer claim in 2 that batch-normalization breaks the theoretical guarantee that the decomposed layers can approximate the operation of the original layer and will largely hurt the inference efficiency: i) most methods actually train the decomposition end-to-end. ii) even for post-hoc decompositions, fine-tuning is typically used to (successfully) restore performance.\n* The experiment on the effect of the l2 regularization is interesting. The same one should be carried for the sparsity inducing regularization.\n* Figure 2 and 3 are hard to read.\n* There should be a table of comparison indicating number of parameters and performance of the proposed methods, compared with previous work.\n* The authors do not mention tensor decomposition based approaches, e.g. \"Speeding-up convolutional neural networks using fine-tuned cp-decomposition\", ICLR 2015, \"T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor\", CVPR 2019. These methods (which validate the rank as an additional parameters) should be compared with the proposed approach, in terms of compression and performance (these methods obtain better performance for small compression rates for instance).\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces an approach to network compression by encouraging the weight matrix in each layer to have low rank. Instead of relying on SVD during training, so as to minimize the nuclear norm of these matrices, the authors propose to explicitly factorize the weight matrices into an SVD-like factorization U*diag(s)*V, and treat the resulting matrices as new parameters. Then, penalizing the rank can be achieved via a sparsity-inducing regularizer on the elements of s and by encouraging U and V to be orthonormal.\n\nThe idea is simple, yet very natural, to the point that I am surprised it hasn't been done before. However, I could not find any other paper doing so.\n\nRelated work:\n- Note that additional methods than those cited in the paper have proposed to decompose the weights post-training, e.g., Denton et al., NIPS 2014; Lebedev et al., ICLR 2015. It would be worth discussing these methods.\n- More importantly, while the authors discuss the work of Xu et al., 2018, which aims to minimize the nuclear norm during training, they fail to discuss that of Alvarez & Salzmann, NIPS 2017, which also proposed to do so. In that work, nuclear norm minimization was achieved via proximal gradient descent. Since the proximal operator for the nuclear norm regularizer has a closed-form solution, this prevents the instabilities mentioned in this submission when discussing the work of Xu et al., 2018.\n\nExperiments:\n- While the experiments show the good behavior of the proposed method, I feel that a comparison with Alvarez & Salzmann, NIPS 2017, would give a more complete picture, due to the aforementioned close relationship.\n- It is also disappointing that the results of TRP (Xu et al., 2018), which constitutes the other closest baseline, are only reported on CIFAR-10, not on ImageNet (based on Table 5 in the appendix).\n- As a matter of fact, the results on ImageNet are less impressive than those on CIFAR-10. As such, I believe that a complete comparison with all the baselines is important.\n- While the results in Table 1 show the impact of the orthogonality regularizer, the reasons why this is the case are not clear to me and are not explained by the authors.\n\nSummary:\nI have mixed feelings about this paper. While I like the general idea, I feel that some experiments are currently lacking to fully support the authors' claims.\n"
        }
    ]
}