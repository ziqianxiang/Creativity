{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper proposes solving the video summarization problem with a knapsack constraint using ILS. They also proposes a new benchmark dataset for this task. Overall, i have many concerns about this paper:\n\nThe connection from video summarization to ILS is novel. However, as it is unwritten, it is unclear what further contribution this paper introduces, and it seems like the novelty ends here. Regarding the ILS parameters defined in the paper - such as perturbation function, acceptance criteria, etc. - did the authors propose any novel mechanisms that exploit structure within the video summarization problem to deliver better performance over standard ILS techniques? Currently it is unclear whether the considered mechanisms are standard for the ILS framework or not. If they are standard, could the authors explain why other strategies weren't considered?\nThere is a lack of experimental evaluation. The authors cite numerous related work; however, Tables 1 and 2 include only the authors' own baselines. While this ablation study is useful, the performance of the proposed method cannot be accurately characterized without evaluation on the complete field of related work cited in the paper.\nThe proposed MIP baseline is weak - PuLP is a modeling language, not a solver. Which solver did you use? There is a order of magnitude difference between free ones and commercial ones.\n\nDue to the 1) seemingly limited novelty and 2) subpar experimental evaluation, I vote to reject this paper. If these concerns are thoroughly addressed, I would be happy to increase my score.\n\nAnother concern I have (that did not influence the decision in my review): the proposed method assumes given the chopped shots of a video. Does prior work make a similar assumption as well? It seems as if there could be large gains made by learning a good way to chop/trim shots along with the shot selection itself. For example, if a given shot is 5 seconds long, but only 3 seconds of it is actually useful, it seems suboptimal to have to either include the whole 5 seconds or exclude it.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "ILS-SUMM is applying ILS (iterated local search) to video summarization problem. The paper is well written and explains the method succinctly. The authors also introduce a new dataset for video summarization which consists of 18 movies of duration 10-104 minutes and summaries of <4 minutes. \n\nAuthors claim that the application of ILS to video summarization is first, which if true (i verified and could not find other references that negate the claim) seems like valuable work and connection. But, I think the authors can strengthen the paper further by addressing improvements along the axis below:\n\n* Authors just apply ILS to the problem of video summarization, is there any change to ILS that we can do to make it work better for video summarization. ANy insights of it's limitations?\n* Both the datasets are very small (25, 50 videos) to do real comparisons of hand crafted vs deep features\n* Authors don't talk about how they can incorporate semantics (specific people, actions etc.) in to the framework which is still important after doing shot boundary"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a method for unsupervised video summarization. The problem formulation considered in this paper is to minimize the total distance between each shot in a video and its nearest shot in the selected summary. The authors propose to use iterated local search (ILS) to solve this problem. To escape local optima, the authors propose to add a perturbation heuristic, which randomly swaps in a number of random shots. The method is evaluated on 3 datasets. The authors choose to use color histograms as shot features. In ablation study, the results show that the proposed ILS-SUMM outperforms a vanilla Local-Search-SUMM and an improved variant of Restart-SUMM. It also outperforms existing methods on all three datasets.\n\nOverall I found this paper clearly written. The method is clearly described. The ablation is helpful for understanding the design choices. It's also nice that it outperforms existing methods in terms of total distance. \n\nMy main concerns are regarding the experiments. \n1. Does total distance reflect the quality of video summarization well? There exist human annotated video summarization datasets. It'll make the results more convincing if the authors can show that the selected metric with the selected features (color histograms) reflects the human-judged quality. \n2. How does it compare with supervised methods? I understand that being unsupervised is a useful feature (e.g., don't require expensive annotations), but it would be useful to assess the trade-off. For example, how many annotations do we need to outperform an unsupervised method? If the number is small, it suggests that unsupervised methods probably haven't reached to point being practically useful yet. Presenting these results and discussions would help readers build a more comprehensive view of the area of video summarization. In addition, if it works worse than supervised methods, knowing how big the gap is would also be useful.  (It's okay to perform worse than supervised methods, but having some comparisons would be useful. )\n3. In Sec 4.3, the authors discussed about the pros and cons of using deep features. While I agree with the authors on the observations, I think it's still useful to present quantitative results, so readers can compare them with color histogram results.\n4. Since the supporting of the proposed method is solely empirical (instead of providing new theoretical results), I believe making the experiments more thorough and extensive would greatly benefit this paper. \n5. I also think the proposed method is technically light-wight. This is not necessarily a weakness, but I'd have a higher standard on experiments if the experiments are among the main contributions. \n\nOverall I think the paper would benefit from extending the experiments to make results more convincing. In its current form I think it's not ready for publication yet. "
        }
    ]
}