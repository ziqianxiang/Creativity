{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\nThe paper proposes a new technique for unsupervised learning of disentangled representations. The new module is called \"split encoder\", and is derived by adding an information isolation requirement for each of the factors of variation. \nDifferent from  unsupervised techniques which enforce independence of the different factors. Here the relation between the representation and the data is discussed, revealing the importance of conditional independence of factors.  \n\n\n\nPros:\nWhile a bit hard to follow at times, the writing is quite good and the topic is important and relevant to the field. \nI appreciated the theoretical derivations of this work, emphasizing the relation to data when \ndiscussing disentangled representations. The connection between the independent information assumption and the two KL terms is also quite interesting. \nFigure 2 and 3 nicely demonstrate the assumed correlation between MIG and the conditional independence. \n\n\nMajor issues:\n\nMy main issue with this work is the split encoder contribution. I didn't find very strong evidence of significant gain due to the split module. To adopt such modifications in standard architectures like VAE's, a much more rigorous experimentation is needed show possibly multiple levels of independence and not just two (split / no-split). \n\nOne important inductive bias is not discussed i believe, which is the number of factors. This seems assumed to be known in advance. Related to this is how can we prevent the case of combining two factors into one latent dimension? I think this will still fulfill the information requirement...,but could surely hurt generalization-ability. \n\nAs explicitly stated in Section 2, the assumption is not sufficient to guarantee disentanglement and the reconstruction error should indicate that one does not converge to the trivial solution of zero mutual information between z and x. That said, looking at the reconstructions in Figure 5, there seem to be a pretty high error for such easy data. I certainly find it hard to agree with the description: \"the learned representations are well disentangled except some small flaws\". In fact, the Vanilla VAE results appear more appealing and not less disentangled to my eyes.  Also, in Figure 8 the highest MIG is for the vanilla VAE. \n\nIn Section 5.2.2 a result for cars 3D behaves differently than dSprites. This is one reason (among many others) to show experiments on more than 2 datasets. I believe the work by Locatello et. al. speaks strongly for that and should be taken seriously. Even more so when using a single metric (MIG). Moreover, the explanation given on the strength of the enforced disentanglement, can it be verified by increasing betta for vanilla-VAE? I would be much more convinced by this explanation. \n\nLastly, in the end of 5.2.2 the authors state: \"o..obviously have better reconstructions...\". IMO, the results are very much alike. Perhaps specific pointer or zoom-ins to better reconstructions example would be helpful, but from staring at the results for a bit, I can't say there is an obvious advantage to the split-encoder. \n\nMinor issues:\nThis one is not clear to me: \".. And rarely ensuring independence like FactorVAE is not sufficient for learning disentangled..\"\n\nthe analysis in 5.2.1 about dSprites being simple, etc. is not clear to me. \n\nin the kl-div Vs. reconstruction error experiment, what are the different data points? also, how are the recon-errors measured? (what units is 1490-1510?)\n\nsome typos spotted:\n\"cannot ensures\"\n5.2.1: combing -> combining\n\"Therefore, for a single factor...\" --> the variable there should be z_j and not z, i think. \n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: \nThe paper provides new information theoretic definitions for disentanglement under which the combination of statistical independence of factors and conditional independence are sufficient for achieving disentanglement. They then argued that existing approach do not successfully enforce conditional independence and thus propose new architecture that they claim improve the enforcement of conditional independence, and then show empirically that their methods work.\n\nDecision:\nReject. This paper makes a number of claims that are either conceptually problematic or mathematically incorrect.\n\nIssue 1: Conceptually problematic definitions of disentanglement\n\nFirst, their definition of disentanglement is problematic as it admits models that are arguably not disentangled. The proof that their definition admits such undesirable models can be constructed exactly via Locatello’s impossibility theorem in conjunction with use of an invertible model to ensure that “for any factor z_j, its containing information in data is irrelevant to other factors”. \n\nFor a simpler example consider the case of an invertible discrete latent variable model G: Z -> X where p(Z) is uniform. Suppose further that each dimension of Z is interpretable. Such a model satisfies the author’s information theoretic definition of disentanglement. However, if you permute the space of Z via a permutation function P, the resulting model will still satisfy the author’s information theoretic definition but fail to be interpretable. \n\nIssue 2: Misuse of the term conditional independence\n\nConditional independence of q(z|x) is strictly a consequence of the variational family of choice. The assertion that increasing the “no-sharing-parameter” block will further encourage conditional independence in q(z|x) is thus incorrect. Once you have committed to using a factorized Gaussian variational family, you are using a q(z|x) strictly satisfies conditional independence. Increasing the no-sharing-parameter block is will not “encourage” any additional independence. \n\nI am completely willing to accept the possibility that increasing the no-sharing-parameter block may empirically improve the recovery of interpretable features. However, attempting to justify the no-sharing-parameter block by appealing to a flawed application of information theory and misunderstanding of the variational family in VAEs hurts the legitimacy of the paper. \n\nI strongly recommend the authors to rethink the narrative of the paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "On its surface structure, the paper looks well organized and existing literature on disentangled representations is well reviewed. However, the paper is extremely difficult to read and it is hard to follow the reasoning of the authors. I hope that the authors can clarify misunderstandings during rebuttal.\n\n“Existing works on disentangled representation learning usually lie on a common assumption: all factors in a disentangled representation should be independent. We argue that this assumption is not sufficient and another assumption is vital for disentangled representation learning: information contained in each factor of a disentangled representation is irrelevant to others, i.e. the containing information about data of factors is isolated. “\n\n-How can the first sentence be claimed if there is not a consensus about the formal definition of disentangled representations as stated in the introduction?\n- Assuming that ‘.. [Independence] is not sufficient  for disentangled representation learning’. Then you propose an additional assumption that seems to be the central contribution of the present paper:\n\n‘information contained in each factor of a disentangled representation is irrelevant to others, i.e. the containing information about data of factors is isolated’.\n\n- I don’t understand this notion of information that must be different than independence, namely that ‘information contained in each factor [...] is irrelevant to other [factors] ’ and \n‘the containing information about data of factors is isolated’.\n\nI guess the notions of ‘data of factors’, and ‘information being isolated’, ‘irrelevant information’ need to be clarified. The text in the introduction is unfortunately also not very helpful\n\n“... Since each factor [...] corresponds to a single factor of variation in data, \neach factor only contains the information of the corresponding factor of variation in data.”\n\nI think there is a collection of ‘factors’ and a collection of ‘factors of variation in data’, and between these two objects there is a correspondence. The statement ‘each factor only contains the information of the corresponding factor of variation in data’ implies that a ‘factor of variation in data’ is an object that posses information that is contained in the factor.\n\t\t\t\t\nAt the end of the introduction, the authors state \n‘conditional independence originates from no-sharing-parameter block and the independence’ of noise in reparameterization trick, which can be regarded as inductive biases on model. \n\nThis statement is alarming as it hints at a clash of terminology, the concept of independence in the information theoretic/probabilistic sense -- that is the density of two random variables can be written exactly as the product of marginals -- with ‘the lack of a functional dependence’. \n(Consider the Box-Muller method for generating independent Gaussian random variables x and y, where x = r cos(2pi u) and y = r sin(2 pi u) where u is uniform on the unit interval and r is exponential with rate 2. x and y are independent random variables but they are both functions of  r and u. Hence parameter splitting is not necessary for independence.)\n\nThen, we encounter (9) and the following statement \t\t\t\t\n\"To conclude, conditional independence originates from no-sharing-parameter block and the independence of noise in reparameterization trick, which can be regarded as inductive biases on model.\"\n\nUnfortunately, this conclusion is not correct. (9) is the defining property of an encoder with a factorized Gaussian -- it does not follow from 'non-sharing' of parameters. To see this, assume each mu_j = f(x), i.e. all means are the same, all parameters are shared. This is not a very flexible encoder distribution but still enjoys the same conditional independence structure, because parameter sharing and independence are different concepts. \n\n(8) is misleading as mu and sigma are not random variables in a VAE.\n\nI suspect there may be some interesting ideas here but the paper clearly needs further iterations for clarification of concepts so it is accessible for the general technical audience.\n"
        }
    ]
}