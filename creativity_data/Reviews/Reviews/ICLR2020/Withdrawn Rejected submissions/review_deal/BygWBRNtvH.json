{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper conducts a large experimental study of various methods for adversarial attacks and defenses. The authors evaluate a wide range of proposed methods in four different attack scenarios (white-box, transfer, score-based black box, decision-based black box). The authors pay particular attention to the trade-off between attack budget (perturbation size) and the accuracy that a method achieves for a given attack budget (as opposed to only comparing at a fixed perturbation size). Moreover, the authors also measure the trade-off between the number of iterations of an attack and the resulting adversarial accuracy.\n\nOverall I find comprehensive benchmarks a valuable contribution to the literature. However, for such benchmarks to be insightful, they need to explore all relevant parameters of the experiment. Partially due to the broad scope of the benchmark (e.g., covering four different attack scenarios), I unfortunately find the current paper lacking in some aspects. Hence I recommend to reject the paper at this point. I encourage the authors to extend their benchmark (and possibly reduce its scope if necessary) and submit the updated results to another conference.\n\nIn particular, the following points would improve the benchmark:\n\n- In the white-box experiments, it is unclear how the hyperparameters of the various attacks (step size, etc.) were chosen. While the values are specified in the appendix, it is known that hyperparameters can sometimes have large effect on the relative performance of various attacks. Hence it would be important to explore the impact of these hyperparameters on the results in the paper.\n\n- The paper measures the trade-off between perturbation budget and adversarial accuracy. While this can indeed give a more comprehensive picture of the performance of various methods, it is also important to note that some defenses are optimized for a certain target perturbation size (e.g., the perturbation budget used in adversarial training). To get a more comprehensive picture, it would therefore be useful to have multiple defense models trained with different hyperparameters.\n\n- For the transfer experiments, my understanding is that the experiments use only a single source model (one for CIFAR-10, one for ImageNet). Why is this the case? The effectiveness of transfer attacks depends on how similar the source and target models are. So utilizing a broader set of source models could yield a more comprehensive evaluation of transfer robustness.\n\n\nFurther comments / questions:\n\n- Why is MIM listed as a transfer attack in Section 3.1?\n\n- As mentioned in the paper, some of the randomization and transformation defenses were already broken in prior work. What additional insight can we derive from including them in the current benchmark?\n\n- Section 4.2 states \"[...] we do not evaluate PGD since PGD and BIM are very similar [...]\". Why do the authors prefer the term BIM considering that projected gradient descent has a long history in optimization?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1100",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper performs a large-scale empirical study by comparing different adversarial attack and defense techniques. From these experiments, they conclude that the robustness of different techniques varies based on perturbation budgets and attack iterations. They also found that adversarially trained models have, in general, the most successful defenses and the randomization based defenses are generally more robust for black-box attacks. \n\n+ The author presented a comprehensive background of adversarial attacks, which itself is a good contribution to the literature.\n+ I think the main contribution of the paper is the used of two curves: accuracy vs. perturbation budget and accuracy vs. attack strength curves to evaluate the attacks and defenses. I agree with the authors that such curves show a complete spectrum of weakness and strength of a technique, and future researches should adapt such evaluation.\n+ I appreciate the authors’ effort to compare 16 states of the art attacks and defense techniques.\n\n- However, the study findings are not that novel. For example, it is intuitive that the robustness of a technique will vary with the perturbation budget and iteration steps. Also, PGD-based adversarial training is known to be state of the art defenses, and I am not surprised at all with their findings.\n\n- Since the authors used different defense models for CIFAR-10 and ImageNet, it becomes difficult to compare the model’s performances across the dataset. \n\n- The title is somewhat misleading---there is no benchmark tool or dataset that the paper contributed. Their main contribution is a thorough empirical study. \n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents an evaluation of a number of kinds of classification models under various adversarial attack methods.\n\n\nFor classification models, the authors consider a collection of defenses ranging from those trained with robust optimization to those utilizing input transformations. The set of models includes those that have been previously shown to be broken (e.g. Song et al., 2018a, Xie et al., 2018, among others), and includes defenses designed for a wide range of threat models.\n\n\nFor attack methods, the authors include various attacks for a mix of threat models covering the white-box setting, black-box settings, and various allowed l_p norm perturbation bounds.\n\n\nThe study presented by the paper has unclear significance, does not present key information, and does not compare with key previous work. For these reasons I do not recommend acceptance.\n\n\nDetailed feedback:\n\n\n- The study is of unclear significance. From a security perspective, attacks on defended models are only interesting if the threat model is well-specified, and the attacker is able to dynamically adapt to the model within the threat model. Here, the attacks are blindly applied to the various defenses---as shown in [obfuscated gradients], this will not give a meaningful worst-case evaluation. Moreover, the fact that breaking the threat model will allow an attacker to easily bypass a defense is well known (e.g. Sharma and Chen 2017 [0]).\n\n\n- In a related note, in the conclusion section, the authors state that it is important to evaluate defenses at various attacker configurations (e.g. varying the number of steps and l_p perturbation allowed) to determine the superiority of one defense over another. However, in the security setting for adversarial robustness, the important metric is the accuracy in the specified threat model that is being defended against in the worst case.\n\n\n- The authors do not clearly state what the defenses' intended threat models are. It is therefore hard to evaluate the impact of the various attacks on the defenses in a meaningful context.\n\n\n- The authors do not discuss a number of previous related works. On the subject of transfer based attacks, Liu et al 2017 (ICLR ‘17) [1] study extremely familiar phenomena around varying the strength of the attack and its effect on transfer attack success. For how models stand up in various threat models, including those that they were not designed for, Kang et al [2] study this in depth as well and they are not cited. Finally, Khan et al 2019 [3] study randomization in the context of black-box attacks as well. \n\n\n[0] https://arxiv.org/abs/1905.09871\n[1] https://arxiv.org/abs/1611.02770\n[2] https://arxiv.org/abs/1905.01034\n[3] https://arxiv.org/abs/1905.09871\n"
        }
    ]
}