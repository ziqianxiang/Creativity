{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary:  Inspired by recent methods that aim to make neural networks (NNs) robust by training on both in- and out-of-distribution (OOD) data [Liang et al., 2017; Lee et al., 2018], this paper addresses the question of if the same level of robustness can be obtained without training on an OOD set.  The authors propose introducing an auxiliary variable $d_{\\text{in}}$ to directly model whether a given input originates from the training data or not.  Of course, training a function to predict $d_{\\text{in}}$ directly would require OOD data or at least a simulation of OOD from in-distribution data.  To bypass this obstacle, the authors make an argument that parametrizing the softmax input with two functions---$f_{c}(x) = h_{c}(x)/g(x)$---is reminiscent of computing the conditional probability $p(y|d_{\\text{in}}, x) = p(y, d_{\\text{in}} | x) / p(d_{\\text{in}}| x)$, with $h$ representing the joint density and $g$ the marginal.  The paper also proposes an input perturbation strategy similar to Liang et al.’s [2017], the difference being the perturbation magnitude is set by maximizing $h$ instead of on OOD data.  Experiments are reported showing performance on OOD detection, OOD detection when having access to OOD data during training vs when not, the effect of different choices of $h$, the effect of the proposed data augmentation strategy, the effect of number of samples, classes, and architecture parameters, and the effect of the distributional shifts being semantic vs non-semantic.       \n\nPros:  Making deep learning methods robust to distributional shift is essential for their safe deployment.  As the paper notes, many previously proposed methodologies require access to OOD data.  However, this reliance on OOD data not only adds implementation costs (extra memory and computation) but also makes the choice of OOD set a crucial modeling decision.  I am unaware of any rigorous guidance for selecting a representative OOD set for a given training set.  This paper makes some incremental but important contributions in relieving ODIN of its dependence on OOD data.  These proposals are supported with a fairly thorough experimental investigation.  The paper reports not only comparisons against sensible baselines but also performs ablation studies to isolate the effect of parametrization choices and their data augmentation strategy.    \n\nCons:  My most significant critique of the paper is that it exaggerates its contributions and ignores a large body of previous work.  There are two falsely claimed contributions in particular.  The first is that the paper is the first to consider OOD detection without training with OOD data: “To our knowledge, this is the first study focused on such a minimal setting, investigating the feasibility of learning out-of-distribution detection without out-of-distribution data” (p 2).  This is patently false.  Classification with a rejection option has been studied for many years, and much of this work does not assume access to an OOD set.  For just one example, Hellman [1970] considers k-nearest neighbor classification with a rejection option and requires no OOD set.  Perhaps the authors meant this as a contribution w.r.t. neural networks, but that is not the case either.  Cordella et al. [1995] studied rejection rules for MLPs, and Geifman & El-Yaniv [2017] studied selective classification for deep networks.  To the best of my knowledge, neither of these works require OOD data for training (although I am not intimately familiar with the methodology). \n\nThe second falsely claimed contribution is the use of an auxiliary variable to represent that the data is in-distribution: “We specifically add a binary variable, representing whether the data is in-distribution or not” (p 2).  It surprises me that the authors would think that this idea hadn’t been proposed before, given its a rather obvious thing to try.  The idea dates back at least to Cortes et al. [2016]; see Section 2.1 on “General Rejection Models.”  Geifman & El-Yaniv [2017] use the same framework in the deep learning setting, defining a binary “selection function.”  Given these omissions, the paper cannot be accepted in its current form.  It requires substantial revisions: discussion of this previous work and modification of the claimed contributions.  I think a proper characterization of the work would to be to call it an extension of ODIN: using a selection function to parametrize ODIN’s temperature.  \n\nYet this brings another question to the forefront: why is $h(x)$ used to determine rejection when $g(x)$ is supposed to be a proxy for $p(d_{\\text{in}} | x)$ (Equation 4)?  In the public discussion, the authors state that $g(x)$ does indeed work to a degree, but just not as well.  This seems to call into question the foundational motivations of the procedure, and the authors should address this by including experiments comparing $h$ and $g$.  \n\nFinal evaluation:  The paper over-claims its contributions and ignores some very related work (e.g. [Geifman & El-Yaniv, NeurIPS 2017]), thus necessitating substantial revisions to the text.  Moreover, the $g(x)$ selection function has not been demonstrated to behave as the core motivation for the work implies it should---namely, modeling $p(d_{\\text{in}} | x)$.  \n\n\n\nReferences\n\nCordella, Luigi Pietro, et al. \"A method for improving classification reliability of multilayer perceptrons.\" IEEE Transactions on Neural Networks 6.5 (1995): 1140-1147.\n\nCortes, Corinna, Giulia DeSalvo, and Mehryar Mohri. \"Learning with rejection.\" International Conference on Algorithmic Learning Theory. Springer, Cham, 2016.\n\nGeifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems. 2017.\n\nHellman, Martin E. \"The nearest neighbor classification rule with a reject option.\" IEEE Transactions on Systems Science and Cybernetics 6.3 (1970): 179-185."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: This paper proposes a method for detecting out-of-distribution (OOD) data. Unlike most methods in this direction, this paper considers the scenario where a sample of out-of-distribution is not observed. Thus, the goal is detecting the out-of-distribution data using solely in-of-distribution data. First, the authors proposed to use input preprocessing similarly to ODIN (Liang et al., 2017) but the proposed scheme does not require out-of-distribution data. The way to search a parameter is different from the existing work (Liang et al., 2017). The proposed scheme looks simple and highly effective in the experiments. They also propose to decompose the learned function $f_i(x)$ into two parts for making a good confidence score function. Finally, they illustrate a good performance on OOD detection using their proposed methods (combining the decomposition and preprocessing).\n\n========================================================\nClarity:\nOverall, I found this paper is well-written and easy to follow. Only Section 3.1 highly concerns me on the clarity and correctness of the paper.\n\nThe part that I am most concern about is the decomposed confidence, I am not sure that the argument of the proposed method about confidence decomposition is correct. If the authors can convince me that it is correct, I will consider improving the score.\n========================================================\nComments:\n\nOn the claim of the novelty of the problem setting:\nThis paper claims that this paper is the first study focused on a minimal setting, where I think it means the setting that we do not assume that we have OOD sample when training a model. However, there already are existing works on this issue. And it is more important to discuss them in the Related Methods section (2.1) rather than mentioning the most related works as the work that requires true OOD sample when training. I read the discussion in the comment section on how the authors tried to state the reason why they think it is novel, but I think it is not sufficient. It is the freedom to choose how to solve this problem, e.g., one may try to construct pseudo-OOD data from in-distribution data. But the core setting is the same, that is, OOD detection without OOD data. The authors may instead criticize that the method that generates pseudo-OOD from in-distribution may be unreliable, biased, and that's why other methods perform better. For the discussion about Hyperparameter-free, to me, it is simply a unique feature that Techapanurak & Okatani (2019) wanted to highlight as a contribution. Thus, the author may suggest a way to criticize that paper, e.g., it's better to tune so we can do well on the dataset we are interested in instead of saying that hyperparameter-free makes the setting different. Not only these two works I mentioned, I believe OOD detection itself has a long history and I do not think it is important to state that this paper is the first one to consider such a setting. \n\nOn the confidence decomposition Section (3.1):\n\nOn why the network is overconfident:\nIn 3.1, the authors attempted to illustrate why neural networks may give an overconfident value for OOD data. I strongly agree that it is an important problem to discuss. In the example in 3.1, it is understandable that in Eq. (4), both values of numerator and denominator can be low when using OOD data. However, from my viewpoint, Eq. (4) suggests that in practice, the left-hand side of Eq. (4) can be unstable when x comes from OOD data (since the denominator can be very small, so as the numerator). And we do not have much data in that region, which makes p(y|x) estimation difficult in that region. From this logic, I think this equation does not only suggests overconfident but also it can tend to be underconfident too, especially in the semantic shift scenario. Thus this Eq. (4) does not suggest that the network tends to be overconfident but tend to have an extreme value (please correct me if I misunderstood). Therefore, it would be better to focus on discussing this issue of stability rather than giving a bit ad-hoc example 0.09/0.1 = 0.9 (overconfident). Although underconfidence can be easily handled because those data are easy to detect them as out-of-distribution.\n\nOn the influence of Eq. (4) to inspire $f_i(x) = \\frac{h_i(x)}{g(x)}$:\nI cannot see how $g(x)$ will relate to the denominator of Eq. (4). It is true that $g(x)$ is a function that is not class-specific, similarly to the denominator of Eq. (4). However, the denominator in Eq. (4) is just simply marginalized y out by taking a summation of all class-specific function. Thus, I think the only way to relate Eq. (4) to Eq. (5) is $\\sum_{j=1}^C \\exp f_j(x)$ is the unnormalized version of the denominator of Eq. (4), while $\\frac{h_i(x)}{g(x)}$ is an unnormalized version of the numerator. Thus, I am not convince that Eq. (4) motivates $f_i(x) = \\frac{h_i(x)}{g(x)}$.\n\n\nOn the input preprocessing Section (3.2):\nI found the method for selecting the perturbation magnitude is very interesting and this is the most thing I like about this paper. It made me convinced that input preprocessing is highly useful and impactful for tacking OOD detection without OOD data (while ODIN already suggested this fact in the context where we have OOD sample). It enhances the performance of almost all cases as we can see from Figure 2. I have a positive impression on this part. It would be nice to see the performance of Deconf-C, which is almost identical to Techapanurak & Okatani (2019) in Table 1 to highlight the improvement from this preprocessing for different OOD datasets instead of averaging them all (as Figure 2). I think it is not too difficult to modify this part to highlight a contribution of the preprocessing part (currently only Figure 2 highlights the usefulness of it).\n\nOn experiments: \nIt is also important to see a standard deviation of the results. Especially if we only run only a few times (three times in this context). If the standard deviation is high, it is highly suggested to run more times to get a reliable result. In addition, we may use a better criterion to bold a score (e.g., one-sided t-test). I also would like to know if the existing method has a higher standard deviation due to less stability when g(x) is not 1 or a constant (T) as they were in most existing approaches. Other than this point, I think the authors did a great job to visualize and show the effectiveness of decomposing a confidence score and the advantage of input preprocessing. Experiments on semantic/non-semantic shift is also interesting.\n========================================================\nDecision:\nAlthough this paper proposed an interesting simple yet effective way to preprocess the data combining with decomposing a confidence function into two parts, I feel that the discussion of the relationship with existing methods and the clarification of the novelty have to be improved. Also, I disagree with the explanation of decomposed confidence part, and it simply looks like a scaling factor that depends on x to me.  Finally, reviewers are suggested to put a higher standard for evaluation for a paper more than 9 pages. For these reasons, in the current form, I vote to reject for this paper. If I am convinced that the discussion about decomposed confidence is valid, I will consider improving the score.\n========================================================\nMinor comments:\n1. Does \\log between gradient and S exist in Eq. (7) like the paper by Liang et al., 2017 ?\n2. typo: Section 4.3: We therfore -> We therefore\n========================================================\nOn the similar work by Techapanurak & Okatani (2019):\nI am aware that the two works are concurrent and I still treat this part as the authors' contribution in my evaluation.\n\nIn my view, both works are are highly related.  I am aware that the work by Techapanurak & Okatani (2019) is also very recent (25 May 2019: arXiv) and the authors suggested it to be a concurrent work. It is also nice and I appreciate that the authors cited them. But since the authors discussed a little bit about their work, I think it is fruitful to discuss them in detail and point out the difference clearly, for example in the related work section. \n\nHere are four reasons I think why they are highly related. \n \n1. Same setting: OOD detection without OOD data\n\n2. The authors attempted to clarify the difference and pointed out that the proposed framework is more general than Techapanurak & Okatani (2019). However, in my view, Techapanurak & Okatani also proposed to decompose the function into two parts, where $s$ (or $s(x)$) in Eq. (3) of Techapanurak & Okatani (2019) is strictly the same as $\\frac{1}{g(x)}$ in this submitted work. Even before Techapanurak & Okatani (2019), people have been using this scaling parameter to improve the class posterior estimation (perhaps s = 1/T is the most common, although it does not depend on x). For the numerator $h_i(x)$, Techapanurak & Okatani (2019) proposed to change the standard $h_i^I(x)$ to $h_i^C(x)$. Although they did not write clearly that $h_i^I(x)$ can be something other than $h_i^C(x)$, the fact that they changed it is straightforward to see that we are able to generalize it and try other $h_i^I(x)$. Thus, the novelty of the generalization of Techapanurak & Okatani (2019) that this paper suggests is modest. \n\n3. The choice of $g(x)$ in this paper and $s(x)$ in Techapanurak & Okatani (2019) are almost identical. It is very slightly different in my view (not sure if it changes the performance). In this context, if I understand correctly, Techapanurak & Okatani (2019) used $g(x) = \\frac{1}{exp(BN(linear-transformation))}$ while this paper proposed $\\frac{1}{(1+exp(-BN(linear-transformation))}$.\n\n4. The $h_i(x)$ that works best is the $h_i^C(x)$ proposed by Techapanurak & Okatani (2019). \n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "- Summary: This paper proposes to train an OOD detection model without OOD data, because OOD is hard to define a-priori, such that OOD observed during training is not guaranteed to be matched with OOD at test time. They propose a decomposed confidence strategy and modification on input preprocessing. They show the superiority of their method over the state-of-the-art methods, when OOD is not available.\n\n\n- Decision and supporting arguments:\nWeak reject.\n\n1. The analysis around `g` is confusing. After equation 5, they state that \"`g(x)` is shared among all classes like p(d_in|x).\" At this point, I expect that `g` is a confidence score, but at last they used `max_i h_i` for OOD detection, and the role of `g` turned out to be just a learnable temperature in softmax. Also, even the learning objective for `g` is not introduced. Also, it seems the authors tried to make some relationship between equation 4 and their method, but I couldn't find their correlation.\n\n2. Contribution is limited. To my understand, the main novelty in this paper is on the learnable temperature `g`, and the others are already explored in the literature. The proposed three similarity measures are used in prior works, i.e., inner-product in Hendrycks & Gimpel (2017), Euclidean distance in Lee et al. (2018b) (though they claimed that Mahalanobis distance is better), and Cosine similarity in Techapanurak & Okatani (2019).\nThe essential modification on input preprocessing strategy is simply removing OOD data from the validation set, and this is a natural setting as their proposal is to train an OOD detection model without OOD data.\n\n\n- Comments:\n1. How is `g` trained? What is the learning objective and how the ground truth value is assigned? Since the activation of `g` is sigmoid, a natural conjecture is that in-distribution and OOD would be labeled as 1 and 0 (or reversed). However, they said they do not train their model with any OOD, so it is hard to figure out the learning objective and the ground truth value for `g`.\n\n2. Why don't you compare with the GAN-based OOD generation (Lee et al., 2018a) and the adversarial attack-based validation (Lee et al., 2018b)? They do not require any OOD as well. In section 2.1, the authors set the hyperparameter `alpha_l` in the Mahalanobis method to be uniform because no OOD for validation is available, but actually Lee et al. (2018b) proposed the adversarial attack-based validation method to avoid the dependency on OOD.\n\n3. Notation is confusing. `f` is used for multiple parts: `f_i = h_i/g` in eq 5 and `h` is a function of `f^p`. Some notations like `w_g` and `b_g` are not defined.\n"
        }
    ]
}