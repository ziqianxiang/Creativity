{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The goal of this paper is to explore the relationship between the capacity of the VAE, how well it fits the data distribution and consequently its utility in detecting out of distribution samples. One conclusion from the Nalisnick et. al work regarding the observation that data from SVHN scores higher in likelihood than data from CIFAR on a CIFAR trained VAE is that \"SVHN simply \"sits inside of\" CIFAR-10—roughly same mean, smaller variance—resulting in its higher likelihood.\". Following on this observation, this work notes that the observation of an out of sample point being scored with a higher likelihood can also happen when the model lacks the capacity to learn the ground truth support patterns of the data.\nThe work focuses on Gaussian VAE and discusses three scenarios of model capacity and their relationship to out-of-distribution detection. Empirically, when repeating the out of distribution experiments of Nalisnick et. al using a VAE (but this time varying the capacity of the VAE), the paper finds that VAEs can detect out of distribution samples and that a model trained on CIFAR has lower error on CIFAR samples than SVHN samples. The model verifies that this phenomena occurs due to the negative log likelihood term of the high-capacity model being much lower than that of the low-capacity model.\n\nOverall, I found the paper both interesting and well written. The contributions of this work are to show that the ability of deep generative models to detect outliers\nis closely tied to the capacity provided to the model.\n\nI have a few questions/concerns about the work that I would like clarity on:\n* One of the concerns I have is regarding the choice of architecture to test the VAE. Ostensibly, a way to make the point that this work does that continues on from the literature would be to start with the same VAE that Nalisnick et. al use and then proceed to show that their story changes when increasing the capacity of the underlying generative model. Was this experimental option considered? Essentially, does the experimental evidence herein suggest that the observation in Nalisnick et. al for VAEs was due to the use of a low-capacity VAE?\n* How does varying the choice of conditional likelihood function in the decoder from Gaussian to Categorical, or product of Bernoulli (distributions wherein there is no gamma parameter to consider during learning) change the story?\n* It is a little unclear what role Section 3.3 plays in the overarching claim being made that increasing model capacity (without overfitting) improves the ability of the model to detect outliers.\n* Do you have a sense of how much modeling capacity is *enough* capacity? Or will the answer to this question always depend on the contrast distribution that is considered \"out of sample\"?\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper provides an in-depth analysis of the recently observed phenomenon [7] that deep generative models may assign higher likelihood to out-of-distribution (OOD) examples for the specific model class of Variational Autoencoders (VAEs). The paper demonstrates that if appropriately designed in terms of network capacity (of the decoder) and latent dimensionality, VAEs indeed are capable of estimating the support of the data manifold and accurately detecting OOD examples. Different failure regimes (insufficient/excessive network capacity and limited/superfluous latent degrees of freedom) are identified and characterized showing that a proper detection model must find the effective balance of a suitable network capacity and sufficiently high latent dimensionality. The paper further formally demonstrates that too high of a latent dimensionality (larger than the original data manifold dim.) is less of a problem for VAEs since uninformative dimensions may just collapse in optimization, but this necessarily requires unbounded gradients to emerge during training. An experimental evaluation on Fashion-MNIST vs. MNIST and CIFAR-10 vs. SVHN/CelebA conclusively confirms the preceding theoretical considerations.\n\nI think this paper should be accepted since it clearly and concisely dismisses the existing hypothesis that deep generative models “don’t know what they don’t know” for the important model class of VAEs. The presentation of the paper is polished and the technical quality is high. The only two (minor in my opinion) pitfalls of this work are (i) some related work is missing, and (ii) the novelty/originality of this work is rather limited as much in this paper is closely related to previous work [4]. Overall, however, I strongly believe this paper makes an important and significant contribution to the ongoing scientific conversation on understanding the anomaly detection capabilities of deep generative models.\n\n(i) Some related work from two lines of research are missing that I believe might lead to interesting connections and insights in the future. Major works on deep anomaly detection [10, 8, 5, 9, 2] and works taking an information-theoretic perspective on VAEs [3, 1, 11]. Specifically, connections to the rate-distortion trade-off [1] and the so-called information preference property [11], which describes the phenomenon that powerful decoders (excessive network capacity) reduce the information encoded in the latent code and well approximate the data marginal p(x) independent of z, might lead to interesting insights in the future.\n\n(ii) As referenced in the paper, this work builds quite a bit on results from Dai and Wipf [4]. However, the previous work specifically does not consider the anomaly detection problem.\n\nSome questions that I have:\n(1) Did you find unbounded gradients to make optimization unstable in practice? If yes, do you have some practical advice?\n(2) Do you employ any weighting hyperparameter between the reconstruction term and the prior KL regularization in Eq. (1) as in β-VAE [6] in your experiments?\n(3) Did you find one of the two terms (reconstruction or prior KL regularization) dominate in the NLL? I’m curious if accurate detection is mainly to due errors in reconstruction or deviations from the prior in the latent encoding.\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. Clear demonstration that VAEs “know what they don’t know” if properly designed in terms of network capacity and latent dimensionality.\n2. Good technical quality. Theoretical considerations seem correct and experiments are scientifically rigorous.\n3. Good, polished presentation. Eloquent, yet not ornamental writing that is clear and concise.\n4. Practical recommendations on how to choose the network capacity and latent dimensionality are presented (Section 3.2).\n\n*Ideas for Improvement*\n5. It took me longer to process Figure 1 than it should have. Maybe reiterate what the latent and what the manifold dimensionality is directly in the figure and highlight the optimal case (d). Otherwise it is a nice illustration of the regimes.\n6. Trends in Table 2 would be easier to see in line plots.\n7. Extend experiments over the full capacity × latent dimensionality grid in the two setups (F-MNIST vs. MNIST and CIFAR-10 vs. SVHN/CelebA).\n8. Might add one-vs-rest deep anomaly detection benchmarks from Ruff et al. [8] and Golan and El-Yaniv [5].\n\n*Minor comments*\n9. Eqs. (1) and (4) numbering pushed down since equations are too long.\n\n\n####################\n*References*\n\n[1] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken ELBO. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 159–168, 2018.\n[2] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[3] X. Chen, D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel. Variational lossy autoencoder. In International Conference on Learning Representations, 2017.\n[4] B. Dai and D. Wipf. Diagnosing and enhancing VAE models. In ICLR, 2018.\n[5] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[6] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR, 2(5):6, 2017.\n[7] E. Nalisnick, A. Matsukawa, Y. W. Teh, D. Gorur, and B. Lakshminarayanan. Do deep generative models know what they don’t know? In ICLR, 2018.\n[8] L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393–4402, 2018.\n[9] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder, E. Müller, K.-R. Müller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019.\n[10] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146–157. Springer, 2017.\n[11] M. Tschannen, O. Bachem, and M. Lucic. Recent advances in autoencoder-based representation learning. In Third Workshop on Bayesian Deep Learning (NeurIPS 2018), 2018."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper studies the problem of detecting outliers using generative models (specifically VAEs) in the detail and describes the condition in which VAEs can be used for outlier detection. The paper is well-written and has a very detailed description of its claims. \n\nI found the paper insightful and clear overall, and I believe we need more papers like this in the area of deep generative models. I have some questions and comments which I think addressing them can increase the impact of the paper:\n\n- The interaction between the encoder/decoder seems to be important based on the latest papers in this area and just assuming “that the encoder network maintains sufficient complexity to produce a reasonably tight variational bound given the decoder” may not be enough for making conclusions about VAEs (see for instance “Tighter Variational Bounds are Not Necessarily Better” by Rainforth et al. 2019). Are there any experiments that you can add to show the robustness of your results with respect to the encoder network. \n- It was mentioned that for choosing the right capacity “it can be increased until there is a significant gap between train and test NLL values on the inlier data”; however, this was not shown in the experiments. Can you add the columns in Table 1 and Table 2 that after some point there will be a gap between the train and test NLL? \n- The current version of the paper seems to be ended abruptly and without any summarization of the contributions, it would be nice to have a discussion/conclusion section which discusses the main assumptions and limitations of the arguments in the paper. "
        }
    ]
}