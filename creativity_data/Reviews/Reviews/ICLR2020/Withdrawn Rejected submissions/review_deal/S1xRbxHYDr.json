{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies robustness and redundancy of deep models with the goal of understanding what controls the capacity in over-parametrized networks. Authors came up with their own definition of robustness and redundancy, ran several experiments and provided empirical observations about how these quantities change as one over-parametrize a network.\n\nWhat I really like about this work is taking an empirical approach to understanding the over-parametrization and providing experimental observations which can be very helpful for better understanding of these phenomenon. \n\nEven though I like the general approach to the problem, I think the paper can be improved in several ways. In general, I think the qualities and experiments can be defined in a more principled way. Below I point to specific issues:\n\n1- Six types of neurons have been defined very early in the paper (Section 2). However, they are not really contributing to the main arguments of the paper and are not used in the experiments in the main part of the paper. If they are not really central to the picture. It might be better to remove them. Otherwise: a)provide a clear definition for each one in such a way that it is possible to write a code to measure them b)argue why these are the only possible ways to limit the capacity? How did you come up with these six? Is there a principle behind it?\n\n2- Robustness: Why choose this definition for robustness? Here, it would be helpful to compare your definition to other definitions of robustness (for example, with respect to input perturbation or parameter perturbations) and argue why this notion make sense. It might be helpful to discuss its relationship to dropout and other methods. Also, is there any generalization bound that implies bounding your version of Robustness might be related to generalization?\n\n3- Redundancy: Here, instead of having a single definitions, authors come up with two different notions \"compressibility\" and \"similarity\". First of all, robustness definition is based on dropout which itself is a form of measuring redundancy so it is not clear why one need to define redundancy separately. Second, the way compressibility and similarity are defined seems arbitrary to me. Why choose this specific definitions? Why separate compressibility and similarity?\n\n4- Experiments: The training and test error is not provided for the plots in the main text which makes it difficult to see how these robustness and redundancy relate to generalization. Moreover, the fact that the redundancy does not show a consistent behavior in different experiments could be due to the definition seem very arbitrary. There are many more experiments in the appendix but in general it is very hard to see a simple and consistent story that is supported by experiments. \n\nMinor:\nIn section 2, I'd avoid using metric since it is not clear if these measures have properties of a metric.\n\nOverall, I appreciate the empirical approach of the paper but I think the definitions, arguments and experimental design could be improved significantly.\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The authors conduct an empirical study of the representations that deep neural networks learn as a function of model size, as measured by two different metrics: how quickly performance degrades when random neurons are removed (\"robustness\"), or how similar neurons are (\"redundancy\"). They show that deep neural networks tend to become more robust and more redundant as model size increases, though this is not necessarily true. In general, I believe that this sort of empirical approach is useful, and I appreciate the authors' efforts to study the phenomena of robustness and redundancy empirically.\n\nHowever, in my opinion, I do not believe that this paper is ready for publication in its current state. My central concern is the current imprecision of the paper, which undermines its scientific contribution. Many terms are ill-defined or fuzzy. For example, in section 2, 6 different capacity-constrained features are enumerated but not well-defined, including \"semantically redundant units\" that are \"units which are not linearly correlated to others and which provide a different representation from the previous one, similarly related to the learning task.\" What does it mean to \"provide a different representation from the previous one\" (and what previous one?), or to be \"similarly related to the learning task\"? \n\nCritically, there is a significant amount of speculation throughout the paper that is not clearly delineated from experimental observation.\nThis speculation involves the above ill-defined terms and/or are not backed up. As non-exhaustive examples:\n\n1) In page 3, robustness (as measured by neuron dropouts) is asserted to aid in distinguishing \"redundant functional units\" and \"semantically redundant units\" from other units because of differences in how robustness of those units change with model size; but this is not backed up and as far as I can tell, not experimentally investigated. Similar statements about redundancy and similarity are made in page 3. \n\n2) In page 3, \"L is more robust and equally or less redundant than S\": what is a holistic class representation? Why is that less robust than a \"bag-like representation\"?\n\n3) Section 4 (results) has a lot of speculation as well. For example, \"we take this discrepancy between robustness and redundancy trends as strong evidence that these models... are autoregularizing largely by forming qualitatively different representations... with units that are merely semantically redundant.\" These terms are not clearly defined, and there is scant evidence to back up these conjectures. \n\nAs it stands, it is difficult to accept the paper because of the many speculative claims it contains. The experimental findings are interesting and should be reported; and implications and conjectures should be discussed; but they should be carefully separated. I encourage the authors to critically examine the paper and remove or reword claims that are not directly supported by the results.\n\nOn the experiments: \n\na) For Figure 3, shouldn't the inceptionv3 and inceptionv3-layer graphs be identical when the model size factor is 1x, since the models are identical? What explains the large discrepancies?\n\nb) The number of similar neurons per neuron (e.g., Figs 3c and 4c) should increase with model size just because there are more neurons, right? (Mathematically, if we consider a set of $n$ random vectors in $\\mathbf{R}^m$, then as $n$ increases the number of vectors that correlate with each other should increase.) Is the increase that is experimentally observed any different from this?\n\n==\n\nAnother comment (no need to respond): Experimental procedures should also be well-defined. For example, since neuron ablations are central to the experiments, the procedure should be explained in the main text instead of being buried in a citation."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to evaluate two characteristics of DNNs, robustness and redundancy, in an attempt to link them with generalization and overparameterization.\nA central hypothesis in this paper is that increasing the number of parameters increases at least one of these two characteristics. As far as the empirical results show, this is true. \n\nWhere the line gets blurred is in the causal relationships between generalization, overparameterization, robustness and redundancy (and other unstudied phenomenons). A strict interpretation of the results in this paper show that: when there is overparameterization, there is also robustness and/or redundancy. Beyond that, the experiments in the paper are not fully able to answer harder questions with certainty:\n- does overparameterization really increase \"capacity\"? Extra robustness and redundancy could both be explained by a structural failure of DNNs to take advantage of having more parameters (I don't believe so, but it would be nice to [dis]prove!)\n- do robustness and redundancy arise from the same phenomenon that leads to generalization, or do they arise from the mechanisms that we usually associate with overfitting/fitting high-frequency-noise patterns? The paper's results suggest the latter, since the same phenomenons are observed when fitting random lables.\n- is this phenomenon limited to classification? visual problems? The synthetic problems show weaker trends and happen to not have a lot of structure within them.\n\nNonetheless, now that we know some consistent behaviours that emerge from overparameterization, we may be better able to continue digging.\n\n\nOverall I think the results of this paper are novel and valuable. They confirm a few hypotheses that were emerging in the literature concerning the odd behaviours of overparameterized DNNs. On the other hand, I think more could have been done to understand the _why_ of the observed phenomenons. \nOne big aspect that is missing is time. Beyond Fig. C3 which only looks at behaviours after convergence, there is no data regarding the progression of robustness and redundancy during training. Such trends might be vital in order to understand the order in which phenomenons happen in DNNs during training, and superposing such trends with other measures that have been linked to generalization and overfitting may help clarifying the dynamics of DNN training.\n\n\nSome comments:\n- why weren't the MLPs initialized with standard initializers (Glorot, etc.)? \n- The presentation of the paper is good. There are a few typos here and there but the paper was otherwise easy to read and understand.\n- I'm not sure I fully agree with the \"He\" initialization scheme being considered a \"high-variance\" scheme nor the \"LeCun\" initialization scheme being \"medium-variance\". Both these schemes, as well as Glorot et al, were designed with the intention of keeping variance _constant_ in depth. A better test would have been to scale these initializers by some constant c around 1.\n- \"Xavier/Glorot\", these are not two separate things but rather, Xavier is the first name of Xavier Glorot, author of the \"Glorot et al.\" paper.\n- I'm not sure where you found LeCun initialization to be sqrt(3) / fan_in, it should be 1/fan_in, see http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n- Even though the initializations, optimzers, models and data are fairly standard, it's still expected to cite the original papers. You cite none of them.\n"
        }
    ]
}