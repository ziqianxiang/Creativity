{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "# Review ICLR20, Geometry-Aware Visual Predictive Models of Intuitive Physics\n\nThis review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper.\n\nI apologize in advance for being reviewer 2.\n\n## Overall\n\n\n**Summary**\n\nThe article introduces an algorithm that given multiple cameras spectating a robot, can learn a joint 3D representation of the scene by embedding all individual 2.5D RGB-D images into a consistent 3D feature map over time and then using this to plan how actions would affect these entities.\n\n\n**Overall Opinion**\n\nI'm not entirely convinced about the novelty and practicality of this approach:\n- You need ground truth 3D positions and orientations to train this, as stated in your loss (Eq.1), right? So how is this useful? For any real robot, you hardly have access to that (with the robot itself occluding the views if you're trying to use a 3D tracking system). I could easily be swayed on this by seeing some real-robot experiments, but I understand how difficult and expensive they are to come by.\n- I understand that this is an extension of [Tung et al. (2018)][1] to include a forward model for planning, but I think you're vastly overselling the idea. In your list of contributions, you have 4 bullet points. (1) is basically Tung, so not novel, (2) is novel and at the heart of your paper, (3) is false. You just mention \"2-object scenes\" at some point and that's that. (4) is only overselling slightly - you're not showing ablations (plural), but 1 single ablation of your model which is \"no RGB, just depth\". And save for a single row in Table 3, there is no perceptible difference.\n- The method is underspecified. You included Fig.1, which is very similar to Fig.1 from [Cheng et al., 2018)][2] as well as Fig.3 from [Tung et al., 2018][1]. But you fail to provide enough equations to specify how action-tiling + prediction and unrolling work. Please give us some pseudo-code at least.\n- You mention accumulating error multiple times, but it'd be nice to see the data for that: in tables 1-3, add another 2 rows each for obj pos/orientation after 5 steps or something along those lines. According to yourself, this is where your model should shine.\n- (See section \"Intro\" below): please provide PlaNet (or [SLAC][4] if you prefer) or another contemporary method's baseline results as opposed to your 2D multiview.\n\n[1]: http://openaccess.thecvf.com/content_CVPR_2019/papers/Tung_Learning_Spatial_Common_Sense_With_Geometry-Aware_Recurrent_Networks_CVPR_2019_paper.pdf\n[2]: https://papers.nips.cc/paper/7755-geometry-aware-recurrent-neural-networks-for-active-visual-recognition.pdf\n[4]: https://arxiv.org/pdf/1907.00953.pdf\n\nOther than those general issues, here are some minor...\n\n## Specific comments and questions\n\n### Abstract\n\n- weird phrasing: \"3D scene appearance\"?\n\n### Intro\n\n- citation needed: \"Indeed, we, humans, excel in manipulation despite that less than 1% of us know what inertia is\" (also I _think_ you can remove that second and third comma, otherwise there's a weird pause).\n- The whole section about Newtonian physics can be cut. The idea is clear without that paragraph (from \"Though Newtonian physics also describes ...\" to \"... particles and their displacements\")\n- You've failed to cite the very related work of [Hafner et al.,2018][3] (better known as \"PlaNet\"). And while we're at it - they released their full code, so it'd be nice if you could run it as a baseline on your simulated environment and provide the values in Tables 1-3.\n- The intro needs several edits - there is too much literature that then gets repeated in the related works section and there is a whole paragraph (starting with \"We propose learning models...\") that would be a better fit as overview in the Method section (instead, here a 3 sentence summary would be enough - how is this model extending Tung/Cheng?).\n- The entire paragraph starting with \"ii) Methods that predict the future in a hand-designed 3D space\" is not relevant to your work and can be cut.\n\n[3]: https://arxiv.org/pdf/1811.04551.pdf\n\n### Related Work\n\nall good\n\n### Method\n\n- Figure 1: The whole thing is not entirely clear - crucial details are missing, like how the action tiling and rollouts actually work. And next to \"(B)\" there is this illustration for the 2D to 3D unprojection and after reading Tung and Cheng and this work, I understand the method but I still don't understand what this illustration is supposed to mean.\n- You should mention clearly that you need ground truth positions/orientations of all objects for this to work. That's a huge thing to ask.\n- \"constancy\" -> \"consistency\"?\n- \"map,fed\" -> \"map, fed\"\n- You mention GRNNs being inspired by SLAM - so do they use Levenberg-Marquardt optimization or Gauss-Newton? Or is it just the general concept that you mean of integrating multiple views into one?\n- \"grid location holds an 1-dimensional feature vector F, as we show in Figure 1 right.\" -> where is this shown? Also \"a 1-dimensional\", not \"an ...\".\n- Is \"DRAW\" just a discrete insert operation?\n\n\n### Experiments\n\n- You mention that all your trajectories are 5 steps. Bullet Physics has issues running at lower than 50Hz, so 5 steps = max. 0.1s, right? How much movement can be contained in 0.1s, assuming some realistic velocity constraints on the robot arm?\n- \"With this setting we show...\" -> we _intend_ to show, also comma after \"setting\".\n- 4.1 first paragraph discussion doesn't line up with Table 1 - you describe XYZ performing on par with your method, but it doesn't.\n- Major issue: Why is XYZ not the upper bound? Your model is trying to infer the 3D composition of the scene but applying the same rollout mechanic to the ground truth 3D positions/orientations should result in upper bound performance, not worse performance.\n- Why do you care about the Kuka's different end effectors? Wouldn't it be more interesting to add more objects instead or randomize the shape or color of objects, as you promised in contribution list item 3?\n- \"for different tailored to different object shapes\" -> \"tailored to different object shapes\"\n- In 4.3 you need to specify how you're sampling the start/end positions? Like uniformly in the whole .6m x .6m space, both start and end? And how's the camera randomized? And you're testing all in-distribution, not out-of-distribution?\n\n### Conclusion\n\n- The big bold statement \"state representations that themselves obey physics\" is not properly justified? Why did say that? Not shaking under camera movement is not \"obeying physics\". And how is the camera shaking exactly?\n\n### Appendix\n\nAll good."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Paper summary:\nIn this paper, a model is learned that can construct a 3D scene representation based on 2D images from multiple viewpoints. Additionally, a 3D object detector is trained on top of the scene representation using ground-truth bounding box supervision. A dynamics model is then learned to predict future object positions, given the actions of a robotic arm that pushes the objects. The key idea of the paper is that, because it uses a 3D representation, the dynamics model can use the permanence of 3D object shape to avoid accumulating object shape errors during prediction. The paper shows experimentally that the learned 3D scene representation improves dynamics prediction over a models using either a 2D scene representation or ground-truth object centroids as object representation. Further, the dynamics model is used for model-based control and performs better than the baseline model using ground-truth object centroids.\n\nDecision:\nThe paper is borderline. While the application of 3D representations to dynamics modeling is a well-motivated and promising direction, the paper only adds a simple supervised one-step prediction model on top of the previously published 3D representation (Tung et al., 2018). The experiments show promising results, but many of the claims made about the predictive model are not sufficiently supported.\n\nSupporting arguments:\n1. The abstract claims that the paper empirically demonstrates “that the proposed 3D representations learn object dynamics that generalize across camera viewpoints and can handle object occlusions.” I did not find experiments that test either of these claims. Please add experiments comparing evaluation on seen vs. unseen camera viewpoints. Also add experiments showing robustness to occlusions.\n\n2. The list of contributions in the introduction claims to show “strong generalization to environments of novel objects, novel number of objects, novel objects spatial arrangements and novel camera viewpoints.” None of these cases are shown in experiments. For each of these points, please perform a comparative experiment, showing that the proposed model performs better than the baselines.\n\n3. For the control application (Table 4), why was only the XYZ baseline used? How about evaluating the other baselines (2D-multiview, ours-depth) on this task? It would also be good to compare to some competing models, not just ablations.\n\n4. In Table 3, how can the XYZ baseline (using ground-truth object positions) be worse than all the other models, which use the learned object detector to obtain object positions? Please explain this.\n\nMinor comments:\n1. Please estimate some sort of uncertainty on all of your experimental values, e.g. confidence intervals over a number of random model initializations.\n\n2. It would be helpful to explain Eq 2-4 individually, with text between each of the equations to make it clear what the purpose of each step is.\n\n3. Eq 2-4 use both small and capital Os and 0s in the notation, this is hard to parse. Consider using a different letter instead of O.\n\n4. Also, different letters for m and M might be helpful, since they describe completely different objects (binary occupancy maps vs feature maps, if I understand correctly).\n\n5. In Eq 2-4, the use of subscripted superscripts seems unnecessary. If “o” is an object index, we can simply call a different object “p”, instead of adding another index o_i and o_j.\n\n6. Figure 1 could be clearer, with a clear indication of which part of the figure corresponds to which equation in the paper. Also, the choice of pale blue boxes inside orange boxes is not optimal, things are hard to see.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a model to extract 3D object-centric representations of scenes from multiple RGB(-D) scene observations. These scene representations can be used with an action-conditional forward model to predict the state of the future state scene. The full model is comprised of multiple modules (each individual module is a well-known differentiable module) which can be tuned end-to-end by backpropagating the errors from the forward model. The authors emphasize that this is the first model that computes object centric representations in 3D and reasons about scene dynamics in 3D, providing advantages over previous systems that reasoned in 2D spaces or didn't have an object-centric representation. The authors then test the model to perform single or multi step forward prediction and for model-based control, obtaining an advantage over models that use representations of only object position and rotation or 2D object-centric models.\n\nOverall the paper proposes a highly engineered system that seems i) highly specific to a particular setting/task and ii) very difficult to reproduce.\n\nThe model works by inferring 3D object representations from RGB(D) images through neural networks that predict representations which are then manipulated through geometric operations to rotate/transform the scene. The authors reason that this does not require access to ground-truth (GT) 3D information as other previous models do, and can be trained end-to-end with a forward prediction task. While this is true, it also assumes that we can easily infer 3D object centric representations from scene views and that we can perfectly infer the future scene representation through geometrical transforms such as those performed with a 3D Spatial Transformer. This might be true in synthethic environments with 1 or 2 objects, but it seems quite impossible to infer and manipulate these representations from real world scenes with tens of objects, with extreme variations of relative locations, shapes, sizes, materials across objects. Small errors in inferring the scene representation could lead to large prediction errors, or interactions in complex scenes might not be properly captured by spatial transformers. Therefore, while the prior knowledge added to the system is more general than models that use GT information, it seems that the model could only be trained when it is easy to infer the scene state.\n\nIn fact, the experiments are conducted on a synthetic environment with at most two objects, which perfectly fits the inductive bias of the model. It is not surprising that this model would surpass the baselines if properly optimized. I would like to see experiments on real data or way more complicated synthetic scenes with tens of objects to be convinced that this model could be used in a real world scenario. My guess is that such a complex model with many different modules can only be properly trained end-to-end when each of the individual tasks are relatively simple (inferring the 3D scene state, manipulating the scene).\n\nFurthermore, the model is not described with enough detail to reproduce it. The details of the neural architectures used are not given, how the Mask R-CNN module is repurposed to operate in 3D is not explained and the details of the GRNN used are missing. While most of the individual components are already existing models, knowing the specific details would help understand better the model. \n\nIn summary, the paper proposes a highly complex system to learn 3D object centric scene representations with a forward model that doesn't seem that it would generalize past simple environments and that is not described in sufficient detail to be reproduced. I would change my score if the authors showed that such model can perform predictions in simple yet real world scenarios that go beyond the synthetic environment used. For example, the authors should be able to directly use one of the BAIR Robotic Interaction datasets (https://sites.google.com/berkeley.edu/robotic-interaction-datasets) with their model, which replicates fairly well the synthetic environment used to test the model but has real data, and compare the accuracy of the predictions to a 2D video prediction model such as SVG-LP (Denton and Fergus, ICML 2018) that does not make any assumptions about the 3D world nor has an object-centric representation.\n\nMinor note: I'd recommend not including extraneous or non-supported claims such as \"humans excel in manipulation despite that less than 1% of us know what inertia is\" or \"human brain effortlessly disentangles camera motion from object motion and appearance\""
        }
    ]
}