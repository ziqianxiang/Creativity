{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes using progressive knowledge distillation from teacher classifiers as a signal for training conditional generative models for the purpose of augmenting the classification performance of few-shot learners on novel problems.\n\nI would like to recommend acceptance for the following reasons:\n1) The proposed method clearly performs close to SOTA, yet slightly below. The paper demonstrates top performance in the generative category.\n2) The approach could potentially help augment the current SOTA (e.g. for miniImageNet: Lee et al. CVPR 2019), although this is not shown. Some results along these lines would definitely be impressive!\n3) The paper is clear and well written, which is a nice achievement considering that the approach is quite involved.\n\nI have some reservations because:\n1) The current state-of-the-art on miniImageNet (Lee et al. CVPR 2019) reports slightly better results with a conceptually cleaner approach. While results on the larger dataset are interesting, the improvements are fairly small.\n2) The resulting approach is quite complex technically and conceptually. While this is also true for competing methods, this paper doesn't manage to simplify the path to such level of performance, which I would have found valuable even if not SOTA.\n\nOverall, I found the work thorough and informative so I lean towards acceptance.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper combines generative modelling with few-shot learning, by distilling the information of a teacher classifier (which uses all of the data available) into a student generative model to produce samples with a similar decision boundary when only given a few examples as context. This generator can then be used to produce additional samples for an unseen class at test time. To improve training of the generator and ensure the teacher and student are not too far apart, the teacher is progressively strengthened, or the student progressively weakened.\n\nThe paper is clearly written and easy to follow. I think it could be a good contribution, but I have a number of concerns that I feel should be addressed first before it can be published.\n\nFirst, while the title and motivation are from the perspective of generative models, this is really a few-shot learning paper. The model only generates features from noise+other features (rather than a generative model of pixels), and the entire evaluation is from the perspective of few shot classification. I think the title could be changed to something like \"Few-shot learning via knowledge distillation and generative modelling\", and the introduction and motivation should focus more on few shot learning. As it stands, I don't think there is any evidence that this method leads to better generative models (as it doesn't generate pixels and the evaluation doesn't measure this).\n\nSecond, I think there are a few claims that need to be toned down in the paper:\na) In the context of the above, figure 4 should make it clear upfront that the images aren't *samples* but nearest neighbours of generated embeddings. Using \"synthesized embeddings\" for example, would be better.\nb) The approach is not model-agnostic, as it's specific to classification for this paper. I think it's necessary to demonstrate the generator can be used in other meta-learning contexts (eg. RL) for this claim to be validated.\nc) \"concurrent work\" should be \"prior/previous work\" in the results tables.\nd)  It's not clear to me why the progressive aspect is needed; why is it problematic if the teacher is far different from the student? The results seem to suggest that the progressive aspect doesn't improve performance much, and in the ensemble learning section, \"we found that starting with ktrain = kmax works as well as starting with lower values\". This seems to negate the need for progressive distillation in the first place if low values are fine.\ne) The paper states a few times that existing knowledge distillation approaches focus on distilling from large to small models, but there is plenty of work on distilling knowledge for other purposes (eg. See Teh et al, 2017; Schwarz et al, 2018), so this particular aspect is not novel.\n\n\nFinally, there are also some other questions and comments I have:\n\n1) The notation is confusing with uppercase for both sets and integers (eg. \"a subset L of M categories\"). I strongly suggest using \\mathcal for all sets in order to avoid confusion.\n2) From my understanding, the generator needs features as additional input (compared to more standard generators, eg. GANs) to act as context for unseen classes in the few shot regime. However, the intuition for this seems unclear given that the mean vector for a class is also introduced (this alone could ground the samples, with the noise capturing the intra-class diversity).\n3) The generator is trained to augment a class with samples that preserve its decision boundary (so perhaps it finds samples near the boundary). However, it's not clear to me how this captures information that transfers to unseen classes. Does it rely on shared \"decision boundary structure\" between classes (eg. The difference between cat and dog (in meta-training) might be similar between that of fox and wolf (in meta-testing)?) What sort of held out classes is this likely to generalise well or poorly to? Some intuition and discussion on this would be great.\n4) Typo in bottom of page 5: \"ditillation\"\n5) I think this work also shares similarity with virtual adversarial training (a competitive approach to semi-supervised learning) which should be referenced.\n\n\nMiyato, Takeru, et al. \"Virtual adversarial training: a regularization method for supervised and semi-supervised learning.\" IEEE transactions on pattern analysis and machine intelligence 41.8 (2018).\n\nSchwarz, Jonathan, et al. \"Progress & compress: A scalable framework for continual learning.\" arXiv preprint arXiv:1805.06370 (2018).\n\nTeh, Yee, et al. \"Distral: Robust multitask reinforcement learning.\" Advances in Neural Information Processing Systems. 2017."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThis paper proposes to train a generative model to improve the few-shot classification. Based on the prior work [1], the authors progressively transfer the knowledge of the teacher (trained by real samples) to the student (trained by generated samples). By gradually removing the real samples for the student, the generator produces better samples that retain the decision boundary.\n\nPros:\n- The authors propose a curriculum learning [2] approach to train a generative model.\n- The authors demonstrate that easy-to-hard curriculum improves the performance, in terms of the few-shot classification.\n\nCons:\n\n1. The method is a straightforward extension of prior work.\n\nThe idea of training generative models for a few-shot classification is proposed in [1]. The authors newly propose to apply curriculum learning [2] to improve the generative model. The proposed method is interesting, but the authors need new ideas to meet the high standard of ICLR. For example, the authors may investigate if the progressive distillation improves the generation itself, i.e., the standard knowledge distillation setting for the generative models [3].\n\n2. The performance gain is marginal.\n\nAlthough the proposed method requires an additional computation burden to train a generative model, the performance gain over the prior work is marginal. Also, while the authors denote other methods as \"concurrent work\", most of them are published 2-3 years ago. The most recent one, LEO [4] shows better results than the proposed method for 1-shot learning. Other recent work, e.g., MetaOpeNet [5], are not reported.\n\n3. The presentation could be improved.\n\nFirst, the title is misleading. It would be better to clarify that the goal of the paper is a few-shot classification. Second, the authors should specify what is already known and what is newly proposed. For example, the authors should clarify that Section 3 is preliminary.\n\n\n[1] Wang et al. Low-Shot Learning from Imaginary Data. CVPR 2018.\n[2] Bengio et al. Curriculum Learning. ICML 2009.\n[3] Aguinaldo et al. Compressing GANs using Knowledge Distillation. arXiv 2019.\n[4] Rusu et al. Meta-Learning with Latent Embedding Optimization. ICLR 2019.\n[5] Lee et al. Meta-Learning with Differentiable Convex Optimization. CVPR 2019."
        }
    ]
}