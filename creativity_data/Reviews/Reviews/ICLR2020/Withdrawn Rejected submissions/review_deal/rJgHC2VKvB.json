{
    "Decision": {
        "decision": "Reject",
        "comment": "Based on the Bayesian approach to filtering problem, the paper proves that RNN are universal approximators for the filtering problem.  Two reviewers, however, have doubts about the novelty and difficulty to get the result. Although I do not fully agree that Reviewer3 that the proof is just \"DNN can fit anything\" - it is not this case, but the concerns of Reviewer2 are more strong, especially about the usage of the term \"recurrent neural network\". The paper is purely theoretical and does not have any numerical experiments, which probably makes it too weak for ICLR in this form. However, I encourage the authors to continue to work on the subject, since the approach looks very interesting but it still very far from practice.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper shows that recurrent neural networks are universal approximators of optimal finite dimensional filters. More specifically, the paper provides a theoretical analysis on the approximation error for any stochastic dynamic system with noisy sequential observations.\n\nI find the work interesting. Yet, I don't understand how it substantially deviates from previous work. The cited work by Schafer&Zimmermann only considers deterministic systems -- but what is the distinguishing idea of your paper that makes your more general analysis possible? To me it seems utilizing sufficient statistics? But working with sufficient statistics resembles lossly again working with a deterministic-like abstraction? While I understand that this a purely theoretical work, it would be instructive to have practical demonstrations, showing what's happening when learning is actually done. Finally, interpreting RNNs as bayesian filters was utilized in recent works, e.g. Recurrent Neural Filters.\n\nAs a side note, isn't eq.3 called the Chapman-Kolmogorov equation. Also, the book by Jazwinski is often cited -- I think if the main findings utilized in your text, e.g. the proof of Thm1, were replicated in the Appendix, your paper would feel more complete."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper attempts to establish the asymptotic accuracy of the \"RNN\" (but not the RNN models that are well-known in the literature - see the below comments) as a universal functional approximator. It considers a general state space model and uses feedforward neural nets (FNN) to learn the filtering and forecast distributions. Based on the well-known universal approximation property of FNNs, the paper shows that their RNN-based filter can approximate arbitrarily well the optimal filter. \n\nThe paper targets an important problem in statistical learning and offers some interesting insights. However, their so-called RNN-based filter formulation is not anywhere close to the usual RNN models, such as Elman's basic RNN model or LSTM, that are currently known in the literature. Hence the paper's title \"RNNs are universal filters\" and the way it is presenting are confusing. I think what exactly the paper is about is as follows. They consider a general state space model, then use FNNs to approximate the so-called transition equation and observation equation of that SS model. Then, the resulting RNN-based filter is shown to be able to approximate well the optimal filter of the original SS model. I didn't check the proof carefully but I guess it's intuitively straightforward given the available results from the approximation capacity of FNNs. \n\nTherefore, I suggest the authors re-write the paper carefully to reflect better the problem that it actually targets. The property of the RNN-based filter is interesting, but using it is, I believe, very difficult from a practical point of view. It's well-known that doing particle filters is computationally expensive, especially in high dimensions, and the RNN-based filter might have millions of parameters! Could the authors please give some comments/discussion about this issue?\n\nSome minor points:\n1)  what exactly does \"synthesized\" mean? \n2) Page 5: \"Note that the probability space ... with finite moment...\" doesn't read well. What is the moment of probability space?\n\n\n           "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper shows that RNN (of infinite horizon) can be universal approximators  for any stochastic dynamics system. It does that by using the Bayesian framework of filtering, which shows that if sufficient statistics given the observations so far can be computed at each stage of filtering, then the expected hidden states at each stage can be estimated as well. The paper then follows this framework by constructing deep neural network mapping from “observations so-far” to “sufficient statistics” and by universal approximation theorem this is possible. Then as long as after each stage, the image of such a mapping is bounded into a certain region, and if the mapping is contractive (|C_\\psi C_\\phi| < 1), this can be applied to arbitrarily long sequence (and thus infinite horizon). \n\nThe paper is well-written and easy to follow. However, I have concerns about its novelty. Overall the paper seems to be a straightforward application of universal approximation theorem of deep neural network. The authors don’t specify when the condition will hold (e.g., |C_\\psi C_\\phi| < 1), what kind of empirical RNNs fall into this category and whether the trained models are operating in the contractive region. \n\nAlso with the same technique the authors also reach the same conclusion for nonlinear dynamics system (Sec. 4.2). From the proof, there is no much difference between linear case (Kalman Filter) and nonlinear case, since apparently DNN can fit everything. Then what are the new discoveries and takeaway messages? It is not clear what can we learn from this analysis and its impact is likely to be limited. "
        }
    ]
}