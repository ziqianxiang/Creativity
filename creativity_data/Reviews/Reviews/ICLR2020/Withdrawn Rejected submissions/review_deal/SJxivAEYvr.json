{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presented reinforcement learning based style transformer for text style transfer on unpaired training corpus. The key idea is to combine RL with decoder-only transformer with a few of other enhancements. The experimental results show the performance gains over existing approaches. Text style transfer is quite crowded research area and there are quite rich previous literatures. Although the experimental results showed some advantages, I found the novelty of this paper is just a combination of several existing works. More importantly, I did not see any reasonably justification why some of choices are made. The whole paper reads like some engineering choices without much rationales. \n\n1) This paper is simply a combination of style transformer [Dai et al., 2019] using decoder-only GPT [Radford et al., 2019], and RL based encoder-decoder [Gong et al., 2019] using RL for unpaired text style transfer. These two formulated the overall framework and a few of specific empirical enhancements (such as some other rewards and warm start using (Sudhukar et al., 2019)) were used to improve the performance. In the perspective of scientific novelty, this work is limited. \n\n2) For most of model choices, there are lack of clear motivations and explanations what and why each of model choices were made. This is very important for developing a scientific model, which we would like to know why not the what. Also, it is also important to discuss the connections and key differences between this work and the existing works. \n\n3) Although authors showed a lot of baselines in Table 1, it is kindly of pointless to me to show these numbers. What the reader really need to see is some really relevant baselines and your approach performed better than them, which verified the proposed model choices.  It is important to take a close look at the numbers and try to provide any insightful take-home messages regarding the scientific merits of the proposed method. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In the paper, the authors propose a combination reward, which is composed of three parts, i.e., fluency, content, and style, for text style transfer.  Experiment results demonstrate that the proposed objective aligns well with human perception and has improved baselines. \n\nThe main contribution of this paper is the three designed rewards. The other techniques, such as transformer and reinforce, have been already explored in other style transfer papers. However, I am not convinced by the rewards proposed. Why is the style reward better than a style classifier used previous? If a language model (transformer) is incorporated into the model, the fluency score will be significantly boosted, even without the GPT-based reward. Why is BLUE a good reward for content? The paper just proposed these metrics but lack of intuition and explanation. In addition, I am not convinced of the way that weights/mix all rewards into reinforce.\n\nThe experimental results have shown some improvements over baselines. However, the observation is not consistent. For example, the model utilizes BLUE as rewards but fails to beat other baselines on BLUE on Yelp and Captions. From the ablation study, it indicates that the MLE training is very important. The model is also not able to beat MLE for BLUE. Finally, I suggest also use other metrics to measure content preservation (Santos 2018).\n\nSome related work to this paper:\n1. What Makes A Good Story? Designing Composite Rewards for Visual Storytelling\n2. Towards Coherent and Cohesive Long-form Text Generation"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors introduce RL-ST, an RL-based method for unsupervised text style transfer that:\n1) leverages a pre-trained language model\n2) leans to transfer style without using a disentanglement objective\n3) is warm-started using style-transfer generations from another model\n4) is \"fine-tuned using Policy Gradient, by providing rewards for style, content preservation and fluency.\" Style and fluency rewards are provided by learned models/classifiers, and content preservation rewards are calculated as the BLEU score between the source and target.\n\nThe main contribution seems to be 4; As I understand, 1-3 are helpful techniques from prior work that this paper appropriately leverages to help get the RL fine-tuning to work. For example, for 1-2, XLM from \"Cross-lingual Language Model Pre-training\" first pre-trains a (masked) language model, and then uses back-translation (without disentanglement) to perform unsupervised machine translation (similar to unsupervised style transfer). Point 3 (warm-starting with data from another model) has been shown effective in \"An Effective Approach to Unsupervised Machine Translation\", where Artetxe et al. initialize a neural unsupervised MT model using translations from an unsupervised phrase-based MT model. These two papers should be cited and discussed, and in general (along with other related papers) prior work makes me view this paper's novelty to be in the way that RL-ST uses RL.\n\nThus, I am most interested in understanding the contribution of the RL component of the model in achieving strong performance.  Clearly, the overall RL-ST model is strong (the human evaluation results are quite compelling). However, I think more experiments need to be done to disentangle the contribution of the RL from the MLE pretraining and warm-starting. In particular, I think it is crucial to know the human evaluation results (i.e., as show in Table 3) for the MLE-only model against the RL+MLE (RL-ST) model from Table 4. I do not trust the automatic evaluation metrics to provide an appropriate evaluation, primarily since the authors use those exact metrics during RL training; for example, if a learned classifier is used to evaluate (e.g., the style or fluency), RL-ST has used the same classifier (including weights) during training (the authors should please correct me if I have misunderstood). This means that RL-ST could be overfitting to the particular models/model weights used for evaluation. For this reason, it would also be helpful to know the human evaluation results for other ablations related to the importance of RL (i.e., those on the \"Rewards\" in Table 4).\n\nRegarding the way RL-ST employs RL, I also have several questions, as I found some design decisions unintuitive:\n* Attention weights just indicate what tokens were important in affecting the model's decision. However, sometimes tokens with high attention could be responsible for *decreasing* the likelihood of the generation being in the target style, but those tokens would be given a positive reward if the overall classification results in a sentence that appears in the target style (in spite of tokens in the wrong style). It seems more intuitive to me to use gradients from the downstream classifier to reward (similar to a GAN) or to use the classification predictions of a sequential/autoregressive discriminator as dense rewards (as in ScratchGAN from \"Training Language GANs from Scratch\").\n* Why not use the attention weights to weight the reward for each word (rather than applying a 0 or 1 weight to the per-token reward)?\n* The reward assignment is discontinuous where the style classifier predicts the likelihood of each class to be 0.5 (the reward jumps from -0.5 to +0.5). The discontinuity is unintuitive to me - would it not increase the variance of the reward around the discontinuity? Have you considered a continuous reward function? This seems like a simple change to make (Reward = thr * Prob -> thr * (2 * Prob - 1))\n* For the fluency reward, is the baseline b manually chosen? It seems better to learn b or to dynamically calculate b based on a moving average of past reward, rather than treating b as a constant and hyperparameter\n\nI also have several questions about how this work differs from prior work using RL in text generation:\n* How is the application of RL in this work different than prior applications of RL in text style transfer? Is the main differences from prior work using RL in the use of pre-trained weights and warm-starting on model-generated data?\n* How is the application of RL in this work different than prior applications of RL in the text-GAN/generation literature? Is the main difference the specific rewards used for style transfer? It would be nice to see that literature discussed, so I can know whether the RL itself is performed in a novel way, or if the authors are claiming that the novelty is in the style-transfer reward functions.\n\nOther notes:\n* If it's not too difficult, it would be nice to know how RL-ST compares against the references w.r.t. human evaluation. That would give a less relative / more objective evaluation of RL-ST. I don't think such evaluation is necessary though.\n* In addition to the papers I have brought up above, it would be nice to add a discussion on \"Fine-tuning Language Models from Human Preferences.\" This paper is concurrent and also fine-tunes a pre-trained language model for style-conditioned generation using RL.\n\nOverall, I think RL-ST is clearly a strong model, but I am not sure about the main novelty of the paper (RL), for 3 reasons:\n1) if RL is a significant cause in the improvements\n2) the way that RL is used is unintuitive in several ways to me\n3) how novel the use of RL is compared to prior work\n\nI think several of the above concerns may be addressed in the rebuttal and/or other reviewer's remarks, so I am looking forward to reading what others have to say."
        }
    ]
}