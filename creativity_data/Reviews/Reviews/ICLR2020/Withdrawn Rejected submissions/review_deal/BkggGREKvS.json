{
    "Decision": {
        "decision": "Reject",
        "comment": "After reading the reviews and discussing this paper with the reviewers, I believe that this paper is not quite ready for publication at this time. While there was some enthusiasm from the reviewers about the paper, there were also major concerns raised about the comparisons and experimental evaluation, as well as some concerns about novelty. The major concerns about experimental evaluation center around the experiments being restricted to continuous action settings where there is a limited set of baselines (see R3). While I see the authors' point that the method is not restricted to this setting, showing more experiments with more baselines would be important: the demonstrated experiments do strike me as somewhat simplistic, and the standardized comparisons are limited.\n\nThis might not by itself be that large of an issue, if it wasn't for the other problem: the contribution strikes me as somewhat ad-hoc. While I can see the intuition behind why these two auxiliary objectives might work well, since there is only intuition, then the burden in terms of showing that this is a good idea falls entirely on the experiments. And this is where in my opinion the work comes up short: if we are going to judge the efficacy of the method entirely on the experimental evaluation without any theoretical motivation, then the experimental evaluation does not seem to me to be sufficient.\n\nThis issue could be addressed either with more extensive and complete experiments and comparisons, or a more convincing conceptual or theoretical argument explaining why we should expect these two particular auxiliary objectives to make a big difference.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper proposed two approaches to encourage cooperation among multi-agents under the *centralized training decentralized execution* framework. The main contribution of this paper is that they propose to allow agents to predict the behavior of others and introduce this prediction loss into the RL learning objective. One approach is teammate-regularization where each agent predicts the behavior of all others (and therefore makes the total model complexity increasing quadratically with the number of agents), while the other is a centralized coach-regularization. The performance of these two approaches are compared with MADDPG and its two variants on 4 games. Experiments show that their approaches surpass a few baselines in certain game settings. However, the novelty of this algorithm is not strong, given that the similar idea of 'predicting the behavior of other agents' can be found in related work with discrete action spaces. Experimental results also show unstable advantages of their approaches.\n\nMethodology:\n\nTeam-reg: One main difference between this approach and Jaques work: * Intrinsic social motivation via causal influence in multi-agent*  (or other MARL algorithms for discrete action spaces) is that in this work, each agent predict the action of other peers instead of the policy, since this algorithm is only based on DDPG with continuous action space. However, It seems that this idea is transferrable to discrete action spaces as well, so long as we change the MSE loss between actions to KL loss between policy over states. The novelty of this approach is not strong. Also, one hypothesis on which their work is based is that promoting agentsâ€™ predictability could foster such team structure and lead to more coordinated behaviors. Does predictability really improve cooperation? Could it be the other way around? Experiment results in the right-below penal (CHASE Game ) and left-above penal (SPREAD game) of Figure 5 actually strength my concern. \n\nCoach-reg: Descriptions of the coach regularization approach is quite vague. I doubt the motivation and effectiveness of this approach. More questions are as follows:\n\n* What is the role of the policy mask in deriving a better action? Is it just a dropout before the activation layer with a fixed $p$ proportion (which is related to the choice of $K$)? If so, this is like you first assume the policy network is somewhat overfitting, then alleviate this issue by letting the coach adjust which part to keep and which part to drop.  How would different $K$ values affect the performance?\n\n* Does this framework really improve cooperation? Intuitively, all agents will finally reach a point where they agree on the same policy mask distribution (which is the one generated by the coach). How does the same policy mask lead to an improvement in cooperation? Empirical or theoretical explanations are strongly needed in the main results, not in the appendix section.\n\nExperiments:\n\n- All variants of MADDPG in the experiments are weak baselines, assuming that $MADDPG + sharing$ means all agents share the same policy and Q-function. However, since this algorithm only works for continuous action space, available relate work to compare with is quite limited. All environments only include two agents. Experiments with more than two agents should be implemented. \n- Even for only 3 games (excluding the adversarial game setting), both branches failed to beat the *sharing* baseline for the CHASE game. For the ablation study, *agent-moduling* even works better than *TeamReg* for the CHASE game, so as the case when *policy mask* beats *CoachReg* in the SPREAD game (Figure 5). They seem to show that under these two proposed frameworks, the predictability of agents does not always encourage cooperation, at least for 2 out of 4 game settings mentioned in this section.\n- This work makes a lot of effort on the hyper-parameter tuning part, which however does not provide a systematic solution to the tradeoff between improving predictability and maximizing reward. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Contribution:\n\nThis paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents:\n - TeamReg: the agents are trained with an additional objectives of predicting other agents' actions, and make their own action predictable to them\n - CoachReg: the agents use a form of structured dropout, and at training time a central agent (the \"coach\") predicts the mask dropout mask that should be applied to all of the agents. An additional loss is provided so that the agent learn to mimic the coach's output from their own input so they can still apply the dropout at test time, when the coach is not available anymore.\n\nThese two methods are evaluated in 4 different MARL environments, and compared against their ablations and vanilla MADDPG.\n\n\nReview:\n\nThe paper is well written and easy to follow. It generally motivates well the design choices, both intuitively and experimentally, using numerous ablations. It includes the analysis and explanations of the failure modes, which is valuable in my opinion.\n\nThe two main limitations of the work are the following:\n - Limited scale of the experiments. The majority of the experiments contain only 2 agents, and the last one merely contains 3. It is unclear whether the additional losses proposed in this work would still perform correctly with more agents, since the regularization pressure will increase.\n - No comparison to SOTA MARL methods. The only baseline method presented here is MADDPG, upon which this work is built. Recent work tend to show that it is easily outperformed, hence comparison to stronger baselines (QMIX, COMA, M3DDPG, ...) would be advisable to assess the quality of the policy found.\n\n\nAbout the policy masks:\n- what are the value chosen for K and C?\n- Is there a reason why this particular form of dropout was chosen? Since it occurs after a fully-connected, it should be equivalent and more straight-forward to mask out C contiguous values.\n\n\nFinally, it seems to me that the two methods presented here (TeamReg and CoachReg) are not mutually incompatible. Is there a reason why you didn't to apply both of them simultaneously?\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes two methods of biasing agents towards learning coordinated behaviours and evaluates both rigorously across two existing and two novel multi-agent domains of suitable complexity. The rigour of the empirical evaluation and its documentation is exemplary. \n\nThe related work section could be improved by including a section on the closely related work in the area of opponent modelling. This literature is touched on shortly afterwards alongside the related proposed method in Section 4.1 but only by a single citation. A more thorough review in Section 3 would improve the positioning of the paper.\n\nFrom figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better than the existing baseline method MADDPG on the CHASE environment. In particular I think it would improve the discussion of the results to elaborate further on why the proposed methods do not improve, but also do not detrimentally affect the learning performance of agents in this environment. \n\nIn Section 6.2 the authors note \"MADDPG + policy mask performs similarly or worse than MADDPG  on all but one environment and never outperforms the full CoachReg approach.\" I found it also interesting to note that in that one environment (SPREAD) adding a policy mask both outperformed MADDPG and was more sample efficient than CoachReg. Again what is it about this specific environment that causes the policy mask ablation to be sufficient, enabling it to match the asymptotic performance and learn quicker than the full CoachReg method proposed?\n\nAll references to papers both published and available on arxiv should cite the published version (e.g. Jacques et al. 2018 was published at ICML). Please revise all references if accepted.\n\nMinor Comments:\n1) Page 2 \"two standards multi-agent tasks\" -> standard\n2) Page 7 \"improve on MADDPG apart on the SPREAD environment\" -> apart from on \n3) Page 9 \"each sampled configuration allows to empirically evaluate\" -> allows us\n"
        }
    ]
}