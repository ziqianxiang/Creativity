{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper addresses the task of optimizing the point architectures for geometric deep learning. The authors propose three approaches to improving the performance of the point-based architectures, arguing that the key to performance improvements is increasing the depth of the network. To increase the depth, a memory efficient grouping block is designed, which significantly lowers the amount of memory required to train a point-based network; thus, deeper models may be trained with the use of residual connections. A multi-resolution block is then introduced, which helps propagate the information inside the model. The performance is evaluated with respect to the baseline PointNet++ (and several other models) on semantic labeling task on a number of commonly used datasets. \n\nI believe that the motivation to training more lightweight models in the context of geometric deep learning is very pronounced, since most existing models tend to have a heavy memory footprint or a large computational overhead. This becomes even more important in the context of embedded applications (e.g., robotics or autonomous driving), where range sensors collect measurements online, or in the context of leveraging very large annotated collections such as ABC. Thus, the overall motivation of having efficient models is clear. However, the second point that deeper models automatically mean better performance doesn’t seem so obvious to me, as the authors didn’t provide evidence that the depth is a factor limiting performance for point-based deep models. So, why go exactly deeper with point-based networks? Maybe one rather needs to design, e.g., more complex convolution-type blocks? \n\nRelated to that, another question concerning the experimental evaluation arises. While the authors did show that their network demonstrates superior performance to PointNet++, they didn’t investigate whether or not the performance comes from increasing the depth of their architecture. Would increasing the depth of PointNet++ (and possibly related methods) lead to a similar improvement in performance? (Implementing PointNet++ on a deeper encoder may require sharing the model on multiple GPUs, but we’re talking only about semantic labeling performance here, not efficiency).\n\nI believe the paper deserves to be accepted once the effect of depth is demonstrated and confirmed. Otherwise the paper shall look as trying to fix a non-existent issue just for the sake of depth. Should the authors clarify the effect of depth, I will be reconsidering the rating. \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents an approach to improve (i) the memory footprint of point processing networks, which has been a main bottleneck for they use, and (ii) their accuracy. Specifically three point processing blocks are presented that enables wider and deeper architectures with improved information sharing. The approach is thoroughly evaluated on point segmentation benchmarks. \n\nThe paper is very well written and a pleasure to read, even for a reader unfamiliar with point networks.  The proposed blocks are clearly and intuitively presented and their impact convincingly demonstrated. \n\nFor the sake completeness, it would be important to include memory usage and inference time in Table 2, especially as it deals with comparison against Deep CGN, so one can better assess the advantages/disadvantages of both approaches. Timing results would also be important to include in Table 3.\n\nOverall, it is a very interesting problem and the proposed approach could have broad applicability."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "TLDR: The paper proposes architectural variations/engineering to speed up the PointNet and reduce the memory footprint, but the paper is riddled with biased coverage of related works, no comparisons against any of the recent works, and a serious misunderstanding of the basic concepts resulting in misleading names.\n\nThe paper proposes 1) grouping operation they named \"point convolution block\" 2) multi-scale hierarchy, residual link and hypercolumn like links.\n\n1) Point convolution block is not actually a convolution. The authors claim that MLP + MaxPooling is the counterpart of the 2D convolution. Mathematically, \n\nConvolution\n\n$\\sum K_{ij} v_{x+i, y+j}$ note that weight is different for all spatial locations\n\nMLP + MaxPooling over N-nearest neighbors\n\n$\\max_{i} Kv_{i}$ note that weight is common\n\n\nThe MLP + MaxPooling cannot approximate the convolution. Also, the authors proposed to reconstruct the activation while backpropagation, but ReLU and Dropout are non-reversible operations. How do you recompute the activation?\n\n\n2) The second contribution is multi-resolution hierarchy, residual, and cross-connections. However, these have been already proposed / used widely in 3D perception. There has been a large body of works that discussed how to increase the receptive field size by downsampling / strided convolution / strided pooling / dilated convolution. This is no exception in 3D. Strided 3D convolutions, poooling have been widely used. One example is the Monte Carlo Convolution [1] Fig.6 and all 3D sparse convnet [2] Fig.4 and [3] Fig.4 variants use the strided convolution for multi-resolution. Similarly, these networks have used the residual connections in 3D semantic segmentation which the paper heavily relies on. Finally, the cross-link was first proposed in [4] for semantic segmentation.\n\nHowever, in p.5, the authors claim that they observed the problems with no multi-resolution networks and claim that they came up with this idea inspired by the mechanism in biological vision.........\n\nAlso, the paper claims that they compared their method with SOTA methods on Table 2 caption, but they do not actually cite any SOTA methods. For example, [3] outperformed their method by a significantly large margin on the S3DIS dataset. Similarly, for the ScanNet, which has an online leaderboard, the authors do not report any of the SOTA methods that exceed their performance. Instead, they introduced new metrics that are not used in official benchmarks for ScanNet.\n\nOverall, the paper has a lot of technical issues and recommend rejecting this paper.\n\n\n- [1] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds, Siggraph'18\n- [2] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, CVPR'18\n- [3] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR'19\n- [4] Hypercolumns for Object Segmentation, CVPR'15"
        }
    ]
}