{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the problem of unrestricted adversarial attack in the image domain. Building upon the idea of previous work (Song et al. 2018), this paper leverages the state-of-the-art image-to-image translation model SPADE (Park et al. 2019) to synthesize high-resolution adversarial images from semantic segmentation maps. Compared to Song et al. 2018 which mainly experimented with MNIST and face images, the proposed AdvSPADE conducted experiments on both real images captured indoor (ADE20K) and outdoor (Cityscapes). Experiments demonstrated that the proposed AdvSPADE is able to generate white-box and transfer-based black-box attacks with reasonable performance (see Table 1). Furthermore, the paper compared against traditional Lp-bounded attacks in terms of attack success rate (see Table 4), mIOU (see Table 5) and robustness.\n\nOverall, reviewer feels the paper in the current form is not ready to be published at ICLR for the following reasons.\n\n(1) Main motivation of the paper is not very clear. First, images generated by the AdvSPADE have noticeable artifacts (e.g., distortion, textures) and look quite different from the corresponding real images. Reviewer is not pretty sure if such “unrestricted” image manipulation (every pixel has been re-synthesized) is really meaningful or not for semantic segmentation model in the real world. In contrast, “restricted” image manipulation or local image editing seems like more natural settings for studying the unrestricted adversarial examples.\n\n-- 3D-Aware Scene Manipulation via Inverse Graphics. Yao et al. In NeurIPS 2018.\n-- Context-aware Synthesis and Placement of Object Instances, Lee et al. In NeurIPS 2018.\n-- Learning Hierarchical Semantic Image Manipulation through Structured Representations. Hong et al. In NeurIPS 2018.\n-- Free-Form Image Inpainting with Gated Convolution. Yu et al. In ICCV 2019.\n\n(2) Technical novelty of the paper is limited. As far as reviewer can understand, this paper looks like a straight-forward extension of Song et al. 2018 by leveraging the state-of-the-art image-to-image translation model. There is not much technical novelty in the paper. Also, the paper writing (especially Section 3) requires significant improvement. For example, the second term \\mathcal{L}_{FM}, the third term \\mathcal{L}_{VGG}, and fourth term \\mathcal{L}_{KLD} are never formally defined (as they are not clearly defined in SPADE paper).\n\n(3) Ablation studies on hyperparameters \\lambda_{0,1,2,3} are missing. First, it is not crystal clear how these hyper-parameters were decided for the proposed AdvSPADE. Second, in the comparison with vanilla SPADE + perturbations, it is not clear whether the hyper-parameters are optimal for the baseline vanilla SPADE + perturbations in terms of adversarial attacks.\n\n(4) The proposed pipeline (see Figure 2) is largely adapted from Song et al. 2018 and Xiao et al. 2018 (advGAN). But the second paper is never mentioned in this submission. The difference is that Song et al. 2018 and AdvSPADE used a conditional GAN framework to synthesize images from class label, while Xiao et al. proposed to synthesize perturbation using an encoder-decoder GAN framework.\n\n-- Generating Adversarial Examples with Adversarial Networks, Xiao et al. IJCAI 2018.\n\nFor a fair comparison to Lp-bounded attack using a generative model, this paper could consider a much stronger AdvGAN baseline (generate perturbation instead of image) using the SPADE architecture.\n\n(5) Reviewer is surprised to see the ineffectiveness of pixel-perturbation attack on real images or synthesized images (see Figure 1), as the Houdini loss proves to be very effective in attacking the semantic segmentation model detailed in the following paper.\n\n-- Houdini: Fooling Deep Structured Prediction Models. Cisse et al. In NIPS 2017.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper mainly studies the robustness of semantic segmentation network by designing novel unrestricted adversarial examples. The authors propose to obtain adversarial examples through generative adversarial networks with SPADE structure. Through experiments, the authors try to prove that the adversarial examples are indistinguishable\nto natural images for humans, while can cause great attack for the current state-of-the-art models. Furthermore, the authors argue that these adversarial examples can help improve the robustness of existing models through adversarial training.\n\nClarity:\nI think this paper has complete experiments to support their opinions. However, there should be some additional experiments to make the claims more credible. Thus, I think this paper could be weakly accepted with some supplementary experiments detailed in the following.\n\nNovelty:\n1. In practice, users will often notice that there are adversarial perturbations in images if we adopt unrestricted adversarial attacks. How can such unrestricted adversarial attacks be employed in reality? Or just to improve the robustness of networks?\n2. The SPADE structure is built upon existing ideas and concepts. Are there some more effective generation structures for this task besides SPADE?\n\nExperiments Results:\n1) The authors execute experiments on two popular semantic segmentation datasets, to illustrate the attack ability of their methods by comparing with traditional norm-bounded attacks. \nLimits: \na. In Table 3 and Table 4, the authors have made a comparison under the white-box setting while not consider the setting of the black-box setting which is a more practical situation. \n\n2) The authors use FID and human evaluation to indicate that adversarial examples are indistinguishable to natural images for humans, and the semantic meanings of adversarial examples are consistent with corresponding ground truth.\nLimits: \na. For the human evaluation, the authors should indicate the total number of persons in evaluation, rather than only quantitative result in Section 5.3 and Table 5.\nb. Although authors have proved that AdvSPADE has better generation performance, this is already pointed out by the paper “Semantic Image Synthesis with Spatially-Adaptive Normalization”. \nIn fact, the authors should also compare with these generation methods under the attack setting: whether the adversarial examples are generated from other methods, such as Pix2PixHD, have higher attack ability if the SPADE generator is replaced by other structures.\n\n3) The experiments in Section 5.4 are designed to show that AdvSPADE can help improve the robustness of the network. \nLimits:\na. The authors have not indicated the training epoch for AdvSPADE in adversarial training.\nb. In adversarial training, the adversarial examples for training are generated according to the state of the network. Thus, the training adversarial examples are different even for the same training setting. The authors should indicate whether the adversarial training results are stable or not."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper uses GANs to generate unrestricted adversarial examples for the task of semantic segmentation, which is traditionally harder to attack via traditional methods (like norm-bounded perturbations). They add an extra term to the GAN loss function for the SPADE generative model. The authors show through experiments on two datasets that their newly generated examples preserve most of the visual realism while also being adversarial for semantic segmentation models.\n\nI vote to weakly accept this paper. The key reason is that the authors present impressive experimental results in terms of both visual realism and fooling classifiers more consistently.\n\nThe authors show through experiments on two datasets that their method, called AdvSPADE, generates images that are almost as realistic as the original SPADE. The authors achieve this by adding the Unrestricted Adversarial Loss defined in equation (1), which seems like a reasonable way to encourage generated examples to fool semantic segmentation models. While the difference from SPADE is fairly small, the authors do perform extensive experiments and provide good baselines to show the effectiveness of their method.\n\nIn particular, I am happy that the authors compared AdvSPADE with SPADE + FGSM/PGD, which seems like a reasonable baseline. They show that, if we are encouraging realism of the image to be maintained, AdvSPADE is more effective than SPADE + FGSM/PGD. For various different segmentation models, the authors show improvements in terms of the mIoU metric, and they also show in Table 2 that their generated AdvSPADE examples are still fairly realistic (at least by the FID metric).\n\nI would like to see certain things addressed in a revision though.\n\nThe text in Table 1 is quite small even though the results are important. I would want bigger text. Additionally, it is a bit eye-opening that the method degrades accuracy for DRN-105 on Cityscapes by so much, but it does not degrade accuracy for similar models like DRN-38 and DRN-22 by much. If this can be explained, that would be helpful.\n\nFor the PGD attacks described on page 7, how can you set the number of attack iterations if eps is not an integer (wouldn’t eps+4 not be an integer either?)?\n\nFor the Semantic Consistency Test, I think the human task (choosing “matching” or “not matching”) is a lot easier than the machine task (of segmenting the whole image), so it’s unclear what those results should indicate. Perhaps a better task would be to show the real annotation and the annotation from the segmentation network on AdvSPADE, and ask a human to classify them as “similar” or “not similar.”\n\nFinally, I am also confused by the Fidelity AB Test, where Amazon Mechanical Turkers are asked to compare the realism of images generated from SPADE with AdvSPADE. In these, AdvSPADE is actually favored by a fairly large margin over the original SPADE. Do you have any insight for why this happens? It seems like AdvSPADE is unintentionally improving image quality by a lot.\n\nAdditional Feedback:\n\n- “synthesis images” -> “synthetic images” everywhere\n- The last sentence of “Related Work” has quite a few errors and should be corrected.\n- In Section 4, for the very last paragraph, first sentence, it should be “does not lead an image to fall” instead of “does not lead an image fall”"
        }
    ]
}