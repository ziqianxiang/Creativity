{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "1) Summary\nThe manuscript explores an architecture for visual reasoning i.e. being able to respond to questions about the objects and their relations present in a 2d image. The focus is on interpretability rather on performance and not on predictive performance.\n\n2) Quality\nThe paper addresses an important problem but falls short on the analysis part.\n\n3) Clarity\nThe manuscript is reasonably well written and most of the technical and experimental content is accessible. However, the architecture is descibed in somewhat vague and informal terms and there seem to be quite some details not covered in the main text.\n\n4) Originality\nThe basic composition of the MMN seems somewhat unexplored.\n\n5) Significance\nThe proposed method seems to be a variation of one of the major architectures to visual reasoning monolithic and compositional. The performance -- among the compositional methods -- seems to improve but the interpretability is not properly demonstrated.\n\n6) Reproducibility\nThe data is from published sources (GQA, VQA) and the results for the baseline methods are available. However, there is no code for the method nor for the experiments, which makes the results pretty tricky to exactly reproduce.\n\n7) Evaluation\nWhile the empirical analysis puts a lot of effort on accuracy and ablation studies, the focus interpretability and compositionality is not the core of the investigation. So it seems semi-conclusive whether these aspects are sufficiently addressed.\n\n8) Questions\n  A) What does the last line of Section 2.2 mean?"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors describe MMN meta module network hybrid approach to visual reasoning combing the neural module network with explicit multi-hop hand crafted reasoning. The proposed method parses the input into functional program using a recipe encoder, resulting in instance modules, after which a teacher-student framework is used to exploit scene graphs to provide guidelines for instantiated modules for the student to follow. The authors leverage the prior work from Hudson and Manning, Dong and Lapata and , Vawani et. al. The authors describe the method in visual diagrams and supporting experiments on GQA data set, comparing against state of the art, detailed impact of attention, teacher network (module) and bootstrap, culminating with results on error analysis on difference functions and the ability to cope with unseen functions.\n\nThe improvement points for the paper are\n1. The results in Table 1 are not the best throughout after discounting the MCAN/NMN/NSM (for e.g. BAN and LGCN are better in one each), the authors didn't discussed this\n2. The authors mention that the results in NSM are hard to compare as it uses well-tuned external scene graph generation model, what if the authors employed a similar model in their setting for scene graph generation \n3. The method  uses teacher/student network for module supervision but it's training/setup is left undescribed in results section (table 1)\n4. the guideline in table 2 needs definition \n5. For the described functions in figure 9, how the arguments are chosen (from multiple possibilities) and once a single argument is chosen, how the execution graph is obtained needs some explanation. If a parser is employed what are the details? and then the additional coverage in results\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this work the authors address major deficiency of Neural Module Networks, aiming more effective compositional reasoning for VQA. NMN has a lack of automated layout construction and specificity of the modules. This is addressed by leveraging the new GQA dataset which provides scene graphs and whose questions are constructed by a Program Generator.\n\nI am familiar with the GQA dataset and related work, yet I had a few issues when reading the paper to understand precisely what is done. It would be good to have just one quick summary of what is done including where supervision is applied what the student is doing precisely, how the ground truth programs are used, etc. There are multiple diagrams which are hard to reconcile with each other and lots of invented language. Here is a few specific comments. \n\nFor example the penultimate paragraph of Intro the system is described and references Figure 1, however I found it challenging to associate figure 1 to the text description.\n\nSupervision/Teacher/Student: This is the part I struggled the most to understand and hope the reviewers can clarify this in the rebuttal. I failed to understand from Figure 4 and the associated description what is the form of the student. Note this diagram doesn’t clearly distinguish where the student and teacher start and end. My current understanding is that the student (aka the final inference model) does not ever build an explicit scene graph. Does it build an explicit program? What is the final form of the student? \n\nA potential way the authors can improve clarity is by showing more explicitly in the diagrams how certain steps would be done in the NMN. \n\nExperiments: \nI believe Table 1 can be unclear/misleading to the casual reader in terms of what information is available at training time. E.g. most of the models listed aren’t using the scene graphs, I believe the MMN is the only one using the programs as well.  I would like an additional column or table added that specifies what external information besides Q+Image are used. Note this confusion is also present in other works like the NSM paper.\n\nRegarding the NSM, I believe the authors attribute the poorer performance to using a weaker pretrained visual feature extractor (which in the NSM is used to output scene graphs). Is this the case? If so the text should be clarified to state this more explicitly. \n\n(minor) the Meta in the name gives the impression there is some meta learning involved. \n\nOverall, I think the direction the authors take is interesting, explicitly representing reasoning steps. I am currently giving the paper a weak reject due to clarity issues, but will strongly consider to raise my score after some clarifications.  Besides clarity my main concern about the paper is that it might be overengineering a solution using the exact information provided in the GQA dataset: scene graphs, programs etc. Is the model essentially being heavily supervised to infer backwards the data generating process of GQA, might this mean it will heavily overfit to the questions posed in this dataset. I would like the authors to address the generality of the method in the rebuttal. \n"
        }
    ]
}