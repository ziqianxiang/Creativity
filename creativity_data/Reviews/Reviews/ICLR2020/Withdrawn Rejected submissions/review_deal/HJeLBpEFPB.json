{
    "Decision": {
        "decision": "Reject",
        "comment": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The submission proposes a graph neural network based on propagation with the attention mechanism. Then the output function uses the summation of node vectors to read out information about the graph. \n\n While the design is good, all components are all known techniques: the sampling procedure is like GraphSAGE; the propagation rule is similar to GAT, and the output function is wide uses in graph neural networks. \n\nCritics: \n\nThe writing is not clear. At the top of page 4: quote: \"... and produce an output sequence {h_vi^t}i=1^N+1\". Do you keep only the vector h_v1 and throw away other vectors? Because you will also put v_2 at the center and compute its vector in a different self-attention computation. If this is the case, why not just say the output is h_v1? If this is not the case, then each node will have multiple vectors: one is computed when the node is at the center, and others are computed when the node is sampled as a neighbor. \n\nBelow Boris Knyazev has several comments, which are not well addressed by authors. There is a discussion about transductive learning and inductive learning. However, it seems the authors still don't know how to run inductive learning on the graph classification task (quote \"... still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.\"). Boris does not suggest to use part of each graph; instead, he suggests not using test graphs. I believe this is the standard practice in inductive learning (e.g. kernel methods). \n\nAnother comment from Boris about the case when T=1, and the response is \"T=1 does not correspond to a single layer network\". I don't understand the response either. When T=1, a node only gets information from its neighbors. It is similar to a one-layer GCN or GAT, in which a node also only gets information from its immediate neighbors. \n\nI also don't understand why the author insists that the proposed model has a layer-based architecture. In my view, it is a graph neural network by the standard of propagation rule and output function. \n\n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\n\n\nThe paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects:\n\n1.\tIf we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough.  Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy.\n2.\tThe description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all.\n3.\tThe results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods).  I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it.\n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task.\n\nThere are several points that are unclear in the paper.\n1.  The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired.\n2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important.\n3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1.\n4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification.\n5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others.\n"
        }
    ]
}