{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper aims to extract the set of features explaining a class, from a trained DNN classifier.\n\nThe proposed approach relies on LIME (Ribeiro et al. 2016), modified as follows: i) around a point x, a linearized sparse approximation of the classifier is found (as in LIME); ii) for a given class, the importance of a feature aggregates the relative absolute weight of this feature in the linearized sparse approximations above; iii) the explanation is made of the top features in terms of importance.\n\nThis simple modification yields visual explanations that significantly better match the human perception than the SOTA competitors. \n\nThe experimental setting based on the human evaluation via a Mechanical Turk setting is the second contribution of the approach. The feature importance measure is also assessed along a Keep and Retrain mechanism, showing that the approach selects actually relevant features in terms of prediction. \nIncidentally, it would be good to see the sensitivity of the method to parameter $k$ (in Eq. 1).\n\nAs noted by Rev#1, NormLIME is simple (and simplicity is a strength) and it demonstrates its effectiveness on the MNIST data. However, as noted by Rev#4, it is hard to assess the significance of the approach from this only dataset. \n\nIt is understood that the Mechanical Turk-based assessment can only be used with a sufficiently simple problem.  However, complementary experiments on ImageNet for instance, e.g., showing which pixels are retained to classify an image as a husky dog, would be much appreciated to confirm the merits and investigate the limitations of the approach.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper proposes a new method for DNN interpretability based on LIME, itself based on ensembling of large numbers of low-complexity models called \"local explanation models\". This method allows to better capture the relative importance of each feature, and is also able to recover the class-specific signals that DNNs use to distinguish between a number of classes. \n\nScore: Weak Accept\nWhile this method seems like a good step forward, I am uncertain about its significance, mostly because of the choice of dataset and because it appears to be a minor change to a well known algorithm.\n\nI think some high-level aspects could be better motivated:\n- Why is interpreting black-box models a better avenue than building interpretable models?\n- Why should we expect this method to work beyond MNIST? Will it work on large images where translational invariance is a vital aspect? Will it work on non-image data?\n- Why does this relatively subtle change from LIME create such a dramatic visual difference? Do the authors have a stronger intuition than what is hinted at in the paper?\n\nOther comments:\n- The human evaluation is very interesting, and suggests that this method correlates much better with human judgement than previous ones. One key aspect that is missing, but hinted at, is where this difference comes from. It could be from the proposed change or from the class-specific information. (It should be easy to run KAR to test this?)\n- It's not always clear how this should be reproduced. For example, the authors specify the number of samples for SmoothGrad and VarGrad but not LIME nor NormLIME."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Months ago, I read this article on arxiv (https://arxiv.org/pdf/1909.04200.pdf). It is an interesting work that tries to propose a simple yet effective interpretable model. I am not familiar with this research direction, and I try to make an educated guess.\n\nPros:\n-- As the author suggested, the method is simple and effective.\n-- The authors conducted user studies to demonstrate that the results generated by their proposed method is strongly favored over previous methods.\n\nCons:\n-- Subscripts in equations need improvement to make them consistent. For example, in Equation (7), we have E_{y}, but in Equation (8), we have E_{Y_j} and E_{Y_k}.\n-- Section 4, Figure 3, top, it seems obvious to choose the fourth one to distinguish the number 9? I feel this example is too easy and not convincing enough."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\nThis paper proposes a novel scheme to aggregate local explanations\nfrom LIME to generate global and/or class-level interpretations in the\nform of feature importances. The technical novelty lies in the\nnormalization of the feature weights across local explanations where\nthe proposed scheme utilizes a well-motivated L0 normalization in\nplace of the existing L2 normalization in Ribeiro et al., 2016. The\nsparsity induced by the L0 normalization leads to more \"easy-to-read\"\nglobal (or class level) \"explanations\" since the number of features\nwith non-zero importances is reduced.\n\n\nThe simple idea of generating sparse aggregations of local\nexplanations to obtain global (or class-level) interpretations is well\nmotivated and seems fairly intuitive (and I consider simplicity a\nstrength) and the extremely favourable empirical results, I am leaning\ntowards a reject for the current version of the paper. The main\nreasons for this decision are as follows: \n\n- The proposed scheme is presented as closely tied to LIME, which\n  severely limits the scope of the proposed solution. However, I can\n  imagine that the high level idea of sparse aggregation of local\n  explanations can easily generalize to many local explanation\n  schemes. \n- The empirical evaluation needs to be improved (or better clarified)\n  in my opinion. There are many potential baselines that are also not\n  considered (see details below). Moreover, the evaluation is\n  completely limited to a single data set (which is good as a strawman\n  but not sufficient to make significant claims).\n\n\nClarifications re: baselines and empirical evaluations:\n\n- No comparison to Ibrahim, et al., 2019 -- you can cluster in\n  unsupervised setting but you can also aggregate within class in\n  supervised setting. Generating class-level interpretations is an\n  interesting idea but its novelty is somewhat unclear.\n- In terms of explanations, there are contrastive explanations [1]\n  which are not really discussed in this paper. This scheme could be\n  more useful for generating class level interpretations.\n- It is not clear why the negative correlations were removed in the\n  user study. In certain situations, such as when distinguishing\n  between digits 3 & 8, the absence of weights (potentially\n  corresponding to negative weights) can be crucial explanations.\n- It is not clear why in the SmoothGrad and VarGrad baselines, the\n  class level interpretations are generated with just 10 images per\n  class. Are the class level interpretations of LIME and NormLIME also\n  generated from just 10 images per class. If that is the case, 10\n  seems like a very small number and can potentially create noisy\n  explanations. If LIME and NormLIME used more than 10 images to\n  generate class level interpretations, the comparison does not seem\n  fair. Please explain this discrepancy.\n- For the evaluation in Sec. 5, it is not clear how the features are\n  removed. Are they removed based on the global explanations (feature\n  importances) of each baseline? If this is the case, are the global\n  explanations of each baseline generated using the same amount of\n  local explanations? Please clarify.\n- The KAR analysis results in Fig. 4(b) need to be further\n  investigated. First, it is not clear if this difference between the\n  different baselines something that happens in multiple datasets or\n  just this one. Moreover, it seems natural that something like\n  SmoothGrad and VarGrad would be able to capture the feature\n  redundancies if the redundancies were the actual cause for this\n  marked difference between the relative performances in Fig. 4(a) and\n  4(b). This could definitely use a better discussion.\n\n\n[1] Dhurandhar, Amit, et al. \"Explanations based on the missing:\nTowards contrastive explanations with pertinent negatives.\" Advances\nin Neural Information Processing Systems. 2018. "
        }
    ]
}