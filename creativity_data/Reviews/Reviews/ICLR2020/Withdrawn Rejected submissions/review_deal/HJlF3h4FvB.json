{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper tries to bridge early stopping and distillation.\n\n1) In Section 2, the authors empirically show more distillation effect when early stopping.\n2) In Section 3, the authors propose a new provable algorithm for training noisy labels.\n\nIn the discussion phase, all reviewers discussed a lot. In particular, a reviewer highlights the importance of Section 3. On the other hand, other reviewers pointed out \"what is the role of Section 2\", as the abstract/intro tends to emphasize the content of Section 2.\n\nI mostly agree all pros and cons pointed out by reviewers. I agree that the paper proposed an interesting idea for refining noisy labels with theoretical guarantees. However, the major reason for my reject decision is that the current write-up is a bit below the borderline to be accepted considering the high standard of ICLR, e.g., many typos (what is the172norm in page 4?) and misleading intro/abstract/organization. In overall, it was also hard for me to read the paper. I do believe that the paper should be much improved if the authors make more significant editorial efforts considering a more broad range of readers. \n\nI have additional suggestions for improving the paper, which I hope are useful.\n\n* Put Section 3 earlier (i.e., put Section 2 later) and revise intro/abstract so that the reader can clearly understand what is the main contribution. \n* Section 2.1 is weak to claim more distillation effect when early stopping. More experimental or theoretical study are necessary, e.g., you can control temperature parameter T of knowledge distillation to provide the \"early stopping\" effect without actual \"early stopping\" (the choice of T is not mentioned in the draft as it is the important hyper-parameter).\n* More experimental supports for your algorithm should be desirable, e.g., consider more datasets, state-of-the-art baselines, noisy types, and neural architectures (e.g., NLP models).\n* Softening some sentences for avoiding some potential over-claims to some readers.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a new perspective on understanding knowledge distillation as a transfer of information defined with respect to the neural tangent kernel. Additionally, a new framework for learning the classifier on a noisy labeled dataset is proposed based on the knowledge transfer framework. \n\nOverall, I think the paper lacks justification (and explanation) for its main statement on how knowledge distillation is related to the early stopping of the teacher network. Especially, it is confusing since Section 2 and 3 make different statements. Specifically, Section 2 shows that early stopping \"helps\" knowledge distillation while Section 3 shows that knowledge distillation can \"replace\" early stopping. The former observation implies that early stopping is complementary to knowledge distillation, while the latter implies otherwise.\n\nFurthermore, Section 2 mainly explains why the eigenspaces associated with the largest eigenvalue of the neural tangent kernel is \"informative information\". However, there is no elaboration on how the knowledge distillation process leads to the transfer of such information, i.e., there is no connection between the neural tangent kernel and the knowledge distillation process. Although Figure 1. suggests that early stopping indeed improves the knowledge distillation process, they are not enough to support the statement convincingly enough. \n\nWithout proper support on the main statement of this paper, the paper looses much of its claimed contributions. The label refinery algorithm for the noisy labeled dataset is interesting, but it is not evaluated thoroughly enough to demonstrate its superiority over existing algorithms. It also does not have much originality when compared to similar algorithms [1, 2]. Bagherinezhad  et al., [1] also tried to remove \"noisy supervisions\" that were generated by harsh augmentation on images. Han et al., [2] and Li et al., [3] also use distillation-like processes to learning noisy datasets.\n\n[1] Label Refinery: Improving ImageNet Classification through Label Progression, Bagherinezhad  et al., 2018\n[2] Co-teaching: Robust Training of Deep NeuralNetworks with Extremely Noisy Labels, Han et al., 2018 \n[3] Learning from Noisy Labels with Distillation, Li et al., 2017"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes a self-distillation algorithm for training an over-parameterized neural network with noisily labelled data. It is shown that for a binary classification task on clustered data (same as [Li et al. 2019]), even if the labels are corrupted, the self-distillation algorithm applied on a sufficiently wide two-layer network can recover the correct labels (in l_2 loss). Experiments on CIFAR-10 and Fashion MNIST are provided, which show that the self-distillation algorithm is effective for label noise with relatively little tuning.\n\nAlthough the theoretical part of the paper has a large overlap with [Li et al. 2019], I find the self-distillation algorithm very interesting and it's nice that it can achieve zero l_2 loss w.r.t the correct labels. However, I think the paper could still use some improvement.\n\n1. The theorem in Section 3.3 is only for binary classification. Can it be generalized to multi-class classification?\n\n2. The theoretical guarantee is only for training data. Is it possible to prove a generalization bound? There's a remark on top of page 8 about margin. It would be nice to elaborate on this and maybe make it formal.\n\n3. Section 2 is not very satisfying. I don't quite see the point of this section. In particular, the concept of AIR is nothing new and its connection to NTK top eigenspace has already been written in previous work (e.g. [Arora et al. 2019]). I'd suggest to not have this section and to make Section 3 the main contribution of this paper.\n\n\n-------\nupdate:\nThanks to the authors for the response and for adding a generalization bound.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper provides a theoretical framework to understand the regularization effect of distillation. Based on the observation that a overparameterized NN has the ability to memorize all the data, thus early stopping is essential for an overparameterized teacher network to extract dark knowledge from the hard labels, the author starts to analysis distillation from an early stopping view point.\n\nThen the author consider to use distillation to learning with corrupted labels inspired by the previous works using early stopping to clean label noise. The author also tries to formulate the “informative information” via the NTK theory. Theoretically, the author provide a proof of convergence to the ground truth labels in terms of l2 distance. While the previous result was on convergence in 0-1 loss, which means the distillation can enlarge the margin the classifier. From this proof, aurthors also make an interesting discussion to show how distillation introduces further information rather than just early stopping. Authors also demonstrate their result on Fashion MNIST and CIFAR-10 to convince the reader the benefit of the algorithm.\n\nThe paper is interesting and brings new theoretically thoughts to understand the distillation method. The relationship between early stopping and distillation can inspire the machine learning researchers to explore more about distillation both empirically and theoretically.\n\nMinor Questions:\n1. The analysis is based on the assumption that the teacher is overparameterized. What will happen if the teacher network is not overparameterized?\n2. Does the assumption of dataset is too strong in the theorem?\n3. Some notation is not clear, e.x. in  Algorithm1 what is $\\mathcal{N}(x_i, w_t)$ and what is base network and mother network? These should be instead by student network and teacher network.\n4. Author claims that early stopping is hard to tune while introducing extra hyper-parameters in the self-distillation algorithm.  Would the extra hyper-parameter makes the algorithm might be even harder to tune?"
        }
    ]
}