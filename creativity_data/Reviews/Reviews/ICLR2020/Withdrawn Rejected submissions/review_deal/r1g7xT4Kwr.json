{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\n\nThe paper presents a hierarchical approach for learning a policy to navigate indoor environments. It consists of a two cooperative policies consisting of a planner which provides sub-goals to the executor achieves the sub-goal. The executor provides a summary of it's trajectory back to the planner which helps the planner decide the next sub-goal better. They show results on Habitat for the task of point goal navigation. \n\nMy overall rating is mostly based on the points raised in the section below around experiments and and analysis. \n\n- Strengths :\n    - Instead of using a single way communication from planner to executor, they also provide a summary vector from the executor to the planner that gives feedback about the given sub-goal.\n    - The paper uses a hierarchical approach for point goal navigation and gives some theoretical explanation on how a long rollout horizon has negative impact on the exploration efficiency. Even though the proof was only for the 2 actions, they ran an empirical experiment to show that in the case of point-goal navigation task, as the horizon increases, the exploration efficiency begins to rapidly decline as evident by the normalized geodesic distance.\n\n- Weaknesses:\n    - Clarity: The paper is very light on details in section 3. Both the algorithms are mentioned in the supplementary but should be mentioned in the main paper. Also some of the notations are not mentioned clearly. For instance, what does the terminal (t_i) mean.\n    - Experimentation: The paper compares the proposed approach with Savva et al 2019. But the comparison is not fair.\n        - (1) the number reported in the paper are not the best numbers from Savva et al. The best numbers come by just using the depth sensor so showing results by using just the depth sensor modality might be better. Or replicating the experiments with all the different sensor modalities (RGB only, Depth only and RGB-D) will be better.\n        - (2) The approach mentioned in Savva et al doesn't build an egocentric map, so either endowing that approach with egocentric map, or removing that from the proposed approach will be a better evaluation.\n    - The authors also don't substantiate the reason for not using SPL enough in the form of citations? The paper mentions that maximizing on this metric as an objective can result in undesirable behavior but Savva et al doesn't maximize SPL reward as part of the objective. The metric is only used at evaluation time.\n    - The authors claim that the proposed approach is specially useful in the case of sparse reward, but their reward definition suggests that it's not a sparse reward task. \n    - Analysis and ablations: Even though it has been shown empirically that this hierarchical nature helps, it'd still be good to tease out the gains properly. Like mentioned before, are the gains simply coming from an agent having access to an egocentric map. Is the egocentric map even important for the planner. Giving some insights about what the planner has learnt and what the kind of sub-goals it generates for the executor would be helpful.\n    - Did the authors try simple baseline such as using a random policy for sampling sub-goal. There are other heuristic based baselines such as always generating a subgoal within k meters of the current position in the direction of the target.\n    - For pointgoal navigation, is the egocentric map all that useful? If the agent is building the egocentric map as it executes a policy, how does the planner plan ahead based on parts of the map that it hasn't discovered. Does it help in situations where it went down the wrong path, and has to come back?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a hierarchical learning method for the Pointgoal navigation task. It consists of two agents, a Planner and an Executor. The Planner agent proposes subgoals to the Executor which learns to reach the subgoal and passes back information to the Planner. Results on Gibson dataset in the Habitat environment show that the proposed method outperforms PPO LSTM and SMT baselines on RGB-D Pointgoal navigation task.\n\nThe authors motivate the method by stating that there is a need to plan across long time horizons with sparse reward signals. They state in their contributions that their method significantly improves sample efficiency. However, the experimental task setup and empirical results do not justify any of the above claims.\n\nSample efficiency: A simple classical map and plan deterministic baseline achieves a success rate of 0.976 on the validation set [2] and an SPL of 0.89 on the test set [1] for the RGB-D Pointgoal Navigation task without any training. This is much higher than the success rate of 0.81 reported by the authors using 30 million frames. \n\nSparse reward and long-time horizons: The task involves intermediate rewards equal to the reduction in the geodesic distance at each step. This is not a sparse reward task.\n\nThe current set of results do not empirically justify the significance of the proposed method as there exists a deterministic algorithm that performs much better than the proposed method without requiring any training data. The proposed method is specifically designed for the Pointgoal task and it requires the depth channel as input to build the map which is given as input to the Planner. The method can not be trivially applied to any other navigation task or RGB setting in the Pointgoal task. The theoretical result is trivial. It is widely known that the exploration sample complexity grows exponentially with the time horizon of the task. \n\nDue to the lack of empirical evidence to justify the effectiveness and significance of the method, I vote for rejecting the paper.\n\nOther comments and suggestions:\n- In the list of contributions in the introduction, I suggest changing the first contribution to a hierarchical RL approach ‘for navigation’ as it is not a generic HRL approach for any task.\n- The theoretical result in Section 3.1 is not required in my opinion. It could be replaced with Algorithms in the Appendix.\n- I can see why optimizing for SPL can lead to undesirable behavior, but there's no harm in reporting SPL even if you do not optimize for it, as your baselines are also not optimizing it. I would recommend reporting SPL as it is fairly popular metric and it helps in comparing with prior methods. \n- I suggest tackling RGB variant of the Pointgoal task or other navigation tasks such as Room, Object or Image Goals by modifying the proposed method.\n\n[1] https://evalai.cloudcv.org/web/challenges/challenge-page/254/leaderboard/839\n[2] https://github.com/s-gupta/map-plan-baseline"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a hierarchical policy consisting of a planner and an executor policy for solving navigation tasks. A planner policy consumes the desired target goal and a ELI context from the executor to propose subgoals for the executor. The executor consumes these subgoals to output actions that the agent should execute, and returns the ELI context to the planner policy.\n\nStrengths:\n1. Decomposing the problem hierarchically makes a lot of sense for the given task.\n2. The design of the ELI vector that passes information seen by the executor during the intermediate steps back to the planner seems novel, and has been shown to experimentally help with performance.\n3. Paper conducts experiments in visually realistic environments and presents comparisons against methods from recent papers.\n\nShortcomings:\n1. Limited novelty: Hierarchical decomposition is extremely specific to the task of pointgoal navigation. The proposed decomposition employs use of pointgoal targets for the executor. While this works for navigation, it is unclear how this can be made to work for other robotic tasks such as manipulation, or for other long-horizon problems like montezuma revenge. In fact, such a decomposition of the problem have been investigated in past works for navigation [A,B].\n\n2. Experimental evaluations:\na) Lack of use of standard metrics. Paper uses tasks in Habitat for evaluation, but doesn't use the metrics that come along with it. It is fine to point out problems with standard metrics, but it will really help if the original metrics were included (possibly in addition to other metrics that will address the shortcomings like #collisions, etc as mentioned).\n\nb) Additionally, there was a challenge https://aihabitat.org/challenge/2019/ at CVPR 2019, and there is a leaderboard with results: https://evalai.cloudcv.org/web/challenges/challenge-page/254/leaderboard/839 The paper should include these previous results on this task on the dataset. While I agree that details for some of these methods may not be available, making meaningful comparisons and discussions difficult, details about some of these methods are in fact available, and these methods should be included in the paper.\n\nc) Following up on point b), the paper is tackling the RGBD version of this problem and details about a very strong baseline `Map and Plan Baseline` is available along with code. I am not sure if success rates presented in Table 1 are directly comparable to these publicly available numbers, but it seems like this baseline outperforms the methods proposed in this paper by a healthy margin (81 vs SPL of 89, which means success rate would be even higher). This is striking as this baseline does not need any training at all. This being said, I can imagine that the proposed method can work with RGB images alone (while such a baseline may not), but this should be experimentally demonstrated. Even then, paper should compare to other methods for this task as available on the leaderboard.\n\nThus, while I like the hierarchical decomposition and the exchange of information between the executor and the planner via the ELI. However, I believe the contribution is limited, in that it is not clear how generally applicable the framework is, and the experimental validation for the problem where it is indeed applicable is weak.\n\n[A] Combining Optimal Control and Learning for Visual Navigation in Novel Environments\nSomil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik, Claire Tomlin\n\n[B] M. Müller, A. Dosovitskiy, B. Ghanem, and V. Koltun. Driving policy transfer via modularity\nand abstraction. arXiv preprint arXiv:1804.09364, 2018."
        }
    ]
}