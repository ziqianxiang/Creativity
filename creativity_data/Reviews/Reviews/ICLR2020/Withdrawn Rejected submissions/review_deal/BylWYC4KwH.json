{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper introduces an unsupervised concept learning and explanation algorithm, as well as a concept of \"completeness\" for evaluating representations in an unsupervised way.\n\nThere are several valuable contributions here, and the paper improved substantially after the rebuttal.  It would not be unreasonable to accept this paper.  But after extensive post-review discussion, we decided that the completeness idea was the most valuable contribution, but that it was insufficiently investigated.\n\nTo quote R3, who I agree with: \" I think the paper could be strengthened considerably with a rewrite that focuses first on a shortcoming of existing methods in finding complete solutions. I also think their explanations for why PCA is not complete are somewhat speculative and I expect that studying the completeness of activation spaces in invertible networks would lead to some relevant insights\"\n\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "Update: after reading the rebuttals, I raised my rating to weak accept. Authors provide an interesting and possible limitation of TCAV during rebuttal (in the XOR case TCAV might not be able to identify meaningful subspace). It would strengthen the paper by somehow proving it. Maybe you can visualize the TCAV vectors and your vectors in the simulation and use it to explain why TCAV selects unrelevant concept 11, or designs another simple experiments. \nAlso, if accepted, please consider more parameter stability analysis in the real datasets, including ablation studies of different regularizations. This helps readers understand the value of this method in the real data.\n\nOverall I like the paper much better after rebuttal, but still have a lukewarm feeling about it. I do not see lots of value in the current method, probably due to not very interesting or interpretable results in the real data. However, it does provide a new concept of completeness, and I agree with R1 that it's interesting and relevant. So I would change my rating to weak accept.\n\n====================================================================================================\nOriginal evaluation:\n\nThis paper can be seen as an extension to the paper Ghorbani et al., 2019 to try to extract \"complete\" unsupervised concepts as well as maintainting interpretability. Compared to Ghorbani et al., they have 2 modifications. First, they try to learn a set of concept vectors such that the resulting projected activations on this concept basis would not lose too much accuracy, which they call completeness. Second, to maintain interpretability for these vectors, they regularize these vectors to sparsely match to some input clusters (generated by the k-means) that human can examine, and regularize no two concept matches to the same cluster. They validate their concepts in the simulation, text and image datasets.\n\nOverall I feel the idea is interesting but a bit straightforward. The resulting concept vectors are also not very interpretable or interesting in the text and image domain, which seems to limit its utility to practioners. \n\nAlso, one of the problem is how to select the coefficients for the regularization. In the supplements authors mention that they choose lambda=10 and it produces \"reasonable\" results in the simulation. This decreases the credibility of the simulation that shows it outperforms the baselines such as TCAV, as it sounds like you set the parameters to get good metrics on this simulation. Moreover, in the real-world data there might not be an easy way to set these parameters. I think the authors could strengthen the paper by experimenting with different coefficients and examine the stability of the resulting explanations, or at least indicate how results differ if the coefficients are set differently. Or do an ablation study about how different objectives shape the resulting concepts.\n\nBesides, I will suggest to plot a figure to show number of concepts used v.s. the completeness score. It will motivate why choosing specific number of concepts in the experiements. Also experimenting with different number of concepts is helpful for readers to see the stability of this method as well. \n\nSome minor questions:\n1. The claim of the result of the image dataset says it focuses on the texture. I think it might be caused by the super-pixel segmentation, which already removes most of the edges and leaves with the texture clusters. \n2. Any intuition of why TCAV performs worse in the simulation? How do you select the top 5 vectors for the TCAV, and why does it select non-useful concept 11?\n3. In the definition saliency score (page 4), I am just wondering if the normalized vector could be better? So the resulting value would not be affected by different magnitude of u_k or concept vectors c_j.\n4. In the definition of the conceptSHAP score, shouldn't it be the average of conceptSHAP score across all the classes instead of the sum (inline equation near eq. 6)? If you have 1000 class, then the conceptSHAP score could be something around 1000, that will be wierd.\n\nWriting:\n- I am confused at first read in the 2nd paragraph of the section 3 in page 4. The word \"concept\" is overloaded and represent both as input concept clusters and the resulting concept vectors. Such as \"...candidate clusters of concepts...\" or \"... each concept is salient to one cluster ...\".\n- Typo: \"They main takeaway\" -> \"The main takeaway\"\n\nStrengths:\n+ Good related work\n+ Somewhat complete evaluation\nWeaknesses:\n- No analysis with so many hyparparameters (reg lambda, number of concepts), and thus not sure about the validity of the simulation\n- Idea is interesting but straightforward\n- Not very interpretable results",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The authors are concerned with improving algorithms that propose sets of\nconcept vectors to help explain the prediction of a pre-trained network on a\nheld-out test examples. The paper examines \"completeness\" (Gilpin et al 2018) of\nthe context vector set---projecting activations on to the concept subspace does\nnot hurt predictive performance---as a desired criteria that was overlooked by\nprevious concept-based explanation methods. They show that PCA on activations\nyields complete concepts under some bijectivity assumption on the final layers\nof the network. They then discuss how to produce a set of concept vectors that\nmaximize completeness under sparsity priors when supervised or unsupervised\nclusters of candidate concepts are given. They also show that SHAP values can be\napplied to compute the relative importance of each concept to the overall\ncompleteness score.\n\nI find the focus of this research---on the completeness of concept\nexplanations---to be quite interesting and relevant to the interpretable machine\nlearning literature. The authors have approached this question from several\ndirections and have demonstrated a fluency with the existing literature and its\nlimitations. However, I think they could do much more to motivate their\nproposals by convincing the reader that (a) existing concept-based explanations\nare not complete in practice and (b) PCA in the activation space is not\nsufficient as a baseline method for proposing concepts. In the current revision,\nthe paper lacks coherency---the proposals in each section do not seem connected\nto one another---and it is difficult to assess from the experiments whether the\nproposed algorithms address a known problem in existing methods.\n\nFor example, the propositions in S2 rely on the ability of the post-concept\nfunction h to bijectively map between the Frobenius norm in the concept space to\nthe loss function in the logit space. Therefore it is logical to ask whether\nconcepts can be found directly via PCA on the concept space. The authors\nspeculate that this bijectivity of h is unlikely to hold in practice, and \ntherefore PCA would be insufficient. However I would have been more convinced by\nsome empirical evidence for this claim. What about the case where f is bijective\n(i.e. an invertible neural network); is PCA on the activation space sufficient\nto produce complete and useful concepts in\nthis case?\n\nI also wondered whether there is a corner case where the functional form of h\ncauses the components that capture the most variance in the activation space\n(i.e. top PCA vectors) to capture the least variance in the logits space. This\nseems plausible if h is an unconstrained linear mapping or non-linear mapping;\nthink about a simple whitening function that scales principal components in\ninverse proportion to their eigenvalue. In this case we may be better off using\nthe bottom PCA vectors than the top ones. Admittedly this is only thought\nexperiment, but is there any way for the authors to show that this does not happen\nin practice by characterizing the h functions we see for neural networks applied\nto application areas of interest: sentiment analysis and image classification? \n\nThe synthetic experiments seem useful to the overall story (although the data\ngenerating process is somewhat hard to parse at first), and I like that\nthere are many baseline methods in this study. It seems that PCA is neither\ncomplete nor aligned in this simple setting. It also seems that TCAV is not\ncomplete. Unfortunately the authors fail to show that similar trends hold in the\nnon-synthetic setting. My feeling is that a more comprehensive and thorough\nexperimental study---starting from a characterization of a problem with existing\nmethods---would strengthen the paper considerably.\n\n\nSome superficial comments:\nS1\n* \"This has thus lead to an increasing interest...\" It would be good to add\n  references to this claim.\n* \"Of course such degeneracy assumptions likely not hold\" -> \"Of course such\n  degeneracy assumptions likely do not hold\" -> \n* \"...which can explain how much does each concept contribute to the\n  completeness score\" -> \"...which accounts for the contribution of each concept\n  to the completeness score...\"\nS2\n* \"...that capture how complete is a given set of given concepts\" -> \"...that\n  capture the completeness of a given set of given concepts\"\n* \"...concepts hold sufficient info for prediction\" -> \"...concepts hold\n  sufficient information for prediction\"\n* \"...prediction scores for examples in class A won't be much different from\n  examples in class A\" -> \"...prediction scores for examples in class A won't be\n  much different from other examples in class A\"?\nS3\n* \"...each concept direction is semantically meaningful to human\" -> \"...each\n  concept direction is semantically meaningful to humans\" \nS5\n* \"y1 = ~...\" could you make a note that \"~\" denotes logical not? \n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors build on the work by Ghorbani et al. in concept-based interpretability methods by taking into account the \"completeness\" of the concepts. This basically tests whether the models accuracy holds if the input is projected onto the span of the discovered concepts. They propose \"ConceptSHAP\", based on Shapley values, to assign importance to the learned concepts. These could be shown to satisfy some reasonable properties such as efficiency (sum of importances equals total completeness value), symmetry, additivity, dummy (a concept that does not change the completeness universally should have zero importance) as stated in Prop. 4.1. The method is finally tested on a variety of datasets, including a synthetic one, which shows that optimizing \"completeness\" helps in discovering a richer variety of important  concepts than prior work). \n- I was wondering how the completeness and importance measures change when the input is perturbed slightly such that the classifier output doesn't change? \n- How the concepts would change if the completeness measure is removed from the optimization in text and image-classification?"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary \n\nThe paper proposes metrics for evaluating concept based explanations in terms of ‘completeness’ -- characterized by (1) whether the set of presented concepts if sufficient to retain the predictive performance of the original model and (2) how is performance affected when all information useful to a complete set of concepts (as per (1)) is removed from features at a specific layer. Assuming concept vectors lie in linear sub-spaces of the activations of the network at a specific layer, the underlying assumption is that if a given set of ‘concept’ vectors is complete, then using a projection of the intermediate features from input onto the sub-space spanned by concepts should not result in reduced / affected predictive performance. Based on these characterizations, the paper proposes an objective to discovering complete and interpretable set of concepts given a candidate cluster of concepts. Furthermore, the paper proposes metrics to quantify the importance of each concept (using Shapley values) and per-class importance of concepts. The authors conduct experiments on toy data, image and text classification datasets and show that their proposed approach can discover concepts that are complete and interpretable.\n\nStrengths\n\n- The paper is well-written and generally easy to follow. The authors do a good job of motivating the need for the completeness metric, characterizing it specifically in the case of concepts spanning sub-spaces of activations and subsequently utilizing the same to motivate an effective concept discovery method.\n\n- The proposed approach and angle being looked at in the paper is novel in the sense that while prior work has mostly focused on characterizing concepts which are salient. Ensuring that concepts are sufficient for predictive performance ensures the fidelity of the interpretability approach.\n\n- I like the fact that the authors decided to capture both aspects of the completeness criterion -- (1) projection to concept-space should not hurt performance and (2) how does removing concept-projected information from the features affect performance. Capturing both provides a holistic viewpoint of the features of the concerned layer -- (1) can explain features/decisions with an associated metric based on the ‘imperfect’ set of concepts and (2) captures the effectiveness of the information present in the features if we remove all concept-useful information.\n\n- The choice of using Shapley values in ConceptSHAP as a metrics provides a whole range of desirable properties in the metrics used indicate the quality of concepts at different levels of granularity -- per-class importance of concepts across classes adding up to the overall importance of a concept. Furthermore, the observation that under certain conditions, the top-k PCA vectors maximize the defined completeness scores is interesting as well.\n\nWeaknesses\n\nHaving said that, I’m interested to hear the thoughts of the authors on the below two points. My primary (not major) concern is the fact the proposed approach is only centered around ensuring and evaluating fidelity of the discovered concepts to the original model. \n\n- While the proposed approach and evaluation metrics are novel, and the results generally support the claims of the paper -- toy experiments result in recovery of the ground-truth concepts, concepts identified for text and image classification offer feasible takeaways -- there is still a lack of proper human-interpretability aspect of the discovered concepts. The proposed approach to discover complete concepts mostly acts similar to a pruning approach on top of a candidate set of concepts based solely on fidelity to the original model. The obtained concepts are mostly explained via feasible hypotheses. One possible experiment that can be used to capture the reliability aspect of the discovered concepts could be as follows -- “Given the set of concepts (and representative patches) and SHAP values across all (or most relevant) classes, are humans able to predict the output of the model?” Is it possible to setup and experiment of this sort? I believe it might help understand the utility of the SHAP values in this context (beyond the advantages in terms of manipulation and characterized completeness).\n\n- The experimental results for image classification are presented on the Animals with Attributes (AwA) dataset. AWA is a fine-grained dataset with only one class present per-image. I’m curious to what happens when the same approach is applied to datasets where images have multiple classes (and potentially distractor classes) present. Is it possible that it becomes harder to discover ‘complete’ concepts (subject to the availability of a decent approach to provide an initial set of candidate clusters). Do the authors have any thoughts on this and any potential experiments that might address this?\n\nReasons for rating\n\nBeyond the above points of discussion, I don’t have major weaknesses to point out. I generally like the paper. The authors do a good job of identifying the sliver in which they make their contribution and motivate the same appropriately. The proposed evaluation metrics and discovery objectives offer several advantages and can therefore generally serve as useful quantifiers of concept-based explanation approaches. The strengths and weaknesses highlighted above form the basis of my rating.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}