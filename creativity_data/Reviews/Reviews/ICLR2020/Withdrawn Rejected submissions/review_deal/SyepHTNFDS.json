{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose a graph residual flow model for molecular generation.  Conceptual novelty is limited since it is simple extension and there isn't much improvement over state of art.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "GraphNVP is the first paper to introduce the concept of \"invertible flow\", that is to construct the invertible mapping from latent vector z to the graph G. By constructing the mapping from G to z, GraphNVP first changes the discrete feature vector into continuous variables, then update this matrix representation by scaling and transforming functions (Eq. (2)-(5) in this GRF paper). In each iteration the matrix is only updated by one row (one slice for the tensor), while keep other rows intact. Then for constructing the inverse mapping, we can first sample a random vector and then apply the “inverse” of the update rule to recover the edge matrix and node matrix respectively.\n\nThe main contribution for GRF paper is to find a new update rule from the idea of ResNet. The author thinks that GraphNVP only update one row each time, which is less efficient, and the model can only cover a limited number of mappings. Then he proposed a new function for update (Eq. (6)-(7)), which updates all rows each time. The author shows how to approximate the determinant of the Jacobian matrix, and how to construct the inverse mapping from fixed-point iteration, as the mapping is Lipschitz. Lastly the author shows that GRF uses much less parameters than GraphNVP both theoretically and practically, which means that the new model is more expressive. However, the new model may output the same molecule for several times (shown by “Uniqueness\"), and it favors the striaght-chain molecules.\n\nAnother question related to the experiment is that how does the method compared to methods which first generate SMILES string and then convert to molecule graph? Eg. Dai Et al. ICLR 2018, Syntax-directed generative model for structured data. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces an invertible deep generative model architecture for modeling molecular graphs. The model is based on graph residual flows (GRF), which is a graph variant of normalizing flows. The GRF model is a refinement of the GraphNVP generative model, which is also invertible,  but which does not seem to work very well for sparse low-degree graphs (such as molecular graphs). \n\nThe main new idea seems to be to replace the regular coupling flows of GraphNVP with residual blocks: the residual blocks ensure invertibility while also helping to \"mix\" the information in the (tensor) representations in each layer better. This leads to more compact generative models. Due to the residual structure of the encoding model, the inversion (backward mapping) is a bit more expensive since it requires solving a fixed point problem in each layer, but in principle it can be done provided the layer weight matrices have small spectral norm.\n\nExperimental results on two molecular datasets seem to confirm the validity of the proposed architecture.\n\nThe paper is nicely written and the techniques/results are clearly described. However, I have two main concerns:\n\n- Conceptual novelty seems to be limited. The method seems to be a basic modification of the GraphNVP approach by introducing ResNet-like skipped connections. Not entirely sure if this counts as a major contribution. \n- Experimental results do not seem to suggest improvements over the state of the art. I am not an expert in molecular generation, but looking at Tables 1 and 2 seem to suggest that the smaller number of parameters in GRF (compared to GraphNVP) come with dramatically reduced performance measures pretty much across the board. This seems like a weak result So I do not know whether the tradeoffs are worth it or not.\n\nOther comments:\n- The detailed pseudocode of Algorithm 1 isn't really necessary considering it is just a fixed point iteration.\n- Unclear why the reconstruction error in Fig 2 does not monotonically decrease for the ZINC dataset.\n- Unclear why Fig 3 suggests smooth variations in the learned latent spaces. I can only spot mode collapses and sudden jumps. It might help to plot VAE/GraphNVP embeddings here too.\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\n\nThe paper proposes a flow-based model for generating molecules, called Graph Residual Flow (GRF). GRF is a version of iResNet that is tailored to molecule generation. The experiments show that GRF achieves similar performance to GraphNVP (a previous flow-based model for molecule generation), but has about 100 times fewer parameters.\n\nDecision:\n\nThis is interesting work, but in my opinion the current version of the paper needs significant improvement. For this reason, my recommendation is weak reject.\n\nContribution & originality:\n\nI think the application of flow models to the generation of molecules is an interesting research direction. Since molecules are structured objects that can be represented as graphs of connected atoms, care has to be taken to design suitable generative models for them, and this paper is a step in this direction.\n\nThe proposed model (GRF) is similar to iResNet in most aspects. The main differences are:\n- The linear layers in iResNet are modified to take into account the connectivity of the molecule.\n- Separate networks are used to generate the adjacency tensor and the feature matrix. The adjacency tensor is generated first, and then the feature-matrix network is conditioned on the generated adjacency matrix.\nOther than the above, GRF is a straightforward application of iResNet in molecule generation.\n\nTechnical & writing quality:\n\nThe experiments are interesting, and GRF is shown to perform similarly to GraphNVP while using 100 times fewer parameters. The comparison between GRF and GraphNVP is in my opinion the most interesting part of the paper. The experiments that show GRF can reconstruct molecules exactly and that generated molecules are a smooth function of the latent variables are less interesting in my opinion; exact invertibility and smoothness are inherent properties of flow-based models that exist by design, so there isn't anything surprising about them.\n\nThe main issue with the paper is that it's poorly written. In my opinion, the writing quality of the paper needs significant improvement for the paper to qualify for publication. The model is explained very poorly; if I weren't already familiar with iResNet I don't think I would be able to understand the model and be able to reproduce the results by reading the paper. Moreover, the use of English throughout the paper is poor: there are a lot of grammatical mistakes (such as incorrect usage of articles or lack of subject-verb agreement), and awkward expressions. In the following I will do my best to make concrete suggestions for improvement, but I would strongly encourage the authors to revise the paper thoroughly and have it checked for grammar and writing quality.\n\nSuggestions for improvement:\n\n\"The decoders of these graph generation models generates\" --> generate\n\n\"The state-of-the-art VAE-based models [...] have good generation performance but their decoding scheme is highly complicated\"\nIn what way is it complicated? Please provide more details.\n\n\"invertible flow-based statistical models [...] does\" --> do\n\n\"[flow-based models don't] require training for their decoders because the decoders are simply the inverse mapping of the\nencoders\"\nThis is an odd and unusual description of flow-based models. The way I see it, flow-based models aren't autoencoders, but functions that reparameterize data (here molecules) in terms of simpler random variables (here Gaussian noise). Saying that flow-based models don't require training for their decoders makes little sense.\n\n\"Liu et al. (2019) proposes\" -->  propose\n\n\"GraphNVP has a serious drawback when applied for sparse graphs\" --> applied to sparse graphs\n\n\"only one affine mapping is executed for each attribute of the latent representation\"\nThere are also coupling layers that are non-affine and therefore more flexible. For example:\n- Neural Autoregressive Flows, https://arxiv.org/abs/1804.00779\n- Sum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325\n- Neural Spline Flows, https://arxiv.org/abs/1906.04032\n\n\"show that a generative model based on the GRF has much fewer trainable parameters compared to the GraphNVP, while still maintaining a comparable generation performance\"\nIf GRF can achieve a performance similar to GraphNVP with fewer parameters, shouldn't it be able to achieve much better performance with more parameters? If this is true, I would have expected to see an experiment were GRF uses more parameters and as a result outperforms GraphNVP. If GRF doesn't outperform GraphNVP even with more parameters, then I would expect to see a discussion explaining why.\n\nit would be good to explain fully the format of the data as early as in section 2.1. For example, it wasn't clear to me until much later how a flow-based model can be applied to discrete data, and whether the rows of the adjacency tensor and feature matrix are one-hot or not. Currently, the format of the data is partially explained three times (in section 2.1, section 3.1 and section 4.1) and only in section 4.1 things become clear for the reader.\n\nBeginning of page 3: the dequantized matrix X' is used before having been defined. This is an example of why the format of the data should be fully explained early on.\n\n\"The abovementioned formulations map only those variables [..] (Eqs.(2,5), and the remaining nodes [..] (Eqs.(3,5)\"\nThe first reference should be Eqs. (2,4). Also, the parentheses should close.\n\n\"ResNets [...], the golden standard for image recognition\"\nThe expression is \"gold standard\" not \"golden standard\". Also, it's too strong a statement and rather subjective to say that ResNets are the gold standard for image recognition; better say that ResNets have been successful in image recognition, which is an accurate and objective statement.\n\nSection 2.2 introduces iResNets, but it doesn't explain clearly that making the residual block contractive is a sufficient condition for invertibility. This is a crucial element for understanding iResNet that is used later on in the paper, so it should be clearly explained early on.\n\n\"i-ResNet [...] presents a constraint regarding the Lipschitz constant\"\nToo vague, and it doesn't explain what the constraint is.\n\n\"MintNet [...] derives the non-singularity requirements of the Jacobian of the (limited) residual block\"\nAlso vague and not informative.\n\nBeginning of section 3: GraphMVP --> GraphNVP\n\n\"we cannot directly apply the change-of-variables formula directly\"\nUses \"directly\" twice.\n\n\"we do not take it consideration\" --> we do not take into consideration\n\n\"an tensor entry X'[i, m] is a neighborhood\" --> a tensor entry X'[i, m] is a neighbor\n\n\"Similar discussion holds for A'\"\nIt's unclear to me what exactly holds for A', please be more specific. The previous statement mentioned neighbours, how does this apply to A'?\n\nIn the text after eq. (10), R_X uses a different font.\n\n\"is subject to update in every layers\" --> in every layer\n\nSection 3.3. is particularly poorly written. Given that this is one of the main contributions of the paper, it is really important that this section is written well and clearly. Currently, the section contains two one-sentence paragraphs that are clearly out of place. Also, eq. (12) makes little sense. In particular:\n- Isn't A a tensor of shape NxNxR? How did it become a matrix?\n- In what sense is X a matrix representation of z_x? How is X computed? Shouldn't X be of shape NxM? Also, please use a different symbol other than X, as X is already used for the feature matrix.\n- What is D and how is it defined?\n- Is vec() the vectorization operator? Does that mean that the input and output to the residual block has been flattened? This has not been explained or mentioned anywhere.\n- Is W a fully-connected linear layer or a convolution as in iResNet?\n- Where did the term D^{-0.5} A D^{-0.5} come from? How is it motivated / derived? Why does it make sense to use it? I understand that it's related to the graph Laplacian, but more information is needed for readers who are not familiar with graphs.\n\n\"is a Lipschitz-constant of for a certain function\" --> of a certain function\n\nThe explanation of eq. (13) is very poor, and I don't think a reader who is not already familiar with iResNet can follow it. Also, how was the infinite sum approximated? Did you use truncation or the Russian-roulette estimator?\n\nSection 3.4 introduces the term \"atomic feature tensors\" which is not defined. I presume it's the same as the feature matrix. Please be consistent with terminology throughout the paper.\n\n\"adjacent tensor\" --> adjacency tensor\n\n\"we have configure\" --> configured\n\nSection 3.4.2 finally explains the invertibility condition. Clearly, this is an important element of the algorithm, and should be explained fully and clearly early on. By this point, the invertibility condition should be clear, and no further explanation should be needed.\n\n\"we selecte\" --> selected\n\n\"satisfies the differentiability\" --> \"satisfies the differentiability condition\", or even better \"is differentiable\"\n\nWhat does \"kekulized\" mean?\n\n\"conmprises\" --> comprises\n\n\"we adopt node-wise weight sharing for QM9 and low-rank approximation and multi-scale architecture\"\nThese are important aspects of the architecture that are nowhere else mentioned, and should be explained more thoroughly.\n\n\"p_z(T_X, T_A; z)\"\nShouldn't this be p_z(z; T_X, T_A)?\n\nWhat does \"unique molecules\" mean? Are they the molecules that don't exist in the data? How do they differ from \"novel molecules\"?\n\nIt would be good to provide additional results. In particular:\n- It would be good to report training/validation/test log likelihood, or learning curves, that compare between GRF and GraphNVP.\n- It would be good to show how the metrics V, N, U, R in tables 1 and 2 vary with respect to the temperatures (for example, show curves of the four metrics as temperatures vary). Currently, results are reported for only one setting of temperatures, which doesn't show how sensitive the performance is to the temperatures.\n\nThe temperatures for ZINC-250k are really low (0.15 and 0.17), which to me is an indication that the flow model may have fitted the data quite badly.\n\nHow does GRF compare to GraphNVP in terms of generation time? How slow is it to generate from GRF given that it takes 30 iterations for each residual block?\n\nWhat is GraphLinear? Maybe give a citation?\n\n\"Since one of the most GPUs currently used\"\nThis phrase doesn't make much sense.\n\n\"principle components analysis\" --> principal components analysis\n\"Principle\" has a different meaning from \"principal\".\n\nFigure 3: Wouldn't it be more informative to centre the visualizations at the mean and use the two principal axes, rather than random mean and axes?"
        }
    ]
}