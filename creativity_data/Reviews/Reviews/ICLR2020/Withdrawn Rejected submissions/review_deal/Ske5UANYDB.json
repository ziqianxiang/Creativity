{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "The paper studies theoretical perspective of double descent phenomenon for the interpolated K-NN classifier.\n\nThe paper is works in several interesting directions and gives theoretical reasoning to how interpolated K-NN could exhibit the double descent phenomenon. They give theoretical justifications albeit with strong assumptions.\n\nI think the paper is a good paper. However, I have concerns with the presentation quality of the paper. It is very tough to get through the paper till the end. \n\nIn my view, it would have been an Accept if the paper was well written.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018).\n\nUnder various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding).\n\nOverall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted.\n\nIn particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). \n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper is about interpolation schemes in the particular case of the\nNearest Neighbor algorithm. The authors investigate the bene\ft, mainly\ntheoretical, of the proposed interpolation. They study minimax rates of\nthe proposed interpolated-NN for both classi\fcation and regression. The\nstatistical stability of the Interpolated-NN is adressed.\nThe paper is easy to understand and correctly written. Nevertheless,\nIt is a particular case of ( \\Over\ftting or perfect \ftting? Risk bounds for\nclassi\fcation and regression rules that interpolate\", Belkin, M., Hsu, D.,\nand Mitra, P. (2018a)) with an explicit interpolation schemes given by the\neuclidien distance power gamma. It appears as an application of the above\npaper which brings only few theoretical advantages and not enough to justify,\nalthough intuitive, the choice of these weights. Only few discussions and\nno comparison to others bounds (as those in the above paper) of the main\ntheorem are given. The paper is too much incremental from the papers of\nBelkin et al. (2018) and Xing et al. (2018) and the bene\fts of the proposed\ninterpolation are limited. Furthermore, the empirical performance of the\ninterpolate-NN is clearly not convincing and show no signi\fcant practical\nadvantages of the proposed method. As the goal of the paper is clearly\ntheoretical, the 'real data analysis' part is not necessary in my opinion."
        }
    ]
}