{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary: This work presents a \"factorized multimodal transformer\" (FMT) as well as the concept of \"factorized multimodal self-attention\" (FMS). The basic idea behind these approaches is to structure the self-attention layers in a transformer to explicitly account for the fact that there are different modalities in the input, i.e., attention is computed across modalities as well as within modalities in a structured way---rather than just naively applying a transformer to the concatenated multi-modal input. Numerous implementation adjustments are made in order to ensure the model is not too overparameterized, with the primary strategy being to use 1D CNNs to reduce the dimensionality of the internal representations after applying FMS layers. One multi-modal language benchmarks the proposed approach outperforms a number of strong and recent baselines.\n\nAssessment: Overall, this is a borderline paper. It is a very incremental contribution in terms of methodology, but the empirical results are strong. The authors themselves acknowledge that there proposed approach is very similar to Tsai et al. 2019, which also proposes a multi-modal variant of the transformer model. The key distinctions---as the authors point out---is (a) the use of 1D CNNs to reduce the dimensionality of the intermediate layers of the network and (b) the use of trimodal interactions, and (c) the use of unimodal self-attention in addition to intermodal attention, and (d) the fact that this approach uses the full time-domain as input. These differences seem to lead to better performance. However, they are not major conceptual changes or advancements and essentially amount to minor architectural differences. \n\nAs another minor comment, the introduction frames the contribution as being generally relevant for multi-modal learning. However, the proposed architecture has many design decisions made specifically for the task of multimodal language modeling, and the model also assumes a time-varying input. This should be acknowledged in the paper and the framing should perhaps be adjusted. \n\nFinally, the paper is very lacking in detail (e.g., detailed equations and derivations), and the paper does not appear reproducible based on the paper description alone (i.e., without code). If the code were not linked, this would be a major issue. The paper is only understandable by reference to the original transformer work and---due to the lack technical detail---does not stand alone as a research contribution. Indeed, I believe I understand the workings of the model, but there are certain aspects that would require me to investigate the code to fully understand, which indicates that the paper could substantially benefit from improved technical writing and more detailed descriptions of the methodology. \n\nReasons to accept:\n- Strong empirical results\n- Thorough treatment of baselines and related work\n\nReasons to reject:\n- Incremental contribution (especially compared to Tsai et al., 2019)\n- Writing lacks sufficient technical detail\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a method for multimodal sequential learning. The input consists of three modalities (e.g. visual, language, acoustic data). The output is the classification label for multimodal sentiment analysis, multimodal emotion recognition, and multimodal speaker traits recognition. The approach explicitly accounts for possible unimodal, bimodal and trimodal interactions existing within the multimodal input space by using the factorization. Compared to previous works, this method is able to model the intra-model and inter-modal dynamics within asynchronous multimodal sequences.\n\nStrengths:\n1. Based on an intuitive idea, this paper shows its method outperforms the previous works on several tasks. The brief method they proposed achieves a powerful result instead of using complex structures. \n2. The overall experiments and analyses support the theory of the paper and show the necessity of the factorized method. the problem of multimodality is solved well.\n\nWeakness:\n1. The paper dis not show the training epoch and the convergence time, so we could not compare the effectiveness of the proposed method with the other methods.\n2. Theoretical justification is not sufficient."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes Factorized Multimodal Transformer (FMT) for multimodal sequence learning. \nThe “architecture” consists of multiple multimodal transformer layers (MTL), which in turn consist of multiple Factorized Multimodal Self-attentions (FMS). \n\nUnfortunately, I cannot describe their methods and model in any detail. This is because the paper lacks details about the used methods and contributions. There is not a single equation that explains the model or objective function. In fact, the paper does not contain a single equation at all (there is 1 inline equation describing the data). \n\nInstead, the paper reads like a vague procedure, lacking explanations and details. Some examples:\n- “The input first goes through an embedding layer, followed by multiple Multimodal Transformer Layers (MTL). Each MTL consists of multiple Factorized Multimodal Self-attentions (FMS).” -> However, the paper never explains what FMS or a summarization network is. Please just put some equation or reference there.\n- “S1 and S2 are two summarization networks.” -> What is that? \n- “the output of each of the attentions goes through a residual addition with its perceived input (input in the attention receptive field), followed by a normalization.” \n- “For supervision, we feed this input one timestamp at a time as input to a Gated Recurrent Unit (GRU) (Cho et al., 2014). The prediction is conditioned on output at timestamp t = T of the GRU, using an affine map to dy.”\nBesides these procedural explanations, the paper contains figures to visualize the model. I am sure the authors tried their best to make this very intuitive and easily accessible for readers. I certainly value that intention. \nHowever, these visualizations are useful only if presented together with a detailed description (e.g. a formula) of the model, objective function, etc. I can only speculate what is shown in Fig. 2. The caption does not at all explain what is shown. \n\nI also can not tell what the contributions of this paper are either. The authors propose this FMT consisting of MTL consisting of FMS. But what exactly is novel here? What distinguishes it from related work? Is it the factorization of … something? Where does anything factorize actually? This seems to be an important part of their approach since the term is used in the title of the paper and in most of the components. But again, there is not a single equation that would show the factorization. \n\nIn summary, this paper should be rejected, because it lacks details about used methods, models, objective functions, algorithms, etc. This makes it impossible to judge the quality and validity of the work and further makes it irreproducible. I am sure the authors can greatly improve the quality of this paper by writing it in a more scientific manner and providing all necessary details. "
        }
    ]
}