{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a compression technique for neural nets based on matrix factorization. Inspired by an SVD low rank approximation, the idea is to replace each n x m parameter matrix by a multiplication of n x p and p x m matrices, with p sufficiently small. This is applied as a systematic architecture change, rather than only after training is complete as in related work. Experiments with LSTMs and Transformers for neural MT over several language pairs show that the technique works better than pruning weights with small values.\n\nStrengths: very clearly written, very straightforward technique, impressive experimental results.\n\nWeaknesses: missing experiments on larger datasets and other problems, missing comparisons to more competitive compression techniques.\n\nI don’t think the paper builds a convincing enough empirical case for the proposed technique. The main problem is that the good performance of the compressed models could be solely due to regularization, since the corpora used are small by NMT standards. Further experiments with regularized models (eg by optimizing dropout) and larger corpora would be needed to exclude this possibility. Also, since this is an ML conference, and the technique is extremely simple and general, results in other settings besides NMT should be provided.\n\nAnother problem is the comparison to compression baselines. The results in table 5 show that post-training factorization looks competitive at 22% reduction; why not show similar results for the broader range of compression levels in tables 1 and 2? Distillation is another popular technique that should be compared to. Finally, I am not an expert on pruning, but it looks like there has been more recent work than See et al (easy to find by chaining forward on Scholar), some of which claims even more impressive compression rates than reported here.\n\nFurther comments:\n\nThe fairly large advantage of LSTM over Transformer in tables 1 and 2 (26.7 > 26.0) indicates that Transformer might not be trained/configured properly\n\nIn table 1 and 2, should either change “Size reduction” heading to “Relative size” or make entries positive."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper considers training neural networks with a low rank constraint on the layers for parameter reduction and as a regularization. The paper considers machine translation task, and experiments with LSTMs and Transformers. The paper uses IWLST 2014 dataset for experiments with English to German, Portugese and Turkish translation pairs.\n\nThe paper enforces this factorization of layers during training, rather than post training compression techniques. This is appealing as it results in smaller models during training.\n\nThe paper while considers an important question about training neural networks with a low rank constraint, I find it sadly lacking in details and is hard to understand the exact experimental setting and the reason for the shown gains. \n\nWhat are the number of steps each model is trained for? What is the stopping criterion used? Are the same settings (batch size, learning rate, stopping condition, optimizers, regularization) used for training all the models compared in Table 2, for example? What is being factorized in the attention layer? Are you factorizing all the query, key, value and projection matrices? What do you do for multihead attention? Multihead already adds a low rank constraint. Do you add another rank constraint on top of this?\n\nIt looks like different batch sizes are used for training factorized vs standard models, as shown in Fig 2, is it the case even with the results in Table 2? Even in Fig 2, it is not clear that factorized models offer an advantage. What happens if the training is continued for more iterations in that figure? Are the gains mainly because of using  a larger batch size or because of the regularization effects of low rank constraint?\n\nAlso if we compare the results in Table 1 and Table 2, we see that Transformers have smaller BLEU score than LSTMs. It seems like the models are not sufficiently trained, and may be if they are trained for long enough (to see competitive BLEU scores), it is not clear if the regularization will still help in that setting.\n\nMost of the existing works observe that try to apply a low rank constraint during training observe negative effects in performance, resulting in reliance on  post training model reduction techniques. What is the difference in the approach followed by this paper that actually helps in performance with this additional regularization, compared to existing works? As far as I can tell from the paper, the approach is to just replace the standard layers  with factorization, which is known to hurt the performance (Kuchaiev and Ginsburg 2017).\n\nFinally the paper is only experimental, having experiments only on IWLST 2014 seems quite limited and hard to know if the conclusions will generalize to other datasets/tasks. The paper needs more experiments to sufficiently conclude the advantages of the low rank regularization.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies the model compression for deep neural networks of machine translation. Unlike previous works compress the model after all the models are well-trained, the paper proposes to compress the model using matrix factorization during training. The paper then shows some experimental results. \n\nI agree with the paper that in-training compressing can help prevent performance degrading. However, doing in-training matrix factorization is too time/computational consuming and may not be practical unless some time efficiency results are provided. The paper only compares with pruning and after-training matrix factorization in the experiments. As there are many model compression methods as listed in Sec.5.2, it is better to compare more advanced methods than the pruning method. Finally, I cannot see why the proposed method can only work on translation tasks. I am wondering has any other tasks such as computer vision been tested? What are their results? And why does the proposed method work/not work?\n\nTo summarize, the paper is weak in both technology and experiments. I vote for a reject of the paper.\n"
        }
    ]
}