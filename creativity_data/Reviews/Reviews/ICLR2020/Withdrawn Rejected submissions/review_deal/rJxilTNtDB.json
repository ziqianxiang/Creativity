{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a reverse self-attention on graph neural networks and integrate it with graph-based molecular description. It provides an efficient-automated and target-directed way to estimate the atomic importance without any domain knowledge on chemistry and physics. In particular, it trains graph attention network to predict the molecular property using the molecular network and calculate the atomic importance scores based on the sum of the incoming attention weights.\n\nPros:\nThis paper presents an interesting application of the graph attention network to other scientific areas. And the developed reverse self-attention score, although simple, is useful in estimating the importance of atoms in a molecular network. \n\nCons:\nHowever, the paper also needs some improvement in its evaluation (see detailed comments below).\n\nDetailed comments:\n•\tIn addition to the evaluation metric (7), it is also important to hire human experts to evaluate the performance. For example, just sample some test samples and annotate them by human experts so that they can be compared with the predicted results.  \n•\tIn order to further justify the metric (7), it is also important to report how consistent (7) is with the human evaluation.\n•\tThe method is not compared with any of the previous methods/baselines. At least, it should evaluate how accurate of the proposed automatic ML-based method is when compared to previous (more expensive) human-based method.\n•\tIt would be better to mention earlier what are the molecular property to be predicted by GAT during training. Just a few simple examples would be sufficient.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors propose a method to find predict the atomic importance of an atom in a molecule. In practice this is computed by computing DFT and then using the domain knowledge of human experts. DFT calculation has a huge computation cost and further depends on labor intensive human expertise. In order to solve this problem, the authors represent the molecule as a graph neural network and use it for predicting a property that is related to atomic importance. Then the attention weights are used to determine the importance of each molecule. The authors call this reverse self attention. \n\nI have several problems with this approach.\n\n1. First, the role of attention as a measure of feature importance is debatable. For example, see this paper https://arxiv.org/abs/1906.03731\n\n2. Second, attention is used a measure of feature importance when the true values of feature importance are not available. in this case the author's assume that atomic importance for each atom is available during training (They use this to compute loss) in Eqn 7. If this is the case, why not directly predict Atomic Importance by trying to predict a property  for each node in the molecule graph? That will be a more principled way to do this.\n\n3. The authors don't compare their approach with simple baselines.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "~The authors propose a modification to the attention mechanism to understand the importance of molecular moieties on DFT calculations.~\n\nTo me, it is unclear what the inverse attention mechanism is truly capturing. How does the importance feature correspond to the predicted DFT value? Does the sum of the importances correspond to the predicted DFT value? Can I compare DFTs across different molecules? What is the distribution of importances look like across a given molecule?\n\nGenerally, is there any bias for the valency of the atom and the importance?\n\nFrom “Step 3” on page 5, which layer from the graph neural network do to take the importance features from? Are they aggregated across layers at some point?\n\nFor Section 4.4, please show a plot relating atomic importance to whatever metric you are comparing to.\n\nIn Figures 7 and 8, the selected submolecules are not the same as those in the larger structure. You have added hydrogens and removed double bonds that were present. Please just highlight the molecule as you did in Figures 4-6.\n\nFor section 4.5, I’m not sure what I’m looking for at all for the qualitative analysis, and I have no idea if your figures are representative of the data.\n\nThere are much easier, simpler ways to determine the importance of molecular moieties on predicted outcome WITHOUT using a graph. Please compare to making a fixed length vector via fragments and performing regression or using a random forest. How does your model compare? What about an attention mechanism directly on the SMILES itself?\n\nIt seems like it is hard to get “ground-truth” data to compare to. Does your dataset have molecules that differ by only one, or a few, atoms? If so, how do the importance features compare between those molecules compared to the output prediction?\n\nHow well does your method work out of distribution? What if molecules are held out by Tanimoto distance?\n\nFinally, this work should not be limited in scope to DFT calculations. Determining the importance of features in a graph is of importance in a wide variety of fields, including social networks and pharmaceutical applications.\n"
        }
    ]
}