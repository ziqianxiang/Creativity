{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The proposed regularization method is simple and straightforward. However, many details and equations are not stated clearly. Experiments are performed on small-scale image classification data sets. It will also be more convincing to try larger networks or data sets. More importantly, many recent methods that can train mixed-precision networks are not cited nor compared. Figures 3 and 4 are difficult to interpret, and sensitivity on the new hyper-parameters should be studied. The use of \"best validation accuracy\" as performance metric may not be fair. Finally, writing can be improved. Overall, the proposed idea might have merit, but does not seem to have been developed enough.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The authors propose  an adaptive regularization function that can be pushed to either 2-bit or 3-bit through different parameterization, in order to automatically determine the precision of each layer. Experiments are performed on small-scale image classification data sets MNIST and CIFAR-10.\n\nThe proposed regularization method is simple and straightforward. However, many details are not stated clearly enough for reproduction. E.g, since the proposed regularization already promotes binary or ternary weights, whey is there still a thresholding operation at the end of Section 3? Is it because the proposed regularization can not provide strict binary or ternary weights? Does the method require one more hard binarization/ternarization step after \\beta is learned. Indeed, tan(x) is not well-defined when x=pi/2, and the derivative tan'(x)= 1+tan^2(x) can be large when x is near pi/2, and does gradient descent work well in this case?\n\nThe experiments are only performed on small-scale data sets. Thus it is hard to tell if the proposed method also works for larger networks or data sets? Moreover, it is not fair to use \"best validation accuracy\" for comparison with other methods since the validation set is seen during training and it is not clear if the hyper-parameters of the proposed methods are tuned for best performance on the seen validation set. It would be more fair to compare the test accuracy like in the BinaryConnect (BC) paper. Yet another concern is that many recent methods that can train mixed-precision networks are not compared. For instance, the HAQ method [1] searches for precision for each layer using the reinforcement learning method, how does the proposed method perform when compared with it?\n\n[1]. Wang, Kuan, et al. \"HAQ: Hardware-Aware Automated Quantization with Mixed Precision.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The Paper talks about the Smart Ternary Quantization method that improves the quantization over binary and ternary quantizations by specifying an adaptive quantization. The proposed regularization function is covered in detail and the results are evaluated on MNIST and CIFAR10 datasets\n\nThe authors can improve the submission by \n1. evaluating more modern networks with bigger datasets, as opposed to the ones demonstrated.\n2. describing gamma (eq 10) , it wasn't clear why that parameter was introduced (in addition to the beta) and it's significance, what was more confusing is coverage for this instead of beta in the experimental setup\n3. why the LR needed to be modified for the described method"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents an approach where the regularisation is used to optimise whether each layer of a DNN is binary or ternary. The paper presents an equation for performing this along with two examples of the process in use.\n\nThe paper is inconsistently written with work described at different levels in different sections and has an inconsistent feeling about it. For example the introduction seems to stop abruptly before it describes all the parts of the paper.\n\nThe paper seems to contain an idea which might have merit. However, the idea just does not seem to have been developed enough.\n\nMajor concerns:\n1) The authors claim that this is the first attempt at a training algorithm for mixed precision training. However, a simple google search throws up many papers in this area. Many of which are not mentioned in the related work.\n\n2) Equations are not discussed in enough detail, nor are the parameters defined. Or if they are defined they are done so much later in the work.\n\n3) There doesn’t seem to be enough material here to reproduce the work.\n\n4) In the results you talk about ‘the best’. Given that there has been much criticism over the last two years about good academic practice the fact that you don’t say at least ‘best out of …’ is worrying. \n\n5) You have magic parameters lambda and gamma. You say that these effect the outcome of the work but in your examples you only state values these are set to. One would expect to see at least some analysis of how varying these values effect the outcome. But better would be to show that you have identified good values for both of these parameters. Ideally would be an evaluation of how others could identify the best values.\n\n6) Figures 3 and 4 are difficult to interpret. They need a clear explanation.\n\nSome more generic comments:\n- The abstract seems to assume a huge prior knowledge by the reader.\n\n- ‘1 bit precision’ - precision seems to have no meaning in this context. Surely just ‘1 bit’\n\n- The related work contains a lot of equations, but no real explanation of what they are.\n\n- ‘we let β very per layer’ -> ‘we let β vary per layer’\n\n- In equation 10 what do I and J represent?\n\n- ’28 × 28 gray-scales images’ -> ’28 × 28 gray-scale images’\n\n- ‘For the training session, we pad the sides of the images with 4 pixels, then a 32 × 32 crop is sampled, and flipped horizontally at random.’ - why?\n\n- ‘As commonly practiced’ - by whom?\n\n- ‘which is costly, specially if’ -> ‘which is costly, especially if’\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper discusses a generalization to low bit quantization and combines the approaches of binary and ternary quantization methods. Past methods such as Binary Connect and Binary Weights Network have shown that you can train a network efficiently with 1-bit quantization, and methods such as Ternary Weights Network demonstrate 2-bit quantization with weights taking one of {-1, 0, 1} * mu, with mu being a scale computed per weight tensor. The authors generalize these two methods so that the choice of binary vs ternary weights can be made per layer automatically during training. The primary contribution to make that work is by incorporating a generic regularizer with addition hyper-parameters to trade-off between the binary weight regularizer and ternary weight regularizer. In addition to that, the regularization also includes a prior to make the layers prefer binary weights by default. This is done by adding a cost that penalizes the choice of ternary weights for each layer.\n\nOverall, the paper is well written and explained, with supporting experiments to show on MNIST and on CIFAR10 that this method performs quite competitively compared to an all-binary or all-ternary weights network. Low-bit quantization is an important research area and this paper makes a strong contribution by studying mixed-precision low-bit quantization. The mathematical explanations of the generalized regularizer are well justified. For instance, the regularization constant lambda is normalized for each layer by the total number of weights to evenly weight all layers.\n\nAlthough the experiments cover MNIST and CIFAR10, it's not clear how mixed-precision low-bit methods perform on models more prevalent in the real-world. Supporting the experiments with ResNet variants on ImageNet would help clarify that further.  The paper very well explains the fundamentals of 1-bit and 2-bit methods, and the contribution of this paper (generalization of these two methods) is a somewhat natural extension without significant novelty. Moreover, in addition to the accuracy, it would also be better to understand how the run-time performance of such models (on existing software and hardware implementations) compares to pure binary and ternary networks, as knowledge of that would reveal insights into systems optimizations to be made in future work in this field. Given all of this, my rating is a weak accept.\n\nPros:\n- The problem and the fundamentals (prior work) are very well laid out.\n- The regularization component that generalizes the two types of quantization is sound.\n- Experiments on CIFAR10 strongly show improved accuracy and higher compression ratio compared to ternary weights network.\n\nCons:\n- Lacking more realistic experiments on larger datasets such as ImageNet and models like ResNets.\n- The runtime performance of how STQ compares to 1-bit and 2-bit quantization variants isn't shown.\n- The regularizer introduces more hyper-parameters to tweak and it's not clear how sensitive these are to the choice of the architecture. Different values of gamma are used in the two experiments, and further analysis on how the performance varies for various values of lambda and gamma would shed further insight.\n\nMinor comments:\n- In equation (3), the term under argmin should be mu and not alpha.\n- Section 3, line above equation (9) reads \"very per layer\" instead of \"vary per layer\""
        }
    ]
}