{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "####\nA. Summarize what the paper claims to do/contribute. Be positive and generous.\n####\n\nThe paper looks at ways to rank the importance of filter channels in a convolutional network. It proposes two ways to rank the importance of filter channels: (1) - the Shapley Value which quantifies the value of a filter by marginalising out the performance of all subnetworks (subsets of filters) and averaging the difference between the performance of a subnetwork with and without the filter in question. The second (2) uses a latent variable model and variational inference. In the latent variable model, each filter is scaled by a positive coefficient (latent random variable), and the coefficients within a layer are normalized to sum to 1. Thus the given layer's coefficients are a random probability vector. A dirichlet prior is adopted over each layer's coefficients and variational inference is used to learn a dirichlet posterior. Both methods score a neuron relative to other neurons in a layer.\n\nThe paper's experiments looks at which neurons are deemed important by these two ways of quantifying it. It uses these two methods to prune some neurons. The experiments seem to show that pruning neurons according to one of these rankings beats a baseline of pruning according to L1/L2 magnitude of the weights connected to that filter.\n\nAnd it also uses the methods to look at filters deemed relatively important and unimportant (high/low ranking), in an effort to use these neuron importance rankings for model interpretability.\n\n####\nB. Clearly state your decision (accept or reject) with one or two key reasons for this choice.\n####\nI vote to reject the paper in its current state. \n\nB1: The methods are not explained well enough.\nB2: The experiments are not explained clearly enough.\nB3: The experiments do not adequately test the claims or modeling decisions, nor are they justified theoretically.\nB4: Some concerns about the correctness of the method and implementation.\nB5: The Shapley Value is completely not scalable.\n\nThe Shapley Value sounds like a reasonable way to measure the importance of neurons, but it is ridiculously not scalable (it requires evaluating the accuracy of each network in the set of all subnetworks, i.e. N! different networks if N is the number of convolutional filters). The approximation proposed is not justified or evaluated experimentally, and anyway, the approximation is still not scalable (requiring arbitrary k! instead of N!, where k < N), so only tiny networks are used and only subsets of size k=25 are considered for these tiny networks.\n\nThe latent variable model also sounds reasonable, but it is trained in an extremely unprincipled way which is again not justified theoretically or experimentally. Instead of using stochastic gradients to approximate the ELBO, the analytic mean of the variational posterior is used. This can't really be called variational inference, and the quality of this approximation is not evaluated anywhere.\n\nThe experiments do not have well-explained methods or baselines so I have really no idea what to make of the numbers. The baselines are chosen seemingly arbitrarily to be tiny models, and the methods in the tables are not described other than with acronyms without reference.\n\n####\nC. Provide supporting arguments for the reasons for the decision.\n####\nB1: How is the characteristic function of a coalition measured? Do you prune away neurons outside the coalition and evaluate the accuracy on the full validation set?\nB2: What value of alpha_0 is chosen, the parameter of the dirichlet prior? Is there any weighting on the KL divergence? These must be crucial to reproducing the paper's results.\nB2: What are the other entries in Table 2? It's not explained anywhere. In fact the figure captions don't say anything at all in Figures 3/4 aside from what dataset is being looked at.\nB3: It is claimed the paper confirms that some neurons are more important than others. But it doesn't verify this by showing that e.g. the variational bound is improved by including this prior (versus a dirichlet which generates uniform distributions over the importance switches). It only shows an example in Figure 1 but it is not clear how this example is chosen.\nB3: No metric is used to compare the Shapley Value approach to the Importance Switch approach, aside from showing that they turn up similar rankings of importance for a toy network.\nB3: Using the posterior mean for q.\nB3/B5: Using only k=25 for computing the Shapley value. Modern neural nets have many millions of neurons. If you can only compute this quantity on randomly sampled subsets of 25 it doesn't bode well. At the very least this should be justified by comparing some metric for k=5, k=10, k=20, k=25, etc etc or something and showing that going up to 25 doesn't help. But even this would be deeply unsatisfactory.\nB4: The fact that neurons with an index > 9 are deemed insignificant by both methods, as mentioned in Section 4, is deeply troubling, and suggests a bug in the implementation.\nB4: Why bother talking about permutations when leading up to equations 1/2 when the characteristic function of a coalition is invariant to the ordering?\n\n####\nD. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n####\nThere are a number of references which should be included for (1) interpretability, (2) pruning and/or a bayesian treatment of the importance of neurons.\n\n(1) interpretability:\nOlah et al. - https://distill.pub/2018/building-blocks/ and other references therein\nKim et al. - https://arxiv.org/abs/1711.11279\n\n(2) pruning/bayesian treatment of importance of neurons\nGal et al. - https://arxiv.org/abs/1506.02142\nKingma et al work on variational dropout - https://arxiv.org/abs/1506.02557\n\nNits: \nLarge sections of the document could have the writing improved for conciseness, grammar, and word choice.\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors define two new methods for ranking the influence of individual units on the output of convolutional networks. They show that some overlap exists between the units chosen by these two methods, they use these methods to select filters to visualize, and they use one of the methods to prune/compress trained convolutional networks. Their compression method outperforms prior work.\n\nPros\nThe neuron ranking method produces compressed models that compare favorably with prior work (Table 2).\n\nCons\nI found the clarity of this paper overall to be somewhat poor. These points especially need to be addressed\n\nThe acronyms for the baselines in table 2 are not defined or cited.\nLack of clarity about the pruning experiment: Why is the Shapley value method not included here? I'm assuming NR stands for neuron ranking.\nLack of clarity about visualizations: \nAre the rankings used to choose filters to visualize from Shapley values or neuron ranking? The units [1,8,3,7] don't seem to be seem to all be selected by either method for MNIST conv1 in Table 1. \n\nI wasn't convinced by the feature map visualization section, because as far as I can tell the unimportant nodes don't look very different from the important nodes. Specifically, in both the important and unimportant node feature maps, a class example is clearly visible. Perhaps the authors could clarify what the critical difference is between these feature maps.\n\nThe neuron ranking method seems to give only a marginal advantage over simple magnitude pruning.\n\nOverall\nThe compression results seem strong in comparison to past work, but marginal in comparison to the simple baselines. The feature map visualizations were not compelling, and may require clarification.\n\nComments\n\"the deeper architecture also become wider\" -- I'm not sure what this means.\n\n\"the units in the network (both convolutional filters and nodes in fully connected layers) are not equally important when it comes to performing an inference task.\" -- Morcos et al., 2018 seems relevant https://openreview.net/pdf?id=r1iuQjxCZ.\n\n\"We visualize the most significant features which significantly show the significance of repeated and complementary features\" -- I didn't quite understand this.\n\nTypos\ngaining insight what the CNN -> gaining insight about what the CNN\nfilter rankingse -> filter rankings\nin the the case\non the other and\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper proposes two approaches to quantify neuron importance in CNNs. The first method based on Shapley value computes the marginal contribution of each filter; and the second approach uses probabilistic variational inference. The algorithmic contribution is demonstrated with a suite of experiments on MNIST and Fashion-MNIST, using LeNet and VGG architectures. \n\nThis paper should be rejected primarily because (1) the algorithms are not well justified either by theory or practice; (2) the two approaches described are loosely connected and therefore making the paper lack of focus; and (3) the experiments lack comparison with many other existing state-of-the-art neural compression methods. \n\nSpecifically, this paper lacks theoretical and/or empirical justification on why individual neuron’s contribution during the learning process can be characterized equivalently as a coalitional game. How is the characteristic function chosen in practice? Does this depend on the neural network architecture in use? Further experiments should be provided to show ablation on this. The authors should also consider reporting the measurement in time for computing the Shapley value, in order to justify the computation feasibility. How well does the approximated solution compared to the optimal solution? What’s the time and performance tradeoff? \n\nIn the experimental section, the author should consider providing details on how to choose the number of channels (for different layers) to prune based on the ranking. How does the pruning strategy affect the model performance?\n\nThis paper can be strengthened by comparing with existing state-of-the-art compression methods such as knowledge distillation (Hinton et al), SqueezeNet, ShuffleNet and other quantization based methods. \n\n\n\n\n"
        }
    ]
}