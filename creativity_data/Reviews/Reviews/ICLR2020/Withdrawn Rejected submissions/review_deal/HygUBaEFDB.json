{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "*Review\nIn this paper, the authors proposed a new framework to jointly learn multiple tasks from heterogeneous sources. Most existing multitask learning frameworks require the input data to come from the same source. To overcome the limitation of those multi-task learning frameworks, the authors proposed to use tensor ring decomposition to decompose the weights of each layer into a sequence of 3rd-order latent cores. Then, some cores were selected to be shared for different tasks. The other cores were the task-specific cores that capture the task-specific features. The number of shared cores could be different for each layer. Experiments were done on multiple datasets.\n\nDetail comments:\n\n1. The writing of the paper is not good. The methodology needs to be more detailed. It is hard to follow. \n\n2. The novelty of the paper is not enough for an ICLR paper. It just applied an existing tensor decomposition method to multitask deep neural networks.\n\n2. In the first experiment, the authors divided the MNIST into two tasks. The first one was to classify the odd digits. The second one was to classify the even ones. I didn't see the reason for splitting MNIST into those two tasks. \n \n3. In figure 5, the authors only provided the performance of the difficult task and the average performance. Please add the performance of the easy task to show how much the performance drops for the easy task. Also, please report the performance on task C for monoglot dataset for 4.2.3.\n\n4. The setting can also be seen as transfer learning between different domains. In the experiments, the authors addressed the difficulty tasks' performance was enhanced. This is the contribution of the proposed method. However, we could see from the experiments that the performance of the easy task dropped. Why not compare with the transfer learning methods that do not affect the performance of the source domain? If the proposed method cannot get better performance compared with those transfer learning methods for difficult tasks, the existing transfer learning methods would be good enough to deal with the settings in this paper.\n\n5. Some baselines are missing. There is no single task performance shown in Figure 4. \n\n6. For 4.2.4, the authors just compared the results of their proposed framework under different settings. Please also provide the performance of other baselines to prove the proposed method is better than others."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose a tensor based approach to multi-task learning. The layers of a deep neural networks are parameterized with a low-rank tensor decomposition (tensor ring). Some of the cores are shared between all tasks, while others are task specific.\n\nHowever, the novelty is low as this has been already well studied, and the novelty is limited to the use of a tensor-ring decomposition instead of tensor-train or Tucker decomposition.\nWhile the use of tensor-ring instead of tensor-train is intuitively a good idea, since it removes the boundary conditions, it is not enough.\nOverall the paper feels a little \"all over the place\" with several small parts, not well connected, or justified.\nIt would be more convincing to focus on a few established experimental scenarios and demonstrate better performance on these.\nThe best such scenario is probably the Visual Domain Decathlon [1], which would allow to both evaluate the method and rigorously compare it with previous works.\n\nThe paper is relatively hard to read. The use of summation based notation for tensor operations (e.g. in 3.1) makes it unnecessarily cumbersome. \nThe paper could be made much clearer by reorganising it and improving the writing. In particular, the contributions are not made clear enough by the authors.\nThe sharing of the cores seems somewhat arbitrary and not grounded in any theory. How is this chosen? Note that this adds a significant parameter to optimize.\n\nThe experimental setting is not convincing. In particular, MNIST and Omniglot do not demonstrate any advantage of the proposed method. \nThe experiments on UFC11 are not informative as the proposed method is not compared with any state-of-the-art method or even baseline.\nRather than hand picking datasets, it would be more convincing to test on an established benchmark for multi-task learning (e.g. [1]).\n\nThe proposed methodology should be compared with other similar, recent methods (e.g. tensor based \"Incremental multi-domain learning with network latent tensor factorization\", or SVD based, \"Learning multiple visual domains with residual adapters\").\nTable 3 and 5 could be moved to appendix and replaced with a table of results comparing the proposed approach to existing methodologies.                                                                    \n\nHow are the various hyper-parameters of the models chosen in practice? e.g. the choice of shared factors, rank of the decomposition, etc? In general, the paper is not easily reproducible due to the lack of implementation details.\n\n\n[1] Learning multiple visual domains with residual adapters, NeurIPS 2017.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper applies tensor ring factorization to deep multi-task learning (DMTL) and shows that it leads to improved performance over existing DMTL factorization methods, more compressed representations, and the ability to share structure over more heterogeneous settings.\n\nHowever, the experiments do not clearly demonstrate the advantage of heterogeneity and consider mainly non-standard MTL settings. If these drawbacks were addressed, the paper could be quite strong. \n\nThe opening of the abstract discusses applications to diverse architectures and data modalities, but the experiments are limited to very similar image classification models. Could simply resizing the smaller image to the size of the larger images and using the same model work just as well as having heterogeneous models? Demonstrating sharing across more diverse settings, e.g., along the lines of the framing in [1], would make the contribution more significant. Can TRMTL be applied to share across any set of architectures?\n\nFor the experiments in the paper, it is also not clear why each architecture is suitable for each experiment. Some of them appear to be oversized models TRMTL can most effectively shrink.  E.g., the layer size of (23,328x256 FC) seems quite large for the Omniglot CNN, and the comparison methods in Table 1 look overparameterized. In each experiment, are the architectures standard architectures? If not, can we expect the results to translate to standard architectures?\n\nSimilarly, is there a reason MRN is missing from the Office-Home experiment? Also, Office-Home is usually validated on 5%, 10% and 20% training data. Is there a reason it is run on 50% to 80% here? Since Office-Home is the most standard benchmark used in the paper, it would be useful to have a table of detailed results (similar to the MRN paper), at least in the Appendix. \n\nOverall, the approach seems promising, but is missing some experiments and explanation that would validate its innovations.\n\n[1] Meyerson, E. & Miikkulainen R. “Modular Universal Reparameterization: Deep Multi-task Learning across Diverse Domains”, NeurIPS 2019.\n"
        }
    ]
}