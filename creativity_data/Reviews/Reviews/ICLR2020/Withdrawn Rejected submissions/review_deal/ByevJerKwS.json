{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this work, public data is used to bootstrap a model inversion attack. Specifically, public data is used to help train a generative model, which is used to generate images that reconstruct the average representation of a class that was learned by a victim target model. \n\nMembership inference is not a violation of individual training point privacy, it reconstructs an average representation of the class. This can violate privacy in some limited cases where attributes of the dataset were not known previously (e.g., the fact that points from a particular class possess in general a certain feature), but it does not violate the privacy of individuals whose data is used to train the model. This explains why differential privacy does not defend against model inversion (as observed in Section 4.3.6): model inversion is not an attack against data privacy as defined by differential privacy.\n\nThe threat model could be described more clearly in Section 2.1: what does “having access” mean? Can the adversary observe labels and predictions? From Equation 3, it looks like the ability to query the full model’s output is required to compute C(G(z)). Furthermore, is it realistic to assume that the adversary runs an optimization process with thousands of iterations for each attack run? (See page 6) If this strategy improves upon previous instantiations of the model inversion attack, this would be helpful to measure and report in experimental results. It is a proxy for the adversary’s cost. \n\nThe experimental protocol considers a best-case scenario when the adversary has access to data from the same exact distribution (because public data is obtained by partitioning a dataset into a training and public subsets). This does not correspond to the scenario described earlier, where the adversary accesses public data by searching for images on the internet for instance. The approach will likely perform worse when the distributions used to train the target model and the generator mismatch, as indicated by preliminary results in Section 4.3.2. Expanding these experiments to characterize what knowledge of the distribution is needed for the attack to succeed would help address this limitation. \n\nEditorial comments:\n\nP2 one dominate \n\nP2: unnecessarily emphatic language at the end of Section 2.1\n\nP3: Why choose the Wasserstein-GAN?\n\nP7: Please don’t make assumptions about the gender of adversaries."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposed a GAN-based model inversion attack method. Moreover, the authors present a theorem that a well-trained model would be more vulnerable to model inversion attack. This paper finds a practical and meaningful problem, model inversion attack, of deep learning. However, the novelty of the paper is limited, and the comparison with baselines in the experiments section is unfair. \n\nStrong points of the paper:\n1) The well-organized structure.\n2) Complete experiments. \n\nWeak points of the paper:\n1) The comparison of baselines is unfair. The EMI[1] does not require the auxiliary knowledge, i.e., the blurred input image as an additional input to the adversary. The EMI only requires the label and confidence score information to generate the output image. As shown in figure 3, it makes sense that EMI's output is more blurred than the proposed GMI. Furthermore, the distance in table 1 shows that EMI is not much worse than the proposed GMI since it does not have the blurred input images. Similar to the EMI, the baseline PII does not have the additional confidence score of the target model. It is quite unfair to compare these the method since they do not receive the same input information. Lastly, please cite and indicate what image inpainting method is selected as the baseline. \n2) The proposed theorem 1 is another significant contribution of this paper. However, the proof of theorem 1 (noted as theorem 2 in appendix) is not convincing. If the insensitive component of the image is very few, and it could be ignored, then the proof is just the objective of the classification model. The proof would be very similar to the maximum a posterior estimation. That gives minimal theoretical insights. If the authors could prove theoretically or empirically that the insensitive component exists and is as much as the blurred background. The proof would be more meaningful. \n3) The authors seem to be very rush in submitting the work. The dropbox link is not anonymous; the abstract is not well-formatted in openreview, and some notation is not correct. \n\nOverall, the paper is far from being published. \n\n[1]M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322–1333. ACM, 2015.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Overall:\n\nInteresting take on the MI problem, which in fact reduces it to a two stage GAN problem where for the second stage the discriminator loss in fact integrates a component due to the model it tries to attack. The analysis is essentially experimental but seems to display that the technique does fulfil its promises.\n\n\nQuestions:\n\n* Is the attack sensitive to the bias of the data used to train the classifier ? For example, if the classifier has been trained using western faces and the attack consists of an eastern face with one eye removed, is this eye going to be biased towards the “western” model of the training data ?\n\n* I have had a quick look at the proof of Theorem 1. Isn’t the Theorem an equivalence instead of just an implication ? Also I believe its writing is a bit approximative: what seems to hold (for just one implication) is in fact \\forall label y, IF U >= U THEN SKL >= SKL. Otherwise there is lacking a \\forall label at the end of the statement for the SKL part I suspect.\n\n* The paper assumes that the logistic loss is being used to train the NN model and then does the MI attack using a loss (Lid) that is of the same family. This implies therefore that the loss used is public knowledge, which seems to be a too stringent assumption. This begs the question: did the author try a different loss to train the model and yet used the Lid loss to do the MI attack ? What did happen ?\n\n* Is it possible to put the “latent vector” you search for (Sec 2.2) in Figure 2, so we know exactly where this vector stands in the problem ?\n\n* Assuming my description of the problem above is right (Overall), I suspect that some formal guarantees should / could be obtained on reconstruction provided the problem of MI attack is in fact folded in the training of the GAN. If it is too costly, then perhaps let us assume that we have not one but a set of images with missing parts for MI attack (call it the “hole” data). Assuming we can constrain the support of the generated distribution to match exactly or approximately that of the “hole” data, we can probably get GAN type guarantees on the reconstruction. Since after all, the GAN used is primarily used to stage the MI attack, did the authors try to fold both problems (GAN training, MI attack) into one ?\n\n* In the experiments, did the authors try the MI attack with random masks instead of just the “square + T” ?\n\n* I believe there is a misunderstanding on the DP experiments. DP just guarantees that the model learned would be approximately the same if we changed one training data example. This does not guarantee the non reconstruction (approximately, depending on epsilon) of an arbitrary image, *even* if it were part of the training data. You are therefore not attacking DP models-as-in-DP-aimed-at-protecting-against-your-attack, but just showcasing that your technique also manages some reconstruction on those models. The Abstract sentence is highly misleading and the whole paragraph deserves a good polish from this standpoint.\n\nTypos (?)\n\nIn (3), Lprior depends on a variable which does not appear on the RHS. Similarly for the Lid loss the argument is missing in (3).\n\nAlso, the font of Lid is different from the one in the text two lines above\n"
        }
    ]
}