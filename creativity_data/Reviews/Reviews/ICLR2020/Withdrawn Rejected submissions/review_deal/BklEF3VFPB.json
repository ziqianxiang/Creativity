{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.\n\nReviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.\n\nOverall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\n###Summary###\nThis paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. \n\nThe motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. \n\nIn the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. \n\nThe paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. \n\nThe paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. \n\n### Novelty ###\n\nThe model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  \n\n###Clarity###\n\nOverall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. \n\n###Pros###\n\n1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. \n2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. \n\n3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. \n\n###Cons###\n\n\n1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. \n\nIt will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:\nDomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/\nOffice-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/\n2) The organization and presentation of this paper should be polished.\n\nBased on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.\n\nPros:\n- The writing is good\n- Satisfactory empirical results\n\nCons:\n- The proposed method is very similar to certain methods in the literature\n\nDetail comments:\n(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.\n\n(2) Experiments\n- How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.\n- What does the * in DRCN* mean in Table 1?\n- ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.\n\n(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.\n\nTypos:\n- In Eq.(1), there is a missing D in the first term. D_s should be \\mathcal{D}_s to match previous notation.\n- In Eq.(2), the \"0,\" is not meaningful given the definition of []^+.\n\nRefs\n- Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.\n- Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.\n- Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.\n\n# Update after rebuttal\n\nThank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are \"totally different\" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposed a new method for unsupervised domain adaptation. Different from a conventional domain classifier based adaptation, they propose to utilize the loss of autoencoder to extract domain-invariant features. They trained reconstruction network to reconstruct source examples well whereas making reconstruction loss of the target examples large with some margin. Their goal is to stabilize the training of adversarial training for domain adaptation, incorporate pixel-level information, and give interpretable learned feature space. They performed experiments on digits datasets and WiFi Gesture Recognition datasets. Through experiments, they have shown that their method shows better performance than baseline methods and their method is not parameter-sensitive, is stable and provides interpretable adaptation results. \n\nI think their method is interesting and motivation is important. However, their experimental results are not convincing enough. \nFirst, they did not compare their method with recent state-of-the-art methods.  For example, there are classifier's discrepancy based adversarial learning method, Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\". In addition, they did not compare with \"A dirt-t approach to unsupervised domain\nadaptation\", which they cited in the paper. I think their method is for stable and interpretable adversarial learning. So, it does not have to outperform other methods in accuracy. However, they need to show some superiority over these representative adversarial methods. \nSecond, their experiment is only on digits and WIFI datasets. Is the method effective for object recognition datasets, such as Office or OfficeHome? This is an important question to be addressed because the two datasets are benchmark domain adaptation dataset and the behavior on this dataset will show how this method is applicable to various datasets. I would say that the method does not have to outperform state-of-the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. \n\nIn addition, this method seems to have clear connection with \"Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\" arXiv preprint arXiv:1609.03126 (2016)\". They need to add this paper to a reference  and explain some connections. \n\nTo sum up, due to the two questions listed above, I think this paper is marginally below the acceptance threshold. Please respond to the questions.\n "
        }
    ]
}