{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a Neural Networks based algorithm for the two-sample test problem, which is one of the most fundamental problem in statistics and machine learning. Given independent samples from two distributions p and q, the goal is to tell whether p = q or p = q. The test is usually based on some test statistic T and a threshold T_0 where Pr(T > T_0 | p = q) < a (a is the tolerance for the false alarm rate and usually set to be 0.05 for statistical significance). The decision rule is simply testing whether T > T_0 holds. \n\nIn this paper, the authors propose a test statistic based on the difference of the sums of output logit s(the difference between the output neurons of the last layer) of a trained neural network classifier on both sample sets. The training of the network is done using half of the samples and the threshold is set using a permutation test on the training set. The final decision is based on the outputs of the network on the other half of the data (test set). The paper shows that the test is consistent asymptotically if the network provides a good approximation for the JL divergence between the two distributions, which connects recent advances in network approximation theory.\n\nIn the experiment part, the paper considers synthetic experiments using 1-d and 2-d Gaussian distribution and real-data experiments using the MNIST dataset to compare the proposed method with other methods including net-acc (using the difference in the output probabilities) and variance kinds of Gaussian kernel MMD methods. \n\nThe paper presents some nice ideas, but overall I find the theory part not deep and fail to provide enough insights. In the training phase of the network, not only the number of the parameters in the network matters, but the number of training samples is also crucial for the network to approximate the optimal function. It would be nice to have some discussions on this. \n\nI also find the experiment part not convincing. In the experiment part, I think the fairest comparison to Gaussian kernel MMD is to use gmmd+ and gmmd++. For the 1-d Gaussian case (figure 2), the experiments show that the proposed test is not always better than these two in some distribution cases. In the 2-d case, the experiment is only done for one distribution. For real dataset (MNIST, figure 5),  results for gmmd+ and gmmd++ are not presented. I would expect more thorough experiments on different distributions to make a more convincing statement. \n\nSo overall I would not recommend this paper for admission.\n\nMinor comments:\nSection 2.2, line 3: y_i's should have label 1."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposes using a neural network to perform the two-sample test to distinguish if two distributions are different from each other.\n\nThe idea is not very different from the discriminator network used in GANs where a classifier learns to distinguish between two distributions. The threshold is computed using the permutation test, and I wonder how the logit test compares to a simple permutation test.\n\nThe authors show that the logit method has high power when the distributions are different. How do the results look if the distributions are the same, and yet the classifier is trained with (fake) 0-1 labels. Would the net logit method do a better job in predicting that the two distributions are the same as compared to other methods?"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "# ==== Summary of the paper ====\n\nGiven two samples (two sets) $X = \\{x_1, \\ldots, x_n \\} \\sim p$ and $Y = \\{y_1, \\ldots, y_n\\} \\sim q$, two-sample testing seeks to decide whether $p=q$ using only the samples $X$ and $Y$. This is a long-standing problem in statistics and machine learning. Many papers have addressed this problem. Among these, some approaches that are relevant here are: 1. Maximum Mean Discrepancy (MMD, Gretton et al., 2012), 2. Classifier two-sample test (net-acc, Lopez-Paz & Oquab, 2016) which sees two-sample testing as a binary classification problem.\n\nThis paper proposes a new test statistic for two-sample testing. It is closely related to net-acc. The proposal is: \n\n1. Train a classifier network with the softmax loss (or Bernoulli log likelihood)\n2. Consider the two outputs in the last layer before the softmax (i.e., the logits). Call these u(x) and v(x) (corresponding to the two classes). Define $f(x) := u(x) - v(x)$.\n3. Define the test statistic to be (see Eq 1) $\\hat{T}$ = empirical mean of f wrt X - empirical mean of f wrt Y.\n\n\nThe paper contributes a number of interesting results:\n\n1. Theorem 4.1. Briefly it states that (only the main message), if p!=q, then there is a neural network architecture such that the resulting test will eventually decide that p!=q (i.e., reject the null hypothesis H0: p=q), as the sample size n goes to infinity.\n\n2. An extension of Theorem 4.1 to the case where the distributions p,q are supported on a manifold. \n\nThe paper concludes with simulation studies on toy low-dimensional problems  (Section 5.1), a 2d problem on a sphere (manifold), and a real problem with MNIST (distinguishing real digits from a mixture of real and generated digits). \n\n\n# ==== Review ====\n\nThe paper is well written overall. One main message from the paper is that the new statistic is good for detecting the difference in the tails of p and q.  Under this setting, the paper shows with a 1d toy example that the new test has higher power than the MMD with a Gaussian kernel (Section 3). I think this is an interesting point and should be emphasized more. The proposed approach is most closely related to net-acc (Lopez-Paz & Oquab, 2016) in the sense that both see the two-sample problem as a binary classification problem, and define a test statistic based on a trained neural network classifier. The main difference is that net-acc uses the classification loss as the statistic, making it less sensitive to small changes between p and q, compared to what is proposed in the present paper. The presented theoretical result (Theorem 4.1) is interesting. It is rare to see this kind of \"approximability\" result in the context of two-sample testing i.e., by this, I refer to the statement \"there exists a network with O(..) many parameters that can detect the differences.\"  Having said that, the proof is largely based on the work of Yarotsky, 2018.  Still, I feel that the present paper has put together nice theoretical results.\n\nI also would like to praise this paper for its honesty. I appreciate that the paper comments on when the proposed method works well (i.e., difference in the tail), and when it does not (e.g., mean shift, or variance difference in Figure 2), without overselling it. The paper also directly mentions that the comparison in Figure 1 is unfair to MMD, which is true. I reassure the authors that doing this only had positive impact on the evaluation, and encourage the authors to keep this practice. Well done.  Having said that, I do have some concerns. I hope that the following points can be addressed. \n\n# ==== Major comments, questions ====\n\n1. The work \"empirical density\" is used a few times. Do you mean empirical measures? \n\n2. According to Section 2.3, the null distribution is simulated by permutation. Do you need to retrain the network for each permutation? If so, this would be quite expensive. Also in the 3rd line, \"... recompute the test statistics for $m_{perm}$ times, typically a few tens.\", I don't think that is enough. I expect at least a few hundreds.\n\n3. Major comment: As noted in the paper that the proposed approach is unstable (to train) when the sample size is small. The proposed approach is good when the difference is in the tail area. I really think this point should be studied more. It would be nice to have more theory to justify this point. Alternatively, one can also consider a simulation study with large n (to make training more stable), and consider p,q that differ in the tail areas. Also consider D (input dimension) much larger than 1. This will make the paper stronger.\n\n4. Why is there no loss of generality to consider the unit ball as the domain (in Section 4.2)?\n\n5. The setting of the 2d manifold experiment is unclear. Write the definitions of p and q. It is unclear what \"density departure\" means here.  Also, why is the proposed net-logit good for this problem?\n\n6. Corollary 4.2: This is a \"semi\" asymptotic statement, isn't it? Is it correct that the power expression on the right hand side is only approximate (for large n)?\n\n7. Section 5.1: for gmmd-ad, why did you choose to maximize the kernel MMD (for choosing the right Gaussian bandwidth)? Isn't it more principled to maximize the test power as done in Sutherland et al., 2016? See Gretton et al., 2012b (in your citation list) for an empirical result that compares the two ways for choosing the kernel bandwidth. They showed that maximizing the test power gives better test power. Note: Gretton et al., 2012b considers a weighted sum of MMDs which is slightly different. But still, it is worth trying maximizing the test power.\n\n8. Section 5.2: is it correct that gmmd uses a Gaussian kernel on the raw pixels of MNIST? If so, the poor test power of gmmd, gmmd-ad is not surprising. This is to do with a wrong kernel rather than the statistic itself. More interestingly, why does net-logit (proposed) have slightly higher test power than net-acc in this case? I understand that both use the same network.\n\nWilling to reconsider my evaluation after checking the authors' responses.\n\n\n# ==== Minor. Did not affect the score ====\n\n* In the intro: \"... often restricted to data of small dimensionality and/or small sample size, or certain specific classes of densities ....\". This sentence is a bit vague. MMD is a metric if used with a characteristic kernel.\n\n* Intro, 2nd paragraph: \"... distinguish the model density q produced by a generative network ...\" GAN models in general do not have a tractable density function available. For flow-based models, yes. But the cited papers in this sentence do not study flow-based models (normalizing flows).\n\n* I would move section 1.1 (related works) to be after Section 3. Better get into the main section quickly. But just a suggestion. It is up to the authors.\n\n* Section 1.1: \"...Compared to kernel methods, neural networks are algorithmically more efficient ...\", I would avoid saying this since kernel methods cover a broad class of models/algorithms, depending on the kernel. For a finite-dimensional kernel (say, defined as the inner product of the outputs of a network), the MMD can be computed in O(n).\n\n* Section 2.2: (yj, 0) -> (yj, 1) \n\n* Throughout the paper: gaussian -> Gaussian. \n\n* Section 3: eq 2, write p and q. Also in Figure 1, put a legend for p and q in the first subfigure (p=blue, q=red).\n\n* After eq 2: \"difference of the densities\" -> \"mixing proportion\".\n\n* Section 4.2: \"The above analysis is for general p and q...\". -> \"...for general compactly supported $p,q \\in C^2(\\Omega)$.\n\n\n"
        }
    ]
}