{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper builds on the hyperspherical energy minimization line of research and proposes that instead of optimizing the distance between actual filters (or network parameters) it optimizes the distance between a projection of the filters (or network parameters). The idea is quite straightforward. Nevertheless, I do not fully understand the rationale behind it. Also, the developed theory is a straightforward application of well-known theorems and lemmas of random projections. There is a variety of experiments showing negligible performance improvement. \n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper extends previous work on using a hyperspherical energy function for regularizing neural networks, an apporach called minimum hyperspherical energy (MHE). The authors point out several problems with optimizing the original MHE objective in high dimensions and propose the compressive minium hyperspherical energy (CoMHE) to address these problems. The main idea is to project the network weights to a lower-dimensional space and then apply the original MHE approach to the projected weights. The paper proposes numerous different ways to perform these projections, where the two main variants are based on random projects and angle preserving projections. The paper provides a number of theoretical results, showing that these projections approximately preserve the angles between pairs of weight vectors. The paper has a comprehensive experimental section, where the authors empirically evaluate the different variants of CoMHE, and they compare CoMHE based models to state-of-the-art models on CIFAR-10/100 and ImageNet. Most considerable, with a plain 9-layer CNN they obtain close to a 2 percentage point improvement over ResNet-1001 on CIFAR-100.\n\nOn the positive side, the paper is well written and easy to follow. The paper both investigates the theoretical aspects of the proposed methodology and has comprehensives empirical evaluation. In particular, many of the reported results are comparable to the state-of-the-art, showing that this is effective regularization approach that gives good generalization.\n\nThe main shortcoming of the papers is that while it contains many new methodological contributions, the paper appears to be somewhat incremental work. Furthermore, while the experimental results are comprehensive, it appears that only for figure 1, multiple training-runs have been performed. When standard deviation over multiple runs is not reported, it makes it hard to draw conclusions from the reported results. For instance, are there any significant differences between the results reported in table 2, or does the table show that the methods are not sensitive to the dimension of the projection?\n\nGiven these shortcomings, I recommend a weak reject of the paper.\n\nTo clarify my understanding of the paper, I would like the authors to answer the following questions:\n\n(1) How can you conclude that your empirical results do not show any noticeable performance gain with using multiple projects for AP-CoMHE when you only report the result for one project in table 3?\n\n(2) In table 4, I do not understand why you did not increase t even further.  When do you start to see overfitting? Could the baseline archive the same performance as the other models with a larger t?\n\n(3) On the top of page 2, you write \"Third, when the number of neurons is smaller than the dimension of the space (it is often the case in neural networks), MHE defines an ill-posed problem because the neurons can not even fully occupy the space.\" I do not understand which \"space\" you refer to here. Could you please clarify this?"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper presents a method for regularizing neural network weights which encourages diversity in a layer’s activations. The regularizer is motivated by encouraging the weight vectors to be uniformly distributed on the hyper-sphere. This is a challenging optimization problem which can get stuck in many local minima. The authors propose a way to avoid many of the known issues with this regularizer by projecting the weight vectors down to a lower dimension where the optimization problem is less susceptible to getting stuck in a local minima. The authors propose multiple ways to project the weight vectors to lower dimensions with random and learned projections. The authors motivate the validity of their approach by providing some theoretical guarantees that by minimizing the hyper-spherical energy (HSE) in the projected space, they are minimizing the HSE in the weight space. Further, the authors explore each approach and compare their proposed hyper-spherical regularizer to prior work and find it performs favorably. \n\nWhile I am not an expert in this specific area, I would vote overall to reject this paper.  While the authors present a through comparison of their method to other hyper-spherical regularization methods, I felt that there was an insufficient comparison to other regularization methods. I do not feel the experimental results demonstrated the benefit of the author’s approach over other HSE regularization methods and more traditional forms of regularization. \n\nI feel the paper could be greatly improved if the authors provided a comparison with more common regularizers. They provide a convincing demonstration that their method performs better than other HSE-based regularizers, but they do not provide much of a reference for how these methods compare in general to other types of weight regularization, so it is hard to tell from the results here that this improvement is significant. \n\nA main argument for the use of hyper-spherical regularization methods is their generality to be used across domains and while the authors did show this in image and point-cloud classification, their comparison was limited only to other hyper-spherical regularization methods, so again it is unclear how the proposed method compares to simpler regularizers such as a well-tuned l1/l2 or orthogonality regularizer. \n\nSpecifically on the CIFAR10/CIFAR100 datasets, the difference in performance between the reported HSE methods is less than the difference in performance observed between minor architecture changes in the Wide Residual Networks paper (a strong model for these problems), so I was not convinced of the benefit of this method of regularization as opposed to minor architectural or hyper-parameter tweaks to standard methods.\n\nIf the authors were to provide a more thorough comparison with simpler forms for regularization and demonstrated that their method provides a significant performance boost, then I could be convinced to change my score. "
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "I like the regularization about diversifying the weights in the convolution layers. The main contribution of this paper is to design the better optimization algorithm for compressive Compressive Hyperspherical energy. The difficulty lies in that the dimension is too high. So this paper propose to adopt two dimension reduction schemes: random project and angle-preserving projection. The whole algorithms are clear and easy to understand. Experimental results also demonstrate that the algorithms are effective and get better results in various problems. \n\nThere is theoretical analysis about angle preservation for the projection algorithms. And there are also statistical insights. The theoretical and statistical  analysis should be correct though I did not check them completely. However, the analysis seems not related to network. And I am wondering how the projection helps optimize the networks. This will strengthen the quality of this paper."
        }
    ]
}