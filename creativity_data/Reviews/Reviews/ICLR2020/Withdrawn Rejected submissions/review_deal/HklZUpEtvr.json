{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper provides a novel approach for addressing ill-posed inverse problems based on a formulation as a regularized estimation problem and showing that this can be optimized using the CycleGAN framework. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to a critical gap between the presented theory and applications. The paper will benefit from a revision and resubmission to another venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper, the authors present two contributions:\n1)\tThe primary contribution is to show that CycleGAN can be formulated as a probabilistic version of a particular penalized-least squares problem (theory)\n2)\tAs proof of concept, they apply their version of CycleGAN to accelerated MRI and deconvolution microscopy (application)\n\nWhile I find the idea to be potentially interesting, the presentation of the theory is unclear and not well-motivated; after reading, I’m not convinced that the connection to CycleGAN is as significant as the authors claim. The experimental results are preliminary. My decision is to reject. Below are separate critiques on the sections.\n\nSection 2-3: Hope the authors could clarify / strengthen these points in revision:\n-\tSince the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well-motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al. However, after taking a look at those papers, I don’t understand where this regularization term comes from. In these papers, the regularization term (i.e. equation 2 or 3 of Zhang et al) appears independent of y. Since this term is key to the paper, it should be well explained here. E.g. at the end of section 2.2: G_\\theta(y) is a CNN pretrained on what task?\n\n-\tIn the inverse problem, the objective is to estimate x from y. Therefore we care about \\argmin x in Equation (7). In the probabilistic setting presented in Equation (8), analogously the objective is to estimate \\pi^*, which is the solution to the primal problem. The theory shows that the primal formulation in Equation (8) is equivalent to the dual formulation in Equation (16), but does not show how the dual solution yields the primal solution, which is lacking as obtaining the primal solution seems to be the point of solving the PLS problem. (Interestingly, in Section 4, the authors are using the dual solution x = G_\\theta(y) as if it is the mapping given by pi(x|y)… this needs to be explained.)\n\n-\tThe authors claim that Proposition 1 shows that the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN. But looking closely at Proposition 1 and its proof, it seems that the equivalence holds only for specific weights, not for arbitrary weights. Additionally, the specific weights are unknown (they depend on the solution \\pi^* to the primal problem…). I do not understand the claim that this is a generalization of cycle-consistency loss, nor do I see how the authors implement their version of the cyclic loss as it depends on unknown weights.\n\n-\tThe connection to CycleGAN seems to hold only when p=q=1?\n\n-\tEnd of section 3: The authors conclude “our cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used”. I don’t see the connection between the theory and this claim. Even with CycleGAN, both generators can be arbitrary or fixed if one of them is known. \n\n-\tThe proofs are easy to follow, though perhaps they could be moved to the Appendix in favor of providing more motivation and explanation in the main text. \n\nSection 4:\n-\tThe authors motivate the problem with the PLS setup but then they use the learned regularization term x = G_\\theta(y) as if it is the mapping given by pi(x|y).  I am confused by this.\n-\tPutting aside the connection to the PLS problem, my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss, where one of the generators is known (and hence only one generator/discriminator pair is needed). I might be missing something, but I’m not sure that this approach is different enough from CycleGAN.\n-\tConsidering that the authors have the ground truth, they could provide quantitative evaluation of their method against other methods, rather than showing a few qualitative results where it is working.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper frames and contextualizes CycleGAN as a stochastic generalization of penalized least squares for inverse problems, providing several unifying theorems, rederiving some modern CycleGAN architectures within the optimal transport framework, and also demonstrating the practical use of modified architectures derived using this framework for accelerated MRI and microscopy. \n\nWhile I did not check the proofs in detail, the additional generalization of a well-known, working architecture and additional variants used practically seem significant to me. Demonstrating the deep relationships in the proofs and propositions shown here, followed by two clear and concise derivations *and their practical application* is compelling, making this paper broadly applicable to both practitioners of compressed sensing and generative modeling. Frankly, it will take me some time to digest the proofs and overall relationships shown in the Propositions here, as I am not deeply familiar with optimal transport. The applied sections are direct, with Figure 5 being especially meaningful.\n\nThere are a few small grammatical issues in the text - a careful re-read and edit with particular focus on grammar and style would be beneficial, though as it stands these small flaws didn't meaningfully detract from the paper.\n\nThe primary thing the authors could do in order to raise my score, would be to take an additional pass at grammatical clarity for the paper. More experiments are always beneficial, and I would encourage the authors to release source code to replicate some form of their experiments if possible.\n\nSome additional references which may be useful additions, primarily for interested readers to gain further background on the use of deep models for compressed sensing:\nCompressed Sensing with Deep Image Prior and Learned Regularization https://arxiv.org/abs/1806.06438\nCompressed Sensing using Generative Models https://arxiv.org/abs/1703.03208\nDeep Compressed Sensing http://proceedings.mlr.press/v97/wu19d/wu19d.pdf"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents an interesting connection between cycleGAN, penalized Least Squares (PLS) and optimal transport (OT). From the PLS with deep regularization formulation, the authors cast a general OT problem with a specific cost combining measurement and reconstruction errors (eq. 9). From this formulation, the problem is expressed as a combination of a cycle consistency loss and an OT loss. The formulation is more generic than the classical cycleGAN formulation. Qualitative experiments are conducted on two different scenarii: accelerated MRI and deconvolution microscopy, for which the proposed method achieves good performances.\n\nIn general I like the paper and the corresponding idea. However, I had a hard time understanding some of the key elements of the proposed method. Notably, in lemma 1, it is hard to understand in which case \n\\mathcal{X} \\times \\mathcal{X} / A \\union B is not an empty set. It is clearly the case whenever G or H are bijections, or whenever an exact reconstruction is achievable, but I guess a more open discussions could be conducted. Also, the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation. As noted in proposition 2, however, the corresponding OT loss can be computed on the entire subset X and Y (since the Kantorovich potentials match those obtained on the restricted set). As a consequence, we can see the overall training procedure when H is fixed (as it is the case in both experiments), as learning for a generator in a WGAN way, that also enforces a consistency constraint (that can be seen as a regularization of the OT problem). While I like this idea, I think there might be some better justification for it (it all starts with Eq. 9 and the choice of this model. Why enforcing this minimal cost equation ?). Note that the model imposes that p=q=1, which is somehow limited. I guess other choices of p and q could be possible, eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials    \n \nI am willing to revise my note positively provided that a sufficient number of convincing explanations are given on my previous remarks.\n\nMinor remark\nEq. 8 should include a weighting factor between the two terms since the dimensions are not the same. \nIn the proof of Lemma 1, since you derive an equality for T, do you have to establish the first inequality ?\nProp. 1 I guess there is an error on the first line (the integral is not over \\mathcal{X} \\times \\mathcal{X} / A \\union B given the previous definition\nMissing related work \nDLOW: Domain Flow for Adaptation and Generalization Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2477-2486\n\n"
        }
    ]
}