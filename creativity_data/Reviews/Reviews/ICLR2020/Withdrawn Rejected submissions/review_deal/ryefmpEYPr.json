{
    "Decision": {
        "decision": "Reject",
        "comment": "Thank you very much for your feedback to the reviewers, which helped us a lot to better understand your paper.\nHowever, the paper is still premature to be accepted to ICLR2020. We hope that the detailed reviewers' comments help you improve your paper for potential future submission.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a sparsification technique that seeks edges contributing negligible amounts to the performance of a network. \n\npros)\n(+) This paper is written well but needs more clarity.\n\ncons)\n(-) This paper did not cite modern pruning/sparsification methods published recently.\n(-) The proposed method has only compared with some outdated methods.\n(-) Only LeNet-5 and VGG-16 have been used to validate the proposed method.\n(-) This paper lacks any analysis of why the proposed methods would work well. Specifically, on the formulation of the sparsification method, it is hard to find sufficient backups why the authors did like that.\n\nComments)\n- How would you guarantee using the infinite feature selection method could give proper score in general?\n- How did you determine theta_l for each layer?\n- The major problem of this paper is the experimental section. This paper only compared with outdated methods, so it is hardly verifying the effectiveness of the proposed method compared to other methods. \n- It is necessary to involve ResNets as one of the baselines."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This work proposes iSparse framework, which aims to sparsify a neural network by removing redundant edges. Unlike previous works where the edges were removed based on their weight value, or based on the relationship between input and output neurons, iSparse selects the edges to remove by computing the contribution of each edge with respect to the final outcome. The experiments show that, compared to several baseline methods, iSparse perform favorably on multiple datasets. \n\nComment:\nAlthough I am not an expert in network pruning or network sparsification, I know that the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) were able to remove at most 80% of the weights of neural networks (both fully-connected and ConvNets) and still retain the original performance level. Compared to that, iSparse's performance does not seem too impressive. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposed a method for network sparsification based on the significance of each edge (weight). Unlike some of the existing works, edge significance in this work is explicitly defined based on their influence over the network output.\n\nThe algorithm can be summarized as follows:\n1. Compute the significance of each activation (neuron) using an existing method (infinite feature selection).\n2. Compute the significance of each edge as the product between the neuron significance and the absolute edge weight.\n3. Sort the edges according to their significance scores and keep the top portion.\n\nI'm mainly concerned about the limited technical novelty: the proposed technique is essentially a heuristic. Specifically, it is not clear why the significance of each edge should be defined according to equation (5) rather than some other forms. The intuition behind masking out the gradients of in-significant edges in (10), i.e., \"we argue that any edge that does not contribute towards the final model output, must not be included in the back-propagation\", is again a heuristic that lacks justification. If theoretical analysis is not possible, it might be necessary for the authors to conduct controlled experiments/ablation studies to show that some of the design choices made in the paper are indeed superior over other alternatives.\n\nAnother of my concern is that, if the goal is to make the sparsification decision aware of the network output (e.g., the value of the loss function), a simpler approach would be to enforce L1 regularization over the edges. This way, edges that do not lead to significant impact on the loss would be automatically pruned away. I wonder how would the proposed approach compare against this simple baseline.\n\nOther suggestions:\n* Empirical evaluation is conducted using LeNet and VGG16. It would be interesting to extend the analysis to some other seminal architectures, such as ResNet, Inception and MobileNets.\n* It would be informative to report the hardware configuration used to obtain the execution time in Figure 5. Note the relative inference cost of different models may differ substantially over different hardware platforms.\n* Writing of the paper can probably be polished further for better clarity. "
        }
    ]
}