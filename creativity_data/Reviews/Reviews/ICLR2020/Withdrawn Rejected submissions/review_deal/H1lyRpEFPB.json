{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper investigates the effect of adding noise to action selection and transition dynamics estimation in model-based reinforcement learning (MBRL) for improved exploration. The authors use the iterative Linear Quadratic Regulator (iLQR) algorithm with their methods for inducing noise and evaluate the performance on a robotic arm simulator.\n\nI think the paper explores an interesting problem which does not receive a lot of attention (exploration in MBRL). However, given that it is an empirical paper, I don't find the empirical evaluation thorough enough and some of the results found seem questionable. Further, the paper is completely focused on iLQR on the robotic arm task, so it's not at all clear whether the results carry through to other algorithms or other environments.\n\nSome more detailed comments:\n- Intro: \"combines two extremely data inefficient techniques\": are you referring to model-free versus model-based? they're not really \"combined\" in RL, and \"extremely data inefficient\" seems rather strong.\n\n- Intro: \"In model free methods, exploration is usually done by adding noise to the action suggested by the optimized policy.\": not really, most of the interesting exploratory algorithms use more sophisticated techniques like intrinsic rewards, pseudo-counts, and many even have theoretical guarantees (see all the PAC-MDP work).\n\n- Sec 2.1: There's a whole line of related work missing here. For example:\n  * \"Near-Optimal Reinforcement Learning in Polynomial Time\", Kearns & Singh, 2002\n  * \"R-max – A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\", Brafman & Tennenholtz, 2002\n  * \"PAC Optimal Exploration in Continuous Space Markov Decision Processes\", Pazis & Parr, 2013\n  * \"Unifying Count-Based Exploration and Intrinsic Motivation\", Bellemare et al., 2016\n  * \"Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment\", Taiga et al., 2019\n\n- Algorithm 3 line 6: Where does \\hat{\\pi} come from?\n\n- Section 3: \"aim of reinforcement learning is to find a stochastic policy\" it doesn't have to be stochastic\n\n- Section 4.1, Action-maximum-entropy: You use $Q^{-1}_{a_t, a_t}$, what does this mean? Why does Q have two a_t subscripts?\n\n- Section 4.2, Transition-fixed-covariance: \"we set $\\epsilon^2_i = 1.0$\": this seems quite high. is there a reason why? did you try other values?\n\n- Equation (4): Is this a standard cost function or is it only specific to this work? If the former, add reference, if the latter, explain the choice.\n\n- Section 5: It's not clear what you mean by \"inverse kinematics\"\n\n- Figure 1: It's not clear where the targets are. It might help if you draw a trajectory from the initial to target position on each, otherwise it's hard to tell them apart.\n\n- Figure 2 (a): What's the difference between the three plots?\n\n- Figure 2 (a): The labels on the legend don't match anything that was previously introduced, so it's not clear what's what.\n\n- Section 5: What's MPC?\n\n- Table 1: There are a number of different hyperparameter choices for the different exploration methods you evaluate. How did you pick them? Did you do a sweep? Are the results averaged over multiple runs? How many?\n\n- Section 5.2: What is \"finite-difference approximation\"?\n\n- Section 5.2: \"We run the same set of experiments as above, but only with inner-loop exploration\": why?\n\n- Table 2: The algorithm with no exploration does *much* worse when using the ground truth dynamics compared with using the model in table 1. This is totally counter-intuitive: shouldn't the results be strictly better given that there's zero model approximation error?\nI'm not sure I buy the justification you gave in section 5.3. There is a >2x degradation when using ground truth without exploration for the 0.10m target, which is significantly worse than the degradation for the harder 0.45m target. If exploration really is the cause, I would expect to see the decrease in performance correlate with the difficulty.\n\n- Section 5.3.1: This section seems somewhat unrelated to the main thesis of the paper. If you do want to investigate this problem, it calls for a much more thorough investigation. Right now the investigation provided is somewhat superficial, which detracts from the paper.\n\n- Section 5.3.1: \"Thus, we conclude that... for a particular target\": Is this really that surprising? Ground truth dynamics are agnostic to the target, whereas the learned models build their estimates from collected trajectories, which are gathered using the current best policy, which is aiming to minimize the cost to the specific target.\n\n\nOne minor comment:\n- Section 5: \"We generate targets three distances 0.1m, 0.45, 0.8m away from the initial...\" would read better as \"We use targets at three different distances from the initial end-effector: 0.1m, 0.45m, 0.8m\"."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This paper presents an empirical study of different exploration techniques for model-based RL (specifically, iLQR) on a simulated manipulation environment. The experiments show the following results:\ni) None of the exploration techniques outperform the basic MBRL loop without exploration.\nii) MBRL loop w/o exploration outperforms MBRL with ground-truth dynamics on a single task, due to induced exploration by the imperfect model. However, it performs worse on generalization to new targets.\n\nDecision: Reject\n\nMain reason for the decision: I think the paper contributes an insightful observation (that an imperfect model induces exploration & curriculum in model-based RL, and works better than other exploration techniques in their manipulation domain). I am largely concerned about the lack of variety in experimental domains. The main contribution of the paper is an empirical evaluation of different exploration techniques (noise in action space vs. transition space vs. environment model; and exploration during optimization vs. environment interaction). However, the paper only presents experimental results on a single domain (simulated manipulation environment with 7 DoF). It would be helpful to compare the different exploration algorithms on a more diverse set of domains with different action spaces, transition dynamics, and task complexity.\n\nClarification questions / additional feedback:\n\n---Problem Motivation---\n1) It would be helpful if the authors elaborated on why there is a relative lack of prior work on exploration for MBRL (compared to model-free RL). Is exploration in MBRL more difficult (and if so, why?)? Is exploration less important in domains where MBRL is preferred over model-free RL (e.g., domains where environment interactions are expensive)?\n\n---Figure Presentation---\n2) Fig 2a: What is “ee dist”? Also, the three learning curve plots don’t seem to add much useful information to the paper, so they could be moved to the Appendix.\n3) Fig 2b: Why was “best reward seen so far” used instead of “average reward over trajectories”? Is it a more useful or fair evaluation metric, and why?\n\nMinor comments on grammar & style that did not impact the score:\n“but a relatively unexplored topic for model-based methods”  --> “but relatively unexplored for model-based methods” or “but *is* a relatively unexplored topic”\n“like the ground-truth simulation”  --> “like ground-truth simulation”\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Exploring by Exploiting Bad Models in Model-Based Reinforcement Learning\n============================================================================\n\nThis paper investigates the importance of exploration in model-based reinforcement learning.\nIn particular, the authors suggest that the *implicit* exploration generated through randomly initialized dynamics models can be competitive with, or even outperform alternative methods.\nThe main support for this claim comes through improved performance on a 7-joint robotic arm.\n\n\nThere are several things to like about this paper:\n- Highlighting the (potential) importance of exploration in complex model-based RL is important, especially since it is known to be a bottleneck in the field that usually relies only on reward shaping.\n- The observations that \"random initializations\" can provide some prior effect and that this can implicitly drive exploration is interesting.\n- The resultant algorithm is quite sensible and it generally appears to perform well.\n- The overall writing is reasonably clear.\n\n\nHowever, there are some places where this paper falls short:\n- The discussion of past work on \"efficient exploration\" in \"model based\" RL is sorely lacking, and in many places it is even wrong. In fact, it seems like *most* of the research on efficient RL has been done in the model-based setting... e.g.  E3, R-max, UCRL2, PSRL and more... Now I think that what you mean is model-based plus generalization but even then there are bounds for PSRL https://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension and more!\n- The connections to PSRL here are particularly interesting due to the argument that \"randomly initialized\" neural networks can approximate the posterior distribution... actually I think that there is some precedent of that in RL / exploration too e.g. bootstrapped DQN.\n- Actually I think that the authors mean to restrict their focus to \"model-based Deep RL\" but I'm not exactly sure how they draw the line there. And if the focus is specifically on the scalable Deep RL method, then there are even some questions about how scalable the model-based versus model-free approaches are... and you should address why this is such a focus.\n- I'm not sure that these experiments are very targeted in terms of insight, it would be nice to see some experiments that specifically target \"efficient exploration\"... potentially something like bsuite could help with that https://github.com/deepmind/bsuite.\n- I don't find the tables an easy way to parse data, some plots or bar charts could help to make this more clear!\n\n\nOverall I think this has the potential for good work, but there really are a lot of loose claims regarding exploration and model-based RL.\nBased on this, and some issues I have with the clarity/insight of the support for the claims I will tend to reject."
        }
    ]
}