{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper extends Adam by adding another hyperparameter that allows the second moments to be raised to a power p other than 1/2. This certainly seems worth trying. The paper is well written, and the experiments seem reasonably complete. But some of the reviewers and I feel like the contribution is a bit obvious and incremental. The \"small learning rate dilemma\" needs a bit more justification: since the denominator has a different scale, the learning rates for different values of p are not directly comparable. It could very well be that Adam's learning rate has to be set too small due to some outlier dimensions, but showing this would require some evidence. From the experiments, it does seem like there's some practical benefit, though it's not terribly surprising that adding an additional hyperparameter will result in improved performance. The reviewers think the theoretical analysis is a straightforward extension of prior work (though I haven't checked myself). Overall, it doesn't seem to me like the contribution is quite enough for publication at ICLR.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new variation on adaptive learning rate algorithm that unifies SGD with momentum and Adam/Amsgrad. They provided convergence proof for this algorithm. The effectiveness of the proposed method was demonstrated through various domains and neural networks architecture. Though the empirical results are extensive, I am leaning towards reject because (1) The reason why the method works isn't clear. (2) The theory doesn't justify the practice. (3) The practical usefulness of the algorithm isn't clear. Here are my detailed comments:\n\n(1) The paper provides an observation which they call \"small learning rate dilema\": One often uses a smaller base learning rate for adaptive gradient methods than SGD with momentum. This makes the boost one can gain from applying learning rate decay to adaptive gradient methods not as significant as applying to SGD with momentum. Based on this observation, they propose to penalize the adaptiveness by adjusting the value p in their algorithm. However, the proposed adjustment seems like a trivial one, without giving too much insights into why learning rate decay is not compatible to adaptiveness. An insightful analysis should try to first answer the following questions:     \n      (a) Why shall one start with a large learning rate in the beginning?\n      (b) Why does learning rate decay gives a boost to performance?\n      (c) If one uses an adaptive method, how does it affect one's choice for the initial learning rate? \n      (d) and how does the adaptiveness changes the effect of learning rate decay?\nTo answer those questions, I suggest the author to read [1] where they gave partial answers to (a) (b). If one tries to do similar analysis performed in [1] for adaptive methods, one might be able to answer (c) and (d). \n\n(2) I have two criticisms to the theoretical analysis carried out in the paper. The most important issue is that the analysis is not useful to show the effectiveness of the proposed method:\n     (a) The rate of convergence matches with SGD + momentum. So it is not better than the baseline. \n     (b) It does not show the relationship of adaptiveness and decaying learning rate schedule. \nThe second criticism is related to the novelty of the theorems.  Please correct me on this because I did not go over the theorems carefully. But based on my crude assessment,  theorems are mostly mechanical applications of prior work to the current extended version. Hence it does not provide any further insights into the convergence of nonconvex optimization methods.\n\n(3) Though the empirical results are good, where the proposed method matched or outperformed all previous methods in The method introduces one extra hyperparameter, p, for tuning. It is then questionable whether the algorithm is efficient in terms of hyperparameter searches, i.e., how many hyparameter sweeps are needed for finding a good run, versus baseline methods like SGD+momentum. Since the performance of the methods are mostly the same, if the proposed method requires as many hyperparameter tuning as SGD+momentum, then it makes the proposed method less useful in practice.\n\n[1] Understanding short-horizon bias in stochastic meta-optimization. Wu et. al. ICLR 2018."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "It has been empirically observed that adaptive optimizers such as Adam/Amsgrad\nlead to worse generalization than SGD + momentum when used to train neural\nnetworks.\nMotivated by this observation the authors suggest Padam, a modification of Adam/Amsgrad.\nPadam contains a parameter p that, when set to 0 reduces their method to SGD + momentum\nand when set to 1/2 reduces their method to Adam/Amsgrad.\n\nThe Padam iteration is roughly $x^{k+1} = x^k - \\alpha * g / v^p$\nwhere $g$ is an exponential moving average of stochastic gradients and\n$v$ is an exponential moving average of the squared gradient.\n\nIn that way Padam is capable of interpolating between the two methods in order\nto find a good trade-off between the improved convergence of the one and the\nimproved generalization capability of the other.\n\nThe authors also suggest an explanation for why the generalization gap\nof Adam happens:\nThey claim that it is due to the \"small-learning rate dilemma\" that happens\nas follows.\nA small second moment of the stochastic gradients as approximated by $v^{1/2}$\n(think variance) in can lead to large effective steps in some components.\nTo balance this effect out, Adam needs to choose smaller step sizes than\nSGD + momentum.\nIf the same learning rate schedule is used based on a smaller base step size,\nAdam under-trains at the end of training.\n\nThe authors suggest that Padam with p < 1/2 can use larger learning rates\nbecause it does not have as large effective steps if second moments are small.\n\nThe authors also prove convergence rates for making gradients small with Padam.\nThey use the setting of nonconvex optimization and not online convex regret\nanalysis like the Adam/Amsgrad papers.\n\nFinally, the presented experiments using image data sets and standard neural\nnetwork architectures suggest that Padam indeed shares the advantages of both\nSGD + momentum and Adam and thus obtains a best of both worlds.\n\nI suggest to accept the paper.\nIt introduces an elegant generalization of Amsgrad that appears to be\nempirically useful in experiments.\nThe experiments seem to be fairly performed (in terms of hyperparam search).\nThe rigorous convergence analysis is laudable although perhaps not as relevant\nas the practical usefulness of the suggested approach.\n\nI do however suggest that some changes be made.\n1. The \"small learning rate dilemma\" phenomenon needs to be more clearly defined\n\tand explained.\n\tThe whole relationship of SGD step size schedules to generalization\n\t(e.g. simulated annealing analogy) is certainly nontrivial.\n\tI would rather the paper not make some nonrigorous claims if there is no\n\tproof.\n\tOr state clearly whether something is conjecture or proposition (with proof).\n\n2. Explain why the same learning rate schedule should or is (in practice) used\n\tfor Adam as for SGD + momentum, as this does not seem to make sense given\n\tthe aforementioned dilemma.\n\nSpecific notes / suggestions:\n- Page 2: \"We proposed a novel\" -> \"We propose a novel\"\n\n- Page 4: \"bridging this generalization gap\"\n\t\"this\" does not make sense to me in that context, rather use \"the\"\n\n- Page 5: Figure 1, p = 1/16 seems to still be the most attractive in terms of\n\tgeneralization in the long run. Why would I not want to wait till the model\n\tis fully trained?\n\n- Page 4: \"It is very likely that Adam/Amsgrad is \"over-adaptive\"\n\tThis seems to me a strong claim and the explanation that follows it to me\n\tis not rigorous enough.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "% post author response %\nThanks for your detailed response. \n\nR1. Note that in almost all classical optimization routines, the learning rate has a (very intuitive) scaling on the problem parameters - for e.g. in gradient descent, the learning rate looks like 1/smoothness. This is mirrored in the definition of Newton methods - where, in the direction of hessian^{-1} grad, one uses a scale free line search to estimate an appropriate stepsize between 0 and 1 (to emphasize, this value has *no* dependence on problem scaling). While this begins to fall apart with the case of adaptive gradient methods, how can we even hope to justify the potentially arbitrary power of, say, 1/4 or 1/8 used by Padam? This is the reason behind my comment that the algorithm is unnatural. By using such a power of the smoothness of the problem, the other component of the learning rate (alpha_t) is no longer a scale free quantity. It has to depend on other problem dependent parameters for the overall learning rate to be scaled appropriately based on the problem characteristics. \n\nR2. While the paper performs grid search (sec C.3 in the paper) for the partially adaptive parameter, the lambda value for YOGI is set as one suggested in their paper. I can accept the claim of authors if I see more experiments tuning the lambda parameter for YOGI as well.  Otherwise, I dont quite see why one specific parameter value for lambda works for every problem.\n\nR3/R5. My point is that the original paper for adaptive methods (adam/adagrad) never mentions learning rate decay. This seems to have been added in subsequently just to boost the performance. What I do not understand is what is the specific advantage of adaptive methods over SGD if every component used by SGD (including learning rate decay) is used even by adaptive methods. Even from a theoretical bound perspective, to the best of my knowledge, there is no clear indication that the partially adaptive momentum methods actually improve over vanilla sgd (+momentum) in settings that do not involve dense parameters/gradients.\n\nR4. Again, the learning rate decay has a specific use in the papers I referenced in my review. Somehow, the bounds presented in the paper do not reflect the use of a step decay schedule on the learning rates, so, I see that the theorem (aside from the assumptions mentioned) is detached from the practical results even in this respect. \n%%\n\nThis paper considers generalization issues experienced by adaptive gradient methods compared to well-tuned SGD + momentum, a topic that is of interest in the development of optimization methods for deep learning. The paper is well-written and elaborates on (i) issues faced by adaptive gradient methods in contrast to standard SGD + momentum, (ii) presents experimental results on training standard conv-net based architectures on image classification benchmarks and on training LSTMs on PTB and (iii) presenting theoretical analysis relating convergence of the method to a first-order critical point for smooth stochastic non-convex optimization.\n\nI have questions about certain aspects of the paper, which I will elaborate below:\n\n— If one takes a step back to understand the origins of diagonal adaptation methods (introduced by Adagrad), this was motivated by the infeasibility of using the inverse square root of a full pre-conditioning adaptation matrix. If we think of such full matrix adaptation methods, is this paper implying the use of other matrix powers (other than a square root) as used by adagrad (or other adaptation approaches)? This appears very unnatural to me.\n\n-- If the main issue is that it is not possible to typically use a large learning rate at the start with ADAM or other preconditioning methods and that leads to using other powers of the diagonal adaptation matrix, a more natural fix would be to use a conservative (trust region inspired) approach to reduce aggressive steps at the start. What I mean is as follows: Suppose H_t is a preconditioning matrix, g_t is the gradient (or the discounted sum of gradients with/without bias correction). The current approach is to make H_t diagonal and use H_t ^{-1/2}g_t as the update matrix. One option to prevent aggressive initial steps is to use (H_t + \\lambda)^{-1/2} g_t, either with having lambda fixed through the optimization or making it a function of iterations. \n\n— I am not aware that papers which employ adaptive methods also tend to use some form of learning rate decay (on the alpha_t ’s) - at least, by looking at the original papers of Adam/adagrad, I do not see the combination of learning rate decay and adaptive methods. In a sense, if one has an `\"adaptive\" optimization method, it’d be unnatural to have to use some form of step decay of the learning rates (alpha_t's) in conjunction with these methods. One would typically just use SGD+momentum with some form of such a step decay of the learning rates. This to me is a serious shortcoming - it appears to make the use of adaptive methods almost irrelevant because the only hyper-parameter that it gets rid of is the dependence on the initial learning rate (because, for SGD, we anyway have the other hyper-parameters like momentum, stepdecay factor, when to decay learning rate etc.)\n\n— With regards to theory (and connections to experiments): Despite the fact that the paper employs a step decay schedule on the learning rates (alpha_t), their theorem statement (or any corollary) doesn’t actually employ this specific step size decay scheme (on the alpha_t ’s) and attempts to understand what are the advantages of the step decay schedule on the convergence statements provided.  The step decay schedule has featured in several recent efforts in the stochastic optimization community (both with convex (https://arxiv.org/pdf/1607.01027.pdf, https://arxiv.org/pdf/1904.12838.pdf)/non-convex (e.g. https://arxiv.org/pdf/1907.09547.pdf) objectives), where, the results indicate non-trivial advantages of using these step-decay schemes on alpha_t’s (though, these are with non-adaptive optimization methods).\n\n— Along these lines (of the previous point), it is important to note what the performance of the optimization method is when alpha_t’s are fixed to a specific value (without being decayed over the course of optimization) - since this relates to the most standard definition (and advantage) associated with using adaptive gradient methods. My understanding is that this result continues to be fairly sub-optimal compared to using SGD+momentum with a step decay schedule. \n\nOther minor comments:\n— I do not understand the use of the term “second order” momentum for calling variables that have a running average of squared gradients. This term is misleading in what it represents.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}