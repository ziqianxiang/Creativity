{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose a new model for novel view synthesis under positive camera translation along the z-axis for a single image. The model firstly builds a sequence zoomed-in versions of a single image. Then it blends these images with weights learned from a network.  The model is trained end-to-end in an unsupervised fashion, with the help of a single image depth estimation method. \n\nMy main concern of this paper is the lack of quantitative evaluation. The authors can obtain the ground truth 3D zoomed results, as shown in Figure 6, so I believe some types of quantitative evaluation can be conducted. It is also difficult for me to tell whether the results are good or not because the ground truth is not shown in most of the figures.\n\nAnother concern is the generalization capability. It seems to me that the model may not work well beyond the images in Cityscapes or KITTI. The paper will be not well-motivated if it does not work well in the wild. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposed an algorithm for synthesizing 3D-zoom behavior when the camera is moving forward. The network structure, called deep 3D-zoom-net, incorporates disparity estimation in a GANs framework to synthesize novel views. Experimental results on KITTI and Cityscapes datasets seem to suggest the proposed approach makes sense.\n\nThe proposed task is a new computer vision task, and the formulation of the approach (network structures, training protocols) makes sense. Some visualizations in the paper suggest that the prediction is sensible, and can potentially be useful for other computer vision tasks. \n\nHowever, I have a few questions regarding the intuitions behind this paper:\n- Why the proposed task is useful? Since this is a new task, the paper didn't discuss one concrete use cases beyond the scientific interest. I understand that it may potentially be useful for special effects or AR applications, but it'll be helpful to discuss more on why certain applications may need the proposed system, especially that the camera is only moving along the z-axis.\n- I think some of the experimental results make sense, but as I zoom in the pictures, most of them have very strong halo artifacts, and it doesn't seem to meet a very high standard in terms of the qualities. Moreover, those images comes from outdoor scenes, when generalizing the algorithm to indoor environments where the disparities are bigger overall, and perspective distortion is more severe, how will the proposed system perform? Regarding the realism metric, I think performing a user study is more legitimate, rather than verbally say that the proposed result is of high quality in the paper. \n- I think a better experimental setting would be testing the whole system on a synthetically generated dataset, where we know exactly what the ground truth image will look like. KITTI or cityscape are nice, but it's no guarantee the camera moves exactly along the z-axis. Using synthetic datasets can also help benchmark the quantitative performance of the system. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a method for creating a \"zoomed image\", for a given input image. The method is trained in an  unsupervised way.  For training, a recent method is used to predict a depth map for the input image, so that a target image (with some artefacts) can be generated automatically. Then, a network is trained to predict this target image, with  perceptual and adversarial loss to make the predicted image look more realistic.\n\nThe paper tackles a fun problem, however I have several concerns:\n\n* I am not sure the method is interesting for the ICLR audience.  The paper claims to introduce \"a novel back re-projection reconstruction loss, that allows the network not only to learn the underlying 3D structure but also to maintain a natural appearance\", however I found the loss is quite straightforward and only includes existing loss terms.\n\n* There is no quantitative evaluation of the accuracy of the predicted images. I understand there is no readily available dataset for such evaluation, however the ball is in the camp of the authors to solve this problem. \n\n* If I understand correctly, the network is trained for a single scale factor. A network has to be trained for every new scale factor.\n\n* I found the paper quite unclear. More details below.\n\nFor all these reasons, even if I found the work overall interesting, I don't think the paper should be accepted at ICLR.\n\n------\n\nThe description of the method could be much clearer. The equations often have a \"Python code\" taste (see eg Eq (8-10)). A more mathematical definition of the loss would be clearer I think.\n\nEq (7): The \"selection volume\" is not defined.\n\nMaybe I missed them, but the \"refinement block\" and the \"final blending operation\" are not really described.\n\nMore minor errors:\n\nSection  2.3: \"3D-zoom can be defined as the positive translation of the camera in the Z-axis\" -> No!  A zoom is a decrease of the focal length. The proposed method still seems to correspond to a focal change, though. It is parameterised by a \"zoom_factor\". The link with Eq (8-10) and Section 2.3 should have been made clearer, currently Section 2.3, which gives the equations of perspective projection is quite disconnected from the method.\n\nSection 3.1.1: The network is not an auto-encoder, even if it has a similar architecture. It would be an auto-encoder if the output was the same as the input.\n\n\"the g(Â·) is open not capable of\" -> \"g() is not capable of\"?\n\n"
        }
    ]
}