{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a deep clustering method based on normalized cuts.  As the general idea of deep clustering has been investigated a fair bit, the reviewers suggest a more thorough empirical validation.  Myself, I would also like further justification of many of the choices within the algorithm, the effect of changing the architecture.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper suggests a differentiable objective that can be used to train a network to output cluster probabilities for a given datapoint, given a fixed number of clusters and embeddings of the data points to be clustered. In particular, this objective can be seen as a relaxation of the normalized cut objective, where indicator variables in the original formulation are replaced with their expectations under the trained model. The authors experiment with a number of clustering datasets where the number of cluster is known beforehand (and where, for evaluation purposes, the ground truth is known), and find that their method generally improves over the clustering performance of SpectralNet (Shaham et al., 2018) in terms of accuracy and normalized mutual information, and that it finds solutions with lower normalized cut values.\n\nThe method proposed in this paper is very simple and appears to work well, and so this paper represents an important contribution. However, there are some issues with the presentation that I think should be fixed before publication:\n- Equation (3): I'm not sure I understand the sum over z; don't we just want w_{ij} Y_{ik} (1 - Y_{jk})?\n- Equation (6): I don't think the final objective should be presented as an expectation. It is rather the quotient of two expectations. In general, it might be better to just present the objective as a relaxation of the normalized cut objective.\n\nA question regarding the results and parameterization: was using Gumbel-Softmax necessary to get good results? Did ordinary softmax not work?"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents an end-to-end approach for clustering. The proposed model is called CNC. It simultaneously learns a data embedding that preserve data affinity using Siamese networks, and clusters data in the embedding space. The model is trained by minimizing a differentiable loss function that is derived from normalized cuts. As such, the embedding phase renders the data point friendly to spectral clustering. \nThe paper follows the general setup of deep clustering: map data to a feature space while maintaining data distributional characteristic, and make data clustering-friendly in the feature space. The authors use Siamese networks for the first part and use a normalized-cut motivated loss for the second part.  The choices are reasonable and the loss is somewhat novel. \nCNC is evaluated on standard datasets, including MNIST, Reuters, CIFAR-10, and CIFAR-100. The results are impressive. However, deep clustering has been around for quite a few years. It might be time to move on to more challenging benchmarks. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes a new clustering method, called CNC, which is composed of two-step procedures.\nIt first embeds an input dataset into a d-dimensional space, followed by performing relaxed normalized cut to detect clusters.\nAlthough the contribution of introducing a new relaxed formulation of the normalized cut is interesting, I have the following concerns regarding with the clarity, significance, and evaluation of the proposed method.\n\n- The paper is not clearly written at many points and the quality of presentation is not high, which also deteriorates the significance of the paper.\n    In particular, the optimization process for clustering discussed in Section 4.1 is not clearly presented.\n    Although the objection function, which is the expectation of the Ncut, is introduced in Equation (6), how to solve it is not presented.\n    Since this is the key step for CNC, it should be carefully discussed.\n- In the embedding step, how to choose the dimensionality d?\n    This is not even reported in experiments.\n- Empirical evaluation is not thorough and important evaluation is missing.\n    * First, the contribution of embedding is not evaluated.\n      The performance between CNC with the proposed embedding and without it should be compared.\n      Moreover, the sensitivity of the performance with respect to changes in d should be examined.\n    * A number of resulting scores are missing; in particular, CNC is compared to only SpectralNet for CIFAR-10 and CIFAR-100 under NMI.\n      It would be more convincing if the NMI for other methods are also reported.\n- Parameter sensitivity is not evaluated, while there are a number of parameters in the proposed method as reported in Section 5.7.\n    Since parameter tuning is fundamentally difficult in the unsupervised setting, parameter sensitivity is crucial.\n    Also how to choose such parameters is not clear.\n\nMinor comments:\n- In Algorithm 1, line 1, \"X \\in R^n\" -> \"X \\subseteq R^m\"?\n- In Algorithm 1, the dimensionality \"m\" of data points and the batch size \"m\" are the same. Is it correct?\n- At the first line in Section 4.1: \"for each data point\" -> \"For each data point\"\n- P.4, L.-4: \"nreast-neighbor\" -> \"nearest-neighbor\"\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}