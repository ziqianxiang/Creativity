{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper discusses the behaviour of distributions in neural networks with finite width and claims that those distributions are non-Gaussians. \n\nPros: \nThe paper studies architecture of neural network with finite (but asymptotically large) number of hidden units, which is what we have in the reality. The  non-Gaussian process behaviour is obtained through the distribution corrections in the recursive formula. The authors also showed a regularisation effect, based on those corrections. I believe that the algebra here is correct. Overall, the results are interesting and well-places in the literature and the 'dreams', i.e. future directions, provided by the authors sound interesting. \n\nCons:\nI find a paper is hard to follow. I think it’s better to change a structure or a way of introducing the results. It would be better to state at the beginning in simpler words what are the results and how to get those results from an infinite regime to a finite one. Already giving more explanations for the terms in Equation (KS) would help the motivation. And, as authors also claimed, it would be nice to have additional experiments on commonly used neural networks structures. \n\n\nSome major suggestions: \n* There is a not cited paper that I think is closely related to this work (Vladimirova et al., 2019). They study tails of distributions for the finite number of hidden units . As a result, the tails become heavier-tailed as depth increases, therefore the distributions are further from Gaussians if we go deeper. So obtained non-Gaussian behaviour does not contradict with the one in the published work. \n\n* As I understood, the non-Gaussian process behaviour is obtained by adding the self-energy correction term into the correlation functions. How far the preactivation distribution is from the Gaussian depending on the self-energy? \n\n* Section 2.3 ‘Related work’ is hard to understand and, therefore, to relate to the current work. For example, there was no words before about a Schwinger operator approach. \n\n* There is only one toy experiment for 2 hidden layers neural networks with different width. Does it work for deeper layers? \n\nComments that didn’t have an influence on the decision: \nI also find a style of the paper a bit too casual: ‘Inception’, ‘you guessed it’, ‘the graduation from the magical school of Wick’s crafts and wizardly cumulants’, ‘without solicitation’, ‘let us take off from the terminal point’, etc. \n\nI wonder if this result can be applied to CNNs or ResNets? \n\n\nReferences: \nMariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding Priors in Bayesian Neural Networks at the Unit Level, ICML (2019). "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper proposes a method based on Wick's contractions that can integrate out intermediate layers of a Bayesian neural network with finite width. Similar to the connection between GP and infinite-width NN, the marginalization of intermediate layers could allow Bayesian inference for the network.\n\nThis is a very technical paper with mainly the derivation of the recursion relations and the posterior mean for Bayesian inference. The math is tantalizing. But the main concern is how effective it's gonna be when being used in a neural network inference problem. \n\nThe simulated example shows that there could be a mismatch between theoretical predictions and experimental data when using ReLU. How much effect it's gonna have in a prediction task?\n\nThe recursion relations need to be evaluated numerically for most of cases. Then what's the computational difference between the proposed method and general MC based integration?\n\nI would suggest the authors move some derivations to the appendix and add more experimental analyses and discussion about the model. Like what's the trade-off between approximation accuracy, running time, predictive performance etc, like other papers did. \n\nThe current format is not mature enough to get published I think. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents recursive flow relations of preactivation distributions from lower to higher layers and utilizes them for Bayesian inference. The authors have shown that the recursive flow relations can be reduced to analytic form for a series of activation functions.\n\nThe authors have used nearly the entire paper to present the derivations. I would have preferred instead to see part of the space of the paper dedicated to empirically evaluating its predictive capability against other Bayesian deep learning models like deep Gaussian processes; some of the derivations in the main paper can be deferred to the appendix.\n\nThe paper is hard to follow, primarily due to the cumbersome (at times undefined) notations. May I suggest that the authors consider a simpler configuration of the neural network to ease explication and defer a reader to the appendix for a general configuration? Secondly, is it possible for the authors to also convert some of the expressions to the neater matrix/vector form? It also doesn't help to not use numbers to reference equations.\n\nI have some questions below:\n\nCan the recursive relations be evaluated analytically for other activation functions like tanh and ReLU in the general case?\n\nWhat can the authors conclude from Sections 4.3 and C.3 for those results where the theoretical prediction deviates from experimental data? \n\nCan the posterior belief/distribution be expressed in \"actionable\" form like that in Appendix D?\n\nEquation 7: Can the terms within the expectation be expanded? Currently, they seem to imply that there are the same number m of outputs as the number of input data in each layer ell. \n\n\n\nMinor issues\nPage 1: are build upon?"
        }
    ]
}