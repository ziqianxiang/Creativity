{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "In this paper, a domain adaptive object detection model is proposed. It mainly includes two additional components : 1) stacked complementary loss, which includes multiple adversarial losses on intermediate layers, and a instance-context alignment loss at the final layer; 2) a gradient detach strategy to stop the backpropagation to  intermediate layers through the context features. Experiments are conducted on several benchmarks to valid their claims.\n\nThe paper is well written, and easy to follow. It shows good performance on a number of benchmark datasets. Overall, this is an incremental work. While the tricks discussed in the paper sounds reasonable, it provides very limited insights into the domain adaptive object detection problem,\n1) complementary loss: actually, applying adversarial training on multiple levels have been discussed in the literature. For example, in the CAN model [Zhang et al., 2018], they append domain discriminators on different layers, and automatically learn if GRL layer should be activated or not. In DA-Faster-RCNN and SWDA, the authors used two levels of adversarial training. And in the recent MAF work [He and Zhang 2019], they also applied adversarial trainings on multiple levels. From those works, it can be seen that applying multiple adversarial trainings help to improve the adaptation, and carefully tuning the strengths of adversarial trainings would gain further improvements. For example, in CAN, they found the lower layers favors non-adversarial trainings or smaller weights for adversarial loss. Similarly, in SWDA, they observed that a weak alignment at higher layer with FL, and a strong alignment at lower layer with LS gives improvement. This paper also discussed different configurations for loss types on different layers, however, the designs basically followed the above existing works, and the observations are also similar.  \n[Zhang et al. 2018] Zhang et al. Collaborative and Adversarial Network for Unsupervised Domain Adaptation. In CVPR  2018. \n[He and Zhang 2019] He and Zhang. Multi-adversarial Faster-RCNN for Unrestricted Object Detection. In ICCV 2019. \n2) context loss has also been discussed in SWDA, where they augmented the features from discriminators to instance features for a more stable training. In this paper, they used a different way to use the features from different conv-layers, however, this looks quite arbitrary. It is desirable to give more explanations on what motivates authors, and discussion and comparison to SWDA’s strategy are necessary.   \n3) gradient detech has also been exploited. In SWDA, the gradient w.r.t. context features are also detached in the backpropagation for a stable training. However, neither SWDA nor this work present a convincing explanation on why gradient detach should help the adaptation. I noted that authors provided discussions in Section 4, but those observations are trivial. Could you explain how those gradients would demagage the model if no gradient detach is applied, and why? \n\nFurther questions: \n1) Moreover, the experimental results are also inconsistent in some cases. For example, \nIn table 1, ILoss(FL)+detach does not always give improvements. In the first group,  LS+LS+FL+X+X gives 35.0, while LS+LS+FL+FL+detach gives 34.9; in the second group, LS+FL+FL+X+X gives 35.5, while LS+FL+FL+FL+detach gives 35.5; in the last group,  LS+CE+FL+X+X gives 35.3, while LS+CE+FL+FL+detach gives 37.9. It seems the proposed ILoss+detach is quite tricky technique, it works only when they use LS+CE+FL, but performs in an opposite way for other settings. This means people should carefully design their models for using this trick. There are also a few cases of ablation studies missed in Table 1, for example, there is no corresponding “w/o detach version” of LS+FL+FL+FL+detach, making it difficult to valid if the detach generally helps.  I suggest the authors reorganize Table 1, and complete the missing cases, to verify their claims on complementary loss, context loss, and gradient detach separately and thoroughly. \n\n2) Similar ablation studies on other settings like PASCAL VOC->Clip Arts, Kitti->Cityscapes and INIT are needed. Cityscapes -> Foggy Cityscapes is a relatively speicial case, since the foggy images are synthesized images. \n\n3) There are a few works in CVPR 2019 and ICCV 2019, which provides state-of-the-art domain adaptation object detection performance. Those works should be included in experimental comparison, and carefully discussed. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduces a new algorithm for learning to detect objects in two domains (eg. sunny and night images), in which one domain is labelled (object categories and bounding boxes) and the other domain does not have labels. To do so, the paper introduces a neural network trained with multiple loss functions and a modified gradient update similar to (Arpit et al., 2019). The method achieves few percentage better accuracy than state-of-the-art methods.\n\nThe problem of domain adaptation is of crucial importance and seeing improvements is exciting. Yet, I find the following points not convincing: \n\n-Novelty: The algorithm in the paper is strongly based on the network introduced by (Ren et al., 2015) with the domain adaptation strategy of (Chen et al., 2018; Saito et al., 2019). The novelty of adding additional loss functions and adapting the gradient update of (Arpit et al., 2019) seems prima facie incremental. \n\n-Analysis of the approach: It is unclear from where the effectiveness of the approach comes from. Figure 1 visualizes the features in the two domains with tSNE, and the features from this paper and from (Saito et al., 2019) have the same properties. From the figure it can not be concluded that \"our feature embedding results are consistently much better than previous approaches\" as the caption says. The ablation study in Table 1 seems to indicate that the gradient update almost does not bring any improvement and it is unclear what else can be learnt from this analysis. In the text of the paper nothing is mentioned. Finally, the attention maps in Figure 6 show no differences with or without detached gradients, but the description of this figure in the text is not in accordance to what is shown in Figure 6.\n\n-Incremental accuracy improvements: Also, when looking closely to the final accuracies, they are  ~3% larger than the best published results which contradicts one of the conclusions \"Our experimental results outperform the state-of-the-art approaches by a large margin on a variety of benchmarks” (ie. 3% is not a large margin). \n\n-Generality of the approach: The algorithmic improvements presented in this paper are specific for object detection and it is unclear if these ideas would transfer to other domains and how.  In fact, the algorithmic choices are described but not derived from some principles, ie. the paper justifies the algorithmic choices because they are \"simple yet effective” and presents them as a series of steps to follow. This obscures how the approach is connected to previous works and how to use it in new works.\n\nIn summary, the paper tackles an important problem and shows few percentage improvements over state-of-the-art accuracy but the method is based on modifications of previous works that are not well justified.\n\n "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper solves the problem of unsupervised domain adaptive object detection. They proposed a method on top of Strong-Weak alignment (Saito et al., 2019). In this previous work, two losses are proposed, a weak-alignment loss for global features and strong-alignment loss for local features. In addition, the previous work proposed to utilize context-vector for effective object detection. \n\nTheir contributions of this paper are summarized as follows.\n1, adversarial alignment losses for low-level features by adding domain classifier to many low-level layers\n2, adding context features from the low-level features \n3, they proposed to detach the gradient from the context features to avoid a negative transfer.  \n4, the effectiveness of their method is validated through extensive experiments on many object detection datasets. \n\nI lean toward rejecting this paper from the following weakness.\n\n1, Their method lacks technical novelty to accept. Adding many domain classifiers to lower-level layers and adding more context vectors are just minor modifications to Strong-Weak alignment (Saito et al., 2019). In Strong-Weak alignment (Saito et al., 2019), the idea of performing adaptation in multiple-layers and using context-vector are proposed. In Chen et al., 2018, using instance-level features for alignment is proposed. Although the use of these techniques is a slightly different from these existing works,  the difference is too minor. \n2, The presentation is poor, especially in the introduction. For example, they introduced COL in the introduction. However, I could not see a close connection with this paper. In \"Our key ideas\" part, it is also unclear what was the key idea and novel idea. \n"
        }
    ]
}