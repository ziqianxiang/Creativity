{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Overall, I think the authors tackle a meaningful task of translating images when not all modes are available. The introduction, background and related work are adequate. \n\nHowever, I would like to point out that one of the paper's main contributions, namely the content and style separation, was not addressed enough in the body of the paper. The abstract mentions disentanglement of content and style by making an analogy to skeleton and flesh. In the last paragraph of section 2 (right before 2.1), the paper goes on about how content and style can be separated in the current scheme. However, in the evaluation section, there is no evidence that this separation actually happened. There is good amount of result that shows the image translation quality is competitive, in Table 1, 2 and 3, but none of the results suggest that the model actually learned meaningful separation of content and style. For example, I would expect to see how the generated image changes as the style vector is varied, or how the generation quality degrades if the model is trained without the style part. In fact, I almost suspect that the style-content separation is not really happening in the current formulation, because of the following reasons. Hopefully, the authors \n\nFirst, the content bottleneck is very large. In D.2. of Appendix, the size of the content encoder was W/4 x H/4 x 256. The size of the content bottleneck can contain 16 grayscale input images without any compression. Therefore, without additional constraint, the model can store all information in the content code. The only such constraint in the loss formulation seems to be Equation (3) that enforces that a randomly sampled style code is recoverable by E(G(s)), but it seems rather a weak constraint. \n\nSecond, the reconstruction loss may cause the model to be invariant to style. The formulation of the reconstruction loss, specified in Eq (5) is quite confusing, but let me state what I understood. The content code is sampled to be a subset of the content codes produced by E(x1, ..., xn). (It was rather confusing, because it is unclear what p(c) is, and how it relates to xi. Is the content generated from all existing modes of image x? If so, how is the probability distribution p(c) defined? The explanation under Eq (3), \"p(c) is given by c = E(...), and x ~ p(xi)\" needs more clarification.) The style code is randomly sampled from unit normal distribution. Then, no matter which value of style code is sampled, the loss dictates that the generated images should be same as x_i. Wouldn't this formulation make the generator essentially invariant to the style code? I do not see how this can encourage content-style separation. \n\nThird, the content consistency loss (Equation (2)) is somewhat ill-defined, because the loss of Eq (2) can approach arbitrarily close to zero. Since there is no prior distribution assumed on the content code, given E(x) and G(c), E'(x) = 0.1 E(x) and G'(x) = 10 G(c) will show the exactly same behavior. As such, the content and style code can shrink indefinitely without changing the behavior. \n\nTherefore, I think the paper's claimed contribution on content-style disentanglement is a bit overstated with several remaining points of unclarity. \n\nAdditionally, the paper's claim that the proposed method is a generalization of MUNIT and StarGAN because it can perform image translation when variable number of input modes are missing was not extensively verified. All results of the main body of the paper shows the case when exactly one mode of the input images is missing. For missing more than one mode, the only result is on the face dataset in the Appendix. I think this result is rather weak, because it only makes one qualitative result, and the reader would not have a sense for estimating the quality of the translation. In addition, it would be nice if the columns and rows are more clearly labeled. \n\nTo summarize, the paper is tackling a meaningful problem, and I think it could be a solid paper, but it does have major weakness that makes me hesitant to accept the paper. The two main contributions of the method, namely the content and style separation, and generalization of previous image translation method, were not evaluated or justified. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "[Summary]\nThis paper proposes an image translation method that generates images of missing domains from images in multiple available domains. i.e., ReMIC. For achieving this, the authors employ using separated latent codes for content and style from shared representation. They compare the proposed model with StarGAN, MUNIT, CollaGAN on two medical datasets (BraTX, ProstateX) and RaFD datasets, using NRMSE, SSIM, and oracle GT.\n\n[Pros]\n- Task is novel and important, in particular, in medical domains.\n- Experiments are extensive and the results on medical data look promising.\n- Well written.\n  \n[Cons]\n- Technical novelty of the method is incremental. Most components are from previous work.\n- Even if the authors argue that their model uses disentanglement, it is overclaiming that to separate style and content is disentanglement. There exsist several previous studies to separate style and content. To claim disentanglement, it is required that more specific information should be separated from styles such as hair-style and lip color. \n- The experimental setting is not consistent with task definition. Task definition assumes k-missing domains among n domains, i.e, (n-k)-to-k. However, experiments cover k=1 case only: (n-1)-to-1. This setting is similar to that of CollaGAN. I think it is not trivial to extend k to multiple missing domains.  \n- Figure 1 can be improved. StarGAN performs 1-to-n mapping with target style one-hot encoding (The authors also refer to that in page 6), unlike Figure 1. \n- Experimental results on RaFD are not impressive. Could you evaluate the proposed model on high-resolution data such as CelebHQ? Also, for RaFD, other conventional metrics can be used such as FID.\n- A figure on the detailed architecture can help to understand the method. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this work, the authors proposed a deep learning-based method for synthesizing the missing modality/features/domain in input images. The proposed method is based on generative adversarial networks utilizing several loss functions for generating accurate missing modalities. Moreover, they have extended the proposed method for an alternative task, image segmentation. They validated the proposed method on natural images using RaFD dataset for generating the missing facial expressions, and biomedical images using BraTS and ProstateX datasets for generating missing sequences and implementing lesion segmentation.\nThe content of the paper is almost clear and it is well written. However, there are some major comments that I would like to mention. \n\n1.\tThe paper provides very weak literature regarding the deep learning methods for synthesizing missing modality. For instance, one of the first multi-input, single-output (many-to-one) method was proposed by Jog et al. [1]. See more in [3, 4, 6], and especially recent benchmark methods [2, 5, 7].\n2.\tThe authors claim that the proposed method has start-of-the-art performance. However, we found that they did not compare the proposed method with state-of-the-art methods. Please check the latest state-of-the-art in MRI sequence synthesizing [2, 5, 7].\n3.\tEquation 7 is not the Dice loss function [8], instead, it shows Tversky loss function which is slickly different than Dice loss [9]\n4.\tIt is missing a clear justification for not using the weight parameter (lambda) for segmentation loss function. In particular, how they can control the trade-off between segmentation and image generation performances during the training. \n5.\tIt is also missing information about which view of the 3D image is selected to extort 2D slices, as well as the reason for selecting the mentioned view. Moreover, most of the slices, especially in the brain images are without tumors lesions (BRATS). Therefore, when the proposed model is trained based on whole slices, it will have a bias. There is no mention nor justification for this issue.\n6.\tMulti-input multi-output (n-to-n) image translation is not new. See references above, which are related to the same idea [6, 7]\n7.\tIf your method supports n-to-n image translation, regarding the literature, you should show the performance of the proposed method for all possible scenarios as the literature provides. See table 3 in [7] that shows performance in the BRATS dataset.\n8.\tPeak signal-to-noise ratio (PSNR) is another important metric for measuring the performance in synthesizing missing sequences. This measure was not reported in the paper. \n9.\tThe reported results in Table 1 are related to the single modality synthesizing (n-to-1). However, regarding the literature, for correct comparison, you should consider all possible options and then measure the mean value for all reported results. See table 4 in [7]. \n10.\tTo show that your method provides comparable performance in natural image segmentation, Dice measure could be enough. However, in medical image segmentation, having a good overlap between segmentation output and ground truth (Dice) does not imply that your method provides good segmentation performance. In medical image segmentation, specifically lesion like multiple sclerosis or brain tumors, we have to use other metrics like lesion-wise true positive rate, lesion-wise false positive rate, positive prediction value, etc to prove the validity of the proposed method. Such as the metrics mentioned in [10] for MS lesion segmentation. \n11.\tThe results of the methods reported in segmentation parts for BraTS and ProstateX are not related to the state-of-the-art segmentation methods. They are related to some previously proposed methods which are not state-of-the-art. Even if the authors want to concentrate on methods with an initial generating step for missing modality, you should compare the proposed method with the other baseline methods like the ones proposed in [3] and [4].\n12.\tTable 8 (column 3) shows that synthesizing a missing modality and using the generated fake modality combined with all other available modalities for segmentation gives a better performance than using all real modalities in a network like U-net. However, there is not any justification for this strange performance. Why a set of modalities including fake/generated modality can give better segmentation performance than a set including all real modalities? May you need to measure the aforementioned metrics to see whether your method has over-segmented regions or not. If your method creates better Dice measure performance, it does not prove that you have better segmentation performance, instead, you may have very low true positive rate or very high false-positive rate) \n13.\tAn ablation study is needed to prove the impact of the proposed loss functions.\n14.\tThe authors did not discuss the disadvantages of the proposed method.\n\n[1] A.Jog, A.Carass, D.L.Pham, and J.L.Prince, “Random forest FLAIR reconstruction from T1, T2, and Pd-weighted MRI,” in Proceedings - International Symposium on Biomedical Imaging. IEEE, 2014, pp. 1079–1082. \n [2] A. Chartsias, T. Joyce, M. V. Giuffrida, and S. A. Tsaftaris, “Multimodal MR Synthesis via Modality-Invariant Latent Representation,” IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 803–814, Mar 2018. \n[3] M. Havaei, N. Guizard, N. Chapados, and Y. Bengio, “HeMIS: Hetero-Modal Image Segmentation,” in Medical Image Computing and Computer-AssistedIntervention–MICCAI2016. SpringerInternational Publishing, 2016, pp. 469–477. \n[4] T. Varsavsky et al., “PIMMS: Permutation Invariant Multi-modal Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer International Publishing, 2018, pp. 201–209. \n[5] S. Ul et al., “Image Synthesis in Multi-Contrast MRI with Conditional Generative Adversarial Networks,” IEEE Transactions on Medical Imaging, pp. 1–1, 2019. \n[6] R. Mehta and T. Arbel, “RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours,” in International Workshop on Simulation and Synthesis in Medical Imaging. Springer, 2018, pp. 119–129. \n[7] A. sharma., G.Hamarneh, “Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative Adversarial Network”, IEEE Transactions on Medical Imaging, 2019.\n [8] Milletari, F., Navab,N., Ahmadi,S.-A., 2016. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 3D Vision (3DV), 2016 Fourth International Conference on. IEEE, pp. 565–571. \n[9] Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, and Ali Gholipour. Tversky loss function for image segmentation using 3d fully convolutional deep networks. In International Workshop on Machine Learning in Medical Imaging, pp. 379–387. Springer, 2017.\n[10] Carass, A., Roy, S., Jog, A., Cuzzocreo, J. L., Magrath, E., Gherman, A., But- ton, J., Nguyen, J., Prados, F., Sudre, C. H., et al., 2017. Longitudinal mul- tiple sclerosis lesion segmentation: Resource and challenge. NeuroImage 148, 77–102. \n"
        }
    ]
}