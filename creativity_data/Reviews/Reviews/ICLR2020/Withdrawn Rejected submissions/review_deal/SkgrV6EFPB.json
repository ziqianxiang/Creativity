{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\n------------------------\n\nSummary:\n\nThis paper derives a new generalization bound for CNNs that includes a term, NN-mass, based on PAC-Bayesian setup and Network Science. Notably, NN-mass can be calculated from a CNN architecture *without training*. It plots some graphs showing that log(NN-mass) correlates strongly with the test accuracy of CNNs of varying depth. It also proposes new architectures for the CIFAR-10 dataset, and compares its results against DARTS.\n\n------------------------\n\nDecision: Reject.\n\n1) Theory contains a fatal flaw.\n\nMcAllester's Bound only applies to Gibbs classifiers -- every time a prediction is made, a new classifier is randomly sampled from the posterior distribution. (See: The Proof of McAllester's PAC-Bayesian Theorem, M. Seegar 2002.) In other words, Theorem 1 does not bound the generalization gap for individual classifiers drawn from Q; it only bounds the generalization gap for the expected accuracy of a Gibbs classifier defined by the distribution Q. I didn't check the proof for Theorem 2, but even if the steps are correct, Corollary 1 interprets f_Q^L and f_Q^S as individual models drawn from Q, which is incorrect.\n\n2) Experiments do not directly support the generalization bound in Corollary 1.\n\nCorollary 1 is not about the relationship between log(NN-mass) and test accuracy; it is about the relationship between (generalization gap of deep model) - (generalization gap of shallow model) and the square root term. Unfortunately, data augmentation complicates this formula. For a proper evaluation, the models should not be trained with cutout. Besides, figure 10 consists of trend lines with just 20 data points, which is not very convincing. It would be much more convincing if this paper had a Q-Q plot of generalization error against NN-mass over a dataset of ~100 data points.\n\n3) Empirical study does not verify whether different optimization and regularization settings would affect the results.\n\nRecent advancements in the performance of image classifiers came not only from architectures; it also comes from innovations in optimization (e.g., batch norm, learning rate scheduling) and regularization (e.g., cutout, drop path). A training-free architecture search procedure thus faces this concern: if we change these training hyperparameters, will the results still hold? Thus, experiments to confirm Corollary 1 should be repeated with/without cutout, with/without batch-norm, and with different learning rate schedules.\n\n------------------------\n\nNovelty: Applying Network science to the study of generalization of untrained CNNs seems novel to me.\n\n------------------------\n\nClarifications:\n- Would this theory predict that DenseNets with a large number of channels have superior generalization performance compared to ResNets? If so, why are current state-of-the-art ImageNet models more similar to ResNet than DenseNet?\n\n------------------------\n\nOther issues:\n- Why does Table 1 choose DARTS, an architecture for which the NN-mass cannot be computed? Why not compare against DenseNet?\n\n------------------------\n\nMinor issues:\n- page 1, \"In contrast, traditional wisdom suggests that models should overfit when the number of parameters is much larger than the dataset size (which is true in practice).\" It should be \"(which is *not* true in practice)\"?\n- page 2, s/Densenet/DenseNet\n- page 6, Remark 1: \"Theorem 2 guarantees that the test error should reduce as NN-Mass increases\". This overstates what can be deduced from a PAC upper bound; an upper bound alone can *suggest* a relationship, but it cannot guarantee anything because we don't know if the bound is looser at lower values of NN-Mass.\n- page 6, Corollary 1: f^S_Q and m_S correspond to the shallower network, and f^L_Q and m_L correspond to the deeper network. Why not use D instead of L to label the deeper network?\n\n------------------------"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors analyze NN architectures from the perspective of network sciences and propose markers that correlate architectures and generalization performance. In addition to providing theoretical insight, these markers (e.g. NN-Mass) can then be used to help direct neural architecture search.\n\nGraph theoretical analysis of neural network architectures is conceptually appealing and will hopefully prove useful for improving topics such as NAS. Unfortunately, the authors' presentation does not instill the reader with confidence regarding their arguments. \n\nIdeally, the authors would start by analyzing simple multilayer perceptrons with random skip connections as a base case and derive theoretically sound principles from there. Instead, we jump directly into DenseNets. Similarly, the NN-density and NN-mass metrics (both of which are central to the piece) are introduced with little in the way of content or explanation. \n\nGiven the repeated reference to small-world networks, these choices of metrics seem unusual. Alternative characteristics, such as clustering coefficients and statistics regarding the distribution of geodesic path lengths, would seemingly be more natural. Together, these peculiarities make the work feel rushed and, at times, superficial. As a non-expert in topics such as network science, I fully admit that I could be mistaken here.\n\n\nQuestions:\n  - What is the purpose of \"contribution probabilities\" $\\alpha_{i,j}$ and in what sense are they \"probabilities\"?\n  - In Sect 4.2, how were clusters assigned?\n\n\nNitpicks, Grammar, and Spelling:\n  - I suggest using italics much more sparingly.\n  - The authors spend an inordinate amount of time pointing out things that they are the first to do and/or that related works did not do. This repeated exercise does not reflect well on them.\n  - Please explain how clustering was performed (shown in purple boxes and ellipses)\n"
        }
    ]
}