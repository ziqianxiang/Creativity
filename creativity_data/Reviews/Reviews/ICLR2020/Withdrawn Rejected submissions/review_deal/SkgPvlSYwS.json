{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Overview\n\nIn this paper, the authors proposed a new version of graph convolutional network for visual question answering. To enable the “reasoning” ability for visual question answering model, the authors proposed a new module called reasoning-awere GCN (RA-GCN). In RA-GCN module, it has a graph learner to learn the connections for visual graph extracted from the image. Then, a conditional graph convolution is performed to modulate the node features in the visual graph based on the question and then update the feature for each node specifically based on the edge context. Based on this RA-GCN module,  the authors further perform an iterative process to perform visual question answering. In the experiments, the authors worked on three datasets, VQA-CP, GQA and CLEVR. On these three datasets, the proposed method achieves better or comparable results to previous SOTA. A detailed ablation study further helps to understand the RA-GCN model more closely.\n\nPros:\n\n1. The authors proposed a new module called RA-GCN for visual question answering. In this new module, the authors used a graph learner to learn the structure of the visual graph, and then used a conditioned convolution to update the node features in the graph.\n2. In the experiments, the authors evaluated the proposed model and previous model on three datasets and achieved better or comparable performance compared. The ablation studies further helped to understand the model more clearly\n\nCons:\n\n\n1. It is not very clear why the proposed model is called reasoning-aware GCN. Though the proposed RA-GCN module perform message passing across graph conditioned on language, i do not think it has learned to perform reasoning for visual question answering. In this sense, the claimed contribution is a bit exaggerated.\n\n2. The difference between the proposed model and other graph-based visual question answering model is not very clear to me. The graph learner in RA-GCN is similar to the dynamic connections learned in LCGCN, while the conditioned convolution is also used on LCGCN.\n\n3. The motivation for RA-GCN is not clear to me. First, why the authors used a graph learner to prune the edges in the graph. Pruning the graph seems not reliable without any supervision on the connections. Second, why the authors used a node-specific updating by introducing a edge tensor. As it is in GCN, a straightforward way is directly regard the edges as nodes in the graph and then perform graph convolution to propagate the context from edges to object nodes. Also, the description of RA-GCN module is vague at some points. First, it is not clear what the Fuse function is in graph learner. Second, it is also not clear how to get the tensor P for node-specific graph convolution. \n\n4. In experiment, it is shown that the proposed model either outperforms or underperforms previous methods. On VQA-CP v2, all models achieved close performance. On GQA and CLEVR, the proposed model is worse than state-of-the-art. Based on the above comments and the experimental results, it is hard to justify the novelty and significant contributions behind the proposed model. What are the main merits in the proposed model is not clear to me.\n\n5. Finally, it would be great if the authors can present some visualization results for the proposed model, showing how the graph is pruned and how the node feature updating is modulated. These would help the reader to understand the “reasoning” process behind the model.\n\nConclusion:\n\nIn this paper, the authors proposed a new module called reasoning-aware GCN for visual question answering. It is reasonable to use question to modulate the graph convolution. However, from the method description and the experimental results, it is hard to verify the novelty and significant contributions behind the proposed model. I would suggest the authors explain the motivations behind the model and explain it more clearly. Also, the authors should present some visualization results for the VQA task.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary:\n\nThe paper discusses relational reasoning for visual question answering, e.g. encode image-region-pairs conditioned on relevancy to question.  The paper approach is novel: The use of FiLM architecture to condition relevancy to question. The decoding is done using a multi-step graph reasoning directly pooled from the graphs. The model is evaluated on several datasets (e.g., GQA, CLEVR, VQA-CP)\n\n\nStrengths: \n\n    1. The model is novel. Although follows  Norcliffe-Brown et. al.; The proposed approach adds a multi-step graph reasoning and also replaces the idea of attention mechanism for question conditioning with FiLM. \n\n    2.  Evaluation includes major datasets. Model achieves SOTA performance in VQA-CP,  and performs well in other major datasets, e.g. GQA and CLEVR.\n\n\nWeakness: \n1. I find it peculiar to not include VQA2.0 dataset. \n2. Ablation study is  weak, several new concepts are proposed, but there is no ablation analysis. Does FiLM is the best approach for conditioning over a graph? How is it compared to attention, or other types of Hypernetworks? Does the old approach of graph as image representation is wrong? Currently,  it’s not clear what is the direct cause for performance boost on VQA-CP for instance.\n3. Qualitative evaluation is extremely important for this type of paper, observing differences between question-condition techniques. Here it is completely missing.\n\n \nTo conclude: The proposed architecture is intuitive, and more dependent  on  the relation graph. Unfortunately a good study is missing (e.g., ablation/qualitative), which lowers the quality of the paper.  \n\n\nTypos:  espressive -> expressive , also in the end of the paragraph the Section missing a number.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a reasoning-aware graph convolutional network for visual question answering. By incorporating feature-wise linear modulation to GCN, the proposed method performs visual reasoning with respect to the questions. Experimental results on several VQA benchmarks are reported and discussed.\n\nPros.\n1. Visual reasoning is an important and challenging research problem. This paper studies how to equip GCN with the visual reasoning capability.\n2. Three benchmark datasets are employed in the experiments. Details on experimental settings are also provided.\n\nCons.\n1. The major technical contribution of this paper might be the integration of FiLM and GCN. A similar idea has been discussed in [a]. \n[a] GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation. arXiv, 2019.\n2. Some technical details should be clarified. For instance, In Section 3.2, the node features v_i and v_j are fused using function Fuse(v_i, v_j). It's unclear what is the Fuse function.\n3. In experiments, important baselines are missing, such as [b-c]. Also, for VQA-CP v2 dataset, it would be helpful if the authors can show the detailed results for each category of questions, such as Yes/No, Num, and Other.\n[b] Self-Critical Reasoning for Robust Visual Question Answering, arXiv, 2019.\n[c] Overcoming Language Priors in Visual Question Answering with Adversarial Regularization, NeurIPS 2018."
        }
    ]
}