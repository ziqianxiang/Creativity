{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper compresses by 4x the # parameters and reduces latency of BERT by 4x using a combination of the following ideas:\n1. training a BERT like teacher model with inverted bottlenecks and a MobileBERT student model with bottlenecks in each layer so that within the MobileBERT model we reduce the number of parameters and computations by reducing width, but at the beginniong and end of each layer the widths and attention is identical. Thus the student can be trained one layer at a time to mimic the teacher model.\n2. Bottom to Top progressive feature map transfer and attention transfer\n3. Further fine tuning of the Mobile BERT on the training corpus\n4. increasing the number of feedforward layers following each multi-headed attention  layer\n5. replacing the layernorm with an element wise normalization, and the gelu with Relu activation function. \n\nThrough the above relative simple steps the authors pre-train a generic model which can then be fine tuned just like the original BERT on any task with a limited amount of training data. \n\nEmpirically their accuracy on GLUE and SQuAD1.1 benchmarks is similar to the BERT_{base} model though they use 4x fewer parameters and have 4x less latency. They are also significantly better than Sun et al (2019) whose approach also compresses BERT. \n\nOverall I think the paper is well motivated and well explained. Its experimental analysis is sufficient. I like the paper quite a bit in terms of its likely impact on practitioners (eg if distributed as part of the transformers package at https://github.com/huggingface/transformers). However I am a bit concerned about the relatively limited novelty. All the key ideas introduced here can be directly traced back, and I am struggling to point to some unique novel contribution here other than combining them together. Further they have not compared against DistilBERT which is also at least as good in terms of results. Finally there is no ablation study for the impact of the inverted bottleneck. For all these reasons the paper seems to not be fully ready for publication yet.   "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents an interesting knowledge distillation method MobileBERT, which is a specially designed method to compress and accelerate large scale pre-trained language model. \n\nThe main contributions include: the progressive knowledge transfer with considering feature map transfer and attention transfer, and the new architecture design with introducing the Bottleneck and InversedBottleneck-modules into the student and teacher networks respectively. \n\nThis KD method is proposed based the following three constraints, the teacher BERT-large and student MobileBERT should have the same feature map size, the number of layers, and the number of attention heads. So the small student model should have the same number of layers as its teacher model. The reduction of model size mainly comes from the smaller hidden size.\n\nThe authors evaluate their approach on GLUE and SQuAD datasets and compare it to other state-of-the-art models.\n\nThe paper is well-written and organized, I have several concerns:\n\n* Progressive knowledge transfer problem: the progressive knowledge transfer process as shown in the left panel of Fig.2 is one of the main contributions of this work, while I cannot find any experimental analysis on its importance in comparison with jointly training the losses from all the layers.\n\n* Number of layers problem: The compression of MobileBERT mainly comes from the reduction of hidden size and it is still a 24-layer deep model. However, as stated in the blog of describing the DistilBERT, “the number of layers was the determining factor for the inference time, more than the hidden size”, especially when executing on GPUs, the proposed MobileBERT has the limitations of having the same number of layers as its teacher, is it possible to reduce the number of layers of MobileBERT? \n\n* Baseline: In the Table 3, it is better to add the results from teacher network IB-BERT_large, the student should be directly compared to its teacher, but not the BERT-base. \n\n* In the Fig.1(b), in contrast with the standard transformer layer (as shown in the Fig. 1 (a)), a shortcut connection is introduced from the input embedding layer to the top Add&Norm layer, are there some insights behind this architecture design? Or is this a widely adopted design of bottleneck module?\n\n* In the equation (1), the layer normalization is added to H_{t,l}^{tr} to stabilize the training loss, and in the Fig.1(b)(c) the feature map transfer is illustrated as the outputs from a Add&Norm layer, does the H_{t,1}^{tr} denote the output from the Add&Norm layer? or does this mean we need to do layer normalization again on the outputs from a normalization layer? \n\n* In the section 4.1, to align the feature maps from teacher and student, we may only use a linear transformation to reduce the hidden size of teacher to the student model, without introducing the B or IB module, is this a reasonable method? Do you once try this method?\n\n* In the section 4.3, which operational optimization is more effective, removing layer normalization or using relu activation? And only using the relu activation will also hurt the performances? "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors tackle the problem of compressing and accelerating the popular BERT model for representing text, and propose a layerwise feature distillation approach for the task (progressive knowledge transfer), and investigate several transfer objectives (self-attention transfer, std. distillation). In contrast with most other current work, their focus is on producing a task-agnostic model, that is, a compressed pre-trained model trained for finetuning on simple downstream tasks via the self-supervised BERT training objectives (masked language modeling, MLM, and next sentence prediction). Thier model, MobileBERT, is about 4x smaller, faster than the base BERT model, and outperforms it.\n\nStrengths:\n-Clear presentation. Good results. Most elements ablated. Important problem to NLP community.\n\nLimitations:\n-The technique advertised in the title (progressive knowledge transfer) is apparently not ablated! The feature distillation/matching is, and plays an important role, but PKT specifically apparently is not, and it is not at clear that it should be neccessary...\n-DistilBERT was just recently published/released, and is mentioned in the appendix, but is not compared to. It is a simpler model/approach, and produces similar results and speed/compression gains. \n-Quantization and pruning are left to future work.\n-The model is apparently not being released. \n-On the lower side wrt ML novelty, standard techniques.\n\nOverall:\n\nAn okay paper. Comparisons with DistillBERT and model release would make the contribution more significant. Lower novelty, advantage of progressive knowledge transfer not clear. Current evaluation is weak reject.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nThis paper proposes MobileBERT, a compressed version of BERT. MobileBERT achieves comparable performance as BERT_BASE while being 4X smaller, which is accomplished through careful engineering design of the network structures and a bottom-up progressive knowledge transfer approach.\n\nI recommend for acceptance, as the proposed MobileBERT successfully reduces the computation cost and model size of BERT, without sacrificing the performance (when compared to BERT_BASE). The paper provides an efficient model architecture for BERT compression, and also an approach to train the student network layer by layer in a bottom-up way. Compared to previous work on model compression of BERT, the proposed MobileBERT is more generic -- once pre-trained, it can be simply fine-tuned on down-stream tasks like the original BERT, without requiring a teacher model. It enables the efficient application of BERT to a wide range of NLP tasks.\n\nHowever, I would also like to note that the main contribution of this paper lies in its engineering efforts -- accomplishing a task-independent compressed version of BERT with good performance. On the other hand, the proposed knowledge transfer (distillation) approach is rather unsurprising."
        }
    ]
}