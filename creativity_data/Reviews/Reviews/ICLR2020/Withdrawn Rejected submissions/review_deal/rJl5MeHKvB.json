{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper addresses  an important problem of self-supervised learning in the context of time-series classification. However, all reviewers raised major concerns regarding the novelty of the approach and the quality of empirical evaluation, including insufficient comparison with the state-of-art and reproducibility issues. The reviewers agree that the paper, in its current state, does not path the ICLR acceptance threshold, and encourage the authors to improve the paper based on the provided suggestions.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes the use of many auxiliary tasks to boost the performance on a target task by means of `'self-supervision'. Specifically, they considered auto-encoding, forecasting, partial-segment auto-encoding, and piecewise-linear auto-encoding. \n\nThere are major concerns that should be clarified or described in detail.\n1) The overall architecture is not complete. the architectures used in the experiments are not described concretely.\n2) To this reviewer, the idea of self-supervision is similar to the unsupervised learning for representation learning. \n3) The methods of BERT (Bidirectional Encoder Representations from Transformers) [Devlin et al., 2018] or BRITS (Bidirectional Recurrent Imputation for Time Series) [Cao et al., 2018], although different for their target tasks in their original work, could be also regarded as self-supervision technique and could be interesting to compare with them.\n4) The experimental settings are not described well, thus lack of reproducibility\n5) It is unclear which aux-tasks were applied in Fig. 2. Further to better understand and analyze the results, it is required to conduct more rigorous ablation studies.\n6) There is no comparison with recent work on the same datasets."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper propose an approach for self-supervised learning on time series.\nThree datasets are considered (simulation and 2 healthcare datasets).\nThe gist of the contribution is to both optimize prediction loss\nof the true task and at the same time do a good job for a family\nof auxiliary tasks. 4 auxiliary tasks are considered. While\nthe first 3 auxiliary tasks are quite common, the 4th tasks\ncalled piecewise-linear autoencoding appears novel. The idea\nis that the hidden representation of the LSTM should be a good predictor\nof the past using a piecewise-linear approximation.\nThe author coin the term \"limited self-supervision\" for their approach\nalthough it's not clear why it is fundamentally not just self-supervised\nlearning as it has been proposed in the past.\n\nThe paper is overall well written and addresses the relevant issue\nof learning from limited annotated data.\n\nMajor concerns\n\n- It is yet another way to do self-supervised learning on time series\nand no clear benchmark with alternatives is provided (time contrastive\nlearning (TCL) or Contrastive Predictive Coding (CPC) https://arxiv.org/pdf/1807.03748.pdf\netc.)\n\n- On any of the applied problem it is not clear if the proposed\napproach brings an improvement on the state-of-the-art or if it's\njust an illustration of the method disconnected from the literature\nof the application.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a so called self-supervised method for learning from time series data in healthcare setting. Specifically, here self-supervision is achieved via designing auxiliary tasks based on data's internal structure to create more labeled auxiliary training tasks.\n\nFrom both perspectives of methods and applications, the proposed model has very limited novelty. It is just one application of multitask learning. Also very similar idea has been implemented by [1]. In [1], the authors learn multi-level embedding to make disease/risk prediction, where the embedding was jointly trained by performing auxiliary prediction tasks that rely on this inherent EHR structure. The authors need to state what is the novelty of the proposed method compared with [1].\n\nIn addition, the performance evaluation missed many baselines. Table 1 seems more like a ablation study rather than a performance comparison. You need to compare with all state-of-the-art models in computational phenotyping in order to show the performance advantage brought by the proposed mode design.\n\n[1] Edward Choi, Cao Xiao, Walter Stewart, Jimeng Sun, MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare,  NeuRIPS, 2018\n\n "
        }
    ]
}