{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This work presents a neat discussion on how to mitigate the huge cost of lottery ticket training. Specifically, it finds a setting that produce boosting tickets which can converge faster than lottery ticket. Moreover, it proves that pruning after FGSM and PGD can produce nearly same boosting ticket, which will largely save the adversarial training time. \n\nDespite the above insights, I still have several reservations.\n\n1) In the paper it says “We observe the standard technique introduced in Frankle & Carbin (2019) for identifying winning tickets does not always find boosting tickets.” It will be very depressed to see the boosting ticket only exists in small learning rate since small learning rate can only produce a weaker performance compared to lr=0.1 (roughly 92% vs. 93%). I still want to see results of the same setting (warmup, epochs) except changing learning rate to 0.1. The author should show some bad case under large learning rate setting.\n\n2) The boosting tickets are only compared with winning ticket rewinding to epoch=0 (use the initial weight). Actually some work [1][2] has shown that winning ticket rewinding to epoch=k (using the weight after training k epochs) can outperform the original weight. Rewinding to k epoch has a benefit that the pruned model do not need to train such a long time as the inherit weight already have some knowledge. This is also another “boosting” effect. I expect the author to compared the boosting ticket with wining ticket under the setting that rewinding to k epoch (k is set to different values like 5, 20, 60, and final epoch). When rewinding to k epoch, the learning rate scheme should also follow that one. \n\n3) Pruning ratio is a important setting to winning ticket and boosting ticket. However, Sec. 3.3 could not give a direct conclusion on how the pruning ratio affects the boosting ticket. Also, the pruning ratio of experiments on Figure. 2 is not clearly explained. I expected to see the final accuracy of boosting tickets that pruning ratio equals to 30%, 50%, 70% and the same pruning ratio while using random weight.\n\nConsidering the rebuttal time is limited, the rewinding experiment in (2) should at least give the results when k=60 and being equals to the final epoch, and also give the whole training time of these settings (the searching time plus the time of training the subnet for both wining ticket and boosting ticket while k is set to different values.)"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents some follow-up work on the lottery ticket hypothesis (LTH). The iterative magnitude-based pruning (IMP) used in the original LTH paper by Frankle and Carbin (FC19) failed to find winning tickets on large-scale vision networks without modifying the training procedure. This paper describes a change in the learning rate schedule that allows the authors to find subnetworks that outperform randomly reinitialized winning tickets. The authors name these subnetworks “boosting tickets”, and demonstrate that boosting tickets train faster. The authors then use these boosting tickets to speed up adversarial training. They propose to use FGSM-based adversarial training to find the boosting tickets (since this part is computationally intensive) and then train these obtained boosting tickets using the more expensive and better performing PGD-based adversarial training. \n\nThe idea of using winning tickets for adversarial training is quite interesting. However, given the speedup times (49%), the simplicity of the idea (combining two existing things, adversarial training and pruning via IMP) and the limited number of experiments, the overall contributions seem to be minor. The findings presented regarding the slight change in the learning rate schedule are not very novel, nor give any new surprising results. A lot of the things that are presented in the paper also appeared in the original paper FC19. Further, it looks like the authors misunderstood what the LTH says and what the definition of a winning ticket is (see below).\n\nOverall, the paper presents very incremental and low-impact work. The experiments should be expanded considerably (and corrected accordingly, given the correct definition of a winning ticket) in order for me to recommend acceptance.\n\nDETAILED FEEDBACK\n\nWINNING TICKETS, Section 3: “In particular, we show that boosting tickets are winning tickets, in the sense that they outperform the randomly initialized models”. Based on this sentence, I believe that the authors seem to think that a winning ticket (subnetwork + initialization) is the one that outperforms the same subnetwork when it is randomly reinitialized. However, this is incorrect. Frankle and Carbin define it as a subnetwork+initialization that can be trained at least as fast to the same or higher accuracy as the original (!) network.\n\nTRAINING TIME. FC19 (v3 on arxiv) contains graphs demonstrating that winning tickets train faster. This submission does not have an explicit comparison how their new learning rate schedule affects the performance of winning tickets (!) compared to the learning rate warmup done in FC19. \n\nRANDOM TICKETS. There is no experimental evidence that boosting tickets do much better than random subnetworks when trained in an adversarial way (potentially even more impressive computational savings...).\n\nLEARNING RATE SCHEDULE, Section 3. The only difference in the learning rate schedule is that a different schedule is used for pruning and for training. One of the key properties of the LTH is that the subnetworks  found via IMP can be trained under *the same* training procedure. In my opinion, introducing some unprincipled/theoretically unjustified learning rate tricks has none to little impact on future research.\n\nSCALE, Introduction: “Although FC19 show that winning tickets converge faster than the full models, it is only observed on small networks, such as a convolutional neural network..”. While it is true that in FC19 the experiments were performed on relatively small networks, Frankle et al. 2019 have another paper containing experimental results on large-scale networks (all the way up to inception on imagenet). Further, I would like to highlight that this submission only contains experiments on wide Resnets and VGG-16. If one of the suggested contributions is doing IMP at scale, why are the empirical results limited to these relatively small networks? \n\nPRUNING RATIO, Section 3.3. It looks like the pruning ratios are tested only for one shot pruning. Could the authors please explain their choice, and elaborate on the reason for this particular experiment.\n\nUNSTRUCTURED PRUNING, Section 2: “One of the limitations of the LTH…winning tickets are found by unstructured pruning.” I cannot see how the submission addresses this. Same holds for the rest of the paragraph.\n\n\nOther minor comments:\n\n- “We observe the standard technique introduced in FC19 for .. does not always find boosting tickets”. I am not sure how this is different from what was already pointed out in FC19, and “we observe” suggests that the contribution of observing was done by the authors in this submission.\n - “boosting effect” in the third paragraph in the introduction is undefined.\n - none of the plots contain error bars.\n - In section 5 under “Accelerate adversarial training” a future research direction is suggested: combine “recycling the gradients” idea with the idea proposed in this paper to speed up overall training time. This seems like a trivial application rather than a research idea. What did the authors have in mind here?\n - Calling the model “Madry’s” in all the tables seems extremely unfair to all of his (more junior) collaborators. \n - The comment under Figure 5, “While a wider model always boosts faster..”, seems unsupported by the figures."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper build on the recent Lottery Ticket Hypothesis described by Frankle and Carbin. They focus on a subset of the lottery tickets that they name “boosting tickets”. They argue that these boosting tickets converge faster than the original model and the other lottery tickets. They take advantage of this faster convergence to efficiently train model robust to adversarial perturbation. This application is a good idea and some results are encouraging. However, it is my opinion that the paper in its current form is difficult to follow and that some claims would gain to be justified more thoroughly. \n\nIn particular:\n- The difference between lottery tickets and boosting tickets is unclear. The reader struggles to find any clear definition of “boosting tickets”, besides lottery tickets that converge faster than others. \n- The procedure used to find these boosting tickets (and separating them from the other lottery tickets) is unclear too. The second paragraph in section 3.1 suggests that the main difference between the author’s strategy to find the boosting tickets and the original work lies in the learning rate warm-up. However this warm-up is used only for the retraining once the model is pruned, meaning when the lottery ticket is already found.\n- No theoretical justifications are suggested regarding the existence or the faster convergence of the boosting tickets. Although I agree that a purely empirical work can have a lot of merit, experiments on only a couple of toy datasets, without theoretical justifications, seems not enough to convince of the existence of these boosting tickets.\n\nI’m not an expert on adversarial training so I will let the other reviewers comment on this part.\nLeveraging the performance gain of the lottery tickets to more efficiently train models robust to adversarial perturbations is a great goal. I think that this paper would benefit a lot from extending this part, and dropping the first part on the boosting tickets. It was already noted by Frankle and Carbin that the lottery tickets can be retrained faster than the original model, which seems sufficient for what the authors want to accomplish.\n\nAll in all, I think this paper is not good enough in its current form.\n"
        }
    ]
}