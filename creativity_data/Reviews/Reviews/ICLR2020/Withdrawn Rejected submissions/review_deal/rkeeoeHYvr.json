{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a tree-structured autoencoder.\n\nI side with the two more confident reviewers, and argue that this paper doesn't offer sufficient evidence that this method is useful in the proposed setting. I'm particularly swayed by R1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. I'm also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a tree-structured decoder encourages the model to generate grammatical sentences—I see no reason why this should be the case in the setting described here, and the paper doesn't seem to offer evidence to back this up.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Motivated by recent development of attack/defense methods addressing the vulnerability of deep CNN classifiers for images, this paper proposes an attack framework for adversarial text generation, in which an autoencoder is employed to map discrete text to a high-dimensional continuous latent space, standard iterative optimization based attack method is performed in the continuous latent space to generate adversarial latent embeddings, and a decoder generates adversarial text from the adversarial embeddings.  Different generation strategies of perturbing latent embeddings at sentence level or masked word level are both explored. Adversarial text generation can take either a form of appending an adversarial sentence or a form of scattering adversarial words into different specified positions. Experiments on both sentiment classification and question answering show that the proposed attack framework outperforms some baselines. Human evaluations are also conducted.\n\nPros: \n\nThis paper is well-written overall. Extensive experiments are performed.\n\nMany human studies comparing different adversarial text generation strategies and evaluating adversarial text for sentiment classification/question answering are conducted.\n\nCons:\n\n1) Although the studied problem in this paper is interesting, the technical innovation is very limited. All the techniques are standard or known. \n\n2) There are two major issues: lacking a rigorous metric of human unnoticeability and lacking justification of the advantage of the tree-based autoencoder. I think the first issue is a major problem that renders all the claims in this paper questionable. The metrics used to define adversarial images for deep CNN classifiers are indeed valid and produce unnoticeable images for human observers. But in this paper, the adversarial attack is performed in the latent embedding space, and there is no explicit constraint enforced on the output text. It’s unconvincing that this approach will generate adversarial text that seems negligible to humans. Therefore, the studied problem in this paper has a completely different nature from the one for CNN image classifiers and it is hard to convince readers that the proposed  framework generates adversarial text legitimate to human readers. \n\n3) It is unclear why tree-structured LSTM instead of a standard LSTM/GRU should be chosen in this framework for adversarial text generation. If this architecture is preferred, sufficient ablation studies should be conducted.\n\n4) In section 3.3, the description about adversarial attacks at word level is unclear. More detailed loss function and algorithms along with equations should be provided.\n\n5) In section 5.2, it is unclear that the majority answers on the adversarial text will, respectively, match the majority answers on the original text. Moreover, it seems that there is a large performance drop from original text to adversarial text. Therefore, it is valid to argue that whether the proposed framework can generate legitimate adversarial text to human readers or not.\n\n6) It’s better to include many examples of generated adversarial text in the appendix.\n\n7) Missing training details: It is unclear how the model architectures are chosen, and learning rate, optimizer, training epochs etc. are also missing. All these training details should be included in the appendix.\n\n8) Minor: Figure 1: \"Append an initial sentence...\",  section 3: \"map discrete text into a high dimensional...\",  section 3.2.2: \"Different from attacking sentiment analysis...\" ....\n\nIn summary, the research direction of adversarial text generation studied in this paper is interesting and promising. However, some technical details are questionable, and the produced results without rigorous metrics seem to be unconvincing. \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a new attack framework AdvCodec for adversarial text generation. The main idea is to use a tree-based autoencoder to embed text data into the continuous vector space and then optimize to find the adversarial perturbation in the vector space. The authors consider two types of attacks: concat attack and scatter attack. Experimental results on sentiment analysis and question answering, together with human evaluation on the generated adversarial text, are provided. \n\nOverall, this paper has a nice idea: use tree autocoders to embed text into vector space and perform optimization in the vector space. On the other hand, it is not clear to me why the proposed method would not change the ground truth answer for QA. Currently the authors claim to achieve this by carefully choosing the initial sentence as the initial point of optimization, which seems a bit heuristic. The authors could add more discussion on this and more experimental results to justify this claim. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposed a new adversarial text generation framework based on tree-structured LSTM. Compared with two existing methods, the proposed method gives better successfully attacking rates. The tree-structured LSTM model is an existing work but applying it to generate adversarial text is new. \n\nThe difficulty of generating good adversarial text lies 1) high success rate and 2) the generated texts are reasonable (e.g. syntactically correct) and are not contradictory to the original texts. The paper achieves good success rate based on its experimental results but doesn't convince me that 2) is also guaranteed. The paper mentioned that human can ignore irrelevant tokens added by the proposed scatter attack method but it is an extra assumption added to the grammatical correctness. The classification model was trained on texts without these randomly added tokens or typos. In the results, I saw the scatter attack was applied to sentiment analysis but not QA tasks. Is this method not effective to attack QA task?\nAlso, the paper reports the human evaluation on adversarial texts which shows accuracy degradation and low votes. Ideally, the human accuracy on adversarial texts should also be compared to justify 2). More examples can be added to reduce \"noise\" mentioned in the paper. And, the paper can be improved by adding more details on training and optimization.\n\nSome extra questions and comments\n1. in figure 1, will you encode the original text along with the appended sentence into one vector? then, how do you guarantee that the perturbation only applies to the appended sentence but not the original text for the ADVCodec(sent)? or the original text will be reproduced due to the autoencoder?\n2. it will be helpful to add more details on training and optimization. For example, is the autoencoder trained by the authors or is from the existing model? what does the confidence score in (5) means empirically and how to choose its value? "
        }
    ]
}