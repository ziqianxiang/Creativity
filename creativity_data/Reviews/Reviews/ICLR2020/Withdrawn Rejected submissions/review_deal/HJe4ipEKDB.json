{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "### Summary\nThis paper proposes a new framework to generate video frames from a single motion-blurred image. While prior work [1,2] has dealt with this problem, they can suffer from error compounding as they they first predict middle frame and then use it for other frames.\nTo this end, key insight in this paper is to learn a separate spatial transformer network to produce each output frame. This removes the dependence on middle frame reconstruction quality, but assumes a fixed number of frames due to lack of recurrence. Experiments show better results than [1], but lack any comparison with [2].\n​\n​\n### Strengths\n- The synthetic method to generate rotational blur from panaromic images can be reusable for future work in this domain.\n- The network architecture look intuitive for a fixed-size video prediction. The proposed losses also make sense at a high-level, and are shown to be helpful through ablation studies.\n- Ablations for various components of the proposed method are well done - in terms of network architecture and loss components proposed.\n​\n### Weaknesses\n- **Inconsistent and hard to follow math in the paper**: e.g. k is used for temporal dimension in Fig. 2, and scale in Eq. 3; w_l in Eq. 3 for multi-scale photometric loss is not described at all. Similarly the dual subscripts in Eq. 3 are not well-defined at all, making an otherwise easy equation very hard to understand. The meaning of predicting different \"scales\" is also not clearly defined anywhere and is left to user to interpret - they are used interchangably with multi-level features (e.g., in Fig. 2) making things very confusing.\n​\n- **Lack of Pairwise terms in losses**: It seems more intuitive to have pairwise losses for transformation consistency loss (i.e. temporal consistency across *all* scales) and penalty term (i.e. penalizing similar frames across all generated frames). Especially for the penalty term, can the authors explain the reason for choosing penalty in symmetric manner? It seems like an arbitrary design choice. It is also unclear how temporal consistency loss is making the model design symmetric around middle frame (as mentioned in first line on Page 7)?\n​\n- **Lack of comparison with [2]**: This is the reviewer's primary complaint with the paper. The authors mention that [2] have no open source code, so they did not compare any results. However, the high-speed video dataset used is exactly the same in [2], and they provide many metrics in their paper. In fact, [2] also show many improvements over [1], which is the main comparison in this paper. So without comparing with [2], it is very hard to assume that the problem this paper tries to solve even exists - whether recurrent dependence on middle frame is bad for video restoration. Therefore, the reviewer recommends to compare with this more recent paper [2]'s results in the form it is present in that paper.\n​\n- **Using 10 pages**: While the reviewer appreciates the ablation studies and qualitative results mentioned in the paper, there are many architectural and data-processing details that would be more suitable for Appendix section, hence reducing the main paper length to 8 or 9 pages for the interest of readers.\n​\n​\n​\n#### Minor and nits:\n- From abstract and introduction, it seems that restoration of video frames from single blurred image is a new problem proposed by this paper. Correct word choice should be used to mention the precise way this work differs from prior work.  \n- Page 3, \"Later\" is misspelled.\n​\n​\n### References\n[1] Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to extract a video sequence from a single motion-blurred image. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.  \n[2] Kuldeep Purohit, Anshul Shah, and AN Rajagopalan. Bringing alive blurred moments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6830–6839, 2019."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "OVERVIEW:\nThe authors present a framework to generate video frames from a single motion-blurred image. Their framework is based on an encoder-decoder architecture with Spatial Transformer and Image Warping layers. A main difference between their work and prior work is that they reduce dependency on predicted central frame and thus have lesser error accumulation which prior works have. They also predict the non-central frames using same decoder but with appropriate global and local transforms. They train their network using a combination of (i) photometric loss (predicted image is close to ground-truth), (ii) transformation consistency loss (transformations that lead to nearby frames are not very different) and (iii) penalty term (different predicted frames have different content). They demonstrate results on two datasets: (1) Synthetic dataset with rotation blur and (2) High-speed video dataset containing dynamic blur. \n\nCOMMENTS:\n1. I like the proposed approach of modeling motion via the STN and LW layers. However, I wonder if both are required simultaneously. Any arbitrary motion could in principle be modeled using the LW layers that estimate the motion flow with the caveat that predicted frames are close (to approximate a complex motion with linear estimates in delta time steps). Why is the STN then important and useful? Can it be avoided? Any empirical evidence to back up claim?\n2. Table 1 seems to indicate that you get better center frame prediction also compared to prior work. Would it make sense to run their algorithm with your predicted center frame and add it as a baseline for comparison. It would emphasize to the reader that even with your predicted center frame, prior work fails in the non-center frames.\n3. Will the synthetic images generated as part of the Rotation Blur dataset be made public for future evaluation?\n4. Please discuss the limitations of the proposed approach specifically how much motion can be deblurred and using how many frames (maybe relative to magnitude of blur). How is the number of predicted frames determined?\n\nDECISION:\nI think the proposed framework to generate multiple de-blurred video frames from a single motion-blurred image is very interesting and the ability to handle arbitrary motion makes it very appealing. However, I do not know enough about the area to make a strong decision. Hence, I give WEAK ACCEPT (subject to change based on discussion)."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: The paper proposes to generate video frames from a single blurred image in an end-to-end manner. The neural network architecture comprises a feature transformer network and a spatial transformer network. The optimization includes a multi-scale photometric loss with a transformation consistency loss that ensures efficient training of neural network. The proposed approach is evaluated using two datasets: (1). Camera rotation blurs generated from the panorama scene (Xiao & Torralba); (2). High-Speed videos. \n\nStrengths: \n\n+ This work is technically sound, has an intuitive formulation, and nicely contrasted with prior work.\n\n+ demonstration of superior qualitative and quantitative results. \n\n\nConcerns: \n\n- This work is dealing with an ill-posed problem, and that proposed formulation can only work for a specific setup, and not in-the-wild videos. Here is a detailed reasoning for my concerns: \n\n(i). One of the most potent applications of this work is temporal super-resolution (see Mahajan et al. SIGGRAPH 2009). Is it possible to use the model trained on high-speed video dataset and be able to do temporal super-resolution for the kind of videos used in Mahajan et al.? More specifically, given a 30 fps video -> generate a 240 fps by inserting seven frames in between. \n\n(ii). The current approach is seemingly working on the panorama dataset because it is a static content. The texture around the surroundings do not change too rapidly in panoramas. It is, therefore, able to learn how to move around pixels. It is not clear to me how arbitrary motion can be handled using the proposed approach.\n\n(iii). The results demonstrated on high-speed videos are not convincing. The demonstration in Figure 4 and Figure 5 further reinforces confidence in my concerns that the proposed approach is learning to move laterally by doing a sort of texture-inpainting as it moves on either side.\n\n(iv). It is not clear to me how can the proposed approach learn about human motion in general? The plausibilities are too large that it is not just possible to show one set of outputs from a single blurred image. \n\n(v). Even more challenging is to learn about the generalization of scenario when both object and camera moves arbitrarily. How is it that a single blurred image provides suitable information that can enable reconstructing video frames?\n\n"
        }
    ]
}