{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose an algorithm for meta-rl which reduces the problem to one of model identification. The main idea is to meta-train a fast-adapting model of the environment and a shared policy, both conditioned on task-specific context variables. At meta-testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out-of-distribution tasks than similar methods.\n\nThe reviewers agree that the paper has a few significant shortcomings. It's unclear how hyper-parameters are selected in the experimental section; the algorithm does not allow for continual adaptation; all policy learning is done through data relabelled by the model. \n\nOverall, the problem the paper addresses is very important, but we do not deem the paper publishable in its current form.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "###  Summary\n1. The paper proposes an algorithm capable of off-policy meta-training (Similar to PEARL) as well as off-policy policy adaptation (By relabelling previous data using the adapted model and reward function). \n\n2. The basic idea is to meta-learn a model that can adapt to different MDPs using a small amount of data. Moreover, the adaptation is done by only changing the latent context vector (Similar to CAVIA or CAML). The remaining parameters of the model (theta) are fixed after meta-training. \n\n3. The paper also proposes learning a universal policy that, when given the context vector of a task, can maximize the reward for that task. This means that for with-in distribution meta-testing tasks, the policy can be used as it is (by giving it the right context vector which can be computed by adapting the model). For out-of-distribution tasks, however, it is important to update this policy. \n\n4. To update the policy, the paper proposes combining previously stored data (for example data used in meta-training) with the adapted model to do off-policy learning (Using SAC).  \n\n### Decision with reasons \n\nI vote for rejecting the paper in its current form for the following reasons:\n\n1- The paper assumes that it is possible to learn models for out-of-distribution tasks with a few samples that are accurate on all the previously stored data. This is fundamentally incorrect. If the MDP changes in a significant way, it is not reasonable to expect that we can adapt a model from a few samples. Moreover, even if we can adapt the model using a lot of new experience, it is not reasonable to expect that we can use this model to accurately label all previous data. The authors do acknowledge this when describing results in Figure 3, however they seem to underplay this limitation. \n\n2- Turning the meta-RL problem into a supervised learning problem has already been explored. For instance, Nagabandi et al. (2018)[1] showed that it is possible to quickly adapt models to changes using meta-learning. They, however, used decision time planning for the control policy (By random shooting method). This paper, on the other hand, uses Dyna style planning with an off-policy learning algorithm on previously stored data. The only difference is the choice of off-the-shelf planning algorithm which is not a significant contribution (There are some other small differences, such as learning a context vector and not model initialization, learning a universal policy etc, however, I don't see how they are essential for the proposed approach; maybe the authors can clarify why those choices are essential) \n\n3- The paper assumes a context vector alone is sufficient to capture changes in MDPs (It keeps the rest of the model fixed at adaptation). This might be reasonable if the context vector is sufficiently large, but the paper does not even mention the size of the context vector. It also skips other important details. For example, it does not mention any details about hyper-parameter selection, how the context-vector used in the model, etc. It's hard to judge the importance of the experimental results because of this. \n\n### Questions \n\n1- \"Effective model training requires the validation batch to contain data corresponding to optimal behavior for the tasks, which we obtain by training a universal policy conditioned on the context descriptor\"\n\nIt's not clear to me why the validation batch must contain data corresponding to the optimal behavior. \n\n2- Is the proposed framework really consistent? At adaptation, only the context vector is being updated whereas model parameters (theta) are fixed. Why is a context vector alone sufficient to adapt the model to drastic changes in the MDP? \n\n\n[1] https://arxiv.org/abs/1812.07671\n\n### UPDATE\n\nThe authors gave a detailed response to the reviews and answered some of my main concerns. However, I'm still not convinced that the paper, in its current form, can be accepted. My issues are: \n\nThe paper combines some existing ideas in a new way but falls short of justifying the choices it made. The proposed contribution is that it is consistent (meta-learning methods that learn a network initialization are also consistent), can do off-line meta-training (So can PEARL) and can use old meta-training data at meta-test time (This is novel to this paper). However,  the proposed methodology also has some downfalls. For example: \n\nIt does not allow continual adaptation. This is an important limitation of existing consistent meta-learning methods and this paper does not address it. Nagabandi et al 18 [1], on the other hand, propose a similar solution that is also capable of continual adaptation. \n\nMOST IMPORTANTLY, the empirical evaluation in the paper is very unsatisfactory. Even though the authors have included hyper-parameters in the appendix in the updated version of the paper, they still do no specify how these parameters were selected. Were the parameters selected to maximize the performance of their method and then copied for the baselines? This would not be a fair comparison. \n\nGiven the above-mentioned issues, I don't think the paper in its current form can be accepted and I'm maintaining my initial score. I think the authors should do a more thorough empirical investigation and tune the baselines and their method separately (using comparable compute budget). They should also report results on multiple environments using the same parameters (i.e. tune hyper-parameters on one or a few environments and reports results on some other environments as commonly done in Atari) \n\n[1] Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\n-------------\nThe authors propose an algorithm for meta-rl which reduces the problem to one of model identification. The main idea is to meta-train a fast-adapting model of the environment and a shared policy, both conditioned on task-specific context variables. At meta-testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out-of-distribution tasks than similar methods.\n\nMajor comments\n--------------\nMaking meta-rl algorithm generalize better outside of the meta-training distribution is a relevant open problem, and this work proposes nice ideas towards its solution. The paper is well-organized and easy to read. The idea of reducing meta-rl to a task identification problem is not completely novel since some recent works have been proposed in this direction (see later). Anyway, the proposed approach is interesting and seems (at least from the proposed experiments) effective. My main concerns follow.\n\n1. Though they attempt to address all relevant questions about the proposed approach, I found the experiments quite weak. Only two Mujoco domains are used for the standard meta-rl experiment, and only one of them (HalfCheetah) is used to test the out-of-distribution capabilities. Regarding the first experiment, MIER always performs comparably or worse than PEARL. What is the intuition behind this result? Does it suggest that MIER is paying additional sample complexity in \"in-distribution\" tasks in order to be more robust to out-of-distribution ones? On the other hand, the generalization experiments seem much more promising, but I would like to see more (at least the humanoid robot as well) to confirm that this result is not only a specific case of this domain. Furthermore, from Figure 3 it seems that MIER improves over PEARL even on in-distribution tasks, while it performed significantly worse in Figure 2. Why does this happen?\n\n2. Related to the previous point, I did not find any description of the parameters adopted in all experiments (learning rates, batch sizes, etc.). I do not believe I would be able to reproduce the results at the present time.\n\n3. The proposed method is somewhat related to other recent works [1,2]. In particular, [2] presents similar ideas, where the authors meta-learn a fast-adapting model (actually, a task encoder) and a shared universal policy conditioned on the task representation. The main focus is still to improve the generalization to out-of-distribution tasks. Can the authors better discuss the relations to these works?\n\nMinor comments\n--------------\n- In the introduction: \"Effective model training requires the validation batch to contain data corresponding to optimal behavior for the tasks...\". Why? In principle we could train a good model of the environment by running a sufficiently-explorative policy.\n- In the related works: \"Our method does not suffer from this problem since we use our model to train a model-free policy\". It is not clear why (though it becomes later) since simulating long trajectories from a learned model could lead to the usual divergence issues.\n- In Sec. 3.2: r in \\hat{p} should not be bold. Also, \"f\" in the subscript of the expectation was not defined (is it \\hat{p}?).\n- In Sec 3.4: there is a minimization over \\phi which however does not appear in the objective.\n- In the optimization problem at page 5: \\phi_{\\Tau} should probably be \\phi.\n- In Fig. 2, why is MIER run for less steps than the other algorithms?\n\n[1] Humplik, J., Galashov, A., Hasenclever, L., Ortega, P. A., Teh, Y. W., & Heess, N. (2019). Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424.\n[2] Lan, L., Li, Z., Guan, X., & Wang, P. (2019). Meta Reinforcement Learning with Task Embedding and Shared Policy. IJCAI 2019."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposed a novel approach to reformulate the meta-RL problem as model identification with a gradient descent based algorithm. Such innovation not only allowed us to perform meta-learning via supervised learning of the model, which is more stable and sample efficient, but also allowed us to leverage off-policy RL to learn the policy instead of meta RL.\n\nPros:\n1. Paper clarity. Although the submission had a few typos (I am not a native English speaker, but I'd encourage the authors to polish the writing of the paper), it's a very well-written paper overall. The flow and logic of this paper was clean, and the authors stroke a good balance between being focused about the core contribution of the paper, and reviewing related work and introducing sufficient preliminaries. As a result, I think this paper was accessible to both domain experts and the broader ICLR community. \n\n2. Novelty. This paper proposed a novel approach to reformulate the meta-RL problem as model identification with a gradient descent based algorithm. To the best of my knowledge, this was the first paper broke the meta-RL problem into a simpler meta supervised learning problem and an off-policy RL learning problem. Although each component of the proposed solution was not new, e.g., \"relabel\" was used in Dyna, MAML was first introduced in 2017, the combination of each component to address the meta-RL problem seemed to the novel to me. And I think the idea could be interesting to the ICLR community. \n \n\nCons:\nIt's weak accept rather than accept from me because of how the empirical evaluation were conducted in the paper, and I think the experiments conducted in the paper were a little bit weak (common for most ICLR submissions). Examples:\n\n1. Number of gradient steps is an important tuning parameter for MAML, it would be interesting to discuss number of gradient steps within the context of MIER.\n2. It might be useful to conduct some qualitative results to understand the model learned with MIER against the baselines, e.g., how well MIER adapt to the out-of-distribution tasks with simulated data points (examples o such qualitative studies could be found, say, in Finn et al., 2017).\n3. Given the fact that one major contribution of this paper was reformulating the meta-RL problem as model identification, it would be useful to conduct some quantitative study to help the readers understand the effectiveness of learning the environment model p(s’, r|s,a) compared to ground-truth, and how the quality of the learned environment model made an impact on the overall performance of the model.\n4. Some implementation details of MIER were missing, I don’t feel confident about how reproducible this research would be. For example, the specification of both environment and policy models were not discussed in the paper.\n5. In general, it would be useful to conduct more experiment results on more diverse data sets, say, in a supplement material.\n\n\nA few questions to the authors:\n1. Section 3.2: I assume the expectation should be taken w.r.t. p’ rather than f?\n2. In Algorithm 1 & 2, how was the adapted context \\phi_T used to update policy \\psi? Was it as input to the model parametrized by \\psi? It might be useful to make it clearer.\n"
        }
    ]
}