{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors present architectural changes to existing automatic music transcription models (AMT) to incorporate more expert knowledge/bias about the harmonic structure of musical notes. The use a variable-q transform (VQT), instead of a constant-q transform (CQT) or mel-spectrogram, as the input to the convolutional acoustic model. They compare performance on the MAPS dataset and find improvement over other convolutional acoustic model baselines. \n\nWhile the paper is clearly written and straightforward, the broader relevance to the ICLR community is not very well supported by the experiments. Incremental improvements are made on a smaller domain-specific dataset with feature engineering and minor architecture tweaks. The results are clear, but perhaps more appropriate for a domain-specific conference such as ISMIR. The paper could be strengthened by performing an apples-to-apples comparison to the sota model of Hawthorne et al. (2019) on the larger and more difficult MAESTRO dataset. The authors claim that only focusing on MAPS better tests generalization (because of the lack of instrument overlap between train and test), but MAPS is composed of more synthetic data, and it is not clear if the specific biases introduced by the authors just align better to those peculiarities of the dataset, or if they would transfer to more realistic data such as MAESTRO. The authors seem to have a comparable setup (with data augmentation and LSTM), and showing compelling results on MAESTRO would greatly support their claim that the harmonic input representation is not just injecting bias that could be compensated for with a larger network. If they did so I would be open to changing my rating.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper describes a convolutional network (and later, conv-recurrent network) for automatic transcription of polyphonic piano music recordings.\nThe architecture borrows from prior work (Elowsson 2018) in automatic transcription to exploit regularities among harmonically related time-frequency bins, and the empirical evaluation appears to demonstrate that it performs comparably (in aggregate, on the MAPS dataset) to the onsets-and-frames method (Hawthorne et al., 2018) that was trained on a much larger corpus.\nWhile the work here seems to be well executed, the technical presentation is unclear throughout the article, and there is no clear statement of how exactly this work differs from prior work.\nMoreover, there is no attempt at any qualitative error analysis, nor is any attempt made to explain what the model is capturing specifically to improve over existing methods.\nThese two factors ultimately obscure the take-home message of this paper, and I do not recommend acceptance at this time.\n\nGoing by the arguments put forward by the authors, the crux of this model appears to be the \"sparse convolution\" layer, which is not novel to this article, nor does it receive a high-level explanation.\nMoreover, there is no ablation study conducted in this work to try to quantify its efficacy, so the reported gains may be due to other features of the architecture.\n\nThe authors refer several times to the \"HCQT\" model of (Bittner et al., 2017), though their description of the method seems to conflate the input representation (HCQT) with the convolutional network described in the paper (\"deep salience\").\nThis makes for a somewhat confusing presentation of the literature, which is not improved by the fact that no comparison to that method is included here.\nThis seems strange, as from what I can tell, the ideas in this paper are not incompatible with the HCQT idea (or HVQT, if you will).\n\nThe empirical results in table 2 do indeed seem promising, primarily due to the reduced size of the training set compared to (Hawthorne et al. 2018).\nHowever, it does lead me to wonder if part of the issue here is domain adaptation from MAESTRO to MAPS.\nIt seems that the proposed model performs comparably to Hawthorne's (within 1% on F-score), but strikes a different balance of precision vs. recall.\nThe comparison to the closely related (Elowsson 2018) does show a large improvement, but there is no concrete explanation given for why; the penultimate paragraph of section 3.3 is quite vague on this point.\nIn both cases, some qualitative error analysis would go a long way to help illustrate where these different models disagree and why.\n\nFinally, there are various \"magic numbers\" sprinkled throughout and used without justification.  For example:\n    - Why 14.1 Hz as a minimum bandwidth in eq1?\n    - Why downsample the spectrogram by 22?\n    - Where do the lines in Figure 2 come from?  It's fine to refer readers to prior work for a detailed explanation, but this figure conveys almost no information."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "A Harmonic Structure-Based Neural Network Model for Musical Pitch Detection\n\n*Overview* \n\nThis paper presents a new architecture for performing frame-wise pitch detection for piano music. Compared to existing models for this task, this study proposes a so-called “sparse convolution layer”, where the distribution of weights along the frequency axis is informed by prior knowledge about the distribution of partial frequencies that constitute a pitch. The primary shortcoming of this study is the lack of experimental rigour while testing the main hypothesis in the paper that the sparse convolution layer results in improved accuracies. \n\n*Shortcomings of the Evaluation Section*\n\nThe proposed architecture has 13 layers including the input. The input is in the form of the variable-Q transform (VQT) while the output layer is of the form Tx88, where T is the number of input frames and each frame has 88 outputs between [0,1] that indicate the probability of the MIDI notes between [21,108] being on/pitched. The main novelty in the paper is the sparse convolution in Layer 8, where the distribution of weights along the frequency axis is inspired by the design proposed in Elowsson (2018). Note that this layer also contains the most number of parameters (256x1x79 = 20,224). Table 2 presents comparisons with the most popular models in the literature. \n\nMy main criticism of Table 2 is the fact that all the models have been trained using different data which makes a direct comparison of accuracies unsatisfactory. For example the Hawthorne et. al. 2019 model is trained on the MAESTRO dataset while the proposed architecture is trained on the synthesised data from the MAPS dataset only. The authors claim that the fact that the proposed model yields better results than the Hawthorne et. al. 2019 model is impressive since the MAESTRO dataset is 15x larger. However, I do not agree with this claim since the repertoire of training data in the MAPS dataset might be more suited to the eval set than the repertoire of training data in the MAESTRO dataset. Furthermore, all the models compared in Table 2 have different numbers of trainable parameters which can also contribute to the difference in eval accuracies. There might be other confounding factors that might contribute to this result and therefore the only way to control for them is by designing a better experiment. \n\nA first step towards this would be to replace the sparse convolution layer in the proposed architecture with a standard convolution layer of shape (256x1x79) with varying dilation factors along the frequency axis. This would allow the main hypothesis in the paper to be tested i.e. sparse convolutions somewhere in the middle of the acoustic model are better than regular convolutions and that knowledge of the distribution of harmonics is a useful inductive prior. Secondly, it would be extremely informative to train the proposed model on the MAESTRO dataset with a similar number of parameters as Hawthorne et. al. 2019 and then compare results on both the MAESTRO and MAPS eval sets. The MAESTRO dataset is freely available for download and the evaluation section would be significantly better if the proposed model was trained on both MAPS and MAESTRO datasets. \n\n*Minor Comments*\n\n1. The description of pitch and harmonics in Section 1.2 is extremely informal. Although I agree there isn’t enough space for a rigorous motivation of these concepts, a couple of references here would be extremely useful for the reader. \n2. “To tell if a specific note is active, intuitively we will first check if the pitch and harmonics are light”. What does “light” in this context? Why not directly say if there is some energy distributed along the frequencies? \n3. “Since the constituent frequencies of harmonic patters are sparsely distributed, it is inappropriate to capture harmonic patterns”. I don’t buy this argument, an acoustic model with many layers can in theory capture complex distribution and patterns along the frequency axis. As mentioned before, the experiments do not do a good job of testing this intuition. \n4. In Section 2.2, the training, validation and eval sets for the MAESTRO dataset are mentioned, but is this dataset used for training any of the proposed models? I got the impression that only the MAPS dataset was used for training. \n5. “For pitch detection, we do not like the hop size to be too small, because in this case the adjacent frames in the spectrogram will be highly correlated. So we down-sample the resulting spectrogram by a factor of 22”. I do not follow this argument. Why is it is detrimental for neighbouring frames/windows to be correlated? This has been stated without any reasoning. Secondly, don’t we lose training data by downsampling by a factor of 22? Where does this factor come from? Given that training data is already very sparse, doesn’t it make sense to train on higher resolution inputs? There should be more discussion about this choice. \n6. In Equation 2, sr which I assume is Sampling Rate has not been defined. \n7. “..solely measured by f-measure”, the F should be capitalised. \n8. Given that the precision, recall and F-measure is computed frame-wise, I don’t understand the difference between the average and ensemble results clearly. From Table 2, the differences seem to be very minor. It would be useful to explain this more clearly. \n"
        }
    ]
}