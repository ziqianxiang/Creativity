{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\nThe paper proposes a method called UniLoss, which decomposes a task-specific loss into 4 components and essentially performs continuous relaxations on some discrete components. The paper reports some results on image classification and pose estimation.\n\nWriting\nThe content presented in this paper is well written; but as an ICLR paper, many technical details about the methods seem missing, and prevents me from having a holistic picture of the entire work. And the results presented in this paper do not suffice to show the effectiveness of the proposed method. See below.\n\n\nMotivation\n\nI am wondering, besides multi-class classification (which seems to have been abused hence less interested), what other loss metrics and problems have their loss function following your decomposition. It would be good to see an enumeration of such problems/loss metrics to show how general the proposed decomposition is and how many problems could benefit from your formulation (besides the very well-defined multi-class classification loss.)\n\nI do acknowledge there are some good innovations in this paper, but I think the introduction has some overclaims, as there exists several works (w/ code) published recently that consider similar problems. See below.\n\nAutoLoss: Learning Discrete Schedules for Alternate Optimization. Xu et al.\nAddressing the Loss-Metric Mismatch with Adaptive Loss Alignment. Huang et al.\n\n\nResults\n\nThis paper lacks convincing results to verify its claims. See detailed reasons below:\n- Mnist, and correspondingly the binary classification over MNIST, are too artificial a dataset and a task to show the effectiveness of your proposed method. I would expect to see more REAL experiments on more challenging tasks. Even for image classification, it is better to run the experiment on larger-scale data. \n- In terms of the results of MNIST, it seems UniLoss is not able to outperform cross-entropy loss. In terms of results on CIFAR, the improvement is very marginal.\n- Another major concern of mine is how much manual human effort and extra training are needed to deploy a training task using UniLoss. From the MNIST results I gained a sense that UniLoss involves a lot more design effort but still is not able to even perform better than a straightforward cross-entropy (even on this very artificial and abused dataset MNIST).\n- The paper claims for many times that UniLoss bypasses “effort on manual design”, but does not have a quantitative metric on how much effort it can save or how much other costs it incurs. My understanding is that UniLoss itself needs to be designed based on the task of interest, and seems to introduce some cost of training or extra implementation.\n- For pose estimation, why don’t perform experiments to report the results on the complete set of human joints, but choose head-only, even if you claim the method could be applied w/o any modification?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents UniLoss, a general framework for training a neural network model with respect to a non-differentiable evaluation metric as an objective function. The key idea of this paper is to decompose an evaluation metric into a sequence of functions (f, h, g), where: f maps scores computed by a neural network model to comparisons with the ground truth; h maps the comparisons to binary variables; and g computes the performance value from the binary variables. Even with the decomposition, the evaluation metric is still indifferentiable. In order to make the evaluation metric differentiable, this study approximates h with the sigmoid function and g with an interpolation method (inverse distance weighting). The experimental results show that the loss functions obtained by UniLoss can achieve roughly comparable performances to the conventional loss functions on MNIST (binary classification, average precision, cross-entropy loss), CIFAR-10/100 (multi-class classification, accuracy, cross-entropy loss), and MPII (pose estimation, PCKh, MSE with the manually-designed \"ground truth\" heatmaps).\n\nThis paper is well structured and easy to read. The idea of decomposing an evaluation metric into functions and considering the approximated versions the functions is interesting. It is great to see that the empirical results demonstrated that the proposed method could achieve the same level of performances with the conventional losses. The impact of this paper would be greater if the proposed method could discover a better approximation of an evaluation metric and achieve a better result.\n\nAlthough the procedure for obtaining a loss function looks systematic, I am wondering of the generality of this framework. For example, I'm not sure whether this method can be applied to other evaluation metrics such as F1 and BLEU scores. Even if this method can support these metrics, I would like to know how much effort we need to craft functions f, h, and g. A discussion about the applicability and limitation of the proposed framework would be useful.\n\nI'm also wondering the additional computational cost introduced by a surrogate loss. The binary classification on the MNIST, for example, requires $O(n^2)$ computations for binary variables whereas the original evaluation metric and cross-entropy loss require only $O(n)$.  For this reason, I'd like to read a discussion and empirical results about the computational cost.\n\nMinor comments:\n\nSection 1:\n\"For example, cross-entropy is a popular loss function for training a multi-class classifier, but when it comes to comparing models on a test set, classification error is used instead.\"\n\"For example, cross-entropy or hinge loss for classification, both of which have proven to work well in practice.\"\nThis discussion is a bit confusing because the former sentence claims that the cross-entropy loss is inappropriate, and the latter suddenly explains that the cross-entropy loss works well in practice.\n\nSection 1: \"Non-differentiability occurs when the output of the task is discrete (e.g. class labels)\"\nThis sentence is a bit misleading because the cross-entropy loss is differentiable and used for multi-class classification (even though labels are discrete).\n\nSection 3.1: \"Our approach seeks to automatically find a surrogate loss to minimize e\"\nWhen we call $e$ as 'performance', we want to 'maximize' (maximize the performance) instead of 'minimize' (because we do not want to minimize the performance). We may call $e$ as 'error' to be consistent."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary: This paper proposes a way to construct a surrogate loss function given an evaluation metric of our interest. Traditionally, given one evaluation metric, e.g., the zero-one loss for binary classification, we minimize the risk with respect to the zero-one loss by instead minimizing a surrogate loss, e.g., logistic loss. This paper suggests to take a different approach by decomposing an evaluation metric of our interest into a form that we can simply minimize it. More specifically, they decompose the metric into differentiable part and non-differentiable part. Then, by substitute the Sign() function to a sigmoid function for differentiability, and also utilize the adaptive interpolation technique to make another non-differentiable part to be differentiable, we can use the deep neural networks to effectively minimize the proposed surrogate (Uniloss). There is no theoretical analysis whether minimizing this surrogate will yield a minimizer of the target evaluation metric (e.g., for classification, there is a notion of classification-calibration (Bartlett+, JASA2006) to guarantee the optimal solution of the surrogate is identical to the target metric under some conditions). Nevertheless, the experimental results show that the performance of the proposed UniLoss is comparable to a famous manually designed surrogate loss for several tasks.\n\n========================================================\nClarity:\nOverall, I found this paper is easy to follow and Figure 1 really helps me understand the core concept of this paper.\nHere are a few minor comments/questions:\n1. In Eq. (2), does the domain of f is wrong because \\phi gives R^d?\n2. Does the example of the decomposition of UniLoss for multiclass classification has the domain/range of f and \\phi the same way it is stated in 3.1?\n3. 3.3: This decomposition holds under the assumption that there is no learnable parameter in the evaluation. << I am not sure what this statement means. Could you please elaborate it?\n4. In general, we obtain good anchors by flipping some bits from the best anchor << this is the first time the word \"bit\" shows up. I couldn't understand what this means and I think it should be explained in more details how to construct anchors. I guess it means that we check the \"b\" of the best anchor and pick good anchors as any points that is different from \"b\" at most k position (for example k=1 as in 4.1)? Is it possible that good anchors do not exist? Then, how to deal with this problem?\n\n========================================================\nComments:\nThis paper gives an interesting idea to find a surrogate loss for general evaluation metrics. Although no theory of surrogate losses are provided, I am convinced that the proposed scheme should work empirically as long as we can find an appropriate decomposition. And this means experimental results are highly important to validate the usefulness of the proposed method.\n\nOn the anchor points:\nI think the choice of anchors is also very important to make UniLoss effective. I think there is a need to discuss more about how to adjust the appropriate anchors. Or more experiments about several strategies for anchors, or the effect of anchor point selection. I think this part needs further explanation and in my impression, there are many things going for designing appropriate anchors to get a good performance.\n\nOn the limitation of the proposed method:\nI would like to know the limitation of the proposed surrogate loss. For example, area there any known evaluation metrics that currently Uniloss cannot be used? For example, can we use Uniloss for top-k accuracy, jaccard index, f1-measure, or area under the receiver operating characteristic curve (AUROC). Or is it straightforward to trivially extend it for these metrics? I am still not convince that it is easy to find an appropriate comparison function for general metrics to use UniLoss.\n\nOn the experiments: \nI would like to know how many trials are run for each experiment. And it would be nice to see a standard deviation of the performance to see how stable UniLoss is. It would be very insightful to compare the performance of different anchor selection strategies or is it impossible tune appropriate anchors for our target evaluation metric?\n\n========================================================\nDecision:\nAlthough the idea is interesting, it is highly experimental but I found that it is not straightforward to use it for general metrics although the authors said that it is trivial to generalize this for many metrics. Moreover, in my opinion, the discussion about anchor points, limitation, and the details of experiments are not sufficient in the current form. Thus, I give a weak reject. I am happy to improve the score if I am convinced that it is easy to construct a surrogate for several metrics I mentioned above, and more explanation of limitations and anchor selection are provided. Also the experiments to evaluate the importance of anchor selection scheme are conducted.\n\n\n"
        }
    ]
}