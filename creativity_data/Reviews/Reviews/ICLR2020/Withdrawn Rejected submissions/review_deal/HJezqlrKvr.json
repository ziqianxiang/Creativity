{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents an approach and implementation to schedule operators in deep learning inference to better utilize parallel hardware. The work is a nice piece of engineering work with good results, and I enjoyed reading the paper. However, the scientific novelty in the paper is very low. \n\nThe approach relies on that we have measured the execution times of each kernel / operator / convolutions in advance. Further, the execution time should not vary too much, otherwise the schedule will be non-optimal. This assumption can be true in some contexts, but hard to achieve in other. For example, it will depend on whether the data is in memory, caches, or somewhere else. Depending on the data location, access times (and thus execution times) may vary. However, the empirical data suggets that this is not a big problem.\n\nThe approach to find an optimal schedule is very straight-forward and done in a standard way. Thus, the scientific novelty is low. For example, there is a lot of work since decades about scheduling and load balancing of multiple tasks on parallel hardware. However, none of the work from that domain is mentioned. In general, there seems to be a big gap between the DL/ML/AI communities and the computer system community. I think there are great benefits for both sides to collaborate a bit more.\n\nThe approach is evaluated on three hardware / graphics cards and four DNN models. The results are good, and the approach performs better than, e.g., TensorRT et al. The work can have some practical impact, when/if implemented in existing frameworks.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a trivial scheduling approach to split computation graph of a DNN into several groups. The operations in each group, which is called a stage, will be executed in parallel to improve utilization of hardware. They claim that the proposed Deep Optimal Scheduling (DOS) can discover a globally optimal schedule, but no theoretical proof is provided. This claim is apparently wrong, it can not even find optimal solution for the following example:\n\ninput ---> op1-------+->ouput\n|                                |\n+---->op2---->op3---+\n\nIf execution time of op1 is larger than summation of op2 and op3, the optimal solution should be start two parallel computation threads, one for op1, the other one for op2-->op3.  If DOS is used, it will first process op1 and op2 in parallel, than process op3, or process op2 first, then process op1, op3 in parallel.\n\nFor the experimental part, they only compare DOS with Metaflow in terms of search time, runtime latency comparison should be provided.\n\nThe writing of this paper is bad, for example, they use \"S\" to represent stage, then it is used to represent  \"state\" or \"set\". It makes me confused.  They should polish their paper before submission."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "Summary of the paper:\n  \n        This paper describes a method to accelerate DNN by combining inter-operator and intra-operator parallelizing. The proposed method is validated on several hardware and inference setting with improvements ranging between a 1.2 or a 1.4 factor.\n\nDetailed comments:\n\nThe has important grammar errors: e.g. \"Existing deep learning frameworks has focused...\" and \"Deep neural networks (DNNs) have achieves...\" etc. This makes the text almost unreadable.\n\nThe latex template has been modified. For example, there is no space below some subsections like 4.2 and 5.3.\n\nThe experiments do not seem to have error bars. This questions the statistical significance of the results.\n\nThe improvements obtained seem small which questions the practical utility of the method.\n"
        }
    ]
}