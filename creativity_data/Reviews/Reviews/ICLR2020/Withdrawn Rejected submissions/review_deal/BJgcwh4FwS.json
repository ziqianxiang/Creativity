{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposed graph neural networks based approach for subgraph detection. The reviewers find that the overall the paper is interesting, however further improvements are needed to meet ICLR standard: \n1. Experiments on larger graph. Slight speedup in small graphs are less exciting.  \n2. It seems there's a mismatch between training and inference. \n3. The stopping criterion is quite heuristic. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This paper proposed a graph net based approach for subgraph matching. The general idea is based on the graph matching network (Li et.al, ICML 2019) that computes the node embeddings of two graphs with co-attentions. The training requires the supervision of ground truth matching. During inference an iterative method with heuristic stopping criteria is used. Experiments on tiny graphs show better results than learning based baselines, but worse results than MCS solver.\n\nOverall the paper is well motivated. However there are several major concerns with the paper:\n\n1. Since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. Also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. \n\n2. It seems there's a mismatch between training and inference. The inference method is done iteratively, where the Eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. \n\n3. Iâ€™m not sure why the two stop conditions are needed. One can easily check (incrementally) whether the added nodes are isomorphic.  \n\n4. The graphs used in experiments are too small. \n\nSome other minor issues:\n\nIt would be better to define Y with Eq (1) and Eq (2) in the paper. There seems to be no explicit definition of Y. \n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. \n\nThe authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used  to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. \n\nOverall the paper is well written, and the experiment is good and solid. \n\nSome suggestions:\nThe GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. \n\n[1] Deep Learning of Graph Matching, CVPR18\n[2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19\n[3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19\n[4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19\n[5] Neural Message Passing for Quantum Chemistry, ICML17"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS.\n\nThis paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice,  the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. \n\nThe algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part.\n\nI understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes."
        }
    ]
}