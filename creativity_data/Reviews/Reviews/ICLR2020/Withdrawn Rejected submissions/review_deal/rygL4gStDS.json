{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a method to replace dense networks with structured (a combination of diagonal and circulant) weight matrices which only uses 2n coefficients instead of n^2 for dense networks. The authors propose a tighter bound for the number of DC (diagonal-circulant) matrices required to approximate a dense network (of depth L), which is dependent on the sum of the rank of matices used in the neural network, as compared to previously used bounds which were dependent only on the dimension of the matrix. They also prove that any DCNN of bounded width and small depth (9k where k is the total rank of all weight matrices used in the deep network) can approximate a deep ReLU network of low rank with comparable accuracy on the downstream tasks.\n\nThe authors perform experiments to compare with existing compression based methods and outperform them with almost half the total parameters used in the next best approach. Furthermore, they extend their setup to train deep networks with an initialization scheme to avoid vanishing/exploding gradients. \n\nComments: The paper is well organized and seems to carry out substantial experimental work. However, I have some issues with the paper.\nThe proposed rank dependent bound (4k + 1) is useful only if the rank of matrix A (the matrix  of dimensions n*n to be approximated) is < n/2. Although the authors have cited papers that claim to explicitly train networks to have low-rank weight matrices, it would be useful to have some guarantees about how low a rank these deep networks can have.\nThe authors claim to have come up with a method to smartly use non-linear functions to train DCNNs. However, the paper doesn't seem to reflect anything in these lines. The experimental results shown in Figure 2 requires a more in depth study. It would be useful to have a baseline without any sort of non-linearity (with just DCDC pairs in Fig. 2) to have a better understanding of the proposed claims.\nSome more questions to the authors:\n1) It is not very clear as to how the initialization (and the corresponding variance of each layer being independent of the depth) relates to vanishing/exploding gradient problem. Doesn't the bound have to be on the gradients? Additonally, in the Proofs of Section 5 (of Supplemental Material), how do you account for the decrease in variance due to the use of ReLU?\n2) In Figure 3, the convergence of the DCNN model occurs instantly (around the 1st step). Does it mean, it doesn't require training at all?\n3) Is it a standard practise to use ReLU as a nonlinearity in experiments?\n4) The results in Table 3 suggest that Convolutions with Fast Food (FF) perform far better than DCNN, with a very small difference in the number of parameters used. It would be interesting to see if the convolution net can be rolled out in the form of dense networks and approximated using a DCNN. The authors can add this baseline to their results.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors study the diagonal circulant neural network (DCNN), where the weight matrix is a product of alternating diagonal matrices and circulant matrices. They provide a theoretical analysis of the expressive power of DCNNs. Furthermore, they propose an initialization scheme for deep DCNNs. The performance of the DCNNs is evaluated on synthetic and real datasets. \n\nThe theoretical analysis seems interesting. However, the experimental results do not seem to support the effectiveness of the DCNN. The performance of DCNN, for example on CIFAR-10, is much worse than the state-of-the-art models, which makes the model barely usable by practitioners.\n\nThe baseline compression-based models are not the latest. More recent compression or pruning methods might be considered as well, for example, SqueezeNet, The Lottery Ticket Hypothesis. \n\nFurthermore, there is no empirical analysis of the new initialization scheme.\n\nIt would be interesting to see any experiments corroborating the theoretical results, for example, Lemma 1. That is to compare the performance of a deep ReLU network of width n and depth L and a DCNN N 0 of width n and of depth (2n-1)L.\n"
        }
    ]
}