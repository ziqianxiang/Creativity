{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper is about learning policies in RL while ensuring safety (avoid constraint violations) during training and testing. \n\nFor this meta review, I ignore Reviewer #3 because that review is useless. The discussion between the authors and Reviewer #1 was useful.\n\nOverall, the paper introduces an interesting idea, and the wider context (safe learning) is very relevant. However, I also have some concerns.\nOne of my biggest concerns is that the method proposed here relies heavily on linearizations to deal with nonlinearities. However, the fact that this leads to approximation errors is not being acknowledged much. There are also small things, such as the (average) KL divergence between parameters, which makes no sense to me because the parameters don't have distributions (section 3.1). \n\nIn terms of experiments, I appreciate that the authors tested the proposed method on multiple environments. The results, however, show that safety cannot be guaranteed. For example, in Figure 1(c), SDDPG clearly violates the constraints. The figures are also misleading because they show the summary statistics of the trajectories (mean and standard deviation). If we were to look at individual trajectories, we would find trajectories that violate the constraints. This fact is brushed under the carpet in the evaluation, and the paper even claims that \"our algorithms quickly stabilize the constraint cost below the threshold\". This may be true on average, but not for all trajectories. A more careful analysis and a more honest discussion would have been useful. In the robotics experiment, I would like to understand why we allow for any collisions. Why can't we set $d_0=0$, thereby disallowing for collisions. The threshold in the paper looks pretty arbitrary.  Again, the paper states that  \"Figure 4a and Figure 4b show that the Lyapunov-based PG algorithms have higher success rates\". This is a pretty optimistic interpretation of the figure given the size of the error bars. \n\nThere are some points in the conclusion, I also disagree with:\n1) \"achieve safe learning\": Given that some trajectories violate the constraints, \"safe\" is maybe a bit of an overstatement\n2) \"better data efficiency\": compared to what?\n3) \"scalable to tackle real-world problems\": I disagree with this one as well because for all experiments you will need to run an excessive number of trials, which will not be feasible on a real-world system (assuming we are talking about robots).\n\nOverall, I think the paper has some potential, but it needs some more careful theoretical analysis (e.g., effect of linearization errors) and some better empirical analysis. \n\nAdditionally, given that the paper is at around 9 pages (including the figures in the appendix, which the main paper cites), we are supposed to have higher standards on acceptance than an 8-pages paper.\n\nTherefore, I recommend to reject this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This is a very complete submission. There is a novel analysis,\nsimulations, as well as some results on real data. The authors propose\nLyapunov-based safe RL algorithms that can handle\nproblems with large or infinite action spaces, and return safe\npolicies both during training and at\nconvergence. As far as I can tell the approach is novel, makes sense,\nand requires a lot of technical innovations. I was impressed with the\nmethod and the analysis behind the method. The incorporation of the\nLyapunov idea from control theory makes a great deal of sense in this\napplication. However, it is not trivial to get from using this tool to\na working method."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper presents a technique for extending existing reinforcement learning algorithms to ensure safety of generated trajectories in terms of not violating given constraints.\n\nI have very little knowledge of this area and as a result was not able to evaluate the paper thoroughly. However, the problem addressed is certainly a very important one and based on my high-level understanding of the concepts involved the approach seems sensible. The experiments are clear and well designed, showing the trade-off between performance and safety."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary:\n\nAuthors propose ideas to perform safe RL in continuous actions domain with modifications to Policy Gradient (PG)  algorithms via either constraining the policy parameters or constraining the actions selected by PG with a surrogate/augmented state dependent objective. The paper is well motivated and the experiments (although I have some reservations about the setup) demonstrate efficacy of the proposed method.  \n\nReview:\n--> Introduction\nI do not agree with the statement that value function based algorithms are restricted to discrete action domains, especially when you rely on  “ignoring function approximation errors” for some of your claims. \n\nAgain in switching from value function to PG is true for traditional RL/Control theory but this is not valid here. ( your methods rely on Q(s, a) which is action-value function, or the constraint in equation 3 is integral of Q over all actions which would be value-function in traditional definition )\nNote: this is explained very well towards the end in the Appendix B, but this is a review of the paper and not Appendix B or C. \n\n\n--> Section 2\nsection 2.3 I would strongly advise the authors to rewrite this, this section reads like it was copied as is from the reference [Chow et al 2018].  especially the way Lyapunov function is defined. And the language and arguments are almost same. Some sentences cite the reference but conclusions drawn on these are not cited, are you claiming that these conclusions are original from this paper ?\n\nIt is not clear to me how the feasibility of initial pi_0 is ensured ? Did I miss this somewhere ?\n\n\n→ Section 3\n\nSection 3 is pleasant to read and very easy to understand, however, same cannot be said of the\nsection 3.1. I had to spend significant time reading 3.1 and I am still not sure I have understood it very well. \n\nExperiments:\n\nI don’t think halfCheetah-Safe is actually actually an useful experiment, Limiting the joint torques is perfectly understandable, just limiting speed and getting smooth motion could just be an artifact of the simulation environment. Are both constraints applied simultaneously (torque and speed) ? It is unclear from the text. \n\nI am not sure CPO without linesearch is actually a fair comparison. Line search may actually deem of the actions unsafe and therefore I would presume original CPO do be less prone to constraint violation than the proposed modification in your experiments. Again PPO is more heuristic than TRPO which makes it hard to compare like for like. PPO might give higher rewards but constraint violations may increase as well. An important point for Safe-RL I feel.\n\nFigure 6 \nCan you be more specific as what the figure 6 is showing ? Constraint ? is this constraint violation count? or cumulative sum of constraint slack over the whole trajectory ?\n \n\nNot part of assessment :\n\n\nUnclear Statements:\n\nPage 7, DDPG Vs PPO: explain clearly what you mean by  “covariate shift” or remove the statement altogether. \n\nPage 7, section 5 second paragraph, “The actions are the linear …. center of mass” I couldn’t understand this ? What do you mean by actions are velocity ?\n\n\n\nMinor points (Language, Typos):\n\npage 3, last paragraph,  Chow et al. is repeated, I can see why this happens there but suggest editing to avoid this. [This is also in intro paragraph, there it is just a typo and should be rectified]\n\nFigure 6: Captions labels are incorrect.\n\n\n\n\n\n"
        }
    ]
}