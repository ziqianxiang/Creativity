{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Overall, I personally really like the idea of masked translation model (MTM) with single transformer-encoder module and perform decoding in an iterative non-autoregressive refinement fashion. The source and target representations can interact with each other through a unified attention mechanism at every layer. The proposed MTM model can be seen as a unified framework where we can adjust the decoding strategies easily. \n\nHowever, I still have the following concerns about this submission:\n-- My first concern is the novelty of the proposed approach:\nThe idea of “iterative refinement based on mask prediction particularly in machine translation” is not new. There are several approaches have been proposed (and even published) before this submission was made. For instance:\n\nGhazvininejad, Marjan, et al. \"Constant-time machine translation with conditional masked language models.\" arXiv preprint arXiv:1904.09324 (2019).\n\nGu, Jiatao, Changhan Wang, and Jake Zhao. \"Levenshtein Transformer.\" arXiv preprint arXiv:1905.11006 (2019).\n\nMansimov, Elman, Alex Wang, and Kyunghyun Cho. \"A Generalized Framework of Sequence   Generation with Application to Undirected Sequence Models.\" arXiv preprint arXiv:1905.12790 (2019).\n\n-- The training and inference choices are pretty similar. It seems that the biggest difference is that the proposed model uses a single module for both encoder and decoder. However, this particular model choice is also not new to the field where people already proposed to share part of the attention each layer.\n\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise coordination between encoder and decoder for neural machine translation. In Advances in Neural Information Processing Systems, pp. 7944–7954, 2018.\n\n-- Sharing the same model for both encoder and decoder should also bring several drawbacks. For example, it actually increase the computational time for every forward pass as we cannot pre-compute the encoder representation before hand. More importantly, the reason to merge the encoder and decoder into one model is not well justified. We cannot choose the particular architecture just because it is simpler. We also need to consider other real advantages such as quality, latency, memory usage, etc.\n\n-- My biggest concern is in the experiments.\nAs mentioned earlier, the particular choice of using a shared module for both encoder and decoder is not well justified by the experiments. It is essential to have a fair comparison between a encoder-decoder based model with mask prediction.\nThe paper only conducts experiments only on one direction of “Romanian-English (Ro-En)” which is not very enough to prove the author’s claim. Prior work also used other datasets such as “WMT En-De and IWSLT En-De”.\n\nThe shown performance on Ro-En is also not convincing. Prior literatures (Ghazvininejad et, al. 2019; Gu et, al. 2019) show much higher BLEU score on Ro-En with similar iterative refinement process. Note that, the claiming of not using “distillation” does not hold for Ro-En because this dataset is a bit easy and a refinement-based non-autoregressive model can easily match the autoregressive model’s performance without distillation. However, for harder dataset such as WMT En-De it is not the case. It would be nice to have a fair comparison with prior works on En-De.\n\nAlso, the paper did not perform any speed/latency comparison using actual execution time on GPUs or CPUs. As commented earlier, MTM needs to recompute the encoder features for every refinement steps.\n\nMinor concerns:\n-- The paper’s title is called “masked translation model”, however, the model also predicts tokens even though the word is unmasked (depending on how we choose the decoding approach). It seems it is a bit misleading to use this title.\n\n-- How likely a wrong word that is unmasked can be corrected compared to mask-predict? \n\n-- Why the model is with 12 layers and 16 attention heads? It is not comparable with other previous work which utilized a norma, transformer-base parameters.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents an application of masked language models for non-autoregressive machine translation. The presented work is closely related to cross-lingual representation learning models like XLM (Lample and Conneau 2019) where a masking function is applied to a source-target parallel sentence pair (dubbed TLM in their paper) and sequence-to-sequence variants where masking is applied only on the target side like Ghazvininejad et al 2019 and Lee et al 2018. The primary contribution of this work is the idea of concatenating source and target sentences into a single long sequence while masking only target side tokens, allowing a single transformer encoder to learn source-target interactions at every layer. At inference, this allows for different decoding strategies including autoregressive and non-autoregressive approaches by choosing different masking functions.\n\nI think closing the gap between autoregressive and non-autoregressive translation systems is extremely important and the problem being addressed is certainly important. The idea of concatenating source and target sentences and training a single transformer is sound - source-target interactions at every layer. However, I think this paper requires more work. 1) The technical and architectural contributions over Lample and Conneau 2019 and Ghazvininejad et al 2019 are fairly minimal 2) Ghazvininejad et al 2019’s sequence-to-sequence model achieves better performance on WMT’16 En-Ro with half the number decoding revisions at test time 3) None of the decoding strategies are particularly novel or specific to the particular architecture, with the full unmasking variant having already been explored in Lee et al 2018 and the confidence-based variant in Ghazvininejad et al 2019.\n\nI have a few questions:\n\n1) Could one of the reasons for performance not being as good as Ghazvininejad et al be the absence of a distillation objective? Their improvements on WMT’14 En-De appear to be pretty substantial with distillation, especially when using fewer decoding revisions (not as significant on WMT’16 En-Ro however).\n\n2) Ghazvininejad et al report much better results with just 10 revisions, while you run twice the number, is this because of a difference in your confidence annealing parameters K(T) compared to theirs?\n\n3) In response to “Kaaliya Budhil”, you say “Also, our formulation enables a broad generalization on decoding schemes including confidence-based (similar to [1]), left-to-right (or right-to-left) and many others, while [1] has only one decoding strategy” - From what I understand, the decoding strategies you present can also be used by [1] ( Ghazvininejad et al) and also their confidence-based approach seems to perform the best.\n\n4) In the same response you also say “Nonetheless, we did not compare our performance to [1] since our MTM currently does not support a decoding with multiple target hypothesis lengths, while all results of [1] rely on such multiple-length decoding. This is not consistent and there might be a non-negligible gain by the multiple-length decoding, so we only compare with single-length decoding results of other related work for the sake of fairness.” - Ghazvininejad et al report numbers with single length candidates as well (In Table 5 of their paper), you could compare with respect to those for a fair comparison.\n\nI would encourage the authors to continue working on this while experimenting with all of the bells and whistles of current non-autoregressive translation systems like multiple target length hypotheses and distillation using an autoregressive teacher."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper combines encoding and decoding of sequences within a single model component and uses masked language modelling for language translation task. Experiments were done for the task of translating Romanian to English and the reported results show that the model works successfully learns to perform the task.\n\nWhile the paper is well written and the model is clearly explained, I am not convinced about the soundness and the original contribution of the paper. The main arguments in support of my decision are as follows.\n\nI fail to comprehend the main novelty of the approach proposed in the paper. The model closely follows the masked language modelling and Transformer architecture. In my opinion, there should be a clear distinction with those models and an emphasis on the originality of the proposed approach. \n\nIt seems that the choice of decoding strategies has substantial impact on the performance of the model. In this paper, decoding strategies  followed in Devlin et al. How robust are these strategies with variation in the tasks? Is it possible to learn ps and pc for a given task? \n\nThe inequality in (11) is crucial for the model to learn. Although there is an empirical evidence of this shown in Fig. 3, there is no intuition explained in the paper. After all, the parameterized policy is learned in a greedy manner, so one would expect an argument in the line of stochastic gradient descent. \n\nThe advantage of the proposed method over Transformer in in the decoding complexity when T << I. I fail to see the guarantee of that happening. "
        }
    ]
}