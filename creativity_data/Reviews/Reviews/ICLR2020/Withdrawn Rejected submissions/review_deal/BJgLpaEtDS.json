{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper received 3, 3, 6. All reviewers agree that the method is technically interesting. The main concern shared by the reviewers are the experiments which are somewhat underwhelming. The AC believes that this is a solid technical paper that needs a little bit more work. The authors are encouraged to strengthen their evaluation and resubmit to a future conference.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The work extend the variational autoencoder model to the hyperbolic space for exploring the hierarchical data structures. Experimental results on the synthetic and real data sets show the performance of the proposed model. In general, the paper is well organized.  The proposed algorithm is promising. My main concern is the motivation of the paper and the experiments. In particular, \n \n(1) It is unclear what is the major contribution of this paper over the existing work, e.g., [10,22,23]. The authors may want to provide more discussion regarding this in the literature review. \n(2) It is unclear where is the impact of the proposed PWA model in real applications. Also, it is unclear why the data like MNIST, CORA, CITESEER, PUBMED exhibit hierarchical data structure. \n(3) The experiments are questionable. In table 2, why  CORA, CITESEER, PUBMED are adopted as the evaluation datasets? why only VGAE is considered as the baseline methods in link predict? "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this paper, the authors proposed a Poincare Wasserstein autoencoder for representing and generating data with latent hierarchical structures. \nThe proposed model extends the Wasserstein autoencoder to the hyperbolic space. It is a new and powerful member of the family of VAEs. \nA hyperbolic Gaussian reparametrization method is designed and a Wasserstein loss with MMD regularizer is applied as an objective function.\nThe paper is well-organized and easy to read. The notations are clear. \n\nOverall, I think this work is interesting. My main concern is about the experiments. \nThe results on MNIST is not sufficient to demonstrate the usefulness of the proposed method. \nAs the authors mentioned, a potential reason is that the MNIST is not a typical hierarchical dataset. \nI suggest adding experiments on real-world hierarchical datasets, e.g., representing/generating sentences (which can be viewed as trees of words) or articles (which can be viewed as trees of sections/subsections)."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "*Paper summary*\n\nThe authors endow a variational auto-encoder with a hyperbolic space for the latent space. This enables them to model hierarchically ranked relationships in the data in a natural way. They use a Wasserstein formulation instead of the standard ELBO, which they claim reduces the variance of the gradients during training.\n\n*Paper decision*\n\nThanks for an interesting paper, I enjoyed reading it. While there are parts of this paper I like very much and I think it is technically sound, I feel the experiments section is lacking somewhat (see comments below). I therefore am recommending a weak reject; although, upon a favourable rebuttal would happily upgrade this position.\n\n*Supporting arguments*\n\n- In the introduction is well written and the motivation for the introduction of a hyperbolic later space is well laid out.\n- The section on Hyperbolic geometry is very well written indeed and presents this non-trivial material in a very simple to understand manner. Maybe what it lacks a bit is motivation for a machine learning audience, for why these mathematical tools from differential geometry would be useful or needed by the community to tackle presenting machine learning problems.\n- The structure of the method is well laid out and every hyperbolic version of an auto-encoder operation is explained. I would prefer to see methods from cited works, which are deployed in this paper, written in the paper instead of just referred to, so that I don’t have to search through the cited works to find out how a key operation works.\n- I think the method itself is technically sound. \n- I am not sure about the novelty of the method, seeing that there is a very similar paper from earlier this year (Mathieu et al. 2019).\n- The experiments are lacking a little. I do not feel that they show a clear reason why a hyperbolic latent space should be used and compared to standard methods, they do not perform any. better. It would have been nice to have a more structured discussion on the merits of the method.\n\n*Questions/notes for the authors*\n\n- Section 3.2, please explain the significance of \\lambda_x^c\n- Please explain in the related work and/or introduce in more detail than you already have the differences and similarities between this work and your nearest neighbour “Hierarchical representations with poincaré variational auto-encoders” by Mathieu et al. (2019). I would like to know the advantages/disadvantages of using a Wasserstein formulation instead of a sampled ELBO.\n- In section 3.3, please point readers forward to the Appendix for a list of gyrovector operations.\n- Please use equation numbers on all equations. \n- In your section on “Dispersion representation” you state: “Since the maximum mean discrepancy can be estimated via samples, we do not require a closed form definition of the posterior density as is the case with training using the evidence lower bound. This allows the model to learn richer latent space representations.” Why exactly does not using a closed form posterior density lead to a richer latent space representation? Please back up this statement.\n- In the presentation of the ELBO, there is a typo after the second equality where p(z) is listed twice.\n- This may just be personal preference on my behalf, but I think a short treatment on VAEs and optimal transport would be useful in the related work and background sections.\n- Experimentally it would have been nice to have a direct comparison between this method and Mathieu et al. (2019), that paper being very similar to this one. At least a shared experiment would have been useful.\n\n\n\n\n\n"
        }
    ]
}