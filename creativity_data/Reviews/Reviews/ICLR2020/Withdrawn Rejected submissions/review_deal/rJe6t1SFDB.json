{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a model for dialogue understanding, named AMUSED (A MUlti-Stream vector representation method for USE in natural Dialogue). The method has three main components to understand a sequence of dialogue utterances. A) syntactic component based on dependency parse of utterances. B) knowledge base module based on entities linked from the utterances using off-the-shelf entity linker. C) memory network module which conveys information from previous utterances. The method uses a triplet loss, which compares the correct question - answer pair, with the negative example (question - randomly sampled answer) pair to train the network parameter. \n\nI don’t think this paper is strong enough for ICLR as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written. \n\n(0) The proposed method lacks novelty: the paper basically combines many existing ideas and stacks them together for the task of answer retrieval for conversational question answer dialog. While the paper argues using “multi-head attention” as contribution at the end of introduction, this is more or less standard practice these days. Overall, I don’t see a technical novelty for this paper. \n\n(1) I don’t think “Next Dialogue Prediction Task” is very well motivated, and this is main evaluation measure of this paper. It’s not very realistic scenario, and proposing this new task makes the work hard to compare with existing work. The authors should motivate this task more clearly. \n\n(2) The experimental result doesn’t support valid comparison to existing methods. Moreover, important experimental details and ablations are missing. While the paper puts together many modules, the experiments don’t justify those modules. For instance, what was the benefit of using KB? Table 1 does not isolate its effect as it’s conflated with memory network component. I’m skeptical whether it does anything. Similarly with GCN. The paper should present the model’s best performance, and best performance - GCN component to show its effectiveness.  The paper is also missing many details: for example, for knowledgeable module, how frequently entity mentions were detected and used? (Because it must have an entity name embedding in GLoVE vocabulary?) \n\nHuman evaluation section also needs more rigor. It isn’t clear how many examples were manually evaluated, and how reliable were the annotators. How would you describe “expert linguists”? Any measure of inter-annotator agreement? \n\n(3) The paper is not very clearly written, leaving readers confused. For example, “distribution bias” at the end of the second paragraph of page 1 should be better explained and defined. So is “Interesting response retrieval” at the end of Section 1. What is “Multi-stream”?  Table 2 is really confusing, from what I understand, the last two rows are of the same system on different datasets. \n\nOther comments:\nIn section 4.1., the paper talks about how syntax information have been helpful. I think it’s a bit dishonest to stress it here, given most state-of-the-art NLP models (pre-trained LMs) doesn’t use any syntax. \nThroughout the paper, capitalization is inconsistent. \nThe paper uses “next sentence prediction” , “next dialogue prediction” interchangeably. Please be consistent.\nPrecision@1 isn’t the accurate or intuitive description of what’s happening. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a unified representation of query-response pairs for response selection in chit-chat dialogues. The representation includes the following four core components: syntactic embedding by graph convolution network, sentence representation from BERT, knowledge embedding based on entity linking to Wikipedia, and context representation with memory networks. The experimental results show the effectiveness of each proposed representation in both automatic metrics and human evaluations.\n\nThe main contribution of this work is to combine various different representations into a single end-to-end model architecture for dialogues response selection. However, each of the proposed ideas is not novel enough. And many parts of the paper are not very clearly written. The experiments also need to be further improved to support the contributions of this work.\n\nPlease find below my major concerns:\n- The motivation of each proposed component is not very clear. I suggest to highlight this more in the Introduction.\n- It would be better to emphasize the contributions of this work in Related Work section. Now the section just lists the related studies with no explicit comparison to this work.\n- I'm not convinced that the proposed methods are novel enough to be presented at the conference. I don't find any significant contribution of this work from the original work for each core component.\n- I'm curious how much the long range dependency problem affects to the dialogue response selection task in general. The experimental results show that the model with 'Bi-GRU & GCN' helped to improve the performances. But there should be an additional configuration only with 'Bi-GRU' with no 'GCN' to see the impact of the dependency parsing.\n- It's a bit confusing why the BERT embedding is considered as a knowledge module along with the actual KB representation with Wikipedia.\n- I'm wondering what if the Bi-GRU in the syntactic module is replaced with BERT. In the experiment, the BERT only model already achieved higher performances than the Bi-GRU+GCN only. It would be also interesting to see the performances of BERT+GCN.\n- It's not clear how to get the KB embedding in Section 4.2.2. Did you take the title of Wikipedia article or also with body texts for each entity? More details are needed.\n- This model uses Stanford CoreNLP for dependency parsing and entity linking. I guess there might have been some errors from CoreNLP models due to the characteristics of chit-chat conversations. I suggest to add some analysis to show the impact of the pre-processing errors to the overall performances.\n- It's not clear why DSTC2 dataset is used in the experiment. I don't think this dataset is appropriate for response selection. I suggest to use DSTC7 Track 1 dataset at https://github.com/IBM/dstc-noesis instead.\n- It would be great to compare the model performances with other stronger baselines on Persona Chat."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors described a complete system that can conduct natural language conversations with human in English. The system consists of multiple layers of state-of-the-art DNN models. There are a number of special consideration in additional to standard bidirectional RNN embedding. The syntactic long-distance dependency is captured by a graph convolution network; multi-head attention layer is used; external knowledge from one-hop wikipedia expansion is added as additional BERT embeddings; a memory module is used to capture conversation history; triplet loss is used to optimize the whole model with a positive and negative response sample for each user input. The resulted system is fairly complete and performs end-to-end conversations  well when evaluated by Persona-Chat dataset and human raters.\n\nThe paper described the system in great detail. Reviewer appreciates the completeness of the system and good evaluation. It can serve as a good reference paper for others to build.\n\nFeedback:\n\n1) The authors use \"multi-stream\" heavily in the introduction and claimed conclusion, but spent much less in the main body to explain why multi-stream is a key innovation or performance differentiator. Reviewer suggests either add more explanation or drop that term.\n\n2) in Section 4.1, authors mentioned self-loop in dependency tree is also included in the graph. Pardon my lack of recent knowledge but it could help other readers like me if you can give some examples of those self-loop dependencies and show why they are important to keep.\n\n3) In Section 53 second paragraph, the candidate set was created by using word-overlapping, is this about any word or some salient terms (I guess the latter). If so, this may exclude the candidates with no direct word overlap but still relevant, such as paraphrases or synonyms. Love to see some notes about the potential effects while acknowledging the difference might be minor.\n\n4) Reviewer didn't see any example conversations from the system. Given the difficulty in evaluating a dialog system objectively and quantitatively, it's equally important to highlight the actual conversations.\n\n5) Will the system be open-sourced? It's a great contribution of the paper if that's the case, otherwise it could be laborious for others to reimplement it as reference system or in real-world application which damages the long-term impact of the paper. \n"
        }
    ]
}