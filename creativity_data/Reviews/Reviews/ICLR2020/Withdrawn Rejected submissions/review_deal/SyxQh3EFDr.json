{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "While the general direction of this work is nice it is unfortunate that the authors did not compare to the many related approaches. \nIt is unclear how the approach would be used for tree structures like wordnet and imagenet. \n\nUnclear novelty due to lack of comparisons to many very related approaches. \n                                                                                                                                                                                                    \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "1 For the segmentation experiments, the authors only compare HCOT with COT. We canâ€™t know whether adopting the hierarchical mechanism will improve the segmentation performance or not.\n \n2 For the image classification experiments, the authors only evaluated the situation of two levels of labels. What is the performance of more than two levels? \n \n3 It seems like there are no validation datasets in the experiments. It is not clear whether they are overfitting or not. \n \n4 In the real-world, the classes often have tree-like hierarchies. This manuscript did not consider this situation. There are also no experiments in dealing with the tree-like label hierarchy.\n \n5 Training by alternating optimization (apply twice loss update from both cross-entropy and complement loss) doubles training steps. Not quite a fair comparison (not sure / also not explained).\n \n6 Not supporting details on why one method is better than the other in specific cases. Lack of insight into why it is better than the state of the art."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\n\nTo utilize the hierarchical label structures, this paper proposed Hierarchical Complement Objective Training (HCOT) which regularizes the entropy of the posterior distribution over the rest of classes (all classes except ground truth class) based on the label hierarchies. The authors showed that the proposed method can be useful for image classification and segmentation tasks. \n\nDetailed comments:\n\nEven though the proposed method improves the performances on image classification and segmentation tasks, the proposed idea looks like a simple variant of existing work [Chen' 19]. Because of that, it is hard to evaluate the novelty of this paper. Also, there are many papers [Bilal' 17, Goo' 16, Xie' 15, Jiang' 17] to utilize the label hierarchies for various purposes in the literature, but the authors did not compare the proposed method with them. \n\nMinor comments:\n\n- In equation (5), is there no hyper-parameter to balance the XE and HCE?\n\n- I think it would be interesting if the authors can consider a case with (partial) hierarchical label structures. \n\n[Chen' 19] Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng Juan. Complement objective training. In ICLR 2019.\n\n[Bilal' 17] Bilal, A., Jourabloo, A., Ye, M., Liu, X. and Ren, L., 2017. Do convolutional neural networks learn class hierarchy?. IEEE transactions on visualization and computer graphics, 24(1), pp.152-162.\n\n[Goo' 16] Goo, W., Kim, J., Kim, G. and Hwang, S.J., 2016, October. Taxonomy-regularized semantic deep convolutional neural networks. In European Conference on Computer Vision (pp. 86-101). Springer, Cham.\n\n[Xie' 15] Xie, S., Yang, T., Wang, X. and Lin, Y., 2015. Hyper-class augmented and regularized deep learning for fine-grained image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2645-2654).\n\n[Jiang' 17] Jiang, Y.G., Wu, Z., Wang, J., Xue, X. and Chang, S.F., 2017. Exploiting feature and class relationships in video categorization with regularized deep neural networks. IEEE transactions on pattern analysis and machine intelligence, 40(2), pp.352-364."
        }
    ]
}