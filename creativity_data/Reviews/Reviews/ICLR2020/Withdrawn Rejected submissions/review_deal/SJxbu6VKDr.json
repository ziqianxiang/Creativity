{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The core of the paper is around a block to gate cnn outputs between layers controlled by a global context. The outputs of the block are combined with the original cnn outputs via a residual connection.\n\nNone of the ideas is particularly new, but the main contribution seems to be to show that we can still get benefits in various tasks by using a very simple setup with l2 global norm per channel for the global pooling context scaled by simple linear transformations per cnn output channel + tanh activation for the gating function. By itself I think this is a good practical finding and good tinkering results should always be welcomed in my opinion.\n\nHowever, had the paper been more succinct, with less attempts at interpretation and without trying to present a fundamentally new unit (we have too many acronyms already), I'd be more open to accept it. Sentences like \"This capability is more consistent with the training process in biological neural networks...\" in section 3  and various attempts at interpretation (as the whole section 4.4) come across as a little pretentious and only obfuscate the main finding in my opinion. \n\nThe experiments are pretty extensive, and I believe just studying the effects of adding global pooling contexts between the layers on the various tasks is interesting by itself.\n\nSome minor comments on presentation:\n\nFigure 1: It would be clearer not to include the residual connection inside the building block (i.e., just tanh and not (1+ tanh). Same for \"Gating Adaptation\" on Section 3. I would just highlight that the proposed gating block is combined with the original output with residual connections.\n\nA short segment of code (as reviewer #3 points out) would make it immediately obvious what is being proposed and highlight the simplicity of it.\n\nConclusion and Future Work: \"We conduct expensive\" -> \"We conduct extensive\" ?\n\nAppendix. Experiments on CIFAR seem unnecessary at this point, and I don't think the visualizations (and accompanying interpretations) add much to the paper.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose Gated Channel Transformation (GCT) for efficient and accurate contextual information modeling for CNN. They introduce using l2 normalization instead of FC-layers (and instead of batch norm, etc.) and they introduce channel-wise parameters for controlling the behavior of the gated adaptation of feature channels. These three parameters (gating weights and biases to control the activation of the gate; embedding weights) per channel.\n\nThe performance is measured in three different experiments on standard datasets (some of them a bit old, however, sufficient); One can see a significant performance increase of the proposed method using various architectures even though the method uses less trainable parameters.\n\nAll in all, the paper reads well and gives sufficient information about the parameters and experimental details used. I just think that the possible reader group would not be so big. It would be great if the code would also be publisehd, and maybe that the whole experiments could be made purely reproducible.\n\nThe paper has a long list of references. I think that some could even be removed (e.g., sometimes you list 4 rreferences for a single statement); furthermore, it is enough to add the citation only once (especially when being in the same sentence).\n\nIt is advised to avoid abbreviations which are very commonly used for different methods in common literature (e.g., GA would be genetic algorithm).\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an alternative to Squeeze and Excitation (SE) Block to learn a gating function that re-scales a feature tensor which is passed as input to it. On the positive side, there are certainly differences in the design between this paper and SE, and the proposed block seems to be more lightweight than SE (although the cost for employing SE is tiny) .Moreover, the authors present a large number of experiments by applying their block to several different problems and architectures. \n\nHowever, on the negative side, I feel that the contribution is too incremental for ICLR, since the proposed block seems like more like a heavy engineered extension of SE. Moreover, comparison with SE is missing from many experiments. It is absolutely critical that SE numbers appear in all reported results."
        }
    ]
}