{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper on extending MLNs using NNs is borderline acceptable: one reviewer is strongly opposed, although I confess I don't really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). I'm not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.\n\nThe remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. My view is this paper brings something new to the table which will interest the community, but doesn't oversell the result.\n\nGiven the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high-quality papers reviewed and the limited space of the conference. I have no doubt this paper will be successful at another conference, and it's a bit of a shame we were not in a position to accept it to this one.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "I think their strong point is at the same time their weak point. \", they do not rely on explicitly specified first-order logic rules.\" My question would be, why not use that information if available as prior knowledge? Perhaps I'd like to see a stronger motivation for the use of having to learn this part. I can see how its might be useful but would be very happy to see more motivation for this. I think I've seen Kotler learn the rules of sudoku but sudoku is such a specified problem that Im not convinced yet this is usueful to learn. Thats a different paper but I missed the motivation for that here too.\n\nI like the honesty of the authors for saying it doesn't scale to larger problems. Regardless, I think this paper is good to push the field in that direction. I particularly like the graph generation task. Graph generation, afaik, is not easy. \n\nWhat I invite the authors to do is to not be restricted by theoretically/principled motivated ways. I believe its better to find things that work well first and then to find a theory (the other way round). This is not enough to reject the paper for me because I do believe this is pushing the field forward in a good direction. If possible I'd suggest to relax the theory and then compare the two models if possible.\n\nIn the contribution it says \"(i) we introduce a new statistical relational model, which\novercomes actual limitations of both classical and recent related models such as \" I would have really liked it to have been spelt out which limitations, very specifically and concisely the paper overcomes in that section.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper presents Neural Markov Logic Networks  (NMLN), which is a generalization of Markov Logic Networks (MLN). Unlike MLN which relies on pre-specified first-order logic (FOL) rules, NMLN learns potential functions parameterized by neural networks on fragments of the graph. The potential function can possibly take into account the constants present using embeddings to better solve transductive problems（otherwise the potential can only use relational structure). To make computation tractable, the size of local potential functions is constrained. Training of this MRF is performed by solving a min-max entropy problem: conditioned on an informative potential, the uncertainties shall be decreased. Experiments on a knowledge base completion task and a graph generation task show superior performance compared to baselines like neural theorem provers.\n\nPros:\n1. no need to specify FOL rules and can potentially discover subtle relations not evident to us.\n2. can be used for generation since the learned rules might be more fine-grained than what we can specify.\n3. it's interesting that on nations knowledge base completion problem even without constant embeddings it works fine, which shows the power of just using relational structure.\n\nCons:\n1. the computation complexity of the global potential function grows combinatorally with the clique size k and polynomially with graph size n, which is unrealistic to any larger graphs than the small molecules, if any higher order statistics matters (e.g. in molecules there are rings).\n\nQuestions:\n1. for training can we use MLE?\n\nOverall this is an interesting work. I think it is a natural generalization of Markov Logic Networks and works on two small problems. I am inclined to recommend this paper to the community.\n\n\n-----updates after reading rebuttal-----\nThanks for the clarification. I don't have further questions.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The following paper provides an extension to Markov Logic Networks(MLNs), by removing their dependency on pre-defined first-order logic rules.  This is handled via neural networks which are able to capture the statistical relations, so-called Neural Markov Logic Networks(NMLNs). As this is an implicit representation from the neural network, the rules act as potential functions on the MLN structure. As general MLN techniques are reliant on domain experts or exhaustive structure learning approaches, NMLNs are able to model more domains as provided in the work with knowledge-base completion tasks and generative modelling of molecules. \n\nThe primary contribution in this body of work is based on the observation that relational structure repeats regularities in the data, and where deriving the statistics of these regularities is what allows for improved accuracy in a model.  The proposed NMLN  is architecturally identical to MLNs with the difference being the addition of the potential function.\n\nAs defined by the paper, fragments are connected subsets derived from relational data. The authors derived sets of fragments with constants defined by values in the data and anonymized fragment sets with integer assignments. With potential functions sampled from the anonymized and the true value fragments. The objective is a search for a maximum-entropy distribution to model the data derived fragments. The neural network aspect comes in the form of the minimum-maximum entropy modelling with weights for given fragments being learned by minimising the entropy of the fragment potential function. Where the model also maximizes the log-likelihood related to the anonymized fragments. The intuition in this work is that by selecting the maximum entropy distribution while also minimizing it by selecting the most informative statistical information for it, we will derive an accurate probability distribution given the possible worlds.\nOverall, the paper performs a well enough job explaining the technical aspects. It does a thorough job explaining the algorithmic detail in the main body, and the appendix provides clear and implementable pseudocode equations. It is not exactly clear to me why the anonymization of fragments is necessary, but the authors suggest this places a greater focus on the graph structure and minimizes the model acting differently with different constants. The min-max entropy modelling also appears to be a novel approach in terms of statistical relational modelling. The results also demonstrate the success of NMLNs modelling on relational data and KB completion.\n\nRegarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains. This is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper. It is also difficult to measure the success of their model with generative modelling as no baseline was present for the molecule experiments. I still view this paper as a positive contribution to MLN research, with a technique that is successful among the few experiments tested.\n\nAt the same time, however, given the emerging contributions in the area, such as relational neural networks (Poole et al.), although the technical development is sensible, I don’t find the contribution that technically novel, and seems somewhat straightforward. Given the preliminary investigations, this is a reject from my side. "
        }
    ]
}