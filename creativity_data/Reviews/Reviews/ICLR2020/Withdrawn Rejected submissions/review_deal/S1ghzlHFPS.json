{
    "Decision": {
        "decision": "Reject",
        "comment": "While reviewers find this paper interesting, they raised number of concerns including the novelty, writing, experiments, references and clear mention of the benefit. Unfortunately, excellent questions and insightful comments left by reviewers are gone without authors’ answers. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": " Summary: \n\n \nThe paper proposed to use Datalog rules to specify the design of the LSTM architecture for event data in continuous time. The LSTM module will be used to model the rate of the events. By incorporating Datalog rules, the paper aims to encode informed inductive biases into the model. \n\nComments:\n\n    After reading the entire paper, I think the main idea of this paper is to use sparse and structured weight matrices (called structural zeros” in the paper) to substitute the dense weight matrices in the original LSTM, and to split the hidden state into blocks where each block refer to a different world’s state.  \n\n\n         How to design the structured weight matrices and how to define the node blocks, it is informed by the  Datalog rules. This design, however, will lead to a huge weight matrix and a very  long hidden state once the types of events and number of entities grow. The proposed model will face a severe scalability issue. From this point of view, only “structural zeros” weight matrices are not enough for an elegant model.                \n\n \n\n        How to smartly share parameters and how to control the number of parameters will be an interesting direction to explore. This submission touches on this a bit but not in a principled way. For example, in Eq. 18(a) and 18(b), the embedding vectors for the grounded predicate is a summation of the embedding vectors of the entities and the predicates. This final embedding is empirically validated or is based on some permutation invariant property? This needs more clarification or some references.\n\n \n\n \n        2.      The presentation needs to be polished. The current writing is not easy to follow. Especially for section 3. The architecture design needs to be clarified more. When I read this part, I felt a little difficult to map the Datalog rules to your model. \n        3        Since you are learning the vector embeddings for event types and entities, what are the advantages of this compared to the marked point process model, where the event types and entities are treated as discrete markers and are a much more parsimonious model. The Datalog rules can also be defined on the marker level by introducing a structured dependency structure over the markers. What are the potential benefits of learning the embeddings? The explanation is missing in this paper. \n\n       4     Lack of references. The proposed neural-symbolic architecture shares some similarities to the following papers: \n          (1) End-to-End Differentiable Proving \n\n          (2) DeepProbLog: Neural Probabilistic Logic Programming \n\n          (3) Neural Logic Machines.\n          \n        What are your contributions and differences in terms of the neural-symbolic architecture design?\n\n \n       As for introducing logic rules to guide event predication, this is not a new topic. Here is a list of references:\n\n \n         (1) PEL-CNF: Probabilistic event logic conjunctive normal form for video interpretation. \n          (2) A general framework for recognizing complex events in Markov logic. \n\n          (3) Learning Bayesian networks for clinical time series analysis. \n\n          (4) Logical Hierarchical Hidden Markov Models for Modeling User Activities. \n\n          (5) Slice Normalized Dynamic Markov Logic Networks. \n       5.         Lack of strong baselines. The paper only did a small-scale experiment study. It only compares a neural Hawkes process model. The experimental evaluation also needs stronger baselines. Specifically, methods that can handle continuous-time (e.g. marked point process) or probabilistic logic methods that can discretize time (as mentioned in the above references). The baselines are not quite strong and appear a bit arbitrary in the paper.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Review for Temporal Modeling via Logical Specification of Factorial\nLSTMs\n\nThis paper addresses a key problem in machine learning: how to control\nthe inductive bias of a model in an interpretable way.  The paper\ncontributes a Datalog-based language that allows a human to hand-code\nstructural assumptions (typically based on domain knowledge) that are\nautomatically translated into sparsity patterns in the parameter\nmatrices of an ML model (in this case, a neural Hawkes process,\nalthough the idea would [probably] generalize to other cases).  The\nlanguage plus structured-neural-Hawkes process is demonstrated on a\nfew very small problems, with mixed results.\n\nThis paper is borderline.  However, I tend to favor rejection because\nwhile the ideas are very interesting (and potentially impactful),\nvalidation of the claims is weak.\n\nContributions:\n\nOn the positive side:\n\nA Datalog interface to specifying structural zeros in parameter\nmatrices is a good idea.  The language is natural, and the high-level\nmapping from structure and objects to low-level parameters seems\nreasonable and potentially useful.\n\nThe method makes it easier to specify an inductive bias.  This is a\nstep in the right direction; but at its heart, this paper does not do\nanything that couldn't have been done by hand - it only makes it\neasier.\n\nThe method is potentially more interpretable than other attempts at\ncontrolling inductive bias (for example, simple weight regularizaion),\nbut see below for why this might be a red herring.\n\nThe paper is very nicely written.  It's clear that a lot of attention\nto detail went into writing it.  Well done.\n\nWeaknesses:\n\nThere are a few major points to criticize about this paper.\n\nFirst, there is no clear learning or prediction benefit.  The results\nare mixed: while it appears that the SHP learns faster than the\nunstructured HP, they appear to be asymptoting at the same point.\nThis is perhaps to be expected, as the structural zeros introduced by\nthe corresponding Datalog program effectively reduce the parameter\ncount, but the shape of the learning curves is unchanged.\n\n(Also: please include error bars in Fig. 2(a1) and 2(a2))\n\nThe proper comparison would probably be to a low-rank parameter\nmatrix, where the parameter count is similarly reduced, but in an\nunstructured way.  That would allow us to disentangle \"parameter count\nreduction\" from \"inductive bias\", which is currently not done in the\npaper. \n\nThe results in Figure 3c are mixed - it appears that SHP is only\nbetter in 1/4 of the cases; in all other cases, the error bars seem to\nindicate that there is no predictive power.\n\nFinally, I am concerned that the method may give a false sense of\nexplainability to the model - why it is true that a highly structured,\nsymbolic language is being used to craft an inductive bias, there is\nno \"symbol grounding\".  That is, there is no guarantee that the neural\npart of the learning algorithm will use the parameters in the way the\nhuman intended it to, because the parameters are ultimately\ndisconnected from the symbols.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper builds an interesting connection between Datalog rules and temporal point processes. The novelty of the approach is to factorize the latent state of LSTM into different blocks that represent three major interactions between temporal events, including: dependency, affects, and updates. The design of the node blocks within the hidden state allows the modeling of fine-grain structure of a given event type. Based on the Datalog program and the logic rules, the intensity function of the temporal point process can be formulated from facts in a database. The problem of enabling a flexible family of intensity functions is one of the most important topics in point processes, and a paper advancing knowledge in this area is certainly welcome.\n\nThe paper is in general well written. Section 2.2 can be more clarified by explicitly comparing the concepts of blocks and entities using \"mind(Alice)\" and \"body(Alice)\" before introducing the hidden state h_mind(Alice)(t). It took me some time going back and forth to understand these examples here. With respect to the design of the Datalog interface, it looks like it covers the assertion involving two arguments. Since these arguments affect the partition of the number of node blocks, it would be more clear to illustrate how to design the node blocks as the number of arguments increases (say beyond 2 arguments). In fact, if we know the number of entities in each event type, say the number of node blocks to partition is 3 per hidden state in advance, we can leverage three separate small LSTMs each of which has the private hidden state with the same number of nodes as that in one of the node blocks. Then, we can determine the interactions among these separate small LSTMs based on the logic rules, so it will be helpful to elucidate the additional advantages of partitioning these node blocks in the same hidden state. The proposed technique mainly considers how to incorporate the block design into the LSTM hidden states as a general sequence model. What is the unique characteristics of Neural Hawkes Process have been particularly exploited from this perspective? It looks like it can be applied to other LSTM-based approach as long as the predictions are functions of the hidden states. For the synthetic experiments, it is obvious that single Neural Hawkes process has more challenges to fit the mixture of processes. It will be more convincing to compare with a mixture model, like \"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering\" with the proposed approach, and the same as in the real experiments. Also, a standard test-of-goodness fit like QQ-plot will also be more useful to improve the experiments."
        }
    ]
}