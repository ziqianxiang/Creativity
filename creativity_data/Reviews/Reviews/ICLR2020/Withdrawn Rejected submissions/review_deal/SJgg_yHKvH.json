{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors take four current graph neural network architectures: 1. The neural message passing networks of Jorgensen et al. 2. The invariant graph networks of Maron et al., 3. The graph isomorphism networks of Xu et al. and 4. The decoding hypernetwork of Nachmani & Wolf and extend them by adding extra layers to learn the wights of the network itself. Networks following this strategy are called hypernetworks and apparently they have shown to have good performance in other domains. \n\nIndeed, on the QM9 benchmark the modified architectures give promising results. Results are also presented on the MUTAG, PROTEINS etc graph classification datasets, but these are really too small to be relevant today. The authors also tried an error correcting code dataset, but the way the results are reported it is hard to understand how good they are.\n\nThe paper is a purely empirical study. Obviously the authors have put a lot of effort into experimenting with different architectures and improving the state of the art starting with four different architectures. However, the way the results are presented is confusing, so altogether the message is not clear. For example, there are two different tables for QM9. It seems like the NMP type algorithms are compared to each other in one table, and IGN type algorithms are compared to each other in the other table. However, it seems like the two sets of algorithms are not compared to each other, and even the units are different in the two tables. Even the units and the order of the targets is different in the two tables, which is super confusing. I don't know how well the GIN and DHN type networks would do on these benchmarks. So overall it is difficult to know what to make of all this."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper applies hyper-network idea to graph-neural networks and reports strong experimental results across different graph-learning datasets. More specifically, for each of three different GNN architectures (NMP-edge, graph isomorphism networks, and invariant graph networks) the paper identifies one particular MLP in the architecture and uses another network to assign the parameters of this MLP as a function of the input. What is novel here is that they use damping (i.e., averaging of the current input and the first input), so as to stabilize training. \n\nI have mixed feelings about this paper: although the theoretical contribution and novelty of the paper are minimal, using hyper-networks seem to significantly improve the performance of existing GNNs. That is why despite a lack of significant novelty in over 9 pages, I have voted for accepting this paper.\n\nAlthough there are typos and sometimes awkward sentences, the writing is generally clear. Since the proposed modification is minimal, the majority of the paper discusses the related works, where the changes are made to an existing GNN architecture. In addition to typos, there are some errors: for example, I think that the variant of the Invariant Graph Networks used in this paper uses matrix multiplication (along the first two dimensions) rather than element-wise multiplication stated in the paper. I also think that the paper should discuss why this use of hyper-networks in the architecture does not affect the invariance properties of the original networks.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "~The authors propose methods to apply hypernetworks to graph neural networks. They also introduce a Taylor approximation to one model with the hypernetwork to allow for stable training.~\n\nI think this work is a very interesting application of hypernetworks, but the contributions by the authors is limited in scope. Moreover, the paper is not clearly written and the results for improvement of the method are not conclusive.\n\nQuestions about the work in decreasing importance:\n\nHow many parameters do your hypernetworks have relative to the control models? How much does one need to tune the hypernetwork parameters?\n\nThe tables shown on pages 8 and 9 need considerable description that should make this work self-sufficient--someone shouldn’t have to read another paper to understand your tables. \n\nTable 1 and Table 3:\n-What are the other Cormorant, Incidence, SchNet, and enn-s2s networks?\n-What are the Targets that you are predicting? Which one is challenging, or should we really care about improving?\n\nTable 4:\n\t-Almost all of your improvements are within the standard error of other models.\n\nTable 5:\n\t-Why don’t these predictions have standard errors associated with them?\n\nI am still confused on the need for the Taylor approximation to the arctanh function. This needs to be explained in much further detail.\n\nWhat am I supposed to glean from Figure 1? What is BCH, POLAR, and LDPC ARRAY? What is the x-axis (EbNo(dB) and y-axis (BER)?\n\nIn section 3.1, what was the decision for the choice of hyperparameters and modeling decisions on page 4?\n\nWhy is “c”, the learned damping factor used throughout the paper learned as a scalar value, and not as the output of the neural network that employs a sigmoid activation as an output?\n\nIn the abstract and introduction, I am confused by the phrase: “We tackle this by combining the current message and the first message”, especially since the word “message” has not been described in the abstract. Please clarify.\n\nOther notes that did not impact paper review:\n\nThroughout the paper, the word “parameter” is used instead of “metric”, “statistic”, or condition, e.g. from page 6: “...the NMP-Edge architecture achieves state of the art performance on 9 out of 12 parameters, and in only one parameter it is outperformed by the original NMP-Edge model.\n\nIf the hypernetwork is changing due to input molecules or proteins, it would be great to show how the weights change to different .\n"
        }
    ]
}