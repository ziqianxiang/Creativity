{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents an analysis of recurrent networks with random weights, showing that certain input phases can lead to more stable neuron trajectories over time. To explain the phenomenon, the author’s present a mean field analysis, and experimentally show that certain input phases lead to more robust readouts.\n\nMajor comments:\n* I found the paper poorly motivated and hard to follow. I didn’t understand the connection to disentangling, which focuses on learning nonlinear (not fixed) feature extractors. A more concise statement of the problem setting and why it is interesting would be useful.\n* Much of the analysis focuses on the canonical angle between the driven subspace and the spontaneous subspace, but this only makes sense when there’s primarily one dimension of variance in each space. What are the dimensionalities of the subspaces you are analyzing? For chaotic activity, shouldn’t it produce similar variance in all dimensions in which case the principal directions are not well defined? I’m generally confused why there are directions with dominant chaotic activity, and how that depends on initial conditions / parameters of the network.\n* The mean field analysis is done in a regime which doesn’t make sense, and the accuracy of the approximations made are never evaluated on any networks (especially the Taylor series expansions about g = 1 + delta isntead of g = delta).\n\nMinor comments:\n* Intro: I didn’t understand the connection between your example of untangling images and the work you present here on entraining neural dynamics.\n* Please define “spontaneous” vs. “chaotic” activity. Use them many times but never define what you mean.\n* You say W is sparse, but if it’s just drawn iid from a Gaussian it doesn’t meet any of the typical definitions of sparsity\n* If the driven a\n* Eqn 2 doesn’t make sense as written. The first principle angle would be the first element of arccos(SingularValuesOf(V1^T V2)), without the indices a and b and the min.\n* Canonical angles go from 0 to pi/2, but the y-axis in Fig 1 goes 0 to 180 for subspace orientation\n* The procedure you use to identify input phases that align with chaotic activity requires sweeping all phases and then computing the one that has the lowest principle angle. This doesn’t seem like a scalable or useful learning procedure.\n* Are the <PC1 notation denoting perpendicular to PC1? Please note that in the text.\n* Fig 2: instaed of using trajectory distance, can you report results on a task where the different inputs are explicitly trained to produce different outputs, and then report MSE vs. the target outputs?\n* I didn’t follow the discussion of attractor states or locally stable channels, and you don’t define either of those concepts. \n* “Can be expanded with Taylor series for small values of g”: g should be near 0, not near 1. You need g > 1 for chaotic activity, but that’s a regime in which the Taylor series approximation you make breaks down.\n* I don’t get how Fig 3 and section 5 are connected to the phase-dependence you observed in previous sections.\n* Section 6: instead of just looking at two phases, could you plot accuracy of the output as a function of phase? If you paired that with a plot of canonical angle as a function of phase it may provide stronger evidence for your claims."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Overview: Using a reservoir of recurrent, randomly-connected spiking neurons, this paper studies how input stimuli may be modified to produce more robust responses. Specifically, this entails adjusting the phase of a sinusoidal input such that the leading principal components of the variation in the reservoir’s response match the leading principal components of its spontaneous and unstimulated chaotic dynamics. The reservoir’s ability to discriminate different stimuli, even in the presence of noise, is shown to be improved by aligning them to different principal components of its chaotic dynamics, both between stimuli and within a single stimulus in the presence of noise. The authors provide a statistical physics-driven outline of the theoretical analysis underpinning this result, and qualitatively demonstrate the effectiveness of their approach to a simple handwriting generation task. \n\nThis paper addresses an important question and provides a result that is of interest, however, the overlap between this and previous related work is quite significant. In particular, this paper is heavily indebted to Rajan et al’s work in “Inferring Stimulus Selectivity from the Spatial Structure of Neural Networks” and “Stimulus-Dependent Suppression of Chaos in Recurrent Neural Networks”, two papers from nearly a decade ago from which it borrows most of its approach and theoretical analysis. The work presented here is a logical extension of these papers, and in some respects lacks originality. That being said, the demonstration that merely aligning the phase of a sinusoidal input (as opposed to identifying a resonant frequency, as in Rajan et al) is adequate to improve stability is an interesting result. The theoretical analysis is not at all new, but does provide a nice way to understand the empirical results, and in its presentation here is more understandable to non-statistical physicists than in the original formulation.\n\nSpecific comments:\n- “Anatomical studies demonstrate that brain”, “Brain actively…” -> most instances of “brain” should be changed to “the brain”\n- Given that this is a standard liquid state machine, it should be explicitly called that and some citations to the relevant literature (e.g. Maass, , \"Real-time computing without stable states: a new framework for neural computation based on perturbations\") should also be provided.\n- It’s difficult to understand why the effect of changing input phase changes (granted, somewhat unpredictably) the orientation of the resulting circular oscillation. Providing some intuition would be helpful.\nIn particular, why is the relationship between phase and subspace orientation in the left plot of Fig/ 1 (c) symmetric about 180°?\n- Reservoir model schematic from fig. 2 should be moved to fig. 1\n- Fig. 1 (a), thetas appear to represent the normal vectors of the first two PCs of activity, which should be made explicit.\n- Equation (2): Notation is unclear. Basis vectors should either be arranged into matrices, or “SingularValueOf” should be changed to an inner product\n- Visualization of the chaotic activity in 3D PC space would be helpful, consider moving it out of the appendix.\n- Fig. 2 (d), dashed lines should be noted as direction vectors in caption\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This papers studies reservoir computing in random RNNs. I *believe* this paper finds that by aligning inputs with directions that are high variance for spontaneous activity (which in this case means phase shifting inputs to specific phases relative to spontaneous activity), that the inputs then more effectively drive the network. I found the presentation in this paper extremely confusing though.\n\nDetailed comments:\n\nIn many places:\n\"brain\" -> \"the brain\"\n\"\\citet\" -> \"\\citep\"\n\nIt's unclear in the abstract whether you are studying biological or artificial neural networks.\n\"brain reformats input information to generate reliable responses for performing computations\"\nI don't know what this means\nOverall, I only had a vague idea what high level results the paper was going to show based on the abstract.\n\nEq. 2 and surrounding text:\nIs theta being used as a scalar angle, and then also being used as a subspace? This is confusing. (it's more confusing in that the first use only determines a relative orientation, while the later provides an absolute orientation)\n\nre: definition of theta_chaos\nAre the spontaneous dynamics in the first two PCs more chaotic than in other directions? I would expect the opposite. What justifies calling these directions the chaotic directions?\n\n\"previous work Rajan et al. (2010a;b) have shown that spatial structure of the input does not have a keen influence on the spatial structure of the network response. Here, we bring in this association explicitly with subspace alignment.\"\nHere, and elsewhere, I am consistently confused about the relationship between spatial structure, structure across hidden units, and phase structure.\n\n\"almost similar\" -> \"similar\"?\n\n\" In the 180◦ phase difference case, we see exactly opposite stability phenomena with “chaos” converging to more stable attractor.\" I have a hard time understanding why these two handwritten words should be expected to have these specific different properties."
        }
    ]
}