{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. This is an important problem that is of general interest in our community.\n\nReviewer 2 found the paper to be clear, provided a set of weaknesses relating to lack of explanations of performance and more careful ablations, along with a set of strategies to address them. Reviewer 1 recognized the importance of being useful for pretrained networks but also raised questions of explanation and theoretical motivations. Reviewer 3 was extremely supportive, used the authors' code to highlight the difference between far-from-distribution behaviour versus near-distribution OOD examples. The authors provided detailed responses to all points raised and provided additional eidence. There was  no convergence of the review recommendations.\n\nThe review added much more clarity to the paper and it is no a better paper. The paper demonstrates all the features of a good paper, but unfortunately didn't yet reach the level for acceptance for the next conference. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. The method only requires in-distribution data for tuning its hyper-parameters and can use a pre-trained classifier directly. Its performance is evaluated with small image datasets, which are commonly used in this line of work. Two additional experiments are performed to analyze the effect of two factors (the layer of the feature and the order of Gram matrix) in the method.\n\nThe overall vote for this paper is a weak reject. The clarity of the paper is good. The primary strength of the work is the very strong performance given the setting, which does not require OOD data for tuning. However, we give a rating slightly below the borderline, because of lack of in-depth explanation of why pairwise feature correlation is helpful for the problem. The explanation could be either theoretical or empirical, while the latter can be a set of carefully designed ablation studies. We do not see sufficient arguments or experiments to ensure the performance gain comes from the pairwise feature and is not from other parts of the design.\n\nThe above weakness could be addressed by answering the following questions:\n1. If the pairwise feature (G) is replaced by the unary feature (F) while keeping the other parts of the pipeline the same (including the use of p-th power), would it reach a similar performance or much worse?\n2. Why does the implementation use the statistics of min/max values instead of mean and variance? The latter can also calculate the deviation for the method using a Gaussian model, which is usually a more natural choice.\n3. What is the motivation for using p-th power, and why does it help?\n\nWe present the additional feedbacks in the form of questions which could further strengthen the work if answered:\n4. Based on Figure 2, having p=10 is sufficient to give a good result. Why does the method still use all p-th power (p=1..10)?\n5. The study in Figure 1 only presents specific OOD data (Tiny Imagenet). Does the trend still hold with other OOD data, such as SVHN or LSUN?\n6. Why are the networks used in figure 1(a) and (b) different? Does the trend still hold when the networks in both cases are changed?\n7. The statistics (Mins/Maxs) to be saved can be very large. The paper provides a strategy to reduce that by using the row-wise sums of G. However, the summation has an effect of mixing the features, causing a weaker signal from the deviation. Why does the method still works with the row-wise sums?\n8. Would the method work if the networks have no batch normalization layer?\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a strategy for detecting out-of-distribution samples based on feature representations obtained via neural networks. Given a test sample, the proposed strategy checks whether the correlation values between the features of the test sample obtained at different channels of the same layer are coherent with those of the training samples known to belong to the estimated class of the test sample. \n\nThe proposed strategy can be applied to pretrained networks, as it only requires the channel activation values to determine whether a sample is out of distribution or not. The studied problem is an important problem and the experimental results show that the proposed strategy leads to some performance gains in comparison to reference methods. However, in my view the main drawback of the study is that it is based on an ad-hoc methodology whose theoretical foundation is not quite clear. In particular, it would be good to provide some more explanations on the following issues:\n\n- What exactly motivates the assumption that the correlation values between different channels of the same layer provides a discriminative characteristics of the classes from each other? It would be natural to assume that different classes will have different activation levels at a certain channel of a certain layer. However, the idea of the paper is to look at how different channels correlate with each other. I cannot entirely grasp the motivation for this, as looking at the feature correlations is a bit more indirect than looking at the features themselves. It would be good to provide the justification of this choice.\n\n- What is the theoretical motivation behind using the p-th order Gram matrix, instead of using the original Gram matrix (e.g. p=1)? Some experimental justification is given, but it is also important to provide some theoretical insight if possible."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper uses Gram matrices for OOD detection. This enables the reliable detection of far-from-distribution examples, which is a long-standing and surprisingly difficult problem in OOD detection.\nThis is the best paper in my batch due to the strength of the results. However, this paper should more accurately reflect the contribution: this helps with far-from-distribution examples, not near-distribution yet OOD examples.\nFor instance, I used their code and found that their technique leads to an AUROC of 79.01% when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set. Likewise, with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95%. This is much worse than currently existing techniques. Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies. This paper makes a solid stride in improving the detection of garbage inputs, but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks.\n\nSmall comments:\n\n> Lee et al. (2018b)â€”to the best of our knowledge, the current SOTA technique by a significant margin\nThis should be again qualified and expanded. If you assume access to knowledge of the test distribution, then Mahalanobis is easily the best. If not, the Outlier Exposure is best. If you assume access to no extra data during training, then the maximum softmax probability + rotation prediction is best [1].\n\n> However, while the OE method is able to generalize across different non-training distributions, it does not achieve the SOTA rates of Lee et al. (2018b) on most cases.\nThere are different senses of state-of-the-art and these should be qualified.\n\nDoes this technique do better if you do 5th and 9th percentile instead of min and max? Is it important to do the min and max with training examples instead of validation examples? (Not a pressing question.)\n\n> Can work without access to OOD validation examples?\nTable 1 is deceptive. OE does not need the \"validation\" examples.\nI suggest two columns instead of one. \"Can this work without knowledge of OOD test examples? Does this use OOD training examples?\"\n\nShow AUROC and AUPR. Detection accuracy is an unusual metric relative to AUPR (OOD as positive).\n\nShow CIFAR-10 vs CIFAR-100 in the tables or I'll downgrade my rating, since otherwise the paper is not leaving an accurate impression.\n\nSince OE is complementary, perhaps this technique can be combined to tackle these near-distribution cases?\n\nThe title is confusing. \"Zero-Shot\" could be applied to various techniques in this space. Perhaps emphasize Gram matrices?\n\nIn their code:\n        validation_indices = random.sample(range(len(all_test_deviations)),int(0.1*len(all_test_deviations)))\n        test_indices = sorted(list(set(range(len(all_test_deviations)))-set(validation_indices)))\nThese indices change with every power, which is unrealistic. Please fix the sets beforehand.\n\nSince neural style transfer, which uses Gram matrices, works much better with VGG architectures than ResNets, does this technique work better with VGG architectures?\n\n[1] Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. Hendrycks et al. NeurIPS 2019.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}