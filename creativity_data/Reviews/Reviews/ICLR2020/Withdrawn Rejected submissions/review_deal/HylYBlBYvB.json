{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a new method for interpreting CNN predictions. This approach takes a pre-trained classifier and determines regions of high-relevance for the prediction by selecting neurons with large activation values, and following them backwards through the fully-connected and convolutional layers. This is quite similar to the approach taken for the \"CNN-fixations\" method of Mopuri et al. The primary innovation in this paper is to select which K neurons in each layer are \"important\" based not only on their activations, but also based on the activations of neurons in the two neighboring layers, before and after. This entails taking a weighted score following eq (1), where the weights are selected via a bandit-style RL algorithm run on a small validation set.\n\nWhile the few figures presented in the paper look plausible, the paper is unfortunately lacking both in evaluation and in presentation of the method. I raise a few issues here as questions:\n\n* In many parts of the paper, including the abstract and algorithm block, the adjacent layers are described as \"support\", e.g. Z \"are the calculated supports\". What does \"support\" mean in this context? There are two common usages I am aware of in ML — support of a distribution (i.e. the set of values with positive probability), and support vectors in an SVM. This doesn't seem to be very analogous to either, and \"support\" as used here is not formally defined. This in particular makes the abstract confusing.\n\n* What is the motivation for the particular functional form of equation 1? Why are the current and preceding layers summed, and then multiplied by the succeeding layer? This seems like a rather arbitrary choice of functional form.\n\n* What sorts of values alpha are considered? It seems like a severe weakness that these are selected from a small, discrete set of possible tuples of values. How many are considered? Are these values strictly positive, or may they be negative? How stable are the learned values across different classifiers, layers, and architectures? How different at each layer are the actual values from those of CNN-fixations?\n\n* It is claimed in section 2 that this method requires no hyperparameters, which seems incorrect. At the very least there are parameters in (a) the gaussian blur, (b) the outlier selection / rejection step, (c) the IOU threshold choice for learning layer-wise weights, (d) the candidate set for layerwise weights.\n\n* Figure 5 is a photo of macaroons. Why is it labelled \"bath towel\"? The perturbed label is \"ice cream\". Are these both perturbed labels? Why does the heatmap not match the image on the \"original\" bath towel label, while finding the object on the perturbed label?\n\n* Figure 4 seems very sensitive to the method by which outlier points are discarded (i.e. the bounding box generation scheme of section 4.1.2). The pointing game metric seems rather unreliable — it doesn't seem to penalize the number of hits, so why not simply throw away all points except for the median? That would probably score 100% unless it completely misses the object!\n\n* It would be great to see some evaluation along the lines of mentioned in \"Sanity Checks for Saliency Maps\", Adebayo et al. 2018. In particular, how well does this work when tested on a network with random weights? How well does this work when tested on data with labels permuted? Tests like these are important to prevent confirmation bias — the few visual examples provided (figs 3, 5, 6) provide good feedback, but are not a substitute for quantitative testing."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The goal of the paper is to identify salient image regions to explain a prediction of a vision model.\n\nThe method proposed is a complex heuristic based on connection weights and activations of the current, previous and successive layer, as well as the neurons already selected in the successive layer. The method makes some intuitive sense, but a lot of the choices seem entirely arbitrary.\n\nSection 3.2 describes a reinforcement learning approach for selecting three scalar parameters to set in the heuristic. Reinforcement learning in this case seems completely unnecessary. The authors could have used a super simple technique like MERT, very common in machine translation for example. Moreover the setting of these three scalar parameters requires some validation data annotated with salient image regions, making the method inapplicable when such data is not available.\n\nThe evaluation of this method is quite interesting. The paper reports that their method is quantitatively better than competitive alternatives in a \"pointing game evaluation\", where the method is tasked with identifying salient points within a ground truth bounding box, and an \"increate in performance\" evaluation, where the method is tasked with producing a bounding box which should increase the confidence of the model's decision. A number of subjective evaluations are also presented to show that the method indeed works reasonably well.\n\nThe paper would benefit greatly in my opinion from a simpler exposition of the method, from dropping the reinforcement learning step if possible, and from investigating some theoretical backing for the chosen method."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper is concerned with trying to \"explain\" the predictions of a deep conv NN by identifying input neurons which most contribute to the prediction (\"salient input regions\"). This seems to be done in a similar way than a lot of prior work, by defining some sort of relevance which can be backpropagated from the final softmax to the input image. Comparisons to some selected other methods are provided.\n\nI am having a very hard time to understand how the proposed method works in detail. The authors choose a notation that is, nicely speaking, extremely non-standard (using maths operators in circles as indexes, instead of letters, for example). There are equations without clear definitions of the symbols in them, and in general a lot of text talking about \"preceding layer\", \"succeeding layer\", talks about a \"counter\", and other unclear speaking. The equations contain argmax and set unions, and all symbols have at least 2 subscripts and 1 superscript, none of which are explained properly. Despite the abundance of sub- and super-scripts, it is interesting to note that in eq (2), activation and weight are indexed by the same scripts, even though many weights feed into an activation. Somehow, this weight is connected to a \"selected neuron from the succeeding layer\" (which one??). \nSince previous work, like layer-wise BP, ended up with pretty simple formulae, I suspect this to be the same here, but this would be a random guess, as what is written here is (at least to me) inpenetrable.\n\nIn section 3.2, the authors choose to use reinforcement learning in order to learn some parameters, and a bit further down they talk about binary MAB problems. It is beyond me why RL would apply here, even though I know it is in fashion right now.\n\nFinally, there are some results, consisting of reasonably looking pictures. They compare against 3 other methods, while citing a lot more. While I am not an expert in visualization techniques, I note that some very popular ones are just cited, but not compared against (check for example http://www.heatmapping.org/). There is no mentioning of why the authors compare against exactly these 3, because in general the authors do not motivate what is not optimal about prior work and what is better about their proposal.\n\nIt does not help that the writing of the paper is pretty sloppy, so in general most sentences first have to be slightly translated so they make sense in English. This would not be a problem if anything was clearly defined here in mathematical terms, but nothing is."
        }
    ]
}