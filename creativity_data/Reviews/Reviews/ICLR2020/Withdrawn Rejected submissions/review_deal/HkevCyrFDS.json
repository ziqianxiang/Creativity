{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a new way for video to video translation with style control.\nThe author claims to have a better optimization objective with interpolation and style encoder loss.\nA. It would be good to provide video supplementary materials for generated videos. From the generated images,\nIt is hard to compare the performance with RecycleGAN.\nC. The interpolation loss is not very innovative, it is a light variation of recycle loss in RecycleGAN.\nD. Typos are quite a few, for example, in figure. 1: translatier...\nE. Please help re-examine the equation. Why in the optimization procedure, minimizing the discriminator\nloss.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "SUMMARY: Perform unsupervised/self-supervised image-to-label and label-to-image translation on the Viper dataset\n\nCLAIMS:\n- Perform self-supervised training\n- Have consistency in long-term style and short-term content\n\nLIT REVIEW: Sufficient literature review for the task at hand.\n\nDECISION: Good work, but paper needs a lot of improvement\n\nThe work done and effort put to get good results seem worthwhile. The idea is simple enough, but slightly hacky, in the sense that they use cycle-consistency in style domain (as is done with images in a lot of literature), but improve upon RecycleGAN by adding a recurrent neural network to consume multiple frames instead of just one. In addition, possibly to improve the representation learnt, they combine with an interpolation loss, which is another common idea in video literature. It seems like there needs to be a more coherent idea to string the work together. If it is a pure combination of different methods to make the results work, the authors can mention so.\n\nThe writing is not clear, and it seems hasty. A lot more time could have been spent on making the story clearer, having a better explanation of the method. The method seems to be explained in parts in the introduction and in later sections. It would be better to explain it fully in one section instead of in parts.\n\nIt is also not clear that there are two tasks being tackled - interpolation and translation - to achieve the overall objective, I had to realize it myself as I was reading. There is no clear explanation provided as to why interpolation is being done if the overall task is translation. It could be that interpolation is an excellent task to discover good content in videos in a self-supervised manner, but it is not mentioned anywhere in the paper.\n\nSufficient experiments have been done to prove that their method works better than previous methods, on the Viper dataset on translation between images and their semantic segmentation labels. However, the claims of being long-term consistent in style is not explicitly shown in a quantitative manner, it is only perhaps shown qualitatively. First of all, it is not clear what the authors mean when they say \"long-term\": how many frames is the threshold between short-term and long-term according to the authors? Since the translation task is conditioned on style, it need not be that the new style is \"consistent\" in the long term, it is simply conditioned upon. It is one way of tackling \"long-term consistency\", but not very satisfactory.\n\nThere are several mistakes in the content of the paper. The math used is very weak and unnecessary, In my opinion it is alright not to have detailed math, but it is not good to express it badly. The conditional formulations are very wrong, I would advise the authors to recheck the expressions.\n\nUnfortunately, it is not possible to verify how good the method is until:\n1) examples of the videos are shown, and not just images. This could be done by making a website and uploading example videos.\n2) the code is released.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a network architecture for video-to-video translation. The architecture is essentially an encoder-decoder  with recurrent units in the latent space for modeling the motion dynamics.  \n\nThe paper is quite poorly written. \n- The title doesn't say much about what is in the paper.\n- The paper is barely motivated and it seems that the goal of the paper is just to improve the video to video translation quality for no good reason. \n- Related work study is mixed with the introduction in a not coherent way.\n- The main contributions of the paper is not clearly stated.\n- In the experimentation section, there is almost no analysis of the results to provide reader with insights. \n- The strongest point of the paper is their experimentation which still can be improved. All of the experiments are on a SINGLE dataset which makes the quantitative results less than acceptable. \n- For a video paper, unfortunately the authors did not provide any video (in the forms of supplementary material) which makes qualitative compression hard as well. \n- The authors compared their model to an \"Improved Recycle-GAN\" but they did not provide any details on what this version is!\n- Comparison with vid2vid is missing from Table 3 to 5 and only appears in Table 6. \n\nOverall, the paper (in the current written form) is weak with marginal contribution at best, surely not suitable for a top conference such as ICLR. Only weak rejecting the paper since this is not my primary area of research (although I'm quite familiar with it), so maybe I'm missing something ..."
        }
    ]
}