{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes an interpretable machine learning method, ProtoAttend, that bases decisions on few relevant \"prototypes.\" The proposed method uses an attention mechanism (possibly sparse, via sparsemax) that relates the encoded representations to samples in order to determine prototypes. The resulting model enables similarity-based interpretability, confidence estimation by quantifying the mismatch across prototype labels, and can be used for distribution mismatch detection. \n\nWhile the proposed model is interesting, the reviewers raised several concerns regarding the choice of prototypes and the evaluation of human interoperation. The paper would benefit from more experiments besides the provided user studies to check if the provided prototypes can help human users correctly guess the model prediction. I encourage the authors to address these suggestions in a future resubmission.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a sample-based self-explaining method for image classification. The basic idea is adopt the attention mechanism to learn the relation between the latent representation of the query sample and training samples, and identify the training samples with higher similarity as the prototype. The classification decision is based on the label consistency between the identified prototypes (with the relation score in attention mechanism as the weight of different prototypes in determining the  label agreement)\n\nThe proposed model is intrinsically interpretable since the prototypes with higher weights can play as the decision explanation. And the authors have conducted experiments to show that such self-explaining mechanism based on attention model can achieve comparable classification accuracy with original black-box models.\n\nThe presentation of the paper is clear and easy to follow. But I have several concerns regarding the choice of prototypes and the evaluation of the interpretation:\n\n1) According to Eq. (2), it seems that all training samples are used as the prototypes (but with different weights). Why not just use the top few prototypes? Would this such setting introduce a lot of noise, since many training samples are from different classes? \n\n2) Since one focus of the paper is to provide interoperation of the classification model, some more experiments are needed to evaluate how well the interoperation is. For example, some crowdsourcing experiments to check if the provided prototypes can help human users correctly guess the model prediction.\n\n3) I think the authors should also compare with the black-box model when we use the attention mechanism as a post-hoc interpretation. One straightforward baseline is that use the black-box model for classification, and pick the top \"prototypes\" with the highest similarity in the latent representation. Such comparison can help to validate if incorporating attention mechanism in the model design can provide better quality prototypes."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The aim of this work is to make deep learning classifiers more interpretable by \"projecting\" each input sample into a small collection of prototype examples (with some weighting over those) and then basing the decision on a combination of the latent representations of the chosen prototypes. In this way, the chosen category can be justified as the input being similar to the selected prototypes. Additionally, this approach makes it possible to obtain a confidence score at test time.\n\nThe choice fo the encoders for the prototypes and the examples is asymmetric (the first using keys and the second queries). This is not justified. Is it empirically better than using the same encoding for both before feeding them to the relational attention?\n\nThis work aims to satisfy many desiderata (listed in section 3). The decisions made to accomplish these are reasonable although somewhat arbitrary. In fact, several ways to encode the desiderata in the loss function are listed in Table 1.\n\nQualitatively, in the presented comparison with influence networks and representer point selection, ProtoAttend seems to choose more representative examples.\n\nIt is not easy to find a direct, quantitative way to compare this type of work with the existing literature, but from a qualitative perspective, the set goal (which is an important one) is achieved.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This work proposes an attention-based prototype learning algorithm, which introduces an attention operation to assign different weights to the prototypes. Comprehensive experiments demonstrate that the proposed method is efficient and effective in various tasks. \n\nI have the following comments:\n-\tThe authors did a really good job in empirical studies, verifying the superiority of ProtoAttend.\n\n-\tThe novelty of the main idea is limited and may provide a limited contribution to the research community.\n\n-\tThe authors clarified that ProtoAttend is an inherently interpretable algorithm. However, the interpretability is proved by na√Øve Prototype learning only. A rigorous theoretical proof should be provided to demonstrate its interpretability.\n\n-\tI would be appreciated if the authors provide the pseudo-code to show the training procedure of ProtoAttend.\n\nOverall, I think this work is not ready for publishing unless the theoretical property is well understood.\n"
        }
    ]
}