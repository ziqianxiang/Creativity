{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents a method for multi-label text classification as a sequence generation task and using Bert model as an encoder. The paper shows that some improvement is obtained over methods on a collection of public and private datasets.\nThe paper's overall contribution and imapct seems quite limited. More concretely, some of the concerns regarding the paper are the following :\n1. The proposed method of using Bert as an encoder does not appear really novel as it has been proposed in other works such as XBert [1] and to datasets at a much bigger scale.\n2. The improvements over other methods is very little (around 0.5%) which can also be result of hyper-parameter tuning among other reasons.\n3. The authors should evaluate their method on much larger scale datasets in multi-label classification and also compare with other state of the art methods such as those available from Extreme multi-label classification repository [2].\n\n[1] XBert : X-BERT: eXtreme Multi-label Text Classification with BERT\nhttps://arxiv.org/abs/1905.02331v2\n[2] http://manikvarma.org/downloads/XC/XMLRepository.html"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This is an empirical paper about the MLTC (Multi Label Text Classification) problem. They experiment on three different models. One method is the famous BERT model, which they use in the multi-label setting, by simply changing the activation function of the last layer of the original model to sigmoid. The second method, is BERT encoder combined with a Sequence Generation Model (SGM). For this one, they replace the encoder of a previous work (Yang et. Al.) with Bert encoder. And finally, the third model, is just an ensemble of the two previous mentioned methods: vanilla BERT and BERT+SGM.\nPros:\nThey have experimentally shown some improvements in the different MLTC metrics with (BERT+SGM) and mixed methods, compared to the vanilla BERT, specially, for data sets with hierarchically structured classes. \nThey have shown better performance of (BERT+SGM) rather than BERT for training with 3-4 epochs, however with higher (20) epochs, BERT will eventually win!\nCons:\nTheir experimental improvement for their mixed method, is not as significant for public data sets, less than 1% on average.\nThey have reported 0.4%, 0.8%, and 1.6% average improvement in miF1, maF1, and accuracy measure for public data sets. I believe this is not exact. \nFor example, the average set accuracy on public data sets is 0.8% instead of 1.6%. Or maF1 measure improvement on average is much less than 0.8%. For RCV1-v2 they have 0.55% improvement. But for the two other data sets they have lost 0.1% and 0.6% compared to the vanilla BERT. I am wondering how that comes to the average of 0.8% improvement on maF1 measure(?)\nOn one data set they have got 1.6% better accuracy but that is not on average, as how they have reported. \n\nI do not see much novelty in this work. They have combined two previous methods. Although it is always interesting to see that previous good methods can be combined to get better models, I expect a better performance improvement compared to the state-of-the-art methods for a paper that is empirically based.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes sequence generating BERT (BERT + SGM) and a mixed model for Multi-label text classification (MLTC). The mixed model is an ensemble of vanilla BERT and BERT+SGM models. The methods are examined on three MLTC datasets in either English or Russian. The mixed model has a 0.4%, 0.8%, and 1.6% improvement in hamming accuracy (HA), micro-F1, macro-F1, and accuracy compared with vanilla BERT.\n \nEven though there are some accuracy improvements, it seems that most of the improvements are coming from the downstream task fine-tuning of BERT. Several questions and suggestions:\n \n1. The mixed model seems to have a better performance on most of the datasets over original BERT on HA and micro F1, while it fails to compete with original BERT on macro F1. This may be a sign that the mixed model is overfitting and cannot generalize to an unbalanced dataset. \n\n2. Next, it would be great to add more math deductive reasoning and model analysis. Can you show the dramatic difference between BERT with one more layer and  BERT + SGM? In figure 1, it looks like we have an attention as a final “decision maker”, but does it make more correct decision compared with the normal BERT?\n\n3. Furthermore, it would be better to have more convincing experiment results. The mixed model performs well around 0.45. Can you interpret with more reasoning or just intuitions? Or is it just a random threshold that we get for this downstream task?\n \n "
        }
    ]
}