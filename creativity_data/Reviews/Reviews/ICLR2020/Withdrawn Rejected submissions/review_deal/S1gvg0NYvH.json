{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies the evolution of the mean field dynamics of a two layer-fully connected and Resnet model. The focus is in a realizable or student/teacher setting where the labels are created according to a planted network. The authors study the stationary distribution of the mean-field method and use this to explain various observations. I think this is an interesting problem to study. However, the reviewers and I concur that the paper falls short in terms of clearly putting the results in the context of existing literature and demonstrating clear novel ideas. With the current writing of the paper is very difficult to surmise what is novel or new. I do agree with the authors' response that clearly they are looking at some novel aspects not studied by the previous work but this was not revised during the discussion period. Therefore, I do not think this paper is ready for publication. I suggest a substantial revision by the authors and recommend submission to future ML venues. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the mean field models of learning neural networks in the teacher-student scenario. The main contributions are summarized in Theorems 2-4, which characterize the stationary distribution of gradient-descent learning \nfor what are commonly called committee machines. Theorem 2 is for a committee of simple perceptrons, whereas Theorem 3 is for what the authors call the ensemble-ResNet, which is in fact a committee of ResNets, and Theorem 4 is for what the authors call the wide-ResNet, which is actually a stacked committees of simple perceptrons.\n\nThese three theorems are straightforward to derive from differentiation of the expected squared loss E[\\rho] with respect to \\rho and equating the result with zero. The argument in Example 2 in Section 2.1 has flaws. The argument on the wide-ResNet is based on the linear approximation, which effectively reduces the stacked committee network to a large committee of simple perceptrons, so that its significance should be quite limited. Because of these reasons, I would not be able to recommend acceptance of this paper.\n\nThe authors do not seem to know the existing literature on analysis of learning committee machines. See e.g. [R1] and numerous subsequent papers that cite it.\n\nIn Example 2, the authors claim that when \\sigma is an odd function \\rho is a stationary distribution if and only if the difference is an even function. This claim is not true in general. Consider the case where \\sigma is a sign function. Then for any function f(\\theta) satisfying \\int_0^\\infty f(\\theta) d\\theta=0 (such functions are straightforward to construct), let g(\\theta)=f(\\theta) if \\theta\\ge0, and g(\\theta)=-f(-\\theta) if \\theta<0. Then \\int \\sigma(x\\cdot\\theta)g(\\theta) d\\theta=0 holds, whereas g is an odd function. This is a counterexample of the authors' claim here. Many more such counterexamples are quite easily constructed on the basis of orthogonal polynomials, demonstrating that the stated stationary condition in Theorem 2 may not be so strong for characterizing the stationary distribution.\n\nPage 2, line 30: Given a function (f -> y)\nPage 2, line 38: I do not understand what the subscript $x$ of the integral sign means.\nPage 2, line 40: The last term should be squared.\nPage 3, line 7: Should \\theta_j be \\theta_j(t)? Should \\rho(\\theta) be \\rho(t,\\theta)?\nPage 3: Both \\rho(t,\\theta) and \\rho(\\theta,t) appear, which are inconsistent.\nPage 3, line 16: under (the the) Wasserstein metric\nPage 3, line 22: \\rho_0(x) should probably read \\rho(0,\\theta).\nPage 3, line 26: \"should follow normal reflection\" I do not understand the reason for it. How \\theta_i(t) should behave when it hits the boundary depends on how the gradient-descent algorithm is defined in such cases.\nPage 6, line 7, page 7, line 18: \"The GD/SGD algorithm\": What is shown here is the GD algorithm and is not the SGD.\nPage 7, line 14: and (vanishes -> to vanish)\n\n[R1] David Saad and Sara A. Solla, \"On-line learning in soft committee machines,\" Physical Review E, volume 52, number 4, pages 4225-4243, October 1995.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the mean fields limit of neural networks and extends the mean field limit treatment to resnets. The paper seems to be presenting non-trivial formal results but I could not really understand what are their specific consequences of interest. What are the concrete insights that the results of this paper bring? Even as a theoretician, I did not really understand what should I take from this paper. As such I cannot recommend its acceptance. But it is well plausible that with better explanations I will understand its value and change my mind. Some more concrete questions/issues follow: \n\nThe introduction several times states: \"First, for the two-layer networks in the teacher-student setting, we characterize for the first time the\nnecessary condition for a stationary distribution of the mean field PDE and show how it relates to the parameters of the teacher network. We also discuss how different activation functions influence the results.\" The outline states: \"Section 2 concerns the mean field description of two-layer networks and provides necessary conditions for stationary distributions.\"\n\nBut where is this discussion of necessary condition? I cannot find a single mention of \"necessary\" in section 2 that only includes formal derivations and statements. Can the result of Section 2 be translated to somewhat more general audience? What is the take-home message there?  \n\nWhere is the discussion on the role of the activation function? I see the two examples stating the formal result for two different activations, but again I am unable to understand what should one take from this?\n\nI could understand the result of section 2.2. about stronger weight-teachers being learned first. But this is completely intuitive in the same way as independent components corresponding to larger eigenvalues would get learned first in PCA. See also\nsimilar conclusions in works on so-called \"INCREMENTAL LEARNING\" in Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. And subsequent results. So this alone does not seem very novel result. \n\nAs I had hard time understanding the section two on the simpler feedforward case, Section 3 was even less clear to me. How does what is done here compare to what is known on resnets and what should somebody interested in resnets (but not specifically in the mean field equations) take out from this. Some non-technical summary of the findings is seriously missing in this paper.\n\nComments that are less fundamental to the overall understanding:\n\n** The mean field treatment was also extended to the multi-layer case in: https://arxiv.org/abs/1906.00193\n\n** The paper present nice account on previous results involving the mean field limit. The manuscript should also discuss the long line of papers analyzing the teacher student setting on one-hidden-layer neural networks. After-all this seems to be the main object of study here so the results only make sence presented against what was previously known. Notably the case of eq. (1) where the 2nd layer weights are fixed to one is a model called the committee machine widely considered in previous literature (in the case of finitely wide one-hidden-layer network).\n\nThe (non-exhaustive) list of paper on the teacher-student setting is (more references are in those papers):\n\n-- The teacher-student setting was introduced in Garder, Derrida'83 (model B, https://iopscience.iop.org/article/10.1088/0305-4470/22/12/004/pdf).\n\n-- In the classical textbook on neural networks Engel, Andreas, and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press, 2001, there is a rather detailed account of many results on the setting from 80s and 90s.\n\n-- Notably the plain SGD was analyzed via ODEs for the teacher-student setting in classical papers: David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E, 52 (4):4225, 1995.\nDavid Saad and Sara A Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. In Advances in neural information processing systems, pp. 302–308, 1996.\n\n-- This line of work was recently extended with a focus on the overparametrized regime (but not infinitely wide) in: Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup, Sebastian Goldt, Madhu S. Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová, NeurIPS'19.\n\n-- There even in analysis with more than one hidden layer in Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized\nneural networks, going beyond two layers. NeurIPS, 2019a.\n\nIt would be really interesting to see a discussion of what is similar and different between these works and results and the present work. And what is the added value of the present one about understanding the teacher-student setting.\n\n\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper studies the dynamics of neural network in the Teacher-Student setting, using the approach pionnered in the last few years.\n\nConcerning the presentation and the review of other works, I am a bit surprise that the \"teacher-student\" setting appears out of the blue, without any references to any paper. This is not a new area! Especially when it concerns learning with SGD, these were the subject of intense studies, especially in the 80s, see for instance:\n* Saad & Sola On-line learning in soft committee machines, '95\n* Exact solution for on-line learning in multilayer neural networks '95\nor the book \"On-Line Learning in Neural Networks\", By David Saad, with contribution of Botou, etc...\n* Many of these results were proven rigorously recently in \"Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup\" by Goldt et al,\n\nThere is a difference with the current formulation: in those papers, both the teacher AND student had a finite-size second (or more) layer, while here, one is working in the mean-field regime where the student is infinity wide. This is indeed a different situation, where the system can be much more \"over-parametrized\". But this does not mean that the subject is \"terra incognita\".\n\nThere are three main sections in the paper, discussing the results.\n\n* The first result is a theorem that, if I understand correctly, says that the stationary distribution of gradient descent on the population loss (i.e. the test error) a necessary condition for the stationary distribution is that it has zero generalisation error (Eq. 4). That seems like an incremental-step compared to previous results that write down the PDE (the four mean-field papers from last year) and it looks very similar to results that show gradient descent provably converges to global optima etc. I am not sure I see the importance of the result. Also, it is not clear \"how long\" SGD should run to reach such a point and this regime might be entirely out of interest.\n\n* Sec. 2.2 also discusses the difference in learning to teacher nodes with large or small weights, resp. Again, this is well-known in the asymptotic limit and rather unsurprising. This is also discussed in, e.g.  in Saxe et al \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\" at least in the linear case.\n\n* The extension to ResNets is definitely more interesting. The authors write down mean-field equations for two models, and prove, if I understand correctly, that it is a necessary condition to recover the teacher weights to generalise perfectly, which, as I said above, seems unsurprising.\n\nIn the end, given the paper is not discussing the relevant literature in teacher-student setting, and that I (perphaps wrongly)\n do not find the results surprising enough, I would not support acceptance in ICLR.\n\n"
        }
    ]
}