{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper sets up the problem of training fault tolerant RL agents as a stochastic game between a stoppage inducing adversary, and an agent that tries the minimize the penalty from stoppage. This should drive an agent to avoid states where there is a high cost for stoppage. \n\nI think training RL agents to be fault tolerant is an important problem, however I am not convinced that the current framework proposed in this paper is the right approach to tackle the problem of catastrophic failures. Are abrupt stoppages the right framework to train RL agents to be fault tolerant? I think a key desired feature for FT systems that this formulation might miss is that of graceful degradation that we might want from our RL agents. I am not sure if the current level abstraction with the optimal stopping criteria for an adversary is sophisticated enough to capture this aspect. \n\nI am not an expert in this area, but I found the setup a fairly straight forward extension of minimax two player games, and value iteration via the Bellman operator. I am not convinced that the theoretical contributions here are extremely novel, and justify acceptance on their own. If the current work could be backed with some illustrative/meaningful demonstrations demonstrating the proposed approach, I think it would make the paper stronger.\n\n\nHow does this work relate to approaches such as CVaR where the agents are trained to avoid high risk (catastrophic) states?\n\nMinor comments/typos:\nEquation 3 — Did you mix up min and max? Stopping time is maximizing the reward?\n\n“which the agent has concern the process at a catastrophic system state.” — sentence seems to go wrong\n\n“the transition kernel P ·” — weird symbol after P\n\nP^a_{sds} — what is ‘d’? I am not sure if it has been defined before?\n\nNotation in 5 is confusing. What is the minimum over?\n\n“Additionally, Theorem 1 establishes the existence of a given by J ? , the computation of which, is the subject of the next section.” — grammar goes wrong"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #2",
            "review": "This paper studies the problem of learning RL agents that are robust to adversarial episode termination. The paper casts this problem as a two-player, zero-sum game between a policy player, which takes actions in the environment, and a termination player, which chooses when the episode should terminate. The paper develops some theory surrounding this problem, showing that the game has a well defined value, the Bellman operator continues to be a contraction, and the optimal policy for the termination player is well defined. The paper then proposes an algorithm for approximately finding the optimal policy.\n\nI am leaning towards rejecting the paper, primarily because it is unclear whether the proposed algorithm actually works. I would consider increasing my score if the paper were revised to include experiments showing that the proposed algorithm does, indeed, find the optimal fault-tolerant policy, and does so better than naive baselines (e.g., training with an environment that terminates uniformly at random).\n\nI am not an expert in stochastic games and stopping games, so it is not immediately obvious to me how significant the theoretical results are. For example, why does the existence of a Nash equilibrium not follow immediately from the Nash existence theorem?\n\nMinor points:\n* The large number of abbreviations (SG, OSP, FT, SPE, BR) make the paper a bit hard to read. Since most are only used a handful of times, I'd recommend spelling each out.\n* \"optimal stochastic control\" -- I think that \"stochastic optimal control\" is slightly more standard wording.\n* \"and \\gamma is a discount factor\" -- One of the square brackets is backwards.\n* \"is initialised with weight vector…\" -- What does r_0(P) mean? (Isn't P the dynamics of the MDP?)\n* \"Enhancing R&D…\" -- This citation is repeated in the bibliography.\n\n--------------------- UPDATE AFTER (NO) AUTHOR RESPONSE ----------------------\nSince the authors did not post a response, I maintain my vote to \"weak reject\" this paper. I do think that the problem considered is interesting, and would encourage the authors to (1) clarify how the theory differs from prior work and (2) run experiments to empirically validate the algorithm. If these were done, I think the paper would make a strong submission to a future conference.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper tackles the problem of learning optimal policies robust to system failures. This problem is modeled as an MDP in which there is some unknown process (potentially an adversary) which may terminate the agent at any point, and exact a termination penalty on this agent. The paper proposes to approach this problem as a two-player game. The paper provides a number of theoretical arguments to show that the two-player game may be solved via a value-iteration approach.\n\nOverall, the paper lacks sufficient significance (there are no demonstrations of the practical performance of the algorithm) to merit publication in its current state.\n\nAdditional comments:\n-- The ability of the adversary is very unclear to me.  Is it Markovian?  I don't think so, because then the back-up value iteration approach would not work.  The use of \\Omega after Eq 2 is also unclear.  Can the authors please clarify the mechanism in which the adversary operates?\n-- The paper lacks practical experiments.  I encourage the authors to either submit to a more appropriate venue or add experiments to the paper."
        }
    ]
}