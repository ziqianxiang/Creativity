{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new algorithm to learn latent disentangled representation in the framework on variational autoencoder (VAE).  Keeping the backbone architecture, an unsupervised approach and a supervised approach were proposed.  For the former, a PCA based decoder was introduced to the latent layer which concurrently models geometric transform and principal components.  The supervised approach, on the other hand, resorts to an adversarial excitation and inhibition mechanism.  The experimental results demonstrate better disentangled representation learned for classification, meta-learning, and synthesis/sampling.\n\nThe paper is clearly motivated and easy to follow.  Disentangled representation learning is an important task which has been addressed by a large of papers recently.  The approaches and models leverage existing methods such as RASL (Peng et al., 2012), the excitation-inhibition mechanism (Yizhar et al., 2011), domain-adversarial neural networks (Ganin et al., 2016), etc.  The introduction tried hard to delineate their difference to the proposed approach.  I think the novelty in methodology meets the minimum requirement for ICLR, but as a result, the bar for the experiment part needs to be high.\n\nIn experiment, a variety of datasets are used for supervised, unsupervised, and few-shot learning, using reconstruction error, classification error, and visualization.  Ablation study was also conducted on the impact of geometric transformation and adversarial excitation and inhibition.  My major concern is that the comparisons have been made only against weak baselines for disentangled representation learning, such as VAE and \\beta-VAE. The paper did include CC\\beta-VAE and JointVAE, but only for qualitative evaluation in unsupervised learning.  Given that there are so many variations of disentangled learning algorithms for supervised and unsupervised learning in the recent two years, why not compare with them?  Many of them are also based on VAE, and use mutual information.\n\nTo summarize, the paper presents an incremental modification to VAE to learn disentangled representation, and experiments lack comparison with strong baselines.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "A variant of VAE, named Guided-VAE is presented in this paper. The basic idea is to add guidance in the learning of VAE by adding a sub-component. Unsupervised and unsupervised models are built to introduce guidance information (affine transformation in unsupervised model and meta-data/class labels in supervised model). Experimental results show that the idea does work well. Specifically, I have the following comments.\n\n1. I think the affine tranformations in Equation (3) is pre-specified, but it is unclear in the paper. Please clarify.  The affine transformation part in the first term of Equations (3) also needs more explanations. \n\n2. It is unclear how traversal of latent variables was conducted. \n\n3. In data types other than images, how is the unsupervised model guided? Can you give a specific example not in the image domain? \n\n4. In Figure 3, is the PCA basis interpretable?\n\n5. I was not surprised that the the proposed method works better than beta-VAE in Figure 4, because these factors are supervised imposed to representation learning. The interpretation of z^rst was not explore. It is important and interesting to investigate that whether these variables not enforced in supervision also produce disentangled representation. \n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "# High-level assessment\n\nThe manuscript proposes two different extensions of variational autoencoders for learning disentangled representations of image data. The first is an unsupervised approach to disentangle factors that deform the generated images from any other factors such as content whereas the second is a fully supervised approach for disentangling a subset of observed binary factors from any other latent factors.\n\nFrom a purely methodological perspective, these could constitute a fair contribution, albeit leaning on the incremental side (e.g. [8-14], also see major point 1 below) and lacking a sound theoretical motivation as presented by the manuscript. \n\nUnfortunately, while further pursuing these contributions might be worthwhile, I believe the submission in its current form has important shortcomings regarding:\n\n1. Lack of clarity/reproducibility (see major point 2 below).\n2. Insufficiently compelling experimental results (see major point 3 below).\n\nTherefore, while I cannot recommend the current version of the manuscript for publication, I would encourage the authors to address or clarify these issues in their rebuttal.\n\n# Detailed summary\n\n### Method summary\n\nIn this paper, the authors propose two extensions of the variational autoencoder (VAE) framework [1] to learn disentangled representations of images. These two extensions operate on different regimes (unsupervised vs supervised) and are presented and evaluated independently of each other in the manuscript.\n\n###### Proposed extension (A) \n\nThe first extension aims to disentangle generative factors into two groups: (i) those which modify the generated image via affine transformations and (ii) any other factors, including content. Factors within each of those groups might nonetheless be mutually entangled. In a nutshell, the authors accomplish this by incorporating prior knowledge into the model as follows:\n\n1. The latent space is partitioned a priori into two disjoint subsets: (i) a set of $d_{z} - K$ “deformation” latent variables and (ii) a set of $K$ “content” latent variables.\n\n2. A secondary decoder is introduced to complement the VAE decoder. This decoder generates an image by applying an affine transformation, parametrized as an unspecified function of the “deformation” latent variables, to a linear combination of $K$ basis vectors, with weights given by the $K$ “content” latent variables.\n\n3. The VAE evidence lower bound (ELBO) is augmented with an additional mean squared error (MSE) reconstruction loss for the output of the secondary decoder described above, as well as a regularizer to encourage its basis vectors, which are learnt alongside the rest of model parameters, to be pairwise orthogonal.\n\n###### Proposed extension (B) \n\nThe second extension is a fully supervised approach for disentangling a subset of $T$ binary factors, whose ground-truth value is assumed known for all training samples, while the remainder $d_{z} - T$ latent variables might remain jointly entangled. To this end, the authors:\n\n1. Assign one latent variable to each of the $T$ observed factors.\n\n2. Augment the ELBO with $T$ additional losses that simultaneously encourage each observed binary factor to be predictable from its corresponding latent variable while being unpredictable from the rest of $d_{z} - 1$ latent variables.   \nMore precisely, each of these $T$ losses is itself composed of (i) an “excitation” loss, given by the hinge loss between the ground-truth factor and its corresponding latent variable and (ii) an “inhibition” loss, given by the log-likelihood of an unspecified classifier that is to be trained adversarially alongside the rest of the model to predict the ground-truth factor from the remainder $d_{z} - 1$ latent variables following an unspecified procedure.\n\n### Experimental results summary\n\n###### Proposed extension (A) \n\nThe performance of proposed extension (A) is assessed on the MNIST dataset:\n\n1. Qualitatively, by visualizing a small number of latent variable traversals, in comparison with the original VAE [1], the $\\beta$-VAE [2], the CC $\\beta$-VAE [3] and the JointVAE [3].\n\n2. Quantitatively, by (i) reporting the reconstruction error and (ii) evaluating the classification error to predict digit class with an unspecified linear model using the resulting representations as input, in comparison with the original VAE and the $\\beta$-VAE.\n\n3. The disentanglement of the representations obtained by the proposed approach is measured by comparing the classification error of a model trained on the “deformation” latent variables with that of a model trained on the “content” latent variables, corroborating that class identity is mostly encoded on the latter.\n\n4. An ablation study is performed by comparing two latent space traversals of the original proposed approach with a variant that does not apply an affine transformation in the secondary decoder.\n\n###### Proposed extension (B) \n\nThe performance of proposed extension (B) is assessed on the CelebA dataset, modelling \"gender\", “smile” and “color” as fully observed factors. \n\n1. Qualitative evaluation is again based on visualization of a small number of latent variable traversals, in this case compared to the (unsupervised) $\\beta$-VAE alone.\n\n2. Quantitative evaluation is performed by comparing the classification error of models trained to predict the ground-truth factors \"gender\" and \"smile\" from (i) their corresponding latent variable and (ii) the remainder $d_{z} - 1$ latent variables.\n\n3. Results for few-shot classification using the learned representations on the Omniglot dataset [4] are presented, where results are inconclusive relative to the performance of baseline approaches such as Matching Networks [5].\n\n4. An ablation study is performed by comparing one latent space traversal of the original proposed approach with a variant that does not use the \"inhibition\" losses on the \"smile\" factor. \n\n# Major points\n\n### 1) Significance and novelty of the contribution\n\n**The manuscript motivates its two main contributions primarily in comparison to fully unsupervised methods for learning disentangled representations, such as the $\\beta$-VAE. However, I do not believe this comparison is adequate for neither of the two proposed extensions, as they differ in scope and more closely related work and relevant baselines have been missed.**\n\nThe first proposed extension only aims to disentangle a group of factors accounting for deformations that can be represented as an affine transformation from any other generative factors. In particular, this means that: (i) the model uses prior knowledge about the way in which the generative factors affect the data and (ii) the model does not lead to fully disentangled representations, as it only aims to separate latent variables into two groups that might remain themselves entangled. In contrast, unsupervised disentangled representation methods such as the $\\beta$-VAE and its more recent variants (i) aim to discover the factors of variation in the data with no additional implicit biases than those induced by the model architecture and (ii) try to disentangle all generative factors. In this way, I believe this contribution might be more closely related to work that aims to disentangle certain generative factors in specific applications, such as disentangling content from dynamics in temporal data [6, 7] or, more relevantly, disentangling content from style [8] or translation/rotation [9] in image data.\n\nPerhaps most importantly, the second proposed extension, which is fully supervised, ought to be related and compared to other methods for learning disentangled representations which also make use of known ground-truth factors when available, such as [10-12], to cite a few, rather than to fully unsupervised approaches such as the $\\beta$-VAE. Moreover, I believe this contribution should be discussed in relation to prior work which: (i) has already studied the effect of making use of ground-truth factors by incorporating supervision during training with an additional loss, in this case closely related to the proposed “excitation” losses but using cross-entropy rather than a hinge term [13] and (ii) has already proposed measuring disentanglement in terms of the difference in predictability of the ground-truth factor from the assigned latent and other latents, in this case relative to a model using the second most predictive latent rather than all other latents jointly [14].\n\n### 2) Lack of clarity/reproducibility\n\n**The manuscript, in its current form, is severely lacking in clarity and would be, in my opinion, extremely difficult to reproduce.**\n\nA non-exhaustive list of relevant information that is entirely missing from the manuscript includes:\n\n1. The specific form and parametrization of the affine transformation $\\tau(z_{def})$.\n2. What type of classifier is used for the “inhibition” losses, and how exactly is it trained alongside the rest of the model.\n3. How are the model hyperparameters (such as $K$) to be selected, specially in the unsupervised case.\n4. What are the model architectures and training procedures for all variants of the proposed approach and all baselines used to obtain the experimental results.\n5. How were hyperparameters selected for all variants of the proposed approach and all baselines  to obtain the experimental results.\n6. What was the effect of random variability due to changes in the seed and other factors.\n7. How exactly were the results of Tables 2, 3 and Figure 5 (left) obtained, including what classifier was used, how was it trained and its hyperparameters (if any) selected, on which data was it trained and tested and what was the input to the classifier in the case of GuideVAE (when relevant).\n8. Likewise, virtually all necessary details to reproduce the results of Table 4 are missing, including a description of which data splits were used to learn the representations and which were used to train, validate and test the few-shot learning models.\n\n### 3) Insufficiently compelling experimental results\n\n**The experimental results do not clearly convey the benefits of the two proposed approaches, due to a combination of insufficient/inappropriate baselines, limited number of benchmark datasets and metrics, inconsistencies/lack of detail in the way results are reported and not accounting for random variability.**\n\n1. The experimental results should be extended to include datasets and metrics commonly used in the relevant literature. In particular, some datasets with known ground-truth factors such as dSprites, Cars3D, SmallNORB or Shapes3D and disentanglement metrics such as MIG, DCI or SAP should be included as part of the experiments (see [15] for a comprehensive example and relevant citations for each of the datasets and metrics). Likewise, inclusion of other commonly used datasets such as 3DChairs or 3DFaces would also be helpful.\n\n2. Important recent variations of the $\\beta$-VAE, such as FactorVAE [16] or $\\beta$-TCVAE [17] should be included in all relevant experiments and results for CC $\\beta$-VAE [3] and JointVAE [3] should not be provided only for Figure 2.\n\n3. Regarding Figure 2, it appears to me that the latent traversals for $\\beta$-VAE, CC $\\beta$-VAE and JointVAE are identical to those in [3, Figure 3]. If this is the case, it should be clearly stated and be done only with permission from the author, otherwise it could constitute an instance of plagiarism.\n\n4. As discussed above, comparing the fully supervised proposed approach to a purely unsupervised approach like $\\beta$-VAE is, in my opinion, inadequate.  \nInstead, I believe more relevant baselines could include some of the approaches in [10-13], among others.  \nAlso, tangentially related to the inadequacy of the unsupervised baselines for Section 4.2, I am of the opinion that the latent traversals in Figure 4 are presented in a somewhat misleading manner. At the moment, it creates the impression that the traversals for $\\beta$-VAE ought to be aligned with “gender”, “smile” and “color”, respectively. However, once again, these traversals seem to have been reproduced directly from [2], which reports them instead as “age/gender”, “image saturation” and “skin colour”. Since $\\beta$-VAE arrived at those factors in an unsupervised manner, interpretation of their underlying meaning is necessarily inexact, and Figure 4 should ideally be modified to reflect that and include other fully supervised baselines.\n\n5. The results on Figure 5 (left) seem to have been obtained by training and testing the classifiers on the same data used to learn the (disentangled) representations.  Unfortunately, the lack of detail in the description of the experimental setup makes it difficult to assess the correctness of this evaluation procedure. However, I would argue that these classifiers should instead be trained on held-out data, not used for representation learning, in order to assess generalization. The same consideration would apply to the results presented in Tables 2 and 3, which again are hard to assess due to lack of clarity.  \nAlso, results for “color” should be included in Figure 5 to avoid potential “cherry-picking”.\n\n6. Additional latent space traversals should be provided in an Appendix, as qualitative evaluation in the current version of the manuscript relies on a very small number of samples.\n\n7. The ablation studies in Section 5 should not be limited to latent space traversals, but include a quantitative evaluation as well.\n\n8. The results in Table 4 should be reported alongside relevant error bars to allow the reader to assess their statistical significance.\n\n# Minor points\n\n1. The proposed contributions are motivated in a largely heuristic manner. While in my opinion not being strictly necessary if strong and comprehensive experimental results were provided instead, the manuscript could benefit from accompanying the proposed method(s) by a sound theoretical motivation.\n\n2. The second, fully supervised contribution is somewhat limited in that (i) it can only deal with binary attributes and (ii) it requires the full dataset to be labeled, both of which might be unrealistic requirements in practice. Given that prior work exists that partly circumvents those limitations (e.g. [10-13]) it could be worthwhile to study extensions of the proposed approach in that direction.\n\n3. In its current form, I believe that the manuscript reads almost as two individual smaller papers, corresponding to the unsupervised and supervised parts respectively, rather than one cohesive contribution. This could perhaps be improved by presenting and evaluating both contributions towards a common goal, and exploring variations of the approach using both contributions simultaneously.\n\n4. The submission would benefit from being proof-read for English style and grammar issues.\n\n5. Additional ablation experiments could be interesting, such as showing the performance of the secondary decoder in comparison to that of the primary decoder.\n\n# Proposed improvements\n\nI would encourage the authors to address the major points listed above by:\n\n1. Including additional related work as context and clarifying the differences in scope with other unsupervised representation learning approaches throughout Sections 1-3.\n2. Revising the manuscript to clarify all aspects of the proposed approach and the experimental setup to guarantee reproducibility. \n3. Enhance the depth and breadth of the experimental results, making them more directly comparable to prior work and including more relevant/adequate comparison methods. \n\n# References\n\n[1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" In *International Conference on Learning Representations*. 2014.  \n[2] Higgins, Irina, et al. \"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.\" In *International Conference on Learning Representations*. 2017.  \n[3] Dupont, Emilien. \"Learning disentangled joint continuous and discrete representations.\"  In *Advances in Neural Information Processing Systems*. 2018.  \n[4] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. \"Human-level concept learning through probabilistic program induction.\" *Science* 350.6266 (2015): 1332-1338.  \n[5] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" In *Advances in neural information processing systems*. 2016.  \n[6] Li, Yingzhen, and Stephan Mandt. \"Disentangled sequential autoencoder.\" In *International Conference on Machine Learning*. 2018.  \n[7] Hsu, Wei-Ning, Yu Zhang, and James Glass. \"Unsupervised learning of disentangled and interpretable representations from sequential data.\" In *Advances in neural information processing systems*. 2017.  \n[8] Wu, Wayne, et al. \"Disentangling Content and Style via Unsupervised Geometry Distillation.\" In *International Conference on Learning Representations Workshop (Deep Generative Models for Highly Structured Data)*. 2019.  \n[9] Bepler, Tristan, et al. \"Explicitly disentangling image content from translation and rotation with spatial-VAE.\" *In Advances in Neural Information Processing Systems*. 2019.  \n[10] Kulkarni, Tejas D., et al. \"Deep convolutional inverse graphics network.\" In *Advances in Neural Information Processing Systems*. 2015.  \n[11] Narayanaswamy, Siddharth, et al. \"Learning disentangled representations with semi-supervised deep generative models.\" In *Advances in Neural Information Processing Systems*. 2017.  \n[12] Klys, Jack, Jake Snell, and Richard Zemel. \"Learning latent subspaces in variational autoencoders.\" In *Advances in Neural Information Processing Systems*. 2018.  \n[13] Locatello, Francesco, et al. \"Disentangling factors of variation using few labels.\" In *International Conference on Learning Representations Workshop (Learning with Limited Labeled Data)*. 2019.  \n[14] Kumar, Abhishek, Prasanna Sattigeri, and Avinash Balakrishnan. \"Variational inference of disentangled latent concepts from unlabeled observations.\" In *International Conference on Learning Representations*. 2017.  \n[15] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\"  In *International Conference on Machine Learning*. 2019.  \n[16] Kim, Hyunjik, and Andriy Mnih. \"Disentangling by factorising.\" In *International Conference on Machine Learning*. 2018.  \n[17] Chen, Tian Qi, et al. \"Isolating sources of disentanglement in variational autoencoders.\" In *Advances in Neural Information Processing Systems*. 2018.  "
        }
    ]
}