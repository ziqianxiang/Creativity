{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper centers on an unbiased variant of the Mutual Information Neural Estimation (procedure), using the so-called \"eta trick\" applied to the Donsker-Varadhan lower bound on the KL divergence. The paper's contribution is mainly theoretical though experiments are presented on synthetic Gaussian-distributed data as well as CIFAR10 and STL10 classification experiments (from learned representations).\n\nR1's criticism of the theoretical contributions centers on fundamental limitations on finite sample estimation of the MI, contending that the bounds simply aren't meaningful in high-dimensional settings, and that the empirical work centers on synthetic data and self-generated baselines rather than comparisons to reported numbers in the literature; they were unswayed by the author response, which contended that these criticisms were based on pessimistic worst-case analysis and that \"mild assumptions on the mutual information and function class\" could render better finite-sample bounds. Some of R3's concerns were addressed by the author rebuttal and associated updates, but remained critical of the presentation, in particular regarding the dual function, and downgraded their score.\n\nBecause R2 disclosed that they were outside of their area of strong expertise, a 4th reviewer was sought (by this stage, the paper was the revised version). Concerns about clarity persisted, with R4 remarking that a section was \"a collection of different remarks without much coherence, some of which are imprecisely stated\". R4 felt variance and sample complexity should be dealt with experimentally, though this was not directly addressed in the author response. R4 also remarked that the plots were difficult to read and questioned the utility of supervised representation learning benchmarks at assessing the quality of MI estimation, given recent evidence in the literature.\n\nThe theoretical contributions of this submission are slightly outside the bounds of my own expertise, but consensus among three expert reviewers appears to be that the clarity of exposition leaves much to be desired, and I concur with their assessment that the empirical investigation is insufficiently rigorous and does not draw clear comparisons to existing work in this area. I therefore recommend rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "\n\nThe paper contains one main contribution, the unbiased version of MINE obtained using the eta-trick, however a lot of theory is presented and not always very clearly. While some results are interesting (like the constrained ratio estimation interpretation of the dual) some parts are unclear and of lesser relevance. The section on “What Do Neural Estimators of LK or MI Learn” is a collection of different remarks without much coherence, some of which are imprecisely stated. The comparison with the estimator from Pool et al. (2019) could also be much simplified, in particular I would review the list of remarks below theorem 2.\n\nAnother weakness of the paper is that two important aspects in assessing the quality of an estimator are overlooked:  the variance of the estimator and the performance on finite data. Bias is not the only property which matters, both the variance and the dependence on the number of samples should be assessed experimentally, especially when no discussion or theoretical results are provided.  \n\nI liked to see experiments performed on different tasks and datasets but overall that section could be significantly improved. \n\nThe experiment comparing different MI estimators on synthetic Gaussian data is interesting. The plots are however difficult to read, it would be good to make them larger or split the results in different panels. For the 2D and the 20D case, the results reported in the MINE paper are much closer to the true MI than what is reported here, could the authors explain this difference? It would also be good to see some experiments done with a higher ground truth MI, 3.5 is not a lot for higher dimensional cases.\n\nConcerning the Deep InfoMax experiment, we see some improvements when using the proposed MI estimator with Deep InfoMax, however I doubt that this task is an ideal test case for an MI estimator since it has been shown (On Mutual Information Maximization for Representation Learning, Tschannen et al.) that the performance on downstream supervised tasks often does not clearly depend on the quality of MI estimation. \n\n\nSome additional points:\n\nAssuming a finite dimensional feature map in section 3 actually is a loss of generality. \n\nThe proof of lemma 3 is  hard to follow with some notations used without being properly introduced or changed in unexpected ways (eta switches places in f_b(w, eta), what does q do here?, what does PSD mean?). \n\nThe proof of theorem 1 is again unclear. It starts with a typo in the second line (E_y~Q should be replaced by an integral). A couple of lines below a function alpha is introduced without further explanation. \n\nTypos:\n\nThere is one x missing in the proof of Lemma 1 Appendix A.\n\n\n\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper propose a variational bound on the Mutual Information and showed how it can be used to improve the estimation of mutual information. \n\nI am afraid I cannot judge of the quality and correctness of the method introduced by the authors. I am not familiar with the subject and will be a poor judge of the quality of the paper. Nevertheless I find that the presentation of the paper could be improved. \n\nFor instance,  while I enjoyed Fig. 3 that showed the performances of different estimators of the entropy, i add the zoom out on a big screen to be able to see anything at all! Clearly, one the most important figure of the paper will be unreadable when printed! It is also not entirely clear how this figure supports the claim of superiority of the proposed method.\n\nThe only comment I may have is that it would be interesting --since the authors want to apply their bound to the case of neural networks-- to compare with, the rigorous estimation of the entropy of NN with random weights in Gabrie et al, NeurIPS 2018, Fig. 2 . It would be a much challenging task, albeit a synthetic one, than the Gaussian dataset one presented in Fig. 3. \n\n\n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The paper presents to use \\eta-trick for log(.) in Donsker-Varadhan representation of the KL divergence. The paper discusses its dual form in RKHS and claims it is better than the dual form of f-divergence (while I don't think it's a correct claim). Nevertheless, in experiments, we see that it outperforms the original neural estimation of mutual information.\n\n[Overall]\nPros:\n1. The idea of avoiding the log-sum-exp computation in the DV representation of KL is good. One of the main reasons is to get rid of biased estimation. This idea may not be too novel, but definitely is useful.\n\nCons:\n1. I don't agree with some claims in the paper. Nevertheless, these claims are some of the main stories supporting the paper.\n2. The presentation of the paper should be improved. Including the presentation flow between sections, and also misleading part in experiments.\n3. There are TOO MANY typos in equations. \n\n[Cons In Details]\n<The role of discussing the dual formulation.>\nThe paper spends huge paragraphs discussing the role of the dual formulation. And it also introduces \\eta-DV-Fisher and \\eta-DV-Sobolev, which can be seen as extensions of the proposed method.\nNevertheless, the author doesn't present an evaluation using the dual form. It's a pity that this part is missing. Having Section 3 makes the paper contain several sporadic arguments unrelating to the research questions. Re-organize the paragraphs/ presentation is suggested.\nAnother example is introducing perturbed loss function into the proposed loss function to make it strictly convex. This paragraph is misleading and can be moved entirely to the Appendix.\n\n<Claim on the Proposed Method is Better than f-divergence>\nThe author emphasizes (in multiple places) that the f-divergence biases the estimate. And it exemplifies from eq. (15) to eq. (16). Nevertheless, in eq. (16), when r* is optimum, there should be no second term. The author's claim is based on the comparisons between eq. (13) and eq. (15) when assuming only the MMD term reaches zero. The statement may not be fair.\n<Section 4>\nSimilar to Section 3, this section is cluttered. I don't get the reason why the author specifically come up with a new section comparing only to one mutual information estimation approach (a-MINE). Another irrelevant part of the research question is point 4 under page 7. Why discussing the extension of the a-MINE to conditional MI estimation?\nSome Typos: In equation (21) and (22), \\eta is missing. In Algorithm 1, there are too many typos such as missing \\theta under f, entirely wrong equation for the Output.\n\n<MI Estimation Experiment>\nCan the author discuss the standard deviation for various MI estimation approaches? The large standard deviation for MINE seems unusual.\n\n<Self-Supervised Experiment>\nThe author mentioned that the network considering only the first two convolutional layers, followed by a linear classifier, leads to the best performance. Is there no other layer in between? Also, in Figure 4 (a) and (c), is the purple-color layer means the final classification layer? It is a bit confusing.\n\n<Appendix>\nI understand most of the people would not read the Appendix, but I do. Missing brackets, grammatic errors,  missing integrals, wrong notations, missing punctuation marks, ill-structured presentations, etc., are the problems in the Appendix. I would greatly appreciate the author also spend some time in the Appendix.\n\n[Summary]\nI would vote for a score with 4 or 5 to this paper.\nRegarding there're only 3/6, I'm proposing a score of 6 now. But I look forward to the authors' response and then addressing the problems that I identified. I feel the paper should be a strong paper after a good amount of revision.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "This paper develops variations on the Donsker-Varadhan (DV) lower bound on KL divergence which yields a lower bound on mutual information as a special case.  The paper is primarily theoretical with an emphasis on the case where the witness function f is drawn from an RKHS (a support vector machine).  They discuss, and present experimental results where the feature map of the RKHS is computed by a neural network, although the theoretical results largely do not apply to optimization of the neural network.\n\nI have two complaints.  First, the authors ignore fundamental limitations on the measurement of mutual information from finite samples.  See \"Fundamental Limitations on the Measurement of Mutual Information\" by McAllester and Stratos.  This paper makes the intuitively obvious observation that I(X,Y) <= H(X) and one cannot give meaningful lower bound on H(X) larger than about 2 log N when drawing only N samples --- the best test one can use is the birthday paradox and a non-repetitive sample only guarantees that the entropy is larger than about 2 log N.  This is obvious for discrete distributions but holds also for continuous distributions --- from a finite sample one cannot even tell if the distribution is continuous or discrete.   So meaningful lower bounds for \"high dimensional data\" are simply not possible for feasible samples.  Given this fact, the emphasis needs to be on experimental results.\n\nThe experimental results in this paper are extremely weak. They should be compared to those in \"Learning Representations by Maximizing Mutual Information Across Views\" by Philip Bachman, R Devon Hjelm, William Buchwalter\n\n\nResponse to the author response:\n\nThis was written earlier but there was a mishap when I attempted to submit it and it was not actually submitted until comments were no longer available to the authors so I am putting this in the review.\n\nBounding the ratio of the densities already bounds the mutual information.  In order for the actual mutual information to be large (hundreds of bits) the log density ratios must actually be extreme.  We do believe, presumably, that mutual information in video data can be hundreds of bits.  So I am still not convinced that lower bounds on large quantities of mutual information are meaningful.\n\nAlso, your bound, like the DV bound, involves an empirical estimator $\\frac{1}{N} \\sum_{i=1}^N\\; e^{f(x)}$ of an expectation $E_x e^{f(x)}$.  The true expectation of an exponential seems likely to be dominated by rare extremes of f(x) which contribute exponentially to the expectation.  I see no reason to believe that the empirical estimate is meaningful in a real application such as vision.\n\nRegarding the experiments, I do not have much interest in experiments on synthetic Gaussians.  The most meaningful experiments for me are the pre-training results for CIFAR.  But you seem to be comparing yourself to your own implementations of (weak?) baselines rather than performance numbers reported by, for example, Hjelm et al.  or van den Oord et al. (CPC).\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}