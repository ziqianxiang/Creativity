{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This manuscript describes a new extension of the very popular and powerful deep residual neural network, mainly inspired by the Gauss-Newton optimization method. The main idea is to treat the original deep residual network as a difference equation and then add a second-order term to this equation. The idea of treating the residual network as a difference equation is not new, but to the best of my knowledge adding the second-order term seems to be novel. The authors have also presented a few experimental results to validate their idea. The major advantage of this idea is that by using the second-order term, a residual network with fewer layers can achieve similar performance as the original residual network with more layers. As such, using such a network architecture can potentially reduce the GPU memory consumption, which is important since most GPUs have a limited amount of memory which is not enough for some specific applications. Overall, I like the idea presented in this paper, but it also needs some improvement. \n\n1) There is no theoretical analysis of time complexity at each training iteration.  How expensive is it to calculate the second-order term and its corresponding gradients?\n\n2) In addition to the basic residual network, I suggest the authors compare their network with some variants that also treat residual network as a difference equation.\n\n3) I do not think that Fig. 1 is needed. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a modification to ResNet architecture, motivated by Gauss-Newton optimization method. They change the residual block to include a weighted product of residual input with itself. I did not understand how exactly this product operation is defined, it is not clearly explained. In figure 2b there is a sign \"*\", not defined in text, which looks like Hadamard product of input convolved with weight and input itself. I do not see a link between Gauss-Newton and this operation, though, and will let the authors to correct me. The proposed residual block is compared to the original ResNet on a number of image classification datasets. The authors also compare their quadratic residual block to a residual block without non-linearities.\n\nI propose reject mainly because experimental validation is not aligned with paper claims.\n\nThe authors claim that their block aims to \"accelerate convergence of ResNet\", meaning to decrease the number of residual blocks in the network, and test this on CIFAR-10 and CIFAR-100 with 18 to 34 layer networks, claiming that Newton-ResNet is achieving the same performance with fewer blocks. However, state-of-the-art results on these simple datasets can be achieved with even 10-16 layers. On the difficult ImageNet dataset they show no improvement over ResNet, since the proposed network has more parameters than ResNet-50.\n\nIn figure 1 the authors visualize loss surface of ResNet with their modification with original ResNet. It is not evident from the figure why ResNet takes more steps to reach the minimum, and how this it related to the number of blocks. Specific details of this experiment are not provided.\n\nI would also disagree that Gauss-Newton was dominant method for solving optimization problems before deep learning, this is arguable.\n\nThe authors also claim that the proposed residual block eliminates the need of nonlinearity, which is false, because the weighted product is a nonlinear quadratic operation.\n\nAnother remark, CIFAR and ImageNet datasets are so well known that their description could be removed.\n\nExperiments with generative models are a nice addition, but do not support the claims.\n\nPyTorch or tensorflow code defining the proposed modification to residual block would be very helpful."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a new network structure  Newton-ResNet motivated by Newtonâ€™s numerical optimization method. The structure proposed by the author(s) shows less parameters to get comparable performance with ResNet. \n\nQuestions:\n1. Does last term in (2) is a scalar? What's the dimension of $x_t$, please specify the vector dimension since it is confusing.\n2. Why compare with removing activation? We need nonlinear activation to improve the performance. What's normalization you use and why use normalization?\n3. Due to the limit motivation of your structure, maybe you need more comparation with other structures."
        }
    ]
}