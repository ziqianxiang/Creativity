{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper seems interesting and seems to contains relevant material. However, it is not easy to read.\n\n- Section 4.2 is completely obscure. I do not see any importance sampling any weights etc.\nPlease clarify. \n\n- Can you also apply a Multiple Importance Sampling scheme or a Generalized  Multiple Importance Sampling scheme in your method? please discuss.\n\n- Maybe the following paper is related with your work:\n\nL. Martino, V. Elvira, \"Compressed Monte Carlo for Distributed Bayesian Inference\", viXra:1811.0505, 2018.\n\nPlease discuss relationships, differences and possible use in your setup.\n\n- In Section 4.4 you refer to an online scheme, maybe you must use a sequential importance sampling scheme there. Please discuss. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\nThe paper introduces a novel method for quantizing neural network weights and activations that does not require re-training or fine-tuning of the network. To this end the paper notes that weights and activations in a ReLU network can be scaled by a multiplicative factor, in particular such that weight-magnitudes of a layer sum to one, thus defining a probability mass function. The main idea of the paper is to cleverly sample from this PMF such that a histogram of the sampled values produces an approximate integer-representation of the original weights. The quality of this approximation can be refined arbitrarily by taking more samples, however, the value of the bin with the highest count defines the maximally required bit-precision, such that higher approximation accuracy comes at the cost of requiring larger bit-precision. The method is evaluated on several networks on CIFAR-10, SVHN and ImageNet, as well as some language models. On the image classification tasks, the paper also reports previously published relevant results from competitor methods for comparison.\n\nQuality, Clarity, Novelty, Impact\nThe paper is well written, the main ideas are presented clearly and concisely and necessary experimental details are given in the appendix. The main idea of the paper is neat and I have not seen its application to neural network quantization before. Unfortunately I am not sure how the paper exactly fits within existing methods: while the method could in principle be used to train low bit-width networks (<4bit), the results suggest that in this domain the method is outperformed by other methods. Particularly <=2-bit weights and activations are interesting since they can be used to construct networks that replace multipliers with much faster and more energy-efficient bit-wise operations (XNOR bit-counting). But this regime seems not to be the strong suit of the proposed method. On the other hand models in the >=8-bit regime can be readily obtained with most major deep learning frameworks (e.g. Tensorflow lite and Pytorch), such that having to train or fine-tune a model is rarely a problem, unless one does not have access to training data. But even for the latter, many papers have proposed fine-tuning on dummy-data, or not fine-tuning at all (using linear or non-linear quantization schemes). If this is the targeted regime, then the literature review is missing quite a few important references that should also be compared against in the experiments section. So if the method is specifically targeted at the regime between 4- and 8-bit when re-training or fine-tuning is not possible (for some reason), then the motivation, literature comparison and experimental comparison should be much more targeted at methods that perform well in this regime. While I agree that the main idea is neat and it is nice to see it performing reasonably well, I currently do not see how the method provides strong and convincing advantages over the existing body of methods. I am very happy to be convinced of the opposite during rebuttal by the authors or other reviewers of course. Currently however, I would suggest to take a bit more time and think about particular use-cases that the method is targeted at and flesh this out much stronger - I am not sure whether the rebuttal phase is sufficient for this and therefore (currently) vote for rejection.\n\nImprovements\na) The paper and the method needs a clear focus. I personally would roughly group methods into >=8-bit or <2-bit. The first one is becoming the new out-of-the-box standard and the latter allows for very efficient multiplier-free implementations (even on standard hardware to some degree, particularly for ternary/binary networks). Anything in-between might be interesting but would also probably require non-standard hardware, or variable precision hardware to be practically useful. If the target regime is between 2- and 8-bit, please expand the literature review accordingly and compare against relevant methods previously reported that operate well in this regime. \n\nb) If the emphasis is on training-free methods, also adjust the literature and comparisons accordingly, but also have a strong point when and why training-free methods are important (especially when they come with an overhead for each forward-pass during test-time). This does not mean to shorten the literature review or remove comparisons, but add the most relevant ones given that there is such a large body of network compression methods published in the last three years.\n\nc) I currently have a hard time judging to which degree the benefits reported could actually be exploited by (somewhat realistic future) hardware. Is it possible to sketch an efficient way of implementing the activation-quantization (together with quantized weights of course) on some low-bit-width hardware? While the results show that the method can work in theory with <8-bit activations, I am not sure how the quantization scheme could be efficiently implemented on actual hardware. Is it possible to do some back-of-the-envelope calculations on how much computational overhead the quantizations would add to each forward-pass (e.g. for one of the CIFAR-10/ImageNet networks). For instance, how many 32-/16-bit floating point operations would the sampling of activations roughly require, and how many would it then correspondingly save by performing 4- to 8-bit operations? Or could the whole scheme be implemented in low bit-width (<16bits)?\n\nComments\nii) Related to c) above: at which point would the computational overhead for sampling quantized activations in each forward-pass exceed the cost of retraining a network that can natively deal with low-bit activations. I don't expect a quantitatively precise answer here, but just something that gives a rough idea of the orders of magnitude. Currently I find it very hard to judge how expensive the sampling would be after deployment.\n\niv) In the results-tables the fractional numbers for the bit-widths (e.g. 5.6bit) are a bit misleading. While it is ok to report them, the comparison against other methods should probably use the bit-widths rounded up to the next integer. This should be mentioned at least in the text."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThe paper focuses on the quantization of Neural Networks (NNs) for both weights and activations. The proposed method jointly quantizes the weights to signed integers while inducing sparsity. The idea is to leverage both importance sampling to sample from the normalized weights or activations distribution together with Monte-Carlo methods. Indeed, a weight can be \"hit\" zero times (inducing sparsity) or multiple times (thus being quantized as an integer).\n\nStrengths of the paper:\n-  The authors exploit the rescaling-invariance of ReLU NNs to sample from the CDF of weights by normalizing them. Furthermore, to increase the number of weights to sample from, they normalize per layer (instead of per input neuron). This is a nice application of the scale-invariance properties of ReLU NNs that has been studied theoretically (see Neyshabur et al,\"Path SGD\").  \n- The experiments are performed on an impressive variety of tasks and models: vision models on CIFAR-10, SVHN and ImageNet (image classification), Transformer on WikiText-103 (language modeling) and WMT-14 (machine translation), LSTM on WikiText-2 (language modeling), DeepSpeech2 on VCTK (speech recognition).\n- The method is clearly explained (in particular by Fig. 1) and the combination of importance sampling together with Monte-Carlo methods in this context seems novel and of particular interest for the community. \n\nWeaknesses of the paper:\n- Although the results are performed on a wide variety of tasks, the compression rates obtained are not on a par with the quantity and abundance of experiments provided. For example, the authors use 8 bits per weight on ImageNet (Table 3), which is a x4 compression ratio compared to fp32 (or x2 compression ratio compared to the model trained in fp16, which is becoming the standard way to train large models, see the apex toolkit provided by Nvidia and its use for instance). As a comparison, see the recent release of PyTorch 1.3 and their tutorial on static quantization: 0.9 percentage points loss in accuracy on MobileNetv2 on ImageNet (4x compression ratio). Using mode epochs, this results can be reduced as stated and the methods used are standard (source: https://pytorch.org/tutorials/advanced/static_quantization_tutorial)\n- The argument that the quantization does not need any pre-training is true but maybe its practicality seems limited: companies would be willing to spend some time *once* to get a proper quantized model (including any re-training) before deploying it after to millions of devices. \n- I raise some concerns about the inference time when the activations are quantized. Indeed, while the weights are compressed offline, the activations need to be compressed online. The sorting phase is definitely not linear and may reduce inference time (note: this is concern and not an affirmation). Moreover, does the \"linear time and space\" complexity take into account this sorting phase?\n- Minor typos/incomplete sentences: for example in 4.2, \"Sorting groups smaller values together in the overall distribution\".\n\nJustification of rating: \nThe proposed method is nicely explained and interesting. However, the results are not convincing enough yet. I encourage the authors to (1) publish their code so that the community can build upon this interesting work and (2) to pursue in this direction as quantization has both research and industry implications. "
        }
    ]
}