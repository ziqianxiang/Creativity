{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a method for learning contextualized, word-level sparse representations for the phrase retrieval problem. Their model dynamically computes the weight of each n-gram depending on the context. The authors show that their approach outperforms most competing methods on SQUAD (open) and CURATEDTREC. \n\nThe biggest problem I see is that a BERT-based approach (Wang et al., 2019) outperforms the proposed approach by a large margin and doesn't seem to be discussed. I understand that it's concurrent work, but have the authors investigated if there might be any setting where their method would be preferable? This does seem important.\n\nThe authors further perform an ablation study of different components of the model. The subsequent analysis is short, but I like it, and I think it is helpful in the context of the paper."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposed a novel sparse representation to improve the phrase retrieval question answering framework DenSPI proposed by Seo et al., 2019 (ACL2019). The proposed sparse representation leverages self-attention to reassign weights to the one-hot representations of the n-gram features with the aim of capturing sparsely activated lexical features.  Experiments are conducted on both the SQuAD and CuratedTrec datasets, and show improved performance over the DenSPI strategy. \n\nThe paper is easy to read. My main concern on the paper is its technical novelty and its performance when compared to the state-of-the-art method. \n\nMain remarks:\n\nSince the idea is almost exactly the same as that of DenSPI (published in ACL2019) except the sparse representation, I consider the novelty of the paper is limited. The only difference to me is that the tf-idf representation in the DenSPI is replaced by the one-hot representation of the n-gram with self-attention. Here the self-attention computation is speeded up by a kernel trick. In addition, although the proposed method improves over the DenSPI strategy, but the performance is still much lower than some state-of-the-art methods such as the Multi-passage BERT model as presented in Table 1 of the paper.\n\nOther comments:\n\n1. Some notations in the paper need to be defined, such as d_{se} and d_{c} on page 4. \n\n2. It would be beneficial to present the computational cost on training the self-attention of the n-gram sparse embeddings. \n\n3. Some observations and heuristic tricks deserve further discussions, such as “adding tf-idf matching scores on the word-level logits” in the Negative Sampling section; “adding dense-only loss” in the Kernel Function section. The impact of these tricks should be extensively discussed and included in the paper. \n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Paper Summary:\n\nThis paper proposes a contextualized sparse vectors, called CoSPR, for encoding phrase with in open-domain question answering. It is different from existing static sparse vectors such as tf-idf in that CoSPR dynamically computes the weight of each n-gram that depends on the context.  The authors argument the baseline model DenSPI (Seo et al., 2019) that uses tf-idf with their contextualized sparse representations (DenSPI+CoSPR). Experiments with SQuADOpen and CuratedTREC show the effectiveness of CoSPR.\n\nStrengths:\n\n—The model that uses contextualized encoding by BERT and the training strategy that leverages a kernelization is simple but effective.\n\n—DenSPI+CoSPR achieves 97x speedup in inference compared to a state-of-the-art pipeline approach, BERTserini (Yang et al., 2019). Also, the inference speed of DenSPI+CoSPR is comparable to that of the original DenSPI.\n\n—The paper is well written and well organized.\n\nWeaknesses:\n\n—This study is based on DenSPI (Seo et al., 2019). There is no novelty in the dense representation part, since the focus of this study is on improvement of sparse representation.\n\n—Multi-passage BERT (Wang et al., 2019) clearly outperforms DenSPI+CoSPR in terms of question answering accuracy. However, it is quite slower than DenSPI+CoSPR.\n\nQuestions:\n\n—Can we use CoSPR as a passage ranker to find the passages that contain answer-phrase candidates?  How well the pipeline method of CoSPR (as a passage ranker) and BERT (as a passage reader) work?\n\nReview Summary:\n\nThe paper is well motivated, and the proposed model is simple but effective. I think this paper can be accepted.\n"
        }
    ]
}