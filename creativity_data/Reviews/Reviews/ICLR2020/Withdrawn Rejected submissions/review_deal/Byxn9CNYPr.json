{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors consider the problem of transfer learning by fine-tuning pre-trained contextualized embeddings (e.g., BERT, XLNet) for the task of question-question similarity of medical advice questions. Methodologically, they propose starting with standard pre-trained {BERT, XLNet} and double fine-turning; first with {Quora Question-Question pairs, Medical Question-Answer pairs, Medical Answer-Answer pairs, Medical Question Classification} and then with a Medical Question-Question dataset developed for this study. The basic finding is that first stage pre-training with medical QA performs better than with Quora Question-Question mappings — implying that in-domain is more important than in-task. However, this doesn’t apply to the question classification task. As a byproduct, the small question-question dataset for medical advice is developed and released (and its development is described in good detail). Finally, there is some error analysis and discussion regarding generalizing to other settings.\n\nThe primary finding of this study is interesting, if not entirely surprising, in-domain related tasks result in better contextualized pre-training than out-domain data from the same task. In particular, this is unsurprising for a more technical domain such as medical. Additionally, the released dataset will be useful. However, in its present state, the scope of this study is too narrow to have impact within the broader community as it doesn’t point to how to get this to work in general. Admittedly, this requires some definition of domains (probably around distributional similarity, etc.), task-similarity, amongst other properties — or at least a larger-scale empirical study along these dimensions to conceptually point to a potential theory. As the paper stands now, the most one can say is “for medical QQ similarity, first-stage fine-turning of pre-trained contextual embeddings with medical QA is better than general QQ.” This might be impactful for medical QQ similarity, but isn’t a significant methodological contribution, nor even really a question-answering contribution (as the title would imply), but the narrow statement said above. Thus, I recommend rejecting for ICLR.   \n\nA few less significant comments:\n— I wasn’t able to match the performance numbers in the abstract to the paper. I am guessing somewhere in Figure 2?\n— One direction to go with the paper (IMO) is to follow the title and do various community QA tasks in multiple domains, thus making a contribution in this community (NLP/IR) both in terms of data and methodological interest…and maybe even a new pre-trained embedding.\n— Another direction to go with this paper (IMO) is the transfer learning route. So far, theory in this space has been elusive (e.g., [Ben-David, et al., A theory of learning from different domains, Machine Learning 2010]\n— There has been a lot of transfer learning by fine-tuning contextualized embedding variants lately (admittedly, not a definitive paper yet). I would situate this work better within this space also. For example, https://ruder.io/state-of-transfer-learning-in-nlp/\n\nI hope the authors release the dataset and continue to pursue this direction by maybe first submitting to a medical NLP or question answering workshop/specific track, etc. However, I think significant additional development is needed before this work is ready for a top-tier conference.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Authors studied the value of embeddings from medical specific text data in assessing question to question similarity. They compared different strategies in leveraging the domain specific data, namely on q-to-q, q-to-a, anwer completion and question classification sub tasks. Comparison was also done with previously published in-domain fine-tuned BERT family of models (Bio/Sci/ClinicalBERT). Sound experiments showed statistically significant improvements when medical-domain data is used for fine-tuning the embeddings.\n\nIn addition to the comparison and validation, the authors also promised to release a medium-sized new data set, which has 3000 question pairs with expert (doctors) annotation. The reviewer appreciates the value and cost in the dataset and believes it could be a good contribution to the Q&A research community.\n\nHowever the reviewer is not totally convinced about the conclusion in the paper. The experiments conducted in the paper are limited to medical domain, which is highly specialized and can't support the generalized conclusion: \"... that the semi-supervised approach of pre-training on in-domain question-answer matching (QA) is particularly useful for the difficult task of duplicate question recognition.\"  To have that conclusion, the reviewer would like to see the same comparison in other domains as well.  \n\nNote that in Section 8, authors did try to repeat the experiment to AskUbuntu data, but the result is not convincing. The authors themselves noted \"the improvement may not be statistically significant\" as well.\n\nAnother concern is that there is too little discussion and deep-diving about why only QA helped the task while other in-domain knowledge failed (as noted in Figure 2 and surrounding paragraphs). The experiments shown in Figure 2 suggests not all domain specific data can help embedding, some of them actually made it worse. The reviewer thinks that an investigation of why would add a lot of value to the research.\n\nThe suggestions are \n\n1) Make the claim that matches the evidence. We have two choices here: a) narrow the claims to medical domain, but that limits the scope of the paper too. b) more experiments to gather evidence from other domains too, that means substantial more work.\n\n2) Dig deeper into why only QA actually helped.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors compare a number of different tasks and datasets for fine-tuning in the medical domain. The tasks/datasets are compared by applying a final fine-tuning step (on a new dataset to be released by the authors), and then reporting final performance on a test set. Their experiments show that pre-training a question similarity model on in-domain (in this case medical) information improved over out-of-domain pretraining.  \n\nThey define three pre-training tasks that they compare on different BERT models: 1. Classifying a question-answer pair as matched or not; 2. Classifying a beginning-ending pair of an answer to be a match or not; and 3. Classifying a question-category pair as a match or not. Their experiments show that only the first task helps improve their evaluation task, while the latter two actually decrease performance.\n\nFor the new dataset the authors plan to release, they asked doctors to generate 3000 question pairs by taking a patient generated question from HealthTap and 1) rewriting the question it in a different way 2) come up with a related question for which the original answer would be wrong.\n\n\nI believe that, while a better understanding of pre-training regimes is needed, the experiments in this paper do not offer enough insights to be interesting to the ICLR audience. I’m leaning towards rejection.\n\n\nThe main positive finding, that training the same task with in-domain training data (on medical data-sets) is better than training on out-of-domain data (Quora questions), is not very interesting. The exploration of different tasks to adapt to a domain seems more interesting to me.\n\nHowever, while the set-up of of the different tasks and datasets is interesting, the paper does not offer a good explanation of why two of these tasks have failed. Are there maybe experiments that could be run to exclude obvious shortcomings (e.g. maybe these tasks are too easy and overfit in their training)?\n\nIt would also have been interesting to explore the relationships between the different tasks and look into different training regimes. For example, multi-task training with the different tasks might help prevent over-optimization for a single task and thus help generalize better to the downstream task.\n\nI have one concern with the new dataset used for evaluation, that I didn’t see addressed in the paper. From what I understand, when creating the new dataset, the annotators were asked to create different looking questions with the same meaning, and similar looking questions that have a different meaning. I understand that these instructions were added to counter a tendency to have a correlation between lexical and semantic similarity. However, if these instructions were consistently implemented by the annotators there would be an easy way for a model to learn this relationship.  Have you looked into this possibility?\n\nLastly, while the qualitative analysis of the examples is an interesting starting point, it is hard for me to see if these really show meaningful differences between the models. It would have been more interesting to see a way of quantifying the differences between models instead of just considering a few examples.\n\nA few final remarks:\n\n  - I might have just missed this, but while you report variance for your results (which is great!) I don’t remember seeing any information about how many times each experiment was repeated.\n  - There is little information conveyed in Figure 1, and it could be better used by adding to the discussions.\n"
        }
    ]
}