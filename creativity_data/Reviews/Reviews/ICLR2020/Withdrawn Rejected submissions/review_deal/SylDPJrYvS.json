{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents an approach to avoiding catastrophic forgetting in neural networks trained on classification tasks in an incremental manner. The authors term their approach “EnsembleNet”. The approach is very straightforward: the network architecture utilizes one sub-network for each class that outputs an “opinion” score for an input, which indicates whether the input is a member of the class associated with that sub-network. A multi-layer perceptron then takes the opinion scores of each of the sub-networks and classifies the input. Whenever a new class is introduced, a new sub-network is trained up. The authors provide data showing that this approach does better at avoiding catastrophic forgetting than other approaches that have been proposed.\n\nThis paper was reasonably easy to read, and the motivations clear. However, it suffers from some major flaws that make its contributions quite limited, in my opinion:\n\n1) This approach can only ever work on classification tasks where we know the correct label for the data, or tasks where there is an explicit signal of learning from a new distribution occurring. If we do not know that we are now receiving data from a new category/task, then the network has no way to accommodate the new data. In other words, this approach essentially puts all the onus on the programmer/dataset for indicating whether to use new weights or adapt old ones, unlike other approaches that try to do this in an automated manner (e.g. Kirkpatrick et al., 2017). That effectively makes the problem trivial, by assuming that the hard work has already been done. This is essentially just a standard ensemble approach. Indeed, one could take this approach to the extreme and simply train a new neural network every time data from a new class is encountered. As such, I don’t see how this paper makes any truly interesting theoretical or practical contribution to solving the problem of catastrophic forgetting.\n\n2) The claim that there would be time savings is not well supported. First, the time savings only happen if one assumes that the training is done in parallel on different machines. But, if training of each sub-network is incremental, then it can’t be done in parallel! Moreover, the mathematical analysis in the supplementary is ridden with errors. Aside from numerous typos and errors in writing (see for example the equations at the bottom of page 10, or the broken equation reference on page 11), the basic analysis is wrong in several places, from what I can tell. Consider, “Lemma” 7.1.3: the authors start by assuming alpha <=1, they then demonstrate that this can work for any value of c or |S|, since -4*c/|S| < 0 for all c and |S| > 0. Fine, but that doesn’t prove that alpha <= 1! All that shows is that if alpha <= 1 it does not lead to a contradiction. Indeed, they can’t assume that alpha <=1, because alpha is how the training time scales with the number of datapoints, and there is zero guarantee that this is sublinear. From what I can tell, the “proof” that T_en<T_ord is, in fact, invalid.\n\nSome slightly more minor points:\n\n- Critical measures like Omega_new, alpha_ideal, etc. should be defined, at least in the supplemental, not left for readers to go and look up in another paper.\n\n- I am not convinced that the benchmarking results from Kemker et al. (2018) provide fair comparisons. How well were these networks optimized in that work? Did EWC really only achieve 0.001 for Omega_new? That strikes me as potentially a hyperparameter tuning problem.\n\n- Figure 4 seems to tell us nothing beyond the easily stated: we can train each sub-network on a different computer (but, see my first major point above).\n\n- Is the final classifier only trained after all the sub-networks have been trained? That’s what Algorithm 1 seems to imply. In which case, it’s not truly incremental training!\n\nAltogether, given these various considerations, I cannot recommend this paper for publication at ICLR."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose an ensemble model with deep networks. The authors suggest its relevance for incremental learning. The proposed model is evaluated appropriately. There are many issues with the work. The presentation and description needs a lot of improvement. \n\nWhat exactly do the authors mean by rehearsal? It is confusing term. \"... architecture contains two layers of neural networks and only the neural network in final layer is exposed to rehearsal ...\". Networks themselves have layers, so what does layers of networks mean here? The language needs a lot of improvement. \nAssuming that rehearsal means re-training or tuning or some sorts, the authors propose to split the network into two parts and retrain the second set of layers. How is this fundamentally different from retraining last few layers which is what most state of the art methods do? Further, architecture wise how is the proposal different from learning a layer of length C with softmax activation output for each class-specific index? What are we gaining in terms of learnability here?\n\nAs per description only a fraction of points from the rest of the classes are utilized as negative examples for learning a given class? This fraction seems very important/sensitive? What is its influence in evaluations? When a new class comes in in the form of incremental learning, one needs to retrain all the class specific models! Unlike claimed in the introduction, how is this small rehearsal or how is this different from retraining like in the conventional case?\n\nIn section 3.1, we still have a vector space, and nothing really special. Not need to tag them this separately!\n\nFor tables 2 and 3 we need confidence intervals for better interpreting the performance. "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Paper proposes an approach for incremental learning where a network has to train new classes while not forgetting previously learned classes. \n\nThe work in its current version is not ready for publication and I recommend rejection. \n\n- The proposed method is not clearly explained. The reader is referred to an algorithm for the explanation of the method. -No clear motivation for the method is provided in the introduction. \n-Work is not placed in context of lifelong learning literature (continual learning). \n-Text needs still serious editing. Still includes text like [reference of algorithm], and authors do not cite papers correctly (difference between cite and citep)\n-The introduction only dedicates two lines on the proposed method, which were not clear for me. \n-It would be better if the training and evaluation protocol is explained before starting to explain the proposed architecture. What setting of incremental learning are the authors considering?\n"
        }
    ]
}