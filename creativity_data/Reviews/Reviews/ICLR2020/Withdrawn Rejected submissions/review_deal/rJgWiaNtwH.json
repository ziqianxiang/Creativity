{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to modify the ELBO loss for VAE in order to learn better latent representation of the data. There is a large literature on this topic and the related work section should be written with more care and details. Some references are missing:\n[a] Fixing a Broken ELBO by Alemi et al.\n[b] Learning Disentangled Joint Continuous and Discrete Representations by Dupont\nOn a general level, the motivation for this work is not completely clear. As mentioned in the related work section, one could use a more complex prior such as a mixture of Gaussians or an architecture like [b] to do clustering. At least, in the experiment section, results shuold be compared with these natrual approaches. It should be easy as [b] for example provided his code and gives results on the same datasets as those used here.\nAnother remark is that the title is quite misleading. The connection with MMD is very weak in my opinion. Indeed modifing the KL term in the loss by using MMD has already been proposed in Infovae (last reference of this paper) and in InfoVAE a MMD-VAE is also introduced. This connection should be explained and comparison should be given with this work."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThe authors propose changing the objective function used to train VAEs (the ELBO) to an alternative objective which utilizes an L1-norm penalty on the aggregated variational posterior means. The authors motivate this modification through the maximum mean discrepancy but I was unable to follow this argument --- adding additional details to the paper for this argument would greatly help. The empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup.\n\n\nOverall:\n\n1) One general issue I found with this work is the claim that the KL divergence is the overarching cause of posterior collapse. This remains a popular story within the community but existing work has provided evidence that the actual cause is not so simple. For example, [1] showed that with a sufficiently powerful decoder posterior collapse may occur even when training with marginal log-likelihood. For the authors reference, [2] is a concurrent ICLR submission which breaks down various categories and causes of posterior collapse.\n\n2) In several places the authors write that a smaller KL term leads to less informative latent variables. While I understand the intention of the authors I believe this argument is too imprecise. For example, one can encode an arbitrary amount of information into a single real-valued scalar variable (even doing so when that value is << 1). In which case, all dimensions but one can be collapsed completely (significantly reducing the KL) and the final dimension can feed all information into a decoder powerful enough to decode it.\n\nFurthermore, the breakdown of equation (3) is a bit troubling. The same analysis could lead one to assume that L2 regularization forces parameters to be zero leading to problematic predictions. It is important that the balance between the KL term and the conditional log-likelihood is considered together.\n\n3) Page 1, footnote 3 refers to the KL divergence as \"one such term\" for regularizing a VAE. The KL divergence is necessary for the VAE objective and any other regularization term would imply a difference model. In fact, in this work the model is referred to as the $\\mu$-VAE; while relatively minor I would argue that this is inaccurate as we are no longer optimizing a VAE objective or even recovering a tractable lower bound on log marginal likelihood. This leads into my primary concern, which is that it is not clear what the objective function to be optimized represents or indeed what the advantages in doing so are compared to ELBO.\n\nReplacing the L2 norm of the mean with the L1 norm of the aggregated means (note that this is not merely replacing the L2 norm in the per-datapoint KL with the L1 norm) changes the objective significantly but is not discussed in enough detail in the paper. The authors claim that this is emulating MMD but I am not sure exactly what they mean. MMD depends on a feature map lying in a reproducing Hilbert space and measures the distance in expectation between the features of two distributions. What is the feature map being used here? How is this objective related to MMD exactly? Moreover, the remaining terms in the KL divergence are retained in the objective which adds further confusion.\n\nOverall, I felt that the justification for the change to the objective function was lacking and am ultimately unconvinced that this is a sensible alternative to training with ELBO.\n\n4) How exactly is the latent clipping enforced? There are several possible approaches which may impose different training dynamics.\n\n5) I feel that Section 4.1 is a good inclusion as it allows us to better understand the effects of each of the changes to the training regime. Unfortunately, I felt that the conclusions I drew from these experiments mostly suggested that latent clipping is likely to be most responsible for the performance gains observed in the $\\mu-$VAE. I suspect that the reason we don't see the $\\mu-$VAE without latent clipping is that it experiences unstable learning dynamics and fails to converge. Latent clipping is not applied to the standard ELBO objectives in later empirical experiments --- I believe the authors should also compare downstream utility under the ELBO objective with latent clipping to convince the reader that the $\\mu-$VAE objective really does provide benefits.\n\n6) The empirical evaluation is very limited. The authors compare four different VAE-like objectives on MNIST and FashionMNIST. From the appendix, it seems that the optimizers for all models share the same hyperparameters --- in my experience VAEs are susprisingly sensitive to settings of learning rate and batch size (even when using Adam) and so I would expect a grid search over at least the learning rate.\n\nAdditionally, it seems that the reconstruction loss is simply the L2 distance between the decoder outputs and data and does not include rescaling by the observation noise. This is essentially the same as keeping the observation noise fixed to 1 and has serious ramifications on the quality of the learned latent space [3].\n\n7) The experiments focus on downstream utility which I agree is a valuable metric. However, it would also be valuable to compare the ELBO of these trained models. Even if the $\\mu-$VAE is not trained to maximize likelihood of the same probabilistic model, one might hope that if it reduces posterior collapse it could lead to a better model.\n\n\nMinor:\n\n- In the abstract you claim \"this problem is exacerbated when the dataset is small and the latent dimension is high\". Could you please provide a reference for this claim.\n- \"encoder learns to map the data distribution $p_d(x)$ to a simple distribution such as Gaussian\". This is not quite correct, each data point is mapped to a simple distribution but the data distribution is mapped to a more complex distribution (i.e. the aggregated posterior $q(z) = E_x[q(z|x)]$).\n- What is meant by an \"approximately diagonal co-variance\" at the top of page 5.\n\nReferences:\n\n[1] Fixing a Broken ELBO, Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy\n[2] The usual suspects? Reassessing Blame for VAE Posterior Collapse, Anonymous, https://openreview.net/forum?id=r1lIKlSYvH\n[3] Understanding posterior collapse in generative latent variable models, James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi\n\n\nNote: This review was edited after release to remove a duplicate paragraph. Paragraph indices were updated accordingly.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to modify the KL term ($KL(q(z|x)||p(z))$) in VAE so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \\int q(z|x)p(x)dx to be close to p(z). It is argued that the original VAE objective leads to posterior collapse because it requires q(z|x) to be close to p(z), leading to similar latent code across different data points. \n\nI find the motivation in this paper rather weak, and the authors not sufficiently familiar with prior work. Therefore I could not recommend acceptance at this time.\n\nMore specifically, \n1. the fact that minimizers of E_p(x)[KL(q(z|x)||p(z))] are posterior collapse solutions does not imply this term is a bad regularizer, since in general, the optimization process will not end up in such parameters becaues of the data fitting term. In fact, [1,2] showed that this regularizer should have a desirable pruning effect, removing unneeded latent dimensions while keeping the useful dimensions intact. See also the information-theoretic interpretations of ELBO in e.g. [3,4]. [2] also gives a more rigorous discussion on the relation between ELBO and posterior collapse.\n2. The authors propose to match the marginal distributions in latent space. This idea is explored in previous work such as Wasserstein and adversarial auto-encoders. None of this type of work is discussed in the text or included as baselines.\n3. The proposed regularizer, as shown in eq (6), do not fully emulate the behavior of MMD (or any other divergence measure between the aggregated posterior and prior) as claimed: e.g. mu^{(i)}_d can be arbitrarily far from origin. Thus justifying (6) as such is not convincing.\n\nRegarding the experiments, there is a notable lack of baselines as mentioned above; also, for sample quality it is desirable if quantative measures (e.g. FID or inception score) are included.\n\nFor future improvements, the proposed objective could be worth exploring if a new justification is found. Also, the empirical evaluation should be more thorough.\n\n# References\n\n[1] Diagnosing and Enhancing VAE Models, ICLR 19.\n[2] Understanding Posterior Collapse in Generative Latent Variable Models, NeurIPS 19.\n[3] An information theoretic analysis of deep latent-variable models. arXiv 1711.00464.\n[4] Elbo surgery: yet another way to carve up the variational evidence lower bound. AABI workshop, NIPS 16."
        }
    ]
}