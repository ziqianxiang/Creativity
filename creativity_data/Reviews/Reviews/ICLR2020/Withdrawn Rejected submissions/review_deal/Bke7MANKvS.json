{
    "Decision": {
        "decision": "Reject",
        "comment": "This is an interesting paper that aims to redefine generalization based on the difference between the training error and the inference error (measured on the empirical sample set), rather than the test error. The authors propose to improve generalization in image classification by augmenting the input with encodings of the image using a source code, and learn this encoding using the compression distance, an approximation of the Kolmogorov complexity. They show that training in this fashion leads to performance that is more robust to corruption and adversarial perturbations that exist in the empirical sample set. \n\nReviewers agree on the importance of this topic and the novelty of the approach, but there continue to exist sharp disagreement in the ratings. Most have concerns about the formalism and clarity in the presentation. Especially given that the paper is 10 pages, it should be evaluated against a more rigorous standard, which doesn't appear to be met. I encourage the authors to consider a rewrite with a goal towards clarity for a more general ML audience and resubmit for a future conference.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a novel approach to training classifiers inspired by concepts from algorithmic information theory. Specifically, the inputs are augmented with additional features formed by encoding the image (using a source code). In addition, the paper argues to use the normalized compression distance as a loss criterion when training; this is a computationally tractable proxy for the normalized information distance, which the paper argues is what one really should use to generalize well, but which is not computationally tractable. (In particular, evaluating the Kolmogorov complexity, which is required to compute the normalized information distance, is not tractable.) The idea is demonstrated via experiments training common image classifiers (VGG-11 and ResNet-18) on clean and corrupted versions of the CIFAR-10 dataset, and comparing how they fare against adversarially trained methods on noisy and adversarial examples.\n\nI believe that this paper contains interesting and novel ideas, but there are also some inconsistencies and some points that are vague. I would consider raising my score if the authors address or clarify these issues, which are detailed below.\n\nThe introduction discusses generalization very broadly, suggesting that the usual definition of generalization (e.g., accuracy on a hold-out set, drawn from the same distribution as the training set) is not the version of generalization that one should focus on. However, the precise definition of generalization is never provided. The intro also mentions briefly about adversarial examples, and this is the focus of the experiments. It would help to clarify whether the paper focuses on being robust to adversarial examples as a form of generalization, or more precisely, what how is the concept of generalization defined/measured in this paper. (E.g., the first sentence of Sec 2.1 mentions \"our running definition of generalization,\" but no precise definition has yet been provided.\n\nTo make the paper self-contained, it would help to recall the formal definition of Kolmogorov complexity; this could be done in the appendix.\n\nSec 2 begins to make the connection between a classifier and a source code. However, these definitions are not precise. For example, in the usual PAC learning setting one has a distribution D over the training data (x,y). After specifying a particular loss function, the Bayes-optimal classifier is well defined. Does the source code C correspond this classifier? I would guess not, since the paper seems to be introducing a specific loss criterion, but it would be good to clarify this in the paper. Since you are assuming there is a \"true output function\" f, does this correspond to what is usually referred to as the \"realizable\" setting (i.e., where y = f(x) is a deterministic function of x)?\n\nNotation is sometimes used without having been defined. For example, what is $\\vec{x}_{S}$?\n\nIt isn't clear how one can train using the criterion (4), or equivalently, solve the problem set forth in Prop 2, in practice, since the true source code C is not known. Please elaborate on this.\n\nIn the experiments, how precisely is the VGG network modified to handle the encoded inputs? Are these just passed as additional input channels? Is the network architecture need to be modified in any other way to account for these additional inputs? Also, what loss criterion is used for training? Cross-entropy? Or something related to normalized information distance? This isn't clear from the discussion in Sec 3.\n\nIn Sec 3.2, is it really fair for the PGD attacks to only use the gradient of the loss wrt the uncoded input? Is this still really \"white box\"? How does the proposed approach perform if the attack also has access to the encoded input? It would be good to report both settings in the paper.\n\nThe introduction made connections to other forms of generalization, including domain generalization and domain adaptation. Based on this, I had expected to experiments illustrating the utility of the proposed for these problems.\n\nThe existing experiments do make clear that the encoded inputs are more robust to perturbations and aversarial attacks than uncoded inputs. Is the encoding assumed to be performed on the perturbed input, or on an unperturbed input (i.e., before it is perturbed)?\n\nI don't find the results in Table 1 fully convincing that the proposed approach should be preferred over previous approaches. For instance, the approach of Madry et al. (2018) achieves slightly lower test accuracy, but substantially higher inference accuracy. In general, one expects to see a tradeoff here. If one were to plot these points, would the proposed method clearly fall above the Pareto frontier? Minor: Please clarify the definition of \"inference accuracy\" in the paper.\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a theoretical framework for studying the generalization of deep neural networks in a broader sense; i.e.,  generalizing with underrepresented or unrepresented training/test samples. The main idea is to cast the problem of improving generalization as the problem of minimizing the normalized information distance between the learned source code and the true source code, defined using the Kolmogorov complexity. Built upon this framework, the method of using extended encodings of an input sample is presented, which empirically seems to lead to better generalization in the settings of adversarial attack and corruptions.\n\nWhile the idea and the results seem to be appealing, and the concept is novel to the mainstream deep learning community, I suspect that some of the proofs in the paper might not be well grounded. For example, in Equation (3), the authors claim that \"a necessary and sufficient condition ensuring that ... the learned source code C_0 is more general than the learned source code C1 is ...\". However, this is not proved and does not seem obvious to me. Also, in the proof of Theorem 1 in the Appendix, the authors state \"Because a sufficiently high-capacity neural network can memorize its input samples (Zhang et al 2017), the Kolmogorov complexity of the true source code is larger than that of the source code\". This, to me, is more like some intuition or conjecture, rather than rigorous mathematical proof. As a result, I am skeptical about the theoretical claims made in the paper.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper provides a very interesting viewpoint for understanding the generalization in deep learning, where the concept of generalization is defined as “the difference between training error and inference error”, and covers the concept of adversarial robustness.  More specifically, \n1. The author treats the deep model as a source code, and provides some theoretic analysis based on the Kolmogorov complexity. The insight drawn from this theory is: finding a better source code is equivalent to minimize a Kolmogorov complexity term given the true source code; as a result, a learned classification function needs to be sufficiently complex to minimize the generalization gap. \n2. The author tries to approximate the normalized information distance and form an effectively computable optimization problem, which suggests that one can use channel codes on input features as additional inputs. \n3. The experiments show that using additional encodings improve robustness in several settings, including in the sense of adversarial robustness. \n\n\nPros: \n1. The writing is pretty clear. \n2. The complexity viewpoint provides more insights for understanding generalization, which is something not covered by learning theory. \n3. The empirical method that the authors proposed is simple, and computationally friendly. \n4. I personally like the way adversarial robustness is covered in this paper, especially when comparing with tons of other adversarial robustness papers. \n\n\nCons: \n1. Can you provide exact reference for the definition of normalized information distance? It is not directly mentioned in (Bennett 1988) through a quick scan. The normalization seems important for the theoretical analysis, so it would be better to explain where this definition comes from more clearly. \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes an approach to generalization for deep networks based on kolmogorov complexity. The normalized information distance and its approximation via a compression algorithm were developed in earlier work, as noted by the authors. So the main contribution seems to be framing the deep learning classifier as a source code and developing a method to minimize the proposed information distance to improve generalization.\n\nI found a few gaps that I think the authors should clarify for me. Why is framing the deep learning classifier as a source code important? It seems to me that the K(C) is the same as K(f) where the f is the function mapping X -> y, whether it is learned or not. \n\n- Moreover, unless I'm missing something, the source code is a way to encode the values of a random variable such that communication is minimized. If the C=f  is a map from X_i -> y, X_i is an image, and y is a scalar, the source code as defined is not encoding X_i, it is simply encoding a part of X_i that is relevant to the classification task.\n\n1. The claim \" Because a sufficiently high-capacity neural network can memorize its input samples the Kolmogorov complexity of the true source code is larger than that of the learned source code: i.e., K(C) > K(C~).\" needs proof in itself. This stands opposed to the intuition that an over-fit model has larger kolmogorov complexity because it is not \"simple\".\n\n- Further, how does this square with the claim that the K(C~) is increased by adding encodings? If K(C~) is increased, then I think one needs to show that K(C~)< K(C) even with the encodings added?\n- Finally, by adding the encodings, the task has been fundamentally changed to the classification on the dataset [x, E1(x), E2(x) .. ] . So, the insight about minimizing information distance computed between C and C~ applies when the learned and the true \"source code\" correspond to the new task. This does not seem to be addressed in the theory.\n\n2. For the noise robustness experiments, there are no other baselines for robustness provided. \n\n3. For the adversarial robustness claims, I don't believe that the justification provided in the paper is the necessarily only one. The adversarial attacks are only done on the uncoded image. So, if 'encodings' are robust to these attacks, the network could also be robust. Unless this hypothesis is rejected, the provided theoretical justification and experiments do not exactly match-up.\n\n(writing comments): I felt that the paper could use a better structure with terms like source code defined in a consolidated section. "
        }
    ]
}