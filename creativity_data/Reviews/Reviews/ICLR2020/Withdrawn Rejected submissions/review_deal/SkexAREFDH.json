{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "While network pruning & quantization have been studied intensively in the literature, this paper presents a regularizer that can simultaneously achieve the above two goals. The authors further propose an optimization algorithm based on ADMM for training. Experimental results demonstrate great compression ratios compared with various baseline methods.\n\nOverall, I suggest authors have better placement of their novelties in the literature, more justifications on ADMM and stronger baselines in experiments.\n\nQ1. \"automatically according to a target model size without using any hyper-parameters to manually set the compression ratio for each layer\".\n- Personally, I do not feel this claim is proper. Indeed, by setting S_{budget} in (1), one does not need to care about hyper-parameter at each level separately. However, this does not mean all other methods have to set such hyper-parameters in a layer-wise manner. For example, one can add a sparse regularizer and then set the overall sparse penalty, like \"Sparse Convolutional Neural Networks. ICCV 2015\".\n- Besides, I do not think the method works in an automatical way. 1) S_{budget} needs to be set and 2) how rho (for ADMM) is tuned is not clear.\n\nQ2. It is good that authors organize related work in \"pruning\", \"quantization\" and \"AutoML\". It will also be good if authors can organize all tables in this manner as well.  For example, in Table 2, which category does BC-GNJ belongs to? In this way, it will be easier for readers to understand why the proposed method can be better than all compared methods.\n\nQ3. Could the authors offer model size in all tables?\n- Personally, I do not feel comfortable to see such a huge compress ratio in, e.g., Table 4. While 205x can be impressive, the real reduction in model size can be small.\n- Could authors offer STD in tables for small data sets, e.g., Table 4? I am no sure the reduction in memory size is significant, e.g., between \"Ye et al 2018b and Ours\".\n\nQ4. Since the authors have done a search in methods from three directions, i.e., \"pruning\", \"quantization\" and \"AutoML\", could the authors compare with the more recent or stronger state-of-the-art?\n- While 2120x in Table 2 can be impressive, it is not meaningful if the based network is too weak (i.e., LeNet on MNIST). Perhaps, authors can search on networks identified by NAS methods, e.g., \"Efficient Neural Architecture Search via Parameter Sharing\". If authors can achieve the same/or slightly less compressing ratio on these networks, I will definitely give acceptance for this submission.\n- For Table 4, while MobileNet can be a popular choice, MnasNet can be a better network.\n\nQ5. Could the authors offer the learning curves, i.e., epoch v.s training/validation loss?\n- It is better to show how the proposed algorithm converges, and how fast it converges.\n\nQ6. Could the authors show the impact of rho?\n- There is no convergence guarantee of ADMM on such a problem, the choice of rho can be important.\n- How the accuracy/sparsity/learning curves change w.r.t. rho?\n\nQ7. Could authors offer more discussion with Ye et al 2018b?\n- A combination of  \"pruning\" and \"quantization\" using ADMM is already explored in this paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to optimize for a sparse and quantized network under certain memory constraints. Experiments are performed on typical image classification data sets CIFAR-10 and ImageNet. The authors show that the proposed method can achieve very high compression rate without accuracy degradation.\n\nThe paper is clearly written, and the compression rate is very impressive. One of my main concerns is that the training can be very expensive due to the expensive alternating steps in the ADMM solver. For instance, in section 3.3 which describes the solver for problem (5),  though the authors claim that the sorting operation involved can be efficiently done in GPU, the O(n(log n)) complexity can be even larger than the multiplication between the weights and intermediate activations.\n\nFrom steps 13 - 15, it is possible that the proposed ADMM solver does not converge. Can the authors discuss the convergence properties? How does the solution of each alternating step affects the final convergence of the whole algorithm? How often does the algorithm not converge? Will brutely quantize the weights result in undesirable performance?\n\nAnother concern is that though combining pruning and quantization can greatly increase the compression rate, the actual speedup of the inference highly depends on the hardware or operation design. What is the inference speed of the proposed method?\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces an approach to jointly pruning and quantizing a deep network so as to satisfy a global budget constraint. This is formulated as an optimization problem with a constraint involving the number of nonzero elements in the network weights and the bitwidth to encode these nonzero elements. A solution to this problem based on the ADMM is proposed.\n\nMethodology:\n- In general, I like the idea of considering both pruning and quantizing in a single framework. My concern regarding the method is linked to the fact that several approximations are used to derive the solution:\n* The problem is non-convex, and thus the ADMM does not offer much in terms of convergence guarantees (as acknowledged by the authors below Eq. 7);\n* When updating W, the authors use a quadratic approximation of the true objective function (Eq. 5);\n* Because the resulting problem remains NP hard, a greedy algorithm is used to approximately solve it (Section 3.3);\n* Solving for the auxiliary variables V also yields an NP hard problem, which is solved approximately (Section 3.4).\nIt is unclear what these successive approximation entail when it comes to the final solution. As a matter of fact, as acknowledged by the authors at the end of Section 3.4, the final W and V do not truly satisfy the constraints, and thus W is then quantized in a post-processing stage.\n\nExperiments:\n- The experiments show the good behavior of the proposed algorithm. However, the comparison to existing methods is quite incomplete, in the sense that different baselines appear in different tables. In particular, considering that CLIP-Q is initially described as the closest method, I would appreciate seeing its results in more than one table.\n- It also seems to me that a natural baseline would consist of fixing the number of bits, e.g., to 2 or 3, and then perform pruning in a similar manner as done here so as to satisfy the budget. I wold highly appreciate seeing the results of this baseline, to get a better understanding of the importance of jointly pruning and quantizing, and the benefits of allowing different bitwidths for different parameters.\n- Considering that the authors also need to perform a post-processing quantization step, it would also be interesting to provide the results of a baseline doing pruning only, and then applying quantization with the same post-processing step.\n- Detail: From the text, it is not entirely clear to me how the budget was set in the experiments. The number in the tables also do not suggest a very clear budget.\n\nRelated work:\n- Table 1 only shows 3 baselines, with a single one performing joint pruning and quantization. In the experiments, however, the authors mention 3 additional joint pruning and quantization methods (Han et al., 2015, Louizos et al., 2017, Ye et al., 2018b). Why aren't these methods in Table 1 and discussed in more detail?\n- Note that other methods have proposed to use budget constraints and could serve as baselines (maybe not the 3rd one as the constraints are on the energy):\n* Chen et al., Constraint-aware Deep Neural Network Compression, ECCV 2018;\n* Chai et al., Towards the Limit of Network Quantization, ICLR 2017;\n* Yang et al., Energy-constrained Compression..., ICLR 2019;\n\nSummary:\nI have mixed feelings about this paper. While I like the idea, the proposed solution relies on many approximations, the influence of which is not studied. The experiments show that the method works reasonably well, but several important baselines are missing. I therefore feel that the paper is not yet ready for publication.\n"
        }
    ]
}