{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents a way of incorporating word distribution from a BERT model into the training procedure of a seq2seq model as a regularization for better text generation. The claimed advantage is that it will help train the seq2seq model by exploiting BERT's ability of \"looking into the future\". Experimental results show the model performance on machine translation and document summarization. \n\nMy major concern of this work is that it is not clear about the benefit of using a BERT as the teacher model. If the reason for performance improvement is from \"injecting future information for predicting the present\", one big missing part in the evaluation is using a bidirectional RNN as the teacher model. Then, the comparison with BERT as the teacher model will give us more information and also evidence about this work. \n\nSecond, although this paper explains the difference with the Masked Seq2seq pre-training from (Song et al. 2019), those two ideas are still very similar and the novelty of this part is not significant enough. Maybe I missed something. \n\nIn addition, the experimental results in section 4 are not strong enough to support the effectiveness of the proposed method. Specifically, is it possible to combine the proposed method with the Transformer (big) to show further improvement?\n\nOther comments on the detail\n\n- I think something is missing in equation 4\n- I am not convinced about the explanation associated with Table 4 in section 4.3. Unless there is a good reason (other than the test set contains some \"noisy\" example), I do not think modifying the test set to get some favorite results is appropriate. The bottom line is to run other competitive systems with the same internal split. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper uses BERT in text generation via distillation. Specifically, on top of the common MLE training loss for generating tokens from left to right, this paper adds a distillation regularization term in the cross entropy form, in the hope that the likelihood of the masked token predicted by BERT (bidirectional) is close to the likelihood predicted by the autoregressive generation model.\n\nThe experiments are performed on text generation tasks like machine translation, text summarization, and image captioning. Specifically, it got SOTA on IWSLT14 German-English and IWSLT15 English-Vietnamese datasets. \n\nOverall, this paper presents a neat idea for using BERT in text generation. I would recommend a weak accept due to the following issues:\n\n1. The so-called SOTA results are not very surprising because this method uses BERT, which is pre-trained on huge extra datasets. Thus, one may argue that the comparison may be unfair.\n\n2. En-De (and En-Fr) are more important benchmark datasets in machine translation. However, the results in Table 3 are much worse than Ott et al. or Wu et al. The experiments can be more complete if WMT16 is used in training and checkpoint averaging is also employed.\n\nMinor suggestions: the text in Figure 2 are hard to discern. Please improve the quality of the figures.\n\n\n\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper focuses on text generation. It proposes to 1) fine-tune BERT for conditional masked language modeling and 2) use the fine-tuned BERT as a teacher that provides supervision signals during seq2seq training. The paper is clearly written. The proposed approach lacks a strong motivation though; I would like to see more explanation on what problem is solved by adding the BERT teacher. The empirical results are okay but not remarkable. Thus I intend to reject the submission.\n\nApproach:\n- The main motivation as described in the paper is \"BERTâ€™s looking into the future ability can act as an effective regularization method, capturing subtle long-term dependencies that ensure global coherence and in consequence boost model performance on text generation.\" While I agree that looking into the future is useful, I don't see why adding BERT's future-dependent prediction to the objective is helpful. The ground truth label is also dependent on the future.\n- If the proposed approach does help \"looking into the future\", it should improve model performance with small beam sizes or even greedy search. It would be helpful to have this experiment.\n- The idea itself is very similar to label smoothing. Thus I'd like to see comparison with (uninformative) label smoothing in the experiments.\n\nExperiments:\n- How does changing \\alpha affect the results?\n- Section 4.5: Results in Figure 2 do not consistently support the claim that the proposed approach improves on longer sequences.\n\nMinor:\n- Equation 5: notation is loose. Loss should sum over examples. Unclear which words are masked in Y^u."
        }
    ]
}