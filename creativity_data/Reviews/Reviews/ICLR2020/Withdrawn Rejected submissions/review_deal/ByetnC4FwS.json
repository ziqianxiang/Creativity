{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "One problem with modern Transformer-based text representation is the length limit. The authors propose to solve this problem by learning to chunk long documents with RL. The paper also describes a few other modules including answer extractor and a chunk scorer. The authors experiment their model on two conversational datasets.\n\nIn the introduction, the authors claims that the performance of models could be worse if answers are located far from the center of the chunk. One alternative approach to solve this problem could be to have smaller strides. Could you please demonstrate why learning to chunk is better than simply setting smaller stride in terms of performance? And it might be fair to take the stride as a hyper-parameter and tune it for better experiment results.\n\nAnother concern I have is about the experiment results. It seems that when the max sequence length is large, the gain of having a dynamic chunking system is marginal over the BERT baseline. \n\nIt's great to see that the model performs well on long documents (Table 3). But I am not sure if conversational datasets is the best to evaluate this model. As you mentioned in the introduction, some document-level RC datasets may contain very long documents (e.g. a wikipedia page). It'd be great to see how would this model perform on those datasets."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The paper considers the task of extractive QA where the document is longer than the document encoder's size limit. (The conversational part does not play a big role in the method.) One solution is to look at a chunk of document tokens per time step. The paper proposes (1) a way to propagate information between time steps, and (2) a RL policy that selects how many tokens to skip when locating the next chunk. The method was evaluated on CoQA and QUaC, with gains observed when the document is much longer than the chunk size.\n\nThe observation about the answer's location in the chunk is eye-opening, and the need to select the right document chunk is presented well. The recurrent mechanism and chunk scores look correct.\n\nHowever, the paper has two potential weaknesses:\n\n1. The chunking policy looks too complex for the task and might also be incorrect.\n\n- Most of the time, the number of possible chunks (with 16-token strides) is small. One could score all such chunks for relevance at once without having to use RL to look at chunks sequentially. Efficiency-wise, scoring multiple chunks in a batch might be even cheaper than embedding a single chunk at each time step. This process of selecting relevant document sections is related to retrieval-based reading comprehension [https://arxiv.org/abs/1704.00051 | https://arxiv.org/abs/1808.10628 | https://arxiv.org/abs/1808.06528], where the relevant documents are retrieved for extractive QA.\n\n- In Equation 11, q_c contains parameters theta to be optimized, which I think makes the REINFORCE gradient incorrect.\n\n- Equation 13 is missing a term for the negative class (sum_c (1 - y_c) log (1 - q_c)), but this could simply be a typo.\n\n2. There are a few baselines that could have been tested:\n\n-  As stated above, instead of using a policy-based chunk selector, just score q_c on all spans, and use it to either select a span or to compute the span score q_c * p_start * p_end. This is similar to the sentence selector baseline but instead selecting chunks (which is more comparable).\n\n- A model with no recurrence but with RL chunking is missing.\n\n- One upper-bound experiment to try is to just select the span with the correct answer in the middle. This will indicate the amount winnable from doing better chunking.\n\nQuestions and comments:\n\n- Page 2: What is the chunk size for the plot?\n\n- Page 2: \"... the predicted spans are incomplete\" -- Does this mean the span is chopped in the middle?\n\n- What is the distribution of the actions that the policy takes? In particular, does it use the \"-16\" action at all?\n\n- Appendix A.4: Despite \"farmer roast\" not appearing, the answer chunk still has a strong prior (due to features such as similarity to the question and negative words).\n\n[EDIT] I am changing my score to weak accept. Refer to the replies for details.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a recurrent and chunking method for conversational reading comprehension (RC). The motivation is that the given documents in conversational RC tasks are usually long, but most previous approaches do not handle this problem properly. The paper introduces two methods. First, a recurrent module is used for better chunk selection. Second, in contrast to a chunking method with a fixed stride size, the model learns to chunk the document properly so that the answer span is not truncated across different chunks, which is trained with REINFORCE algorithm. The proposed method is evaluated on CoQA and QuAC, two conversational RC datasets, and the proposed method shows 0.4 and 0.5 F1 improvements over the BERT baseline.\n\n<Strengths>\n\nFirst, although the recurrent mechanism was used in several other papers (described in detail later), it is the first to be used in conversational RC.\n\nSecond, learning to chunk through RL has not been explored in the previous literature.\n\n<Weakness>\n\nOverall, I am not convinced that the contribution is significant. The contribution is narrow (focusing on how to chunk), the baselines are not strong enough, and the empirical improvement is marginal.\n\nFirst, recurrent mechanisms for selecting related chunk were explored previously, including “Nishida et al, Answering while Summarizing: Multi-task learning for multi-hop QA with evidence extraction. ACL 2018” and so on, which are not mentioned in the paper.\n\nSecond, learning to chunk seems to be a marginal, focused contribution. Also, the reward is simply based on whether chunks do not truncate the answer span, rather than whether the document is chunked in a contextually meaningful manner. As the answer span is not across different sentences, isn’t sentence-based chunking enough to resolve this problem? If it is not, then shouldn’t the proposed chunking do something more than answer span based chunking?\n\nThird, I am not convinced by the baselines used in the paper.\n1) I believe that the BERT baseline uses a sliding window technique, but this paper does not mention it. Does the BERT baseline in the paper use it too? Also, the paper mentions that the number of chunks is fixed; is there a particular reason for it?\n2) For sentence selector, how many sentences are fed into the BERT model? Are multiple sentences concatenated, fed in parallel?\n3) More meaningful baselines would be: (1) chunking the document while making sure that the split doesn’t happen within the sentence (as mentioned previously), or (2) chunking the document as (1), but do sliding window (e.g. if there are sentences A, B, C, D, and E and one chunk can contain 3 sentences, use “A B C”, “B C D” and “C D E”)\n\nLastly, the improvement over the BERT baseline is marginal (0.4 and 0.5 F1 on CoQA and QuAC, respectively, as mentioned above). In particular, ‘max sequence length’ is a hyperparameter, so only the result with the best hyperparameter should be considered.\n"
        }
    ]
}