{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposed a new member of the GAN zoo named PolyGAN. It differs from the other GANs in that the generator G(z) is a polynomial function of the input noise z. As the noise has dimension d, the number of free parameters grows in the order of O(d^k), where $k$ is the polynomial order. Therefore the authors proposed (1) to constrain k to be small; (2) to design a constrained parametrization of the polynomial coefficients, so as to reduce the total number of free parameters. The authors showed that PolyGAN can generate good-looking MNIST digits.\n\nThe reviewer is performing an urgent review by invitation.\n\nOverall, this contribution is a typical deep learning technical contribution, on the practical side rather than on the theoretical side. The paper is very well written in terms of clarify. However, the reviewer has some concerns about the overall significance and some other concerns on the experimental evaluations, and hence recommends rejection.\n\nFirst, the authors have to mention some literatures of the polynomial regression and have some in-depth discussions. The current written is too technical and lacks theoretical depth. The reviewer does not feel excited after reading the paper. Actually, Figure (4) shows a weakness of the PolyGAN, just as in polynomial regression: polynomial function is unbounded at infinity and therefore loss accuracy on the boundary of the data distribution. This kind of discussion will enhance the overall interestingness of this contribution.\n\nSecond, in the experiments, there has to be a baseline, such as the vanilla GAN (even PolyGAN performs worse than GAN, it will enhance the writing).  And, there has to be an experiment on real images such as ImageNet, just like the other GAN papers. And, it is recommended to show some cases where the order is (much) higher than 4, so as to show the usefulness of the proposed parametrization. The current evaluation is not complete. and is not convincing."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to replace the generator in GANs by a single-layer polynomial generator. In other words, the usual deep network of the generator is replaced by linear tensor product. Although the tensor product are linear in each parameter of the noise vector, they involve multiplications between tuples of input parameters and are therefore more expressive than a single linear layer (single matrix multiplication).\n\nThe results are presented on synthetic low dimensional data, MNIST, YaleB and CIFAR10. Although it is always difficult to judge the quality of GAN generated samples, the samples presented are of poor quality compared to the state of the art.\n\nMy main objection to this paper is that it does not present a clear justification for their approach. It does not seem to work better empirically, and there is no strong theoretical justification. The authors explain well that polynomial can approximate any function, but that is also the case of many approximators, including deep neural networks. The main thing that this paper shows, in my opinion, is that the GAN framework is robust to different kinds of generators, but that is not the stated point of this paper.\nBesides, polynomial approximators tend to produce unstable approximations when their degree increases, which is not addressed in the paper. Why not use approximators that were proven to be stable, such as splines of wavelets, is not well explained. I am actually surprised that the polynomial generator is stable enough to generate samples. One possible reason is that GAN training (with a discriminator) makes the generated polynomials more stable, which could be an interesting result if investigated.\n\nThe overall structure of the paper is acceptable, although many of the notations that are introduced early in the paper are actually only useful in the appendix and should probably be moved there.\n\nI would suggest publishing this paper in a workshop."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper approaches GANs as a polynomial expansion task. While the proposed approach is sound and mathematically appealing, the result of generation was only carried out on an easy benchmark (MNIST). With the recent advancement of GANs, we've been able to generate real-world and high-resolution images. I'd expect some more comparison with the published methods on other more complicated datasets.\n\nSome more details:\n- Figure  5 (d), the digits generated by PolyGAN seems to have a mode-collapse problem, in particular, if you count the number of \"0\"s. \n\n- Would the proposed approach of polynomial expansion applicable to other neural-network related architectures, such as image recognition?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors use a polynomial function approximation model in a GAN setting.   I think this is a completely reasonable area of study, but this paper cannot be accepted.  It gives no empirical or theoretical description of the benefits of this kind of function approximator in this setting over standard neural networks (or in any setting, really).  The only empirical results are images of the generations on MNIST and point clouds from some very simple 2-d distributions.  This is not much more than a white-boarding of an idea."
        }
    ]
}