{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:  This paper proposes defining a regression model of the form $p(y|x) = \\frac{\\exp{f(y,x)}}{Z(x)}$ where $Z(x) = \\int_{y} \\exp{f(y,x)} dy$.  The normalizing constant is analytically intractable, however, and the paper proposes evaluating it via importance sampling: $Z(x) = \\int_{y} \\frac{q(y)}{q(y)} \\exp{f(y,x)} dy = \\mathbb{E}[\\frac{\\exp{f(y,x)}}{q(y)}]$ where $q(y)$ is set during training according to the observed label.  This model is validated on several vision tasks including object detection, age estimation, pose estimation, and visual tracking.  Results show competitive to superior performance against recently-proposed baselines.    \n\nPros:  Defining more expressive predictive models---ones that can handle multiple modes and heteroskedastic response noise---is an important project, and this paper does seem to make progress towards this goal, as seen in Figure 1.  Despite computational downsides, the paper proposes some efficient work-arounds (e.g. importance sampling, introducing the label deep into the architecture) that allow the model / method to be scaled to realistic vision tasks.  The performance in the experiments seems good (but I was not previously familiar with these tasks).\n\nCons:  I have three significant reservations about the work…\n\n(#1) Incomplete validation of the core methodology:  The paper claims to propose a general framework for constructing flexible regression models: “...a novel and general regression method with a clear probabilistic interpretation” (p 2).  However, the paper considers only real-valued regression---when the core method is clearly applicable to other domains---and discusses only one numerical strategy for computing the difficult normalizing constant.  As the model requires a forward propagation for every sample, such a sampling strategy has clear limitations (additional to the usual difficulties with importance sampling).  The authors admit this and bypass the obstacle by introducing the label late into the architecture, meaning the computation specific to the features can be re-used.  Yet, if the paper is truly proposing a “general and flexible framework,” I think ablation studies and runtime experiments showing that the model is applicable to a range of architectures and dimensionalities must be done.  The current draft only validates these choices for specific vision tasks.  \n\n(#2) Relationship to energy-based models: I was surprised that the authors make no mention of energy-based models---this paper’s Equation 1 is essentially Equation 2 from LeCun et al.’s [2006] “A Tutorial on Energy-Based Learning.”  There are a large number of models discussed in this survey, including many for prediction.  Is the proposed model possibly related to one of these?  As the paper has no citations to or discussion of energy-based learning, it is impossible to tell. The authors need to include some discussion of this related work and probably should revise their model name so that it doesn’t seem like a totally new class of model.  \n\n(#3) No discussion or consideration of alternative training strategies: The paper only considers approximate maximum likelihood learning, and the importance sampling approximation has serious limitations due to needing a forward prop for each sample, as discussed above.  However, there is much work on estimating hard-to-normalizing models via other means such as score matching and contrastive divergence.  There is even recent work on scaling these methods up to models for high-dimensional images: see [Du & Mordatch; 2019] and [Song et al.; 2019].  Can these methods be used to by-pass the normalization issue?  If not, there should at least be some discussion of why.\n\n\nReferences\n\nDu, Y., & Mordatch, I. (2019). Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689.\n\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A Tutorial on Energy-Based Learning. Predicting Structured Data.\n\nSong, Y., Garg, S., Shi, J., & Ermon, S. (2019). Sliced Score Matching: A Scalable Approach to Density and Score Estimation. UAI 2019."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this work, the authors proposed an energy-based model (EBM) to capture the conditional distribution P(y|x), where x is the input data and y is the regression target. To train the model, the author used a Monte Carlo importance sampling strategy. The proposed method has been evaluated on several standard tasks and achieved convincing numerical results. \n\nThe following are some detailed comments:\n\n1. Once the method is trained, to get the optimal y given x, the author used gradient descent. I found this a little problematic since the energy landscape is not convex in general. As a result, several tasks are evaluated based on an initialization from the baseline and use DCTD for refinement. So I'm convinced that the model behaves well close to the optimal point. However, I think one necessary experiment is to use a sampling method and start from a random initialization to evaluate the energy landscape in a larger range. \n\n2. The introduction & background is well written, personally I enjoy reading them. But I suggest making it more compact for a conference paper since the technical material of this paper only starts to appear on page 4. \n\n3. I feel the proposed method heavily depends on that the task y to perform is low dimensional since the importance sampling method only handles a low dimensional space. How well does this model behave for a high dimensional regression task?\n\nOverall, I think the idea is interesting and clean and the work is certainly valuable. If the authors can address some of these comments, it should be considered for publishing."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes Deep Conditional Target Densities (DCTD) for confidence-based regression problems. Under the core of the framework, DCTD adopts the Monte Carlo approximations to calculate the integral in the negative log-likelihood Eqn(2). The authors perform comprehensive experiments on four computer vision regression tasks. \n\nThe main concern about this paper is the novelty and the technological contributions. The Monte Carlo approximation is an off-the-shelf numeric integration method. The novelty of this paper is very limited."
        }
    ]
}