{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a local-to-global alignment (L2GA) framework to learn semantic correspondences from loosely related data-text pairs. First, a local alignment model based on multi-instance learning is applied to build the semantic correspondences within a data-text pair. Then, a global alignment model built on top of a memory guided conditional random field (CRF) layer is designed to exploit dependencies among alignments in the entire training corpus, where the memory is used to integrate the alignment clues provided by the local alignment model.\nThis paper is well written and the experiments are also sufficient.\nIn eq.(11), how parameter \\lambda affects the final results?"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper is too far outside my area of expertise for me to provide a substantive technical review. The paper has to do with “learning semantic correspondences” between text and some sort of database-like representations. \n\nI can really say only two things that might be useful. \n\nFirst, the paper is not at all effective at explaining its topic to a reader outside of natural language processing. I read carefully but was never able to get any sense of what problem was being solved. I was never able to understand what the inputs to the algorithm were, or the outputs, or why it might be sensible to think one could have those inputs, or want those outputs. I had trouble understanding any part of the paper at any level.\n\nSecond, I can say that almost every sentence in the paper is not grammatical English. There are problems of singular/plural, in the use of articles, and unlikely combinations of verbs, subjects, and objects.\n"
        }
    ]
}