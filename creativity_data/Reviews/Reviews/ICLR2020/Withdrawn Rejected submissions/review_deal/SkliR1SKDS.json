{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes to use data-augmentation as self-supervision, applied as a regularizer in fully supervised classification settings in order to improve the generalization accuracy of the classifier. It uses a joint (label x augmentation) prediction space, rather than a factorized space of labels and augmentations in distinct prediction branches (multi-task setting). The author argues that this alleviates the requirement of invariance w.r.t. the transformation for the primary classifier and allows to use more diverse augmentations. The author tests this hypothesis on various classification datasets (including few-shot and fine-grained), and also evaluates the approach with test-time augmentation - proposing a knowledge distillation method to alleviate its cost. The author show improved accuracies in these settings w.r.t. simple data-augmentation (DA) or multi-task (MT) baselines.\n\nI am unsure about the justification of the method’s advantage w.r.t. the baseline and MT. I have the impression that the performance improvements w.r.t. MT could be explained by the added capacity. Barring futher experimental validation that this is not the case, I find the contributions of the paper to be minor. I will detail these points hereunder.\n\n* Method *\n1. The features learned by MT are not invariant by the transformations (otherwise the auxiliary classifier could not predict the transformation). The primary classifier in MT could behave in a similar way as the SDA method by aggregating features from a joint (label x augmentation) space, acting as a projection on the label subspace. Therefore, I do not see in MT the core limitation that SDA claims to address (enforcing extraneous invariance of the classifier).\n2. I see the joint classifier in SDA as the bulk of the novelty: the test-time augmentation experiments and teacher/student models to relieve the costs of these test-time augmentations are good to have but do not seem novel, and their applicability is not restricted to the SDA approach.\n\n* Experiments *\n3. It is my understanding that SDA uses FxN(M-1) more parameters in the classifier than the baseline and than MT (F: number of features). This provides more parameters for the network to absorb the data augmentations, and to perform better for the primary classification task. This added capacity could explain the accuracy gains with respect to MT and to the baseline.\nFor a fair comparison, a similar capacity should be added to the baseline, to DA and to MT.\nFor the baseline, this could be done e.g. by using a large F x N x M classifier, and using a pooling over the M dimension.\nFor DA, this could be done by using a large F x N x M classifier, and selecting the FxN slice that corresponds to the augmentation used (just as was done for SDA, but without the data-augmentation loss term in eq. 2).\nFor MT, this could be done by adding an additional fully-connected layer before the two heads.\n\n* Minor *\ntypo Table 2 caption “indenpendent”\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "To solve the distributional discrepancy when performing data augmentation, this paper proposes to learn the joint distribution of the original and self-supervised labels of augmented examples. The idea is easy to understand and implement. The authors then perform an empirical study on a standard setting (e.g. CIFAR 10/100) and a limited data setting (e.g. few-shot). \n\n\nI have some concerns as follows:\n\n1. The proposed method is not scalable. For a classification task with 1000 classes and 10 rotations, there will be 10000 labels to predict. \n2. AutoAugment (https://arxiv.org/abs/1805.09501) can learn a transform function to avoid the distributional discrepancy, but the authors did not compare theirs with this strong baseline. \n3. The experiments done on MIT-67 seems not meaningful to me, as the dataset is old and small (published in 2009). "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper proposes a method for performing data augmentation using self-supervision and self-distillation. One of the main problems of data augmentation is that the data transformations have to be tunned for each dataset. E.g., in MNIST a \"9\" rotated 180 degrees would be a \"6\". This paper proposes a method using self-supervision (i.e., predict the rotation and color channel flips). Instead of having a multitask loss (i.e., one loss for predicting the object class and another for the self-supervision transformation), this paper proposes to frame the problem as a single classification problem where the new label is obtained by the permutation of object labels and self-supervised ones.\n\nThe method is tested in 3 scenarios: image classification, few-shot image classification, and imbalanced data image classification. In the first case, the proposed method is compared with performing data augmentation (without hyperparameter tunning in the best setting, I assume it is just the same transformations used for the self-supervised case) and with the multi-task setting (where the self-supervised and object classification tasks are in different losses). The results show that DA performs worse than the baseline (Obviously because the parameters used does not make sense) and the same for multitask learning. Then several ablation studies are done and also experiments on a few-shot and imbalanced classification.\n\nMain problem:\n\nThe evaluation is not fair. In the case of image classification, the method should be compared to standard data augmentation with correct parameters and with SOTA methods of automatic data augmentation which do not require tunning the DA parameters. It should be compared at leat with:\n - AutoAugment: Learning Augmentation Policies from Data\n - Fast AutoAugment\n - Adversarial Learning of General Transformations for Data Augmentation\n - DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification\n - TANDA: Learning to Compose Domain-Specific Transformations for Data Augmentation\n \nThe evaluation for few-shot and imbalanced is compared with other methods of the literature. However, no-one of these methods use data augmentation and then the comparison is not relevant."
        }
    ]
}