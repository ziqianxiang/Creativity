{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposed a new off-policy actor-critic algorithm. It uses a noisy version of the current policy, instead of the true policy probability, in the importance sampling part and the policy gradient form. The noisy version is achieved by Gumble-softmax. Intuitively, the noisy policy probability will make the importance sampling ratio less variance, thus improve the off-policy actor-critic algorithm.\n\nI think the contribution of this paper is not enough to meet the standard of ICLR in general. The main reason is that this work seems to be a quiet incremental work on top of off-policy actor-critic [Degris et al 2012] and ACER [Wang et al 2016], at the same time, lack of proper empirical and theoretical justification of the algorithm. \n\n1) The proposed algorithm simply replace \\pi(a|s) with F(s, a) in the off-policy actor-critic algorithm where F is a Gumble-softmax over \\pi(a|s). All other parts (replay buffer, clipped IS) are from ACER [Wang et al 2016]. So the main contribution is actually using Gumble-softmax instead of using experience replay.\n\n2) It needs to be explained more why adding noise to the policy scores can make the off-policy learning better. It's probably (?) understandable that adding noise could make importance sampling stable, but for the gradient of log pi part it's less clear. The paper need to provide more insight on why we want to do this.\n\n3) If we want to add noise to the probability score \\pi(a|s), a simple baseline is directly adding a uniform noise over the score. This paper compares several ways of adding noise to the logit of policy, but directly adding different noise on the probability are also natural baselines. An even more baseline could be GumbelClip without the noise at all, which can end up with a simpler version of ACER. It's useful to see these baselines be compared in all domains instead of just 1.\n\n4) The significance of the experimental results is also unclear. In 3/8 domains ACER eventually beat GumbelClip with a pretty large margin, and only in 2/8 GumbelClip ended up with a slightly higher performance than ACER. I acknowledge that ACER ensembles many other techniques to off-policy actor-critic, but I did not see why GumbelClip could not work with these techniques."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: \nEven though on-policy methods are easy to use in comparison with off-policy methods (i.e. more stable, need less hyperparameter tunning ), they are sample inefficient. To address this problem, this paper proposes to collect samples using a behavior policy and keep them in the replay buffer.  Then using the off-policy actor-critic algorithm that uses policy forcing through Gumbel noise and clipped importance sampling to optimize the current policy.  Results on 8 atari games show that GumbelClip performs better than A2C but shows mixed performance vs. ACER.\n\nComments and major concerns:\n- It is not clear to me what is intuition behind forcing policy and why it should work. There are some ablation studies but the paper should be very clear and provide some justification for why one should use GumbelClip.\n\n- Even though GumbelClip shows better performance than A2C, it doesn't outperform ACER which should be considered as a baseline and it is more appropriate for comparison. \n\n- This paper only uses 8 atari games for the experiments. Unfortunately, 8 out of 49 atari games are not near enough to say anything about the significance of this method.\n\n- On page 5, the last paragraph says that they used the best 3 of 4 seeds to report results which indicates that they treated seed as hyperparameters. Seeds should NOT be treated as a hyper-parameters and should not report only on the \"best\" seeds. Please refer to the following papers about how to report results in RL:\n    -- Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents ( https://arxiv.org/abs/1709.06009)\n   -- Deep Reinforcement Learning that Matters (https://arxiv.org/abs/1709.06560)\n\n- Another major problem is that this paper didn't compare with recent methods that use off-policy data. For example, the following method should have been used in the experiment sections and should have been compared to GumbelClip:\n-- P3O: Policy-on Policy-off Policy Optimization (https://arxiv.org/abs/1905.01756)\n\n- Besides, this paper should have included a comparison with PPO (Proximal Policy Optimization Algorithms https://arxiv.org/abs/1707.06347 ) to see how it performs when it compares with a better method on-policy method than [vanilla] policy gradient.\n\nSummary:\nGiven a lack of comprehensive results, an incomplete study of previous related works, not enough conclusive results, and failure to compare with recent methods, in my view, this paper is not ready for ICLR and needs major revision. \n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Overall I don't really understand *why* the Gumbel-softmax 'forced' policy is used over the standard policy. There is no discussion on the motivation at all. Using the 'wrong' policy in importance sampling will increase the bias of the updates, and possibly the variance. Furthermore, the point of using the Gumbel-softmax in other situations is to avoid the use of reinforce, however in this paper policy gradient (basically reinforce) is used anyway. On top of this there are no theoretical results showing that this approach is principled in any way. The experimental results are not enough justification.\n\nFig. 1 is very confusing, it took me a long time to understand what the 'sample' row was trying to show me. This is made extra confusing by eq 2 which is introduced using 'the Gumbel-Softmax distribution is defined as', which, despite the claim, is not the definition of a distribution but actually the definition of a *sample* from the distribution.\n\nSeveral of the claims in the introduction are plain wrong, e.g., \"Unfortunately, all action-value methods, including Q-Learning, have two significant disadvantages. First, they learn deterministic policies, which cannot handle problems that require stochastic policies. Second, finding the greedy action with respect to the Q function is costly for large action spaces.\"\nThe claim that it holds for *all* action-value methods is wrong. Moreover the claim that finding the greedy action is costly is just bizarre, since that amounts to simply taking a max over a set of numbers, whereas any policy method requires sampling from a set of probabilities of the same size - much harder to do!\n\nPaper is over the 8 page limit, but without really needing to be. I think some content could be removed.\n\nThree very relevant papers that I am surprised are missing are the following:\n\nPGQ: https://arxiv.org/abs/1611.01626\n\nThis uses the duality between value methods and policy methods to derive an off-policy algorithm. I really think a comparison to this approach is needed, especially because it is also much simpler than ACER to implement, and it doesn't require access to the old policy.\n\nRETRACE: https://arxiv.org/abs/1606.02647\n\nAnother importance weighted approach, must be mentioned (ACER is based on this).\n\nV-trace (in IMPALA): https://arxiv.org/abs/1802.01561\n\nFollow-up work to RETRACE, also very relevant.\n"
        }
    ]
}