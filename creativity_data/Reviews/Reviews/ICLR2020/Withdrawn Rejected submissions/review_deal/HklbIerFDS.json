{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a methodology to bring together lifelong learning with few shot learning, called Slow Thinking to Learn (STL). STL comprises of task-specific Fast Learners (FL) and a single Slow Predictors (SP). FL has a lookup memory, where learned embeddings of data and its label is stored. SP contains a single model which is trained on batches that comprises of datapoint from the lookup memory of FL and a meta-model trained using predictions from FL, in a way similar to MAML [Finn et al. ICML 2017]. The learned embedding function and the feedback term (in Equation 3) helps to extend the framework to sequential few-shot learning. The methodology is evaluated on standard lifelong learning benchmarks and a few-shot adaptation of CIFAR-100. \n\n+ves:\n- The overall idea to have an interplay between a quick learner and a slow predictor, inspired by the biological brain is novel. The proposed methodology is logical and draws inspiration from [Sprechmann et al. ICLR 2018].\n\n- The problem setting, which seeks to address concerns in existing lifelong learning methods, is relevant. Existing lifelong learning methods do have some limitations in their experimental settings, and this work seeks to address this concern.\n\n- The results for the experimental settings considered are promising.\n\nConcerns:\n- While the overall idea is interesting and relevant, the novelty is incremental in its originality to solve the identified concern, considering the paper mainly strings together existing ideas in terms of the methodologies (as summarized above). \n\n- It would have been nice if the paper motivated the work (and its setting) more strongly. While the paper states that knowledge of future tasks (even their quantity) is a strong assumption, examples of real-world applications (or problem settings) where one may not have access to this would have helped better motivate the proposed problem setting.\n\n- The premise of the work is not very clear: on Pg 1, the paper states “currently, lifelong learning models are usually trained …...for a sequence of tasks arriving at test time.” The knowledge about future tasks is claimed to be a strong assumption. However, in the lifelong learning context, I am not sure if this is not considered “test time”. The very premise of lifelong learning is that the model should continue to learn as newer tasks arrive over time, and that training is fair when newer tasks arrive.\n\n- The experiments don’t seem to rigorously study the key objective/motivation of the work. If the assumption of knowing the number of tasks is considered strong (pg 1 of the paper), it would have been interesting to see the results of the proposed method against existing methods when this assumption is violated, for e.g, as the number of tasks increases (beyond 10 - to, say, 20 or 50). All experiments in this work are eventually on standard settings of lifelong learning followed by other papers, which the work initially questions. Datasets such as Permuted MNIST allow for an indeterminate number of tasks, and studying in a varying number of tasks setting would have helped study the problem setting more closely, and the relevance of this work.\n\n- The experiments in Section 4.1 use a memory size of 1000 (Figs 4a, 4b). According to the methodology, the Fast Learner uses one memory module per task (M^t). Would this require storing 10000 examples each of 10 tasks, or is the memory size fixed to 1000, irrespective of the number of tasks? This is not clear, and is perhaps an important detail considering the focus of this work.\n\n- In CIFAR-100 experiments, are 20 classes added as a single task? How is the growth of the final classification layer handled?\n\n- It would have been nice to see a formal treatment of “slow” and “fast” as used in the SP and FL of this work. The terms seem to be heuristic in the work, and a clear definition of when a model is “fast” vs “slow”, as required for the proposed methodology, would have helped generalize the proposed idea better.\n\n- An important component of the proposed method is the SP (slow predictor). In the SP methodology, why is the cosine distance most appropriate for similarity between embeddings? What distance metric is KNN based on in this methodology? If KNN uses Euclidean distance for finding h (in Eqns before Eqn 1), why is cosine similarity the best option for computing the similarity in the next step? Why not use Euclidean itself?\n\nThe paper builds on an interesting idea, but may benefit from a thorough revision with these concerns in mind. I will wait for the author's comments to decide further.\n\nOther minor comments:\n\n- Some of the important details of the methodology, including certain details of training used, are in the appendix and not the main paper. The paper could be organized better to ensure all relevant details to understand the methodology are in the main paper.\n- The notations should be consistent across the figures and text. In Fig 2(a), the average label is \\hat{y\\prime}_{FL} as per the text, while its marked \\hat{y\\prime} in the figure. \n- In Section 2, in line 3 of the subsection titled Problem 1., there seems to be a typo in the definition of D^{(t)}: {(x, y)}_i instead of {(x, y)}_*t*.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method, Slow Thinking to Learn (STL), that leverages the interaction between a collection of per-task ‘fast learners’ (FLs) and a cross-task 'slow predictor' (SP) in order to enable sequential few-shot learning without having to train on a large number of tasks and without catastrophic forgetting. Each fast learner consists of an embedding network and a memory module [1] that makes predictions by performing a KNN on the embedded inputs, while the slow learner is meta-trained using MAML [2] to make few-shot predictions based on the embedded nearest neighbours outputted by the fast learner. The key novelty of this paper is that each fast learner is trained with an auxiliary loss term that encourages an embedding that improves the inference of the slow predictor. The model is evaluated on standard sequential training settings of permuted MNIST and split CIFAR-100, demonstrating superior ability for mitigating catastrophic forgetting and sequential few-shot learning. While their method for synergising the two types of learner is interesting and seems to have potential for few-shot learning, I recommend that it be rejected for the following main reason: \n\n* The implementation of the model seems to overstep the resource constraints typically imposed in a lifelong learning setting [3,4] in that the external memory size and number of model parameters are allowed to increase indefinitely as more tasks come in. This makes it hard to compare it fairly to the baselines that do not benefit from these advantages to the same extent in the experiments that evaluate catastrophic forgetting and sequential few-shot learning, particularly because there is not a clear comparison of the amount of resources used by each model. The experiments would be much more informative if the method were compared, for example, to other lifelong learning methods that allow for growth in the number of network parameters over time, such as [5,6].\n\nMore detailed concerns / questions:\n* I’m confused about the size of the external memory in the experiments. When the memory size is said to be fixed at 1000, is that per fast learner in the STL model or a total of 1000 across all FLs? If it is 1000 per FL, then is the memory size for the MbPA+ and the Memory Module baselines increased accordingly with each task for a fair comparison? \n* If the external memory grows indefinitely with each task, then the experimental setting becomes more akin to a multitask / meta-learning one than a lifelong learning one. Perhaps a fairer comparison would be to [7], where a meta-learning algorithm is learned in an online setting (i.e. with the tasks coming in sequentially) but where there is access to all previous data. \n* Since each FL has its own embedding network, the total number of parameters of the model grows significantly with each task, giving it an advantage over the baselines that have a fixed number of network parameters (EWC, MbPA+). Figure 4.(b) shows that MbPA+ catastrophically forgets because it uses only one embedding network for all tasks - does the Memory Module baseline use separate embedding networks for each task?\n* In Section 4.1, it is shown that STL is not sensitive to the size of the output network (the SP in the STL model), while EWC is. Firstly, it is stated that only the output network is changed but it also says that the number of convolutional layers is changed, which are previously stated to constitute the embedding network - is the embedding network also changed? In any case, the STL model benefits from multiple embedding networks, each consisting of 4 convolutional layers, resulting in a much higher parameter count than the baselines - in these circumstances, it does not seem fair to claim that STL is not sensitive to changes in the model size relative to the other models, since they have a different starting point. \n* Since the number of network parameters is increased with each task, it would be fairer to compare to other methods that do so, such as [5, 6], in a way that equalises the increase in number of parameters per task. It is claimed that STL has an advantage over these two methods because in those cases one has to decide how many neurons to add for each task - but is this not the case also for STL, since one must decide how large an embedding network to have in the FLs?\n* While the paper says that the method has better space efficiency in RAM than other methods since the fast learners can be kept on the hard disk and used one at at time for training, lifelong learning methods are typically concerned with limiting total memory usage, not just RAM.\n* The Separate-MAML baseline demonstrates the effect of removing the synergistic loss term in the FLs and its significantly worse performance in the small memory experiments (Figure 4.(c) and Figure 7) ostensibly emphasise the importance of this term, but perhaps this is not so surprising since its removal seems to mean that only a fraction of the training examples are ever passed through the SP. While in the full STL model, all the incoming data is passed through the SP in order to train the FL with the feedback loss, in the Separate-MAML model, the SP only benefits from training the output network on the examples stored in the FL memory. For this reason, it’s hard to know whether the superior performance of the full STL model arises because of the interesting dual loss function for training the embedding network or because the gradient is only propagated through the full SP pathway (embedding + output network) on a subset of the training data. In order to understand the importance of the dual FL loss function, it might be revealing to train both the full STL and Separate-MAML models with full access to all previous data.\n* In the few-shot learning experiments,  how is the amount of training equalised between the different methods? Is each method trained to convergence on the available batches or are they each trained for a fixed number of iterations?\n* Are the accuracies plotted over just one run or averages over multiple runs? If they are averages, why are there no error bars?\n* It seems that the dotted and solid lines labelled the wrong way round in Figure 6. It currently shows EWC performing worse with a larger model, which is the opposite of what is stated in the main text.\n\nComments / questions not affecting review:\n* Do the authors have any intuition for why Separate-MAML performs worse than Separate-MbPA (Figure 4c. 5a, 5b) despite the SP having had the chance to learn better initial parameters with MAML?\n* Any intuition for why performances of FL and Memory Module dip as the number of shots increases to 32 in Figure 7?\n* Part 1, last paragraph, line 3: “interdependent” rather than “dependent”.\n* Supplementary 3.1 line 1: “FLs” rather than “FTs”\n* The analogy of gradient steps in the few-shot prediction of the SP to “thinking” seems like a stretch and and a bit misleading.\n\n[1] Kaiser, Łukasz, et al. \"Learning to remember rare events.\" arXiv preprint arXiv:1703.03129 (2017).\n[2] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n[3] Schaul, Tom, et al. \"The Barbados 2018 List of Open Issues in Continual Learning.\" arXiv preprint arXiv:1811.07004 (2018)\n[4] Continual Learning Workshop, NeurIPS 2018. Continual Learning desiderata: https://sites.google.com/view/continual2018/home\n[5] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n[6] Yoon, JaeHong, et al. \"Lifelong Learning with Dynamically Expandable Network.\" International Conference on Learning Representations. International Conference on Learning Representations, 2018.\n[7] Finn, Chelsea, et al. \"Online Meta-Learning.\" International Conference on Machine Learning. 2019."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "I believe you are referring to Daniel Kahneman's book \"Think, Fast and Slow\". I believe you are over using fast vs slow and under using small vs large, complex vs simple, specific vs general. \n\nIt is not clear to me that your manually directing task data into a special FL is not the whole source of your positive results. \n\nI would like to know more about the size of FL memory needed for good results vs the task complexity and size. \n\nI do not see anything wrong I just do not see anything compelling. "
        }
    ]
}