{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper describes a method for learning compact RL policies suitable for mobile robotic applications with limited storage.  The proposed pipeline is a scalable combination of efficient neural architecture search (ENAS) and evolution strategies (ES).  Empirical evaluations are conducted on various OpenAI Gym and quadruped locomotion tasks, producing policies with as little as 10s of weight parameters, and significantly increased compression-reward trade-offs are obtained relative to some existing compact policies.\n\nAlthough reviewers appreciated certain aspects of this paper, after the rebuttal period there was no strong support for acceptance and several unsettled points were expressed.  For example, multiple reviewers felt that additional baseline comparisons were warranted to better calibrate performance, e.g., random coloring, wider range of generic compression methods, classic architecture search methods, etc.  Moreover, one reviewer remained concerned that the scope of this work was limited to very tiny model sizes whereby, at least in many cases, running the uncompressed model might be adequate.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper compresses policy networks using approaches inspired by neural architecture search. The main idea is to have a fixed size weight matrix, but learn how to share weights, so that the resulting network can be compressed by storing only unique weights values. Both the partitioning and weights are trained simultaneously, inspired by ENAS. The partitioning is modeled by an autoregressive RNN and trained via REINFORCE. The weights are modeled by a single set of weights (as opposed to a distribution) which is then updated by using a gradient approximation based on ES. The experiments carried out include comparing to existing works on policy network compression, ablating against random partitioning, as well as a few experiments meant to increase understanding of the learned partitions.\n\nPros:\n - Overall, paper is well-presented and is generally quite clear on its contributions, place in the literature, and experiment details.\n - The approach is straightforward and has applications in compressing RL policies.\nCons:\n - There should be standard deviations in Table 2 across multiple seeds, as it is unclear whether the differences in reward are significant.\n- There is some discussion on scalability in the introduction as part of the motivation, though would be nice to see some quantitative numbers. Is the current method already highly scalable, or is scalable still a potential that has yet to be reached?\n\nQuestions:\n - What is the computational cost in total CPU time, compared to baselines such as fixed random partitioning? The autoregressive sampling of partitioning seems to be a bottleneck during training, as sampling may be slow, especially when scaling to larger weight matrices. Is this why the partitioning is updated much more sparsely (as shown in Figure 5)?\n - For the gradient estimation in eq 3, what exactly is this unbiased with respect to? Did you mean asymptotically unbiased in the limit as sigma -> 0?\n - Do Chromatic networks always use M partitions? (How exactly were the number of partitions chosen, for Tables 1 and 2?) If I understood correctly, Figure 3 is about the Masked networks from (Lenc et al., 2019). If so, is there a similar method in which the number of partitions of Chromatic networks can be learned (or regularized)?\n - (I may have misunderstood these figures, but:) There seems to be quite a large difference in training curves for different workers, e.g. shown in Figures 4 and 5. The red curve seems to almost always be best, while the green curve is much higher variance. Why is this? I also couldn't tell exactly how many colors there are in these plots (introduction mentions 300?), but wouldn't a mean/std plot be easier to parse (or do the specific colors mean something significant)?\n - Is there a reason why Toeplitz and Circulant have missing numbers in the #bits columns in Table 2?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary: This paper focuses on neural architecture search for constructing compact RL policies. It combines ideas from the popular ENAS and ES methods for optimisation. Recent work defined a family of compact policies by imposing a fixed Toeplitz structure. This paper introduces the so-called “chromatic network” architecture, which partitions weights of the RL network into tied sub-groups. The partitioning is searched with ENAS and shared weights are updated via ES. Experiments on continuous control benchmarks show that good performance can be obtained using a very small number of parameters. Favourable reward-compression outcomes can be achieved compared to some baseline alternatives.\n\nTo my understanding the main contributions are (1) the Chromatic parameter sharing scheme and (2) the combined ENAS+ES learning procedure.  \n\nPros:\n+ Chromatic networks provide a neat idea for managing compact networks via parameter sharing. (Although not dramatically novel given cited work by Salimans, Gaier, etc)\n+ Searching both the partitioning + weight values with ENAS and ES provides a nice way to do learning & compact architecture search simultaneously.\n+ Results generally show favourable compression/reward performance vs Mask, Toeplitz  and Circulant baselines. \n\nCons & Questions: \n0. Motivation. The paper claims that RL can be high-dimensional with millions of parameters, and so there is a need to apply NAS to RL. But experiments are conducted in low dimensional environments with only 100s of parameters. The empirical validation doesn’t match the motivating scenario. We would want to see results on successful compression of larger vision-based networks to be fully persuaded. Without this it undermines the significance of the paper. The paper makes a claim about embedded devices, but this is unconvincing, as for these proprioceptive control tasks, the uncompressed networks are already small enough to run on most embedded devices. \n\n 1. ENAS vs Chromatic. Both use RL controllers. The main difference to ENAS seems to be the Chromatic sharing scheme, and use of ES rather than Backprop to update the weights. Some points of motivation/justification are not very clear after reading: (1) Why vanilla ENAS can’t be used for RL? (with the modification of replacing standard backdrop with any standard continuous control RL algorithm for weight updates). It is sort of claimed that ENAS can’t be used, it’s not obvious to me that this is true. (2) How is the  Chromatic strategy specific for RL? In principle it should also be usable in SL. Assessing its performance in SL would be more broadly relevant, interesting and significant since there is more prior compression work in SL.\n\n2. The paper is a bit vague about how number of partitions is set. Is it a user-specifiable parameter, if so how do we set it? It’s suggested that partition number is included in the reward function, but then it’s even more unclear how to calibrate this to hit a particular performance or size target (or optimize performance for fixed size, size for fixed performance constraint) that a user may require in practice.\n\n3. The real (wall-clock) running time for different NAS methods should be reported in the results. \n\n4. Baselines. The presented results are a reasonable start, but they are all very much variations within the same family. One would like to know how the current method compares to: (i) Direct application of ENAS (See also Q1), (ii) Training the full network and applying standard NN pruning techniques (such as magnitude based), (iii) Baseline of training a comparably small sized network directly. (ivv) Importantly, since the size of networks here are very small, classic neuro-evolution algorithms from evolutionary robotics such as NEAT (Stanley & Miikkulainen) seem to provide a reasonable alternative. It’s hard to know if to be impressed with the current result or not without seeing the results for a decent NEAT-like competitor. (See also Q0).\n\nOther:\nA. A presentation of the algorithm with pseudocode would help the reader follow an overview of the algorithm. \nB. Is the RL network/method used for the tasks the same as the network used in paper “Structured Evolution with Compact Architectures for Scalable Policy Optimization”? If so what is the source of the discrepancy between Tab 2 here and Tab 1 in that paper? \nC. The title doesn’t make the content obvious to a naive prospective reader. At least something like “RL with Chromatic Networks for Compact Architecture Search” would be more informative.\nD. Eq (1) has a vspace error. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors propose to construct reinforcement learning policies with very few parameters. For this purpose, they force a feed-forward neural network to share most of its weights, reducing the total number of different weights to at most 23 and therefore compress the network. Instead of manually encoding which weights are shared, the authors propose to use a reinforcement learning method to learn this mapping. The values of all parameters are learned with a gradient-based method.\n\nThe paper is very well-written. The concepts are easy to follow, the related work covers a lot of different but related domains. The experimental section sheds light on some important aspects. However, I have few concerns regarding Table 2.\n\nTable 1 considers more tasks than Table 2. Why did you decide to use only a subset and why these tasks? The architecture of the FFNN is not part of your search space but you admit it is very important. For the experiments you chose a FFNN with one hidden layer. Furthermore, you manually adapted the number of partitions. Would fixing the architecture to one hidden layer work in general? How do you select the number of partitions? Both these choices seem crucial and to invalidate the automation aspect. You select the best run of 300. Do your baselines follow a comparable setup? What if you compare mean rewards? The discussion of random partitions is very important and it is nice to see that you discuss this in Section 4.3. As you mentioned, in NAS random search is a strong competitor. Therefore, this method deserves to be added to Table 2 as well.\nConcluding, this is an okay paper with limited innovation. The comparison to baseline need to be improved or at least justified."
        }
    ]
}