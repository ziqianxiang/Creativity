{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, the authors developed the generalized value iteration network (GVIN) to handle general graphs, which can be either regular or irregular. Different from GVIN, which uses spatial kernel to perform graph convolution, the proposed GrMPN method uses neural message passing, with the aim to be invariant to graph isomorphism. Two particular instances of the neural message passing were realized and evaluated, which are graph network (GN) and graph attention network (GAT). In the experiments, the proposed GrMPN were shown to be effective on motion planning, through either regular or irregular graphs.\n\nThere are several points that the authors may want to further clarify in this paper. Since the proposed GrMPN is claimed to generalize existing graph-based planning algorithms, a discussion on how does the generalization is achieved is desired, e.g., compare with GVIN. It seems the key motivation to develop GrMPN is to obtain the invariance to graph isomorphism which cannot be handled by existing methods. However, the discussions on why is it important to handle this problem, and how can GrMPN achieve the invariance but others cannot, are insufficient in the technical section. Moreover, it is not clear on why imitation learning is selected over reinforcement learning in this work, considering the previous work GVIN uses reinforcement learning. Overall, the technical development on GVIN in this work seems to be limited, which lies largely on the employment of a different graph convolution method.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an extension of Value Iteration Networks that uses graph neural networks to perform value iteration over graphs of arbitrary size and connectivity. The paper demonstrates how graph networks can be used to perform value iteration both in their original form and with attention, and then compares these variants to several baselines across three domains (grid worlds, irregular graphs, and motion planning). Across all these comparisons, the proposed method tends to perform better than the alternatives and with better data efficiency.\n\nOverall, I think that this is an interesting extension to VINs and a natural use of graph neural networks. However, I think the paper is somewhat limited by the fact that it only performs experiments with imitation learning and not with full RL. Additionally, I think the paper suffers from a lack of clarity in some places and issues with the evaluation. I thus lean on the side of reject, but might be willing to increase my score if these issues can be addressed.\n\nMy main concern is that the method is not evaluated in the context of actual RL or even in environments where the model would potentially provide an improvement over symbolic approaches. Specifically, In the imitation learning setting, this means that the network is essentially just learning a shortest-paths algorithm (I’ll also note that this has already been done for smaller graphs as a toy example in Battaglia et al. (2018)). This then raises the question: why do we need to learn a shortest paths algorithm at all? Why not just use a traditional symbolic shortest paths algorithm (as is actually done to generate all the training data in the paper)? The answer in the original VIN paper was that by training the VIN module with a policy, the agent could learn to exploit additional perceptual information that might not be so straightforward to incorporate in the symbolic computation of value iteration (e.g. see the “Mars” domain from the VIN paper). It does not seem to me that any of the experiments in the present paper test on graphs where the reward function is unknown and a function of the input features. Thus, it does not seem to me that the paper sufficiently justifies why the method is better than alternative symbolic methods.\n \nI also found the paper somewhat hard to read. It is a bit overly verbose in some places while not providing enough detail in other places. For example, I do not think it is necessary to go into so much detail about GPPN, Generalized VIT, GCN, or MPNN (probably these details can be included in the appendix). It is of course important to briefly describe what GPPN and Generalized VIT do in relation to the proposed method, but this could be done in the main text in only 1-2 sentences each without including any math. This would make it much easier to identify which equations are actually relevant for the proposed method. However, I thought that other parts of the paper weren’t particularly clear: for example, it’s not clear to me what the graphs look like in the Baxter experiments.\n\nFinally, I appreciate the extensive experiments and comparisons to baselines. However, given that only single numbers are reported, I find these results almost impossible to interpret. I would like to see all results reported with error bars over multiple training runs over the algorithm (ideally at least 3). Otherwise, I do not know whether the difference between (for example) 99.8 and 99.5 is meaningful (Table 2).\n\nSome additional comments and questions:\n\n- The original VIN paper did include results on an irregular graph (WebNAV). It would be nice to see some discussion of this result in comparison to the proposed method.\n- How would this method scale to very large graphs?\n- The paper should include a discussion of other uses of graph networks in RL, such as [1-5].\n- It would be nice to see a comparison that uses max pooling rather than sum pooling. I suspect the reason why the GAT variant works so well is because it is picking out the maximum value, so it would be good to see a comparison of this explicitly.\n\n[1] Khalil, E., Dai, H., Zhang, Y., Dilkina, B., & Song, L. (2017). Learning combinatorial optimization algorithms over graphs. NeurIPS 2017.\n[2] Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., & Battaglia, P. W. (2018). Relational inductive bias for physical construction in humans and machines. CogSci 2018.\n[3] You, J., Liu, B., Ying, Z., Pande, V., & Leskovec, J. (2018). Graph convolutional policy network for goal-directed molecular graph generation. NeurIPS 2018.\n[4] Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L., Kohli, P., Battaglia, P. W., & Hamrick, J. B. (2019). Structured agents for physical construction. ICML 2019.\n[5] Wang, T., Liao, R., Ba, J., & Fidler, S. (2018). Nervenet: Learning structured policy with graph neural networks. ICLR 2018."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper proposes using graph neural networks for learning to plan on general graphs. The proposed method is able to learn from optimal plans and generalize to unseen graphs. Empirically it outperforms several existing baseline approaches on both regular graphs (2D lattices) and general graphs.\n\nThis paper should be rejected because (1) the contribution is incremental, similar ideas has been explored in previous papers; (2) the experiments are too toy to demonstrate the practical use of the proposed method.\n\n==============================================================================================\n\nMain argument\n\nThe idea of this paper is not novel enough. As its predecessors, VIN[1] contributed the idea of embedding the value iteration algorithm into the neural architecture; and GVIN[2] extended the same idea to the general graph domain. This paper instead tries to improve GVIN by replacing the value iteration update with the graph neural network message passing update. The only contribution is showing empirically that graph neural networks are somehow more suitable than the previous parametrizations of update operators. Moreover, the experiment part of this paper is not convincing. Both synthetic domains I and II looks toy and domain III looks very similar to domain II (please let me know if I am wrong). Domain III is more practical but it doesn't look obvious to me why using learning is better than using traditional shortest path algorithm such as Dijkstra? From my understanding Dijkstra would be more efficient and effective. This experiment is not a good example of a potential real-world application of the proposed method.\n\n\n==============================================================================================\n\nWriting, soundness and organization of the paper\n\nThe writing of this paper is acceptable, but the organization can be improved. Using more than 4 pages to introduce the background knowledge is way too much. The writers should focus more on their own work.\n\nThe authors keep emphasizing the idea of invariant to graph isomorphism throughout the paper. It is a property of the proposed architecture but correct me if I am wrong, it doesn't occur to me that VIN and GVIN lack such property. It also confuses me when the author says in the last paragraph in the first section, \"These models are known to be invariant to graph isomorphism, therefore they are able to have a generalization ability to graphs of different sizes and structures\". I don't really understand the rationale behind this. Can the authors explain this in the response?\n\n\n\n[1] Tamar, Aviv, et al. \"Value iteration networks.\" Advances in Neural Information Processing Systems. 2016.\n[2] Niu, Sufeng, et al. \"Generalized value iteration networks: Life beyond lattices.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018."
        }
    ]
}