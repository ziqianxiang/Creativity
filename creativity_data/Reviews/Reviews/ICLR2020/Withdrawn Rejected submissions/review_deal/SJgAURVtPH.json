{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents an approach for learning 3D visual representations by using a set of 2D TGB images as input. Then, they associate a `natural language' utterance and it's parse tree to this spatially aware 3D feature representations  and show benefits of multiple downstream tasks. \n\nI liked the paper mainly because of the approach and the exhaustive evaluation and analysis. However I am not a 100% sure about the novelty and big promises and bold claims. I have given a weak reject because of the concerns mentioned in the weaknesses\n\nStrengths: \nThe paper has an interesting motivation - 2D feature representations present several limitations because they do not obey intuitive physics constraints and are therefore are not a suitable representations. Instead a 3D visual feature representation that are mapped to descriptions / instructions is more beneficial. \n\nThe paper validates the advantage of 3D feature representation over 2D feature representation through an exhaustive list of experiments by empirically showing improvements on range of applications - such as affordability reasoning, referrential expression detection and instruction following. \n\nWeaknesses: \n- Beyond a proof of concept that 3D feature representations are a useful way of incorporating spatial common sense into language understanding, the dataset will potentially be useful for other applications too. \n\n- In section 2.2, the authors mention if the two generated objects interpenetrate in 3D, we sample objects appearances and locations until we find  configuration where objects do no 3D interpenetrate. I wonder why does this constraint need to be hard-coded and not be learned implicitly by the model? \n\n- The paper also uses several terms which might be misleading. For instance - the description of the scene or instruction is generated based on a template but has been called natural language everywhere. Given that the language is templated and uses a limited vocabulary, it doesn't capture all variations and multiple paraphrases of the same description. Additionally the network is called `modular neural networks' which reminds of Neural Modular Networks; Andreas et al, 2015.  \n\n- The paper is also fairly difficult to follow and is quite dense. It took me multiple passes on the paper to fully comprehend every experiment and modeling choices. I think the paper would greatly benefit from explicit discussions around novelty. Specially in terms of modeling, what's the contribution on top of Tung et al (50). Is it the \"modular neural network\" which depends on parse tree of the description or instruction and high supervision. \n\n- The proposed approach, as mentioned in limitation, needs high amount of supervision. This makes the method unlikely to scale to real applications. Some discussion towards how this would scale to current datasets / simulators / Graphics engine would be great. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents methods for linking language reasoning with 3D spatial reasoning. These methods are demonstrated on \n- spatial reasoning: an image is created to evaluate if a natural language utterance about objects is possible in 3D;\n- referential object detection: given the description of an object and an image, localise the object in 3D;\n- instruction following: given an instruction in NL and an image, predict the initial and final 3D locations of the object to be moved.\n\nThe main contribution is to introduce 3D features to several problems involving vision and NLP -- the 3D features come from some previous work (50). 3D understanding is clearly needed to solve properly these problems while most previous work attempted to solve them \"in 2D\", and this is a very interesting and important contribution.\n\nThe work itself is thus very interesting. There is some manual supervision involved to link nouns with 3D bounding boxes (section 2.1) but the approach remains attractive. My concern is that the paper is difficult to follow,  with limited technical details: The paper is very dense, as it tackles several problems, and the links between the different parts are not always clear. This also does not leave much  space for technical details and only high-level descriptions of the methods and the experiments are given. This unfortunately makes the paper difficult to accept as it is, as the work is difficult to understand in detail and not really replicable.\n\nI would suggest the authors to sacrifice one part (maybe the last one, which is close to the referential problem, even if it corresponds to the ultimate goal of the work), to get more space and give a better formalization and technical description of the work.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper address a series of problems in vision-and-language focusing on a representation that has 3D visual features. This include: referring expressions in 3D, image generation from language. The key novelty appears to be adding 3D features to support these three reasoning tasks, as well as a new dataset. \n\nSummary of review: The paper needs some more work before it can be published. It can become a much better paper if the following is handled. (1) Clarify the novelty explicitly; (2) Evaluate the novel components using well-described ablation experiments, SoTA baselines, and demonstrate how the novel representation yields better reasoning. I would also recommend to focus on fewer tasks but with more convincing evidence. \n\nStrengths: \nThe paper addresses exciting problems at the interface of embodied V&L, and 3D vision. Adding 3D may be an important component for understanding complex scenes, and is necessary for some types of spatial reasoning.  \n\nWeakness: \n(1) The paper tries to do \"everything\", hopping from one task to another in a way that makes the logical flow hard to follow. It claims victory in three different problems:  spatial \"affordability\" reasoning, referential object detection, instruction following, while generalizing better across camera viewpoints, and gaining from language-driven feature learning. As an example, Fig 1 illustrates 4 different tasks and architectures, with little insight into how they are related or can be integrated.  \n\nMore importantly, the paper does not delineate clearly which components of the model are novel. For example, which parts in the representation (Sec 2.1) are new , and which are identical to Tung et al (50)? My recommendation to the author is to reorganize the paper to clarify the main novel contribution (the representation), the novelty in he method, and the novelty in the data. \n\n(2) The writing style of the paper keeps the description vague about many of the technical details. This makes the work impossible to reproduce and the paper harder to review.\nAs an example, the paper does not describe the dataset explicitly. It states \"We consider the CLEVR dataset\". One would think they augmented the CLEVR dataset with 3D information, but then it appears that each scene is rendered from multiple azimuths and camera positions. What are the properties of the dataset? How many samples, objects, images? How was it generated? Is the train/test split compositional?\n\n(3) experiments: \n(3a) Affordability is tested on 100 sentences. This is an unusually small dataset that is easy to overfit, specially if multiple experiments are performed aloing time (\"grad student overfitting\"). More details are needed about what measures were taken to avoid this. \n\n(3b) referring expressions. The performance of the baseline is surprisingly poor. RR on CLEVR reports much higher IOU. Here, performance is reported in as 3/5, 0/5 which makes it hard to compare with previous baselines. \n\nminor: \n-- what is the meaning of colored frames in Fig 3? \n-- Fig 2 Image generation. \n   quantitative analysis w.r.t ground truth? w.r.t. baseline? \n   comparison to other image generatiojn from text (e.g. HIntz et al ICLR 2019)?\n   How were these three exaples selected?\n-- Only F1 scores are reported for RR. Need to report IOU as is commonly done.  \n-- Avoid over-hyped promises about common sense reasoning. \n"
        }
    ]
}