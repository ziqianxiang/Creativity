{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper tackles the problem of multiagent reinforcement learning.\nThe authors expand on the work of Iqbal and Sha (2018), which uses an attention mechanism to learn a counterfactual multiagent critic.\nInstead of an attention mechanism, the authors generalize the method to learn a \"headmaster\" critic, which assigns groups of agents.\nThe critic randomly selects n collections with random size s (changing n and s every k epochs), and applies a typical loss for the Q function based off of forming these groups.\n\nI don't believe this paper should be accepted to ICLR based off these observations:\n- The paper claims on L120 the headmaster learns how to group agents, but it is not explained in the paper how it learns it, it seems just randomly formed according to the description.\n- The results are not compelling enough, being barely above the algorithm it is being built off of, MAAC, and below MADDPG+SAC for few agents.\n- There are numerous writing errors - and the description of algorithms are not clear enough, see below for a few examples.\n\n\nSome comments:\n- Eq 10: the Q function should take in an observation and action, but now why is it taking a \"super agent\"? Should the sum also be until lowercase n collections?\n- Eq 12: This sum upper index should be until S_k, if we consider the k-th group? N is not well defined nor obvious.\n- L131 what is the degredation (do you mean degeneration?) and why is it bad?\n- L214 what does \"less poorly performance\" mean?\n- Sec 4.3 ATOC also seems to obtain comparable results as SMAC.\n\n\nnits:\n- L60: \"COMA _proposed_...\n- L106: misspelling \"siven\"\n- L127: misspelling \"contirbutions\"\n- L175: n in 'niceness' conflicts with the n collections defined in the previous sections\n- L229: misspelling \"hige\"\n- Everywhere: add a space before begin parenthesis"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "1.\tSummarize what the paper claims to do/contribute. Be positive and generous.\nThe paper claims to propose a new semi-centralized method for optimizing multi-agent RL problems. It does this by creating random groupings of agents to pass messages between each other in a centralized critic fashion, similar to MADDPG. The method is interesting but I find the paper rather confusing. The method is not well described. While there is a lot of math in the paper it is rather common math that is not specific to the method until section 3.2.3. There are no details on how the method supports random groupings of agents in different amounts. Discussion on how important the random groupings off agents are should also be included. In the papers present state, I will have to vote for rejecting this paper.\nMore detailed comments:\n-\tThe abstract for the paper is rather confusing. There are a number of terms used that are not defined (headmaster, group, timing, etc). This makes it difficult to understand the proposed motivation and solution.\n-\tThe use of terms and vernacular in the introduction is rather loose. If more of these terms are defined clearly and in order it would greatly help the reader\n-\tThe claim that the method support “high dimension settings” is not well supported via experiments or design. Much more analysis related to this claim is necessary.\n-\tFor equation 1. It seems j is used for indices where in the next you refer to i.\n-\tThere is more detail than necessary in section 3.1. Most of this can be moved to the appendix. This space should be used to better explain, in detail, how the method functions.\n-\tHow does the network model cope with variable sized random groups over time? Is there some kind of attention model or sequence model being used in the critic?\n-\tThe tasks used in this work all seem to be new. Is there a reason they were all created and few already created environments were used for evaluation?\n-\tThe description in the caption for Figure 1 is very sparse. What are the coloured blocks for int he Coin game? What do the disks of different sizes and colours indicate in the treasure collection game?\n-\tIn Table 1 there does not appear to be any results for MADDPG on MAgent. Can these be computed and added?\n-\tThe description of the results in Figure 2 is also very sparse. How many random seeds are these plots averaged over? Was a similar network structure used across each model? SMAC does not appear to significantly improve over prior methods.\n-\tI do not understand the claim that the dynamics over time for MAgent causes DDPG based methods to break down. Much more reasoning and experimentation are needed to show that DDPG-based methods truly have trouble in these environments."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The author proposes a high-level headmaster critic to enable more efficient communication between agents in the multi-agent reinforcement learning algorithm training process, thus achieving better collaboration. Experiments show that the algorithm proposed by the author can exceed the performance of the benchmark algorithm.\n\nHowever, this paper has the following problems:\n1. The motivation for this article is unclear. Why does the presentation of headmaster critic enable more efficient communication between agents?\n2. The first paragraph of Section 3.2 of the article indicates that the proposed algorithm can learn how to group agents and when the agents communicate. Why do they need to be grouped? What does communication mean? What is the content of the communication? Section 3.2.1 indicates that agents are randomly grouped, so why to say that this grouping is learned?\n3. Section 3.2.1, what the difference between random groupings with those obtained through learning (like ATOC)? What are the advantages?\n4. Why did the headmaster critic learned in Section 3.2.1 not appear in the subsequent parts of the algorithm? Specifically, which part of the algorithm is used in it?\n5. Why is Equation 14 defined as such? What is its physical meaning? Why are the two terms before and after the plus sign the same but need to be multiplied by two different coefficients?\n6. The definitions of all loss functions in the paper, as well as the definition of Q-value functions, all use the same symbols. How to distinguish their specific meanings?\n7. What are the specific definitions of the state space, action space, and reward function for the three simulation environments used in the experimental section?\n8. There is too little content in the experimental part. There is no further experimental analysis for the motivation of the paper.\n9. It is too rough in content organization, there is no overall algorithm framework, and there is no algorithm pseudo code, which makes the understanding of this paper extremely difficult.\n10. This article refers extensively to communication between agents, but the relevant work section mentions little about communication-learning-based works.\n11. There are a lot of spelling and grammatical errors in the text, as well as formatting irregularities."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper attempts to improve multi-agent RL by introducing a new algorithm, scholastic actor-critic. The paper evaluates their method on three multi-agent benchmarks, including the coin game, cooperative treasure collection, and MAgent.\n\nOn the whole, I found the paper difficult to understand. It was hard for me to parse what the proposed method was actually doing, and what was the motivation behind it. As far as I can tell, the paper introduces a new critic that randomly assigns agents into groups and then computes a Q value on based on the ‘average contributions’ of the agents in the group (making a ‘super agent’). It’s not clear to me how these contributions are aggregated, as well as which agent’s observation is used as input (there’s only 1 ‘o’). There is also a gamma value that is introduced in section 3.2.2 without explanation. Section 3.2.3 talks mostly about how the authors construct their baseline for the advantage function. I don’t see how any of these results in the ‘actors being able to communicate more efficiently during training’.   \n\nIn terms of results, the proposed method doesn’t seem to perform much better than the baselines (it performs worse than at least 1 baseline when there’s fewer than 64 agents, slightly better with 64 and 128 agents). There’s also no error bars to indicate whether the slight improvement with 64 and more agents is significant.\n\nOverall, I do not recommend acceptance in the paper’s current form. I’d encourage the authors to re-write the paper to make the main contribution clearer, including checking for grammatical and spelling errors, and to improve the experimental results with a smaller number of agents. \n\n\nOther comments:\n-\tWhy the name ‘scholastic actor critic’? \n-\tI don’t see how the MAgent environment is an iterated prisoner’s dilemma, as claimed in section 4.1\n-\t“Then other agents’ contributions could be formulated as 15.” -> not sure what this means\n"
        }
    ]
}