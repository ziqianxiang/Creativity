{
    "Decision": {
        "decision": "Accept (Talk)",
        "comment": "Gradient clipping is increasingly popular and it's nice to see a paper theoretically exploring its nice performance. All reviewers appreciated the work and the results.\n\nPlease make sure to incorporate all of their comments for the final version.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper applies new assumption on smoothness that assume the norm of Hessian is bounded by a scalar plus norm of gradient. The traditional smoothness is only bounded with a scalar, the proposed assumption is more relaxed because now the norm of Hessian can grow with the norm of gradient. Under this assumption, the authors show clipped gradient converges faster than gradient for general nonconvex problems. The authors provide insights on why the proposed assumption is good for describing neural networks, and empirically verify the assumption with ResNet on CIFAR and LSTM on PTB. \n\nI like the paper in general. It is well written and easy to follow. The contributions are clearly described and the techniques seem to be solid. \n\nI want a little bit more discussion to help me better understand the paper. \n\n1) Intuitively, try best in plain English, why does clipped gradient convergence does not have dependency on L_1 M? Could the authors provide more discussion on the learning rate (hyperparameters) used for clipped gradient and gradient?\n\n2) The convergence rates under the proposed assumption are slower than traditional smoothness assumption. Could you verify the slow convergence rate under proposed assumption aligns better with practical training? Could you provide a toy example, for example x^3, to show the advantage of the proposed assumption and the convergence under the assumption? What is the gap between clipped gradient and gradient, for experiments in figure 4, and possible toy problem? Could the authors elaborate the difference of clipped gradient and gradient with the details of theory, instead of simply claiming clipped gradient is faster?\n\nTypo, page 5, (0, L) -> (L, 0)\n\n\n============ after rebuttal ===================\nI am happy to see the paper in the conference. The paper is intuitive and well written. My remaining comments are still on the slower convergence rate comparing to previous assumption. Even before, researchers suggest overparameterized network can converge faster than expected. This paper suggests a generally slower convergence rate if the proposed assumption does fit practice better.   \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "In this paper, the authors relax the generally used Lipschitz smoothness condition in optimization, to a more general smoothness condition that may depend on norm of the gradient. The authors proved that, with this relaxed condition, under such cases, both GD and clipped GD can converge within O(1/\\epsilon^2) time, but when their exists x where the gradient norm can be very large, the clipped GD will converge provably faster than GD with a proved GD convergence rate lower bound. The authors also generalize their results to SGD. Experiments show that in deep neural network local gradient Lipschitz constant is scale with gradient norm, and use clipped gradient can accelerate convergence while keep good generalization performance as expected, which is well observed by other researchers.\n\nDetailed comments:\n1. The f^* In theorem 3 is not defined. I think it’s not the f^* in Assumption 1? Should be more clear. Also, in non-convex optimization, can the deterministic GD find the global optimum? If f^* is the stationary point the algorithm find, then the Assumption 1 is a little weird. The authors should better clarify this.\n2. Feel the claim of the Assumption can be relaxed to only hold on \\mathcal{S} is a little confusing. Which Assumption can be relaxed? It seems that in the proof of Lemma 9, we need the Assumption 3 holds for the set \\|x^+-x\\| \\leq min{1/L_1, 1}. I think maybe the other Assumption will also be needed globally, unless the authors can prove that the optimization is only on a compact set in S.\n3. Maybe a discussion connected the existing Lipschitz constant based results to the new results in this paper can make the readers more aware of the contribution in this paper. For example, the Lipschitz constant of neural network can be of order O(L_1 M) due to the definition, so if L_1 M is large, for clipped GD we can still have expected decreasing of O(1/L_0) each turn while vanilla GD with the best O(1/L) step size can still only have O(1/L) where L = L_0 + L_1 M due to the traditional results based on Lipschitz smoothness.\n\nI don’t totally go through the whole proof, but I think the results are reasonable and the most of the techniques are standard to the whole community. So there may be no fatal error that violates the conclusion. I find the whole paper interesting and match the intuition from the practice. However, I think it can be polished to make the whole idea more clear and more easy to accept by the potential theorists and practitioner audience.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors provide the definition of a (L_0, L_1)-smooth function.\nThey motivate this class of function based on theoretical arguments and\nempirical observations in the context of neural network training loss.\nThe authors go on to show that for (L_0, L_1)-smooth loss constant\nstep-size gradient descent can perform arbitrarily bad with poor initialization.\nSpecifically, the largest constant step size that can be used without divergence\ndepends on the upper bound of the gradient norm.\nIf the function is steep (high gradient norm) at some point during the\noptimization process the steps need to become arbitrarily small\n(especially relevant in nonconvex loss, where gradient norm does not always decrease).\nThe small constant step size slows down convergence everywhere else.\nThe described phenomenon is an argument for step sizes adaptivity in general\nClipping the gradient based on the known (L_0, L_1)-smoothness is a form of\nadaptivity that makes steps small enough to avoid divergence only where the\nfunction is steep and can thus be more efficient.\n\nOverall, the authors give a theoretical justification for the use of clipped\ngradient descent in the context of training neural networks.\nThe analysis of clipped gradient descent is performed rigorously (proofs are\nprovided in the appendix) and extended to the stochastic gradient descent setting.\nThe motivation for using (L_0, L_1)-smoothness and gradient clipping is also\nshown to be relevant in practical experiments with language and image\nclassification models.\n\nI recommend to accept the paper.\n\nThe paper is a clearly stated contribution to optimization theory as motivated\nby machine learning applications.\nThe authors give a clean theoretical analysis of a previously empirically\nobserved phenomenon and an algorithm that is already used in practice.\n\nNotes:\n- Page 1, first paragraph:\nIf f(x) = E_xi [f(x, xi)] then f(x) the expectation is generally not stochastic,\nlike you write before. It is clear what you mean but Maybe use a different symbol.\n\n- Page 2, ...we observe the function smoothness has a strong correlation with gradient norm 2:\nPerhaps write something like \"(see Figure 2)\" instead of just \"2\" here.\n\n- Page 2, after definition of Lipschitz continuous:\nNitpick: Formal definition based on Hessian norm upper bound should also have, for all x \\in \\mathbb R^d\n\n- General: Some equations that are not referenced seem to be numbered, some are not.\n"
        }
    ]
}