{
    "Decision": {
        "decision": "Accept (Talk)",
        "comment": "The paper shows empirical evidence that the the optimal action-value function Q* often has a low-rank structure. It uses ideas from the matrix estimation/completion literature to provide a modification of value iteration that benefits from such a low-rank structure.\nThe reviewers are all positive about this paper. They find the idea novel and the writing clear.\nThere have been some questions about the relation of this concept of rank to other definitions and usage of rank in the RL literature.\nThe authors’ rebuttal seem to be satisfactory to the reviewers. Given these, I recommend acceptance of this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduces an interesting idea of exploiting the low-rank structure of the value function to reduce the computation complexity of value-based RL algorithms. Instead of working in the reduced space, they focus to operate on the original space and reduce the computation by looking at few elements and inferring the rest.  They use a matrix completion/estimation strategy to infer the global structure from a smaller set of samples. They show empirical evidence of the low-rank structure in few classical control tasks (Mountain Car, Inverted Pendulum, Cart Pole), and provide an iterative procedure - Structure Value-based Planning (SVP), that is similar to value iteration but is able to exploit the low-rank structure to reduce the computational time.  They also provide a deep RL extension - SV-RL, that can be applied to value-based methods. They test the efficiency of their approach to Atari games. \n\n\nOverall this paper presents an interesting idea, that also scales to the deep RL algorithms. However, there are a few missing components that need to be addressed in order to fully support the claims.  Given these clarifications in the author's response, I would be willing to increase the score.\n\n1) Nature of the regularization for SV-RL.\nThe authors are proposing a form of regularization that enforces low-rank structure for the value function (and target Q-values in particular for deep RL agents).  The authors show that this form of regularization is helpful for improving the learning for low-rank tasks, and for tasks that have a high-rank the performance is worse.  This kind of regularization balances having a low-rank and small reconstruction error. However, shouldn’t the regularization also depend on the size of the sub-matrix (the minibatch)?  However, I didn’t find any experiments related to how changing it affects performance. Also, is the regularization related to due to the random projections (Johnson–Lindenstrauss lemma)?\n\n2) It is important to note that SV-RL is limited to Deep Q-learning based techniques. So it can’t be applied to any value-based method, especially when the samples in the sub-matrix are correlated.  \n\n3) Missing literature that exploits Low-rank structure for planning. There is literature on RL that is based on exploiting the low-rank structure for planning [1, 2, 3].  \n\n4) All the experiments are in deterministic environments? Is there a reason behind this? \n \n5) (Optional) This regularization introduces error in reconstructed approximate Q-values. It will be useful to have some analysis on how far it deviates from the optimal value function. There has been work in the field [4, 5] that I believe can be used to help derive an analysis of the kind of approximation error bounds that are being introduced here.\n\n\n\nReferences: \n\n\n[1] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954–966, 2011\n\n[2] Pierre-Luc Bacon, Borja Balle, and Doina Precup. Learning and planning with timing information in markov decision processes. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence , pp. 111–120. AUAI Press, 2015.\n\n[3] Sylvie CW Ong, Yuri Grinberg, and Joelle Pineau. Goal-directed online learning of predictive models. In European Workshop on Reinforcement Learning, pp. 18–29. Springer, 2011.\n\n[4] Klopp, Olga. \"Noisy low-rank matrix completion with general sampling distribution.\" Bernoulli 20.1 (2014): 282-303.\n\n[5] Recht, Benjamin. \"A simpler approach to matrix completion.\" Journal of Machine Learning Research 12.Dec (2011): 3413-3430.\n\n\n\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure. The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank. The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications. \n\nThe paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model. A few comments and questions:\n\n- The assumption that the Q matrix should be low-rank is demonstrated with several experiments. Is there any theoretical motivation or guarantee for this assumption as well?\n\n- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?\n\n- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?\n\n- Please clearly define the notation used in Section 4.2."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\tThis paper develops a method for taking advantage of structure in the value function to facilitate faster planning and learning. The key insight is that MDPs with low rank Q^* matrices can be solved more expediently using matrix estimation methods, both for classical dynamic programming methods (value iteration) and for learning in rich environments using recent model-free deep RL techniques. Thorough empirical analysis is conducted both for value iteration in tabular MDPs and for deep RL in rich environments. These experiments highlight new findings about the role the rank of the Q matrix plays in planning convergence and learning rates.\n\n\tI view this paper as containing several key contributions: first, the analysis on the role of Q rank in planning and learning---experiments conducted indicate that even complicated environments tend to have low rank Q matrices (when approximated). Highlighting the role of this rank and the corresponding empirical analysis estimating it in benchmark RL and control tasks is, to my knowledge, novel. Second, and perhaps the most significant contribution, is \"Structured Value RL\" (SV-RL), an easy-to-apply method that can be incorporated into many Q-based deep RL methods with little overhead. The empirical results are compelling: across three different variations of DQN-like architectures, the SV RL augmentation tends to improve learning. Presentation of results is rigorous, too, and provide strong evidence that the method works.\n\n\tAs the paper mentions, theoretical analysis on the impact of Q-rank on dynamic programming (and perhaps learning) would be of great interest to the community. I take this analysis to be out of scope for this paper, but could see the work motivating future investigation into these questions.\n\nVerdict: Overall, I take this paper to present many novel insights, establish solid motivation with good writing and examples, and offers compelling evidence about the strength of SV RL. I recommend accepting the paper.\n\nComments:\n\tC1: The visuals throughout the paper are helpful!\n\tC2: The paper is well written: the use of examples was effective in developing the motivation.\n\tC3: Section 2 is helpful for understanding the ideas developed in the paper. However, there are many well developed planning frameworks for MDPs that trade-off optimality with computational efficiency. It might be worth discussing some of these methods up front. For instance, Bounded Real-Time Dynamic Programming (McMahan et al. 2005) explicitly uses value function structure to improve planning speed, with performance guarantees, as does Focused RTDP (Smith and Simmons 2006). I don't take the computational complexity improvements of the proposed method to be the primary contribution, so just a brief discussion to contextualize the work against other planning literature would be helpful.\n\tC4: While the \"rank\" studied here is of a different form, some discussion of the Bellman Rank work (Jiang et al. 2017) might be useful for differentiating the two notions of \"rank\" at play, and how they are each used to expedite learning. The Bellman Rank is used as a measure of complexity of an MDP---Jiang et al. develop an RL algorithm that has sample complexity that depends on this measure. It is not strictly necessary, but I could see multiple uses of \"rank\" appearing in the RL literature as a means of exploiting structure for faster learning being confusing. If space (perhaps in the appendix if not), a sentence or two differentiating the two ideas might be helpful to readers. Additionally, the study of sparsity in value function representation was studied by Calandriello et al. 2014. If space permits, the paper might benefit from some discussion of the relation to this work.\n\nQuestions:\n\tQ1: In the inverted pendulum results, I am curious about the effect of the discretization on plan quality. Specifically: how were 2500 states and 1000 actions chosen? Were different orders of magnitude (for both values) considered? How did this impact SVP? Does the rank change as the discretization becomes more or less coarse? I don't think this is strictly critical for the paper, but a few sentences clarifying this point would be informative.\n\n\tQ2: Figure 4 provides nice insights into how to scale these ideas to deep RL. How were the four games chosen? Is there anything special that motivated their selection?\n\n\tQ3: Additionally, I am curious about whether the results from Figure 4 are the consequence of algorithmic decisions, rather than the environment. Is it possible to determine whether different value based methods (or different choices of hyperparameters) lead to different outcomes? For instance, I could imagine a more shallow network, or a tighter bottleneck, leading to Q evaluations that produce higher rank. \n\n\nTypos and Writing Suggestions:\n\n[Abstract]\n\t- This sentence is quite long, and I had a hard time following it as a result: \"As our key contribution, by leveraging...\". Consider dividing into two sentences.\n\n[Intro]\n\t- Oxford comma: \"control, planning and reinforcement learning\"::\"control, planning, and reinforcement learning\n\t- \"the structured dynamic\"::\"the structure in the dynamics\"\n\t- \"where much fewer samples\"::\"where fewer samples\"\n\t- Consider rewording: \"almost the same policy as the optimal one\". Is it that the policies are in fact the same? Or that their values are close? Perhaps: \"a policy with near optimal value\".\n\t- When introducing Double DQN and Dueling DQN for the first time it would be appropriate to cite each (end of Section 1).\n\n[Sec. 2: Warm Up]\n\t- \"understand the structures\"::\"understand the structure\"\n\t- \"give a strong evidence for\"::\"provide evidence that\"\n\t- \"exploit the structures for\"::\"exploit structure in the value function for\"\n\t- I think the italicized statement at the top of page 3 could be sharpened. The antecedent currently stating \"why not\" is quite a soft statement compared to the motivation the section develops. Consider changing: \"...why not enforcing such a structure throughout the iterations?\"::\"...then enforcing such a structure throughout planning can improve the rate of convergence\".\n\n[Sec. 3: Structured ... Planning]\n\t- \"even non-convex optimization approaches (...\"::\"even non-convex optimization approaches to solving this problem (...\"\n\t- \"offer a sounding foundation for future\"::\"offer a sound foundation for future\"\n\n[Sec. 4: Structured ... RL]\n\t- \"Previously, we start by\"::\"Previously, we started by\"\n\t- \"which in deep scenarios\"::\"which in scenarios with large state spaces\", or perhaps: \"which in deep scenarios\"::\"which in scenarios where value function approximation is used\"\n\n\nReferences:\n\nCalandriello, Daniele, Alessandro Lazaric, and Marcello Restelli. \"Sparse multi-task reinforcement learning.\" Advances in Neural Information Processing Systems. 2014.\n\nJiang, Nan, et al. \"Contextual decision processes with low Bellman rank are PAC-learnable.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n\nMcMahan, H. Brendan, Maxim Likhachev, and Geoffrey J. Gordon. \"Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees.\" Proceedings of the 22nd international conference on Machine learning. ACM, 2005.\n\n\nSmith, Trey, and Reid Simmons. \"Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic.\" AAAI. 2006.\n"
        }
    ]
}