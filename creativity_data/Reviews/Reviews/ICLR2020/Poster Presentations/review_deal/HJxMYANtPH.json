{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents a new phenomenon referred to as the \"local elasticity of neural networks\". The main argument is that the SGD update for nonlinear network at a local input x does not change the predictions at a different input x' (see Fig. 2). This is then connected to similarity using nearest-neighbor and kernel methods. An algorithm is also presented.\n\nThe reviewers find the paper intriguing and believe that this could be interesting for the community. After the rebuttal period, one of the reviewers increased their score.\n\nI do agree with the view of the reviewers, although I found that the paper's presentation can be improved. For example, Fig. 1 is not clear at all, and the related work section basically talks about many existing works but does not discuss why they are related to this work and how this work add value to this existing works. I found Fig. 2 very clear and informative. I hope that the authors could further improve the presentation. This should help in improving the impact of the paper.\n\nWith the reviewers score, I recommend to accept this paper, and encourage the authors to improve the presentation of the paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies an interesting phenomenon in neural network models that the classifier's prediction at a one input will not be significantly perturbed after the classifier is updated via sgd at another input that is dissimilar from the former one. This phenomenon is termed as the local elasticity, which provides another perspective seeking to interpret the neural networks. They present that this local elasticity characteristic does not hold for linear models. To further investigate this property, the paper introduces the relative similarity and kernelized similarity based on which a k-means like clustering algorithm is developed to further find fine-grained clusters within a coarse-grained category, like dogs and cats from the mammal category. Interpreting neural networks is a hot research topic, and a paper advancing knowledge in this area is certainly welcome. The paper is well presented (with a small typo in the definition of S_ker(x,x)). In the experiments, it will be interesting to further investigate how the local elastic property changes with large batch size in that large batch size may encourage more diversity of the examples in a batch. Furthermore, it will be even more interesting to explore how these similarities can improve the performance of a simple k-nearest neighbor classifier."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\n[Summary]\nThis paper proposes and studies the “local elasticity”, a quantitative measure for the ability of neural networks to only locally change its prediction (around x) after a stochastic gradient step at x. The paper verifies experimentally that nonlinear neural nets are locally elastic through showing that an elasticity-motivated similarity score can perform clustering well.\n\n[Pros]\nThe notion of local elasticity is interesting and has the potential of opening up lots of further directions. The way I understand it is to relate to memorization (as the authors have indeed discussed) --- I think “local elasticity” can be viewed as some sort of “local memorization ability”, in that the NN is able to change its prediction only in a small neighborhood of x---without affecting predictions at other remote x’s---after one SGD step on x. Conceptually this is something not covered by the existing narratives in deep learning theory, yet the phenomenon itself is quite convincing and could provide a new perspective into lots of things.\n\n[Cons]\nIt feels like the experimental results in the present paper is a rather indirect evidence for the local elasticity -- that the similarity score coming from the elasticity works well for a downstream clustering task. Could there be some more direct evidence about the local elasticity? How would the elasticity compare on different architectures? In the present form the experiments perhaps at most says that the similarity score makes sense, not yet that a fully quantitative characterization of the local elasticity.\n\nI’m also a little bit concerned about the fairness of the clustering experiment, in that the elasticity-motivated clustering algorithm utilizes an auxiliary dataset whereas simple baselines such as K-means and PCA K-means are not able to use that. Is there a way of modifying the K-means and PCA K-means so that they can also use this auxiliary dataset while still giving a sensible algorithm for the primary 2-class clustering task?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "The paper contributes to the understanding of neural networks and provides a new clustering technique:\n  1) The paper introduces the interesting notion of local elasticity which considers the relative variation in output values for two different inputs before and after an SGD-style update;\n  2) The derived similarity metric between input samples (obtained by as SGD unfolds) can be used for clustering and is amenable to a kernelized formulation;\n  3) Empirical measurements on visual classification tasks show that the new similarity metric can offer a better clustering performance than PCA + K-means.\n\nI found the paper very interesting but the writing appeared somewhat unclear at times. I believe that some rewriting is needed for the authors to argue that the newly introduced elasticity metric provides a significantly new understanding of neural networks. In particular, I did not find the argumentation around explaining generalization to be very convincing or clear.\n\nThe kernelized formulation of the elasticity metric seems compelling and I found that turning the insights developed by the theoretical section of the paper into an actionable algorithm for clustering was a nice contribution.\n\nUnfortunately, the empirical results did not really convince me that the resulting clustering algorithm really improves on the SOTA in clustering as only relatively weak baselines were considered.\n\nI believe that considering more solid baselines for the clustering experiments would help.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}