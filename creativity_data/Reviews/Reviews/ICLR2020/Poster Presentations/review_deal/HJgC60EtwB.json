{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The authors provide a framework for improving robustness (if the model of the dynamics is perturbed) into the RL methods, and provide nice experimental results, especially in the updated version. I am happy to see that the discussion for this paper went in a totally positive and constructive way which lead to a) constructive criticism of the reviewers b) significant changes in the paper c) corresponding better scores by the reviewer. Good work and obvious accept.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "\n\nRebutal Response: \nI keep my rating of weak accept due to the extensive empirical evaluation. However, I want to point out that the Area chair should assign more weight to the other reviews as the other reviewers have provided more extensive reviews and have more knowledge about the existing literature.\n\n\n###########\nSummary: \nThe paper proposes a robust variant of MPO and evaluates the performance on control tasks including the cartpole, hopper, walker & the shadow hand. The performance shows that the robust MPO outperforms vanilla MPO and MPO + Domain randomization. \n\nOpen Questions:\n- I am not convinced by the domain randomization performance as the performance increases only marginally over the vanilla MPO and DR has usually performed quite well for robust policies. Can you explain this marginal increase? \n\n- Could the authors please include a qualitative evaluation of the learned controllers? Especially as the Cartpole stabilization  is a linear system one could compare against the analyitc optimal controllers. \n\nConclusion: \nCurrently, I would rate the paper as weak accept, as the derivation is interesting and the approach seems to work quite well in  simulation. However, the work is only incremental and does not propose a new perspective to robust RL. It would be nice to show that the policy is also robust for a physical system. \n\nMinor Comments\n- What does the bar mean, e.g., \\bar{J} & \\bar{\\pi} mean? I cannot find a definition of the bar.\n- Typo in the Background section 'Kullback-Liebler  (KL)' \n\n(I can try to extend my review if the other reviewers disagree)",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "\n# Summary\nThe paper compares 3 ways to account for the variability of the dynamics model in the TD error computation:\n(i) be robust (take the lowest target value obtained when evaluating the value-function on the training distribution of models);\n(ii) be Bayesian (average over models);\n(iii) domain randomization (compute TD error as usual but randomize the dynamics to obtain a more diverse data set).\n\nThe comparison is carried out using the MPO algorithm. An entropy term is furthermore added to the MPO objective to yield a new E-MPO algorithm, but it didn't affect the performance much (cf. Figs. 5-6 and 7-8).\n\nThe paper is mainly an empirical study.\n\n# Decision\nThe engineering effort of running many experiments and ablation studies is appreciated. However, there are big questions to the evaluation methodology and the contribution of the paper, that preclude me from recommending it for publication in its current form.\n\n# Concerns\n1) First, my concern is the problem setting and evaluation methodology, and in particular Table 3 with varied parameters. The authors chose to have 3 values during training, and then evaluate the optimized policy on further 3 testing environments. There are many questions to this table. I give only a few.\n        - For most environments, training and test ranges are disjoint (e.g., train on 1.00–1.05m and test on 1.15–1.25). Why? Would your results hold if they are not disjoint?\n        - Fig. 14 shows that when trained on a single model with parameters closest to the evaluation range (right column), there is no difference between the algorithms on the Cartpole and a minor difference on the Pendulum. That seems to imply that it is sufficient to just train on one environment which is most similar to the evaluation environments to perform well in the experiments. Is it true? Do you have a counterexample?\n        - Natural question, of course, what do you propose to do when varying multiple parameters? Discretizing over several values will quickly yield exponential explosion.\n        - Did you try using distributions over ranges instead of fixed parameters? That would be a bit more solid than hand-picked values.\n\n2) Evaluation of domain randomization. The authors say that \"TD errors are then averaged together\" when doing domain randomization. That means, when updating the value function, gradients corresponding to various models get intermixed. It is usually better to fix the domain, then do a few gradient updates using the data from that domain, and after that collect data from another domain. Such procedure is similar to keeping a target network in Q-learning, which is absolutely necessary to stabilize the learning process. It would be interesting to see if domain randomization still performs poorly when applied in this way.\n\n3) It would be helpful to sharpen the main message.\n        - Right now there is the E-MPO introduced, but then it is not really compared to plain MPO (only plots are shown in the Appendix); it is only said that it doesn't hurt to have the entropy term. It seems orthogonal to the robustification scheme in the TD error, so maybe it would make sense to omit the entropy for clarity of exposition.\n        - What is the conclusion? The fact that robust works better than non-robust is obvious even without any experiments. So, the only question one could have is whether it is better to average or to take the worst-case model. I didn't get it from the paper whether there is any conclusion regarding that.\n        - In Conclusion, the authors appeal to some theoretical contribution of this paper, such as proof of contraction in Theorem 1. I would consider it a very minor contribution as it is mainly a recapitulation of known results as both the robust Bellman operator and the entropy-regularized one are known to be contractions. Furthermore, this theorem is not really used in the paper, so to me it seems detached.\n\nSmall: typo in Sec. 7.: you say \"to lengths of 2.0, 2.1 and 2.2\", but according to Table 3, these should be \"2.0, 2.2 and 2.3\"\n\n# AFTER REBUTTAL\nThe authors incorporated the feedback of the reviewers and *significantly* rewrote the paper. Now it is more focused and puts the work in context by discussing connections to Grau-Moya etc. I still think the contribution is minor and the method will not scale, but from the formal side, the paper fulfills the requirements to be accepted.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "AFTER REBUTTAL:\nThe authors answered all my questions and (significantly) modified the paper according to all the comments from all reviewers. The paper has significant value given the extensive experimental results and I believe could provide insight to other researchers in the field. I still have (like other reviewers) some concerns about scalability regarding the space of possible perturbations, but, as it is, the paper is worth for publication. For these reasons, *I increase my score to 7*, since I believe it's better than a weak accept (ignore the system rating, since it doesn't allow to put 7). \n\n======================================================\nSummary:\n\nThe authors provide a framework to incorporate robustness against perturbed dynamics to RL agents. They build upon the MPO algorithm and incorporate a few modifications that enable such robustness and entropy regularization. They show in multiple experiments that the robust versions of MPO outperform the non-robust ones when testing on environments with novel perturbations.\n\n\nDecision:\n\nI am happy with the experiments conducted on this paper and I think they would be a nice contribution to the community. However, I have serious concerns about the novelty of the proposed approach. The authors claim many novel contributions, but most of them are special cases of existing literature (not cited), or are simple modifications to existing approaches. For this reason my score is a reject. If the authors clearly cite the existing literature, state their contributions accordingly, and redirect the paper more towards their good experimental results (which they have plenty) I would be willing to substantially upgrade my score. Find below my supporting arguments.\n\n\nAbout novelty\n=============\n\n\nThe authors claim to have a novel framework and novel MDP formulations  for robust MDPs disregarding a substantial volume of the literature on robustness in the context of planning, control, reinforcement learning and MDPs. Since the authors really consider two types of robustness (robustness to dynamics and robustness to action stochasticity) I will expose in the following the relevant literature that is missing on both types, and when combining both.\n\n\n\nRobustness to Dynamics:\n\nBart van den Broek, Wim Wiegerinck, and Hilbert J. Kappen.  Risk sensitive path integral control. In UAI, 2010.\n\nArnab Nilim and Laurent El Ghaoui.   Robust control of markov decision processes with uncertain transition matrices.Operations Research, 53(5):780–798, 2005\n\nWolfram Wiesemann, Daniel Kuhn, and Berc  Rustem.  Robust markov decision processes. Mathematics of Operations Research, 38(1):153–183, 2013.\n\nLars Peter Hansen and Thomas J Sargent.Robustness. Princeton university press, 2008\n\nYun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement learning. Neural computation, 26(7):1298–1328, 2014.\n\nYinlam  Chow,  Aviv  Tamar,  Shie  Mannor,  and  Marco  Pavone.   Risk-sensitive  and  robust decision-making: a cvar optimization approach.   In Advances in Neural Information Pro-cessing Systems, pages 1522–1530, 2015.\n\n\n\nRobustness to Action Stochasticity:\n\nRoy Fox, Ari Pakman, and Naftali Tishby.  G-learning: Taming the noise in reinforcement learning via soft updates.arXiv preprint arXiv:1512.08562, 2015.\n\nJonathan Rubin, Ohad Shamir, and Naftali Tishby.  Trading value and information in mdps.In Decision Making with Imperfect Decision Makers, pages 57–74. Springer, 2012.\n\nDaniel A Braun, Pedro A Ortega, Evangelos Theodorou, and Stefan Schaal.  Path integral control  and bounded  rationality.   In Adaptive  Dynamic  Programming  And  ReinforcementLearning (ADPRL), 2011 IEEE Symposium on, pages 202–209. IEEE, 2011.\n\n\n\nCombination of both:\n\n[1] Grau-Moya, Jordi, et al. \"Planning with information-processing constraints and model uncertainty in Markov decision processes.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2016.\n\n\nImportantly, in [1] above, it is shown a very similar line of research as the authors are proposing. In [1] it is combined entropy regularization with robustness in dynamics. In particular, using the formulation in [1] one can see how the formulations in the present paper can be recovered by setting the parameters in various ways. For example,\n1- letting the entropy regularization coefficient ($\\alpha$ in [1]) to a finite value and the robustness coefficient ($\\beta$ in [1]) to $-\\infty$ one recovers the traditional Robust MDP formulation.\n2- Selecting a uniform prior over discrete theta (where each component in theta would correspond to a particular perturbation)  and setting $\\beta \\rightarrow 0$ one can also recover Soft-RE formulation (although a non-MPO version of it, i.e. just the MDP formulation).\n3- Similarly, one can get rid of the entropy regularization by setting $\\alpha \\rightarrow \\infty$.\n\nNote that in [1] the uncertainty set is determined by a KL constraint whereas here it is determined by the chosen perturbations. However, in [1] setting the proper prior and $\\beta$, one can obtain the same behaviour.\n\n\nImportantly, in all previous references multiple contraction proofs (for entropy regularization Bellman operators, for robust operators alone, and for robust and entropy regularized operators) have already been discovered. \n\nI hope all of the previous points can convince the authors about my decision above regarding novelty. Note, please, that I am aware that a robust version of MPO is slightly novel, however, a robust MDP formulation is more general than that and it could have perfectly been applied to any other RL optimization algorithm. Therefore, I don't think adding robustness to MPO alone is sufficient for enough novelty (other wise we would publish 1 paper per method converting one by one all known algorithms into their robust version of it). \n\n\nAbout scalability\n=================\n\nI liked the experimental section. However, I have a question about practicality of the approach. As the authors show in the paper, the proposed method scales well in terms of  high-dimensional state and action spaces. But what about the scalability in terms of possible perturbations? I can imagine a plausible situation where we are dealing with a high-dimensional robot, let's say a humanoid robot, which allows for many possible \"perturbations\" e.g. longer left leg, longer right leg, longer fingers, shorter thumbs, and so on. The space of possible perturbations can be potentially very big. Could the authors elaborate on scalability issues of this type? Have they already thought about how one would solve this? I imagine that perturbing, let's say in one dimension, might change the dynamics quite in a different way compared to perturbing in another dimension. It would be interesting to see how their method generalizes when testing on unseen \"dimension\" perturbations.\n\nAlthough this might present problems in terms of generalization, I understand that it does not invalidate the results presented here.\n\n\nGood experimental results:\n\nI have carefully checked all experimental results and I must say that they are very well executed e.g. multiple ablation studies and sufficient environments to support their approach.  Maybe a small comment: In figures 6 and 8 in the appendix there are multiple plots that only differ in the data (the titles are the same, e.g. two hopper figures with the same title). Maybe the authors can correct for this.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}