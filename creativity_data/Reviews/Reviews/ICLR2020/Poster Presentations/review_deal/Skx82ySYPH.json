{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a solid (if somewhat incremental) improvement on an interesting and well-studied problem. I suggest accepting it.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The following work proposes several improvements over prior works in unsupervised/self-supervised keypoint-descriptor learning such as Christiansen et al. One improvement is the relaxation of the cell-boundaries for keypoint prediction -- specifically allowing keypoints anchored at the cell's center to be offset into neighboring cells. Another change was the introduction of an inlier-outlier classifier network to be used as a proxy loss for the keypoint position and descriptors. They found the inlier-outlier loss to improve homography accuracy at 1 and 3 pixel thresholds.\n\nStrengths:\n-The ablation study seems complete\n-Clear improvements over state of the art methods\n\nWeaknesses/improvements:\n-The description of the evaluation procedure was a bit vague. Is RANSAC being used to find correspondences? If so, perhaps error bars are necessary to account for variance across multiple runs?\n-Make it more clear in the related works about how the proposed method relates to Unsuperpoint. My understanding is that the proposed work is a somewhat incremental improvement over Unsuperpoint.\n-Section 3.3 (Score learning) was a bit difficult to follow. I find it better to start by stating the high level goal of the loss function before going into the formulation.\n-Captions for Tables 2 and 3 are lacking. At the very least, mention what the numbers being compared are.\n\nOverall, I think the improvements are a bit incremental, but the experiments seem to support the claim that they are beneficial. I had some concerns about the clarity of the paper, and would be willing to raise my rating if addressed.\n\nPost Rebuttal:\nThe authors have adequately addressed my concerns regarding clarity. I have updated my rating to weak accept in agreement with the other reviews.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "#Â UPDATE following rebuttal\n\nScore increased to 6 due to architecture details in supplemental.\n\n\n# Contributions\n\nThe paper contributes a self-supervised method of jointly learning 2D keypoint locations, descriptors, and scores given an input RGB image. The paper builds on previous work, adding:\n\n* A more expressive keypoint location regression, which allows each 8x8 pixel region to vote for a keypoint location outside its boundary\n\n* An upsampling step, similar to a U-net, to allow descriptors to be regressed with more detailed information\n\n* An additional proxy task for the total loss, based on outlier rejection.\n\nThe authors train on COCO by manually distorting images to generate pairs with known homography, and show competitive results for keypoint detection and homography estimation tasks.\n\nDecision: Weak reject. I would give this a 5 if the website allowed me to. A more detailed explanation of the neural network architecture, along with minor fixes described below, would make me increase my rating.\n\n\nI feel the additions to existing pipelines are well motivated but insufficiently explained. In particular, the explanation of the neural network architecture along with figures 1 and 2 leaves many details unclear to me. Phrases like \"a 1D CNN ... with 4 default setting residual blocks\" is to me insufficient - residual networks have many details such as Resnet V1 or V2 style (ie is there a path right through the network which doesn't hit any activation functions), what kind of normalization is applied, number of channels in each block, how to do skips between different spatial resolutions, etc. The upsampling step for the descriptor head, which is claimed as a novel contribution, is not fully explained - \"fast upsampling\" implies (correctly) there are many variants of upsampling with different tradeoffs, but from the text I am unsure whether this is nearest neighbour upsampling, a ConvTranspose, etc. Similarly, \"VGG style block\" leaves some details unclear - whether the resolution downsampling is with a strided convolution / pooling / etc. Lots of the details are implied to be in other previous papers, but I feel that the paper would be hugely improved by exact architectural details.\n\nThere are various minor notational discrepancies in the paper - for example the outlier rejection is various defined as \"InlierOuterNet (IONet)\" and \"The Inlier-Outlier model \\emph{IO}\", which also seems to be the same as the function $C$ defined a paragraph above. Perhaps it is common in this part of the literature, but to me an encoder decoder network is more likely to either be an autoencoder, or for the decoder to output something in the same modality (eg in machine translation). To say that some VGG blocks are an encoder, and the heads which produce keypoint locations / score / descriptor is a decoder, implies all neural networks could be described as an encoder/decoder.\n\nThe two figures showing the architecture are very different in design, which is not in itself a problem but the relationship between them could be clearer. I feel that the 'matching' box in figure 1 is misleading because it implies that matching only happens for the IONet, but the loss function for location described in Eq 1 also requires matching keypoints between the image pair. I'm also unclear on the division between direct and indirect supervisory signal - all the 4 loss components have a clear purpose, but it's not obvious what this partitioning means. \"Indirect\" only appears in this figure and the caption - perhaps.\n\nThe term \"Anchor\" appears only once with no reference, below equation 3 - I appreciate this is an existing term in this subfield, but given that the start of section 3 goes as far as explicitly defining what it means to produce 2D keypoints for an image, I feel defining this term would make the descriptor loss much clearer. \n\nOne of the main contributions, that of allowing locations to regress outside their 8x8 area, sounds like a good idea but I feel that Figure 3 does not adequately show the benefit. In both a) and b), the blue estimates appear to be roughly as good as each other - clearly from the ablation a large benefit is gained from this innovation but perhaps a better illustrative example could be made here?\n\nOn a more positive note, I feel the components of the loss function are in general very clearly motivated and defined, and the description of training & data augmentation hyperparameters appears complete. If the description of the architecture could be improved that would result in a paper very amenable to reproduction.\n\nThe experiments are well explained, and the ablation of the various proposed  components is good. I feel table 1 would be improved with error bars - given that the bold best score is not exclusively next to V4, but in many cases the difference between V4 and the best is ~1%, error bars from different training runs might make clearer that V4 is overall the best configuration.\n\nIn the conclusion - \"even without an explicit loss\" - what is the difference between the loss functions used in this work, and an explicit loss?\n\n\nMinor corrections:\n\nThe euclidean distance between descriptors is various notated as $d$ (section 3), $x$ (above equation 5) and $d$ again (below equation 5).\n\nTypos: \"normalzation\" -> \"normalization\", \"funcion\" -> \"function\", \"tripled\" -> \"triplet\".\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The paper is devoted to self-supervised learning of local features (both detectors and descriptors simultaneously). The problem is old yet not fully solved yet, because handcrafted SIFT is still winning the benchmarks. This work mostly follows and improves upon SuperPoint (DeTone et.al 2017) and the follow-up work UnsuperPoint (Christiansen et.al 2019) architecture and training scheme.\n\nThe claimed contributions are following:\n  - use the recently published Neural Guided RANSAC as additional auxilary loss provider\n  - allowing the \"cells\" to predict keypoint location outside the cell while learning\n  - special procedure for improving descriptors interpolation\n\n\nThe experiments are performed on HSequences dataset (wrongly called \"HPatches\", as HPatches dataset is literally image patches, not full images), showing noticable improvement over the state of the art.\n\n\nStrong points:\n\n - Method is sound, paper is mostly well written and results are good (may be too good, see questions).\n \n \n \n Questions:\n \n  1) Regarding descriptor interpolation, which is claimed as contribution. It is not clear for me, how different it is compared to SuperPoint one, which also do descriptor upsampling, so that network output is H x W x [256], i.e. full resolution. Could you please clarify the differences to it? Also, in Figure 2 it is not clear, how one can do \"feature concatenation\" for blocks with different spatial resolution.\n\n  2) Why the IONet is used only for training?  Wouldn`t it better to actually learn everything end-to-end, which is already done in paper and evaluate? \n  \n  3) How is association in training (e.g. on Fig.3) done, if multiple cells in img2 returns keypoint close to the same keypoint in img1? \n\n  4) HSequences consists of two subsets: Illumination and Viewpoint. Could you please report results per subset instead of per whole dataset? Could you also please specifically report results for the following image sequences: graffity, bark, boat, especially for 1-6 pairs and visualize matches (same way as in Figure 4-6)?\n  The reason that I am asking these, is results looks like too well and I suspect overfitting to a points, which are suitable for estimation (small) homography, not general-purpose points.\n  \n  5) Could you please explain in more details, how did you do homography estimation precision benchmark? Specifically, was Lowe`s second nearest neighbor ratio used for filtering out wrong matches? If not, could you please repeat this experiments with it, at least for SIFT matches?\n  \n  \nSmall comments:\n    - list of contributions in abstract is inconsistent with 3rd paragraph in Introduction, which also lists contributions.\n    \n\n***\nOverall I like the work, but there are unclear moments to me. \n\n\n****\nAfter rebuttal comments. While this paper may appear not \"sexy\" I think it is quite valuable for the local features learning community: both for the main contributions, and small details and tricks evaluated inside. \nI am happy to increase my score to strong accept.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}