{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes an approach to representing a symbolic knowledge base as a sparse matrix, which enables the use of  differentiable neural modules for inference. This approach scales to large knowledge bases and is demonstrated on several tasks.   \n\nPost-discussion and rebuttal, all three reviewers are in agreement that this is an interesting and useful paper. There was intiially some concern about clarity and polish, but these have been resolved upon rebuttal and discussion. Therefore I recommend acceptance. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an efficient sparse-matrix based representation for symbolic knowledge bases. This representation enables fully differentiable neural modules to model multi-hop inferences, which is designed to be scalable to handle realistically large knowledge bases. Experiments using the proposed method with end-to-end architectures on downstream KB tasks demonstrate its effectiveness and efficiency. Overall, this paper makes clear contributions and can inspire other researchers in the community to apply this KB representation for various learning / reasoning tasks on knowledge bases. However, considering the readability, I would like to recommend a weak accept for this paper.\n\nAdvantages of this paper: 1) The proposed method employs three sparse matrices to represent all KB relations, which is more efficient than existing work such as TensorLog and can support relation sets; 2) The reified KB representation is scalable, which can be distributed across multiple GPUs, making it much faster than the naive implementation; 3) The proposed method can be naturally used in end-to-end neural models and efficiently trained with gradient-based approaches.\n\nDisadvantages of this paper: 1) In the introduction part, the authors mention that existing neural KB reasoning methods generally require some non-differentiable mechanism to retrieve small question-dependent subset of the KB, but there exist some existing methods such as memory networks that are fully differentiable for the KBQA task. It would be better to cover and discuss more existing methods when summarizing their properties; 2) In the methodology part (Section 2), the description is clear but lacks some guidance for the readers to understand the motivation behind the scene. For example, it would be helpful to discuss why representing relations as sparse matrices is necessary, whether there is any other choices, and the benefit of making the current choice. This might seem obvious for the authors, but can help readers that are not familiar with the context access the methodology more easily and understand the motivation better."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "The paper proposes sparse-matrix KB representation for end-to-end KB reasoning tasks. They demonstrate that their algorithm is scalable to large knowledge graphs which is the central contribution of the paper.\nThey apply this to a bunch of tasks such as KB Completion and KBQA. This is done by mapping the query to a set (weights) of relations over which reasoning is performed. \n \nI would first like to comment that I found the paper very hard to read. Thus due to my difficulty in understanding the paper, it is possible that I might have misunderstood parts of the paper. \n \nThe notations are overly complex. In my opinion, the notations can be simplified to a considerable extent. I would suggest a Table of notations or a small figure explaining the model. The paper, in my view, requires  considerable rewriting.\n \nThe paper states that the proposed approach encodes three floating point values and 6 integers for each triple. Is this true for KBC task? Because I am quite surprised that ReifKB approaches SOTA which use hundreds of floats for representing each entity and relation (e.g. DistMult/ComplEx).  \n \nHere are somethings which are not very clear to me KBQA tasks: It is not fully clear how the query is mapped to r. In my understanding r is a set of relations. Is the output of linear function taken as weights? \nKBC Task: How do you ensure that the N chains are distinct? Do you have N different linear functions (f_i)? Also why is x_i^t added to follow in KBC task?\n \nThe paper needs considerable rewriting and therefore I cannot recommend this paper for acceptance at this stage.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #1",
            "review": "The paper provides a way to represent symbolic KBs called sparse matrix reified. Relations and entities' types are modelled using sparse matrices. This modelization allows distributing the computation on many GPUs. A neural model is used to manage these matrices. The trained model can be used to perform multi-hop queries. The proposed system has been compared with related systems. The results show that it achieves hits@1 that are comparable with the others.\n\nThe proposed approach seems promising, however, I feel that the paper is not ready for publication. The experiments lack a scalability test with related systems, the scalability test included in the paper only takes into account other definitions of the system. Also, comparison on the run time should be performed to see if the lower performance in terms of hits@1 (which are good anyway) is balanced by a better run time.\n\nTherefore, it is difficult to see whether the proposed system is good or not.\n\nAs for the description of the system, it seems to me to be quite foggy. In my opinion, the neural model should be better described to show how the sparse matrices are mapped into the model.\nAlso, how are the training and testing sentences created? How would the output of a query look?\n\nAfter reading the paper I have the feeling that it was written in a bit of a hurry, without working on the details. There are several typos, parentheses not correctly opened or closed, out of place commas that make some sentences seem unrelated to the rest of the paragraph. However, here last problems are easily fixable.\n\nTo sum up, I am conflicted about the choice of the final score. On the one hand, the approach is interesting, on the other hand the article seems to me not mature enough and strong enough from the point of view of organization, contents and experimental results shown.\n\nMinor problems\nOn page 4, the size of the matrices XM_k uses the factor b that is introduced later.\nIn the introduction, next to footnote 1, I suggest specifying that A is the first query, the one about Tarantino's movies.\nAlso, his surname is misspelt throughout the paper. The correct one is Tarantino.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}