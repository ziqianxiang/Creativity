{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over-parametrized settings. Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood. This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors present a new first order optimization method that adds a corrective term to Nesterov SGD. They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. In the full-batch (non deterministic) setting, their algorithm boils down to the classical formulation of Nesterov GD. Their approach is justified by a well conducted theoretical analysis and some empirical work on toy datasets. \n\nPositive points:\n- The approach is elegant and thoroughly justified. The convergence to Nesterov GD when the batch size increase is comforting.\n- The empirical evaluation, even if it is still preliminary and larger scale experiments will have to be conducted before the method could be widely adopted, are suitable and convincing.\n- Some interesting observations regarding the convergence regimes (in respect to the batch size) are made. It would have been interesting to see how the results from fig3 generalize to the non convex problems considered in the paper.\n\nPossible improvements:\n- In H2, it is mentioned that the algorithm is restarted (the momentum is reset) when the learning rate is annealed. Was this also done for SGD+nesterov? Also, I think it is an important implementation detail that should be mentioned outside of the appendix\n- Adam didn’t get the same hyper-parameter tuning as MaSS did. It is a bit disappointing, as I think the superior performance (in generalization) of non-adaptive methods would still hold and the experiment would have been more convincing. Rate of convergence is also not reported for Adam in fig 5.\n\nI think this is definitely a good paper that should be accepted. I’m looking forward to see how it performs on non-toy models and if the community adopt it.\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper shows the non-acceleration of Nesterov SGD theoretically with a component decoupled model. Moreover, the authors introduce an additional compensation term and derive a novel optimization method, MaSS. MaSS is both theoretically and empirically proved to outperform Nesterov SGD as well as SGD.\n\nPros\n1. It's amazing to see the great improvement introduced by the compensation term into the theoretical result of MaSS. Moreover, the authors generalize the setting of square loss function to other convex loss functions.\n2. The encouraging result in Table 1 in EMPIRICAL EVALUATION shows the consistent outperformance of MaSS over SGD and Nesterov SGD regardless of the changing learning rates.\n\nCons\n1. The discussion on why the zero eignvalue can be ignored in Section 4 is insufficient. \"(stochastic) gradients are always perpendicular to W^*\" seems not that obvious.\n2. The empirical result merely involves two settings of learning rate: 0.01, 0.3. I suggest a wider range of learning rates to show the outperformance of MaSS.\n\nSome typos\nLast line of the first paragraph in INTRODUCTION: there is a redundant \"can\". 7th line of the 5th paragraph in INTRODUCTION: there is a reduntant \"the\" after \"In this case\".\n    "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "1) In the statement of theorem 1, what do you mean by “with probability one” and “convergence in expectation” together? The inequality (7) does not have any random variable anymore after taking the expectation. Need to explain more clearly this part. \n\n2) Basically, all the results of this paper is based on the (or close to) strongly convex property. However, numerical experiments show for some non-convex functions, specifically for deep learning problems. It is unclear what kind of loss function the author(s) are using for training classification problems on MNIST and CIFAR-10. This could be softmax cross-entropy but not quadratic. \n\n3) The theoretical results in this paper are not strong. The interpolation setting could make all solutions of the component function are the solution of the total loss function. In this situation, we know that stochastic algorithms could take advantage because of “automatic variance reduction”. \n\n4) The result in this paper is quite incremental from the one in Vaswani et al 2019, “Fast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron)”. More discussion is needed if the author(s) think that it has significantly improvement. \n\n5) It is true that Nesterov SGD is very efficient for training neural networks and MaSS may have some effect in practice. However, theoretical part needs to be improve. I would suggest to analyze for nonconvex problems or using the assumptions which are verifiable and reasonable for neural network. \n\nThe work may be potential, but in order to convince people to trust this algorithm, rigorous theory must be provided. Some experiments in the paper are not representing all scenarios that MaSS may not work. \n"
        }
    ]
}