{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "All reviewers voted to accept this paper.\nThe AC recommends acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper describes an approach to improve the confidence on deep\nneural networks (DNN). The proposed approach uses a distance-based\napproach (distance to prototypes) and train using a confidence\nmodel with the classification model using mis-classified examples.\n\nIt first learns using a loss function based on pre-computed centroids\nof each class evaluated from the training examples. At inference it\nassigns the class with the closest center.\n\nTo estimate a confidence level, it learns a new model to estimate the\ndistance using only the misclassified examples.\n\nThe authors experimentally showed the benefits of the proposed approach\nover several methods.\n\nFinding prototypes and training with distances, on one hand, and\nevaluating confidence with Gaussian, on the other, assumes\nthat the classes are \"well defined\". Although the authors show\npromising results, it is not clear how well the proposed method will\nbehave in more challenging classification tasks where the classes are\nmixed.\n\nAlthough the authors show that estimating the confidence with\nmisclassified examples works better I will like to see a further\nanalysis in the paper.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper addresses the (long-standing) problem of classification systems being able to output reliable confidence estimates on its own output. The selected approach is to use the distance to (previously computed) class centers in multi-class classification to help compute the confidence interval. The method is shown to have comparable results on well known datasets but higher efficiency than 2 rival methods: ensemble ANNs and Bayesian neural networks. I would point out that such confidence intervals are all intuitive and have no statistical basis or other independent means of empirical validation. Experienced practitioners are aware of this, but I see that the paper steers wisely clear of overambitious claims. The general intuition of hybrid supervised and unsupervised learning for C.I. (or ellipse) estimation is not new, but an effective and compact representation of this intuition in a DNN context is a valuable contribution, well-placed in the literature - I recommend acceptance."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe paper proposes a method to do confidence calibration for deep neural networks. It uses standard\nepisodic training for prototypical networks, and first shows empirically that the distances of the\nembedded test point to its ground truth class center embedding (*not* the predicted class embedding)\nare indicative of the confidence of the prediction. Further it proposes to exploit this by training\nan auxiliary confidence prediction MLP carefully. To do so they demonstrate that the training needs\nto be done of erroneously predicted training examples cf. all the traning examples. They show\nresults with MLP+MNIST, VGG11+CIFAR10, ResNet50+CIFAR100 and ResNet50+TinyImageNet.\n\n\nDetailed comments:\nThe paper is interesting but largely empirical. It shows empirically that:\n1. when prototypical networks (and episodic training) is used the distance of test example to true\nclass center reflects the confidence.\n2. this does not hold when `vanilla' training is used\n3. an auxiliary MLP can be used to learn to predict this while training only with erroneously \nclassified training examples cf. all training examples\n                                      \nThe results are reported on three networks (MLP, VGG11 and ResNet50) on different benchmarks of \nimage classification. The confidence prediction improvements wrt baselines are non trivial, while \nkeeping the accuracy similar, and the computation cost lower than competing methods. Ablations \nstudies are also convincing.                    \n            \nI would have two broad critical comments on the paper:\n1. Would this generalize to other image classification tasks and datasets. Generally distance\n(embedding) based networks perform less than softmax based networks on bigger datasets, so an\nimmediate disadvantage if that happens, is that you would be trading off accuracy cf. vanilla\nnetworks, for better confidence prediction using the required distance based network here.\n2. The vanilla training is never formally detailed. I am assuming it was softmax + cross entropy loss \nwith gradient descent. Would some other loss be helpful? Specially the metric learning based losses like \ncontrastive or triplet losses come to mind, since they are also distance based. Does the method work\nwith prototypical networks only or it generalizes to other distance based methods as well?\n                                                                                    \nMinor comments:\nThe notations are a bit confusing sometimes, and require going back and forth a bit. Eg. \\mu is used\nfor representation of feature (Eq4) while it usually denotes a mean of some sort (so the reader's expectation\ncould be that it represents class center). Similarly, boldface p is used for class centers, which is\nagain a bit confusing as being a probability of some sort. In general the notations are different\nfrom the original prototypical networks paper (which I needed to revise); keeping them similar would\nhelp the reader. The contribution of the present paper is more than that anyway.\n\nThe erroneously classified training examples are called errors (eg. just before eq8). By errors one\ncould think that it is a difference between some sort of prediction and the ground truth. Explicitly\ncalling them erroneously classified training examples would help the reader as well.\n\nThe notation \\odot is not explained (before eq.9).\n"
        }
    ]
}