{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a training method for generative adversarial network that avoids solving a zero-sum game between the generator and the critic, hence leading to more stable optimization problems. It is similar to MMD-GAN, in which MMD is computed on a projected low-dim space, but the projection is trained to match the density ratio between the observed and the latent space.\nThe reviewers raised several questions. Most of them have been addressed after several rounds of discussions. Overall, they are all positive about this paper, so I recommend acceptance. I encourage the authors to incorporate those discussions in their revised paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper proposes GRAM-nets using MMD as the critic to train GANs. Similar to MMD-GAN, the MMD is computed on a projected lower dimensional space to prevent the kernel struggle in the high dimensional observed space. On the other hand, contrary to MMD-GAN, GRAM-nets trains the projection f_{\\theta} trying to matching the density ratio of p/q between the observed and the latent space. The proposed density ratio matching criterion avoids the adversarial training in MMD-GAN, thus can potentially fix the two-player optimization problem. The paper shows improved FID scores and nice-looking CIFAR10 generations. \n\nStrengths, \n1, Matching density ratios is a novel and interesting idea. If the data lives in a lower dimensional subspace, matching density ratio could probably reveal the subspace. Compared to adversarially training f_theta, the proposed approach could lead to more stable training potentially. \n2, By manipulating E (px/qx - pz/qz)^2, they avoid estimating the high-dimensional px/qx and only estimate pz/qz.\n\n\nWeakness,\n1, The paper needs to be more careful with mathematical expressions. 1) Eq(2) should be MMD^2, instead of MMD. 2) Eq(5) and Eq(6) are wrong, although the used Eq(7) becomes true again. In Eq(5), Eq(6), the integration should be over f(x) instead of x, that is to say $\\int ..... df(x)$.\n2, It is unclear why one needs the regularization in Eq(9). In fact, the major problem of the density ratio estimator lies in that r(x) might be negative, so a clipping might be useful.\n3, The major contribution of GRAM-nets lies in removing the adversarial training in MMD-GAN. Therefore, more empirical comparisons should be made with MMD-GAN. For example, how MMD-GAN evolves in Figure 2 is necessary.\n4, In real image generation tasks, it is beneficial to show the stability of training GRAM-nets, compared with GAN, MMD-GAN as well as WGAN-GP; And Inception scores should also be reported for better validating the effectiveness of the proposed method.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "In this paper, authors propose a new generative adversarial networks based on density-ratio estimation. More specifically, the squared-loss between the true ratio function and the model  (a.k.a., Pearson divergence) is employed to train model. Through experiments, authors demonstrate that the proposed density-ratio based approach compares favorably with existing algorithm.\n\nExperimentally, the proposed model performs pretty nicely. However, the derivation is not certain and some important density-ratio based GAN is missing.\n\nDetailed comments:\n1. Some important density-ratio based GAN methods are missing. For instance, https://arxiv.org/abs/1610.02920 https://arxiv.org/abs/1610.03483\n2. Some derivation of density-ratio estimation is not clear. To the best of my knowledge, the ratio model is obtained by minimizing the distance between true ratio function and its ratio model (Something similar to (4)). But, in this paper, the authors describe the ratio is obtained by using MMD density ratio estimator. Could you explain the derivation in detail?  \n3. I guess, (8) is derived in similar way of  http://www.jmlr.org/papers/volume10/kanamori09a/kanamori09a.pdf In this case, (8) is not correct.\n4. The ratio is defined by using kernel. Thus, I guess the kernel model is employed. However, this information is missing. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The proposed approach amounts to a non-adversarial way to update the parameters theta of a kernel K_theta(x, y) = k(f_theta(x), f_theta(y)) in an MMD-net, rather than adversarially as in MMD-GANs. Avoiding the saddle point problem clearly gives a more stable optimization problem, as shown in Section 3.4, which is very nice.\n\n\nTheory/motivation:\n\nThe motivation based on density ratio estimation, however, very much assumes that these densities exist. As convincingly argued by Arjovsky and Bottou (ICLR 17, \"Towards principled methods for training generative adversarial networks\"), the densities p_x, q_x of (4) typically do not exist (w.r.t. Lebesgue measure), and moreover the support of the distributions are typically almost disjoint. Algorithmically, this doesn't seem to be a major issue for your approach: the estimator (8) corresponds to some kind of smoothing by the kernel. But it would be nice from a motivation point of view to have a better understanding of:\n\n- What assumptions exactly does (8) make about the existence of densities? For example, in the case of a Gaussian kernel, does it correspond to the distance between distributions convolved with Gaussian noises with a smaller Gaussian kernel? Does the algorithm \"make sense\" when distributions don't exist?\n\n- Can we say something about when a density exists for the dimensionality-reduced data f_theta(x) even when it doesn't exist for the data x? \n\n- Relatedly, the dimension-reduced Pearson divergence is clearly somehow different from the plain Pearson divergence. Can we understand a little bit more how it's different? How does the choice of architecture of f_theta affect the distance metric?\n\n- Is it the case, as in Arjovsky et al. (2017) or in Arbel et al. (NeurIPS 2018, \"On gradient regularizers for MMD GANs\"), that the overall loss function with an optimal f_theta is continuous with respect to the GAN distribution?\n\nCertainly not all of these theoretical questions require answers for an initial ICLR paper, but they would be illuminating for understanding the method.\n\n\nMotivating experiments:\n\n- In Figure 2, it seems that except in the first image (h=2) the mapping f_theta becomes essentially the identity. Is this expected to be generally true in your 2d cases, because the density ratio can already be well-represented there? A more interesting example for this aspect of the model might involve higher-dimensional distributions with some low-dimensional manifold structure; would f_theta pick up some reasonable representation of the manifold in those cases?\n\n- Can you plot the density ratio estimate in a simple 2d example like this? How does it look visually? Is a good estimate necessary for good performance of your generative model?\n\n\nExperiments:\n\n- As shown by Binkowski et al., it's extremely important to specify the sample size for FID scores, which you don't do.\n\n- That said, the FID scores of the MMD GAN models of Binkowski et al using DCGAN discriminators (which replace the FSR and AE penalties of Li et al. with a single gradient penalty) found substantially smaller penalties for performance of MMD-GANs with using a smaller discriminator (1/4 as many hidden units rather than 1/2 as you use) than you did. They also reported substantially better numbers on CelebA than you did. If not too difficult code-wise, it might be worth trying either their code or that of the followup Arbel et al. (citation above) to compare to your approach.\n\n- The results of, for example, Mescheder et al. (ICML-18, \"Which Training Methods for GANs do actually Converge?\") and Arbel et al. (NeurIPS-18, \"On gradient regularizers for MMD GANs\") seem to be much better visually with comparably-sized models than the results of Figure 8 (and of course StyleGAN/etc are *far* better but with drastically larger models). It would be good to compare empirically to some more recent models than original MMD GANs (whose empirical results were substantially improved with only minor tweaks by Binkowski et al. mere months later) or DCGAN, which are fairly old approaches in this space.\n\n- Two more extremely relevant methods it would be worth comparing to or at least mentioning:\n\n   - Ravuri et al. (ICML 2018, \"Learning Implicit Generative Models with the Method of Learned Moments\") can be interpreted as learning f_theta for an MMD-net as you do by taking the gradients of an auxiliary classification network. Although it is still somewhat \"adversarial,\" they need to update the kernel function extremely rarely (every 2,000 generator update steps) and the optimization seems overall much more stable.\n\n   - dos Santos et al. (https://arxiv.org/abs/1904.02762 \"Learning Implicit Generative Models by Matching Perceptual Features\") effectively use an MMD-net (with some slight tweaks) with a *fixed* kernel. Though results aren't quite as good, it's worth at least mentioning.\n\n\nMinor typos/etc:\n\n- Top of page 3: it is sufficient to choose F a unit ball in an RKHS with a _characteristic_ kernel.\n- Page 4: \"this issue in next.\"\n- Figure 1 caption: (both) should probably be (bottom).\n\n\nOverall initial thoughts:\n\nThis seems like a nice alternative to adversarial methods, but it does not compare to more recent (last 2 years) models in this space or very thoroughly establish the applicability of its motivation. I think it's worthy of a weak accept as is, but could be much more convinced with some additional work."
        }
    ]
}