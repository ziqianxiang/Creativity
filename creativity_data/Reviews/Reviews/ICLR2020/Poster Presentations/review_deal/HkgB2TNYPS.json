{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The reviewers generally found the paper's contribution to be valuable and informative, and I believe that this paper should be accepted for publication and a poster presentation. I would strongly recommend to the authors to carefully read over the reviews and address any comments or concerns that were not yet addressed in the rebuttal.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "Update 11/21\nWith the additional experiments and text clarifications, I'm happy to raise my score to accept.\n\nSummary: This paper addresses the dependence of few-shot classification with Prototypical Networks on “shot”, or the number of examples given per class. Typically, performance suffers if the algorithm is tested on a task with different shot than it was trained on. The paper derives a bound for few-shot performance that depends on the shot. The bound is used to motivate an algorithm which maximizes the inter-intra class variance of the embedded samples. Experiments show that the proposed method generalizes better to different test-time shot, though not significantly better than simply training prototypical networks across different shot numbers.\n\nProblem importance: I think reducing the dependence of few-shot algorithms on rather arbitrary parameters like “shot” is an important and interesting problem. In realistic applications, it’s unlikely that the number of examples for each new class will be the same.\n\nComments:\n- I think this paper is a rather nice reminder not to forget our linear algebra while engaged in deep learning. \n- One main concern is that the method derives quite easily from intuition (low intra-class variance implies tight clusters, and high inter-class variance implies well-separated clusters), and right now I don’t feel that the derivation of the bound is adding much to the paper. Perhaps this bound could be investigated a bit more. For example, is there a way to characterize the tightness of this bound empirically? Perhaps with a linearly separable classification problem where we know R(\\phi) is 1? Alternatively, can the lower bound be directly optimized to find “optimal” intra and inter class variance? Would this be equivalent to the proposed method?\n- Another main concern is that the paper is narrowly focused on one few-shot method, ProtoNets, though it seems like the intuitions and the method could extend to other methods. Could you explain what other few-shot algorithms can use this transformation in addition to ProtoNets? \n- The discussion of VC dimension in Section 3.2 seems quite disconnected from the main idea of the paper, and I don’t see how the conclusion from the VC analysis is used in designing the proposed algorithm. \n- It might be interesting to see how this method fares compared to the “Mixed-k” baseline as the number of shots increase even more.\n- Could you discuss how the number of classes (“way”) might interact with this analysis of shot?\n\nWriting Suggestions\n- I believe Equation (2) is incorrect, it doesn’t make sense that p_\\phi = y since p_\\phi is a probability and y is the ground truth label. Perhaps the right parenthesis is in the wrong place, but then it doesn’t make sense to have an indicator of a probability either. \n- There is occasional sloppiness e.g., “[The presence of k in the first two terms of the denominator of the bound] implies diminishing returns in expected accuracy when more support data is added without altering \\phi.” I think it should say the *bound* is lowered, not necessarily R(\\phi).\n- I suggest adding a related work section\n- Consider moving Lemmas 1 and 2 to the appendix, they don’t add much understanding in my view.\n\nI assumed the correctness of the proofs - it would be good to make these easier to follow for a general audience. I'd consider raising my score if the method was put in more context, if the bound could be analyzed further, and if the paper could be more focused. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "A Theoretical Analysis of the Number of Shots in Few-Shot Learning\n\nThis paper considers the problem of meta-learning a representation that is suitable for few-shot classification using the distance from class centroids. In particular, it investigates the sensitivity of this approach to the number of shots from the perspective of generalization error. This seems to be a valuable and novel analysis of the problem, and the problem is important since the number of shots might not be known a-priori. The paper proposes a transformation (a dimensionality-reducing linear projection) that is based on the covariance of the within-class and between-class variance in the training set. It is shown that this makes the procedure relatively robust to the number of shots during training. The technique is compared to an informative set of baselines: PCA, adding a fully-connected layer and mixed-shot training.\n\nI liked the explanation for the harmfulness of training and testing with different shots: with few shots, the priority is to minimize the within-class variance, whereas with many shots, it's OK to have some examples that are further from the true mean as their effect on the empirical mean will be mitigated. Furthermore, the hypothesis that the ratio of within-class to between-class variance depends on the number of shots was empirically verified on several datasets. This was great to see. The paper additionally argues that, with fewer shots, the meta-learner in Prototypical Networks might reduce the intrinsic dimension of the embedding to improve generalization error (by effectively reducing the VC dimension). This was verified experimentally by examining the dimensionality of a subspace that approximately represents the embedding vectors.\n\nUnfortunately, I had difficulty following the theoretical analysis. Admittedly, I don't often work with generalization bounds. Nevertheless, to make the paper accessible to a wider audience, I believe it's necessary to improve the clarity (see points below). I spent quite a lot of time trying to understand the proof of Lemma 1 and I did not have time to closely assess the remaining proofs. I have given the benefit of the doubt for now, but I might have to downgrade my rating depending on the response of the authors and the feedback of other commenters.\n\nHigh-level concerns:\n\n(1.1) The approach closely resembles (multi-class) Linear Discriminant Analysis, yet this connection was not discussed in the paper.\n\n(1.2) The improvement of EST-ProtoNet over PCA-ProtoNet is often marginal in Table 1. It would have been better to provide an estimate of the variance over several trials. Nevertheless, it's an interesting result that simply applying PCA improves the robustness of ProtoNet to the training shots.\n\n(1.3) I wonder whether whitening would be more effective than dimensionality reduction? This could also avoid the need to specify $d$.  To be clear, by \"whitening\" I mean incorporating a factor of $\\Lambda^{-1/2}$ into the transform to make the covariance matrix equal to identity.\n\nIssues with mathematical clarity:\n\n(2.1) I could not figure out how equation 5 follows from Chebyshev's inequality. I understand that Chebyshev's inequality establishes a bound on the likelihood of a sample being some distance from the mean, and this bound depends on the variance. I do not see how this can be used to bound $\\operatorname{Pr}(\\alpha > 0)$. I also investigated Markov's inequality (sometimes referred to as Chebyshev's inequality) and Cantelli's inequality (sometimes referred to as the one-sided Chebyshev's inequality), but I could not see how either of these could be applied. Please clarify.\n\n(2.2) The notation of vector inner and outer products did not seem to be consistent throughout the paper (i.e. whether vectors are treated as a row or a column). For example, in Lemma 1, it seems that $x y^T$ denotes an inner product, since $\\alpha$ is a scalar. Then in equation 12, it seems that $x^T y$ denotes an inner product. This is particularly bad in the appendix. The multi-line equation at the end of page 12 seems to mix the two notations. If I have misunderstood, please correct me. I feel that the column-vector notation is more widespread ($x^T y$ for inner product and $x y^T$ for outer product), but the important thing is to be explicit and consistent.\n\n(2.3) I found the proof of Lemma 1 difficult to follow. Maybe I am being slow, but it would be helpful to explain the steps more clearly. (Side note: for the purpose of discussion, it would have been good to enable equation numbering here.) I did not understand the step from the 1st to the 2nd line of \"i = …\". I also could not follow the step from the 2nd to the 3rd line of the equation for $\\Sigma_{\\phi(x) - \\bar{\\phi}(S_b)}$. Nevertheless, I feel that the \"1/k\" term feels plausible, because as the number of sample increases, the estimate of the mean will be more accurate. I also could not follow the step from the 2nd to the 3rd line of the equation for $\\mathbb{E}_{a, b, x, S}[\\alpha]$. Please clarify these points or I may need to reduce my score.\n\nMinor errors:\n\n(3.1) Equation 2 should probably be \"arg max p(...) = y\" rather than just \"p(...) = y\"?\n\n(3.2) In the definition of \\Sigma_c, it is not clear what it means to square a vector. I think this should be written as a vector outer product (x x^T) or at least add a note to explain the notation.\n\n(3.3) There might be some small errors in appendix A.5 that don't affect the outcome:\n- Shouldn't the expression for (ii) include a term which depends on $\\mu_a - \\mu_b$, like the expression for (i)?\n- I wondered whether the expression for $\\mathbb{E}_{x,S|a,b}[\\phi(x) - \\bar{\\phi}(S_b)]$ should be $0.5 (\\mu_a - \\mu_b)$, since $x$ has a 0.5 chance of being from class $a$ and 0.5 chance of being from class $b$?\n\n(3.4) In the statement of Theorem 4 in the appendix, there should be something after \"Var\"?\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe author performs theoretical analysis of the number-of-shot problem in the case study of prototypical network which is a typical method in few-shot learning. To facilitate analysis, the paper assumes 2-way classification (binary classification) and equal covariance for all classes in the embedding space, and finally derives the lower bound of the expected accuracy with respect to the shot number k. The final formula of the lower bounding indicates that increasing k will decrease the sensitivity of this lower bound to Σc (expected intra-class variance), and increase its sensitivity to Σ (variance of class means). To reduce the meta-overfitting (when training are test shot are the same, the performance becomes better), the author designed Embedding Space Transformation (EST) to minimize Σc and maximize Σ through a transformation that lies in the space of non-dominant eigenvectors of Σc while also being aligned to the dominant eigenvectors of Σ. The experimental results on 3 commonly used datasets for few-shot learning, i.e. Omniglot, Mini-ImageNet and Tiered-ImageNet demonstrate promising results and desired properties of the method.\n\n+Strengths:\n1. The paper focuses on an important problem: number of shots in few-shot learning, and chooses prototypical network which is a very famous and widely used method for detailed analysis. \n2. The paper provides relatively solid theoretical analysis with careful derivation. The final upperbound of expected accuracy matches our intuition to certain degree. Although some of the assumptions (such as 2-way classification and equal covariance for all classes) are not so realistic, the work is meaningful and very inspiring.\n3. The proposed modification over prototypical network inspired by the formula is reasonable and the experimental results demonstrate its effectiveness.\n\n-Weaknesses:\n1. The first observation says that \"diminishing returns in expected accuracy when more support data is added without altering \\phi\". Does it mean that the accuracy of prototypical network deteriorates with more support data? Will the accuracy saturate and no longer diminish from certain k?\n2. Some of the accuracy improvements are not so significant from the results (even for Mini-ImageNet and Tiered-ImageNet). I was wondering if it is due to the prototypical network itself (the intrinsic property of the prototypical network limits its improvement upperbound) or something else? Please clarify.\n3. Some unclear descriptions.  How is the formulation derived between Eq. 3 and Eq. 4? More details should be given here. The descriptions about EST (Embedding Space Transformation) is insufficient, which makes it hard to understand why such operations are conducted. Moreover, it seems that the proposed approach need to compute the covariance mean and mean covariance of each class. Would it be computed in each iteration? If so, it seems to be very slow."
        }
    ]
}