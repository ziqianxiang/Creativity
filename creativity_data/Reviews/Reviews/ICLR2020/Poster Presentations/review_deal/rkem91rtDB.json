{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper focuses on the problem of finding dense representations of graph-structured objects in an unsupervised manner. The authors propose a novel framework for solving this problem and show that it improves over competitive baselines. The reviewers generally liked the paper, although were concerned with the strength of the experimental results. During the discussion phase, the authors bolstered the experimental results. The reviewers are satisfied with the resulting paper and agree that it should be accepted.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "In this work, a novel graph similarity learning framework SEED is proposed. Given an input graph, SEED proceeds in 4 steps, namely Sampling subgraphs, Encoding sampled subgraphs using autoencoder, aggregating subgraphs' Embedding Distribution into a vector representation.  Theretically, a connection between proposed SEED and the graph isomorphism is established. Experimentally, simulation on DEEZE and MUTAG datasets validated the effectivety of the proposed graph learning framework.  \n\nPro:\nThe paper is well structured and easy to follow.  Experiments appears convincing, especially the t-SNE plots when varying the number of subgraph samples.\n\nCon:\nIt would be better if experiments can be conducted on a few more benchmark datasets used in the compared methods.\n\nMinor:\nLast sentence in Page 6: each component have been -> each component has been\nthe 5-th sentence in Sec. 4.3, focusing -> focuses\nthe 7-th sentence in Sec. 4.3, At the meantime -> In the meantime"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. The most innovative part in this paper is random walks with earliest visiting time (WEAVE) in the sampling part. WEAVE has potential power for capturing structure difference and could reflect isomorphism as well. Instead of using language model like word2vec, encoding part leverages MLP to get the embedding of each WEAVE, which is efficient and intuitive. Then, the group of encoding results from all WEAVEs are aggregated with kernel functions, generating the final embedding of a graph. This method achieves better accuracy in both clustering and classification tasks than previous ones including GraphSAGE, GMN and GIN.\nThis method uses an elegant way to embed graphs in an unsupervised manner, and the new random walk approach provides insights into graph structure encoding. Factors like walk length and number of walks are strictly derived then well examined in real experiments.\nQuestions & suggestions:\n1 In WEAVE encoding part, the paper doesn’t show how much the earliest visiting time information improve the model. In another word, if we leave out the timing term (x_t^{(p)}), will the model still perform well?\n2 In embedding distribution part, it seems that only identity kernel is easy to calculate. For commonly adopted kernels, the MLP is “mimicking the behavior of a kernel”, so it will still be limited by the kernel you choose in MMD. There are some statistical approaches to estimate Ф(z) like Nystrom method, maybe that will be another solution other than taking average."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "The authors propose a method for learning graph embeddings and focus specifically on a setting where not all graphs are part of the training data (the inductive setting). The core problem of graph embedding methods is to find a learnable function that maps arbitrary graphs into a fixed-sized vector representation. There have been several proposals ranging from the class of graph kernels to variations of graph neural networks. The authors propose a method that consists of three steps\n\n(1) sample a number of subgraphs from the original graphs\n(2) learn an encoding function for these subgraphs (subgraph -> vector representation)\n(3) for every graph we, therefore, get a set of vector representations, one per subgraph. We now try to find a similarity measure operating on sets of vectors to compute the distance between graphs.\n\nThe novel bits are\n(a) the way that the subgraphs are sampled (using an algorithm called WEAVE, that stores more information about random walks) and, therefore, is able to be distinguish graphs based on the extracted walks that standard random walk based methods cannot; and \n(b) to define a similarity measure based on the set of vectors. \nThe authors also prove that their method is (under some assumptions) able to decide the isomorphism problem. This is a nice result to have in light of recent papers that have investigated the limitations of GNN in comparison to Weisfeiler-Leman and isomoprhism testing. \n\nUnfortunately, the proposed method has limited novelty. The WEAVE sampling is a small variation on random walk sampling that's been around for a while in graph representation learning. Also, to define the similarity between set of vectors has been addressed before in numerous papers (e.g., all papers investigating learning for sets, DeepSets, etc.) and the method here seems a bit ad-hoc and doesn't compare to existing work. \n\nMy \"novelty\" critique is also made in light of the small number of datasets on which experiments have been conducted. If a new simple random walk strategy would lead to clearly better results on a number of datasets, this would be a significant contribution. As far as I can tell, however, the results are mixed and not very impressive especially due to the small number of datasets. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}