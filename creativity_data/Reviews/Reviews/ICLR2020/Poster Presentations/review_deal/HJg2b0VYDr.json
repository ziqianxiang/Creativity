{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes to perform sample selection for deep learning - which can be very computationally expensive - using a smaller and simpler proxy network. The paper shows that such proxies are faster to train and do not substantially harm the accuracy of the final network.\n\nThe reviewers were all in agreement that the problem is important, and that the paper is comprehensive and well executed. I therefore recommend it should be accepted.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "The paper proposes a method for selecting a subset of a large dataset to reduce the computational costs of deep neural netwoks. The main idea is to train a proxy model, a smaller version of the full neural network, to choose important data points for active learning or core-set selection. Experiments on standard classification tasks demonstrate that this approach can yield substantial computational savings with only a small drop in accuracy.  \n\nThis paper is well-written and was easy to follow, with a clear motivation. The paper does good job of demonstrating that the proposed algorithm is effective through a comprehensive set of experiments. Overall, I favor acceptance and would be willing to increase my score if the following are addressed:\n\n1) Training for a smaller number of epochs was mentioned as a possibility to save computation. In the experiments, this was only done for one of the settings. Is this because models trained for fewer epochs are ineffective for data selection? \nLooking at Fig. 5b, it seems like the error drops significantly around 14 min before plateauing. In practice, for a new dataset, it could be difficult (or impossible) to know when to stop training a proxy a priori so that it achieves good performance relative to a larger model. It could be interesting to look at the effectiveness of a proxy at various points during training to see if the benefits are sensitive to the training time.\n\n2) For the active learning experiments, how many data points were added to the training set in each round? I could not find this number in the paper. Also, how many rounds of data selection were there?\n\n3) Is there an explanation why in Fig. 2b and Fig. 7c, the resnet20 proxy performs better than the larger (and more accurate) models? In particular, it even outperforms the 'oracle' baseline, which I find surprising.\n\n4) On a similar note, in Fig. 7, for small subsets (30%), the random baseline outperforms all of the SVP settings. Why is SVP ineffective in this case?\n\nMinor comments/suggestions:\n- In the abstract: \"improvement in data selection runtime\" Is this \"data selection runtime\" different from the total runtime? If not, it could be clearer to simply state it as the \"total runtime (including the time to repeatedly train and select points)\". I was unsure if this was a different measure.\n\n- Did the authors try further reducing the model capacity? With the success of the smaller models, it seems natural to try pushing further in this direction.\n\n- Since one of the findings was that models with similar architectures were effective as proxies but not models with different ones, perhaps there could be even higher correlation if the proxies were initialized with the exact same weights as a subset of the full model. \n\n- The writing in Table 1 (and the ones in the appendix) is a bit small.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "What is the paper about ?\n*The paper proposes a simple method to speed up active learning/core-set selection in deep learning framework. The idea is that instead of using full scale deep model for data selection/uncertainty sampling, use a smaller/faster proxy model and this proxy model’s uncertainty estimates are good enough for choosing data points even though proxy model’s accuracy may not be good.\n*The paper experiments with 5 datasets with 2 active learning and 3 core set selection strategies and shows promising speed-up with little to no regression in performance in most settings.\n\n\nWhat I like about this paper ?\n*The idea is simple and the paper is well written and easy to follow with ample references.\n*Unlike a lot of active learning papers, this paper considers both image and text domains.\n*Results in both Table 1 and Figure  2 have been reported after multiple runs 3 and 5 respectively. Authors also report standard deviation which is important because the results can vary a lot from run to run in active learning.\n*Plenty of empirical results do make the paper attractive and this definitely makes for a nice contribution to the active learning community.\n\n\nWhat needs improvement ?\n*Novelty is lacking. While the authors may be the first ones to apply this idea in the context of deep learning, they themselves note that the idea of using a smaller proxy like naive bayes for larger models like decision trees is not new. Although I agree this should not be a criteria for rejection especially given the extensive experimentation performed in the paper and has not impacted my score.\n*The choice of datasets is not justified in the paper and could have been better. I don’t see the point of showing that method works well for both Amazon Review Polarity and Amazon Review Full. Same can be said for CIFAR10 and CIFAR100. Would have appreciated a more diverse set of datasets. I would have been happy to see results on WMT given data selection is an open challenge in machine translation.\n*Paper initially proposes two methods for proxy: one by scaling down the model and two by reducing the number of epochs. fastext falls under neither of those categories and the giant speed up claims made by the paper like 41.7x are only with fastext. It is also important to note that while the performance drop using the fastext or smaller version of VDCNN is not much as compare to VDCNN, the performance gain is also not a lot compared to Random. Due to this reason I am not convinced if I should use SVP if I want speedup or just randomly select points especially in text domain.\n*Would have liked to see the comparison of #parameters in your different proxy models instead of just names like VDCNN29 and VDCNN9.\n*A lot of experimental details are missing which would make it difficult to reproduce the results unless the code is released.\n*Why fasttext worked as a proxy for VDCNN and AlexNet did not work with ResNet is unclear from the paper.\n\n\nQuestions for Authors ?\n*Do you plan to release the full code that can reproduce the results in the paper ? Based on my experience, I have found that a lot of active learning results are hard to reproduce and therefore it is difficult to draw any conclusions based on them.\n*How sensitive are the results to hyper-parameter tuning and small things like choice of pre-trained word embeddings etc ? How did you select hyper-parameters at each stage during active learning ? \n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents a method to speed up the data selection in active learning and core-set learning. The authors present a simple idea: instead of using the full model to select data points, they use a smaller model with fewer layers, potentially trained for fewer iterations. The authors show that this simple approach is able to speed up the data selection portion of both processes significantly with minimal loss in performance, and also results in significant speedup of the entire pipeline (data selection + training).\n\nThis paper is timely and important -- there has been a lot of emphasis lately on the environmental costs of training deep learning models (e.g., Strubell et al., ACL 2019; Schwartz et al., 2019 arxiv:1907.10597). This paper shows that simple, almost trivial techniques can lead to significant runtime benefits for active learning and core-set learning. The authors present an extensive set of experiments that validate their hypothesis, and the paper is overall clearly written.\n\nComments:\n\n1. The correlation values in Figure 3 are quite diverse. It seemed \"forgetting events\" is much more correlated that the other two approaches. \n\n2. Why do the authors think that SVP and their baselines failed to outperform random sampling on Amazon Review Full (towards the end of 3.3)?\n\n3. Table 1 is very hard to interpret. I would advise the authors to look for a more succinct way to present their main findings. While this might seem contradictory, Figure 2, which is more reader-friendly, is also hard to interpret without the runtime values (nobody said visualization is easy!).\n\n4. One thing that's missing from this paper is training the smaller networks end-to-end. What would be the effect of using the proxy network as the main network as well? this is likely to lead to very significant runtime savings, and I wonder at what costs.\n\nMinor:\n\n1. Towards the end of 3.2: I disagree that significant speedups are \"uninteresting\" if they lead to small reductions in performance."
        }
    ]
}