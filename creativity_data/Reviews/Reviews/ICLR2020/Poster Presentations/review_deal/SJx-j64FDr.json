{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper studies how the architecture and training procedure of binarized neural networks can be changed in order to make it easier for SAT solvers to verify certain properties of them.\n\nAll of the reviewers were positive about the paper, and their questions were addressed to their satisfaction, so all reviewers are in favor of accepting the paper. I therefore recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "This paper deals with the scalability of Binarized Neural Networks (BNNs) and their representation in Boolean logic. This encoding enables SAT solvers to reason about the underlying variables and query existential or counting clauses. The main contribution of this paper is the analysis of the architectural design choices of the BNN and modifications of the training procedure in order to improve the efficiency of SAT solvers. The modifications are simple but effective and the authors show consistent improvement on different benchmarks without the SAT solver timing out.\n\nOverall, I am leaning towards accepting this paper due to the improved empirical results compared to the baselines the authors build on. However, it does not seem that there is much novelty in the BNN architecture per se, but rather in the training procedure. The modifications are simple but effective. The paper seems to be well-written and easy to follow also from a reader not familiar with the literature in this area.\n\nI would be interested to have an answer to the following concerns:\n\n- It was not totally clear for me what is are the changes that the paper proposes in the block level and network level and if these modifications are present in their experiments.\n- Did you evaluate your method on other image classification datasets, such as CIFAR-10/-100?\n- The thresholding used to sparsify the matrix A^i and seeding the initialization (correct me if I am wrong) when retraining the BNN seems to be really similar to the Lottery Ticket Hypothesis [1]. It would be interesting to evaluate the sensitivity of your proposed method towards the initialization.\n\n[1] Jonathan Frankle, Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In ICLR, 2019",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper approaches the problem of using logic-based reasoning tools to analyze binary neural networks. They propose general changes to BNN architectures at the neuron level so as to adapt them for more efficient usage by such tools. Their main contributions are a new design choice utilizing ternary, rather than binary, quantization, and additional variables to propagate through the network containing upper and lower bounds for each layer output. They also suggest further promising directions to adapt BNNs at the block and network level. Their experiments show that the proposed changes do not significantly alter test accuracy from a standard trained baseline model while dramatically reducing the number of model parameters. Consequently, they are able to demonstrate that SAT solvers can find adversarial examples much more quickly with their new architectural alterations.\nOverall, this paper presents a novel contribution with an idea for more efficient BNNs and experimentally verifies the success of their proposed changes. However, there is a concern regarding the experimental setup. \nA large part of the motivation for this task arises from the field of verification regarding adversarial examples. While binarized neural networks are not my area of expertise, I am familiar with a number of verification papers on full neural networks. As such, I find the datasets used, accuracies reported, and model size a bit peculiar. In ‘Scaling provable adversarial defenses’ (Wong ’18), the authors were able to verify models with a couple million parameters on both MNIST and CIFAR. In my understanding, the primary advantage of BNNs is their efficiency in having only binary weights. If this is the case, I see no reason why the experimental setup would have such a small model. Additionally, it is generally well-known that MNIST and its variants are not a particularly hard datasets to classify. Many different architectures can easily achieve 98 or 99 percent test accuracy. A quick literature search found ‘A review of Binarized Neural Networks’ (Simons ’19), which reports better numbers on MNIST and decent results on CIFAR-10 (and in fact, even ImageNet). Their experimental evidence crucially relies on the conclusion that their techniques allow for faster SAT solver computation *while* maintaining comparable test accuracy. As such, it is my opinion that to be a strong paper, they must show this property holds for larger networks and more significant datasets. \nAdditionally, a comment for the authors to consider: I would suggest switching some of the supplementary materials with the main content. The exposition on BNNs is quite dense and detailed. Some of these details could be moved to the appendix and replaced with more experimental graphs and images. This would provide some breaks in the text so as to provide ease of reading.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "In this paper, a new design of Binary Neural Network (BNN) is proposed. The purpose of the new design is to make the network easier to analyze using the existing Boolean satisfiability (SAT) solvers. An easier analysis is preferable since the SAT solver can be used to infer existing fragility in the network, e.g., adversarial attacks. Empirically, the proposed design of BNN is shown to outperform traditional BNNs by a large margin in terms of detecting adversarial attacks. \n\nOverall, this paper seems to provide a solid contribution to the design of robust BNNs. I like how the authors tried to address various aspects of the networks for easy analysis of SAT solver, i.e., neuron, block, and network-levels even though they only focused on changing the architecture in neuron-level perspective. The experimental results seem to show benefits without any compromise in performance. \n\n"
        }
    ]
}