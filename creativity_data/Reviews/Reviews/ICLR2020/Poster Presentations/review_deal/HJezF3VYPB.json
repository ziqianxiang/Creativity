{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper studies an interesting new problem, federated domain adaptation, and proposes an approach based on dynamic attention, federated adversarial alignment, and representation disentanglement.\n\nReviewers generally agree that the paper contributes a novel approach to an interesting problem with theoretical guarantees and empirical justification. While many professional concerns were raised by the reviewers, the authors managed to perform an effective rebuttal with a major revision, which addressed the concerns convincingly. AC believes that the updated version is acceptable.\n\nHence I recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This paper introduces an unsupervised federated domain adaptation (UFDA) problem and proposes a new model called Federated Adversarial Domain Adaptation (FADA) to transfer the knowledge learned from distributed source domains to an unlabeled target domain. This paper uses a dynamic attention mechanism by leveraging the gap statistics to transfer distributed source knowledge. This paper also proposes a method to disentangle the domain-invariant features from domain-specific features, using adversarial training. Moreover, a theoretical generalization bound for UFDA is derived. An extensive empirical evaluation is performed on UFDA vision and linguistic benchmarks.\n\nThis paper should be rejected because the total pipeline seems ad-hoc except for optimizing the weight of the source domain in the attention mechanism. Although the derivation of generalization bound for FDA in Sec.3 is excellent, it only demonstrates the importance of the weight $\\alpha$. This result is trivial if we assume to have the same source domain as the target and utterly unrelated source domain to the target domain. It seems that proving why minimizing the gap statistics contributes to FADA is more essential in the dynamic attention mechanism. Because representation disentanglement has no relation with the derived theory, it would be better to clarify whether this method is original or not.\n\nIn the UFDA setting, the reviewer has doubts about whether it is realistic that the source node has a rich labeled data assuming our smartphones. Also, the assumption that the system cannot access the source data but must access all source feature seems a significant limitation in terms of privacy issues and communication cost between the target node and the source nodes.\n\nIt is unclear what is the final target classifier. If the target can access the teaching signal (e.g., labels or tags) in the source domains, it would be better to mention whether this situation violates the assumption the authors raised or not.\n\nMinor comments\n1) What is T(p, q, \\theta) in the section of Representation Disentanglement?\n\n2) What is C_s in eq.6? C_{s_i}?\n\n3) In Fig.3, it is not proper to discuss the size of intra-class variance by just looking at the figures because the t-SNE is a non-linear mapping. It is better to show quantitative scores, such as the value of the Fisher criterion.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors present a novel algorithm for dealing with domain adaptation in the setting of federated learning (classification, specifically). That is, they tackle the issue of learning a model on a new domain when access to the data points used in training the source models is not possible due to privacy constraints. The approach uses the gradients of the source models, reweighed to account for the differing shifts between the different sources and the target domain, to fit the model on the target domain.\n\nThe authors motivate their approach by providing a novel bound on the generalization error of transfer learning when the hypothesis function used on the target domain is a convex combination of hypotheses fitted on multiple source domains. This bound shows that the weighted sum of divergences in the symmetric difference hypothesis space controls the generalization error, so the authors aim at deriving feature representations and using aggregation weights that ensure this weighted sum is small. \n\nThe authors use a novel dynamic attention model to get the aggregation weights: they cluster the features in the target domain, and measure how much the intra-cluster variation decreases when information from a given source domain is incorporated. The aggregation weights for the model updates on the target domain are then weighed using a softmax transform of these contribution weights. \n \nThe motivation up through and including section 3 is clear, the theoretical results are presented clearly, but the model details in section 4 are unclear:\n-  In the dynamic attention mechanism, how does one a priori choose the number of clusters in computing the gap statistics, and what is the impact? \n- the notation in the federated adversarial alignment section is unclear: what *exactly* are the model coefficients Theta that are being updated?\n- the statement \"optimize following objective\" is made several times. this is ambiguous, and should be corrected to \"miminize\" following objective.\n- the representation disentanglement process is intricate, and only vaguely addressed. how does one fit the neural net and use (8)? where is the l2 reconstruction loss balanced with the mutual information? the vagueness of this section means Algorithm 1 is not well-specified.\n\nThe experiments are reasonable, and compare to baseline domain adaptation methods.\n\nThe problem considered is of interest, and the approach is novel and interesting. However, the algorithm is not described in sufficient detail. After reading the paper, and spending considerable time rereading section 4, I still do not understand how Algorithm 1 is implemented in practice. For that reason I lean towards reject. I will update my score if the authors clarify the details of Algorithm 1. \n\nComments:\n- the symmetric difference hypothesis space is incorrectly called the HdeltaH divergence in section 3\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposed and studied the unsupervised federated domain adaption problem, which aims to transfer knowledge from source nodes to a new node with different data distribution. To address the problem, a federated adversarial domain adaption (FADA) algorithm is introduced in the paper. The key idea of the algorithm is to update the target model by aggregating the gradients from source nodes, and also leverage adversarial adaption techniques to reduce the discrepancy between source features and target features. Overall, the problem studied in the paper is interesting, theoretical analysis on the error bound is provided in the paper, and the effectiveness of the proposed method has been validated in various datasets. Although the technical contributions of the paper are solid, I still have several concerns about it.\n1. The proposed algorithm is not described very clearly in section 4. According to the paper, DI is used to identify the domain from the output of Gi and Gt and align the features from those domains, then how is it related to the disentanglement in Eq 6. Also in Eq 6, symbol C_s was not introduced in the previous context, which makes it confusing to understand this objective.\n\n2. It would be better if the author(s) can provide some complexity analysis of the proposed algorithm.\n\n3. The paper still contains some typos and unresolved reference issues. "
        }
    ]
}