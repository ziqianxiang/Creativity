{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper received positive recommendation from all reviewers. Accept.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "The paper proposes a DL architecture that achieves better performance on time series prediction. The proposed architecture is relatively straightforward and composes residual blocks. While the paper does achieve superior results, a lot of the text is devoted to comparing to prior work and arguing that DL approaches can do better than hand-crafted approaches, instead of focussing on the importance of specific technical contributions made in the paper. \n\nMy main concerns are: \n(a) The main technical idea in this paper is the use of back-casting and forecasting (i.e. doubly residual connections). However, no ablations are provided to show how important doubly residual connections are. What is the performance, if the DL architecture is kept exactly the same, except for:\n    (i) No residual connections in backcasting (simply feed in the overall input to every block) \n    (ii) In addition to (i), make no backcasting predictions\n\n(b) Given no architectural details of the previous ML methods are provided, its unclear if the current architecture is better because it has more parameters or it is indeed the doubly residual idea that is important. \n\n(c) Authors make a point about interpretability — but interpretability is only achieved using domain specific knowledge. I am not really sure, what is so novel about this. \n\nOverall, this seems to be a good applications paper which has been optimized for performance. As a research paper, the contributions are less clear. Answers to my concerns above will help clarify. Given my current concerns, I cannot recommend this paper to be accepted. \n\n--------------------------\nPost Rebuttal: \n\nAuthors have addressed several of my concerns by providing detailed ablations and the paper reads much better. I still have some concerns: \n(a) Interpretability in this work is obtained by using standard basis functions used by the TS community. Such basis functions can be used with any architecture, not just the doubly residual connection networks. Also, its common for people to make use of domain specific knowledge, for e.g. predicting spectrograms when analyzing audio data. In these cases, yes predictions are interpretable. Not sure, what is the contribution that authors are claiming. \n\n(b) The connection to meta-learning seems to be a bit mis-leading. The authors claim that all the weights of the network form the outer loop and the stage-wise predictions of \\theta corresponds to the inner loop. Can I simply not think of \\theta as features? In that case, this logic will apply to any deep neural network. Not sure what exactly authors want to say. Under this view all deep networks are doing meta-learning. I might have misunderstood something here -- please correct me, if so. \n\nI am changing my rating to weal accept, but I also hope the authors will address the concerns outlined above.  \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper investigates a pure deep learning architecture for Univariate time series analysis by simply ensembling feed-forward networks, along with the residual stacking mechanism for fluid learning. Each of the generic block consists of 4 FC layers followed by the use of forward and backward predictor to have forecast and backcast output of the original input. These blocks forms the stacks, where each stack provides the residuals and the forecast responses further to the next stacks, which ultimately provide the global forecast. To make the internal stack outputs interpretable, assumptions are imposed on the trend model, which follows a polynomial function of time vector, and seasonality model which follows periodic Fourier series. Further, the ensembling of models based on different metrics and input windows is used for better accuracy.    \n\nVery well written paper and easy to follow. It advocates a pure DL framework (instead of hybrid statistical models and DL). I found the idea simple and effective, yielding results better than the previous approaches. Also, the experimental setup is well described. I also found the link between this work and meta-learning approaches interesting.\n\nHowever, I have several questions about the interpretability results. It looks like the inductive bias based on some general assumptions can fail in some cases. For example, in Fig. 2(a), the line for FORECAST-I and FORECAST-G deviates much in case of hourly/weekly/daily data frequency. What is the reason behind this? \n\nAlso, in Fig. 2, while the StackT-I (fig.2(d)) and StackS-I (fig.2(e)) provide response lines different from the counterparts in  Stack1-G (fig.2(b)) and Stack2-G (fig.2(c)), the summations in the combined line (fig.2(a)) yield similar curves of pretty much the same shapes, without much perceived difference. Is it expected or something is wrong?\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This goal of this paper is to present a strong empirical result showing that a \"pure\" machine learning based method can outperform all known methods on some of the most challenging time series forecasting benchmarks (TOURISM, M3 and especially M4). Since I am not from the field of forecasting, I can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field. \n\nOn the most challenging dataset (M4), the best known performing method combines RNNs with a traditional smoothing algorithm. The model proposed in this paper outperforms it without being combined with any classical approach, though it does utilize ensembling.\nThe experimental setup is sound in my opinion, and the result appears to be of high potential significance.\nHowever, despite trying to go through Section 3 multiple times, the exact model architecture is not clear to me. Due to this reason, my current decision is a weak rejection since the model is a central contribution of the paper. I will be happy to increase my score if the authors can make the model description crystal clear.\n\nEven though my expertise is deep neural network architectures, I find it hard to follow the descriptions in Sec. 3. I faced the most difficulty understanding section 3.1, which obviously made the rest of subsections even harder to follow. Here are my main points of confusion:\n\n- One big issue is that the paper uses an illustration (Fig. 1) to explain the architecture instead of equations, but then uses symbols in the main text that do not appear on Fig. 1 at all such as g_theta. Is the \"FC Stack (4 layers)\" g_theta? \n\n- Where are (uppercase) phi functions? I could infer that these are the \"FC\" blocks but they should be labeled.\n\n- The Figure has the symbols g^b_theta and g^f_theta that do not appear in the text description. What exactly do they do? And is the theta that parameterizes each of them the same theta that parameterizes g_theta? How is this possible if g_theta is the \"FC Stack (4 layers)\"?\n\n- The description in second and third paragraph of Section 3.1 is very confusing and unclear. It should be replaced or augmented with equations using clearly defined symbols that match Figure 1.\n\n- More confusion stems from the use of the term \"parameters\" in (I believe) a different context than is used in neural networks, where \"parameters\" refers to connection weights. But here parameters are outputs of some functions, so either they are not connection weights or this is a fast-weight style architecture where outputs are weights [1], in which this should be made clear.\n\n- Design of the doubly residual architecture in Section 3.2 makes sense to me at a high level, but I feel it is still very hard to clearly understand and implement it. Again, use of equations to clearly define the computation would be very helpful.\n\n\n[1] Schmidhuber, Jürgen. \"Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\" Neural Computation 4.1 (1992): 131-139.\n\n\n--- Update after rebuttal ---\n\nI am happy to see the paper greatly improved by the authors in their updates. My concerns related to the presentation of the model have been addressed, and I find the architecture much easier to understand. I also appreciate the detailed supplementary material that is likely to help readers interested in the area. Related to the areas I work in, I noticed the following missing references:\n\nDensenets: Lang, K.J. and Witbrock, M.J., 1988, June. Learning to tell two spirals apart. In Proceedings of the 1988 connectionist models summer school (No. 1989, pp. 52-59).\n\nMetalearning: Schmidhuber, J., 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook (Doctoral dissertation, Technische Universität München).\n\nWhile improving papers is generally the objective of the rebuttal phase, I suggest that the authors to not take this as an opportunity to submit unpolished papers in the first phase. That said, I have increased my rating to reflect my satisfaction with the current version of the paper.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}