{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "Thanks to the authors for the submission. This paper studies differentially private meta-learning, where the algorithm needs to use information across several learning tasks to protect the privacy of the data set from each task. The reviewers agree that this is a natural problem and the paper presents a solution that is essentially an adoption of differentially private SGD. There are several places the paper can improve. For the experimental evaluation, the authors should include a wider range of epsilon values in order to investigate the accuracy-privacy trade-off. The authors should also consider expanding the existing experiments with other datasets. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "This paper considers the problem of achieving formal privacy guarantees in the context of parameter-sharing meta learning. This is a problem that has been studied recently, although generally not under the exact record-level task-privacy model studied in this paper (what they call global task privacy). The problem is well-motivated: since in meta learning we want to leverage information from similar tasks to increate data efficiency, there may be privacy concerns for each of the task owners about both other task owners and the aggregator (meta-learner). \n\n\nTheir approach is conceptually simple; they use standard private stochastic gradient descent within each task to provide task level privacy. In combination with post-processing and composition guarantees this gives privacy for the overall mechanism. They are able to show theoretical guarantees via the standard accuracy guarantees of private SGD and no-regret\nguarantees of OCO. They show a bound on the expected transfer risk:  \nO(V/sqrt(m) + 1/Tsqrt(m)) + o(1/sqrt(m)) which is close to the non-private bound from Denevi (2019).\n\nThey evaluate the empirical performance of these models via a transfer learning setting  where they are training a deep RNN for next word prediction on two large corpi, and the tasks correspond to individual articles. They show that the private variate of Reptile is competitive with the non-private variant in these settings. \nA few comments \n-\tIt is odd to just fix epsilon = 9.2 instead of showing a Pareto curve. Why this particular value?\n-\tSimpler (convex) experiments to illustrate the theoretical guarantees would improve the paper\n\nOverall I like the motivation and the theory results are solid, if not a bit obvious. However, due to the lack of novelty in any of the applied techniques, and the fact that the experiments could be expanded, I recommend a weak accept. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "As a non-expert in differential privacy, my review is based on my educated guess and limited understanding of the paper.\n\nThis paper considers a the differential privacy problem regarding the parameter-tranfer algorihtm in meta-learning, such as MAML and Repile. To me, the setting is very interesting and according to the paper, it seems that it is the first formalization for this setting. Since meta-learning is becoming more and more popular, the paper possesses practical values for privacy-preseving meta-learning algorithms.\n\nThe proposed differential privacy seetings are well illustrated and presented in the paper. The differentially private parameter-transfer is also straightforward but has been twisted a little bit for theoretical guarantees. The theoretical results seem pretty reasonable to me, but I have not checked the proof in detail.\n\nThe experiments demonstrate the effectiveness of the proposed differentially private parameter-transfer. However, the experiments are also very toy-ish in some senses, which reduces the the pritical values. All the datasets the paper uses are very easy ones. It is highly recommended to perform experiments on more challenging meta-learning datasets, such as Mini-ImageNet, CUB, etc. See [A Closer Look at Few-shot Classification, ICLR 2019] as an example for conducting few-shot learning experiments."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes the notions of different privacy levels for different attack models, namely global and local meta-level and within-task level privacy for meta-learning. It proposes an algorithm for global within-task privacy. It provides privacy and utility guarantee of the proposed algorithm and experimental evaluations.\n\nThe proposed definitions make sense to me for the scenarios mentioned in the paper. The utility guarantee also seems interesting. Iâ€™m a little concerned with the significance and novelty of the proposed algorithm (it seems like a direct application of a generic DPSGD algorithm) and the utility analysis. Maybe you can justify more on that part. I think the experimental evaluation can be made more complete, for example, you may consider:\n- a convex setting as was considered in the utility guarantee,\n- varying epsilon values. I think a utility vs. epsilon curve can better support your paper."
        }
    ]
}