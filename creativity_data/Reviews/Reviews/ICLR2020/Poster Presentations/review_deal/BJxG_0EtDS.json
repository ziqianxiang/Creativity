{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper studies optimal control with low-dimensional representation.  The paper presents interesting progress, although I urge the authors to address all issues raised by reviewers in their revisions.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #4",
            "review": "This paper considers learning low-dimensional representations from high-dimensional observations for control purposes. The authors extend the E2C framework by introducing the new PCC-Loss function. This new loss function aims to reflect the prediction in the observation space, the consistency between latent and observation dynamics, and the low curvature in the latent dynamics. The low curvature term is used to bias the latent dynamics towards models that can be better approximated as locally linear models. The authors provide theory (error bounds) to justify their proposed PCC-Loss function. Then variational PCC is developed to make the algorithm tractable. The proposed method is evaluated in 5 different simulated tasks and compared with the original E2C method and the RCE method. \nThe paper is well-written. \n\nPros:\n- The idea in this paper is quite original. The three principles used to formulate the loss function provide some new insights.\n\n- The authors have proposed a theory to justify the use of their loss function. The technical quality of this part seems solid.\n\n- Simulations have been used to show that the proposed PCC method outperforms E2C and RCE.\n\n-The paper is well written. \n\nCons:\n- The tasks in this paper are not that complicated. It is unclear whether the proposed method outperforms other model-based RL methods such as Solar and DSAE for practical robotic applications. More comparisons are needed.\n\n- It is also not that clear why one wants to SOC3 to be close to SOC1 in the first place. It seems the true optimization problem should be posed on the space of the original state s. SOC1 is just a surrogate problem for the original problem.\n\n- There seems to be a gap between the proposed theory  and the algorithm implementation. This makes the theory part less useful.\n\nOverall, I think the idea in this paper is interesting. The authors have made a serious effort in coming up principles for model-based RL control. But at this moment it is not that convincing the proposed method will be the best model-based RL method for practical robotic applications. If the authors can address my comments, I will be willing to increase my score.\n\n\n\nMinor Comments:\n\n- It seems that for the task the authors have tested their method, it is not that difficult to directly estimate the state. Am I correct here? Can the authors make a comment on this? How to compare their approach and a more direct control approach using estimation of state s? \n\n- I have never seen the curvature principle in any control papers. Any control reference on why this is a good principle? It seems that the linearization works well when the control inputs are around the reference points. Does the curvature really matter that much for ILQR to work? \n\n- How to justify the Markovian assumption on x? Just by observation or there is a more principle way to test this assumption on the buffered images?\n\n====================================================\nPost-Rebuttal:\nAfter reading the authors' response, I am changing my score to weak accept. Lemma 4 is nice. I have not seen anything similar to this in the controls literature before. The authors have addressed most of my concerns. I still have a few comments for preparing the final version of this paper. \n1. I still don't see why SOC1 is the \"original problem.\" Yes, it is assumed that the true state cannot be directly observed. But if the observations are Markov eventually, then some estimated version of the states can be obtained, right? I think treating SOC1 as the original problem is one possible way of doing things and clearly the authors have built a principled framework for doing things in this way. But treating SOC1 as the original problem seems not the only way of doing things. I hope the authors can clarify this and do not oversell the proposed approach. \n2. I think it is still worth comparing SOLAR and PCC empirically. This will help the readers to choose algorithms when they need. \n3. The comment on the verification of the Markov assumption is hand-waving.  The authors said \"A simple test would be to see if a control algorithm with the Markovian assumption works well with our representation or not.\" Does this mean that the users will not be able to verify this assumption before using the proposed approach to obtain controllers? It will be helpful if the authors can explain this step for one specific example in details.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper considers from a high level the problem of learning a latent representation of high dimensional observations with underlying dynamics for control.  The authors specifically describe some desiredata for latent representations for LLC algorithms. The authors rigorously construct a learning framework that can satisfy the desiredata and then show how this can be tractably instantiated. \n\nThe paper overall is clear, however there is many equations in 4.2  with heavy subscritping making it sometimes difficult to read. The authours could attempt to better highlight the more critical parts of their propositins (e.g. eq. 8/9). \n\nThe methodology and insights appear novel and well motivated, however I am not familiar with many of the prior work.  The experiments compared to competing methods  show substantial improvement. The authors also motivate well why these improvements over the existing methods should occur and provide ablations to validate all the components of the final loss. Overall the paper appears very solid and may motivate insights and research  in more complex model based control and planning \n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work proposes a regularization strategy for learning optimal policy for a dynamic control problem in a latent low-dimensional domain. The work is based on LCE approach, but with in-depth analysis on how to choose/design the regularization for the \\hat{P} operator, which consists of an encoder, a decoder, and dynamics in the latent space. In particular, the author argued that three principles (prediction, consistency, and curvature) should be taken into consideration when designing the regularizer of the learning cost function - so that the learned latent domain can serve better for the purpose of optimizing the long-term cost in the ambient domain. \n\nThe paper is well written and pleasant to read. One possible shortcoming is that the notations are a bit dazzling. It is almost impossible to follow the notation when first reading this paper. The proofs are very lengthy and thus the reviewer did not check in detail. \n\nThe reviewer has several question:\n\n1) Of course SOC2 makes sense. But what if one models the whole problem as an HMM, and perform control algorithms in the hidden domain of the HMM (and the hidden states can be of much smaller alphabets compared to the observable states), will there be any fundamental difference? Of course learning an HMM is challenging, but approachable. Any comments?\n\n2) The three design principles make sense, but may need more elaboration. For example, it is a bit unclear why f_Z should be with low curvature -- does it mean that you wish the control problem in the latent domain is more like a linear dynamical system, so that the LLC algorithm works better? The argument is a bit unclear, since \"locally linear\" is not a rigorous term. Any smooth function is ``\"locally linear\". Here, how to measure the difficulty of the latent control problem may need more discussion.\n\nMinor: btw,  (5) may contain some typos.\n\n3) In practice, how to balance the three parameters lambda_p, lambda_c, lambda_cur?\n\n "
        }
    ]
}