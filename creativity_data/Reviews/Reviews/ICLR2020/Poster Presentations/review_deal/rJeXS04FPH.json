{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The authors design a deep model architecture for learning word embeddings with better performance and/or more efficient use of parameters.  Results on language modeling and machine translation are promising.  Pros:  Interesting idea and nice results.  New model may have some independent value beyond NLP.  Cons:  Empirical comparisons could be more thorough.  For example, it is not clear (to me at least) what would be the benefits of this approach applied to whole words versus a competitor using subword units.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "This paper describes an approach to learn word embedding functions more efficiently and with fewer parameters. This is done by replacing the embedding lookup function which is typical in NLP tasks such as language modeling and machine translation with a hierarchical embedding model. This allows for a low dimensional embedding layer, reducing total parameters and training time. A novel skip-connections architecture is introduced as a part of the \"embedding generation model\". Experiments are conducted for language modeling and machine translation tasks and performance improvements are observed with a reduction in parameters and lesser training time.\n\nThe direction of this work is nice, the problem that is being tackled is indeed important.\nThe obtained results are nice (though this can be improved) and there is indeed some potential value in this work.\nHowever, I have the following concern. The paper completely ignores a lot of previous and concurrent work in reducing the size of the embedding layer. These works are in most cases not even cited and no empirical comparisons are provided. For example, please see below works in matrix factorization approaches, sparse word representation learning, codebook learning and other quantization approaches for compressing word embeddings:\n\nhttps://www.aclweb.org/anthology/P16-1022/\nhttps://aaai.org/ojs/index.php/AAAI/article/download/4578/4456\nhttp://web.cs.ucla.edu/~chohsieh/papers/Mulcode_Compressor.pdf\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17042/16071\nhttps://storage.googleapis.com/pub-tools-public-publication-data/pdf/f158f7c81ed8e985fd51a20d193103ce427cad51.pdf\nhttps://arxiv.org/pdf/1711.01068.pdf\nhttps://arxiv.org/abs/1510.00149\n\nI would appreciate if comparisons with some of these approaches is provided in the next iteration of this work.\n\nOther suggestions:\n\n1. I think the paper would benefit from some analysis of the differences in the word embeddings learnt by a general lookup table learning model in comparison with the word embeddings learnt by this model. How are the embeddings compressed? How do the decompressed embeddings compare to the embeddings learnt by the lookup approach? More insights in the machinery via some visualizations would help.\n\n2. How do the gains of this method change as more or less training data is provided. For example, are the gains lesser on Gigaword? This would be interesting to know.\n\n3. GLT is mentioned twice in this paper. Perhaps a slightly more detailed explanation of the same would help improve the readability of this paper.\n\nBased on the presentation improvements and new experiments added to the paper in the rebuttal time period, I am updating my evaluation of this work.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper proposes a novel sparse network architecture to learn word embeddings more effectively. \n\nI am not an expert in the area of machine translation, so I am able to sanity-check the results and the reasoning and motivation given in the paper. \n\nGenerally I failed to find motivation as to why this specific architecture was chosen out of many others. I also do not understand the purpose of doing aggressive embedding expansion before another contraction. Why would this allows to learn a more efficient low dimensional embedding than the original one? This may happen to be the case, but why?\n\nOverall, the results seem to be a bit inconclusive.\nTable 1: b) DeFINE uses less parameters but also gives worse results. This does not allow me to conclude anything.\nTable 1: c) DeFINE seems to give better perplexity results, while using less parameters. This is good.\nTable 2: DeFINE uses more parameters and gives better perplexity results. I do not know what to conclude, as ideally I would like to see how would DeFINE do with the same number of parameters.\nTable 3: \"our implementation\" seems to provide much lower scores than the ones found in the literature and thus can not be used as an fair baseline. Once this baseline is discarded, DeFINE seems to be producing worse results while using less parameters. Is this good? I do no know. But certainly this is inconclusive. I do not see how this table allows to conclude the following: \"DeFINE improves the performance by 2% while simultaneously reducing the total number of parameters by 26%, suggesting that DeFINE is effective\".\n\nA few other comments:\n\nFigure 1: if m >> n, why is the bottom (green) of DeDINE network wider than the top (tellow)?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper describes a new method for learning deep word-level representations efficiently. The architecture uses a hierarchical structure with skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods.\n\n1. From table 1a or table 2, the training time of the proposed method is not reduced compared with existing methods. \n\n2. It seems the number of parameters in DeFINE still depends directly on vocabulary size. Methods proposed in [1] and [2] do not depend directly on the vocabulary size.  For dataset that has very large vocabulary size, [1] and [2] could potentially have larger compression rate.\n\n3. The experiments are detailed, and includes ABLATION studies.  \n\n[1] Variani, Ehsan, Ananda Theertha Suresh, and Mitchel Weintraub. \"WEST: Word Encoded Sequence Transducers.\" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.\n[2] Li, Z., Kulhanek, R., Wang, S., Zhao, Y., & Wu, S. (2018, April). Slim embedding layers for recurrent neural language models. In Thirty-Second AAAI Conference on Artificial Intelligence."
        }
    ]
}