{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents the neural stored-program memory, which is a key-value memory that is used to store weights for another neural network, analogous to having programs in computers. They provide an extensive set of experiments in various domains to show the benefit of the proposed method, including synthetic tasks and few-shot learning experiments.\n\nThis is an interesting paper proposing a new idea. We discuss this submission extensively and based on our discussion I recommend accepting this submission. \n\nA few final comments from reviewers for the authors:\n- Please try to make the paper a bit more self-contained so that it is more useful to a general audience. This can be done by either making more space in the main text (e.g., reducing the size of Figure 1, reducing space between sections, table captions and text, etc.) or adding more details in the Appendix. Importantly, your formatting is a bit off. Please use the correct style file, it will give you more space. All reviewers agree that the paper are missing some important details that would improve the paper.\n- Please cite the original fast weight paper by Malsburg (1981).\n- Regarding fast-weights using outer products, this was actually first done in the 1993 paper instead of the 2016 and 2017 papers.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "## Update\n\nI am changing my score to 7 (not adjusting the discretised score in the openreview interface though)\n\n\n\n\n\nSummary: The authors present the Neural Stored-program Memory (NSM), an architectural addition which is suitable for a large class of existing MANN models (NTM, DNC, LRUA etc). The model contains a second memory of key value pairs, which on each timestep can be read via cosine distance similarity on the keys, and for which the values contain all the weights necessary for the Interface Network (IN). The IN also receives the recurrent controller output, and produces an interface vector which is used to read or write to the regular memory. The definition is sufficiently general to allow the computation between an interface vector and the regular memory to be done according to NTM/DNC/etc schemes.\n\nNSM allows a MANN to effectively switch programs at every timestep, more closely matching classical computers and the Universal Turing Machine. The authors include a wide range of experiments, both showing faster learning on standard NTM tasks, and also introduce \"Sequencing tasks\" for which a single episode contains multiple instances of the standard NTM tasks in sequence. In principle this should allow the whole system to learn different programs for the subproblems, and dynamically switch between them as soon as the task changes. In principle a standard NTM could learn to do many of these tasks in a row, but the authors show that even when an NTM combined with NSM (denoted NUTM) has fewer trainable weights in total than a plain NTM, for some task sequence combinations the NUTM learns much faster.\n\nExperiments on continual learning, few shot learning and text based question answering back up the wide applicability of this technique, with many strong results including new SOTA on bAbI.\n\n\nDecision: Accept. Key reasons are that this is a relatively straightforward application of hypernetworks within a recurrent controller, which both has appealing justifications in the context of both Von Neumann and Turing models of computation. This simplicity is a positive, and the authors make a convincing argument that NSM can be applied to any MANN solving a sufficiently complex problem. A secondary reason is the extensive evaluation & hyperparam details, and while I do have some minor points on which I think the paper could be clarified (see below) I think this is overall a very nice paper. With the clarifications below addressed, I would give this paper a 7 but from the options I have available to choose from, 6 is the best fit for the current manuscript.\n\nSupporting arguments: The visualization of 'program distribution' shows extremely clear phase changes as the underlying task changes - both within a single task (the reading phase vs writing phase for repeat copy) and across task boundaries (copy -> associative recall). Combined with the learning curves / results in the various tables, it is to me clear that the model is performing as designed.\n\nDespite the name of the \"Meta Network\", and having a vague flavour of metalearning, the model does not require any elaborate \"backprop through the inner training loop\" of MAML et al, which is a benefit in my opinion.\n\n\n\n\nAdditional feedback: Some of the experiments could be slightly more convincing - particularly Figure 5 which is lacking error bars. In my experience these architectures can have relatively high variance in performance, compared to other supervised domains, as evidenced by the spicy learning curves even in Figure 2 a). Error bars across multiple runs for figure 5 would be good, particularly for the points where the lines are close (eg after training on PS, the performance for C and RC is close for both models).\n\nThe formatting of some figures and graphs could be improved:\n* Figure 1 - the graph could use some more text labels, rather than mathematical notation which needs to be referred to below. The colours are also slightly confusing - the program memory has slightly different shades of orange for the keys, slightly different shades of green for the values, whereas the regular memory has a slightly wider variety of colours. I was not sure at first whether this should be interpreted as indicating something important. Additionally, the value read from the NSM is reshaped to a 2x4 shape with various shades of blue, which then becomes the weights for a pink network? I think the colour adds little and may confuse people. There are some other issues, such as the $r_t$ value which should really come from the main memory as a result of the interface vector. With the supporting text, understanding the system is not hard, but I feel another pass over the diagram would benefit the camera ready version - consider ditching colour entirely, unless there is going to be some consistent meaning to things being the same colour vs not. Text labels with the various arrows (eg \"vector used to lookup in NSM\", \"used as network weights for Interface Network\") may improve clarity. $c_t$ should also be labelled on the diagram.\n\n* The y axis scale in figure 5 is very confusing - it took me several looks before I noticed that it goes from 50% to 100%, due to each number having digits in different vertical positions and different font sizes\n\n* Figure 2 y axis scale is a bit too small to easily read.\n\n* Figure 3: both read and write locations are shown on a single plot, but the green line that separates them is lamost unreadable on a printout. The task dependent, presumably manually chosen, approach to picking where this visualisation toggle should be made is a bit arbitrary - I would prefer to see read and write locations as separate subplots, as in the appendix.\n\n\nI found the exact details of number of programs versus number of heads (and the type of those heads) a bit confusing. In www.github.com/deepmind/dnc the code has 'number of read heads' and 'number of write heads' being two independently set integer parameters. This paper refers to \"$R$ control heads\", but I am not exactly clear on how these are divded between read and write duties. Algorithm 1 references that write heads return zero on line 8 but not other mention of the two mutually exclusive types is made. The text towards the end of section 4.1 refers to the \"no-read\" strategy being assigned mostly to one program - this makes it sound like each program can (softly?) interpolate between reading or writing (or both?). The start of appendix B shows program distribution for read and write heads separately, but this then begs the question of what is happening in the examples for which only one program distribution is shown (eg Figure 3) - clearly we need to read and write for all of these tasks, so is one head with two programs doing both simultaneously? In the interests of reproducability, clarification here is essential.\n\nSeparately, the decision of having a different program memory per control head is interesting - it's not obvious to me why this would be necessary, surely one program memory would be sufficient as long as thre is a different $P_{I,n}$ (alg 1 line 5) network to choose a different program for the head? It would be good to see a line added to the paper justifying this choice.\n\nIt seems like not all training hyperparams are specified in the appendix - eg the settings specified in Table 9 only apply to the few shot learning task, additional hyperparams are specified for bAbI, but I cannot see the training hyperparams for the experiments in sections 4.1 - 4.4.\n\nMinor correction:\n\n\"As Turing Machine is finite state automata\" -> \"As Turing Machines are finite state automata\"\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The authors discuss an interesting idea for memory augmented neural networks: storing a subset of the weights of the network in a key-value memory. The actual weights used are chosen dynamically. This is similar to having programs/subprograms in a classical computer. To the best of our knowledge, this idea was not previously studied, and it was a missing part of the study of memory augmented neural networks until now.\n\nIn the abstract authors mention a von Neumann architecture. However, the von Neumann architecture assumes that the program and the data are stored in the same memory, which is not the case for what the authors propose (a significant limitation is that the vector size of the memory doesn’t match the number of network weights). It is more reminiscent of the Harvard architecture. Please correct!\n\nAt the end of section 3.1 and in Eq 6 and 7 the authors claim that they use the regularization loss that makes the keys different to prevent programs from collapsing. However, this is only one way of collapsing. The other way is more reminiscent of what tends to happen in routing networks, where the controller network (here P_I) learns to attend to a single module (here memory slot) or attend to all of the modules with roughly equal weight. Are such collapses experienced during this work? If not, what could be the reason?\n\nThe main difference between routing networks and NSM is that in the former the selection is done in activation space, while in NSM it happens in weight space. Thus one should expect the same exploration/exploitation, transfer/inference tradeoffs, module collapse, etc to happen [1]. What makes it better/worse compared to these methods?\n\nThe method is mainly tested on methods used to test MANNs. However, because of its relation to the routing networks, it would also be interesting to compare them to [2] or [3] to see whether they suffer from the same problems.\n\nBecause the program distribution is rarely close to one-hot (all programs are contributing to some degree), could you please provide as a baseline a version where the program distribution is fixed and uniform?\n\nIn section 3.2, the last 2 sentences of the first paragraph, in one sentence the authors say “we store both into NSM to completely encode a MANN” and in the next they say “in this paper, we only use NSM to store W^C”. Please make this consistent.\n\nIn the last sentence of the first paragraph of section 3.2, the authors say “is equivalent to the Universal Turing Machine that can simulate any one-state Turing Machine”. Could you further clarify this?\n\nIn the second paragraph of section 3.2, the authors say “P_I simulates \\delta_u of the UTM”. Shouldn’t \\delta_u also include the state transition function, which is not included in P_I?\n\nSeveral sentences mention “direct attention,” which is described as a neural network producing the weights directly. Why call this attention at all? Isn’t this a form of fast weights? Probably they should be called fast weights or dynamic links. \n\nHow does this relate to the original work on this? Note that the first end-to-end-differentiable systems that learn by gradient descent to quickly manipulate the fast weights of another net (or themselves) were published between 1991 and 1993 - see the references [FAST0-3a] [FAST5] [FASTMETA1-3] in section 8 of http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html\n\nThe authors also claim to “When the memory keys are slowly updated, the meta-network will shift its query key generation to match the new memory keys and possibly escape from the local-minima”. Why? What drives the network to use new memory keys in this case?\n\nIn section 3.3, “P_I is a meta learner”. This is not the correct term here, because P_I is not leveraging old knowledge to learn faster (nor in improving the learning algorithm itself, nor in the recently more popular transfer learning sense), but it learns to select which weights to use.\n\nIn figure 2b, could you show the mean with std?\n\n“states of the generating automation” should be automaton instead.\n\n“As NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences” - Why this indication of a causal relationship here? Why would requiring fewer training samples make it generalize better?\n\nIn the sequencing tasks (section 4.3), how does the network know which task to perform? Is the task label one-hot encoded and concatenated with the input? From Fig 13 and 14, it seems to be a fixed concatenation of inputs, without any indication which task is which, and then a fixed concatenation of the outputs is requested from the network. Could you clarify, please?\n\nWe found Figure 5 very confusing. At first it seemed like the X-axis is “time” (where we assumed the network is trained on a sequence of tasks C, RC, AR, PS), and the title above the subplots indicates for which task the performance is plotted over “time” (which in this case would correspond to the time steps after completion of the training phase indicated on the X-axis). However, the second subplot shows perfect RC performance after having been trained only on C. We probably misunderstood the plots: the title plot is the “time”, and the X-axis shows just the datasets (so the order is not important) - so the plots don’t show the performance on the specific dataset over the course of training as assumed initially. But if so, why are they connected by lines and why not use a bar plot? It would be more interesting to see “time” on the X-axis, so one can see how performance degrades while training on new tasks. \n\nIn Figure 5, what is the average performance supposed to show? If you average the performance over each training phase, the dataset trained first will yield better performance than the ones trained later. This is because the last trained one will have a performance near chance for most of the time it is measured, while the first one will have the best performance on the first measurement and will degrade somewhat because of catastrophic forgetting - but hopefully it will still be better than chance.\n\nIn Figure 5, numbers on the Y-axis: the first 10 and 100 place digits are misaligned.\n\nIn the last paragraph of section 4.4, “- over 4 tasks”, the - is confusing because it is not part of the formula. Please rewrite the formula in a different style or remove the -.\n\nIn section 4.5, the authors write that the best hyperparameters they found are p=2 for 5 classes and p=3 for 10. What could be the reason for p and the number of classes being so unrelated?\n\nIn section 4.6, “DNC is more powerful and thus suitable for NSM integration” - this suggests that the reason why it is suitable is that it is more powerful, but that is not the case since it was integrated into NTM, too.\n\nIn section 5, “they impede modularity”. Why? Maybe they don’t increase it, but why impede? “which looses modularity” - Why? The original RNN is not modular either. Why is this even less modular?\n\nIn figure 7 etc. it would be nice to have a title for the topmost subplot, too.\n\nWhat is a dynamic n-grams task? Could you clarify or include a reference?\n\nCould you clarify how the white-black-yellow-orange input charts should be understood (Fig 3d, Fig 15, etc)?\n\nHow to understand the preservation plots (Fig 3d)?\n\nAdditional references besides [FAST0-3a] [FAST5] [FASTMETA1-3] mentioned above:\n\n[1] Rosenbaum et al, Routing Networks and the Challenges of Modular and Compositional Computation\n[2] Chang et al, Automatically Composing Representation Transformations as a Means for Generalization\n[3] Kirsch et al, Modular Networks: Learning to Decompose Neural Computation\n\n\nWe think this paper is quite interesting, and might improve our rating provided the comments above were addressed in a satisfactory way in the rebuttal.\n\nEdit after rebuttal: score increased to 8. However, the authors should not forget to cite the original fast weight paper by Malsburg (1981). And one more thing: in the revised version they write \"More recent works implement fast-weight using outer-products,\" citing papers from 2016-17, but this was actually first done in the 1993 paper they already cite. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "= Summary\nA variation of Neural Turing Machines (and derived models) storing the configuration of the controller in a separate memory, which is then \"softly\" read during evaluation of the NTM. Experiments show moderate improvements on some simple multi-task problems.\n\n= Strong/Weak Points\n+ The idea of generalising NTMs to \"universal\" TMs is interesting in itself ...\n- ... however, the presented solution seems to be only half-way there, as the memory used for the \"program\" is still separate from the memory the NUTM operates on. Hence, modifying the program itself is not possible, which UTMs can do (even though it's never useful in practice...)\n- The core novelty relative to standard NTMs is that in principle, several separate programs can be stored, and that at each timestep, the \"correct\" one can be read. However this read mechanism is weak, and requires extra tuning with a specialized loss (Eq. (6))\n~ It remains unclear where this is leading - clearly NTMs and NUTMs (or their DNC siblings) are currently not useful for interesting tasks, and it remains unclear what is missing to get there. The current paper does not try show the way there.\n- The writing is oddly inconsistent, and important technical details (such as the memory read/write mechanism) are not documented. I would prefer the paper to be self-contained, to make it easier to understand the differences and commonalities between NTM memory reads and the proposed NSM mechanism.\n\n= Recommendation\nOverall, I don't see clear, actionable insights in this submission, and thus believe that it will not provide great value to the ICLR audience; hence I would recommend rejecting the paper to allow the authors to clarify their writing and provide more experimental evidence of the usefulness of their contribution.\n\n= Minor Comments\n+ Page 6: \"As NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences.\" - I don't understand the connecting between the first and second part of the sentence. This seems pure speculation, not a fact."
        }
    ]
}