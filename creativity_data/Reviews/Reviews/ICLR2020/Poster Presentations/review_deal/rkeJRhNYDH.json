{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents a new dataset for fact verification in text from tables. The task is to identify whether a given claim is supported by the information presented in the table. The authors have also presented two baseline models, one based on BERT and based on symbolic reasoning which have an ok performance on the dataset but still very behind the human performance. The paper is well-written and the arguments and experiments presented in the paper are sound.\n\nAfter reviewer comments, the authors have incorporated major changes in the paper. I recommend an Accept for the paper in its current form.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "Updated review:\nThank you for addressing the comments and making relevant edits. Additionally, the HIT section provides a lot more insights into the data collection process. I've updated my score based on the responses/edits made.\n\n------\n\nThis paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. \n\nApart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I'm inclined towards accepting the paper. \n\nA few remarks/concerns are:\n1. Usefulness of the dataset:\nIt seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (‘there are five candidates in total’). With a binary space for supervision, we don’t really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to “do well” without “learning well”, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014).\n2. Related work:\nThe paper talks about introducing a new ‘format’ of evidence (structured text) and talks about ‘unstructured text’ as the only ‘other’ format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with ‘textual’ evidence.\n3. The dataset statistics in Table 1 don’t seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)).\n4. Page 3, Section 2.3: “we further perform quality control” -> a line or two to explain quality control?\n5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study.\n\nMinor remarks:\n- Page 2: Section 2: \n1. overtly -> overly\n2. huge tables(e.g. -> huge tables (e.g.\n\n- Page 3: Section 2.3\n1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements\n\n- Page 4: \n1. candidate) . we need to  -> candidate), we need to",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This work proposes the problem of fact verification with semi-structured data source such as tables. Specifically, the authors created a new dataset TabFact and evaluated two baseline models with different variations. They applied two criteria and different rewards for workers to collect two subsets of different levels (“simple” and “complex”). They also applied a negative rewriting strategy to avoid exploitable cues or patterns in the annotations. They evaluated two baselines models: (1) latent program algorithm (LPA), which makes use of simple string match entity linking and systematic search and trains a neural network discriminator, and (2) Table-BERT, which linearize the table into a sequence through concatenation or template, and treat it as a classfication problem. Both showed reasonable accuracy (~68%), but still below human performance (92.1%) on a held out test set.\n\nI would like to see the paper accepted because:\n(1) it proposes an interesting task (table fact verification) with a clean dataset, and the experiments evaluated the ability of the current neural network models, such as BERT, or hybrid models, such as the LPA baseline, to perform (symbolic) reasoning;\n(2) special care is done to ensure the dataset doesn't contain simple cues or patterns, which is a common pitfall in dataset collection, and the dataset is also validated through two reasonable baseline models. \n\nSome weakness and concerns are:\n(1) some error analysis of the baseline models are missing. For example, what types questions are hard/simple for table-BERT and what are hard/simple for LPA, and some comparison between them. This helps point out where the difficulty come from, for example, whether the difficulty is language understanding or symbolic reasoning.\n(2) \"To focus on statement verification against the table, we do not feed the caption to the model and simply mask the phrases in the statements which links to the caption with placeholders.\"\nI am not sure this is the right thing to do since even the human annotators requires the caption to understand the context, why not also feed the caption into the model? \n(3) Although the Figure 2 showed that the higher order operations are indeed used in a majority of questions, which measures the breadth of the reasoning, it is unclear about the depths of the required reasoning, i.e., how many operations / steps are required to achieve the correct answer. It would help if the average number of steps / operations required for answering the questions are shown. \n\nIf the above concerns are addressed, I will be willing to raise my score.\n\n\nMinor comments:\n\nThe paper, especially the Appendix, requires some proof reading, for example, I believe the caption for Figure 5 in Appendix A is misplaced.\n\n\"... they are explicitly banned bring ...\" -> \"banned from bringing\"\n\n“As illustrated in Figure 6, the title only acts as a placeholder in the statements to make it sound more natural.” -> From the example, it seems the name of the player is kept unchanged in the sentence, which is different from a placeholder.\n\nSuggestions:\n\nI understand this might require some works, but it would be really helpful to add more comments and maybe examples for the functions shown in Figure 5 (you might need to split the table into two pages or have another table for examples), so that it can be used by works that follows the LPA approach.\n\n“During the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent from artificial cues”\nAvoiding simple cues or patterns are important, so it will be good if more details can be shared (for example, the instructions/guidelines you showed to the workers). \n\n==================================\n\nPost rebuttal update:\n\nThanks for the update to the paper. I think this work provides a good dataset and some reasonable baselines, thus should be accepted. \n\nHowever, I am still a bit concerned about the categorization of questions and the analysis on reasoning depth, because they are all based on the programs generated by LPA, while systematic search's recall is just 77% and the programs potentially contains a lot of spurious ones. So the categorization of the questions in Figure 11 and the number of reasoning steps in Figure 12 has to be taken with a grain of salt. It is worth annotating a few hundred, say 100-300 questions, manually with the ground truth programs for question distribution analysis or reasoning depth analysis, and confirm the numbers estimated using programs from LPA. Those manually annotated examples would also be a great addition to the dataset to aid the researchers for better analysis. A good example is the WikiTableQuestions dataset https://github.com/ppasupat/WikiTableQuestions/releases, which contains 300 manually annotated examples (annotated-all.examples), and they are used to analyze the distribution of the questions in the paper. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #1",
            "review": "This paper proposes a new dataset for table-based fact verification and introduces a couple of methods for the task. I think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ML and NLP are overstated in my opinion. In addition the paper needs proof reading as there are many typos, some of which make comprehension problematic. In detail:\n\n- The dataset is the main contribution of this paper. Its size is great. However I have some concerns on its construction. The guidelines described for constructing simple and complex claims are rathe vague, e.g. how does one define \"too much symbolic reasoning\" and explain it to crowdworkers? How was the linguistic complexity difference between the two channels measured? How were the claims sanity checked? In the beginning of section 2.3 it is stated that quality control filtered a substantial proportion of the statements. How was this done?\n\n- A troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the Entailed/Refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion. Thus it will always be possible for models to score highly without doing the right thing. Avoiding trivial re-writes helps, but it is unlikely to be enough as human crowd workers will try to optimize their earnings.\n\n- The two models discussed are OK as baselines, but not particularly interesting or appropriate. Both require substantial rule-based processing (named entity linking, latent program construction. templates) and eventually linearize structured data (the program or the table). I understand that this is not the main contribution of the paper, but given the substantial amount of work on semantic parsing and question answering I was expecting more appropriate baselines taking previous work into account. The performance of the model is not great either, especially if we consider that they are only evaluated on returning a binary label, not the correct evidence from the table.\n\n- While it is true that most of the fact verification work has focus on textual sources, the challenge of combining reasoning over continuous and discrete representations is not new. The various QA works mentioned in the related work section address this, as well as work on theorem proving: https://arxiv.org/abs/1705.11040 Furthermore, there has been at least one more previous work on table based verification against  FreeBase tables: https://www.aclweb.org/anthology/D15-1312/. Thus I believe the discussion of the challenges posed by this dataset should be re-framed, especially given that the kinds of programs that need to be constructed are of similar complexity to previous work like the WikiTableQuestions.\n\n- writing: \"the model is expected to excel... but to fall short\", \"we follow the human subject research protocols\" (which ones?), \"in case of obvious stylistic patterns\" (which ones). On the whole it is understandable, but the writing should be improved.\n\n---- Post author response\n\nI have responded to the author response and I have revised my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}