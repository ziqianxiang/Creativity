{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper offers a new method for scene generation.  While there is some debate on the semantics of ‘generative’ and ‘3d’, on balance the reviewers were positive and more so after rebuttal.  I concur with their view that this paper deserves to be accepted.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The paper proposes a generative model for images. There's a probability mask per-pixel per-component (which yields mixing probabilities), and then a set of latents per-component that yield an image. The system is tested on a set of scenes like the GQN dataset, stacks of blocks, and the multi-dsprites dataset. The system is better than MONet, although there are a few lingering questions.\n\nSummary of positives:\n+ The factoring of the image into various components is eminently sensible and more work should build in the notion of objects\n+ The method is well explained, and I found it easy to understand the entire process.\n+ The method does appear to perform better than MONet, and qualitatively produces good results.\n\nSummary of negatives:\n- There are a few overclaims that should be fixed: namely that the system is a \"generative model of 3D visual scenes\" when in reality it is a generative model of images that has a latent space that is perhaps well-positioned to match images generated by 3D scenes composed of objects.\n- The method section sets up a few questions that are never answered in the paper: GENESIS--S gets introduced as a baseline to test a hypothesis, but this never gets quantitatively evaluated (indeed GENESIS-S beats GENESIS in 2/3 of the categories where they are compared head-to-head); similarly, the fact that GENESIS produces probabilities is repeatedly sold as an advantage over other methods, but it's never used.\n- The experiments are weak: the aforementioned questions aren't tackled, as well as a few other issues.\n\n\nOverall, I lean ever so slightly towards rejection. The results are good looking, and the method seems well-explained. However, in my view, the manuscript seems to fall short of the mark. I am not, however, strongly opposed to the paper's acceptance. If it is accepted, I would urge authors to address the issues to make their paper have maximum impact.\n\nBig picture things:\n-I'm baffled by the repeated claim that this is a generative model of 3D scenes. In the introduction \"first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes\". I'm worried I'm just missing something profoundly obvious and critical -- since it seems obvious to me that it's not 3D. \n\nAs far as I can tell, x here is a HxWxC image (where C is the number of channels and probably 3); the mixing probabilities are over HxW images, so it's a 2D segmentation system. It may be applied to photos rendered with a perspective camera -- but if the requirement for the system is being applied to images that come via projection, then is faster RCNN a 3D object detector? Are normalized cuts, SLIC superpixels, or any of the other myriad unsupervised segmentation methods from vision then 3D scene segmentation approaches?\n\n-Similarly, it's not really a model for objects -- as can be seen in the multi-colored wall example in Fig 3, and Fig 8, 9. its notion of object is a region with uniform color (presumably due to the Gaussian p(x|z) ). So while it's great that it can model regions of uniform color, the system in large part seems to work (in its current form, reading the current manuscript) because the objects are all one color, and doesn't necessarily decompose the scene into objects.\n\nUnless I'm totally missing something, this sort of claim that should be corrected very quickly. I realize that my personal bias is towards more engineering and harder data and I appreciate that there should be different operating points along the spectrum of how much information is provided. However, I think that it's worth making claims and assumptions clear, rather than claiming to solve the general problem on a specific case because of peculiarities about the specific case.\n\n\nMethod:\n-The writing of the method is quite straightforward and written well. The system is clear to me. I largely have no complaints about the method. I do, however, have concerns about the experiments that are done to validate claims in the method section.\n-GENESIS-S appears as this baseline \"To investigate whether separate latents for masks and component appearances are necessary for decomposition\", but then basically disappears. There are, as far as I can see, no qualitative results from it, and it basically appears once quantitatively, in S 4.3 / Table 1, where it appears to be as good if not better than GENESIS. In the appendix it's claimed that GENESIS trains faster than GENESIS-S, but if this is the selling point of GENESIS over GENESIS-S, it would be nice to have at least a little quantification of this.\n-There is considerable fuss made over the fact that the system produces probabilities and this is repeatedly mentioned as an advantage over existing systems, but it is never demonstrated that these probabilities are good or useful. This leaves the reader hanging a bit. \n\nExperiments:\n\nOverall the experiments are a little on the weak side.\n\n+The qualitative results are good, but the primary selling point it seems is that the system is better than MONET. This does seem clear, and GENESIS does appear quite a bit better.\n\n- While the results on factoring scenes do appear to be good, there's no quantitative evaluation of this (although the abstract promises it). Section 4.2 essentially says that ARI is bad, without demonstrating in a compelling way that it is. The paper says that this behavior can be seen in Appendix D, but this only shows the oversegmentation and not that this dramatically throws off performance -- I'd expect a figure showing a reversal of expectations -- a clearly worse result getting a clearly better ARI metric. \n\nAdditionally, computer vision has been evaluating unsupervised segmentation for close to two decades (BSDS came out in 2001). The authors should look at the metrics in e.g., \"Contour Detection andHierarchical Image Segmentation\" Arbelaez et al. PAMI 2010. I realize that coming up with metrics is hard, but I think the burden is on the authors to find the metrics to show their conclusions quantitatively.\n\n- The tower stability/height/view experiments seem incomplete. I find them quite interesting, but I'm not entirely sure of what to draw from it. The paper obliquely comments that the Groth paper gets better results with a more complex backbone network and blames it on lack of augmentation, use of a subset of the data, and a reduced resolution. But why not just do straightforward like train a MLP on an even further spatially downsampled image? Random here isn't a particularly compelling baseline or reference point. \n\nSmall stuff that doesn't affect my review:\n1) It might be worth explicitly pointing out that p_\\theta(x_k,z_k^c) is just a Gaussian soon after Eqn 3. It's written after equation 5, but without making the p_\\theta explicit. This may help some readers.\n2) The tower stability is done with a one-hidden-layer (512) MLP. I'm of the personal opinion that readouts on latent variables are more informative if it's a linear transformation (since this can't do much heavy lifting).\n3) The abstract promises semi-supervised learning -- is this the tower experiment? This strikes me more as transfer learning.\n\n-----------------------------\n\nPost review update: I have read the authors' responses, and found them thoughtful and to have answered my questions. I'm happy to accept the paper and would encourage the AC to do so.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose a probabilistic generative latent variable model representing a 2D image as a mixture of latent components. It formulates the scene generation problem as a spatial Gaussian mixture model where each Gaussian component comes from the decoding of an object-centric latent variable. The contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents. This allows the model to capture autoregressive dependencies among different components and thus help generate coherent scenes, which has not been shown in the previous works. In the experiments, the authors compare GENESIS with MONet and VAEs qualitatively and quantitatively and show that the model outperforms the baseline in terms of both scene decomposition and generation.\n\nThe proposed model seems like the right direction to improve upon MONet and it is nice to see the generation results. It is also nice to see the fully probabilistic modeling of the problem. Although it would improve over the MONet, I'm nevertheless not sure if the framework of sequential component generation (applying both MONet and GENESIS) can be a robust approach to more complex scenes, e.g., with a larger number of objects. Also, the mixture-based approach seems not guarantee the object-level decomposition. For example, if in the scene there are many objects of the same shape, size, and color, I think the proposed model may not properly distinguish them, as shown in some of the wall patterns in the experiments. But, I'm not sure what would happen if K is set to a large number to deal with this. Then, it would have the problem of long-term dependency in sequences.\n\nSome comments and questions:\n1. It would be good to show the qualitative result of the simplified Genesis-s and its qualitative result in the FID experiment.\n2. Is the VAE baselines (BD-VAE and DC-VAE) trained by maximizing the ELBO? If it is the case, will the result of the FID experiment section be different if we train the VAE baseline with GECO?\n3. Some detail of the implementation is missing, e.g. how do we model the posterior q_{\\phi}(z_{k}^{c} | x, z_{1: k}^{m}).\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "UPDATE: I appreciated the authors' discussion. The authors addressed my questions satisfactorily, and I maintain my original rating of accept.\n\n----\nSummary: This papers tackles the question of building an object-centric latent variable generative model of scenes that can sample novel scenes with coherent objects and relationships. To do this, the authors define a generative model, GENESIS, that uses an autoregressive prior over mask variables. The component variables are generated conditioned on these mask variables. The visual appearance of the objects are generated conditioned on the component variables. Inference is done sequentially by inferring some later object varaibles conditioned on others. Results show that the model adopts a consistent strategy in generating and inferring the scene components, first considering the background then the foreground objects. The authors apply GENESIS to three datasets with monochromatic objects and show that GENESIS qualitatively generates coherent scenes and infers coherent scene components.\n\nDecision: Accept. This work clearly addresses a problem beyond current object-centric modeling approaches such as IODINE and MONet, which is the problem of generating novel scenes.\n\nStrengths:\n- The paper is well written and executed.\n- The evaluation is thorough\n- The problem and solution are well motivated\n\nWeaknesses: \n- While the authors demonstrates that GENESIS is able to model static scenes, it is not clear how straightforward it is to extend GENESIS to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation). Whereas approaches such as IODINE or RNEM (van Steenkiste et al. 2018) treat the object latent that can be propagated through time, maintatining that the same latent models the same object may not be a guarantee for autoregressive approaches such as MONet or GENESIS that re-parse the scene at every frame. Object temporal consistency is especially important when considering tasks that have occlusion, which are important problems in robotics.\n- The results in Appendix D seem to suggest that GENESIS decomposes a scene mostly via color segmentation, as IODINE and MONet do. One concern is that such models that rely mainly on color segmentation are not applicable for real world robotics with various lighting conditions and textures because segmenting based on color may not provide coherent object representations. Would the authors be able to provide an empirical analysis of how GENESIS models a real-world scene, analogous to Figure 11 in the IODINE paper?\n\nVan Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}