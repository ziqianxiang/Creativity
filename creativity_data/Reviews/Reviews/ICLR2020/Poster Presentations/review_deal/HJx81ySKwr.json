{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposed to use an autoencoder based approach for anomaly localization. The method shows promising on inpainting task compared with traditional auto-encoder.\n\nFirst two reviewers recommend this paper for acceptance. The last review has some concerns about the experimental design and whether VAE is a suitable baseline. The authors provide reasonable explanation in rebuttal while the reviewer did not give further comments.\n\nOverall, the paper proposes a promising approach for anomaly localization; thus, I recommend it for acceptance.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper improves anomaly detection by augmenting generative models (VAE, etc) by iteratively projecting the anomalous data onto the learned manifold, using gradient descent of the autoencoder reconstruction term relative to the image input. The work seems related to AnoGAN, only instead of iterating over the latent space, the iteration is over the more expressive input space. The method is intuitive and a good parallel to Adversarial projections is made in the paper. To the best of my knowledge, the idea is novel, although I am not completely sure. \nThe second idea in the paper is to scale the losses by the reconstruction accuracy, which also is intuitive and shown to significantly speeds up the model convergence. \n\nThe experimental results are pretty convincing, showing both quantitatively and qualitatively that the method improves consistently over using the underlying vanilla generative models (AE/DSAE/2 VAEs). One desirable improvement is to get error bounds on the results, those are currently missing. Also, based on the inpainting results in Fig 7, it's not really clear if the method generates better results than Ivanov et al. \n\n\n\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper discusses an important problem of solving the visual inspection problem limited supervision.  It proposes to use VAE to model the anomaly detection. The major concern is how the quality of f_{VAE} is estimated. From the paper it seems f_{VAE} is not updated. Will it be sufficient to rely a fixed f_{VAE} and blindly trust its quality?\n\nDetailed Comments:\n- Table 1: It is not clear how \"the mean improvement rate of 9.52% over all baselines\" was calculated.\n- Figure 3: Will VAE-grad or DASE-grad perform better? Since these base lines are used in other places, it is better to compare with them as well. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: The paper proposes to use autoencoder for anomaly localization. The approach learns to project anomalous data on an autoencoder-learned manifold by using gradient descent on energy derived from the autoencoder's loss function. The proposed method is evaluated using the anomaly-localization dataset (Bergmann et al. CVPR 2019) and qualitatively for the task of image inpainting task on CelebA dataset.\n\n\nPros: \n\n+ surprisingly simple approach that led to significantly better results. \n\n+ applications to image inpainting, and demonstrates better visual results than using simple VAE.\n\nConcern :\n\n- While I agree that authors have shown relative performance compared to various approaches,  I am not able to map the results of Table-1 to that of Table-3 (second column ROC values) in Bergmann et al. CVPR'19. The setup in two works seem similar. Can the authors please comment to help me understand this difference?\n\n- The proposed approach leads to better performance over the baseline models; it is not clear what is a suitable baseline model for the problem of anomaly localization is?\n\n- The results for image inpainting looks promising. The authors may want to add comparison with existing image inpainting approaches for the reader to better appreciate the proposed approach."
        }
    ]
}