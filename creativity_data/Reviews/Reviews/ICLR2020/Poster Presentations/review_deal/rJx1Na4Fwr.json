{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The submission proposes a robustness certification technique for smoothed classifiers for a given l_2 attack radius.\n\nStrengths:\n-The majority opinion is that this work is a non-trivial extension of prior work to provide radius certification.\n-The work is more efficient that strong recent baselines and provides better performance.\n-It successfully achieves this while avoiding adversarial training, which is another novel aspect.\n\nWeaknesses:\n-There were some initial concerns about missing experiments and unfair comparisons but these were sufficiently addressed in the discussion.\n\nAC shares the majority opinion and recommends acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "This paper improves the robustness of smoothed classifiers by maximizing the certified radius, which is more efficient than adversarially train the smoothed classifier and achieves higher average robust radius and better certified robustness when the radius is not much larger than the training sigma. It proposes a novel objective which is derived by decomposing the 0/1 certified loss into the sum of 0/1 classification error and 0/1 robustness error. Three conditions are identified to make the optimization doable. Two surrogate losses (CE and hinge loss on the certified radius) for the two 0/1 errors are proposed as upper bounds of the 0/1 loss. Certified radius is derived as a function of the logits of Soft-RS to make the hinge loss differentiable. Numerical stability of the proposed objective is also analyzed by showing its gradient is bounded.\n\nIn general, the paper is well-written and the proposed objective is novel to my knowledge. I tend to accept the paper. Still, I am not sure about how much MACER improves upon the baselines, and would like to ask some questions.\n\n1. Cross entropy is used as a surrogate for the 0/1 classification error. This is true for all cases (including all experiments in this paper) except for binary classification, where the cross entropy is less than 1 when the score on the correct class is around 0.5. It is not important but would be better if you could mention this point.\n\n2. Have you ever tried using a tighter upper bound for the 0/1 classification error, e.g., using cross entropy loss only for the wrongly classified samples? How does it affect the results?\n\n3. Despite showing better results, MACER seems to be using much more epochs than the two baselines (but the total hrs is smaller than (Salman et al. 2019)). Also, MACER is using a much larger k than (Cohen et al., 2019). From Figure 3 (a) we can see a larger k improves the result a lot, and from Figure 3 (b) it seems that setting lambda to a non-zero value only improves the accuracy when the radius is large. For fair comparisons, could the authors give the ACR with different values of lambda while keeping other hyper parameters unchanged? Is Salman's method still not as good when using the same number of epochs?\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new approach to training models robust to perturbations (or 'attacks') within an l_2 radius, by maximizing a surrogate---a soft randomized smoothing loss---for the *certified radius* (a lower bound for the l_2 attack radius) of the classifier.  This approach has the advantage of not needing to explicitly train against specific attacks, and is thus much faster and easier to optimize.  The authors provide certain theoretical guarantees and also demonstrate strong empirical results relative to two baseline approaches.\n\nThis work builds on prior work where the goal of training a classifier that is robust to attacks is phrased as maximizing the *robust radius*, the largest l_2 ball within which a data point x can be perturbed without changing the (correct) classifications of trained classifier.  Since directly maximizing this robust radius is intractable, prior work seeks to derive a lower bound which the authors term the *certified radius*.  In order to directly maximize this, the authors use randomized smoothing- in which a randomly Gaussian-smoothed version of classifier f is used to make predictions- and then motivate and develop a \"soft randomized smoothing\" lower bound surrogate of the certified radius to maximize, which is differentiable and provably numerically stable.\n\nOverall, this paper is well explicated, starting with clearly written background on basic concepts and prior work, stating clear desiderata that the surrogate loss being developed should satisfy, and then providing theoretical proofs as to this.  The experiments are then thorough including core and ablation experiments to showcase the method.\n\nOne downside is that the paper does make fairly aggressive claims (e.g. \"performs better than all existing provable l_2-defenses\"), but then only compares to two prior / baseline approaches in the experiments.  Given the density of the field recently, this seems a bit sparse (although this reviewer is not an expert in this area)?"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents a method for training a certified robust neural network, based on the certification method of cohen et al.\n\nI think this paper is quiet borderline, as it is a natural extension of cohen et al, but is incremental. The authors use a hinge surrogate loss with smoothing to make the certification differentiable and optimize it, getting results that are on par with Salmans et al.\n\nDetailed remarks:\n - From table 3 it seems like MACER was trained for more then the other models, up to almost x3 times more on cifar. This makes the comparison unfair and puts the results into question.\n- It was nice that the authors gave a theoretical guarantee for the soft-RS, but it is not clear if that is needed. You can train with soft-RS and apply the hard-RS at test time which has the standard guarantees\n- Fig. 3 needs labels on x&y axis\n-  The ablation study isn't really an ablation study, it is more testing the sensitivity of various parameters (which is good in itself). However the gamma results and claims are hard to see from the plot.\n\n"
        }
    ]
}