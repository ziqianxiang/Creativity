{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The authors introduce a new associative inference task from cognitive psychology, show shortcomings of current memory-augmented architectures, and introduce a new memory architecture that performs better with respect to the task. The reviewers like the motivation and thought the experimental results were strong, although they also initially had several questions and pointed to areas of the paper which lacked clarity. The authors updated the paper in response to the reviewer's questions and increased the clarity of the paper. The reviewers are satisfied and believe the paper should be accepted.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThis paper presents a new task (paired associate inference), drawn from cognitive psychology, which requires linking many pieces of information together to make inferences with long range dependencies. Experimental results show that standard memory architectures fail on these tasks. To redress this, the paper proposes a new memory architecture with several new features that allow for much better performance on the paired associate task. Finally, the paper undertakes systematic experiments on more traditional domains like shortest path problems, showing that the new architecture achieves modest improvements.\n\nMajor comments:\n\nOverall this is an interesting and useful work which uses a task from cognitive psychology to illuminate reasoning limitations in prior memory augmented neural networks. It then proposes several architectural and algorithmic fixes to improve performance. The resulting solution appears to work based on the experimental evaluation, and although this does not lead to substantial improvements in the state of the art on the bAbI dataset, it seems likely that these improvements could be useful for even harder task settings. \n\nThe new algorithmic improvements are justified mainly by their performance in experiments, which is completely acceptable if the experiments have been done to a high standard. Nevertheless, it would be wonderful if more insight could be gained about why exactly this configuration of architectural changes was selected and what they contribute. The ablation experiments go some way to addressing this question and perhaps the paper would benefit from a greater discussion of these results in the main text.\n\nThe experiments appear to have been done to a high standard, and it is promising that the methodâ€™s advantages are more pronounced as the task gets harder (eg by demanding longer chains of inference).\n\nThe paper could also be improved by more clearly describing how hyper parameters were selected for each experiment (including the different ablation studies). Why were hyperparamters sometimes chosen based on training loss and other times chosen based on validation loss? In general it would be useful to explain the selection procedure very carefully for each result, since it appears to have differed between evaluations.\n\nThe paper is reasonably easy to follow, but could be streamlined to enable more discussion of material that is currently in the appendix.\n\nTypos:\n\nENM->EMN on pg2\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "Summary:\n\nThis paper proposes two main changes to the End2End Memory Network (EMN) architecture: a separation between facts and the items that comprise these facts in the external memory, policy to learn the number of memory-hops to reason. The paper also introduces a new Paired Associative Inference (PAI) task inspired by neuroscience and shows that most of the existing models including transformers struggle to solve this task while the proposed architecture (called MEMO) solves it better. MEMO also works well in the shortest path finding tasks and bAbI tasks.\n\nMy comments:\n\nOverall, I see this paper as an improvement over EMN. The proposed PAI task can be seen as an example task where Transformers struggle while recurrent architectures learn better. Interestingly, the authors use a separate halting policy network to reduce computation time.\n\n1. Section 2.1 requires more clarity. There is a confusion in the usage of I and S. I represents the number of stories or the number of sentences in the stories?\n2. Scaling up NTM/DNC to larger memory work was done in Sparse Access Memory (SAM) by Rae et al. 2016. This needs to be included in section 3.1\n3. In Table 2, why is the prediction of the second node easier for the model than the prediction of the first node? I see this trend only for EMN, UT, DNC. Not in MEMO.\n4. Are the authors willing to release the code and data to reproduce their results?\n\nMinor comments:\n\n1. Page 2, second para: ENM should be EMN.\n2. Vec inverse in Eqn 14 was never introduced.\n3. Table 3: The notation of (20/20) was never introduced. I can guess what it means. But please be explicit.\n\n===============\nAfter rebuttal: Authors have addressed all my questions. I  recommend  \"Accept\".\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}