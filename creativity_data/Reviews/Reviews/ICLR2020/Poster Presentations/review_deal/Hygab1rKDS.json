{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "Four reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission. Especially, the authors should take care to make this paper accessible (understandable) to the ML community as ICLR is a ML venue (rather than quantum physics one). Failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This submission proposed a quantum convolutional neural network (QCNN). The theoretical results in section 3 state the existence of the QCNN satisfying certain conditions. The QCNN is given by sections 4 and 5, with empirical evaluates in section 6. This subject is out of my usual area. However, I tend to think this subject is interesting to the ICLR audience due to the recent advancement in quantum computing.\n\ntitle, remove \"conference submissions\"\n\nSection 2, introduce QRAM.\n\nSomewhere around section 2-3, and in section 6, it has to be mentioned whether the proposed QCNN requires special hardware, and what is the hardware, and why it is required.\n\nNote the cited Cong et al. (2018) has been published in nature physics. As you both used the term \"QCNN\", it is better to explain more clearly what is the main difference in the main text."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors provide a comprehensive study, with theory and classical simulation of the quantum system, on how to increaese the speed of CNN inference and training using qubits. They proved an intrigued compilation of a quantized convolutional system.  \n\nFor an audiance who are not quantum experts one could clarify a few properties of the described \"quantization\".  It is not able to take multiple images in at the same time as quantum superpositions. Sometimes the expectence of a quantum machine leanring is that it would train the system at a single instance. Here the increase in effciency comes from being able to perform marix multiplication in quantum realm.\n\n The manuscripd describes a creative way to bring bolean operartions-defined non-linearities into quantum neural networks, at the cost of having to force the system back to classical domain at each layer - and then encoding it back to qubits for the next layer operations. The price has to payed as unitary operators  (the ones that preserve entaglement) are inherently linear.\n\nRemarks:\nThe authors should point out how this is specific to convolutional neural networks. It looks to me that the same algorithm could be used for fully connected or even attention based systems, as it is just a matrix multiplication as well. Anyway, the manuscript provides the take into account the steps required specifically for a CNN.\n\nSome minor remarks:\nFor classical systems the capped Relu is inferior as it reduces the range of values of activations where there is driving force. Sometimes one is using a parametrized version of ReLU that has a small positive slope for negative values.\nIt is not clear to me how this would not be the case with a quantum implementions. In your simulations, does the value of the saturation constant C, change the speed of convergence to a good solutions. \n\nThe capped Relu reminds me a lot of a Tanh non-linearity  that works well for LSTMs but are not very good for CNNs.\n\nI would suggest that for the conference presentation the authors try to bring out the essential within a less formal setting to open it to a wider ML audience.  For the manuscript the level is good with a proper use of appendix to shorten the main narrative.\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. This algorithm has complexity bounds that would open up (for instance) the possibility of exponentially large filter banks, and the authors show through a simple, classical simulation approach that the resulting network is also likely to be trainable.\n\nFeedback:\n\nA few typos/formatting issues:\n- The title accidentally includes \"Conference Submissions\"\n- The in-text citation format frequently has the parentheses in the wrong place; this is surprisingly distracting!\n\nPreliminaries:\n- Maybe explain what the ith vector in the standard basis is in terms of |0> and |1>? I assume the answer is along the lines of |000>, |001>, |010>, etc.?\n\nMain results:\n- The sentence \"a speedup compared to the classical CNN for both the forward pass and for training using backpropagation in certain cases\" is ambiguous; does \"in certain cases\" qualify only training speed or also forward pass speed?\n\n- There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks?\n\n- Can you briefly justify (or cite) the claim that \"most of the non linear functions in the machine learning literature can be implemented using small sized boolean circuits\"?\n\n- I'm a little confused about the discussion of quantum importance sampling on page 4. Could you give some intuition for the relationship between eta and the fraction of output values that are on average flushed to zero (is this 1 minus sigma?), and perhaps connect this to the literature about activation pruning and sparse NNs?\n\n- Maybe define what you mean by \"tomography\" for ML folks without the quantum background?\n\n- I'm convinced by the simulations, even though I shouldn't really be convinced by anything on MNIST... It just seems like the perturbations you're applying are all things that modern neural networks take in stride.\n\n- The discussion of using a sigma-based classical sampling rather than the eta-based quantum importance sampling mentions a \"Section C.1.15\" which does not exist (I think you mean the end of Section C.1.5).\n\n- Re: \"We will use this analogy in the numerical simulations (Section 6) to estimate, for a particular QCNN architecture and a particular dataset of images, which values of Ïƒ are enough to allow the neural network to learn.\" My understanding is that you're getting empirical estimates of which values of sigma are enough; it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks).\n\n- The sampling procedure based on sigma might be inefficient in your PyTorch implementation, but it's certainly something that GPUs are fairly well suited to computing. There might be other PyTorch operators that would help here (perhaps Bernoulli sampling?) or if nothing else you could write a small custom CUDA kernel."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a quantum version of the convolutional neural networks. They derive equivalent versions the computation of both the convolution operation and the back-propagation in the quantum computing framework. The authors claim a potential exponential speed up in the computation in the size of the kernel which would make possible to process much bigger inputs or just speed up current tasks involving images mainly. They exemplify the method on MNIST using quantum artifacts simulation using PyTorch and show their method is competitive with SoA.\n\nI think the paper is well written and provides a nice discussion about how quantum computing can be applied to CNNs. I appreciate that both the forward and backward passes are studied although most of the technical details are in appendices. So, I felt the paper  itself was a bit optimistic about the impact of quantum computing on CNNs.\n\nEspecially, I found the experimental section was missing details. I am not a quantum computing expert but I was a bit surprised that, in the context of CNNs, the introduction of quantum noise was just considered as introducing noise in the image and the gradient and then applying normal CNNs to the resulting image. Standard CNNs can probably deal with such noise and noisy gradient descent is not a big issue as such (it can even avoid local minima). But CNNs are notoriously known to reduce the number of weights in a network because of weight sharing. So, it is not all about making one convolution faster but also to compute invariant representations by sharing weights over different parts of the inputs. I would have been interested by a discussion about how quantum noise may impact this property and I didn't find this in the paper nor the appendices. \n\nAlso the author confess that the learning is not stable and results on MNIST to be the best they could get. I think it would be worth testing on large scale problems and see whether larger kernels with such noisy conditions would really improve the performance. "
        }
    ]
}