{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper investigates how two means of learning natural language - supervised learning from labeled data and reward-maximizing self-play - can be combined. The paper empirically investigates this question, showing in two grounded visual language games that supervision followed by self-play works better than the reverse. \n\nThe reviewers found this paper interesting and well executed, though not especially novel. The last is a reasonable criticism but in this case I think a little beside the point. In any case, since all the reviewers are in agreement I recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper investigated how two conflicting learning objectives; supervised and self-play updates could be combined with a focus on visual-grounded language tasks. With a different set of their combinations, the authors empirically found that alternating two learning updates may result in the best equilibrium state; consistency with samples in the supervised dataset and optimal state with high rewards in the task environment.\n\nThe paper is very well-written, and I really enjoyed reading it overall. There are some typos, presentation issues, and minor format issues (e.g., wrong naming) though. I do like this kind of simple but insightful result with enough empirical observations and discussions. Even though there is not that novel method proposed, the overall message found from the experiments, their interpretation by the authors, and meaningful comparisons to the past works in emergent communication are fair enough to learn high scientific values from it. The design of the experiment is again very simple (e.g., changing the size of data, switching two setups in different ways) but clear to understand. This work is a good example of how well-designed hypotheses and their empirical validation could contribute to the field. I also appreciate the large spectrum of literature surveys including from the recent advances (Lewis et al., 2017, Lee et al., 17) to the past literature in emergent communications such as Littman (1994) and (Farrell & Rabin, 1996). \n\nOne of my concerns is the lack of applications, especially on the tasks using more natural language. The two tasks; OR and IBR, seem to be very limited settings to evaluate how self-play operates with data supervision. As pointed out by the authors, supervision from the training data itself may include most of the unexplored cases of the task, leading a less chance to learn policies from the high rewards. I think more realistic tasks using natural language need to be considered: negotiation (e.g., Lewi’s task, “Decoupling strategy and generation in negotiation dialogues”), recommendation (e.g., “Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue”), and more. I agree with the point made by the authors that this work mainly focuses on investigation rather than exploitation. But, then it would be adding another emergent task where the self-play can learn many more policies than one in the supervised dataset. \n\nAdding to the point, I was expecting to see non-task related metrics to measure the effectiveness of their appropriate combinations. For example, it would be better to add language-side metrics (e.g., perplexity, fluency, consistency) to measure how language degeneration varies by the different combinations. This issue is not addressed in the paper, and I guess this is mainly because of the limited usage of language in the two limited tasks. If the paper is only focusing on emergent language which is related to specific tasks, it would be better to tone-down a little bit and state the major difference of it with natural language. \n\nThe population-based S2P seems to be a bit incremental and unrelated to the main theme of the paper. To me, the motivation of adding POP into S2P based on the policy variability is somewhat different from the original claim about the combination of supervised and selfplay. Also, the improvements on IBR in Figure 7 are incremental, making the major claim of this work little divergent. \n\nIn terms of presentation, if you like to show how performance changes over the different sizes of data, it would be better to show it by graphs over different variations instead of the bar charts only with 10k and 50k sizes. In addition, the figures and captions need to be improved for better interpretation. I think they are written in a hurry or changed a lot in the last minutes. Please see some minor formatting issues below. \n\n\nMinor comments:\nDuplicate reference of (Lewis et al., 2017)\nSome names defined in Section 3.3 and Section 5 are not exactly matched.\nFigures and fonts in Figures 4 and 7 are a little difficult to understand. Especially, I can’t understand the two upper figures in Figure 4a\nCaptions in Figure 4 are not matched with the sub-figures. \nFigure r4b -> Figure 4b\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nSummary\n---\n\n(motivation)\nTo develop language speaking agents we can teach them to mimic human language\nor to solve tasks that require communication. The latter is efficient, but\nthe former enables interpretability. Thus we combine the two in an attempt\nto take advantage of both advantages. This paper studies a variety of ways to\ncombine these approaches to inform future work that needs to make this tradeoff.\n\n(approach)\nThe trade-off is studied using reference games between a speaker and a\nlistener. Goal oriented _self-play_ and human _supervision_ are considered two contraints one\ncan put on a network during learning. This work considers algorithms that vary\nwhen self-play and supervision are used (e.g., training with self-play then supervision,\nor supervision then self-play, or alternating back and forth between the two).\nAdditional variations freeze the speaker or distill an ensemble of agents into one agent.\n\n(experiments)\nA synthetic Object Reference game (OR) and a Image-Base Reference game (IBR) with real images are used for evaluation. Performance is accuracy at image/object guessing.\n1. (OR) Like previous work, this work finds that emergent languages are imperfect at supporting their goals and cannot be understood by agents that only understand a human language like English.\n2. (OR) Pre-training with supervision then fine-tuning with self-play is superior to pre-training with self-play then fine-tuning with supervision. This is presented as surprising from the perspective of language emergence literature, which is though of as pre-training with self-play.\n3. (IBR) Distilling an agent from an ensemble of 50 independently trained agents outperforms training single agents from scratch, but is still not as good as the whole ensemble.\n\nSelf-play vs supervision schedules:\n4. (IBR) Supervision (using image captions) followed by self-play performs much worse than all other approaches.\n5. (IBR) Alternating between supervision and self play (e.g., randomly choosing supervision or self-play every iteration) performs best.\n\n\n\nStrengths\n---\n\nThe curricula considered by this paper seem to have a sigificant impact on performance. These are new and could be important for future work on language learning, which may have considered the sup2sp setting from figure 7a without considering the sched setting.\n\nThe diversity of experiments provided and the analysis help the reader get a better sense for how emergent communication models work.\n\nIt's nice to see experiments on both a toy setting and a setting with realistic images.\n\nFuture directions suggested throughout the paper are interesting.\n\n\nWeaknesses\n---\n\n\n* The 3rd point of section 5 is presented as a major conclusion of this paper, but it is not very surprising and I don't see how it's very useful. The perspective of language emergence literature is presented a bit strangely. The self-play to supervision baseline seems to be presented as an approach from the language emergence literature. I don't think this is what any of that literature promotes exactly, though it is close. Generally, I (and likely others) don't think it's too surprising that trying to fine-tune a self-play model with language supervision data doesn't work very well, for the same reasons cited in this paper (point 3 of section 5). I think the general strategy when trying to gain practical benefits from self-play pre-training is a translation approach where the learned language is translated into a known language like English rather than trying to directly align it to English as does the supervision approach in this paper. This particular baseline would be more useful if the paper considered learning some kind of translation layer on top of the self-play pre-trained model.\n\n* How significant are the performance differences in figure 7a, especially those between the frozen and non-frozen models? Is the frozen model really better or this performance difference just due to noise?\n\n* I'm somewhat skeptical that these trends will generalize to other tasks/models. The main goal of this paper is to inform future work. That makes it even more important than normal that the trends identified here are likely to generalize well. Are these trends likely to generalize well? Does the paper address when these trends are expected to hold anywhere?\n\n\nMinor Presentation Weaknesses:\n\n* Figure 4: I think the sub-figures are mis-labeled in the caption.\n\n* In the related work I'm not sure the concept of generations is right. I think it should refer to different languages of different agents across time rather than different languages of the same agent across time.\n\n\nMissing details / clarification questions:\n\n* What exactly does Figure 4c compare? Are both methods distilled from ensembles or is the blue line normal S2P while the other is distilled from an ensemble of compositional languages? It's not clear since point (3) in section 5 refers to the S2P result (not Pop-S2P) in that plot. I'm also assuming that PB-S2P means the same thing as Pop-S2P, but that's not made clear anywhere. Does PB stand for Population Based?\n\n* In the rand setting how is convergence defined? Do both objectives need to converge or just one?\n\n* In the sched_rand_frz setting what is r?\n\n* In the IBR how are the distractor images picked?\n\n\nSuggestions:\n\n* Can't both self-play and supervision be used at the same time (just use a weighted combination of the two objectives)? I don't think the paper ever did this but it seems like a very useful variation to consider.\n\n\nPreliminary Evaluation\n---\n\nClarity: The writing is fairly clear, though some details are lacking.\nSignificance: This work could help inspire some future models in the language emergence literature.\nQuality: Experiments are aligned with the paper's goals and support its conclusions.\nOriginality: The distillation approach and curricula are novel.\n\nOverall the work could prove to be an interesting and useful reference point inside the language emergence literature so I recommend it for acceptance.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper explores the effect of ordering supervised learning and self-play on the resultant language learnt between agents. The topic is of high relevance to the ICLR community and makes several interesting insights useful to anyone learning control of a multi-agent system where communication amongst agents is applicable. I have several suggestions for improvements below, but all I believe are feasible to make within the time period of the rebuttal with the most necessary being:\n\n1) The naming of methods in Section 5 is not consistent with those introduced in section 3.3. For example, in the first paragraph ec2supervised is presumably sp2sup and sched is presumably sup2sp? Similarly, on page 7 (S2P and Pop-S2P) and Figure 4. Please revise and ensure consistency throughout.\n\n2) Figures 4b, 6 and 7 only present single values. Are these average values from repeated runs? If so please quantify variance.\n\n3) The conclusion in Section 7 at the bottom of page 8 that \"S2P performs much worse than the other options\" is contrary to previous results. Can the authors please comment on what features of the environment caused this difference?\n\n4) Appendix A includes details of hyperparameters, but some details remain unclear. Specifically, hyperparameter ranges swept over are shown but how were they then chosen from? Are they optimised for each environment and algorithm? What does the bold text in the table represent? If it is chosen values, why do only some parameters have chosen values? These are important details to enable reproduction of the paper.\n\nMinor Comments:\nIn Section 3.3, if all these methods are \"well known ways to combine self-play and supervised learning\" can all be supported by an (or preferably multiple) exemplar publications that used these method previously. Directly linking each to the previous work will further clarify the contribution this specific paper makes and help readers new to the area gain insight across the multiple papers this work builds upon.\n\nFigure 3b, colour is representative of performance. Is this mean accumulated reward? Please clarify to increase how informative this visualisation is, as currently it is unclear if yellow or blue is the desired value.\n\nPage 5, small typo \"introduced in the Lee et al. (2017)\" should be \"introduced in Lee et al. (2017)\".\n\nFigure 4b, the legend is blocking two bars and their corresponding value. It looks like moving to the bottom right may help, or placing above the plot.\n\nFigure 4 caption refers to a subfigure (d) that is not included.\n\nOn Page 6, the reference to babbling equilibrium should include a citation for interested readers to learn more about this well established concept.\n\nOn Page 7 there is a reference to Figure r4b, is this intended to be a reference to Figure 4b right?\n\nFigure 6 appears after Figure 7. Maintaining ordered numbering would be preferable.\n\nOn Page 9 it is noted some experimental results are in the Appendix but as there is a page and a half of space remaining before the 10 page limit, I would encourage to include all results in the main body of the paper.\n\nMultiple references do not list a publication venue (e.g. Evtimova et al., Lazaridou et al. 2018, Tieleman et al. 2018) or cite Arxiv versions when the work has been later published (e.g. Jacques et al. 2018 was published at ICML 2018). \n\nFigure 9 caption should state the environment."
        }
    ]
}