{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a bidirectional joint image-text model using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE-GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Summary: The authors design a new model for bidirectional joint image-text modeling using a variational hetero-encoder\n(VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. The authors also incorporate a deep topic model, a ladder-structured image encoder, and StackGAN++ into their framework for improved photo-realistic images.\n\nStrengths:\n- The authors have proposed a nice multimodal model that allows inference of latent variables given only text or image, and also allows realistic synthesis of images from images, text, or noise.\n- The paper is quite dense but generally well written. \n\nWeaknesses:\n- The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19).\n- It would help if the paper contained more ablation studies across different modules that the framework uses.\n\n### Post rebuttal ###\nThank you for your detailed answers to my questions. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a combined architecture for image-text modeling. Though the proposed architecture is extremely detailed, the authors explain clearly the overarching concepts and methods used, within limited space. The experimental results are extremely strong, especially on sub-domains where conditional generative models have historically struggled such as images with angular, global features - often mechanical or human constructed objects. \"Computers\" and \"cars\" images in Figure 2 show this quite clearly. The model also functions for tagging and annotating images - performing well compared to models designed *only* for this task.\n\nThe authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. Combined with the included code release, this paper should be of interest to many. \n\nMy chief criticisms come for the density of the paper - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. \n\nAs usual, more experiments are always welcome, and given the strengths of GAN based generators for faces a text based facial image generator could have been a great addition. The existing experiments are more than sufficient for proof-of-concept though.\n\nFinally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. Being able to read the code online, without downloading and opening locally can be nice, along with other benefits from open source release. However the authors should release the code however they see fit, this is more of a personal preference on the part of this reviewer.\n\nTo improve my score, the primary changes would be more editing and re-writing, focused on clarity and brevity of the text in the core paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposed VHE-GAN for the text-to-image generation task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. The experiments are conducted on three datasets. \n\nThe motivation for the paper is not clear. Most of the components used, such as text-encoder, image-encoder, generator-discriminator follow previous works. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, I did not see the clear motivation for this part. \n\nBesides the basic version VHE-StackGAN++, it proposed another version  VHE-raster-scan-GAN. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. \n\nThe experimental results are not solid. The comparison only included old baselines. However, several recent state-of-the-art approaches are missing: a. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Without these comparisons, it is difficult to evaluate how the method works. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode).   "
        }
    ]
}