{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a way to analyze overfitting to non-relevant parts of the state space in RL and proposes a framework to measure this type of generalization error. All reviewers agree that the formulation is interesting and useful for practical RL.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Review:\n\nThis paper considers the problem of overfitting in RL through a specific model of overfitting, namely one where noise is introduced in the observation space, independently of the controllable dynamics.  The paper provides both theoretical and empirical insights into the various manifestations of overfitting in this class of domains.\n\nA strength of this paper is to deepen our understanding of the phenomenon of overfitting in RL.  To get a deep understanding of hard problems it is worthwhile to look at sub-classes of problems, and as such the identification of the observational overfitting setting is interesting.  The paper provides insightful findings such as:\n-\tExplicit separation of f and g_theta.\n-\tTheoretical properties of generalization for the specific case of LQR and linear policies.\nThere are also several empirical results, on three contrasting domains which illustrates various aspects of the interaction between parameterization vs generalization.\n\nBut overall, though I read the paper in detail, and am knowledgeable of the topic, I am still struggling with extracting very specific new findings from this work.  Here are a few examples:\np.6:  “We observe empirically that the underlying state dynamics has a significant effect on generalization performance as the policy nontrivially increased test performance” ->  I see where you draw this analysis from the results.  But is this finding new or surprising?  I would have been very comfortable saying (without your results) that the underlying state dynamics can have a significant effect on generalization performance.\np.6: “This suggests that the Rademacher complexity and the weight perturbation bound for rewards vary highly for different environments.”  -> Again, how is this new knowledge?\np.7: “this also suggests that the RL generalization quality of a convolutional architecture is not limited to real world data” -> Same question, what new knowledge have we gained?\n\nPerhaps the key message is that “there are generalization benefits to overparameterization” and that “implicit generalization” plays a key role in controlling this?   If that is the main message, than it is somewhat obfuscated by all the material on LQR, which doesn’t really go in this direction (p.4: “We being with a theorem which implies that a high dimensional observational space directly contributes to overfitting”).  I must say I found most of the material in 3.1 to distrct from the main message – but perhaps it is because I am not very interested in the LQR setting, and did not understand how the findings there supported those in latter sections.\n\nOverall, I would say the paper suffers from some clarity issues.  There are some minor typos, then some key terms of are not sufficiently explained/defined (e.g. g_\\theta in Sec.2.2 is said to project “the latent data to unimportant features”, but then there is discussion of “in settings where $g$ does matter” – I’m confused.)   It’s not clear whether Thm 3.1 is new, or adaptation of existing results.  More broadly, I found Sec.3.1 difficult to follow. The last paragraph of Sec.4 seems superfluous.   I would recommend some good editing throughout.\n\nAnother concern with the work is the fact that several aspects of RL are ignored (e.g. exploration, entropy, \\gamma, noise, stochastic gradients, etc. – as per p.4), yet are known to have an effect on overfitting, perhaps much more than the depth and width of the neural network.  If that’s the case, it is perhaps dangerous to ignore them in the analysis; it is not described in the paper, for the domains studied in the empirical results, whether this is the case or not.  As a result, it’s not clear how far the current analysis carries.  I’m also not clear on the analysis at bottom p.7 “IMPALA-LARGE (…) memorizes less than IMPA due its inherent inductive bias” -> Seems to me IMPALA-LARGE might simply be in underfitting regime.\n\n======\nUpdate post-rebuttal:\nThank you for the thoughtful responses.   Given the other strong reviews, and your careful characterization of various aspects of the work, I am favorable to accepting.  Please try to incorporate elements of this discussion into the paper, which I think will give a richer and more nuanced understanding of the work to future readers.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\n\nThe paper proposes a method for measuring generalization error in reinforcement learning (RL). The paper proposes to disentangle the observations into the features that relevant and not-relevant for the policy and then perturb the non-relevant features while keeping the relevant features constant. If the agent has learned to generalize, then perturbing the non-relevant features should not change the RL score. \n\nComments:\n\n1. The paper addresses an important question in RL:  generalization in the observational space. It is not trivial to define generalization in RL as this differs fundamentally from a SL or unsupervised learning setting.  The paper proposes a metric to measure this generalization error and this can be applied to non-toyish environments.\n\n2. The paper is clearly written and well-motivated. \n\n3.  I do like the proposed method. However, I also see some shortcomings of the method.  \nThe paper proposes to disentangle the features into relevant and non-relevant features. While this might be easier for certain task, it might be much more difficult for other tasks. The relevant features may be some implicit priors that are not easy to extract, for example, the fundamental physics of an environment. I am not sure how this can be addressed in a complicated environment where the relevant features are sampled from an implicit prior.\n\n4. The experiments on CoinRun i think is the most relevant ones. However, in this environments, it seems that although the observational features are quite different (rendering of the environment), the underlying physics or moves/ actions are very much similar. It would be nice to see a more complicated environment where the underlying physics or composition of actions can be different."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Claims: This paper studies instances where observational overfitting is occurring in reinforcement learning, and separates it from other confounding factors contributing to poor generalization. They specifically study observational overfitting with LQR and deep neural networks, and show implicit regularization naturally occurs in this setting.\n\nDecision: Accept. The paper presents relevant bounds on overfitting depending on the dimensionality of the signal and noise. The framework and definition of family of MDPs are interesting and useful, and the focus on the implicit regularization provided by overparameterization when dealing with high dimensional observations with correlated noise is important and useful to decouple from varying dynamics. \n\nSection 3.2 needs more explanation of the experiment, how the train test split is created and if this is on the original rendered images from OpenAI gym or with correlated noise -- I've never seen such stark generalization gaps one the original environments. Are all of these experiments done with the proprioceptive state? Are the experiments in 3.3 then done with learned deconvolutions to generate the 84x84 images? Why this instead of using the rendered images themselves (as f) with added noise parameterized by \\theta?  Fig 8 is also not well explained, there are two sets of lines of each color in the plots with no label as to which is which, I assume these are train and test? Perhaps a plot that shows the generalization gap as the y-axis would be more clear."
        }
    ]
}