{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in ICLR-2020. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper studies (the more conventional) deterministic auto-encoders, as they are easier to train than VAE. To then try to maintain the model's capability of approximating the data distribution and to draw/synthesize new unseen samples, the paper both looks at imposing additional regularization terms towards a smooth decoder and proposes to sample from a latent distribution that's induced from empirical embeddings (similar to an aggregate posterior in VAE). Experiments are mostly around contrasting VAEs with the proposed RAEs in terms of comparing the quality of the generated samples.\n\nThe paper is trying to answer an important and meaningful question, i.e. can a deterministic auto-encoder learn a meaningful latent space and approximates the data distribution just as well as the much more complicated VAEs. And they've made some interesting and reasonable trials. The methods developed in the paper are mostly easy to understand and also well motivated in general.\nHowever my main concern is on the empirical studies, as they don't seem particularly convincing to justify the claimed success of the RAE over VAE. That said, the evaluation of generative models is by itself still an active research topic in development, which certainly adds to the difficulty of coming up with a sensible and rigorous comparative study in this regard.\n\nDetailed comments and questions.\n1. On \"interpolation\": it might provide some insights on how well the decoder can \"generalize\" to unseen inputs (which is still important), but otherwise seems to provide very little insight on how well the actual \"distribution\" is being approximated, as it's much more about \"coverage\" of the support of the distribution than the actual probabilities themselves?\n2. I'm not so sure FID can be regarded as the golden standard when it comes to comparing data generation qualities, and especially for the given setting, as it might give all those methods employing the so-called \"ex-post density estimation\" an edge over VAEs due to the scraping of the gap between the aggregate posterior and the prior (by design).\n3. From Table 2, according to the \"Precision/Recall\" criteria, WAE clearly out-performs RAE on the CelebA dataset, contradicting the result with FID in Table 1. I think this might need a closer look (as to why this happened) and certainly should be worth some discussions in the paper."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "The paper analyzes Variational Autoencoders and formulates an alternative perspective on many common problems in the VAE framework like a mismatch between the aggregated posterior and the marginal distribution over latent variables. The authors perform an autopsy of the VAE objective and show how it could be formulated as a constrained optimization problem. Interestingly, the paper proposes to remove all stochasticity from an encoder and a decoder and use different regularizers instead. In order to be able to sample from the model, the marginal distribution over latents is trained after the encoder and decoder are learned.  In general, I find the paper very interesting and important for the VAE community.\n\n- (An open question) The marginal distribution p(z) allows to turn the AE into a generative model. However, it also serves as a code model in a compression framework (i.e., it is used in the adaptive entropy coding). It would be interesting to see how the proposed approach would compare to other compression methods.\n\n- It would be interesting to see how the post-training of p(z) performs. Maybe it would be a good idea to run some toyish problem (e.g., dim(z)=2 on MNIST) and see how the GMM fits samples from the aggregated posterior.\n\n- Did the authors try a mix of different regularizers they proposed to use? For instance, L2 + SN?\n \n- Could the authors comment on the choice of hyperparameters (weights in the objective)?\n\n======== AFTER REBUTTAL ========\nI would like to thank the authors for answering my questions. I really appreciate all effort to provide more empirical evidence. I still believe that this paper is extremely important for the AE/VAE community and it sheds some new light on representation learning. In my opinion the paper should be accepted. Therefore, I decided to rise my score from \"Weak Accept\" to \"Accept\".",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This paper propose an extension to deterministic autoencoders. Motivated from VAEs, the authors propose RAEs, which replace the noise injection in the encoders of VAEs with an explicit regularization term on the latent representations. As a result, the model becomes a deterministic autoencoder with a L_2 regularization on the latent representation z. To make the model generalize well, the authors also add a decoder regularization term L_REG. In addition, due to the encoder in RAE is deterministic, the authors propose several ex-post density estimation techniques for generating samples.\n\nThe idea of transferring the variational to deterministic autoencoders is interesting. Also, this paper is well-written and easy to understand. However, in my opinion, this paper needs to consider more cases for autoencoders and needs more rigorous empirical and theoretical study before it can be accepted. Details are as follow:\n\n1. The RAEs are motivated from VAEs, or actually CV-VAEs as in this paper. More precisely, the authors focus on VAEs with a constant covariance Gaussian distribution as the variational distribution and a Gaussian distribution with the identity matrix as the covariance matrix as the model likelihood. However, there might be many other settings for VAEs. For example, the model likelihood can be a Gaussian distribution with non-constant covariance, or even some other distributions (e.g. Multinomial, Bernoulli, etc). Similarly, the variational distribution can be a Gaussian distribution with non-constant covariance, or even some more complicated distributions that do not follow the mean-field assumption. Any of these more complex models may not be easily transferred to the RAE models that are mentioned in this paper. Perhaps it is better if the authors can consider RAEs for some more general VAE settings.\n\n2. Perhaps the authors needs more empirical study, especially on the gain of RAE over CV-VAE and AE. \na) As the motivated model (CV-VAE) and the most related model in the objective (AE), they are not appearing in the structured input experiment (Section 6.2). It will be great if they can be compared with in this experiment.\nb) The authors did not show us clearly whether the performance gain of RAE over VAE, AE and CV-VAE is due to the regularization on z (the term L_z^RAE) or the decoder regularization (the term L_REG) in the experiments. In table 1, the authors only compare the standard RAE with RAE without decoder regularization, but did not compare with RAE without the regularization on z (i.e. equivalent to AE + decoder regularization) and CV-VAE + decoder regularization. The authors would like to show that the explicit regularization on z is better than injecting the noise, hence the decoder regularization term should appear also in the baseline methods. It is totally possible that perhaps AE + decoder regularization or CV-VAE + decoder regularization perform better than RAE. \nc) The authors did not show how they tune the parameter \\sigma for CV-VAE. Since the parameter \\beta in the objective of RAE is tunable, for fair comparison, the authors needs to find the best \\sigma for CV-VAE in order to get the conclusion that explicit regularization is better than CV-VAE.\nd) Although the authors mention that the 3 regularization techniques perform similarly, from Table 1, it is still hard to decide which one should we use in practice in order to get a performance at least not too much worse compared to the baseline methods. RAE-GP and RAE-L2 perform not well on CelebA while RAE-SN perform not well on MNIST, compared to the baseline methods. We know that the best performance over the 3 methods is always comparable to or better than the baselines, but not none of the single methods do. It is better if the authors can provide more suggestions on the choice for decoder regularization for different datasets.\n\n3. The authors provided a theoretical derivation for the objective L_RAE (Equation 11), but this is only for the L_GP regularization. Besides, this derivation (in Appendix B) has multiple technique issues. For example, in the constraints in Equation 12, the authors wrote ||D_\\theta(z1) - D_\\theta(z2)|| < epsilon for all z1, z2 ~ q_\\phi(z | x), this is impossible for CV-VAE since this constraint requires D_theta() to be bounded while q_\\phi(z | x) in CV-VAE has an unbounded domain. Moreover, in the part  (||D_\\theta(z1) - D_\\theta(z2)||_p=\\nabla D_\\theta(\\tilde z)\\cdot ||z_1-z_2||_p) of Equation 13, \\nabla D_\\theta(\\tilde z) is a vector well the other two terms are scalars, which does not make sense. There are many other issues as well. Please go through the proof again and solve these issues.\n\n\nQuestions and additional feedback:\n\n1. Can the authors provide more intuitions why do you think the explicit regularization works better compared to the noise injection? Can you provide a theoretical analysis on that?\n\n2. Can the authors provide some additional experiments as mentioned above? Also, can the authors provide more details about how do they tune the parameters \\beta and \\lambda?\n\n========================================================================================================\n\nAfter the rebuttal:\n\nThanks the authors for the detailed response and the additional experiments. I agree that the additional experiment results help to support the claims from the authors, especially for the CV-VAE for the structured data experiments and the AE + L2 experiment. So I think now the authors have more facts to support that RAE is performing better compared to the baseline methods. \n\nTherefore, I agree that after the revision, the proposed method RAE is supported better empirically. So I am changing my score from \"weak reject\" to \"weak accept\". But I still think the baseline CV-VAE + regularization is important for Table 1 and the technical issues in the theoretical analysis needs to be solved. Hope the authors can edit them in the later version.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}