{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function.    Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines. \n\nAll the reviewers recommend accepting the paper. To summarize the discussion:\n\n- R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions. I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail. \n\n-  R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations. The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize.  I believe that this adequately addressed this (and other) reviewer's concerns and the reviewer kept their score unchanged.\n\n- R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).\n\n- R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns. Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow-up work.\n\nWe share the reviewers' sentiment: it is a very nice and interesting paper, and should be accepted.\n"
    },
    "Reviews": [
        {
            "title": "New federated learning methods for heterogeneous devices",
            "review": "This paper proposed a new federated learning algorithm called FedDyn, which was motivated by the observation that the local objectives for each device might lead to inconsistent models between devices for heterogeneous data. Such observation is already observed in SCAFFOLD, and this paper further improves over SCAFFOLD with better communication efficiency. Both theoretical convergence rates and empirical  results about communication efficiency are presented to support the advantages of FedDyn methods. \n\nI have the following comments on the paper:\n1. The choice of alpha: it seems alpha is an important hyperparameter which can largely affects the theoretical convergence and empirical efficiency of the proposed FedDyn method. Unfortunately, I could not find much discussion in the paper about alpha: I think how alpha affect the convergence rates, and how alpha is chosen in the empirical results should be discussed, it would be better if the authors could provide an empirical study about the parameter sensitivity in alpha.  \n\n2. How the local objective is optimized: since at each round FedDyn requires each device to solve a local optimization problem which would requires iterative algorithms (such as SGD) to find an approximate solution, how to solve the local optimization problem, and how the accuracy of the local optimization affects theoretical communication efficiency as well as empirical results should be discussed.   \n\n3. Related work: there is a related paper (to appear in NeurIPS 2020) which proposes to solve a similar local objective, it would be good to discuss the differences: FedSplit: an algorithmic framework for fast federated optimization https://arxiv.org/abs/2005.05238.  ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nThis paper proposes FedDyn, a dynamic regularization method for federated learning. In FedDyn, the objective function of each active device in each round is dynamically updated, so that the device optimum is asymptotically consistent with the global optimum. The authors consider both the convex and non-convex case, and give convergence rates with theoretical guarantees. In the convex case the proposed algorithm converges faster than the SOTA algorithm SCAFFOLD. In the experiments the authors show that their algorithm takes less communication cost to achieve the same performance than existing algorithms. \n\nThe main contribution of the paper includes:\n\n- Proposing the dynamic regularization method to tackle the inconsistency issue in federated learning.\n\n- Proving the convergence rate of the proposed algorithm.\n\n- Saving communication cost compared to existing algorithms, both theoretically and experimentally. \n\n\nPros:\n\n- The problem of finding locally consistent distributed algorithms is well motivated. I appreciate the authors' discussion in the introduction. \n\n- The experimental results seem comprehensive. This includes the comparison between FedDyn, SCAFFOLD, FedAvg and FedProx using both synthetic and real datasets in four regimes of interest. The extensive experiments make the claim (saving communication costs) more convincing.  \n\n- Overall the paper is well written. The proposed algorithm is well justified with the discussion on the so-called fundamental dilemma. Also the comparison between the proposed algorithm and SCAAFFOLD makes the claim (saving communication costs) much clearer. \n\n\nCons:\n\n- What can be said about the computational time of each device during each iteration, compared to that of existing algorithms (say, SCAFFOLD)? This is also mentioned on Page 2 (\"This approach, while increasing computation for devices...\"). This would be interesting, although I understand the authors focus on a communication point of view. \n\n- The proof techniques of the main theorem somehow seem standard. I did not check other proofs though.\n\n-----------------------------------------\n\nPost-rebuttal:\n\nI appreciate the authors' feedbacks and I keep my evaluation unchanged.\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting new algorithm for distributed optimization / federated learning",
            "review": "* Context: the authors propose a new distributed optimization algorithm, called FedDyn, to minimize a sum of smooth functions. Their motivation is the context of federated learning, in which each function is the loss of one user corresponding to its data stored locally. The goal is to reduce the communication burden to achieve the true global solution (not an approximation thereof) to a given accuracy.\n\n* Strengths: \n1. The main strength of the algorithm is its robustness to partial collaboration, in which only a subset of the devices participate at every iteration. Indeed, in case of full participation, it is not clear that the algorithm has any advantage in comparison with classical (S)GD-type methods.\n2. The experiments are extensive, well described, and convincing: the method achieves the goal of convergence to a given accuracy with a speed and total communication load competitive w.r.t other methods.\n\n* Weaknesses:\nThe comparison to methods based on local steps, like FedAvg and Scaffold, is somewhat unnatural: these methods go along the idea of doing more computations between communication rounds. This is not the case of the proposed method, which is much closer in spirit to the class of SGD methods. Thus, I find the discussion, which turns around 'correcting' the drawbacks of  methods based on local steps, in particular the fact that FedAvg converges to an approximate solution, convoluted.  Moreover,\n1. No regularizer at the master in the objective function. \n2. The local loss functions must be smooth and in addition, their proximity operators must be computable (see point 5. below).\n3. No comparison to accelerated SGD-type method with accuracy in O(1/T^2) instead of O(1/T).\n4. No discussion of possible linear convergence in case of strong convexity.\n5. alpha, the inverse of the stepsize, must be large (>25L) for Theorem 1 to apply. This shows that the analysis is not tight at all.\n\n* assessment: I think the paper deserves publication, since the proposed algorithm is new, comes with some convergence guarantees, and shows good performance in practice. However, I view the theoretical analysis as a preliminary one, and several aspects would deserve to be investigated more in depth. \n\nMore detailed discussion:\n\n1. There should be a discussion about the literature of SGD type methods. Indeed, partial participation takes the weak form, in Theorem 1, of a subset selected uniformly at random. Some papers by Richtarik et al coming to my mind: \"A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent\", \"Unified analysis of stochastic gradient methods for composite convex and smooth optimization\", \"A unified analysis of stochastic gradient methods for nonconvex federated optimization\".\n\n2. You should talk, even very shortly, about the other strategy to decrease the communication burden: compression, see for instance the recent papers and refs therein: \"On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning\", \"Distributed learning with compressed gradient differences\".\n\n3. Further on, one could view the proposed method as GD with unbiased compression: all devices compute their new gradient/model but then with probability P/m it is sent to the master, otherwise it is not sent. It would be good to investigate this relationship more closely.\n\n4. When mentioning prior work on methods using local steps of SGD, which is \"inconsistent with minimizing the global loss\" (or later when mentioning that \"performance degrades in non-IID scenarios\"), you can cite the paper \"From Local SGD to Local Fixed Point Methods for Federated Learning\", ICML 2020, which gives a precise characterization of this \"inconsistency\" in the strongly convex case (Theorem 2.14). For the non-strongly convex case, you can refer to the paper \"Tighter theory for local SGD on identical and heterogeneous data\".\n\n5. Each local computation step is actually a call to the proximity operator of L_k: we have theta_k^t = prox_{L_k/alpha}(theta^{t-1}+grad.L_k(theta_k^{t-1})/alpha). This should be mentioned clearly, as well as the assumption that L_k is proximable (in addition of being smooth). The operation is not a proximal gradient descent step, since there is a + and not a - in front of the gradient. Still, the variable h has the flavor of a dual variable. So, the relationship to proximal splitting algorithms should be investigated, since there seems to be a connection there. \nThinking a bit about this connection, I found out the \"distributed Davis-Yin algorithm\" in the paper \"Distributed Proximal Splitting Algorithms\nwith Rates and Acceleration\", see p. 22 of https://arxiv.org/pdf/2010.00952.pdf. Rewritten in your context, its iteration is:\n|for each device in parallel do\n|\ttheta_k^t = prox_{L_k/alpha}(2.theta^{t-1}-s_k^{t-1}\n|\t\t-grad.L_k(theta^{t-1})/alpha)\n|\ts_k^t = s_k^{t-1} + theta_k^t - theta^{t-1}\n|\ttransmit theta_k^t to server\n|end for\n|s^t = s^{t-1} +  1/m.sum_k (theta_k^t - theta^{t-1})\n|theta^t = s^t\nThere seem to be several differences between this algorithm and yours, but still, they look similar in spirit. It would be very interesting to compare them. \n\n6. In the last two steps of your algorithm (set h^t=..., set theta^t=...) you can remove the multiplication and division by alpha.\n\n7. For the nonconvex case, the rate in Theorem 1 is with respect to the gradient norm. You should tell a bit about the literature, see e.g the discussion in \"Primal-dual accelerated gradient descent with line search for convex and nonconvex optimization problems\" by Nesterov et al.\n\nTypos:\n* convergences -> converges  \n* non convex -> nonconvex\n* Table 1: I guess the \"Acc.\" column is to provide the accuracy. This should be said, since the reader might be confused and think that this is an accelerated method included in the comparison\n* theta_k^infty triangle theta^infty: what does the triangle mean?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "It is a good paper but needs more work.",
            "review": "The paper proposes a new optimization method named FedDyn to handle data heterogeneity inherent in FL via a dynamic regularization. Such a dynamic regularization modifies each local objective by adding a linear and quadratic term that makes the local stationary point is asymptotically consistent with that of global objectives. The authors prove convergence results for FedDyn in both general convex and non-convex cases and test FedDyn on synthetic and real-world datasets. \n\nPros:\n1. The paper is well written and easy to follow. All proof seems correct.\n2. The experiment setup follows many previous works, which facilitate comparisons, and I appreciate a lot. Besides, a lot of details are given, which helps reproduction.\n3. The proposed method has great superiority over other baselines in communication efficiency, shown by a lot of experiment results.\n\nCons:\n1. I think the author could also analyze FedDyn in a strongly-convex case since in that well-conditioned case the convergence performance of FedDyn could give more insights for researchers. I expect FedDyn would converge to the optima with a rate exponential in communication rounds. \n2. The author declare that FedDyn has great advantages over competing methods in cases of a large number of devices. However, from the main theorem, I couldn't figure out the reason.\n3. The author declare that FedDyn is more robust to unbalanced data than baseline methods. From experiments, it seems correct as  FedDyn achieves a larger factor of gains over others in unbalanced data. However, again, no theoretical analysis is given. The main theorem is about the convergence of balance data in terms of communication rounds not about the effect of unbalance data.\n4. Since the proof mainly follows from that of SCAFFOLD and many empirical findings have no theoretical support, I would regard the main contribution as the algorithm itself. The experiment indeed did well, however, there is still some shortcomings. There are other algorithms proposed to (or able to) handle data heterogeneity like FedPD [1], FedSplit [2] and VRL-SGD [3]. However, the authors didn’t consider these competing methods and even didn’t mention them.\n\n[1] Zhang, Xinwei, et al. \"FedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data.\" arXiv preprint arXiv:2005.11418 (2020).\n[2] Pathak R, Wainwright M J. FedSplit: An algorithmic framework for fast federated optimization[J]. arXiv preprint arXiv:2005.05238, 2020.\n[3] Liang, Xianfeng, et al. \"Variance reduced local SGD with lower communication complexity.\" arXiv preprint arXiv:1912.12844 (2019).\n\nOther points:\n1. The description of SCAFFOLD’s linear term seems imprecise. The subtracted term is not $\\frac{1}{m} \\sum_{k \\in[m]} \\nabla L_{k}\\left(\\boldsymbol{\\theta}_{k}^{t}\\right)$, since SCAFFOLD only activates a small fraction of devices and this term is impossible to compute. But I can understand what the author wants to convey: their affine function saves communication costs.\n2. The authors mention that at each communication round SCAFFOLD communicates the current model and its associated gradient while others communicate only the former. So, if I want to say FedDyn is more effective in saving communication rounds, I would expect it to achieve at least 2x gains over SCAFFOLD. However, this is a rare case when the participation rate is 10% (see Table 2). It seems that FedDyn is not so effective in saving communication rounds and the main reason for its effectiveness seems to be that the modified affine function requires once communication per round.\n\n\n--------------------------------------------------------------------\nI have read the authors' responses and almost all my concerns have been well addressed. \nSo I increase my point to 8.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}