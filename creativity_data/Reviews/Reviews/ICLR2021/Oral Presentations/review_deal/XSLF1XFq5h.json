{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper presents an uncertainty quantification method that is conceptually interesting and practical. All reviewers are in consensus regarding the quality and significance of this manuscript. "
    },
    "Reviews": [
        {
            "title": "Review for Getting A CLUE: A Method for Explaining Uncertainty Estimates",
            "review": "## Summary\n\nThe authors consider the problem of post-hoc explainability for decisions rendered by machine learning models.  They focus on addressing uncertain model predictions, producing counterfactual data that is both likely under a generative model of the data, as well as more certain in the classification task.  They present both experimental evidence, as well as a user study geared towards practitioners, that show the benefits of counterfactual explanations targeting uncertainty.\n\n## Strong points\n\nWhat really sets this paper aside for me is its explicit focus on uncertain model decisions, as well as its inclusion of a user study.  \n\n- While other works do use the point estimates of model softmax outputs as part of their methods (e.g Progressive Exaggeration), they do not focus on providing counterfactual explanations specifically for those points where the model is uncertain.\n\n- Most works in explainable AI, even those targeted towards practitioners, do not evaluate the utility of their tools in front of an audience of machine learning model developers.  Bravo for undertaking this work which is (currently) under-valued in the ML community.\n\n## Weak points\n\n- In step 4 of Algorithm 1, why do you need to focus on a BNN for obtaining $H(y|x)$?  Going back to my previous point about limiting usefulness of CLUE, there are many methods that yield $P(y | x)$, even by approximate Bayesian inference.    \n\n- Is it the case that as the method converges (if it converges?), $H(y|x)$ is likely to decrease, while $d(x, x_0)$ is likely to increase?  Do you attempt to anneal the relative contribution to the loss to account for this?  Furthermore, do you ensure that the relative contribution to the loss is balanced between $d(.,.)$ and $H(y|x)$??  If these become imbalanced, one will surely dominate the direction of $\\nabla_Z \\mathcal{L}$\n\n- In the description of the baselines used for experiments in section 5, the localized uncertainty sensitivity analysis seems artificially weak; why not include a more robust ensemble of models that produce softmax output over class assignments?  Or a proper probabilistic model like a GP?\n\n## Recommendation\n\nThe effectiveness of CLUE, and indeed of every counterfactual explanation method cited that makes use of an auxiliary generative model of the data is bounded by the faithfulness of this DGM to model the density.  This is a more fundamental limit on the applicability of these methods, not specific to CLUE, but worth stating IMO.\n\nI disagree with the statement detailed in section 4 after the evaluation procedure involving $\\mathcal{H}_{gt}$ capturing ground truth aleatoric uncertainty: $p_gt(y|x)$ is just another generative model trained by estimation of the data, it’s not special.  A better measure of aleatoric uncertainty would involve the variance of $p(y | x)$ taken over multiple independent models (see Snoek et al.).  I also find the argument about adversarial weakness that immediately follows is a bit confusing \"*Approaches that exploit adversarial weaknesses in the BNN will not transfer to the g.t. VAEAC, failing to reduce uncertainty on error*\".   \n\nFundamentally, though I see areas where the work could be improved, I believe the work is sufficiently different to existing counterfactual explanation methods to be accepted.\n\n## Questions for the authors\n\n- In section 3 where you penalize the distance $d(x, x_0)$, given that the motivation of having a penalty on $d(x, x_0)$ is to try and ensure minimal changes, what about instead ensuring this by doing projected gradient onto the space of plausible data?\n\n- Again in section 3, is $d_y(f(x), f(x_0))$ intended to be high, or low?  Traditional understanding of counterfactual explanations in the literature would suggest $f(x) != f(x_0)$, but I can see the value in not caring about enforcing this to focus on driving down $H(y | x)$.  Could you spend some space touching upon this design decision here?\n\n- In Section 5.1, I'm curious why FIDO was chosen as a baseline?  If memory serves, the FIDO objective has additional constraints to try and ensure the B form a contiguous set, which your U-FIDO formulation (equations 6,7) does not admit.\n\n## Suggestions for improving the paper\n\n-  Algorithm 1 presents a minor nomenclature issue: the output $x_{clue}$ produced might not be an actual counterfactual in the sense of Wachter et al., in that no effort is made to orient the latent space edits (z) towards crossing a decision boundary for Y.\n\n- At the beginning of section 4, I find step 4 of the procedure is not clearly presented.  Step 4 suggests it’s used as a way to measure whether the discovered x_c are likely given the density of the model.  Is that so? If not, could you be more clear why evaluation of $\\tilde{x_c}$ by the VAEAC is helpful? \n\n- At the end of section 3, it would be helpful here to consider recent efforts to encourage counterfactual explanations to be confined to small contiguous regions (cf. Dabkowski and Gal 2017, Chang et al 2019).  This issue of potentially large, disparate, sparse signals was a flaw of original gradient based saliency maps, and would be well addressed here.\n\n- In section 5.1 where you discuss the criteria for evaluation of counterfactuals (*We would like counterfactuals to explain away as much uncertainty as possible while staying as close to the original inputs as possible*).  This is achieved, albeit indirectly, by other counterfactual generation methods (e.g Progressive Exaggeration https://openreview.net/forum?id=H1xFWgrFPS), which vary latent representations along a continuum of class output probabilities.  You could use their method as a comparator by selecting uncertain points and generating counterfactuals that move away from the decision boundary instead of towards it.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Sound, well-written paper, interpretability and uncertainty of models, perhaps most contribution to XAI community.",
            "review": "This paper tackles the problem of making Ai/ML-systems more trustworthy making the uncertainty associated with a model, in this case BNN, visible, more interpretable. Interpretability and knowing the limitations and uncertainties associated with a model are definitely very interesting research challenges. These topics are very relevant for ML, AI, Explainable AI etc., but I still think that they are also for ICLR (even if many conferences in the ML/AI/XAI will also fit this paper)\n\nI find the main ideas innovative, the paper is well-written, explained and even includes some kind of “small” user study. The authors also provide a framework for evaluating the counterfactual explanations of uncertainty provided, using informativeness, and they carry out well-designed experiments for validating CLUE.\n\nP. 7, under section 5.2. Can the first sentence be referred to Hoffman? I don’t think so. Are the participants used in the user study be good representatives of the possible users/practitioners of CLUE (as also stated in the conclusions)?\n\nThere has come a recent survey on counterfactuals, that I think it is relevant for this work:\n\nVerma, S., Dickerson, J., & Hines, K. (2020). Counterfactual Explanations for Machine Learning: A Review. arXiv preprint arXiv:2010.10596.\n\nI understand that it is out of the scope of this paper, but the notion of counterfactuals used in the ML community, like the one used in this paper (section 2.3), is quite narrow compared to how we use counterfactuals and contrastive explanations in real life. I think the richness and complexity of counterfactual explanations is well illustrated in \n\nByrne, R. M. (2019, August). Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning. In IJCAI (pp. 6276-6282).\n\nPerhaps this is something to discuss in the future.\n\n(just to make clear: I am not involved in any of the references given, just thought that they can be of interest for this paper).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple, effective approach with strange framing",
            "review": "This paper addresses the problem of explaining the uncertainty of a prediction made by a differentiable probabilistic model (as opposed to the prediction itself) through counterfactual explanations. They propose a technique, CLUE, which optimizes their counterfactual metric in the latent space of a deep generative model. To validate their approach, they introduce a quantitative evaluation for uncertainty metrics and conduct a user study, while also analysing CLUE's reliance on the auxiliary DGM.\n\nStrengths:\nThe empirical validation of the approach is strong. \n\nThe authors deserve particular credit for \"creating their own baseline\" by adapting a previous counterfactual approach (FIDO) to the problem at hand. \n\nThe user study is well-executed, and produces a pretty stark improvement over baselines, including human-selected counterfactuals.\n\nWhile the introduced approach does require the non-trivial complexity of training a DGM, conceptually the method is an elegant way of dealing with one of the big challenges for counterfactual explanations - staying on the data manifold.\n\nWeaknesses:\n1. The framing (abstract/introduction) of the paper took a while to wrap my head around, and could probably be improved. In particular, for classification problems, the \"simple, stupid\" approach of looking for the most negative feature attributions for a prediction seems like it would produce an explanation of uncertainty (though not a counterfactual one, so not competitive to CLUE). This makes Figure 1 pretty puzzling, as it's not clear to me why standard attributions aren't useful for uncertain predictions.\n2. Qualitatively, how would CLUE compare to a standard counterfactual explanation?\n3. Why isn't U-FIDO included in the user study? Given that it performed the best in section 5.1 on the datasets used, the results would be interesting.\n3. In the user study, is the test example linked to the two context points at all? I wouldn't expect unrelated context points to be that useful in classifying a random test example.\n\nNitpicks:\n- In the appendix, CLUE is compared against Shapley/LIME on MNIST. LIME is a pretty strange choice, and has never been shown/claimed to be remotely SOTA on neural networks. Something like integrated gradients would be more relevant/interesting.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper introduces CLUE -- a method to explain uncertainty estimates.  The method utilizes a VAE trained on the original data set to search effectively for low confidence instances.  The method utilizes a gradient based search through the latent space of the VAE.  The authors assess their approach on a variety of tabular data sets and MNIST.  They evaluate along change in uncertainty of the counterfactual as well as human evaluation.  They generally find improvements using their method over baselines.  Additionally, their method works much better in human evaluations.  \n\nComments + Questions\n\nSection 1:\n- The authors argue that CLUE can be used to complement feature importance techniques like LIME, Saliency Maps, etc. They point out that when the model is confident, you can use a feature importance technique.  When it is not confident, you can use CLUE.  However, the motivation behind why this dual approach is useful is not quite clear.  With feature attribution methods, the goal is to understand what features the model is locally relying on.  If the goal is to understand how the model behaves and the model is uncertain for a particular point, it could still be quite insightful to use feature attribution methods.  The authors could better argue why their complementary technique is useful and make explicitly clear why you wouldn't want to use feature importance methods for uncertainty data instances like they suggest. Right now, it is not so clear. \n- One minor point is that in the second paragraph, the authors motivate their method by saying CLUE can be useful to understand features contributing to uncertainty for instances underrepresented in the training data.  This motivation doesn't connect quite so clearly to the example in figure 1 --- we can see that the data instance is somewhat ambiguous.  Would the solution here to be to collect more ambiguous 6's? This motivating example could flow more clearly if it were a tabular instance because it would immediately connect to the motivating scenario given immediately before.\n\nSection 3:\n- In section 3, we see CLUE \"aims to find points in latent space which generate inputs similar to an original observation x0 but are assigned low uncertainty.\"  However, in the introduction it was stated that “CLUEs answer the question: What is the smallest change that could be made to an input, while keeping it in distribution, so that our model becomes more certain in its decision for said input?” These two claims seem slightly at odds.  Should the claim in the introduction be revised? \n\nSection 4:\n- The evaluation procedure claims using data generated through a VAE as the training data will reduce the possibility of clue exploiting adversarial vulnerabilities.  Recent work has pointed out adversarial vulnerabilities might be part of the training data for image data sets as nonrobust features [1] and in this way could be captured by the VAE making this procedure less effective.  The bulk of this work is focused on tabular datasets and MNIST where this is less likely an issue, so I am not too concerned.  However, scaling this procedure up to larger image data sets could produce issues.\n- A larger concern is that by only using data produced through a VAE, CLUE has a bit of an unfair advantage over methods like localized sensitivity which don't explicitly require the use of a VAE.  Meaning, the representational capacity of the VAE trained for CLUE is limited and in this way may not find certain diverse data points with low confidence.  A method that doesn't use a VAE might be able to find these points -- though search could be more challenging.  By forcing the set of images we're considering to be those produced by a VAE, this technique is shifting the playing field in favor of CLUE in what feels like a bit unfair way.  The evaluation should take into consideration that the requirement to train a VAE could be a disadvantage of CLUE but establish it is worthwhile nonetheless.  \n\nSection 5:\n- From table 1, CLUE's performance seems relatively well balanced with U-FIDO in many of the tabular tasks.  The authors point out at the bottom of page 6 that CLUE performs better on higher dimensional data sets.  However, this is only apparent in the MNIST data set.  If CLUE's merits lie with higher dimensional data sets like images, it could be better to provide more evaluation in these settings.  \n-  In the reference for appendix h.2 and the $log p_{gt}(\\cdot)$ test, it again feels like the assessment is a bit unfairly advantaged to CLUE; it feels very likely for CLUE to produce the best counterfactuals according to this metric because we assess data likelihood using a VAE.  At the same time, CLUE only generates counterfactuals on the VAE manifold.  \n- Maybe the authors could consider including some metric like nearest neighbor distance to the original training set for both clue and the baselines (where the baselines are run without being restricted to VAE generated data)? This could help us better understand the limitations imposed by using a VAE with CLUE. I think that an additional metric that disentangles the effect of the VAE within CLUE is needed here.\n- The human study results add a lot of merit to the CLUE approach.  It's clear from these that the counterfactuals produced by clue are much more human interpretable. \n- Though the right hand side of figure 10 helps use understand the limitations of using a VAE and is much appreciated, I still think an additional evaluation metric is needed for the 5.1 experiments.  An additional metric would help understand if the section 4 technique is giving CLUE an unfair advantage over the baselines.  \n\nOverall:\nI am convinced after reading the paper that the method exhibits useful performance in finding human meaningful uncertainty focused counterfactuals.  Further, there are a number of strong experiments in the paper -- I particularly liked the human evaluation and found this convincing. That said, there are a number of weaknesses that I'll mainly divide into two categories:\n(1) Introduction: per my comments in the introduction, I think this section could be significantly strengthened.  Most importantly here, the authors describe their method being used in an explanation workflow where uncertain instances are explained with clue and certain instances with something like LIME.  This thread, which seems like a key focus initially, is dropped for the rest of the paper.  It's currently unclear why this workflow makes sense and warrants much more justification.  Further, it could be worthwhile just to motivate CLUE as a method to explain uncertainty estimates in its own right because the connection with methods like LIME isn't explored in the rest of the paper.\n(2) Evaluation technique from section 4: Using data generated from a VAEAC as the set of legitimate images could give CLUE an unfair advantage over baselines because it reduces the potential effects caused by representation capacity of the CLUE VAE. It would make this section much stronger to include another metric to try and isolate these effects.  I would appreciate some author clarification here as well, in case I am misunderstanding something about the evaluation technique.\n\nOne final minor point is the authors claim their method works better than baselines in higher dimensional data.  However, they only evaluate using MNIST where VAE's tend to be very strong.  To fully substantiate this claim, it could be worthwhile to consider a few slightly more challenging data sets for VAEs (street view house numbers, celeba, etc). \n\nMy sentiments are currently leaning towards reject mainly due to the motivation and experimental issues described in (1) and (2).  If the authors could remedy these concerns, I'd be inclined to raise my score because I think their are a number of potential valuable contributions in the work.\n\nOne related method that isn't discussed is https://arxiv.org/abs/2002.10248.  The authors generate instances at certain levels of prediction confidence --- though different it could be good to bring up.\n\n[1] https://arxiv.org/abs/1905.02175 \n\n---- update ----\nIn response to the author's comments and extensions, I've raised my score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}