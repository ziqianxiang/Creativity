{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "The reviewers agree that this is an interesting and original paper that will be of interest to the ICLR community, and is likely to lead to follow up work."
    },
    "Reviews": [
        {
            "title": "Interesting work on unbiased gradient estimation with lower memory cost",
            "review": "Summary: \n\nThe paper proposes to subsample the computational graph to obtain an unbiased gradient estimator with less memory requirement, in the same spirit as minibatching reducing the memory of the full-batch gradient descent. The authors propose to do so by modifying the forward pass of a linear layer to only save a subset of the hidden units to estimate the gradient. \n\n\nContributions: \n\nJust like minibatching is introduced to reduce the memory requirement of full-batch gradient descent, this paper introduces another dimension to trade off between noisy gradient estimate and memory by subsampling the computational graph for computing a noisy gradient. The experiments, albeit small-scale, clearly demonstrate the effect of using SGD with randomized AD to train standard neural networks. The application on stochastic optimization of PDE is also very interesting and relevant. The paper is very well written and explains the idea very clearly. I look forward to seeing future work using this principle to train larger scale models as well as how this work will enable the development of many more research ideas the same way minibatch SGD acts as a workhorse of deep learning / machine learning. \n\n\nAdditional details:\n\nIn the exposition of the “alternative SGD scheme with randomized AD“, it might help to understand the sources of the variance of the gradient estimator by decomposing it as Var(x_ij) = Var(x_i) + E[Var(x_ij | x_i)], where i is the index for minibatching and j is for subgraph sampling. It will also help to understand the different curves of Fig 5.  \n\nIs there any speculation of why the drop in performance is more drastic in MLP and CNN than RNN in Fig 5? \n\nI’d like to see some negative results where the variance is too high and harms training, to get a sense of how important it is to tailor the sampling strategies for different computational graphs.  \n\nPerhaps explain what M in section 5 is. I had to go to the appendix to figure out that it is a matrix that summarizes the discretization of the space and time. \n\nRelated work: Perhaps in the discussion of constant-memory via invertible transformation, Neural ODE can be included since it’s constant memory the same way as Gomez et al 17’. \n\n--- post rebuttal --\n\nThanks for the response. I think the contribution of this work is solid and I vote for clear acceptance. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Original idea with a lot of potential",
            "review": "The authors introduce the novel idea of producing unbiased gradient estimates by Monte Carlo sampling the paths in the autodiff linearized computational graph.\nBased on sampling paths it is possible to save memory due to not having to store the complete linearized computational graph (or the intermediate variables necessary to reconstruct it). \nMemory is the main bottleneck for reverse mode autodiff for functions with lots of floating point operations (such as a numerical integrator that performs many time steps, or a very deep neural network).\nThe authors' idea can therefore potentially enable the gradient based optimization of objective functions with many floating point operations without check pointing and recomputing to reverse through the linearized computational graph.\nThe tradeoff made is the introduction of (additional) variance in the gradient estimates.\n\nThe basic idea is simple and elegant:\nThe linearized computational graph of a numerical algorithm is obtained by\na) having the intermediate variables of the program as vertices\nb) drawing directed edges from the right-hand side variables of an assignment to its left-hand side variable\nc) labeling the edges by the (local) partial derivatives of assignments' left-hand side with respect to their right-hand side.\n\nThe derivative of an \"output\" y with respect to an \"input\" x of the function is the sum over all paths from x to y through the linearized computational graph taking the product of all the edges in the path.\nThe sum over all paths corresponds to the expectation of a uniform distribution over the paths times the number of paths.\nThat expectation can be Monte Carlo sampled.\n\nThe authors suggest a way of producing the path sampling based on taking a chained matrix view of the computation graph (see e.g https://arxiv.org/abs/2003.05755) and injecting low rank random matrices.\nDue to the fact that the expectation of the product of independent random variables is equal to the product of the expectations this injection is unbiased as well if the injected matrices have the identity matrix as expectation.\n\nTo take advantage of the simple idea it is in practice necessary to consider the concrete shape of the computational graph at hand in order to decide where to best randomize and save memory without letting variance increase too much.\n\nThe authors present a neural network case study where they show that for some architectures the suggested approach has a better memory to variance trade off than simply choosing a smaller mini-batch size.\nFurthermore, they present a 2D PDE solver case study where their approach can save a lot of memory and still optimize well.\n\nI recommend to accept the paper.\n\nRemarks:\n\nI would love to see a more in depth analysis of the variance for example for simple but insightful toy examples.\n\nFor exampl simple sketching with random variates v with E[vv^T] = I can be used to obtain an unbiased gradient estimate via E[gvv^T] = g, i.e. by evaluating a single forward-mode AD pass (or just a finite difference perturbation).\nBut of course the gradient estimate has such a high variance so as to not give any advantage over finite difference methods (since with each sample / evaluation we are only capturing one direction in the input space).\nWe are not gaining the usual benefit of reverse mode autodiff of getting information about the change of the function in all directions.\n\nIn order for paths to be efficiently Monte Carlo-friendly it is probably necessary that they are correlated with other paths.\nIn practice this will perhaps have something to do with e.g. the regularity of the PDE solution (the gradient with respect to the solution is similar to that of its neighborhood).\n\nA simple example (ODE integrator):\n\np1 = x\np2 = x\nfor i in range(n):\n\tp1 = p1 + h * sin(p1)\n\tp2 = p2 + h * sin(p2)\n\ny = 0.5 * (p1 + p2)\n\nThe two paths in the program compute exactly the same values so leaving one path out randomly does not make any difference at all (if we correctly re-weight the estimate).\n\nMini-batches are often like that: Independent samples from the same class give correlated computations, hence the variance is related to the variance in the data.\n\nBut if two paths involve completely independent and uncorrelated computations then the variance is such that we do not gain anything.\nWe need at least two gradient steps to incorporate the information from both paths.\nSince we do not systematically cycle through them but sample randomly, we are actually going to be less efficient.\n\nIn terms of arguing about memory savings for machine learning applications it would be interesting to see a case study with a large scale architecture that does not fit into memory.\n\nThe random matrix injection section could be clarified by moving the sentence \"the expectation of a product of independent random variables is the product of their expectation\" further to the front and state clearly the idea that:\nE[A PP^T B QQ^T C] = A E[PP^T] B E[QQ^T] C = A I B I C = A B C\n\nIn the PDE example you could clarify the notation used to properly distinguish between the continuous and the discretized solution.\n\nAlso the PDE constrained optimization problem is not inherently stochastic (as can be argued for the empirical risk minimization setting in machine learning).\nTherefore, it is possible to use non-SGD methods with linear or even super-linear convergence rates (quasi-Newton methods).\nSGD with unbiased gradients has a sublinear rate of convergence.\nBut the ideas of the paper are of course still useful even when trying to find the optimum up to machine precision in finite time.\nWe can first use the RAD SGD approach in the early optimization and then go to the deterministic setting later in the optimization.\n\n- Page 3: eqn equation 1 -> Equation 1\n- Page 6: figure 5 -> Figure 5\n- Throughout: Perhaps clarify the meaning of batch vs mini-batch (in other papers batch can refer to full-batch)\n- Figure 5 (a) has blue curves but blue is not in the legend of Figure 5 (c)\n- Page 8: backpropogate -> backpropagate \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-written and solid study on the fundemental",
            "review": "In the context of deep learning, back-propagation is stochastic in the sample level to attain bette efficiency than full-dataset gradient descent. The authors asked that, can we further randomize the gradient compute within each single minibatch / sample with the goal to achieve strong model accuracy. In modern deep learning, training memory consumption is high due to activation caching. Thus this randomized approach can help attain strong model accuracy under memory constraints.\n\nThe authors proposed a general framework for randomized auto differentiation to achieve unbiased gradient estimators. This general framework allows randomization at different granularity such as layer level and individual neuron level. It also includes conventional minibatch gradient estimators as a special case at the sample/minibatch level for randomization. The memory saving here is achieved by trading off gradient variance for activation memory saving. \n\nEmpirically, the authors show that for 1) convolution nets on MNIST and CIFAR and 2) RNN on sequential-MNIST, under the same memory budget, neuron-level randomized gradient estimator can achieve higher model accuracy than conventional SGD with smaller minibatch size. \n\nStrong point: This paper is well written with novel thoughts on the fundamental aspects of auto-diff when applied to deep learning (and also to PDE as demoed in section 5.). It can also provide new options for practitioners to train models with high accuracy under memory constraints. Thus I recommend to accept this paper.\n\nI have the following comments / questions on the technical aspects. I only raise these questions up for improvement on the paper; they are not concerns on the quality of the current version. Nonetheless,  I am happy to raise the score if the authors can demonstrate results on these aspects.\n\n1. The authors mentioned about leveraging model specific structures to control/reduce gradient variance for fine-grained randomization (such as at the neuron level). Specifically, they considered using the randomized activation sparsification only for activation memory-consuming layers. I was wondering if other model structures can also help here. E.g. can we sample at the full-channel level for convolutional layers or column / row level for linear layers. Would this has a significant impact of the attained model accuracy?\n\n2. The main focus on practical implications in this paper is about high accuracy training under memory constraints. However, I was wondering how the authors think about the implication on compute, especially related to the sparse training trend. E.g. can we also make the forward compute itself also sparse and still minimally influence model accuracy?\n\n\nNITs: \n\n1. First letter capitalization for figure 3 at the beginning of section 4.\n\n2. Would it be possible to provide some preliminary in appendix on the solution to the section 5 PDE example.  \n\n3. In section 3.3, the discussion on two extremal cases needs a more precision in text. Assuming the sampling on each connection (i.e. segment on paths) are independent, I think both case will have variance exponential in terms of depth. Currently it reads like only the fully connected case have exponentially large variance.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Technical explanation is not sufficient. ",
            "review": "The paper proposes a novel approach to reduce memory in backpropagation by sampling the paths in DAG. The paper developed and proved the proposed method in a general framework (which is nice!). Still, I feel that the explanation for applying this method to neural networks is somewhat lacking. I summarize the missing parts in the following.\n\n(1) For fully-connected layers (matrix multiplication), the explanations in Section 3.2 and Section 4.2 are not consistent --- Section 3.2 suggests sampling from the paths (connections), while Section 4.2 suggests sampling from the vertices (activations). I believe in practice sampling from activations is used (correct me if I am mistaken), which indeed implies that random matrices {P_1, ..., P_n} are not independent (I guess it forms a Markov chain?). Therefore, the analysis in Section 3.2 is not applicable. I think an analysis using dependent sampling (ancestral sampling) is needed in this case.\n\n(2) The explanation for convolutional layers is not sufficient. The paper mentions that the compact structure could be exploited multiple times, but exactly how it is somewhat missing. I am particularly wondering whether the activations (pixels) or the feature maps (channels) are sampled. If the activations are sampled, I think it is not friendly to the CUDA kernels as the activations within the same channel are computed together; And if the feature maps are sampled, it again violates the analysis in Section 3.2 (since some paths must be chosen together as a group).\n\n(3) It is not clear how the proposed method is compatible with normalization layers. In normalization layers (e.g., batch normalization), the statistics of the activations are computed. However, since the activations now are sampled, it is not explained how the normalization layers should be modified accordingly to preserve unbiasedness.\n\n(4) It is not clear how the proposed method is compatible with randomized operations in neural networks (e.g., dropout, reparameterization, etc.) I am wondering if the combination is still unbiased.\n\nGiven that the technical explanation is lacking for the moment, it is hard for me to judge this paper's correctness. I will temporarily give a reject. I am very willing to increase my score if the authors address the confusion above.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}