{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "The paper leverages concepts coming from hindsight relabelling methods to define a novel \"iterated\" supervised learning procedure to learn policies to reach different goals. The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation. \n\nThere is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (e.g., the comparison with Go-Explore) and reinforced the empirical analysis. This is a clear accept."
    },
    "Reviews": [
        {
            "title": "Official review ",
            "review": "the authors of the paper propose a new way to learn goal-reaching policies by utilizing the previously collected trajectories in an iterative manner. Their novel approach, called goal-conditioned supervised learning (GCSL), learns to reach goals from a target distribution by running the policy and collecting suboptimal trajectories, and then relabeling these collected trajectories during training to perform supervised learning on them to update the policy. This behavioral cloning is done iteratively until convergence.\n\nRelatively ample evaluations and experiments were conducted in multiple settings (5 control environments). GCSL outperforms some RL algorithms in these tasks and is shown to be more robust to hyperparameters.\n\nThis work is very close to hindsight relabelling methods [Schaul et al., 2015; Andrychowicz et al., 2017; Rauber et al. 2017], but authors state that their method is more stable and does not estimate a value function.\n\nPros:\nThe paper is well written and structured.\nThe idea of using previous rollouts and relabelling them to use as training data iteratively is very interesting.\nThe analysis and ablation of learned behaviors with different data collection and relabelling settings is well done.\n\nCons:\nWhile many ablation studies have been done, the impact of the frequency of performing supervised learning via behavioral cloning on the performance of GCSL is not clear. How many trajectories are collected and relabelled before every policy update via behavioral cloning? and did you do any ablation studies for this?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice paper with an interesting algorithm, which could be made better by more discussion on some related work and potential limitations. ",
            "review": "In the paper \"Learning to Reach Goals via Iterated Supervised Learning\", the authors propose a new approach to build conditional policies for reaching tasks that can reuse the previous failed attempts as new examples on how to reach the state that was actually reached during the failed execution. This approach is similar to the approach introduced in HER, but is based on Behavioural Cloning algorithm (thus the name self-imitation) instead of a value function. \n\nThe paper is overall well written, clearly illustrated and appropriately structured. \n\nIn my opinion, the proposed method shares significant similarities with the \"First return then explore\" paper (Ecoffet et al. 2020), which is a new version of the GO-Explore algorithm that builds a similar conditioned policy to \"return\" to a state and then explore from this state using a random action, which is then used to extend and improve the learned policy. Given the strong links between these two papers, I am surprised to not see this discussed related work section and compared in the experimental section. \n\nMy main concern about the algorithm is the risk of a lack of exploration. An extreme example is a degenerated policy that outputs 0 (i.e., no movement) regardless of its inputs. The relabelling will just reinforce this behaviour and the algorithms will quickly converge in a local optimum (which is actually far from being optimal). Obviously, this is an extreme example which is quite unlikely, but the same can happen if some states/goals are very unlikely to be observed given the policy. This can happen for instance, if the policy has a strong bias. \nIn GO-Explore this is avoided by performing a random action at the end of a rollout so that the exploration is not conditioned by the current policy. However, the combinatorial generation of goals/labels might be more challenging in this case and thus leading to fewer data per rollout. Ideally, this should be evaluated and compared in the experimental results. However, I have to say that the experimental evaluation is already quite elaborated and provide interesting insights into the performance of the algorithm. \n\nOverall, this is a nice paper with an interesting algorithm, which could be made better by more discussion on some related work and potential limitations. \n\n\n----------- \nUpdate after rebuttal:\nI am very pleased by the answers of the authors, in particular, with the additional experiment showing that the algorithm could be extended with more advanced exploration strategies. I reviewed my rating accordingly. \n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposes a new RL algorithms dedicated to learning goal-oriented policies. In the described setting, a policy has to reach a particular goal in T steps such that s_T = goal i.e the objective is not to reach the goal as soon as possible but in a particular number of steps. The proposed algorithm is very simple: it is an iterative algorithm where, at each iteration, i) a policy is used to collect trajectories. ii) Then, trajectories are relabelled based on the reached states -- reached states are becoming goals in the relabelling, iii) a new policy is learned by using behavioral cloning. In addition to the algorithm, the authors provide some theoretical insight about how and in which conditions one can prove that the algorithm is working well. At last, experiments are made on different environments and compared to some baselines showing that it allows to solve complex problems in less learning iterations. Moreover, an ablation study is provided to allow one to understand the effect of the different aspects of the learning technique. \n\nFirst, the proposed technique is very simple, sharing some similarities with existing techniques (hindsight relabelling), but simplifying these techniques by just using behavioral cloning to discover a new policy instead of other and more complex value-based techniques. This idea may have two advantages: i) it is simple and ii) it may benefit of the robustness of supervised learning techniques, and particularly the fact they supervised learning may need less hyperparameters than other RL approaches. This is an interesting point and a good aspect of the article. \n\nBut the consequence seems to be that the algorithm is optimizing a criterion which is not the classical criterion used when learning goal-oriented policies. Indeed, here, the objective is to find a policy that reach a particular goal in T steps, and not to reach a goal as soon as possible. I think that this aspect is opening different questions: \n\nA) the first one is the interest of learning such policies. The article does not really justify why and in which applications discovering such policies may be interesting. Indeed, reaching a goal in T steps may appear more difficult than reaching a particular goal in less steps, or as fast as possible. Moreover, in a stochastic environment, reaching a goal in T steps may be impossible due to the stochasticity of the environment while reaching the goal in less than T steps may be easier. I think that a better discussion on that point would improve the quality of the paper, while right now, it is not clear to me when such an objective function is interesting to study. \n\nB) The second problem is related and in the experimental section, when comparing to classical RL techniques. Indeed, the reward function defined in this paper is very specific (i.e the agent get one if it reaches the goal at the last state of the trajectory), and classical RL techniques are using a discount factor lower than one which thus encourages the models to discover policies reaching goals as soon as possible. Again, a discussion explaining how exactly the comparison is made, and what is exactly compared would be interesting. It gives me the impression that RL techniques are used in a particularly difficult setting, why they may perform well on different but interesting reward functions encouraging for instance to reach the goal as soon as possible, or to reach the goal once during the T timesteps (and not only at the last state). \n\nC) Also, some aspects are confusing: in section 5.1, it is written \"we parameterize the policy as a neural network that takes in state, goal, and horizon as input\" while in appendix A.1, it is written \"We parameterize a time-invariant policy using a neural network which takes as input state and goal (not the horizon),\" which is completely different. This is a crucial aspect since I do not understand how a policy that is not using the horizon as input is able to reach a goal at a particular timestep. \n\nOther  comments: \nTheorem 3.2 focused on deterministic environments is not really interesting since it considers a very particular case which is not a classical RL setting. \nThe way T is chosen is not clear to me. For instance, in many environments, in order to be able to reach interesting goals, T may have to be very large, and is unknown until a good policy is  known. \nThe extension about using demonstrations is an interesting aspect that would be interesting to develop more in the paper\n\nSummary: \n The paper proposes an interesting and simple model that optimizes a particular and not classical objective. It is not clear if this objective is really interesting, and in which applications such a method can be used. The comparison with other techniques is fair, but considering this very particular objective while other objectives may be easier to learn and more interesting. Some aspects are not clear in the paper and the experimental section has to be improved.\n\n\nRegarding the answers provided by the authors, I increase my score ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "### Summary\n\nThe authors propose “GCSL” (goal-conditioned supervised learning), an algorithm that bridges the gap between reinforcement learning and imitation learning. Specifically, the authors are motivated by the limitations that RL is brittle when used with sparse rewards, and that IL requires expert demonstrations. Their technique performs learning without expert demonstrations, and does not require a learned value function or reward function. The authors formulate GCSL as iterative trajectory collection, goal relabeling, and policy refinement through behavioral cloning, and formally derive performance bounds on this technique. In addition, the authors demonstrate strong goal-reaching performance and robustness improvements over current RL techniques.\n\n### Strengths\n\n- The authors’ proposal is strongly motivated, in that RL techniques are clearly sensitive to hyperparameters and face stability challenges, and that using demonstrations is more robust. The paper is clearly written and the authors very cleanly explain their key ideas and insights. \n- The authors’ proposed algorithm is based on a simple idea, in a very good way. Their technique is stable, requires no value functions or reward function engineering (as is common with traditional RL techniques), and their technique of relabeling generates a large number of samples, resulting in better data efficiency.\n- The authors’ main theoretical insight that iterating on data from sub-optimal agents leads to optimal behavior is both non-trivial as the authors claim, and can likely be used to inspire other similar ideas or areas of study.\n- The authors clearly showcase the experimental use cases of their technique by demonstrating its benefits in terms of stability to hyperparameters and in leveraging expert demonstrations. Apart from this strong set of experimental results, the authors also present detailed ablations in the supplementary pages that are very convincing.\n\n---\n\n### Weaknesses\n\n- The theory section does seem a bit contrived. Specifically, optimality emerges when using the objective proposed by the authors, even if this is not commonly used. Further, the proof assumes that trajectories are collected from a single policy and relabeling is only performed on the last timestep, whereas in practice these conditions are not met. The performance guarantee also only holds with deterministic transitions. Finally, the guarantee on the convergence behavior is a good property, but is maybe less meaningful if in practice GCSL is difficult to converge to (if the optimization is challenging). \n- Notably, the authors do not evaluate the generalization ability of their technique, and instead evaluate with the same train and test environment. Although this is common in RL, it would be interesting to see how GCSL performs in environments that require more sample efficiency and test generalization, like Procgen.\n- The authors quantify performance as the distance of the agent to the goal at the last timestep. I’m not convinced this evaluation metric makes the most sense, especially when success ratio is the main metric we care about.\n- On door opening, do the authors have an idea for why TD3+HER performs so much worse than PPO or GCSL? Similarly, on Sawyer pushing, do the authors have a hypothesis for why PPO and TD3+HER do not learn at all? Is there potentially a qualitative analysis of this unexpected behavior that can be performed? What about GCSL makes it so much more successful on Sawyer pushing?\n- I’m surprised that GCSL is not any more sample efficient, since PPO and broadly speaking other policy gradient techniques are generally well-understood to be greedy sample-wise. \n- The authors earlier in the paper hypothesized that an optimal policy would be non-Markovian, but that GCSL with a Markovian policy would outperform a time-varying one. This seems rather counter-intuitive to me, why do the authors suspect this is the case? Is the model overly exploiting instead of exploring when conditioned on the remaining horizon? Does this behavior change depending on the number of timesteps in an episode?\n- In Figure 4, do the on-policy methods that converge slower do so with an empirically visible better convergence guarantee? For instance, even if it takes longer, the technique is guaranteed in the long-run to arrive at an optima?\n- In Figure 4, the authors compare against limited-horizon relabeling as in prior work and show this method has drawbacks. Are the drawbacks due to the lack of multi-horizon relabeling, or due to having much fewer trajectories after relabeling?\n- The authors compare robustness to hyperparameters against TD3, but do not compare against PPO. Because PPO is generally well-understood to not require much hyperparameter tuning empirically, I would recommend including this comparison as well. \n- The authors claim qualitatively (in Appendix C) that despite the different objective, the learned trajectories generally take a direct path to the goal. Is there a way to quantify this quantitatively against an oracle that does take the shortest path; for example by comparing the time taken to get to the goal, or the time spent at the goal waiting for the episode to complete?\n\n---\n\n### Recommendation\n\nOverall, I vote for accepting. I think this current submission is already relatively convincing as an accept, as it is clearly written, has well explained motivations, strong experimental results, and extensive ablations in the supplementary pages. The authors’ idea is conceptually simple, in a good way. My main gripe is that the theoretical results are only weakly held, or less relevant in practical settings, but that is rather typical and the strong experimental results do speak for themselves. I do have a few clarifying questions on the experimental results, but am regardless confident that this paper meets the ICLR acceptance criteria.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}