{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "The paper is studying a new intrinsic motivation RL setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. The resulting idea is simple but also surprising that it works so well. All reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. In addition, they identified the following strengths of the paper:\n- The experiments are exhaustive, identifying many domains where the approach can be applied\n- The presented results are compelling\n- The paper is well written\n- The paper introduces a new problem setup that has not been studied before\n\nI agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.\n\n"
    },
    "Reviews": [
        {
            "title": "Official Review #2",
            "review": "This work proposes an RL approach SMiRL that is able to learn effective policies in unstable environments without the need for external reward. The idea at a high-level is almost the opposite of intrinsic motivation RL approaches, which encourage novelty-seeking behaviors. The proposed method instead aims to minimize surprise or state entropy. To train the agent, rewards come from state marginal estimates, but because this distribution is changing, the authors create an augmented MDP. Through experiments on game domains and robot control tasks, the authors show that SMiRL outperforms intrinsic motivation methods. The authors also show that SMiRL can be used to do imitation and can be combined with regular reward signals.\n\nPros:\n- The problem formulation is interesting and novel. Intrinsic motivation is well studied, but this problem considers the setting where the environment is unstable rather than static, which requires new methods.\n- The paper is written well and is clear. The motivation is described well.\n- The authors evaluate on many domains, highlighting the diversity of settings in which the approach can be applied.\n\nCons:\n- It seems like this approach is only applicable to unstable environments. Does the approach fail for regular, static environments? I’m assuming the agent might just end up staying still because it’s trying to seek stable states. It can be combined with the external reward signal but will the minimizing entropy objective hurt you?\n- Without common sense knowledge, this approach would take many iterations to learn. So while this formulation might be more similar to the real world, the real world would only allow for a few interactions with the world.\n\nComments:\n- How is SMiRL doing better than the oracle in Figure 3 center?\n- In Figure 3 left, it might be a problem that with minimal episodes, SMiRL does worse. If SMiRL is useful for more real-world unstable environments, this would require a simulator good enough to model the real world.\n\nRecommendation:\nOverall, the paper is interesting and novel. The approach is reasonable and experiments show the value of the method in unstable environments. I recommend “accept”.\n\n------------\nPost-rebuttal response:\nThe authors addressed most of my concerns so I continue to recommend acceptance of the paper. Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency. They also conducted additional experiments to verify some of my questions.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments.",
            "review": "The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments. A state distribution is fitted during the interaction with an environment and the probability of the current state is used as a virtual reward. The parameters or sufficient statistics are also applied to the policy. The motivation is clear and verified. It is generally a good paper.\n\nIt is surprising that the exploration is achieved in the long term even minimizing state entropy. Is that possible the exploration events are from the 'unstable' environment? What if there are some patterns underlying the exploration events but only part of the 'unstable' environment? Is that OK to totally rely on unexpected events from the environment to explore the environment? Is that possible to add some exploration strategy in the developed model? ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper introduces suprise minimization as a intrinsic objective. The idea is novel, well motivated and seem to be supported by empirical evidence. However, some details are missing and there might be a confounding factor.",
            "review": "------------------------------------        \n**Summary:**\n\nThis paper proposes a new intrinsic objective for RL agents: surprise minimization. This may come as a surprise, as other related works usually propose to maximize surprise, or to maximize novelty. The authors present motivations and conduct an empirical study on several environments to support their idea.\n\n------------------------------------        \n**Strong points:**\n\nOverall, I think it is a good paper. Let me list some strong points:\n\n* The idea is simple, novel and well motivated. The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective.\n* It’s well written and organized\n* The algorithm is tested on a relevant selection of environments and against state-of-the-art algorithms using intrinsic objectives.\n* The empirical evidence seems to support the claims.\n* The related work is quite complete.\n* I liked the discussion about stable vs unstable environments. It is the first time I see it discussed.\n* The website brings visualizations of trained policies.\n------------------------------------        \n**Weak points:**\n\nI will now list a few weak points of the paper.\n* Some descriptions of the results are missing. In figures, what does the shaded area represent? (std, sem, confidence intervals, etc). In Table 1, what are the numbers? (what is the central tendency, what is the error, how many seeds?). Same for Appendix D.\n* The paper does not provide all necessary information to reproduce the results. There is no detail about the RL algorithms (TRPO and DQN), no description of the architectures, and no hyperparameters. This is important and should be contained somewhere in the Appendix. \n* Will the code be released? If not, why so? Same questions for the environments, are they accessible somewhere? \n* It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom. The x-axes of the figures also tell us that only a few episodes were needed to solve them. I am not saying that I need the hardest games solved to find an algorithm interesting, but I am wondering whether it would scale. Could you tell us whether you attempted to tackle harder environments, and if so, why do you think SMiRL failed? I think we can gain a lot of understanding by looking at negative results. \nFor example, I feel like testing this algorithm on a 3D humanoid would better demonstrate the power of this approach. \n* It seems to me that there might be a confounding factor that could partially explain the success of the surprise reward. Indeed, it seems that all environments presented here can terminate when the agent dies. This is a guess, as I could not find this information in the paper (please add it). If so, then the expected cumulative rewards is an increasing function of the lifetime of the agent in the game. Because of this, maximizing the cumulative surprise might be a good idea because it goes in the same direction (by construction) as maximizing survival.\nA counter-argument from the paper can be the performance of SMiRL + reward in Walk, that is superior to the performance of reward alone. However, this is unclear as I could not find the description of the reward function in the paper (please add it). \nAnother way to disprove this hypothesis would be to compare the performance of SMiRL to the performance of an agent maximizing a reward function that gives +1 whenever the agent is alive (survival bonus). If it performs better, then surprise maximization brings something to the table, if it does not, then it might work because it is correlated to the survival time.\nAnother way could be to have episodes that do not include death-related resets (fixed length episodes).\n\n------------------------------------        \n**Recommendation and justification:**\nThis is overall a strong paper. However I’m concerned about the potential confounding factor of the survival time. I’ll give a score of 6, but I would happily increase that score if \n1. The authors convince me that the success of the surprise maximization is not due to the survival confounding factor.\n2. The authors include all necessary details for reproducibility and/or release the code. \n3. Discuss the scalability to harder problems (e.g. 3D Humanoid).\n\n------------------------------------        \n**Feedback to improve the paper (not part of assessment)**\n\n* I would move the discussion about how surprise minimization and novelty maximization can be complementary to the intro. These two approaches seem to go in opposite directions and, as a reader, I would be happy to read this discussion early.\n* It would be interesting to discuss how it plays out in natural agents. The intuition is that minimizing surprise leads to finding a stable configuration and staying there. In practice, it is probably balanced with other driving needs like the need for food. I guess it is discussed in related papers like Friston 2009. In natural agents, surprise minimization must also be model-based. Indeed, animals do not need to jump out of cliffs several times to know that it’s bad. \n* Not sure I understand the inequality in Eq1. Maybe I missed something.\n* Discuss the surprise maximization approach of Achiam et al and whether it differs from yours. \n* Table 1: the legend seems to disagree with the results. It seems to me that the entropy difference is as low in your environment as in the others, but the caption says “note the clear negative  entropy gap on our tasks, whereas this clear trend is absent on the Atari games”.\n* Fig 4. Is it really training in 80 episodes? There are very few images to train a VAE, especially if the episode resets when the agent falls (does it?) How many steps per episode?\n* How do you get demos for humanoid tasks? I guess there are not human demos but previously trained agents? \n* Results in Fig. 6 are not super satisfying, they are quite far from the target (although I guess it is a difficult task). I am not even sure the second example is achievable. In the traditional Tetris, wouldn’t cubes fall due to gravity?\n\n**Typos:**                \n* “our results are available online” → missing full stop.\n* “In such environments, which we believe are more reflective of the real world” → previous sentences do not discuss environments but intrinsic objectives.\n* “unexpected events don’t happen” → “do not”\n* “deep DQN” → “Deep Q-Networks”, or “DQN”\n\n\n**Update post-rebuttal**\nThe authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but several issues need to be addressed",
            "review": "This paper presents and studies an unsupervised learning approach to the emergence of \"meaningful/useful\"\nbehaviour in a Deep RL setting, based on an algorithm which objective consists in minimizing surprise.\nIt is possible to see this paper as studying a Deep RL implementation of cognitive homeostasis, an old idea\nrecently popularized through the work on the free energy principle by Friston and colleagues.\nThe paper presents two versions of this algorithm, one without representation learning, and one with\nrepresentation learning (VAE), and compares it to Deep RL algorithms that maximize prediction error/novelty\nfrom two perspectives: emergence of behaviours and utility as an auxiliary reward to solve sparse reward problems.\nThe paper discusses the relevance of the approach in environments that are said to be \"unstable\" (with an attempt\nto formalize this concept). The paper also shortly presents an application of the algorithm for imitation learning.\nExperiments are presented in a variety of environments.\n\nMajor strengths:\n+ The paper is globally clear and well-written\n+ The qualitative results showing and analyzing emergent behaviour are stimulating\n+ Implementation of the free-energy principle in high-dimensional spaces is known to be challenging and this\npaper contributes towards understanding how to do it (yet the precise formal articulation between SMIRL and the\nFEP remains to be done, and this does not seem to be the primary objective of the paper)\n+ The discussion of the complementarity of within-episode SMIRL and across-episode novelty seeking intrinsic\nrewards is interesting\n+ The shortly described application for imitation learning is promising\n+ The are many and varied environments in experiments \n\nMajor weaknesses:\n- The environments chosen for experimentation are such that minimizing surprise aligns very well with either\nproducing \"interesting\" behaviour or maximizing an external reward. One could easily consider slight modifications\nof most environments where a trivial solution to minimizing surprise could be found instead (e.g. giving the ability to push\non the \"pause\" button in the Tetris game or having safe rooms in opposite directions than enemies in Vizdoom/HauntedHouse).\nIn a real robot, this would lead a robot to hide in the corner of a room and just look at a uniform wall that does not move.\nThis problem of trivial solutions to surprise minimization approaches has been called the Dark Room problem in\nthe FEP literature. This is known to be a limit of this general approach when applied to the real world, and how to address it\noperationnally (in a tractable manner) under this paradigm remains an open question.\n- Related to the point above, the fact that different environments are used in different experiments of the paper does not\nallow to get a good grasp of the general behaviour of the algorithm: I would recommend to run all experiments in all\nenvironments (maybe to report in Annex), and at least to justify why environmnets change across experiments.\n-  The paper is positioned in comparison with novelty seeking/prediction error maximizing agents, and motivates\nthe surprise minimizing approach by arguing that real-world situations are problematic for novelty seeking agents,\nas the real world spontenously generates novelty through distractors that should be avoided rather than explored.\nThis is accurate, but an incomplete discussion of the literature in the area of unsupervised learning of behaviours:\napproaches maximizing learning progress were precisely introduced to adress these limits of novelty seeking approaches\nand to scale to real world environments with two families of distractors: novel unlearnable parts of the environments\n(which is a problem for novelty seeking approaches) and trivial low-entropy parts of the environments (which are a problem\nin principle for surprise minimization approaches). Thus, the paper should position itself in comparison with LP-based\napproaches and show experiments with at least some of the existing LP-based algorithms (e.g. Schmidhuber, 1991; Lopes et al., 2012;\nKim et al., 2020).\n- As the authors acknowledge, the version of SMIRL using represention learning is not fully \"within episode surprise minmization\",\nas the learned representation depends on past episodes: as a consequence, it is unclear what one should conclude about the relation\nbetween within vs across episode mechanisms and the properties of emergent behaviours\n- The paper lacks discussion of related work aiming to implement surprise minimization approaches that scale to high-dimensional\nspaces, especially in a RL framework, e.g. Tschantz et al., 2020; Annabi et al., 2020\n- The paper does not provide code to enable reproducibility of results (and does not say it will)\n\nOverall, the topic of this paper is interesting and the work could make a valuabe contribution, especially from the perspective\nof studying incentives to emergent structured behaviour in unsupervised learning,. However, the weaknesses mentionned above need\nto be addressed to better establish the contributions of this paper, both in terms of understanding the generality of the\nresults and the contributions in relation to the existing litterature.\n\nReferences:\n\nAnnabi, L., Pitti, A., & Quoy, M. (2020). Autonomous learning and chaining of motor primitives using the Free Energy Principle. arXiv preprint arXiv:2005.05151.\nKim, K. H., Sano, M., De Freitas, J., Haber, N., & Yamins, D. (2020, January). Active world model learning in agent-rich environments with progress curiosity. In International Conference on Machine Learning (ICML).\nLopes, M., Lang, T., Toussaint, M., & Oudeyer, P. Y. (2012). Exploration in model-based reinforcement learning by empirically estimating learning progress. In Advances in neural information processing systems (pp. 206-214).\nJ. Schmidhuber, “Curious model-building control systems,” in Proc. Int. Joint Conf. Neural Netw., Singapore, 1991, vol. 2, pp. 1458–1463.\nTschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020, July). Scaling active inference. In 2020 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE.\nTschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learning through Active Inference. arXiv preprint arXiv:2002.12636.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}