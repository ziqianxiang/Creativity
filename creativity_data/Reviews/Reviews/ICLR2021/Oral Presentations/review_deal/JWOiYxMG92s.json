{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper proposes a novel and powerful data augmentation strategy for few-shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily  resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.\n"
    },
    "Reviews": [
        {
            "title": "Marginally below threshold.",
            "review": "Summary\n\nThe paper proposes a method to calibrate the underlying distribution of a few samples in the few-shot classification scenario. The idea is to estimate a feature distribution of a few samples of a novel class from base class distributions. The authors assume that every dimension in the feature vector follows a Gaussian distribution. Based on the observation that the mean and variance of the distribution with respect to each class are correlated to the semantic similarity of each class, base class distribution can be transferred to the novel class distribution. After distribution calibration, features can be generated from the calibrated distribution and the generated features are used to train classifiers. SVM and logistic regression classifier are used to verify the approach on the mini-imagenet and CUB datasets.\n\nPros\n-   The idea can be applied to any types of feature extractors or classifiers when the Gaussian distribution assumption holds.\n-   The proposed approach shows competitive performance on two benchmarks, mini-imagenet and CUB.\n-   Extensive ablation studies verify hyper-parameter settings and their sensitivities.\n\nCons\n- Many hyper-parameters are involved in the approach. ( Turkey’s transformation parameter lambda, top-k base distributions, dispersion parameter alpha, number of generated features)\n-   Some hyper-parameter setting should be different depending on the dataset. (dispersion parameter alpha)\n-   The paper uses the previous work (Mangla et al. 2020) as a baseline and applies their approach on top of it. While theoretically the approach can be applied to any types of feature extractors, only one baseline improvement is shown in the paper.\n\n\nRating\n\nI give marginally below the threshold. The paper shows good performance and also claims the approach can be applied on top of any classifiers or feature extractor. The general applicability and effectiveness are the strength of the paper. However, the proposed approach is only verified on one baseline approach (Mangla et al. 2020). More empirical data would strengthen the claim of the paper.\n\nQuestions\n\nThe approach outputs calibrated distribution, i.e., a mixture of Gaussian distributions. These distributions can be directly used to classify query samples by calculating the likelihood of the sample on each distribution. How does likelihood classification perform compared to the retrained classifiers (SVM or RL)?\nThe optimal values for top-k parameter and alpha parameter are different depending on the dataset. Are optimal values for lambda and the number of generated features same or different depending on the dataset? Did the authors use the same lambda and number of generated features for both mini-imagenet and CUB experiments?\nThe authors only applied the proposed method on one baseline approach. Is the approach effective for more variety of backbone networks and losses? \n\nFeedback\n\n- Table1 shows that there is a correlation between feature distributions and semantic similarities. It would be interesting to see how the distribution calibration performs on each class depending on the similarity level of top-k base distributions.\n- The empirical evidence is not sufficient to claim the approach applies to general network architectures and few-shot learning approaches. Experiments with more baseline approach to show the effectiveness of the idea is recommended.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simple and effective method for calibrating the few-shot class distribution",
            "review": "Summary:\n\nThis paper identifies the problem of biased distributions in few-shot learning and proposes to fix it. In few-shot learning, only a few samples per class are available; this makes estimating the class distribution difficult. The paper proposes a distribution calibration algorithm that makes use of the meta-train class distributions to calibrate the few-shot class distributions. Once calibrated, more samples are drawn from this distribution to learn a classifier that generalizes better. This approach does not require additional learnable parameters and can be (potentially) built on-top of any pre-trained feature extractor. Empirical results show that this approach achieves state-of-the-art results on Mini-ImageNet and CUB.\n\nPros:\n1. This paper identifies and tries to tackle an important problem in few-shot learning - estimation of the class distribution. Due to the limited number of samples, this problem is difficult, but important for few-shot learning. The proposed algorithm is simple and effective in tacking this problem.\n2. As opposed to other related works, the proposed algorithm does not have any learnable parameters. It makes use of the features obtained for the meta-train and few-shot samples.\n\nCons:\n1. One of the claims of the paper is that the proposed algorithm is pre-trained feature extractor agnostic. However, there are no experiments to validate this claim. Consider adding feature extractors trained in different ways.\n2. Some of the popular few-shot datasets were not included in the experimental section, namely Tiered-ImageNet [1] and Meta-Dataset [2]. The former is a larger dataset than the ones used, and the latter provides cross-domain results for the approach. The proposed algorithm assumes that statistics from the meta-train classes would transfer to the few-shot classes, which would be tested more thoroughly in the cross-domain setting.\n\nClarifications:\n1. In the feature space, it is intuitive that the means of similar classes would be correlated. Is there a justification for why this is true for the variances?\n2. Tukey's Ladder of Powers transformation is used only on the few-shot samples and not the meta-train samples. Is there a reason for that? If the pre-trained feature extractor is trained using a certain metric (cosine distances for example), I would imagine transforming all the features would be beneficial.\n3. The backbone used in the experiments is trained using a supervised and self-supervised loss. What are the results without the self-supervised loss?\n4. How many samples are drawn from the calibrated distribution for the numbers in Table 2?\n\nNotes:\n1. The performance of few-shot learning algorithms has been traditionally evaluated using the averaged accuracy over multiple tasks, but that is not the only way to do it. Look at [3] for details.\n\n[1] Mengye Ren et al. Meta-Learning for Semi-Supervised Few-Shot Classification.\n[2] Eleni Triantafillou et al. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\n[3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple data augmentation for few-shot image classification",
            "review": "This paper presents a simple and intuitive data augmentation method for few shot image classification. The proposed method assumes the feature distribution of a single class to be gaussian. Based on this assumption, the proposed method samples features from a gaussian distribution that is constructed using the statistics of the similar base classes. Combined with logistic regression, the proposed method achieves strong performance on two standard few-shot image classification benchmarks. This submission also carries out an ablation study on several design choices, which I really appreciate.\n\nThe submission is well-written and clear. The proposed method is novel and can inspire future augmentation-based methods in few-shot image classification.\n\nI have a few more requests/ablations that I am curious to see. \n\n- Looking at Figure 4, the 1-shot accuracy with only 1 retrieved class is already very strong. Instead of sampling from the \"calibrated\" distribution, can we simply retrieve examples from the nearest class? Namely, find the nearest class, randomly sample some examples, and use their features to augment the novel classes. This ablation should make clear what additional value the sampling procedure adds.\n- In equation (6), how important is it to include the novel class feature into the mean? Can we simply do \\mu_prime = \\sum_{i \\in S_N} \\mu_i /  k?\n- This method makes an important assumption that the feature distribution is gaussian. How well does this assumption hold in practice? Can the authors provide some analysis of the feature distribution? Ideally both before and after Tukey’s Ladder of Powers transformation. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}