{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting problem but evaluation needs to be improved",
            "review": "This paper studies the problem of comparing the performance of different neural network checkpoints for downstream tasks. It starts off by defining a benchmark, specifying various sources of obtaining pre-trained checkpoints. A set of evaluation metrics is then proposed to evaluate a given ranking method. It then provides an empirical analysis of a set of ranking methods, including a novel one called Nleep. \n\nPros:\n+ Having an efficient measure to predict the performance of a given model without resorting to actual training is a very practical and important problem, especially for practitioners. \n+ I also appreciate the effort to set up a valid benchmark, by covering a diverse source of obtaining pre-trained networks, along with the evaluation metrics. Both of which seem reasonable to me.\n+ The proposed NLeep metric makes sense to me, and shows promising results.\n\nCons:\n-My biggest concern is the formulation of the problem. In Equation 3, the problem of searching for a proper ranking function is a lot like a meta learning problem. However, in the meta learning setting, there is a clear separation of a meta training set and meta test set. While in this paper, the problem is formulated to search for a R that *overfits* the meta training set, and it is the *meta train* performance that is reported. This leaves the question of how the selected metric generalizes to a different set of checkpoints, even obtained from the same family. I think this is a significant flaw of the evaluation that needs to be addressed.\n\nI suggest the authors provide improved evaluations regarding this, in order to make it possible to calibrate the true value of different ranking functions, especially the proposed one. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Borderline paper with missing experiments and baselines",
            "review": "This paper presents a neural checkpoint ranking benchmark and study different simple ranking measures. Authors also propose a new ranking measure which is an extension of recent LEEP measure which provides the best performance in the experiments.\n\nOverall, I liked the neural checkpoint ranking benchmark and the problem of ranking pretrained models for transfer learning. However, the paper has limited novelty in terms of the proposed measure as it is a simple extension of LEEP via GMM. While the neural ranking benchmark could be useful, I feel it is far from a complete benchmark with lack of many important experiments and analysis.  More specifically, additional experiments and comparisons are needed to justify the future reuse of the benchmark. Thus, my decision is borderline (leaning towards reject) and I would like authors to address my concerns during the rebuttal period.\n\nThe benchmark contains only 4 downstream tasks to measure the performance of different ranking methods. Moreover, all the 4 downstream datasets are from similar domain as the source tasks/pretrained models. Performance evaluation on 4 tasks in transfer learning is too less as conclusion might change while considering large number of diverse tasks. Authors should experiments on publicly available transfer learning benchmarks like VTAB (A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark) to verify the effectiveness of the proposed measure in checkpoints. VTAB benchmark contains datasets from a variety of domains which will further help to see the performance of different ranking measures on transfer learning to cross-domain datasets.\n\nAuthors show the performance of different methods on medium data regime. How does amount of data in the target task affects the performance of ranking measure? Does the same ranking hold in low data regime? Authors should perform experiments with different amount of target data to verify the ranking of different methods in different target data regime. \n\nWhy does some of the discriminators in generative models under-perform From-Scratch? Is there any specific reason for this? Is it somehow related to convergence during finetuning? Does increase in number of finetuning epochs improves the performance of generative method? Additional analysis could be interesting for the benchmark.\n\nThere are few recent papers which discuss selection of pretrained checkpoints in transfer learning. E.g., Duality Diagram Similarity: a generic framework for initialization selection in task transfer learning, ECCV 2020; DEPARA: Deep Attribution Graph for Deep Knowledge Transferability, CVPR 2020. How is the proposed approach related to these prior works? These paper should be clearly discussed with proper comparison in the experiments.\n\nHow does the proposed method compare to a simple baseline on nearest-neighbor accuracy to rank pre-trained models as in Scalable transfer learning with expert models (Puigcerver et al, 2020).\n\nHow does the computational budget impact the findings? What is the effect of number of pretrained models on the ranking?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting direction, but the paper perhaps is rather limited than suggested from the title and abstract",
            "review": "**[Summary]**\nPre-trained networks (checkpoints) can be useful to serve as a starting network for transferring the network to a new yet unknown downstream task. The authors establish a neural network checkpoint ranking benchmark and use that benchmark to measure the conventional selecting checkpoint methods. They also propose a new ranking measure to rank the checkpoints that outperform conventional methods with a little computational cost.\n\n**[Reason for rating]**\nI initially had high hope for this paper, because the problems the authors work on is practical and can be impactful for many industrial users. However, reading through the paper, I realized that I have some doubts about the scale of the benchmarks, and the findings that the authors present seem to be insufficient to the standard of ICLR. Thus, I am rating 4 but leaning towards a rejection now. \n\n**[Pros]**\n- Establish a benchmark for testing methods in selecting checkpoints for transferring a pre-trained network to a new downstream task.\n- Propose a new method for selecting checkpoints\n\n**[Cons/Questions]** \n- The authors seem to claim that the proposed checkpoint ranking method can work with a wide range of downstream tasks. Yet, it seems to only cover object recognition, fine-grained object recognition, scenery image classification, and medical image classification. These are all image-related tasks, but I would suggest the authors to clarify this early on in the paper that this is for (or at least only validated) images only, not covering NLP, speech, etc.\n- There are a total of four datasets used to establish such benchmarks, which, in my personal opinion, seems to be rather small for claiming that this is the benchmark for methods of selecting checkpoints. There are way more datasets/tasks even if we are just talking about image domains, e.g., depth estimation, segmentation, etc. I am not sure the four datasets on classification and recognition are sufficient to be claimed as \"benchmarking\", especially when the type of downstream task can be drastically different from classification and recognition. \n- The flow of paper writing is a bit hard to follow. It reads a bit like an account of things that are flush out. This paper can use a bit of re-organization. For example:\n  - At the beginning of Sec. 4, why do the authors choose to discuss how the free parameters in NLEEP affect its checkpoint ranking performance? And, what was the conclusion of this discussion/analysis?\n  - In \"Comparison results,\" there is no discussion on the results. Instead, some details of the comparing method are flushed out. \n- I believe the 2nd most important part of this paper will be in the \"Main findings\" in Sec. 4. Yet, there is only nearly half of a page to discuss how different traditional ranking methods perform under different groups (different supervision, network architectures, pre-training stages that the authors defined). Also, many of the findings are mostly speculations without proper verifications. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well written paper, interesting results. Some points need clarification.",
            "review": "This paper deals with measuring which pretrained DNN would transfer best to a downstream task. The authors propose a ranking benchmark called NeuCRaB, which covers various types of pretrained DNNs. They also propose a measure to rank the DNNs, called Gaussian-LEEP, which is a modification of the LEEP algorithm.\n\nJustifications for using Gaussian-LEEP are that it is light-weight, can work for DNNs without a classification head (as opposed to LEEP), and does not rely on the DNNs confidence value (which are miscalibrated). Empirical results show that Gaussian-LEEP ranks better than other baseline methods.\n\nOverall, the paper is well written and easy to read.  A few comments:\n\n1. Essentially, what is the difference between checkpoint ranking and task transferability? The paper states that the latter is easier.\n\n2. Table 1 does not show results for LEEP, since the checkpoints in Group I of the benchmark cannot be evaluated by it. Please explain this point further; group I of the benchmark seems to include various training methods on resnet50, why are they considered as not having a classification head and therefore not applicable for LEEP?\n\n3. One of the justifications for Gaussian-LEEP is that it does not need to use the miscalibrated probability scores that LEEP uses.  However: (a)  Is there any evidence that the GMMs probabilities, when calculated at the output of a DNN, are indeed calibrated? (b) The paper states that the class assignment probability in Gaussian-LEEP is more reliable than the one in LEEP, because \"we fit GMM to the downstream task's training data, whereas the softmax classifier is learned from a different source task\".  This may not be a very fair comparison; perhaps an additional baseline to compare to would then be measuring with LEEP but with the classifier layers trained on the downstream task, so that both methods have a fair chance on \"calibration\" on the downstream task.\n\n4. It seems it would be fairest to compare Gaussian-LEEP to all baselines while they are using PCA as well. That way it would be clear how much of the improvement is due to the GMM component versus the dimensionality reduction component. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}