{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviews were largely split in the beginning. Some of the concerns are firmly addressed, e.g. new results evaluating the actual latency in real hardware, and one reviewer raised the score from 5 to 6. However, another reviewer is not fully convinced by the response and decided to keep the original score of  \"3: Clear rejection\".  \n\nThere are mainly two issues here. One is about the novelty of the method. The reviewer asked about the novelty and difference from CondConv/DynamicConv, however the authors emphasized the techniques to successfully train conditional computation models in general. As the focus of the paper is the proposal of the new (claimed as better) method, I have to say it is missing the points.  (The authors could have organized the storyline of the paper as \"best practices for training conditional computation models\" or something like that, if that is the true contribution the paper.) Another issue is about the advantage over the CondConv method. In the newly added results, BasisNet does not show clear advantage in terms of accuracy-speed trade-off without early exiting. Although the authors simply state that it is \"infeasible\" to do early termination on CondConv, the reason is not clear. Indeed, one can easily try layer-level early exiting as done in BranchNet for example, if not the model-level early exiting assumed for the BasisNet. \n\nBase on the discussion above, I conclude that the paper has to clarify many issues before publication and thus recommend rejection."
    },
    "Reviews": [
        {
            "title": "The overall method is easy to understand",
            "review": "This paper proposes a framework for acquiring an efficient network that essentially consists of an ensemble of networks.\n\nStrength:\n1. The overall method is easy to follow and understand.\n2. Good performance with practical appealing properties. Results outperform recent SOTA like the noisy student.\n\nWeakness:\n1. Lacking enough analysis of 'global knowledge' or 'global view'. The authors repeatedly highlight that their method has the advantage of capturing 'global knowledge' when compared to related works. However, there is only an intuitive understanding of global knowledge: the knowledge that is semantic-aware and ready for prediction in the lightweight model is global knowledge. The concept is vague, confusing, and lacks a clear definition. What knowledge can be called global or local? Is there a mathematical definition for such a concept? The authors need to carefully address this issue as the concept is a key motivation but not properly described. Besides, the authors need to add experiments to support their claim that global knowledge is indeed better than previous local knowledge in some aspects (sec3.2 Key difference).\n2. Lacking simple baseline results or ablation results. Baseline A: Directly uniform sampling from basis networks. That is, always fixing \\epsilon to a value close to 1 in Eq. (5). This baseline is to validate the effectiveness of the proposed synthesizing module. Baseline B: Following the cascade mbnet used in Fig. 6 right, the baseline is that applying joint training (Eq6) to cascade mbnet. This baseline is to show the improvements brought by the joint training framework.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "#Official Review 4",
            "review": "This paper present an efficient network, named BasisNet, which combines recent advancements in efficient neural network architectures, conditional computation, and early termination. BasisNet can be applied to any network architectures. BasisNet shows state-of-the-art ImageNet performance in mobile setting.\n\nMy main concern is about the novelty of the paper. The authors also claim that they simply combine three kind of works together and achieve a good performance with bag of training tricks (including AA and KD). The main claimed contribution is the basis model synthesis. However, it is just a simple extension of CondConv [1] and DynamicConv [2,3] to the whole model, where the dynamic inference of whole model is also not new [4,5]. The early exit technique is also widely exploited in previous works and the early exit method here is similar to BrachyNet [6]. Thus, the novelty is very limited.\n\nIn addition to the novelty, another worry is about the actual inference latency and the practicability. To execute BasisNet for each input image, we first need to run the lightweight model to generate $\\alpha$ and initial prediction. Then the weights of N basis model are loaded into the memory and combined using $\\alpha$. The synthesized weights are assigned to the BasisNet layer by layer. This process will cost large latency and memory since it must be done online and one by one. In addition, deciding whether eary exit also cost some time. The huge model size further limits practicability of the method.\n\nOverall, this paper is more like a technical report with bag of tricks but very limited novelty. Nothing new is contributed to the community. Since ICLR is a top-tier conference for presenting excellent ideas and works, I think this paper is below the bar of ICLR.\n\n[1] Yang, Brandon, et al. \"Condconv: Conditionally parameterized convolutions for efficient inference.\" Advances in Neural Information Processing Systems. 2019.\n[2] Chen, Yinpeng, et al. \"Dynamic convolution: Attention over convolution kernels.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n[3] Zhang, Yikang, et al. \"DyNet: Dynamic Convolution for Accelerating Convolutional Neural Networks.\" arXiv preprint arXiv:2004.10694 (2020).\n[4] Chen, Zhourong, et al. \"You look twice: Gaternet for dynamic filter selection in cnns.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[5] Cheng, An-Chieh, et al. \"InstaNAS: Instance-Aware Neural Architecture Search.\" AAAI. 2020.\n[6] Teerapittayanon, Surat, Bradley McDanel, and Hsiang-Tsung Kung. \"Branchynet: Fast inference via early exiting from deep neural networks.\" ICPR, 2016.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A solid paper but maybe with some practical issues",
            "review": "This work shows a hierarchical approach of classification by training a set of basis networks and a light-weighted network to combine them linearly for better accuracy given the same MAdds (or vice).  However, there are three problems the reviewer likes to clarify:\n\n(1) What's the optimal batch size at the inference, given that the network is trained at mb=128? If the mb=1, like in a common inference use case, does the basis model require the model synthesis for each image? If the extra MAdds to synthesize a model is ~16M plus the energy cost to load from the device memory, which is actually much larger than MAdd (please see below), the idea seems to be impractical in hardware. On the other end, if mb = 50k when inferencing the entire test dataset, no model synthesizing is needed, but the accuracy of a combination of the BasisNet should not do better than the backbone network as a single inference rendering no extra capacity.  Thus, the authors should add some comments on the accuracy and cost dependences on the inference batch size.\n\n(2) MAdd is a good indicator of hardware efficiency for a model but definitely not the only one from the point of view of a hardware designer. For example, moving one weight from off-chip DRAM or on-chip memory for MAadd could be 10-100 times more expensive than the MAdd (Fig.22, Sze, Vivienne, et al. \"Efficient processing of deep neural networks: A tutorial and survey.\" Proceedings of the IEEE 105.12 (2017): 2295-2329). The cost of synthesizing the model for each batch of data therefore might be expensive unless all 16 weights to be combined can be stored in the nearest RF of each PE. Could the authors show some advantages of using BasisNet on TPU or other hardware optimized for it?\n\n(3) One minor issue on claiming the early termination as an advantage of the BasisNet over the Mobilenets. This does not seem to be fair: if there's no limitation of storing weights on-chip, MB3 could also store a smaller network to inference the easier samples and improve the efficiency in the same way.\n\nOther than these three points, the paper is well written and enough experiments are done to support the authors' point that the BasisNet can provide better accuracy and MAdds than the MobileNets.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}