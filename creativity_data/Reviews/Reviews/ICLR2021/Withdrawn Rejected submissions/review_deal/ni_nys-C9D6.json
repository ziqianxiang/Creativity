{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "After reading the paper, reviews and authors’ feedback. The meta-reviewer agrees with the reviewers that the paper touches an interesting topic (reversible computing) but could be improved in the area of presentation and evaluation. Therefore this paper is rejected.\n\nThank you for submitting the paper to ICLR. \n"
    },
    "Reviews": [
        {
            "title": "Evaluation and presentation needs to be improved",
            "review": "I apologize for the late review since I did not realize I was assigned an emergency review for this paper.\n\n## Summary\n\nThe paper proposes a new DSL language embedded in Julia that can represent reversible programs. They provide mechanisms to automatically reverse a program given its forward definition in their DSL. Their language would allow reverse mode automatic differentiation to be implemented in lieu of the standard checkpointing schemes. \n\n## Strengths\n\n* A DSL to express reversible programs and implementation in a well-known language\n* Simple description\n\n## Weaknesses\n\n* Very limited evaluation on two benchmarks.\n* Presentation issues that distract from the main point of the paper.\n* Vague language description and limitations not mentioned.\n\n\n## General discussion and Questions for the authors\n\nI liked the high-level idea of this paper, however the presentation and writing need to be drastically improved in order to be accepted for publication. Also, the current results section is limited to two examples and I recommend adding more case studies to validate the usefulness of their DSL.\n\nAt a high-level the authors can do the following to improve their presentation.\n\n* Some of the language implementation details distract the readers from understanding the design aspects. For example. “One can input “←” and “→” by typing “\\leftarrow[TAB KEY]” and “\\rightarrow[TAB KEY]” respectively in a Julia editor or REPL”. The authors can omit the implementation details or move it to appendices and focus more on the design decisions they took and the reasoning for those decisions.\n* The authors can present their DSL language more formally. For example, they can give operational semantics for their forward program and reversed program using deductive rules.\n* What’s supported by the language and what’s not is not clearly mentioned. Specifically, the authors should mention limitations and scope of the language.\n\nEvaluation needs to be improved\n\n* It is not conclusive from the results presented that the DSL (or reverse mode in general) produces better code than a checkpointing strategy. It helps in case of bundle adjustment, however has an overhead in the GMM implementation.\n* Coverage - To showcase the expressivity of the DSL, the authors should implement a known NN architecture (e.g., even a small resnet) and show how training and inference speeds vary.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting contribution but not clearly presented",
            "review": "This paper draws connections between reversible programming and reverse mode automatic differentiation and introduces a reversible DSL in Julia that can be used to calculate first and second order gradients.\n\nI reviewed a previous version of this paper for NeurIPS.\n\nI really like the idea of reversible programming and I think that a clear introduction of reversible programming and its use in automatic differentiation could be of interest to the machine learning community. However, I feel that this paper fails to clearly explain the use of reversible programming and its trade-offs compared to checkpointing and other existing approaches.\n\nAs a paper, the first section is great, but then the authors leave me with many questions: How do checkpointing and reversible programming differ in memory usage? Given that the multiplier from listing 1 has 3 outputs, doesn't that mean that a program consisting of n chained multiplications still requires storing n * 2 + 1 outputs, similarly to regular AD? And doesn't binomial checkpointing allow for logarithmic memory usage in exchange for a logarithmic increase in runtime (rather than polynomial)?\n\nRather than answering these questions, the paper jumps eagerly into Julia code snippets, metaprogramming, and CUDA kernels, which I don't feel actually serve to elucidate the message that reversible programming is of interest to the machine learning community.\n\nAlthough I feel that this version of the paper is an improvement over the version I reviewed for NeurIPS, I feel that it still fails to clearly introduce reversible programming and shed light on the subtle trade-offs between reversible programming, checkpointing, and regular AD. I encourage the authors to rewrite the paper with less of a focus on the implementation details of their framework, and a stronger focus on the memory and runtime trade-offs provided by all of these methods from a more high-level, theoretical perspective.\n\nPros\n\n* Very relevant and interesting topic\n* Well-written introduction\n* Good code\n\nCons\n\n* Fails to introduce the topic appropriately for an ML audience\n* Does not clearly compare to advanced checkpointing methods\n* Not well written; too many details about the software implementation that do not contribute to an understanding of the high-level technique",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Changes after rebuttal\nThanks to the authors for their answers to the questions and their revisions to improve the manuscript. It is useful to have further descriptions of reversible computing for an audience that may be unfamiliar with the topic. I would encourage the authors to make further revisions to more concisely show the scientific value of the work while leaving some of the details to tutorials or other documents.  Other venues more focused on scientific computing, programming languages, or Julia may also be more suitable. If the language also attracts more users and applications built on top of it, then the case for publication will also be stronger (consider that the PyTorch paper was presented at NeurIPS 2019 even though the first release was in 2016).\n\n---\n# Summary\nThe paper presents an embedded domain-specific language in Julia which enables reversible computation and automatic differentiation. In order to compute gradients, we need access to all the intermediate results in a program; typically, it is necessary to store (checkpoint) these intermediate results separately or recompute them from the inputs. With reversible computing, these are unnecessary as we can compute backwards from the output to reach any intermediate result. The paper shows some empirical performance benchmarks on a bundle adjustment program.\n\n# Strengths\n- The proposed system shows strong performance compared to the presented alternatives, especially when using GPU kernels.\n- There are many examples presented which shows that the language is terse.\n- The system is practicality and usable for existing scientific computing applications.\n\n# Weaknesses\n- As a significant portion of the paper is dedicated to examples of the language in use, there is not much room to discuss other aspects of the system.\n- The novel aspects of the work are not made very clear in the paper. It is hard to compare the benefits and drawbacks of the work compared to the alternatives, other than in the runtime performance.\n- ICLR is about machine learning, but there is no evaluation of machine learning workloads. For example, I think the ICLR audience would be quite interested in how such a system might enable very deep neural networks. \n\n# Comments\n- For people not very familiar with reversible computing (like me), it is quite useful to have detailed examples and explanations about the fundamentals. However, it seemed to me that too much of the main body is spent on the fundamentals and the tutorial aspects, and more of it could be moved into an appendix. \n- It would be better to have more comparisons and discussions of which aspects of the system are novel. There is a discussion of some related work on page 2, but it is not very systematic.\n- As the paper states at the end of page 2, it is not possible to rigorously reverse operations on floating point numbers. However, there is no further discussion of the implications of this. It would be good to have some further reassurance that the errors are not important, or to have a discussion of what applications are acceptable and sufficiently tolerant of the errors.\n- Given the lack of checkpointing required for automatic differentiation, it would seem to me that the language can enable significantly lower memory usage than non-reversible languages. So I was surprised that there are no benchmarks discussing memory usage, or showing that the method can operate with larger datasets/parameters on a fixed memory budget.\n- Anonymous submissions should not include acknowledgements and should not include links to non-anonymous GitHub repositories.\n- Please use \\citet and \\citep with natbib so that citations are formatted properly in the text. Citations at the end of sentences, or otherwise not used as a noun in the sentence, should look like (Author, 2020) rather than Author (2020).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents a Julia based DSL to do automatic differentiation using reversible computing",
            "review": "The paper adapts reversible computing techniques to compute gradients. The techniques presented are not new though the Julia based DSL is new. The results presented are for differentiating through a GMM. It is not clear if the technique scale to a modern day neural network models and how they will integrate into current frameworks like JAX, PyTorch or TensorFlow.\n\nThe paper may be more relevant to a Programming Language focused venue or maybe even JuliaCon",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A promising manuscript on a new approach that combines reversible programming and automatic differentiation; Call for improvements",
            "review": "General comments:\n- This paper presents a new approach to automatic differentiation (AD), namely the use of reversible programming to achieve memory-efficient function inverse and adjoint. The authors have done a good job reviewing the background and laying out the motivation for the new apporach. The implementation is based on adding an embedded DSL to Julia called NiLang. Through reversible programming, NiLang gets rid of the need for checkpointing and hence is amenable to CUDA execution. NiLang is benchmarked against native Julia, ForwardDiff and Tapenade. The performance of NiLang is slightly worse than other approachs in the GMM benchmark. But in the bundle adjustment benchmark, NiLang outperforms ForwardDiff and Tapenade, especially with CUDA acceleration.\t\n \nDetailed comments:\n- Section 2.3\n  - I feel the explanation of the pre and post conditions can be made clearer. While most readers are familiar with the idea of pre condition in if and while control flows, some may not be familiar with what post condition is. The example in Listing 4 isn't presented in a way that's most clear and helpful to resolve this confusion either. Please explain post condition more clearly, perhaps be adding annotations to Listing 4.\n  - Also, in Figure 1(a), what does \"pre == post\" mean? On a related note, in Listing 4, if the post condition of the if statement is a placeholder \"~\", how does it work when it's treated as the pre condition during reversal? Explain that it's simply treated as an always-true condition.\n  - Please add a sentences to explain how NiLang handles the errors (as shown in red in Figure 1) that occur during the control flow and their reversals.\n- Section 3.1\n  - Please explain what adjoint is briefly before mentioning it first time, possibly by citing the Tapenade paper (Hascoet & Pascual 2013).\n- Section 4.1\n  - Since the authors emphasize the memory usage advantage of NiLang, why not run memory benchmarking, quantify the results and show them in tables or figures here?\n  - Table 1: Why are NiLang GPU results not included in this table?\n  - Tables 1 and 2: The timing numbers in these tables to not span many orders of manitude. So using regular decimal points (e.g., 0.009844, 0.0351) might be more visually clear and easier to parse than the engineering notation currently used. Also, consider making plots instead of tables for these numbers, because plots will be much more intuitive and facilitate comparisons between the different implementations. The plots can be in logarithmic scale.\n- This manuscript currently lacks a discussion section. Given AD is widely used in machine learning and neural networks (especially within the context of this conference), many readers will be interested in whether NiLang is suitable for training neural networks.\n\nMore detailed comments / writing suggestions:\n- p 2. \"Besides the above “nice” features, it also has some “bad” features to meet the practical needs\" The meaning of the second part of this sentence is unclear. Please rewrite it.\n- p 8, Section 4.1. Do not repeatedly define the ancronym \"BA\". It's already defined above in the same page. Also, since you don't really use the acronym anyway, there's no need to define it.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}