{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper, a network architecture search (NAS) problem in a changing environment is studied and an online adaptation (OA) algorithm for the problem is proposed. Many reviewers found that the OA-NAS problem discussed in this paper is interesting and practically important. However, many reviewers (including those with high review scores) recognize that the weakness of this paper is the lack of sufficient theoretical verification. Furthermore, although extensive experiments are conducted, it is still not clear whether the experimental setups discussed in the paper are generally applicable to other practical problems. Overall, although this is a nice work in that a new practical problem is considered and a workable algorithm for the problem is demonstrated in an extensive simulation study, I could not recommend the acceptance in its current form because of the lack of theoretical validity and evidence of general applicability."
    },
    "Reviews": [
        {
            "title": "A practical fine-tuning method but lacks interesting novelty.",
            "review": "This works aims at task-oriented fine-tuning from pre-trained ImageNet models. It proposes a Neural Architecture Search and Online Adaption framework (NASOA) to perform fast task-oriented model fine-tuning. The NASOA first employ an offline NAS to select a group of models and then pick up the most suitable model from this group via an online schedule generator. \n\nWeaknesses:\nThe proposed method may be useful in some fine-tuning scenarios, however will have low overall impact. From the reviewer's view, lack of novel and interesting part. The proposed approach simply uses and combines existing methods, e.g., NAS, seems more like an engineering project.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4 - post rebuttal update",
            "review": "SUMMARY:\n\nThe paper presents a joint Neural Architecture Search (NAS) and Online Adaption (OA) framework named NASOA, aiming at providing faster task-oriented ﬁne-tuning system. The first step of the approach is an offline NAS to form a pretrained training-efficient model zoo, which is followed by an online scheduler which selects the most suitable model and generates a personalized training regime with respect to the target task.\n***************************************************************************\nSTRENGTHS\n- The paper is fairly well written and easy to follow. \n- The proposed approach is fairly novel: up to my knowledge, this is the first effort to combine NAS and Online Adaptation techniques for faster fine-tuning task. In addition, the proposed joint/block macro level search space is novel too.\n- The practical benefits of the proposed approach are obvious for AutoML systems, and the shared ET-NAS model zoo will help other researchers working on this topic.\n- The paper provides (in appendix B) a short but interesting analysis on CO2 consumption. This is a good practice which is rare enough to be worthy of note.\n***************************************************************************\nWEAKNESSES AND REMARKS\n- I have some concerns regarding the fact that the paper does not compare the proposed approach to a baseline offline setting. The idea would be to collect offline training data by finetuning on several datasets (of different nature) and collect the corresponding accuracies to train a predictor (e.g. an MLP). The paper mentions this possibility, but says that it is less realistic than the online setting. Which is true, but it would have been interesting to compare the two settings\n- I’m not sure to understand the relevance of the preliminary experiments presented in sub-section 4.1. The findings (i.e. 1. fine-tuning performs better than training from scratch, and 2. learning rate and frozen stage are crucial for ﬁne-tuning) seem obvious to me, and correspond to widely-adopted common practices.\n- It’s not clear to me how the models are organized into groups in Table 2. Are they grouped based on their respective complexity? If so, it should explicitly mentioned by adding a column with the number of parameters of each model. \n- When presenting the ImageNet results in Table 2, the paper does not mention that current sota on ImageNet is around 88% top-1 accuracy, i.e. about 6 points above the best result presented in the Table. The paper should add this information to slightly tone down the claim that ‘searched models outperform other SOTA ImageNet models in terms of training accuracy’.\n- It would have been interesting to give more details about the NSGA-II algorithm in the paper itself, rather than only referring to the appendix. I understand that it’s due to the lack of space, but it is a little bit frustrating to have no information in the paper about this point, especially about the modifications compared to the original algorithm.\n- Figures 3 and 4 are quite useless in their current form, since the text size is too small to be read. \n- Table 4 is hardly interpretable without reading its corresponding descriptive paragraph in the paper. Authors should give more information (e.g. in the caption) about each method.\n- Page 5, Line 5: MLP stands for Multi Layer Perceptron (not Perception).\n***************************************************************************\nJUSTIFICATION OF RATING\n\nDespite the concerns mentioned above, I think the proposed approach is an interesting addition to the task-oriented online fine-tuning literature, especially considering its practical benefits for AutoML systems. I also think the paper could be considerably improved by taking into account the comments made above. Overall, I’m leaning to reject (5: Marginally below acceptance threshold), but I would be considering to increase my rating if some modifications are made during rebuttal.\n\n***************************************************************************\nPOST REBUTTAL UPDATE\n\nThe authors provided a detailed rebuttal which addressed many of my concerns, and clarified some points. I will update my rating from 5 (marginally below acceptance threshold) to 6 (marginally above acceptance threshold).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Initial review from R3",
            "review": "This paper is the first to address the problem of searching better backbone architectures for downstream tasks and online hyper-parameter selection.  The whole system proposed in this algorithms offers an end-to-end solution for producing a well-trained architecture for a specific downstream task within a fixed training budget. I believe this system will have a large impact in industry model deployment. \nThis paper separate the overall searching process into (1) generating efficient training model zoo (pretrained on a source dataset) (2) task-oriented fine-tuning schedule (Mode selection, hyper-param selection for downstream task fine-tuning).  The search space of the efficient model zoo is decoupled to two stages: block-level search space and macro-level search space. This design could enrich the model zoo with diverse models architectures.   The major part of task-oriented fine-tuning schedule is a performance predictor which is fed with target dataset embedding, model identity and training hyper-params and output an assessment of the final performance. \n\nThrough extensive evaluation experiments, the effectiveness is of this system is demonstrated.  Through well defined ablation studies, the author offers our insightful conclusion of why their systems works: \n1. A better and more diverse model zoo, where  smaller models have simper block structures and larger models have more complicated block structure.\n2. The well-trained performance predictor in task-oriented fine-tuning  scheduler can successfully capture some sort of correlation between bunch of hyper-param and final performance. \n\nSome of my concerns: \n1. The searched architectures are not  directly optimized for down-stream task performance.  \n2. This system cannot address the problem with consistently changing down-stream task, and the cost of training the scheduler for a new downstream task is a little bit huge because of the large pretraining model zoo. \n\nOverall, I recommend this paper to be accepted due to its engineering efforts, several interesting conclusion drawn from empirical study and great impact in industries. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a joint Neural Architecture Search and Online Adaption (NASOA) framework to achieve a faster task-oriented fine-tuning upon the request of users. My biggest question is why the fine-tuning should be based on a specific network.",
            "review": "In this paper, a joint Neural Architecture Search and Online Adaption (NASOA) framework is proposed to achieve a faster task-oriented fine-tuning upon the request of users. In particular, two main contributions are made in this paper: (1) A fine-tuning pipeline that seamlessly combines the training-efficient NAS and online adaption algorithm is introduced, which can effectively generate a personalized fine-tuning schedule of each desired task via an adaptive model for accumulating experience from the past tasks. (2) A block-level and macro-level search space is introduced in the resulting framework, which enables a simple to complex block design and fine adjustment of the computation allocation on each stage.\n\nI have two  comments about this paper， which  go as follows:\n\n(1)\tIn this work, a joint NSAOA framework is introduced facilitate a fast continuous cross-task model adaption. Why not learn an effective fine-tuning regime based on a hand-crafted network? In my point of view, a good fine-tuning regime should be also independent of the choice of network.\n\n(2)\tThe block-level and macro-level search space can reduce the redundant skip-connections in the resulting structures. To the best of my knowledge, the skip-connection is effective to avoid the gradient vanishing problem in a very deep neural network, therefore the skip-connections at the lower layers are very important in the network design. However, the skip-connections at the lower layers will take more memory cost at both training and testing stages. From this point of view, authors should explain where the redundant skip-connections always occur in the rebuttal.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}