{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper works on semi-supervised domain adaptation (SSDA). It derives a finite-sample generalization bound for SSDA, for both classification and regression problems. This generalization bound then inspires the proposed LIRR algorithm to learn invariant features and risks. The authors conducted experiments for SSDA on multiple datasets. While the results and the algorithms look promising, there are many details unclear in the current manuscript and require further clarifications.",
            "review": "# Summary:\nThis paper works on semi-supervised domain adaptation (SSDA). It derives a finite-sample generalization bound for SSDA, for both classification and regression problems. This generalization bound then inspires the proposed LIRR algorithm to learn invariant features and risks. The authors conducted experiments for SSDA on multiple datasets. While the results and the algorithms look promising, there are many details unclear in the current manuscript and require further clarifications. \n\n# Strengths:\nThe paper derives a finite-sample generalization bound for SSDA, for both classification and regression problems. This generalization bound then inspires the proposed LIRR algorithm to learn invariant features and risks. The authors conducted experiments for SSDA on multiple datasets, for both the classification and regression problems.\n\n# Weaknesses:\n1. Ben-David’s seminal work and Zhao et al.’s work have already shown that (1) the difference in marginal distribution across domains and (2) how well a single classifier can work in both domains affects the performance of domain adaptation. With a slight amount of labeled target data, it seems straightforward to add (2) into the learning objective. Even in unsupervised domain adaptation (UDA), several existing works, such as CDAN, match the marginal and conditional distributions together. The authors should provide more discussions on this line of work.\n\n\n2. The writing of sections 2, 3, and the use of notation can be improved.\n- Theorem 3.1 and 3.2 do not differentiate the labeled (m) and unlabeled target (k) data. The “T” there should be \\tilde{T}, as defined on page 3. The H-divergence terms remain the population bound.\n\n- The use of notation can be improved. Specifically, the authors use S to represent the labeled source data and source domain, simultaneously. It is thus hard to tell if E_S is the expectation on the source data or on the source domain distribution. It is also hard to tell if f_S, f_T are the optimal classifiers on the source and target domain distributions or source and target domain data.\n\n\n3. Several details of the proposed approach are not clear.\n- Eq. (6) seems to not involve the terms that minimize the empirical errors on the source and target labeled data, as stated in Theorem 3.1 and Theorem 3.2. There are no definitions of f_i and f_d in (5) and (6). It is unclear what is the final classifier being trained. Note that, if Eq. (5) is to approximate I(Y;D|Z) whose min value is 0, then a trivial solution for (5) is a constant function f_i and f_d (i.e., always predict 1 or 0).\n\n- It is still unclear why the fourth term in Theorem 3.1 and Theorem 3.2 can be replaced by I(Y ;D| Z). Note that, Theorem 3.1 and Theorem 3.2 involve the optimal classifiers which are unknown.\n\n\n4. Experiments:\n- While the authors experimented on multiple datasets, for each dataset they only evaluate a few pairs of domains. For example, in MME (Saito, 2019), more than 7 pairs on DomainNet and Office-Home are evaluated, and here the authors only show 3.\n\n- The authors didn’t compare to some recent methods in SSDA, which have outperformed MME and maybe the proposed methods.\n\nC. Qin et al., Opposite structure learning for semi-supervised domain adaptation, arxiv, 2020\n\nL. Yang et al., MiCo: Mixup Co-Training for Semi-Supervised Domain Adaptation, arxiv 2020\n\nT. Kim et al., Attract, Perturb, and Explore: Learning a Feature Alignment Network for Semi-supervised Domain Adaptation, ECCV 2020.\n\nWhile some of them are recent arXiv papers, a discussion and a comparison to them would strengthen the paper.\n\n- It is unclear how the authors extend the baseline methods initially proposed for UDA to SSDA. The reported accuracies of MME are much worse than those in (Saito, 2019).\n\n- As mentioned in Theorem 3.1 and 3.2, they are four terms that can be optimized. I would like to see an ablation study on the effects of these four terms in the proposed algorithm.\n\n- The implementation details in the main paper is insufficient. The authors only describe the network architecture in the appendix.\n\n\n5. Others:\n- The related work on existing SSDA algorithms is insufficient. See above.\n\n- It seems that both Ben-David et al. and Zhao et al. have both provided a finite sample bound. Could the authors provide more discussions on this? Also, a recent work [A] has also provided abound with labeled target information. Could the authors discuss this?\n\n[A] J. Wu and J. He, Continuous Transfer Learning with Label-informed Distribution Alignment, arXiv 2020.\n\n\n# Minor comments:\n- I would recommend that the authors provide more intuitions on Eq. (5). Specifically, what are the two loss terms trying to learn?\n\n- I would suggest that the author uses SSDA instead of semi-DA for abbreviation.\n\n- There seems no definition on the function f on page 2. I think it is the labeling function.\n\n- In deriving the bound, the authors seem to assume that the feature function \"g\" is fixed. However, it will be learned in the proposed algorithm. Will this affect the bound?\n\n# Justification:\nWhile the paper presents a new finite-sample bound for SSDA and presents a new algorithm for SSDA, similar concepts of the combination of feature and risk invariance has been proposed in UDA. Besides, some details of the proposed algorithms are not clear, especially that the empirical risks are not added to the learning objective. The experiments are not comprehensive --- only partial results are reported for benchmark datasets like DomainNet and office-home. There are also no ablation studies on the use of different loss terms. I thus give a score of 4.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The theoretical contribution is incremental",
            "review": "###############\n\nSummary:\nThis paper proposes a theoretical analysis of semi-supervised domain adaptation, based on the theoretical results, a bound minimization algorithm, LIRR, to jointly learn invariant representations and invariant optimal predictors. Empirical results justify the effectiveness of the proposed algorithm. \n\n###############\n\nPros:\n1.\tThis paper provides generalization bounds for semi-DA.\n2.\tThe paper is overall well-written and easy to understand.\n3.\tLearning invariant risks (i.e., Eq. 5) is also interesting to me. From my perspective, this is the most significant (algorithmic) contribution of this work, though similar work has been investigated previously [Song et al. 2019, Steinberg et al. 2020].\n\nCons:\n1.\tMy main concern is that the theoretical contribution of this paper is incremental. The authors claim that this is the first generalization analysis in the Semi-DA setting. In fact, some semi-supervised theoretical results have been analyzed previously (e.g. Theorem 3 of [Ben-david 2010], Theorem 3 of [Redko et al. 2017], and Theorem 5.7 of [Redko et al. 2019]). In addition, the theoretical analysis in this paper follows a standard set of steps that has been repeatedly adopted in several previous works (e.g., [Zhao et al. 2019a, Zhao et al. 2019b, Combes et al. 2020]), and didn’t provide any new insights. \n2.\tThe empirical evaluation protocol also seems problematic. Except for MME, the authors compared with lots of unsupervised oriented baselines where the target labels are totally inaccessible. Those methods were designed towards UDA rather than semi-DA, so they may not have good performance even though they were trained under a semi-DA mode. For a fair comparison, I would like to encourage the authors to compare some other approaches which are also trained under a semi-supervised mode, e.g. [Su et al. 2020] proposed the learner to actively select some target instances and retrain the model in a semi-supervised way. More comparison with semi-supervised approaches is needed.\n3.\tIn this manuscript, the authors proposed to randomly select some portion of the target domain instances. However, previous work (e.g. [Su et al. 2020]) showed that random selection might not be a good choice. I’m curious that when the learner only has access to only a few instances in the target domain, will the model perform stable enough to the random choice? For example, in Table 1, in the art -> real-world task experiment on the office home dataset. There are ~4350 images in the target domain (real-world), if you only choose 1% of the target domain, which means you only have 44 images from the target domain. However, the number of classes in the office-home dataset is 65. This means this random selection could not cover all the target labels. I’m curious whether this imbalanced choice will have some negative impacts or not?\n4.\tThis submission mentioned label distribution shifts several times, but I didn’t see any connection between label distribution shifts problems with this submission. Neither theoretical justifications nor empirical evaluations were conducted, which makes me feel confused by this submission.\n5.\tThe authors claim that this paper provides finite-sample generalization bounds. However, the third and fourth terms in Thms 3.1 and 3.2 still rely on the expectation (and labeling functions) on the source and target domains.    \n6.     Additional questions regarding (5): it seems that f_d in (5) is not defined, so it is not quite clear to me how to derive the second term Proposition 4.1. Additionally, in Proposition 4.1, the expectation is taken over Z, but in (5), it is taken over X, which is another issue of the derivation (but I think it could be fixed by data processing inequality).  The last issue is how to compute the expectation in (5). I believe that in practice the expectation is replaced by the mixture of the empirical source and target distributions. Since the size of the target data is very small, the prior probability of target data will also be very small. Do you use some over-sampling approach to re-balance the data?    \n\n################\n\nOverall:\n\nAlthough I appreciate the idea of Learning Invariant Risks, I don’t think this paper does not meet the standard for an ICLR publication. I found very few novel theoretical insights from this paper. I encourage the authors to rethink the theoretical contributions and empirical justifications for a better one.\n\n###############\n\nReferences:\n\n[Ben-David et al. 2010] Ben-David S, Blitzer J, Crammer K, Kulesza A, Pereira F, Vaughan JW. A theory of learning from different domains. Machine learning. 2010 May 1;79(1-2):151-75.\n\n[Redko et al. 2017] Redko I, Habrard A, Sebban M. Theoretical analysis of domain adaptation with optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases 2017 Sep 18 (pp. 737-753). Springer, Cham.\n\n[Su et al. 2020] Su JC, Tsai YH, Sohn K, Liu B, Maji S, Chandraker M. Active adversarial domain adaptation. In The IEEE Winter Conference on Applications of Computer Vision 2020 (pp. 739-748).\n\n[Zhao et al. 2019a] Zhao H, Combes RT, Zhang K, Gordon G. On learning invariant representation for domain adaptation. arXiv preprint arXiv:1901.09453. 2019 Jan 27.\n\n[Zhao et al. 2019b] Zhao H, Coston A, Adel T, Gordon G. Conditional Learning of Fair Representations. In Proceedings of the 8th International Conference on Learning Representations, 2020.\n\n[Steinberg et al. 2020] Steinberg, D., Reid, A., O'Callaghan, S., Lattimore, F., McCalman, L. and Caetano, T., 2020. Fast Fair Regression via Efficient Approximations of Mutual Information. arXiv preprint arXiv:2002.06200.\n\n[Redko et al. 2019] Redko, I., Morvant, E., Habrard, A., Sebban, M., & Bennani, Y. (2019). Advances in Domain Adaptation Theory. Elsevier.\n\n[Steinberg et al. 2020] Steinberg, D., Reid, A., O'Callaghan, S., Lattimore, F., McCalman, L. and Caetano, T., 2020. Fast Fair Regression via Efficient Approximations of Mutual Information. arXiv preprint arXiv:2002.06200.\n\n[Songet al. 2019] Song, J., Kalluri, P., Grover, A., Zhao, S. and Ermon, S., 2019, April. Learning controllable fair representations. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 2164-2173).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "There seems a gap between the proposed bound and the objective actually used. The contribution of the invariant risk part is not convincing enough. ",
            "review": "--------------------------\nSummary:\nThis paper proposes a generalization upper bounds for both classification and regression problems under Semi-DA. This paper introduces a new method to learn invariant representations and risks under Semi-DA settings simultaneously. Extensive experiments demonstrate the effectiveness of the proposed method. \n\n--------------------------\nPros:\n1. The author proposes a generalization upper bound for Semi-DA works both for the classification and regression problems.\n2. The invariant risk is introduced to help to align the conditional distributions.\n3. Extensive experiments are conducted on both classification and regression tasks to demonstrate the proposed method's effectiveness.\n\n--------------------------\nCons:\n1. The bound is a naive extension from Ben-David's.\n2. There is a gap between the proposed bound and the objective actually used, which should be clarified since the weighted sum of the empirical source and target error disappear from the final objective.\n3. The main contribution, a.k.a, the invariant risk part, is not convincing enough. I can see from Eq.5 that the objective tries to find a consistently estimated classifier f_d for the worst feature space induced by feature extractor g through adversarial learning. However, it does not guarantee that optimal labeling functions are close to each other. To make it more convincing, the reviewer's suggestion is to show the feature space like MME at least and check whether the conditional distributions are better aligned. Or, simply learn the estimated classifiers \\hat{f}_s, \\hat{f}_t from the feature space obtained by the proposed method and compare their distance with other methods.\n4. There should be more explanation about f_i and f_d, and the reviewer cannot find which one is actually used for the final prediction. Besides, an illustration of different f might be helpful.\n5. Eq.5 is a little confusing. To my understanding, f_d can represent both f_s and f_t. If that's the case, it would be better to write it separately because there are actually two hypotheses trying to maximize the objective.\n6. The paper mentions the problem brought by the imbalance label distributions, but I don't see any effort which is made to tackle the problem like PADA.\n\n--------------------------\nReasons for Score:\nCurrently, the reviewer rates 5/10 due to my second and third concerns. If more convincing evidence is provided, the reviewer may raise the score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a new semi-supervised domain adaptation method considering both feature and classifier adaptations and evaluates it against other methods on several datasets, with promising but not entirely convincing results.",
            "review": "This paper proposes to learn invariant representations and risks (LIRR) for the semi-supervised domain adaptation problem, with theoretical analysis to back up the idea. Evaluation on experiments on multiple datasets shows the superiority of LIRR, with some insights provided. It is an interesting paper but I do have some concerns below:\n\n1. Section 1 (par 2): The introduction reads like semi-supervised DA is a new(er) problem by focusing on recent literature. However, there is a rich literature on semi-supervised DA that have been ignored (many papers more than 10 years old) though I do see that unsupervised DA is popular today. As a recent post in twitter said, it is not good for science if we think literatures in the past 3-5 years are all/most we have. The so-called invariance representations and invariant risks appear in names such as feature adaptation and classifier adaptation in transfer learning/domain adaptation literature. Therefore, the main idea here is not that new actually. \n\n2. Will invariant representations and invariant risks be related (rather than independent)? The ablation study in Sec. 5.4 did not actually provide an ablation study directly on LIRR. It simply compares with other algorithms with different designs. A more proper ablation study should vary the LIRR model to study the differences, i.e. setting \\lambda_{risk} = 0 and then \\lambda_{rep} = 0, unless this will degenerate to other models compared. \n\n3. Notations are not always clear/consistent. In Eq. (4), should I(Y; D|Z) be I(D; Y|Z) instead? Also, ‘m’ denotes samples in 'T’ in Theorems 3.1 and 3.2. However, in Section 2 (page 3), 'm' denotes the number of labelled target samples “\\tilde{T}” while k denotes the number of unlabelled target samples “T”. This is confusing and clarification is needed. If the definition of “m” in Sections 2 and 3 are consistent, then it seems that the generalisation bounds will not be affected by the number of unlabelled target samples “k”, which could be counterintuitive. \n\n4. The last paragraph of Section 3 says \"Comparing with previous results (Ben-David et al., 2010; Zhao et al., 2019a), our bounds here contain empirical error terms from both the source and target domains.\" However, Theorem 3 in (Ben-David et al., 2010) also gives empirical risk minimization using combined source and target training data. It will be better to clarify further what are the specific differences here (Theorem 3.1 in this paper and Theorem 3 in Ben-David et al., 2010).\n\n5. In Section 5.1, it says \"For each dataset, we randomly pick source-target pairs for the evaluation\". Why random? Have you repeated such random picking multiple times and will the results or conclusions vary? Why not conduct standard evaluation as in the literature? Later it says \"we randomly select a small ratio (1% or 5%) of the target data\". How many times has this been repeated?\n\n6. I cannot find statistics on the datasets (even in the Appendix), e.g. for the NICO dataset, how many images are available and how many have been used in this paper?\n\n7. There is a larger gap between LIRR and LIRR+CosC on the NICO dataset compared to results on other datasets. Moreover, LIRR has lower accuracy than MME on NICO consistently. Could you provide an explanation?\n\n8. To help digest the presented tables, it will be better to provide some summary such as the average performance.\nComparing visualisation of representation learnt by different algorithms?\n\n9. Table 2 says \"Citycam\" in bold, but this name is not used elsewhere. \n\n10. Figure 2 is impressive, but it will be better to see the average results over all pairs to draw a conclusion. \n\n11. Computational complexity is not analysed. \n\n12. The mass citations in Section 6 reduce readability. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}