{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "**Problem Significance**  This paper introduces an interesting taxonomy of OODs and proposed an integrated approach to detect different types of OODs. The AC agrees on the importance of a fine-grained characterization of outliers given the large OOD uncertainty space. \n\n**Technical contribution** The key idea of the paper is to combine the predictions from multiple existing OOD detection methods. While the AC recognizes the effort made by the authors to address the review comments, reviewers have several major standing concerns regarding limited contributions, insufficient analysis, and lack of clarity. The AC agrees with reviewers that the paper is not ready yet for ICLR publication, and can be further strengthened by:\n\n- (R1) reporting the computational cost for the integrated approach. The inference time for approaches such as Mahalanobis is typically a few times more expensive than the MSP baseline. The cumulative time for calculating all four scores may be non-negligible. Authors are encouraged to analyze the performance tradeoff in a future revision. \n- (R2 & R3) discussing the effect of hyper-parameters tuning, which be overly complicated in practice as it involves combinations of multiple methods that each have multiple parameters to tune. \n- (R3) comparing with more recent development on OOD detection and move the new results to the main paper. The AC also thinks it's worth discussing the connection and comparison to methods on quantifying uncertainty via Bayesian probabilistic approaches.\n- (R2 & R4) more rigorous analysis of the benefits of the proposed integrated approach, both empirically and theoretically. Based on Table 7, the performance of using Mahalanobis alone is almost competitive as the proposed approach (except for the CIFAR10-CIFAR100 pair). This may deem further careful examination to understand what value other components are adding, and in what circumstance. \n- (R2, R3 & R4) More discussion on the implication of the taxonomy and algorithm in the high-dimensional space would be valuable. The 2D toy dataset might be too simple to reflect the decision boundary as well as uncertainty space learned by NNs. Moreover, it's important to justify further how aleatoric and epistemic uncertainty manifests in the current experiments using NNs. For example, epistemic uncertainty can arise due to the use of limited samples or due to the model uncertainty associated with the model regularization. \n\nRecent work by Hsu et al. [2] also attempt to define a taxonomy of OOD inputs (based on semantic shift and domain shift), which can be relevant for the authors. \n\n**Recommendation** Three knowledgeable reviewers have indicated rejection. The AC discounted R4's rating due to the less familiarity in this area and lack of participation in the post-rebuttal discussion. \n\n[1] Richard Harang, Ethan M. Rudd. Towards Principled Uncertainty Estimation for Deep Neural Networks\n[2] Hsu et al. Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data\n"
    },
    "Reviews": [
        {
            "title": "A combination of methods for OOD detection",
            "review": "##########################################################################\n\nSummary:\nThe authors explore the different kinds of outliers and show that the methods previously proposed detect different kinds of OOD and not a single one can detect them all. The authors propose an interesting study of the different kind of outlier on synthetic data which  illustrates well the different characteristics of the outlier types. The authors then propose to combine different methods to increase the OOD detection rate. Experiments are conducted on 3 images classification datasets using different deep neural networks. For each dataset, samples from other databases are introduced as outliers and must be detected. The combination method yield better detection rates than baseline methods in almost all configurations. \n\n\n##########################################################################\n\n Reasons for score: \n\nThe main idea of the paper is simple : combine different OOD detection metrics to increase the detection rate on different types of outliers. The proposed method indeed increases the OOD detection rate for almost all the experimental settings tested by the authors. However, the method to create the OOD samples is always the same: in-distribution samples come from a database whereas out of distribution samples are drawn from another database. It would be interesting to show that the method also increases the detection rate of outliers inside a given database. This could be done by reporting the classification rate of the DNN in an abstaining scheme : if the OOD metric is greater than a threshold, the sample is not classified (rejected). If the OOD detection method is useful, the classification rate of the DNN can be freely increased by increasing the threshold and rejecting more and more samples. \n\nThe author do not justify their choice of the combination method. Computing all the OOD metrics can be computationaly expensive, is it necessary to compute them all ? Are this combination of metric the best ? In which conditions ?\n\nThe combination method should be described in the body of the paper, not in appendix.\n\nGuo 2017 appears twice in the bibliography.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting taxonomy of OOD samples but the current paper needs improvement",
            "review": "-- Paper Summary: \nThe paper presents the idea of fusion of attributes from existing sota ood detection methods to achieve higher detection performance.\n\n-- Review :\n\n- The three criteria presented in section two are questions rather than criteria. It is better to be re-worded into criteria. \n\n- Figure 1 suggests the \"tied distribution of all training data\" is different than the combination of \"class distributions\". I wish authors  could explain the difference between Type 4 and 5 in the ood sample taxonomy.\n\n- The relation between five types of OOD with three criteria for OOD categorization is not clear. \n\n- The visualization in all figures could be improved:\n    - figure 1: too many colors. better to use different shape or numbers directly in the figure.\n    - figure 5: not necessary to include, hard to see and comprehend.\n    - the total number of figures can be reduced by eliminating some and combining others. \n\n- What was the reason to choose a subset of cifar100 as ood test set but not the whole dataset? \n\n- Authors emphasize reporting detection TNR in the manuscript while FNR is missing from the measurements. I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve (AUPR) or area under receiver operating curve (AUROC) for reporting as in the Table. \n\n- I can't find an explanation and/or discussion on the final detection score and it's hyperparametere. \n\n- The results from the Mahanalobis technique [7] does not match the original paper. If authors did not use a subset of ood samples for tuning, it should be reported in the paper.\n\n-- Strengths: \n- interesting taxonomy of ood samples and the following conclusion for integrated detection score. \n\n\n-- Weaknesses: \n- limited on contribution\n- no discussion on final detection score and its hyperparameters.\n- comparison with more recent techniques including Outlier Exposure [1], Self-supervised reject classifier [2], Geometric self-superivised learning [3,4], and contrastive learning [5,6] are missing in this paper. \n\n\n[1] Hendrycks, D., Mazeika, M., & Dietterich, T. (2018, September). Deep Anomaly Detection with Outlier Exposure. ICLR 2019\n\n[2] Mohseni, Sina, et al. \"Self-Supervised Learning for Generalizable Out-of-Distribution Detection.\" AAAI. 2020. \n\n[3] Hendrycks, D., Mazeika, M., Kadavath, S., & Song, D. (2019). Using self-supervised learning can improve model robustness and uncertainty. In Advances in Neural Information Processing Systems (pp. 15663-15674). \n\n[4] Golan, Izhak, and Ran El-Yaniv. \"Deep anomaly detection using geometric transformations.\" Advances in Neural Information Processing Systems. 2018. \n\n[5] Tack, J., Mo, S., Jeong, J., & Shin, J. (2020). Csi: Novelty detection via contrastive learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176. \n\n[6] Winkens, J., Bunel, R., Roy, A. G., Stanforth, R., Natarajan, V., Ledsam, J. R., ... & Cemgil, T. (2020). Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566. [8] Liu, Hao, and Pieter Abbeel. \"Hybrid discriminative-generative training via contrastive learning.\" arXiv preprint arXiv:2007.09070 (2020).\n\n[7] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (pp. 7167-7177).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting Work on the Detection of OODs",
            "review": "##########################################################################\n\nSummary:\n\nThis paper introduces a novel taxonomy for OOD outliers. The authors analyze current OOD detection approaches and uncover their limitations. They propose to fuse several existing approaches into a combined one and extensively evaluate it on various data sets (CIFAR,10, SVNH, MNIST, STL10, ImageNet, etc.). The proposed integrated OOD detection approach clearly shows superior performance.\n\n##########################################################################\n\nReasons: \n\nOverall, I vote for accepting. The authors make several key contributions: The introduce a novel OOD taxonomy, analyse current OOD detection approaches on a toy data set, propose an integrated OOD detection approach, which shows a superior performance in their extensive evaluation.\n\n##########################################################################\n\nPros:\n\n* Introduction of a sound and helpful OOD taxonomy\n* Limitation analysis of state-of-the-art OOD detection algorithms\n* Proposal of a new integrated approach to detect different kind of OOD inputs that unifies the advanatges of underlying algorithms.\n* Extensive evaluation of new approach shows clearly superior performance. On a variety of data sets (CIFAR,10, SVNH, MNIST, STL10, ImageNet, etc.) the proposed approach outperforms the baselines on all evaluation criteria (TNR, AUROC, DTACC, AUPR IN, AUPR OUT) for various classifier neural network architectures (LeNet, ResNet, DenseNet).\n\n##########################################################################\n\nCons:\n\n* The demonstration of the limitations of current OOD detection algorithms is solely empirical (based on a toy data set). Theoretic motivations (if possible) would be a great addition.\n* Similarly, a sound theoretical derivation for the proposed integrated approach is lacking. \n* Further toy data sets beyond the two half moon data set would be helpful to better understand the implications of all algorithms.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good paper but need improvement",
            "review": "This paper introduces a taxonomy of OODs and proposed an integrated approach to detect different types of OODs. Their taxonomy classifies OOD on the nature of their uncertainty and they show that no single state-of-the-art approach detects all these OOD types. Motivated by this observation, they combine multiple existing OOD detection methods to detect various types of OODs. \n\nIn general, this paper is easy to understand. But I have the following concerns: \n\n1. Lack of discussions about some important related work. They only compare their method to ODIN and Mahalanobis methods. But there are some other OOD detection methods which also achieve state-of-the-art results, such as [1][2][3]. Could the authors compare their method to these methods? \n\n2. In their taxonomy, they consider examples that are very close to in-distribution as OOD. I am wondering whether we should treat those examples as OOD since they are too close to the in-distribution. I think previous works like ODIN and Mahalanobis all assume that OOD inputs are far away from the in-distribution. In the experimental setup, they consider STL10 as an OOD dataset for CIFAR10. But STL10 contains CIFAR10 alike images. It is unconvincing that we should treat those images as OOD. And I think the classifier trained on CIFAR10 may have correct predictions on some of those images. Could the authors explain why we should treat those images as OOD? \n\n3. I am wondering whether the analysis for the simple two-dimensional dataset could be applied to high-dimensional datasets. In the high-dimensional space, their conclusion about which method detects which type of OOD may not hold. Could the authors explain it? \n\n4. In Appendix A.2.1, they mention that the best results from the twelve combinations of the aforementioned sub-categories (one from each of the four attributions) are reported. Could the authors explain how they select the best results? Do they use the test OOD data to select the best results? \n\n5. Could the author describe how they integrate the existing state-of-the-art detection methods in detail? It is hard for me to understand what they exactly do in their proposed method.  \n\n--------- After Reading the Updated Paper ----------\n\nThanks for the update. After reading the revised paper, I still have some major concerns:\n\n1. The current experiments performed are not enough to demonstrate the effectiveness of the proposed method. The old experiment results (Table 6, 7, 8) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data, which is not realizable in practice. We should assume that the test OOD data are unknown during learning the OOD detector. The new experimental results where they train the binary classifier using adversarial examples generated on in-distribution data (follow the Mahalanobis method) in Table 1 are limited. For example, on CIFAR10, they only report results for ResNet50 and WideResNet, but I also want to know the results for DenseNet (Mahalanobis method [4] performs very well on CIFAR10/SVHN using DenseNet under the same setting).\n\n2. Some experimental details about their method are missing. The authors mention that they train 12 binary classifiers and then select the best one on the validation dataset. But they don't provide the details about the validation dataset, which is critical for their results. Based on their previous response, it seems they use a subset of test OOD data to select the best classifier, which is not allowed I think. Based on the current description of experimental settings, it is hard for me to evaluate the reported results.\n\n3. The proposed approach needs a lot of hyper-parameters (4 attributes, 12 combinations, the weights of the binary classifier, etc) and it is unclear how to tune these hyper-parameters and how they would affect the results. The current ablation study is limited I think.\n\n4. This paper doesn't have rigorous analysis for why integrating different attributions would improve OOD detection. I think this is an empirical paper but the experiments provided are not sufficient to demonstrate the effectiveness of the proposed method.\n\nTo clarify, I didn't agree to raise the score previously. What I said was that the previous paper needed significant revision and I could not recommend acceptance. I still have some major concerns after reading the revised paper. Thus, I keep the same rating and think the paper is not ready for publication. I hope the authors could keep improving their paper. \n\n \n[1] Hendrycks, Dan, Mantas Mazeika, and Thomas Dietterich. \"Deep anomaly detection with outlier exposure.\" arXiv preprint arXiv:1812.04606 (2018).\n\n[2] Liu, Weitang, et al. \"Energy-based Out-of-distribution Detection.\" arXiv preprint arXiv:2010.03759 (2020).\n\n[3] Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" Advances in neural information processing systems. 2017.\n\n[4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}