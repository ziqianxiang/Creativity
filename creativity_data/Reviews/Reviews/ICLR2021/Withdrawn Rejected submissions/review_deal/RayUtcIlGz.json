{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper discusses a method to update/optimise invertible matrices via low-rank updates. The key property of the proposed method is that it keeps track of the matrix inverse and its determinant through the optimisation (with updates that are much cheaper to compute than a direct inversion/determinant computation).\n\nWhile the method of performing low-rank updates for invertible matrices itself has already been extensively studied in the literature as pointed out by reviews, this work focuses (after extensive revision) on the properties of this update method.\n Since the updates may leave the manifold of invertible matrices, a numerical stabilisation step was introduce whereby updates that produce ill-conditioned matrices are rejected during optimisation.\n\nRank-one updates allow for fast update of matrix inverse and determinants. So this is particularly interesting when applied to normalising flows, as it allows for cheaper computation of the log-det-Jacobian terms.  \n\nThe novelty of this approach is rather limited (as also pointed out by R2). The experiments and, in particular, the application to normalising flows are interesting, well-executed. It is not clear if there are advantages of the method in other domains where log-det-Jacobians are not necessary relative to existing literature.\n"
    },
    "Reviews": [
        {
            "title": "A method to optimize invertible matrices through rank one perturbations",
            "review": "This paper introduces an algorithm for training neural networks in a way that parameters preserve a given property. The optimization is based on using a transformation R that  perturbs parameters in a way that the desired property is preserved. Instead of directly optimizing the parameters of the network, the optimization is carried out on the parameters B of the auxiliary transformation R.  \n\nThe method is (only) exemplified with the particular case where one needs to optimize a network with the property of having invertible layers (which is an important use case for example for normalizing flows, and invertible networks). In this particular case, the paper shows that the transformation of the parameters can be cast as a perturbation of rank one matrices using closed-form formulas that can be used to check and guarantee the invertibility. The parameters are updated periodically, after a series of perturbations which helps to stabilize the optimization. The paper shows three experiments on two synthetic datasets (linear data and 2d manifold) and  one on Boltzmann generators to generate samples of Alanine Dipeptide.\n\nThe paper presents an interesting idea and some empirical evidence showing promising results. My main concern with the paper is that the experimental evidence is quite limited so it is hard to judge the real contribution of the method. Additionally, the paper could improve the organization.\n\nIn particular, Section 2 needs a better organization. The title mentions a general idea, but in practice only the case of invertible matrices is analyzed and discussed. Section 2.2. is rather disconnected from Section 1.1. There's no motivation why this is there. (Maybe this should be moved to a subpart of Section 2.3). Same applies to Section 2.4 (implementation details?). It might be better to specifically focus and present the method for the the case mentioned in Section 2.5. Also, it seems more reasonable to discuss the related work (Section 3) before jumping into the algorithm details (part of Section 2).\n\nSome questions:\n\n-- Update u and v. The optimization in u and v has too many degrees of freedom. For example, you could constrain to always have ||v||=1 without losing anything. Will this help to avoid local minima?\n\n-- In the first experiment (linear data) it is mentioned that when training against linear training data P^4Inv can get stuck into local minima, and that this does not happen when using more complex data. Would you mind elaborating a little more on this observation?\n\n-- What can be concluded from Figure 4? It seems to me that all the tested methods did a good job here.\n\nTypos and minor comments:\n--Pag2 - B_0 = ids. I think this should be R_{B_0} = ids\n\n--Pag6 -  B=-I_{101}, I assume this should be A=-I_{101}.\n\n--Pag7 - \"since it with a given\" (writing)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Parameter perturbations for building invertible matrices applied to density estimation",
            "review": "# Summary\nThis work proposes parameter perturbations for the training of neural networks while preserving some properties. It particularly focuses on the case of invertible layers and demonstrates improvements over classical architectures for density estimation with normalizing flows. It is empirically shown that parameter perturbations allow for sign changes in the determinant.\n\n# Major comments\n## Pros:\nThis work is original and tackles an important problem, that is learning invertible matrices. I think the approach is original and the fact that it could be applied for other properties makes the development and the proof of work very interesting. \nAt some point you discuss some weaknesses of your work, I really liked that as it will definitely help people using your method. The paper is clearly written and so easy to follow. The experiments are insightful and help the reader to get a better feeling of what is happening at training time. Some results suggest that using your method clearly helps for certain density estimation tasks. Additional experiments could maybe show that P4 + Bent leads to efficient and expressive normalizing flows.\n\n## Cons:\n1) The focus of the paper is not really on parameter perturbations for training neural networks in general. Most of the paper discusses how perturbations can be applied for learning invertible matrices. Introducing the paper as if it could be applied successfully for any kind of properties does not make a lot of sense to me as only experiments on invertible matrices is presented.\n2) The paper discusses some concurrent methods for learning invertible matrices such as https://arxiv.org/pdf/2006.15090.pdf. However, it is not clear which method is preferable in which situation. What is even more embarrassing is that no comparison at all is performed against this work.\n3) A serious discussion about the computational cost of using perturbation instead of another method for learning an invertible matrix is lacking.\n4) The benchmark you are using to assess the performance gain on the density estimation task is a bit odd to me and does not allow the reader to really assess the usefulness of your method in such a setting. \n\n## What could be done to address my comments\n1) Either perform additional experiments showing the applicability of your method for a large bunch of interesting properties (e.g. orthogonality). Instead, I would suggest clarifying the title, the abstract, and the intro. Adding a discussion about other interesting properties that could be kept with your method between the experiments and the conclusion would make sense to me. As is I feel like you're selling something more generic than what you really show with your experiments whereas I believe learning invertible matrices is already an important and interesting achievement.\n2) Add a comparison with the referenced paper. And discuss the potential advantage of each method.\n3) Discuss the computational cost. As an example how long is the training of P4 inv in comparison to linear in fig 3.\n4) Try to discuss the more general advantage of using learned invertible linear layers in flows instead of predefined PLU factorization. I don't think benchmarking your \"flow\" with respect to the UCI datasets will really improve the quality of your manuscript but it would help the reader to get a sense of some possible additional expressivity gained with your method.\n\nIf you address comments 1, 2, and 3 (or convince these are not important), I will raise my score. Addressing 4 is less important but it would definitely improve the manuscript's quality.\n\n# Minor comments\nPages 2: B_0=id_S -> R_{B_0} = id_S ?\nPage 3: Could clarify/elaborate on the last sentence just before algorithm 1.\npage 5: I would avoid classifying NFs into two categories as you do.\npage 6: Beginning, \"quadratic matrix\" -> \"square matrix\" ?!\n4.3: \"since it with\" ?\nWhy not just using elu, sigmoid, or than instead of Bent identities? Could you explain why if it is important?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review: Nice idea and very clear writing, uncertain about concern raised by reviewer 2. ",
            "review": "### Update: \nI'd like to thank the authors for carefully addressing my concerns.. \n\nReviewer 2 claims $P^4$ training is a special case of previous work. It seems the authors agree at least to some extent *\"we agree that the P4 update scheme is closely related to dynamic trivializations\"*. It is not clear to me how harshly this should be penalized. In some cases, it is interesting to research special cases of known general results. Unfortunately, it is hard for me to judge if this is the case here, as I know of the previous articles reviewer 2 refer to, but I have not studied them carefully. As a result, I am decreasing my confidence from 3 to 1 and my score from 7 to 6. \n\nI hope this does not discourage the authors, and wish them good luck with future research. \n\n___\n\n### Summary \n**Objective:** train neural networks while retaining properties like invertibility or orthogonality of layers. \n\n**Approach:** instead of optimizing normal weights optimize an rank 1 update. Occasionally, the rank 1 updates are merged with the network parameters.  \n\n### Strengths\n**[+]** Preserving properties like invertibility is important and has been studied by much previous work. The authors present a novel approach based on rank 1 updates, which, to the best of my knowledge, is completely novel.  \n\n**[+]** The article is very clearly written, it seems to me that the authors spent a great deal of time polishing the paper. \n\n### Weaknesses\n**[-]** I have a minor concern wrt. optimization of rank 1 updates which I elaborate below. \n\n### Recommendation: 6\n**[+]** The paper presents a novel approach for preserving invertibility, which could benefit many deep learning researchers. \n\n**[+]** The paper demonstrates how rank 1 updates can be used in deep learning, which I believe will inspire further research into this interesting direction. \n\nI condition my recommendation on an MLP experiment I already discussed with the authors before submitting this review. Furthermore, I'll re-evaluate my conviction after reading the comments by the other reviewers. \n\n### Questions \nThe following question was already answered by the authors before the submission of this review. I repeat the question here for completeness. \n\n---\n\nRecent research explore the idea that SGD variants perform better with networks that has more parameters. Informally, it is argued it is harder to get stuck at local minima when SGD can move in more directions. If one optimize rank 1 updates, SGD would have $2d$ directions to move in instead of $d^2$ directions. I am concerned this might impacts the performance of SGD. This concern would be address by the following experiment: train two MLPs on MNIST, one with SGD and one with $P^4$, do they attain similar loss curves?\n\n---\n\n### Additional Feedback\nI didn't find any typos, the article seems to be very polished. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "The general framework is not new",
            "review": "***Summary and general comments:***\n\nThis paper presents a method to parametrize a set of constraints via a linear space that may change. This method was already studied in much more depth in [1], where this idea is explored through the lens of vector bundles and retractions on them, with a convergence result appearing in the follow-up work [2].\n\nThe authors then instantiate these ideas in the setting of normalizing flows via the Sherman–Morrison inversion formula.\n\n***Questions:***\n\n1) For instance, parameterizing the whole orthogonal group (via the Cayley parameterization or matrix exponentials of skew-symmetric matrices) is computationally expensive. In contrast, orthogonal matrices can be cheaply perturbed into other orthogonal matrices using double Householder transforms or Givens rotations.\n\nWhy is that the case? Could you please justify the first assertion?\n\nWhat do you mean by \"it can be cheaply perturbed into an orthogonal matrix\"? The best orthogonal approximation to a given matrix is the orthogonal projection given by the polar decomposition. That is expensive.\n\n2) \"In contrast to this related work our method reparameterizes the update step during gradient descent rather than the parameter matrix itself\"\n\nThis is not really different as explained in [1]. Using a dynamic trivialization changes the metric that you are working with, but you can still use it as a parametrization. In your example, it would account for changing the problem\n$$\\min_{x \\in \\operatorname{GL}(n)} f(x)$$\nto\n$$\\min_{u,v \\in \\mathbb{R}^{2n}} f(X_0 + uv^\\intercal)$$\nfor a fixed $X_0$ and changing the problem via the dynamic trivialization framework every $N$ steps. Using this formulation, you can let the optimizer do all the heavy lifting for you. See also 5)\n\n3) Trivializations are known to have a problem with the momentum and adaptive term. I see that you use Adam in the training of the 2D distributions. How do you solve the problem of the incorrect momentum and adaptive terms when you change the base?\n\n4) A general question. I do not understand what is the motivation behind using the low-rank update besides the fact that it allows for an (amortized) low cost of the inversion.\n\n5) A follow-up of the previous question: How is $R_{X}(u,v) = X + uv^\\intercal$ a sensible map to use? This map does not have its image on $\\operatorname{GL}(n)$ but on all $\\mathbb{R}^{n\\times n}$! For example, $R_{-uv^\\intercal}(u,v) = 0$ which is clearly not invertible. As such, I do not understand the claims in section 2.3 about this being a \"fully flexible invertible layer\". This is the reason behind having to use Algorithm 2 in P⁴Inv. Given that Algorithm 2 is used, what is the reason behind using this parametrization at all over just doing unconstrained optimization?\n\n\n***Citations:***\n- Lezcano-Casado & Martínez-Rubio do not use Givens rotations but the exponential. The first to use Givens rotations in the context of ML was [3].\n\n- When it comes to Riemannian gradient descent, it is probably better to cite Absil's book [4] as a general reference rather than two recent papers, as this is a well studied topic.\n\n- I would recommend to clean-up the bibliography, as there are many citations that point to the arXiv when the articles have indeed been published in peer-reviewed venues.\n\nMinor:\n- \"which is a R-diffeomorphism\" -> \"an\"\n\n***Conclusion:***\n\nI really like the first experiment for its simplicity, trying to elucidate the behavior of the layer. It is also nice to see the improvement that this idea gives over RNVP. At the same time, when it comes to the experiments, I believe that it would have been of interest to compare this approach with other known ways to parametrize invertible linear layers, such as those that use QR, SVD or Choleski factorizations.\n\nThat being said, as mentioned in 4) and 5) I do not see the reason for this being a good way to obtain invertible layers, given that even the image of the parametrization does not lie in GL(n). Furthermore, the ideas behind the framework presented in this paper were already studied in previous papers in much more depth.\n\n\n[1] M. Lezcano-Casado. “Trivializations for gradient-based optimization on manifolds”. NeurIPS, 2019\n\n[2] M. Lezcano-Casado. “Curvature-Dependant Global Convergence Rates for Optimization on Manifolds of Bounded Geometry”. https://arxiv.org/abs/2008.02517\n\n[3] U. Shalit, G. Chechik. “Efficient coordinate-descent for orthogonal matrices through Givens rotations”. ICML, 2014\n\n[4] P.-A. Absil, R. Mahony, and R. Sepulchre.Optimization algorithms on matrix manifolds. PrincetonUniversity Press, 2009- .",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}