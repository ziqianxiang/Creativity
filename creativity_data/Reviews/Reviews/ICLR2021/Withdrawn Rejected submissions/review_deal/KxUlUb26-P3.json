{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper first makes the observation that incidental supervisory data can be used to define a new prior from which to calculate a PAC-Bayes generalization guarantee.  This observation can be applied to any setting where there is unsupervised or semi-supervised pre-training followed by fine-tuning on labeled data.  The PAC-Bayes bound is valid when applied to the fine-tuning.  For example, one could use an L2 bound (derived from PAC-Bayes) on the difference between the fine-tuned parameters and pre-trained parameters.\n\nBut the paper proposes evaluating the value of pre-training before looking at any labeled data. Let $\\pi_0$ be the prior before unsupervised or semi-supervised training and let $\\tilde{\\pi}$ be the prior after pre-training.  The paper proposes using the entropy ratio $H(\\pi_0)/H(\\tilde{\\pi})$ as a measure of the value of the pre-training.  As the reviewers note, this is not really related to PAC-Bayes bounds.  Furthermore, it is clearly possible that the pre-training greatly focuses the prior but in a way that is detrimental to learning the task at hand.\n\nI have to side with the reviewers that feel that this is below threshold."
    },
    "Reviews": [
        {
            "title": "The paper explores a new and exciting territory, but needs a deeper analysis to support the connection with the PAC-Bayes theory.",
            "review": "The use of PAC-Bayes theory for NLP tasks is rare. Although I know little on NLP, the paper proposition to leverage on  PAC-Bayes for evaluating the benefit of various incidental supervision signals seems promising. However, even if the empirical results are good, the connection between PAC-Bayes and the proposed informativeness measure (named PABI) is vague. The paper needs to better situate the proposed analysis compared to classical PAC-Bayesian generalization risk bounds.\n\n**Section 2 contains many assertions that are questionable.**\n1. *\"The training samples [are] generated i.i.d.\"*: It is the case for most PAC-Bayes analysis, but I wonder to which extent this assumption holds for the NLP problems studied as experiments. In a sentence, words are highly dependent on each other.\n2. *\"In the common supervised learning setting, we usually assume the concept that generates data comes from the concept class\"*: This is a surprising claim as the **PAC**-Bayes framework differs from the Bayesian one namely by the fact that we usually don't need to make assumptions about the data-generating distribution other than being i.i.d. In particular, the model does not need to be well specified. This makes me wonder if PABI would not better fit in the purely Bayesian framework (see other comments below).\n3. *\"the generalization bounds in both PAC-Bayesian and PAC frameworks have the square root function\"* : There exist several forms of the PAC-Bayes theorem in the literature, not only the square root ones (e.g., Seeger 2002). In fact, the square root bounds are not the tightest, particularly when the model is well specified.\n**Is PABI really backed by PAC-Bayes theory?**\nAs far as I understand, the procedure PABI is only remotely inspired by the PAC-Bayes bound,  but is not truly justified by it. No PAC-Bayes bounds are fully optimized; PABI borrows from PAC-Bayes the sole idea of relying on the KL between distribution. For this reason, I think that the introduction sentence \"Previous attempts are either not practical or too heuristic\" is harsh, because the proposed method turns out to be a heuristic too.\n\n**Is PABI a more Bayesian method than a PAC-Bayes one?**\nI wonder if one could not do the same analysis in a fully Bayesian setting, maximizing a Bayesian information criterion. This should be appropriate since PABI and the Bayesian setting assume that the model is well specified. Note that there is a direct link between the Bayesian Marginal Likelihood and the PAC-Bayes generalization bound (e.g., Germain, et al., 2016: \"PAC-Bayesian Theory Meets Bayesian Inference.\")\n\nOverall, I think that the paper explores a new and exciting territory, but needs a deeper analysis to support the connection with the PAC-Bayes theory.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unified measure for alternative supervision signals",
            "review": "\n#### Summary\n\nThis paper proposes a unified measure for the informativeness of incidental signals (ie, not standard ground truth supervised labels) derived from the PAC-Bayesian theoretical framework. Instantiations of the score are derived for a variety of these signals, and experiments show good agreement between the measure and true performance improvements.\n\n#### Strong and weak points\n\nThis problem setting is well-positioned as complementary to the growth in \"alternative supervision\" in both research and industry. Besides the directions identified in the paper, one could easily imagine using this kind of a measure in ML applications as a tool to help guide economic decisions about what kinds of datasets or annotations to pursue. The explanatory potential of this approach with respect to observed gains using incidental signals is exciting as well, especially the agreement with empirical findings from Mayhew 2019.\n\nI found Figure 1 to give helpful context, and I found the core technical content in Section 2 to be clear and precise.\n\nThe experimental results were a bit intricate to follow. A key result is Figure 2f and the associated correlations, which show strong correlation between the PABI scores and true performance improvements, this could perhaps be higlighted or emphasized more. Likewise the meaning of Figure 3 is a bit obscured by the poor correlations of the\nbaselines.\n\nOne weakness of the evaluation was that, while the Related Work coverage seemed sufficient, only Gururangan 2020 is included in the experiments. Of course the other approaches have the limitations well-captured in Table 1, but it would have been nice to have some restricted experiments crafted in order to give direct comparisons.\n\nThe supplemental appendix was comprehensive with respect to theoretical derivations and experimental details. \n\n#### Recommendation (accept or reject) with one or two key reasons for this choice.\n\nI would recommend to accept, the work represents an advance across both theory and practice on an important problem.\n\n#### Supporting arguments\n\nThe work leverages a well-studied framework to answer important questions about understanding the utility of non-standard supervision signals, enabling us to reason in a unified way about varied kinds of these signals as well as their combinations. Experimental results\n\n\n#### Questions to clarify / additional evidence required\n\nThe approximation in Definition 2.2 was a little strange for me, and seemed kind of circular: for calculating our approximate PABI, we are approximating the target (gold) distribution with our approximatively improved prior ($\\tilde{pi_0}$)? Is there anything we can say about how good/accurate this approximation is?\n\nSection 3.2: \"much cheaper\" - how or why would we say this is true, can we quantify it? Or are there cites to see?\n\nIs it possible to frame the PABI measures in terms of testable hypotheses about true generalization error, or are the bounds too loose in practice to say anything meaningful here?\n\n#### Additional feedback to improve\n\nSpace permitting, a small diagram of the mappings between different domains and the restrction trick would make Section 3.2 much clearer. Another possibility is some symbol table to keep straight which versions of $c()$ correspond to gold vs silver, incidental, etc.\n\nThe code was great to see as well but is missing dependencies:\n\n- seqeval\n- tqdm\n- transformers\n\nI might suggest adding a requirements.txt or similar.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Great work, minor comments on framing/pitch",
            "review": "This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of “incidental supervision signal” for a downstream classification task. In particular, when labeled data is only available in noisy or partial form, or over a different domain than the target test domain, this data may still be used to improve a classifier, but it’s unclear how to tell which forms of incidental supervision will be most useful. Having a measure which allows us to compare different types of such supervision enables us to make intelligent tradeoffs.\n\nPABI is proposed as a very general framework. The most general form of the measure, dealing with updates to the concept class prior, seems that it could capture any kind of incidental supervision. However, this means most of the work is in understanding how to apply and approximate it. This paper provides several such methods, particularly focusing on “inductive” learning (from constraints or partial/noisy gold labels) or “transductive” learning (from complete gold labels on different input domains). Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors).\n\nComputing PABI may be challenging in some cases. In the case of transductive learning, it seems that a model needs to be trained on the incidental signal, although this is better than the combinatorial explosion of jointly trained models that would be required to test relative improvements directly. However, it’s not clear if efficient approximations for PABI will be feasible in all cases. This and other questions about the breadth of application of PABI are left for future work.\n\n### Strengths\n\nI think this paper is very well-motivated, situates itself well with respect to previous work, and presents clear advantages. Having a unified framework for comparing the utility of different kinds of incidental supervision signals seems potentially very useful, especially these days when incidental supervision of various sorts is instrumental in state-of-the-art models. It is also extremely relevant for data annotation and task design, which often has to make tradeoffs between these factors (i.e., noise versus partial annotation or dataset size).\n\nThere is a lot of content in this paper, including mathematical developments, algorithms, and experimental results. While I did not carefully check the proofs in the appendix, and I am not familiar with PAC-Bayesian theory or the associated literature, the paper seems technically sound to me.\n\n### Weaknesses\n\nWhile the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better.\n\nAs proposed, the PABI framework seems very general—which is good. But the paper only shows how to realize the framework in a couple specific cases, for “inductive” and “transductive” learning independently. This is still more general than previous work, but from the first few pages of the paper I was expecting something even more general.\n\n* It seems to me that the combination of inductive and transductive learning may be possible using something close to the paper's proposed methods , but this isn’t addressed by the paper except a glancing mention in Footnote 6.\n* It also is not clear to me from the paper’s text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. In particular, it seems that in this case the approximation method proposed for transductive learning would indeed have to reduce to training a combined model. Related issues were finally mentioned briefly in the last paragraph of the paper, and something along these lines appears in appendix A.3, but I think a more up-front clarification of the limitations is warranted.\n\nMore broadly, the question in the back of my head when I began reading the paper was if this would help explain why and when language model pretraining (and other more flexible related-task pretraining) works well. The paper points to related work in this area, such as Gururangan et al 2020 (“Don’t Stop Pretraining”), leading me to think this paper would shed light on the issue, but in the end the issue was not mentioned and seems perhaps out of scope.\n\nThis is fine. All I would ask of the authors is to be more explicit about the limitations of PABI (or the proposed realizations of it) from the beginning, laying out the scope of this work and stating the limitations outright instead of only pointing to the appendix. It seems to me like PABI is more of a foundational framework which is ideal for future work to build into, rather than already being a general solution in itself. I think it would be best to pitch the paper this way.\n\n### Recommendation\n\nAccept. Important problem, lots of solid content, clear benefits over previous work and directions for the future. Great work.\n\n### More comments/questions\n\nI think the point of the formulation in Section 2.2 can be made a bit more explicit. It seems like the point is for applying PABI to partial labels. If that’s true (or there’s more to it) then might as well just say it there, or at least give this case as a motivating example.\n\nRegarding the cross-domain results: why are the incidental supervision sets so small? It seems that there is a ton more incidental supervision available for NER, and in both cases the incidental supervision data is even smaller than the test set. Why not use more? It seems to me that the use case here is when a large amount of incidental supervision is available anyway. It also seems like the low-data setting is not totally fair to the vocabulary overlap baseline.\n\n### Typos, style, etc.\n\nWhen describing your experiments, I think it’s worth mentioning that they are on English text.\n\nFigure 3: I don’t understand which numbers correspond to which model in the caption. This would be much easier to read in a table.\n\n* P. 7: something’s wrong with “twitter(Strauss et al., 2016)”\n* P. 7: The FitzGerald et al 2018 dataset is called “QA-SRL Bank 2.0”.\n* P. 7: servers -> serves\n* P. 7: “the lower bound for is”\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A nice unified framework to measure informativeness for various incidental supervision signals, but with the lack of qualitative evaluation of the correlation studies. ",
            "review": "##########################################################################\nSummary: \nThis paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals. PABI can measure various types of incidental signals such as partial labels, noisy labels, constraints, auxiliary signals, cross-domain signals, and their combinations. In NER and QA tasks, they showed the strong correlation signals between PABI and the relative improvements for various incidental signals. \n##########################################################################\nReasons for score: \n \nOverall, my score is marginally below than acceptance threshold. \nPros:\n1. I enjoyed reading the paper, and I like the idea of covering various types of supervision signals at one unified measure. \n2. The definition and approximation of PABI and its generalization to different inductive signals look sound to me.\n \nCons: \n \n1. My biggest concern about this paper is the lack of clarity and presentation. In the introduction, I do understand how conceptually PABI is different from others, but do not know what it is. It would be better describing how PABI works in the introduction, Also, it would be better understanding the Section 2 and 3, if authors provide high-level insights of why each part of PABI’s description is important. Similarly, in the experiment, it was quite difficult to follow the text and capture the main claim. For instance, it would be better to understand if how Figure 2 and 3 should look like first and what trends of the points support the main claim of PABI, etc. Similarly, visual interpretation without specific guidelines make Figure 3 really difficult to understand. I guess some quantitative numbers would be very helpful like the linear regression slope, etc. \n \n2.  Besides the presentation, I don’t quite understand how PABI can be used as a practical measure for other applications. Does the strong correlation with relative improvement mean that it can be used as an alternative measure of mutual information and further applied to other applications using such information measures in their optimization? If so, it would be nice to describe potential applications of these measures and other benefits of PABI in general. This also requires additional experiments that show its effectiveness in other applications. \n\n#########################################################################\nSome typos:\na widely used measure for for noisy signals -> a widely used measure for noisy signals\nthe SQuAD dataset servers as the main dataset -> the SQuAD dataset serves as the main dataset\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}