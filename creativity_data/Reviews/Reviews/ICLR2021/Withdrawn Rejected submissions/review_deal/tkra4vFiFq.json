{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "While all reviewers agree the problem of TEEs for model training is well motivated, the reviewers remain divided on whether the concept of randomly selecting computations to verify has sufficient novelty, and whether the proposed gradient clipping method is well motivated.\n"
    },
    "Reviews": [
        {
            "title": "Practical approach for secure training using a TEE",
            "review": "Paper Summary:\n\nThe paper presents new techniques to enable secure neural training in the TEE+GPU paradigm. This is a natural extension of previous work like Slalom which only handles the inference case. The authors propose a two-step approach. First, they clip the gradients during training to force the attacker to insert multiple deviations to influence the model. They then randomly verify the integrity a subset of the gradient updates to check for tampering. Combining these two ideas the authors demonstrate a 2-20x improvement over a TEE only benchmark.\n\nScore Rationale:\n\n- The paper presents two novel ideas\n  - The use of probabilistic checks on a random subset of the gradient update steps\n  - The use of gradient clipping to force the attacker to tamper with multiple update steps to affect the final model\n- While the core idea of verifying the linear layers is a direct extension of previous work, the probabilistic checks offer a significant improvement in performance\n\nStrengths:\n\n- The paper presents a detailed empirical evaluation of the required verification rate as function of multiple hyper-parameters such as the style of the injected error, aggressiveness, and timing of the poisoning\n- The paper demonstrates that gradient clipping has minimal impact on the network accuracy if a suitable learning rate is chosen\n\nWeaknesses:\n\n- The core contribution of this work relies on the effectiveness of clipping procedure in forcing the attacker to introduce a large number of faults for a successful attack. However, this conclusion is not directly supported by empirical evidence, i.e. there is no ablation study of the verification rate required in the absence of clipping.\n\nMinor Comments/Questions:\n\n- There is a typo in the caption of Fig. 6 which should specify the CIFAR-10 dataset rather than ImageNet\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper targets security challenges of deep neural networks. The authors provide evaluation results to demonstrate their work but more details are needed in some discussions.",
            "review": "The paper targets security challenges of deep neural networks. While solutions can hardly scale up to support realistic DNN model training workloads, the authors propose GINN to support integrity-preserving DL training by random verification of stochastic gradient steps inside trusted execution environments (TEE). GINN combines random verification and gradient clipping to achieve good performance. The experimental results show that GINN achieves 2x-20x performance improvement compared with the pure TEE based solution while guaranteeing the integrity with high probability.\n\n#######################\nPros\n1.\tThe security of deep neural networks is a very interesting and challenging topic.\n2.\tThe paper is well structured, and the assumptions are clear.\n3.\tThe authors provide extensive evaluation results to demonstrate their work. \n\n#######################\nCons\n\n1.\tThe authors illustrated how to do probabilistic verification: randomly decide to verify the computation over each batch. But how is a batch verified inside the TEE? It is not clearly stated in the paper. \n2.\tWhen discussing the limitation of existing solutions, the authors claim that they are evaluated with small datasets (MNIST, CIFAR10). But there is not much evaluation on large datasets for GINN either. Maybe adding an experiment on a large dataset in Figure 3 will be helpful? \n3.\tThe random verification strategy is based on the observation that it is unnecessary to verify all of the computation steps. Where does the observation come from? More details, data or citations will help.\n\n#######################\nMinor comment\n1.\tAs far as I understand, the use of gradient clipping is intended to reduce the bias introduced by the adversary (which might be missed by random verification ) so that the adversary tends to launch multiple attacks? It would be better to explain that clearly in the paper.\n2.\tIt would be good to see some results about Verification Rate v.s. clipping rate.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "It seems that the paper is focusing on the privacy preserving training of deep neural networks by developing a Learning-as-a-Service framework. It assumes that all resources may be penetrated by adversaries except the TEE. In this paper, the authors leverage random verification to detect the attacks and shows how gradient clipping can defend against attacks.\n\nAs an avid user of cloud training, I can agree upon the motivation of the paper that the integrity of training models in the cloud. However, I want to write about some issues that I have about the paper.\n\nFirst, it seems that the main contribution of the paper is two folds (1) using randomized verification over full verification and (2) verifying gradient clipping to defend against the attacks. However, mere randomized runs of verifications speak for a rather humble contribution. It would be interesting to see whether there can be other more intelligent methods that can identify them. It would add to the novelty and merit if there were some evaluations too for this.\n\nFurthermore, to verify the real effectiveness of the gradient clipping as the defense mechanism, it would be nice to see some theoretical analysis on this front.\n\nFurthermore, I have some issues with the evaluation. First, the paper compares to a full verification in Figure 2. However, this information is given without any breakdown of \"actual training\" vs. \"verification\". This makes it hard for the readers to see exactly where the speedups come from and where.\n\nThe paper does not seem to demonstrate any evaluation n terms of the potential accuracy loss for large datasets. While F.3 seems to demonstrate this on CIFAR10, this cannot fully verify that such gradient clipping wont hurt the accuracy of the models.\n\nI am open to reevaluating the paper given the authors' feedback.\n\n============================================\n\nI thank the authors' for their response.\nI am satisfied with the answer regarding gradient clipping. As such, I raised my score to 5.\n\nHowever, I still cannot get away from the thought that random verification seems to speak for a rather humble a contribution. Combined with the lack of breakdown of \"actual training\" vs. \"verification\", I am rather hesitant to give a score higher than this. Regarding the experimentation on ImageNets, I understand that there was lack of time to add more experiments on this front. However, it would provide a more concrete message if the paper includes these additional experiments.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straight Forward Integrity Checking",
            "review": "The paper discuss how to detect erroneous steps in gradient descent on a non-trusted GPU using a separate slower trusted execution environment,\nby randomly deciding in each step whether to check the values returned by the GPU, as well as using small learning rates and clipping the gradients to ensure all updates are small.\n\nThe reason for checking this is based on the assumption that since GPU calculations are out sourced there may be trust issues and attackers with control of the\nvalues returned by the GPU can alter the final network in subtle ways. \n\nThe paper includes experiments and shows that this approach is faster than just running everything on the trusted execution environment.\nThe experiments test an attack approach where the attacker tries to inject some bad samples to get some success of making the network being trained output some particular class on a particular image kind.\n\n\nI have questions about the model. It is explained that the attacker is full control of CPU and GPU etc. but it is also assumed the attacker runs the code on GPU as expected. I would prefer a complete black box model, where the system can input model, batches, parameters etc. to the GPU and get whatever it wants back, i.e. gradients, activations, whatever you desire and given an input the attacker can decide to return whatever you want.\n\nIt seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive. \n\nFinally, the time is measure compared to a pure based TEE solution which is stated in the paper as completely unsatisfactory, and should instead be compared with training time without using any form of verification as this is the target.\n\nIn my opinion this paper is an implementation of a straight forward idea and the theorems for setting the probability parameters  are basic probably computations.\nThere is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics.\n\nIn short, in my opinion the paper is simply not relevant or strong enough to warrant acceptance at ICLR.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}