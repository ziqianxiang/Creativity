{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposed a shot-conditional form of episodic fine-tuning approach for few-shot image classification. There were a number of concerns raised, e.g., there lacks of sufficient comparison with SOTA baselines, the justification on the significance of shot-aware approach is not entirely convincing, and incremental contributions in both novelty and improvements. While some of these issues were improved in the rebuttal, the revision remains not satisfied by the reviewers. Overall, I think the paper has some interesting idea, but is still not ready for publication. "
    },
    "Reviews": [
        {
            "title": "Not familiar with this area, but assigned to review",
            "review": "This paper aims to understand the role of this episodic fine-tuning phase and discovers that fine-tuning on episodes of a particular shot can specialize the pre-trained model to solving episodes of that shot at the expense of performance on other shots. It proposes a shot-conditional form of episodic fine-tuning.\n\nThe main contribution of this work is the training objective, which varying the shots by introducing a distribution over shots $P_k$. In addition, the model parameters are not separately or independently maintained for different shots. The author proposed a conditioning mechanism using FiLM, where $k$ is a conditional variable or input to the FiLM network ($\\gamma$ and $\\beta$). The idea of shot-specific feature extractors is not new, and it is a common trick in amortized variational inference to reduce the number of model parameters.\n\nIn the experimental section, I did not see other baselines in previous works. The comparisons are mostly against the two methods named standard and L2 BN, which seem to be simple variation of Prototypical Networks. Even the proposed SCONE is a variation of Prototypical Networks, it also reduces the novelty or originality.\n\nI am not familiar with this area or related research papers, but was assigned to review this paper. My evaluation is only based on the my understanding of the contents including in the manuscript. I would like to see the comments from other reviewers in this domain.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Good experiments, but have conceptual issues and baseline comparisons needed",
            "review": "Summary\n\nThe paper proposes a solution to few-shot meta learning approaches overfitting to the number of shots they are finetuned on, and not generalizing as well as expected to novel shots. In order to mitigate this problem, the paper suggests a parameterization of the meta learner which also conditions on the number of shots the model trains on. In practice, this is done via manipulation of the batch normalization parameters based on the number of shots. With this conditioning, the paper shows that the models perform better across a range of shots that they are evaluated on, compared to various sensible baselines.\n\nStrengths\n+ The paper does tackle an important problem, a meta learner should not be overfitting to the number of shots\n+ The paper has very thorough experiments and numerous visualizations (for e.g. of the learnt batch norm embeddings) to help gain better intuitions on what the method does\n+ The paper is well written and easy to understand\n\nWeaknesses\n\nImportant points for the rebuttal are marked with (*)\n\n(*) The problem setting of changing the number of shots during meta-training is still somewhat artificial since one could still test the model on a different number of shots during the Query set evaluation. I wonder if in light of this issue, conditioning on the number of shots is getting at the root of the issue. One would rather want to have methods like Cao et.al. which aim to make the model invariant (or insensitive) to the number of shots as opposed to explicitly conditioning on them. Would be great if the authors could clarify this issue.\n\n(*) Related to the above issue, a disadvantage of the proposed method over prototypical networks is that ProtoNets could be applied to episodes of any number of shots, even those unseen during training. Similarly, Cao et.al. can also be applied to any number of shots since it does a projection of the feature embedding. However the conditioning approach fails in this case, which is somewhat unrealistic. I understand that there is a smoothing step in the shot distribution which could partially help mitigate that issue, but it does not seem satisfying. How do the authors think this issue could be mitigated, and how should one view this as a potential shortcoming of the approach in light of the goals of the paper?\n\n(*) In general, it seems appropriate for the paper to have a comparison to Cao et.al. who propose a solution to shot-overfitting in the context of prototypical networks. While I understand that the proposed approach is more general than that of Cao et.al. that is not sufficiently demonstrated via. the experiments which still focus on the prototypical networks, which is not really an issue, but then warrants comparison to Cao et.al.\n\n(*) It would also be really useful to include a comparison to prototypical networks that does sum of the features instead of the mean to compute the prototype, which would preserve information on how many shots there exists in the support set. In general, it would also serve as a more stringent test of whether shot conditioning in the way the paper does it is needed when the method is able to deal with or recognize when the number of shots has changed, which the prototypical network approach is not able to do. \n\nIt would also help to guide discussion on how shot conditioning might look like for different algorithms. For prototypical networks we used FiLM, for something like MAML one could imagine unrolling for a different number of steps based on how many shots there are to prevent overfitting. A discussion on this would help improve the paper. \n\n(*) It would be useful to compare against a single model which trains on “k-shots” and is evaluated on a mixture of k-shots (as in meta-dataset) and search explicitly for the best value of “k” from [1, MAX_SHOTS]. Does the proposed approach still outperform that “best” (over k) model? That seems to be an important baseline comparison. \n\n** POST REBUTTAL UPDATE**\n\nI went through the other reviews and the author rebuttals. I thank the authors for throughly addressing all of my concerns and \nrunning some baseline experiments which I think are quite interesting, and add to the completeness of the paper, including a\ncomparison to EST and an optimal K-shot baseline. I think the idea is simple and interesting, and now the paper has enough\nexperimental comparisons to be a valuable addition to the conference. I have updated my score accordingly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A simple method for well-motivated problem",
            "review": "Summary: \nThis paper proposes to use a shot-conditioned model that specializes in pre-trained few-shot learning model to a wide spectrum of shots. The proposed approach is simple but effective, it trains neural networks that conditioned on a support set of a different number of shots K during the episodic fine-tuning stage, with FiLM as the conditioning mechanism.\n\n\nPros:\n1. This paper is clearly written, well-motivated, and thoroughly evaluated. It is an enjoyable reading experience .\n2. The proposed model and training algorithm is simple but effective\n3. Experiments are well designed, which first verifies that shot-conditioned few-shot learners can achieve relatively good performances on different Ks and then perform the large-scale evaluation.\n\nCons:\n1. Smoothing the shot distribution section is overly simplified. It is very hard to understand algorithm 1 given the limited details presented in the method section. I would suggest explaining in detail the smooth-shot procedure in the paper. \n2. Lack of ablation studies. There are two key designs of the few-shot learner presented in this paper, that does not have detailed ablation study results. The first is whether using the convex combination of FiLM parameters to obtain shot distribution s. The second is whether to use smoothing in shot distribution. How much are these two designs contribute to the model's final performances?\n3. In figure 2, it seems that the method trained with 5-shot has very good performances in all three scenarios presented? Would it be possible that such a good K-shot can be always found and therefore we do not necessarily need this K-conditioned few-shot learner? For instance, just always train with 5-shot and evaluate different shots.\n4. Instead of looking at the results of some instances of different K, it would be nice to have a comprehensive evaluation over different Ks. For instance, evaluating over all Ks from 1 to 40 and compute the average accuracy of each different model. To reduce the computation overhead, we can also try to evaluate Ks in {1, 10, 20, 30, 40}.\n5. Comparison of state-of-the-art methods for meta-dataset experiments.\n6. What is UMAP projection? The content of the experiments is not self-contained. How shall we read Figure 3?\n7. Figure 4 is also not straightforward to understand. What is the value on the y-axis? What does it mean? \n \n\nMinor:\n1. Inconsistent formats for average performances in Table 1. It seems that the results of the first two columns is badly formatted?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "“conditional on the number of shots” is somehow the main contribution which is however not significant, given no comparison to sota methods.",
            "review": "This paper proposed an implementation method of using different numbers of shots of data for few-shot learning such as to mitigate the negative effect of \"different shots\". It optimized the FiLM parameters using meta gradient descent during episodic meta-training with different-shot learning tasks. It conducted the experiments on quite a set of \"meta-dataset benchmarks\".\n\nPros:\n\n1. The study of varying-shot task learning could be interesting, as the quantity of data in each task really matters for few-shot learning.\n\n2. \"Conditional on the shot number\" is a reasonable idea. \n\nCons:\n\n1. The motivation for conducting such a work is not new, and more importantly, the proposed method is too incremental compared to many existing works. (1) An existing work [Cao et al. ICLR’20] (also cited by this submission) studied the same problem both theoretically and empirically and proposed a similar linear transformation method to handle the varying-shot learning issue (Please kindly refer to section 3.3 in [Cao et al. ICLR’20]). This submission makes an incremental contribution of meta-learning transformation parameters conditional on shot numbers (using an existing method called FiLM). (2) Regardless of using such conditional, the idea of meta-learning feature transformation parameters (i.e. scaling and shifting parameters) has been well-deployed in existing few-shot learning papers [Sun et al. CVPR’19] (not cited in this submission) and [Oreshkin et al., 2018; Requeima et al., 2019; Bateni et al., 2019]. While as I can see from some other papers from the same authors of these related works, this transformation seems to be working well in many recent and similar tasks such as class-incremental learning (different numbers of samples for old or new classes) [Liu et al. CVPR’20] and semi-supervised few-shot learning tasks [Li et al. NeurIPS’19]. (3) Conditional on a single scalar ($k$ in this submission) is also not new in meta-learning, please kindly check [Shu et al. NeurIPS’19]. While I admit that this “conditional” is somehow the main contribution proposed in this submission for the few-shot settings. But I don’t think it is significant.\n\n2. The experiments lack comparisons to the sota few-shot learning methods (in the same setting of using varying-shot tasks in episodic training). This is a very clear weakness of this paper. The paper does not prove that using the proposed conditional meta-training can achieve better performance of varying-shot learning over the sota few-shot learning methods. Table 1 makes me feel that this paper proposes a new BN method.\n\n3. I want to talk more about varying shots. Actually, for different classes (different levels of hardness or diversity among the classes), the model may need different numbers of samples to learn a good representation for each of them (e.g. the common sense is that a model needs more samples for the non-rigid class \"person\" than the rigid class \"spoon\"). Given the high randomness of the class sampled in any meta-learning task, it is not clear how many shots a task really needs (for its all classes). So, I am a bit doubting \"is it necessary to study the problems of varying-shot learning\". \n\n[Sun et al. CVPR’19] Meta-Transfer Learning for Few-Shot Learning\n\n[Liu et al. CVPR’20] Mnemonics Training: Multi-Class Incremental Learning without Forgetting (details about feature transformation are given in the supplementary as the transformation method is not the contribution in this CVPR’20 paper)\n\n[Li et al. NeurIPS’19] Learning to Self-Train for Semi-Supervised Few-Shot Classification (details about feature transformation are given in the supplementary as the transformation method is not the contribution in this NeurIPS’19 paper)\n\n[Shu et al. NeurIPS’19] Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (please check fig1(c) and its explanations)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}