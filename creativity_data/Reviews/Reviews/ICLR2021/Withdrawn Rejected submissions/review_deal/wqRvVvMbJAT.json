{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper introduces an adaptive label smoothing technique, where the smoothing factor is computed based on the relative object size within an image, in order to address the problem of overconfident predictions. All reviewers recommend rejection based on limited technical contribution and unclear benefits of the proposed method. During the rebuttal phase, the authors carried out more experiments and clarified several other questions asked by the reviewers. The response was well received, but did not eliminate the main concerns about the paper. While the idea is interesting and has potential, the AC agrees with the reviewers that the paper is not ready for ICLR, and encourages the authors to improve the paper according to the reviews and submit it to another top conference."
    },
    "Reviews": [
        {
            "title": "Simple idea but results are not convincing",
            "review": "This paper is about a study of calibration of deep neural networks in the image classification task, in particular to the possibility of using label smoothing to make the network emit predictions that are more accurate in terms of confidence when few or no pixels of an object are present in the input. The idea is to adjust the entropy of the labels in proportion to the amount of objectness of an image i.e. the amount of pixels related to the object. Experiments shows that the adjusted labels reduce the overconfidence of a classification network.\n\nStrengths:\n+ the paper is easy to read and presented well, beside some small issues that can be fixed easily.\n+ related works are comprehensive and the method is really easy, which is a plus.\n\nWeaknesses:\n- experiments and results show limited benefit from the method. It seems that the only impact is in the object classification task where the overconfidence is reduce at the expense of the underconfidence, which means that there is a tradeoff in the confidence of trained classifier. It is not clear when it should be used and why. Moreover, the object detection results show similar performance of AP, even inferior of CutMix, without a proper measurement of confidence in that case.\n- the method needs the amount of percentage of pixels of the object to adjust the labels. In practise, it needs the bounding boxes of the objects. This naturally suggests that the task should be evaluated in terms of object detection task. Results in this regards show that after finetuning there is minimal or negative impact (e.g. beta <<), rendering the method useless to my understanding.\n\nIn particular:\n- OpenImages dataset is mentioned in 4.3, but COCO is used. COCO is not described anywhere.\n- It is not clear the experimental setting when there are multiple annotated objects (sec 4.1). There is mention that 54k more images are derived but it is not clear how.\n- It would be interesting to compare the method with a baseline neural network that not only emits the label of an object and thus we need to compute the entropy of the answer, but also an additional output with the confidence of the score.\n- Discussion in 4.2 does not discuss the other metrics ECE, U.conf, MCE enough to give some insights about the results. In particular the U.conf gets higher with the method. What is happening and how does this affects the confidence?\n- It is not clear in table 2 what are the two experiments with 0.474M pre train N and what information they give.\n- Sec 4.4 mentions table 1 regarding the beta parameter, but it is not used. Should it be table 2?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea for a relevant problem, but the experimental evaluation is limited",
            "review": "Summary:\nThis paper introduces the concept of adaptive label smoothing (ALS).  ALS uses bounding box annotations of objects to determine how much area in the image is covered by the object and uses this estimate to change the weight in the label smoothing operation, with the goal to encourage the classifier to focus on the object and not on the context around the object during classification. The experiments on ImageNet classification show an improved classifier calibration, while for detection, the improvements are marginal.\n\nPros:\n- The paper is well written and easy to understand\n- The problem of reducing the effect of background biases on a classifier's performance is important\n- The classifier calibration on ImageNet is improved\n\nCons:\n- The method requires additional supervision in terms of bounding box annotations. It would be interesting to study unsupervised methods for estimating the objectness, e.g., using unsupervised segmentation.\n- The proposed method does not significantly improve over the baselines on object detection. I think the authors should discuss this in more detail.\n- It would be great if the paper would also discuss alternative measures for adapting the smoothing factor. \n- The overall contribution of this paper is rather limited. While the idea is certainly very interesting and the problem is important, the authors should give a more elaborate experimental evaluation of their approach on a variety of datasets and with a larger set of network architectures and for different visual recognition tasks to highlight that their strategy indeed is of general relevance.\n\n----------------------------------------------------------------------------------------------------------\nPost rebuttal:\n\nThank you for your response. After reading the other reviewers' comments and your responses, I think the paper is not yet ready for publication. All reviewers are concerned by the lack of a technical contribution and the limited benefit of the paper. While I appreciate the effort of the authors to answer our concerns, I think the paper needs a major revision that incorporates our shared concerns. Therefore, I retain my initial rating.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An adaptive label smoothing approach to learn localized visual representations",
            "review": "**Summary:**\n\nThis paper proposed an approach to learn localized visual representation by using bounding boxes annotations. It uses a label smoothing approach where the smoothing parameter is computed per example based on the proportion of the object in the image. The approach is tested on image classification and object detection.\n\n**Reasons for score:** \n\nI do not think the technical contribution is strong enough for ICLR. The idea of computing the smoothing parameter per example is interesting but it is not enough. I also think that the comparison to some similar works is missing.\n\n**Pros:**\n\n- The idea of learning localized representation is interesting because it allows better performance.\n- I like the idea to compute the smoothing parameter per example where the value is the proportion of the object in the image.\n- I like the analysis to quantify the context dependence. \n- The paper is easy to read.\n\n\n**Cons:**\n\nOverall, I think the technical contribution of this paper is not enough for ICLR. The method section is very small and is based on the label smoothing mechanism. The main novelty is that the smoothing parameter is computed per example and its value is the proportion of the object in the image.\n\nThe paper only focus on the crop-based methods but ignore thee weakly-supervised learning approaches [2, 3, 4, 5]. These papers propose models to automatically find the objects in the images and learn some localized representations. The authors should compare with this types of approaches because they tackle the same problem.\n\nThe idea of using bounding boxes to train deep models is not new. For example, [1] uses bounding boxes. Using bounding boxes allows to learn better representation but these annotations are costly to get. It is easier to collect image classification labels than bounding boxes annotations. It is challenging to build a dataset of 1M images annotated with bounding boxes so it limits the potential impact of the proposed approach.\n\nOverall, I like the analysis to quantify the context dependence. I just wonder how the mean pixel values and the shape of the box contain some information. For example, is it possible to predict the class of an object by using the shape of the bounding box. I think it can be interesting to verify it.\n\nIt is difficult to read the tables 1 and 2. The authors should improve the presentation of these tables.\n\nThe authors should comment if it is possible to generalize this approach to a multi-label dataset like MS COCO or OpenImages. Natural images are usually multi-labels because the world is a composition of objects.\n\nIn section 4.3, the authors wrote they trained models on OpenImages but they did not present the results.\n\n[1] Oquab ., Bottou ., Laptev ., Sivic . Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In CVPR, 2014.\n[2] Oquab ., Bottou ., Laptev ., Sivic . Is Object Localization for Free? - Weakly-Supervised Learning With Convolutional Neural Networks. In CVPR, 2015.\n[3] Sun C., Paluri M., Collobert R., Nevatia R., Bourdev L. ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks. In CVPR, 2016.\n[4] Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A. Learning Deep Features for Discriminative Localization. In CVPR, 2016.\n[5] Durand ., Mordan ., Thome ., Cord . WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea but the experiments are not meeting the expectations",
            "review": "**Summary**\nMany modern classifiers are trained with class labels - it's natural because you train the model with precisely the target label you want the model to produce! But let's think out of the box and introduce stronger forms of supervision - e.g. bounding boxes for the object of interest. What benefits will they bring? The paper is arguing that the added strong supervision (boxes on top of ImageNet dataset) helps image classifiers reduce their over-confidence. This is done by training the classifier with the **adaptive label smoothing**, where each CNN input is assessed according to the actual proportion of the foreground object and the target vector is label smoothed to reflect the proportion.\n\n**Pros**\nThe paper is one of those papers which dig into the overlooked bits in the widespread practice. Given that the point is valid and solidly tested under extensive experiments, the paper is likely to leave a strong mark in the research community, providing researchers and practitioners chances to re-think about their habits and hidden assumptions. This paper has great potential to guide the researcher in such a direction. I believe we can also learn from this paper potentials to use strong supervision for improving not only the main task performance (e.g. classification) but also guide \"how\" the models should achieve the recognition - e.g. by improving the input attribution (explainability), robustness (adversarial and natural), bias (e.g. texture or bg bias), and uncertainty (as done in this paper).\n\n**Cons**\nIt is a little unfortunate that the paper falls short in terms of presentation and experimental depth. It is not fully convincing yet that the proposed solution is working, given the width and depth of experiments. I would highly suggest the authors re-structure the paper and add substantially more experimental validations for the next paper. Below are more specific comments on the weaknesses.\n\n1. **The proposed adaptive label smoothing actually worsens many of the key performance metrics.**\nIn Table 1, we observe that compared to \"hard label\" (0.669 top-1 accuracy), \"A. L. S.\" (or adaptive label smoothing) has only 0.655 top-1 accuracy on ImageNet validation set. It is a bit difficult to swallow the result, given that ALS is **requiring more annotation budgets** (bounding boxes) to prepare the training set. It is not only the main task where ALS is falling short. Also on calibration metrics (ECE and MCE), ALS is doing **worse** than the baseline, with higher ECE and MCE measures (lower is better). The authors argue that ALS successfully decreases the overconfidence, which is true indeed, but this is only half of the story! By decreasing the overall confidence, ALS turns out to further decrease confidence in correct prediction (underconfidence scores). Table 1 seems to suggest that ALS is actually not working.\n\n2. **Given the breadth and ramifications of the claim (see pros above), the set of experiments seems limited (only ImageNet + ResNet50 + COCO transfer learning).**\nGiven that the issue 1 above is resolved, I would still suggest doing more experiments on more datasets (e.g. OpenImages?) and more architectures (e.g. Other ResNet or EfficientNet families) to ensure that ALS is actually working, independent of the specific dataset-architecture pair.\n\n3. **Structure the paper better.**\nThe paper spends multiple paragraphs repeating points that are already made (sections 1,2) and spends so little on important implementation details and evaluation setups (section 4). There are seven paragraphs in section 1 - please aim to make it four paragraphs. This will include moving the experimental analysis on the confidence scores to section 4 (introduction is not a good place to already talk about numbers!). Try to shorten the six paragraphs in section 2 into three paragraphs. \nThe paper will now be only 6.5-7 pages long. With the remaining pages, describe the following in greater details:\n- The different splits of ImageNet-1K training set: please give them names & use the designated names in Tables 1&2.\n- Write down the precise implementation details and the definitions of evaluation metrics in 4.2 (e.g. ECE, MCE, under/overconfidence) instead of referring readers to the previous papers. Please indicate whether higher or lower values are better (also in the tables).\n- When discussing the results, please talk about **all** the results for all evaluation metrics, instead of just overconfidence, as done in section 4.2. Please also talk with numbers, instead of just saying e.g. \"these results have a low overconfidence score\".\n\n4. **Nits**\n* When I checked last time, ImageNet training set had **42%** images annotated with bounding boxes. Is **38%** (section 4.1) correct?\n* (Lin et al., 2014) --> MS COCO (Lin et al., 2014) \n* described in -2 --> described in table 2\n* allows the CNNs to cheat: I find it difficult to agree with that statement. There are many papers saying the use of context has helped a lot in their application scenarios. Please tone it down.\n\n5. **Final idea to throw**\nDid you consider using unsupervised objectness methods - like Edgebox, Selective search, MCG object proposals? Or saliency detection methods (https://paperswithcode.com/task/salient-object-detection)? Being able to use them will replace your need for bounding box supervision.\n\n**Key reasons for the rating**\n\nWhile the paper has and triggers many great ideas for researchers in this field, the experimental results are not supporting the main idea and claims. The presentation is quite a bit of an issue too. My suggestion is to substantially improve and scale-up the experiments for the next conference.\n\n**Response to the last comments made by the authors**\nThe referenced paper (https://arxiv.org/pdf/1906.04933.pdf ) is not answering the question I have. Nor does section 4.2.\n\nMy question is: does ALS really improve the quality of uncertainty?\n\nThe paper's answer seems to be no.\n\nLook at Tables 1, 2, 3, and 4 in the revised paper. It is great that ALS achieves a lower O.conf (overconfidence), meaning that for wrong predictions, ALS helps to produce lower confidence values. However, this comes at the cost of higher U.conf (underconfidence), meaning that even for correct predictions, ALS makes the model produce low confidence values. A confidence measure that always produces a low value, regardless of whether the prediction was correct or not, is not useful.\n\nThe authors may say \"look at Table 1 - our uncertainty measure produces lower scores for images with objects removed\". While this is true and it is a good signal, object removal is only a particular case for introducing uncertainty in an image. Eventually, I believe a good uncertainty measure should first of all produce a good ranking of test images such that the correctly predicted images are ranked first (this paper does not quantify this). Then comes the question of calibration (like ECE and MCE). Then comes the evaluations with specific uncertainty scenarios (like OOD images, object removal, or other controls on image uncertainty).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}