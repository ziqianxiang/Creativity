{
    "Decision": "",
    "Reviews": [
        {
            "title": "A study of how long-range links change gradient propagation in dense networks.",
            "review": "The paper investigates the effect of some topological properties (more precisely, long range links) on the gradient flow in a dense network. The main contribution that can be drawn from the experimental results in Section 4 is that the so-called mass $m$ of a dense network of $N_c$ cells each of width $w_c$ and depth $d_c$, containing each an additional set of $l_c$ (random) long-range connections between non-adjacent layers, which can be expressed as \n$$ m = 2 \\sum_{c=1}^{N_c} \\frac{d_c l_c}{(d_c-1)(d_c-2)w_c},$$ \nis a better indicator of (has a higher correlation with) the test loss than the sheer number of parameters of the model. \n\nThe authors also claim to address the ability of topological properties of DNNs to indicate without training the models that achieve a similar accuracy, despite different sizes, but that would then lead to improved NAS solutions that could be compared to the existing state of the art, which the authors next disclaim to be a goal of the paper. \n\nInstead, the authors claim to establish a theoretical link between the mass $m$ and gradient flow characteristics, with a formal proof in terms of Propositions 1 and 2 in Section 3.3. The definitions are however sloppy. For example, Definition 4 is supposed to formally define the mass $m$, yet it only states that the mass \"quantifies\" how effectively information can flow through a given DNN topology (which does not define precisely $m$) and that \"for a given width, models with similar mass [...] *should* exhibit a similar gradient flow and, thus, achieve a similar accuracy\" (which is a wish, not a definition). \n\nThe proofs are unclear and also possibly flawed.  In the proof of Proposition 1, the definition \n\n$$ \\overline{k}_{R|G} = \\frac{w_c \\sum_{i=2}^{d_c-1} \\min \\{w_c(i-1),t_c\\}}{w_cd_c} $$ \n\ndoes not lead to\n\n$$ \\overline{k}_{R|G} = \\frac{m(d_c-1)(d_c-2)}{2d^2_c} $$\n\nwith the formula given after Definition 4, where $m$ is defined as a sum over all $N_c$ cells, whereas $\\overline{k}_{R|G}$ seems to be a cell-dependent quantity independent of $N_c$. The \"intuition\" that given $m$, the average degree is independent of $d_c$ needs to be carefully assessed. \n\nThe statement of Prop. 2 is not precise: it simply states that the expected singular values of the deep and shallow models are \"similar\" (how similar?) if they have the same mass $m$ and width $w_c$. More importantly, very strong assumptions are made in the proof that are not made in the statement of the theorem (the matrix $M$ in Appendix D appears to be the same as the Jacobian matrix $J_{i,i-1}$ but assumes that all its entries are i.i.d zero-mean Gaussians, the proof includes even a simulation of a \"more realistic scenario that will happen in the case of initial layerwise Jacobian matrices $J_{i,i-1}$\"...). Some steps in the proof are not needed (why do you compute the pdf of $z_k$ in (9) since you only need to show that $E[z_k]=0$, which is obvious since $z_k$ is the product of two zero-mean independent Gaussian random variables.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Work",
            "review": "Summary:\n\nThe authors propose a new metric, NN-Mass, to describe the long-range links in neural networks. Through extensive experiments, they identify the better prediction of model accuracy by looking at the NN-Mass rather than #parameters, #FLOPS, or #layers.\n\n\nStrength:\n\n--The paper is presented well and quite easy to follow.\n\n--The overall idea makes sense and the experiments are convincing.\n\n\nWeakness:\n\n--The assumptions might be unclear in proof.\n\n--Most experiments are based on synthesized models.\n\n\nComments:\n\n--I like the idea that topology matters more. However, the definitions in Eq. (1)-(2) rely on a simple base topology (width-wc and depth-dc), thus the general forms in practical models (with variable wc across layers) are unclear. This is not skepticism, instead, I believe the conclusions of this work are sound and just hope the authors can generalize the theory part.\n\n--From Eq. (2), it seems that the NN-Mass is more tightly associated with tc rather than the depth. Does the conclusion, that NN-Mass positively reflects the model accuracy, mean a larger tc can yield higher accuracy (if with the same wc)? And does it mean more long-range links can produce higher accuracy? In my opinion the practical situations would be much more complicated and there should exist a saturation region.\n\n--Most experiments use synthesized models. If the authors can add extra experiments to explain the different performance of some existing typical neural network models by analyzing the NN-Mass (also wc), the quality and impact of this work will be significantly improved. Although the authors have mentioned this in Section 4.3, I still expect more data.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas and results, but more evidence is needed to draw conclusions",
            "review": "Summary:\n\nThe authors propose a metric, NM-Mass, that is based on the topology of a neural network architecture and attempts to capture how efficiently information is able to flow through the architecture. The authors explore whether their metric is a better predictor of test accuracy than other network properties like parameters.\n\nPros:\n\nThe authors’ direction of exploring the impact of information flow on test accuracy is interesting and the grounding in network science and relation to existing metrics like LDI is interesting and appears to be promising.\n\nCons:\n\nSome of the writing and explanation of the NM-Mass metric is unclear. In Section 3.3, the proof is hard to follow and it’s not clear that it’s a proof at all. The authors discuss the similarity of the mean singular values of two networks with the same width and NM-Mass but different depths. However, the notion of similarity is never formally defined. The purpose of this section is to establish a connection between a model’s topology (i.e., the NM-Mass metric) and LDI, but the reasoning is not clear. Section 4.2 seems to support the fact that this isn’t a proof: there is correlation between LDI and NM-Mass for fixed width models but they are not the same. Possibly as a result of this lack of clarity, it’s not clear why the correlation between the mean-singular values of the model and NM-Mass is desirable. LDI states that the singular values of the Jacobian being close to 1 is desirable for stable gradient propagation throughout the network. If NM-Mass is correlated with the mean singular values of the Jacobian, wouldn’t this imply that there is a sweet-spot for NM-Mass that corresponds roughly to an LDI of 1?\n\nAdditionally, the experiments in this paper are not sufficient to back up the author’s claims. In section 4.2, the correlations are interesting but this is a very limited set of models on a simple dataset. The single experiment in Figure 3 does not demonstrate that the authors’ hypothesis is true. Significantly more evidence is needed. In section 4.3, the accuracy of all models in Figure 4 is so close (range of ~0.7%) that it’s hard to tell how significant these trends are. There is still a significant range of NM-Mass’ that correspond to a given test accuracy in the CIFAR-10 results. The ImageNet results do look more clustered, although it’s not clear exactly how variations in parameters should be compared to variations in NM-Mass. For example, models W & X are roughly 15% different in parameter count and roughly 5-10% different in NM-Mass. Reporting results in terms of relative changes in parameters/nm-mass would likely make these results much more clear.\n\nMain results referenced in the text should not be in the Appendix (figures 17-19; design of CNNs using NM-Mass).\n\nImprovements with more experiments (e.g., a wider class of models, wider range of accuracy levels and parameter counts, more diverse tasks than computer vision tasks) that show a clearer trend would go a long way to improving this paper. One study to consider would be to calculate NM-Mass for popular DNN architectures (AlexNet => VGG => GoogLeNet => ResNet => MobileNet => EfficientNet) and show that progress in test accuracy has corresponding improvements in NM-Mass.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}