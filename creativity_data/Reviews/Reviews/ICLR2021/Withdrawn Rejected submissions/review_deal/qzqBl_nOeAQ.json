{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting work towards differential NAS",
            "review": "The authors introduced EnTranNAS and EnTranNAS-DST, tailored for the handling gap architectures search and evaluation. Results on large-scale datasets such as ImageNet confirm the validity.\n\nStrength:\n1. The overall idea is quite interesting. By introducing some parameters to support differentiable sparsification, the architecture search turns out equivalent to that by choosing the non-zero connections. \n2. Experimental results are promising. The proposed EnTranNAS-DST showcases good results, proving the effectiveness of the differentiable search. Moreover, searching with additional operations doesn't seem to scale up the cost.\n3. The submission is generally well organized and easy to follow.\n\nWeakness:\n1. Is there any quantitative analysis of the reduced gap? It is observed that the accuracy drop is decreased in Tab.1, yet still, it doesn't necessarily mean that the architecture gap is reduced. An apparent example is that, in the first configuration, the performance drop is smaller than that of the last one, but the architecture gap between the two appears to be the same. Try to use some other metrics for evaluations.\n\n2. It would be great if more theoretical analysis, at least some formulations, are provided as the foundation of the design. The current design appears somehow ad-hoc. \n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2 ",
            "review": "The paper proposes EnTranNAS to reduce the gap between the search and evaluation in cell-based NAS. Specifically, it builds the super-net with engine-cells and transit-cells. Engine-cell keeps the whole DAG (same as the cell in DARTS) while transit cell only keeps the currently active sub-graph. A feature sharing technique is proposed to alleviate uneven optimization on candidate operations. It also introduces an architecture derivation method to further reduce the gap between engine-cells and derived-cells. \n\nPros:\n1. The idea of building the super-net with two different types of cells (engine-cell that keeps the full DAG, transit-cell that only keeps the active sub-graph) is interesting and novel for cell-based NAS. \n\n2. The proposed method achieves good results on CIFAR-10 (similar or higher accuracy while having lower search cost). \n\t\nCons:\n1. Experiment results on ImageNet are a bit confusing. EnTranNAS-DST gives higher accuracy but also has higher #FLOPs than the baseline models, making the comparisons confusing. I suggest adding a figure showing the trade-off curve (accuracy-FLOPs) of the searched neural network architecture by adjusting the width multiplier or input resolution. \n\n2. Table 2 only shows the efficiency effect of each component. It would be interesting to add accuracy results to show the accuracy effect of each component in Table 2.  \n\nOverall, I think the proposed method is novel for cell-based NAS. But I also find the empirical results are a bit weak and confusing. Therefore, I recommend \"Marginally above acceptance threshold\". ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Motivation behind the approach is good, but the novelty part is not that significant",
            "review": "This paper proposes a method for reducing the gap between the search and final evaluation phases in differentiable architecture search.\n\nContributions:\n- This paper introduces a modification to the cell-based architecture search via gradient-based methods such as DARTS [1] by modifying the search space in order to enable sparse architectural weighs and therefore more efficient computation and lower memory consumption during the search.\n- The results on the standard CIFAR-10 and ImageNet benchmarks are competitive.\n\nDetailed comments:\nDespite the motivation behind the proposed approach is plausible, I think the contributions of this paper are incremental considering the myriad of follow-up papers of DARTS [1], some of them also considering to tackle the same issue as in this paper [2, 3]. The proposed approach seems useful from an engineering perspective, since it results in lower search runtimes and memory costs, however the methodological contributions are not enough. Moreover, there are some research questions which are not answered throughout the paper like these one: \n- How is the performance of the final architecture retrained from scratch correlated with the one-shot performance or the perfomance of the same architecture evaluated with the one-shot weights?\n- How sensitive is the search towards search hyperparamters such as the softmax temperature (how did you tune it in the first place?) or the \\lambda in equation (11)?\n\nConsidering the aforementioned concerns, I am leaning towards a reject.\n\nOther:\nThe related work section is well-written, however some important references are missing such as the works that study the performance gap between search and final evaluation [4, 5] or the DNW paper [6].\n\n[1] Hanxiao Liu,  Karen Simonyan,  and Yiming Yang.   Darts:  Differentiable architecture search.   In ICLR 2019\n[2] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridgingthe depth gap between search and evaluation. In ICCV 2020\n[3] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, Cho-Jui Hsieh. DrNAS: Dirichlet Neural Architecture Search. In ArXiv 2020\n[4] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.  Understanding and robustifying differentiable architecture search. In ICLR 2020\n[5] Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-based regularization. In ICML 2020\n[6] Mitchell Wortsman, Ali Farhadi, Mohammad Rastegari. Discovering Neural Wirings. In NeurIPS 2019",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #3",
            "review": "**Summary:**\nThis paper an EnTranNAS method to reduce the gap between the architectures in search and evaluation. To accelerate the search process, the authors further propose a feature sharing strategy. This paper is well-written. However, the importance of the considered problem since it may not necessarily appear in all NAS methods.\n\n**Strengths:**\n1. The authors propose a new method to reduce the gap between the architectures in search and evaluation for gradient-based NAS methods.\n2. The authors propose a new search space that consists of Engine-cells and Transit-cells.\n\n**Weaknesses:**\n1. The results on ImageNet are not very promising. Most NAS methods follow the mobile setting where the number of multiply-add operations (MAdds) in the model is restricted to be less than 600M on ImageNet. However, the best searched architecture has much more MAdds than the constraint. More critically, the searched architecture that satisfies this constraint yields lower accuracy than existing methods, e.g., PCDARTS. \n2. The scope of the considered problem is very limited. The gap of the architectures between search and evaluation only appears in gradient-based methods, e.g., DARTS. As for those reinforcement learning based methods, e.g., ENAS, both the search and evaluation phases train a single sub-graph. Thus, there seems no gap of the architectures between search and evaluation for these methods.\n3. The authors argue that “feature sharing strategy is introduced for more efficient parameter training”. However, it is not clear why the feature sharing strategy is able to accelerate the search process. More discussions should be provided.\n4. The proposed method uses a different search space from DARTS. Thus, it is hard to judge whether the performance improvement comes from the proposed search method or the changed search space. Ablation studies should be provided.\n5. The assumption “the same operation from node i to other nodes j > i always shares the same feature in one cell” is questionable, since the parameters of these operations may be different. Are there any experiments to verify this argument? More discussions should be provided.\n6. It is not clear why the proposed method has to perform normalization using Eqn. (7). The motivation should be clarified.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}