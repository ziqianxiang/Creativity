{
    "Decision": "",
    "Reviews": [
        {
            "title": "Result is good but lacks explanation, analysis, and ablation study",
            "review": "Summary:\n\nThis paper proposes a lightweight generative adversarial network that captures long-range dependencies in both spatial-wise and channel-wise directions. Moreover, the authors introduce metadata (a feature vector extracted from real images by VGG network) into the image generation process to provide additional information for the generator and stabilize training.  Experiments are conducted on CUB, FFHQ, and ImageNet Church datasets. The lightweight model performs well in terms of image quality, inference speed, and the number of parameters.\n\nPros:\n\n1. The proposed model is lightweight. It is faster and uses less number of parameters than other GAN models.\n\n2. The quality of generated images shown in this paper looks good.\n\nCons:\n\n1. I think the authors did not explain clearly how does the long-range module capture the long-range correlations. The authors claim that $\\alpha_{i,j}$ in equation(2) denotes the extent of correlation between spatial locations $i$ and $j$. However, the feature $n$ is derived from applying 3x3 convolution over the input feature map (and then reshape). The channel dimension of $n$ is $HW$, but this does not mean that the $k$th channel of $n$ represents the $k$th pixel in the $H*W$ feature map. The features of each spatial location in $n$ is still not having a global receptive field. How does it come that $\\alpha_{i,j}$ denotes the correlation between spatial locations $i$ and $j$? In addition, the authors should provide some visualization of the correlation weights and give readers an intuition of what kind of long-range dependency the long-range module learns.\n\n2. Since convolution layers already capture the long-range dependency between channels, does the proposed long-range module in channel-wise direction improve performance? There should be an ablation study on how much improvement gain is brought from long-range module in spatial direction, and how much gain is brought from long-range module in channel-wise direction.\n\n3. What's the motivation of introducing the scaling weight $w$ and $c$? And what's the performance of the long-range module without $w$? Is $w$ randomly sampled from a Gaussian distribution during training and inference, or a learnable network parameter that is initialized by Gaussian distribution and fixed during inference?\n\n4. What's the role of the metadata in generation? From the generated images and their meta-images in Figure 3, I did not find the common attributes or styles between meta-image and generated images, and did not find the differences between images generated from different meta-images. So I doubt if the model is actually using the information from the meta-image from generation. If the meta-image does affect generation, I would expect that the images generated from the same meta-image share some common attributes or styles. Perhaps the authors can also analyze the differences between generated images when using different meta-images and the same input noise $z$.\n\nMinor problems:\n\n1. In Page 4, Section 3.2, the last line of subsection \"Long-range module in spatial direction\", \"element-wise multiplication between $h$ and $w_n$\" should be \"matrix multiplication between $h$ and $w_n$\"? And in the same paragraph, \"matrtix multiplication between $\\alpha$ and $w^\\prime$\" should be \"hadamard/element-wise multiplication between $\\alpha$ and $w^\\prime$\"? (According to the notations in Figure 2.)\n\nReasons for rating:\n\nThe quality of the generated images look good, and the model is lightweight. But I have some doubts about the method. I think some motivation and architecture designs are not explained clearly. More analysis and ablation studies are also needed to justify the effectiveness of each component. I expect the authors to explain my concerns in the rebuttal.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Albeit the idea of long-range module is interesting, it actually is similar to existing non-local modules. ",
            "review": "The paper presented a long-range module for capturing long-range dependencies across spatial positions and channels. A new generation strategy is further suggested to stabilize the training process. However, I do not think this work meet the requirements of ICLR:\n\n1. Albeit the idea of long-range module is interesting, it actually is similar to existing non-local modules, especially [r1]\n[r1] Dual attention network for scene segmentation, CVPR 2019.\n\n2. The proposed generation strategy may be beneficial to training, but is likely to restrict the application range of the learned generator. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Technically sound but exposition needs improvement",
            "review": "The introduced method describes a lightweight GAN architecture based on CNNs that focuses on exploiting long-range receptive fields for image generation. In particular, the method proposes to use adjustable and augmented sampling locations for the receptive fields, that allow the feature extraction to take place over large spatial regions in the image. \n\nOverall, I think this is an interesting paper that has most things going for it. The method seems novel, the results are promising, and the method is evaluated well. Please see strengths and weaknesses, as well as additional comments below. \n\nStrengths: \n\n- The idea to use free-from spatial sampling for the receptive field seems novel and interesting. \n- The method uses an image generation strategy based on image meta data to improve the training process, which seems to be effective. \n- The method focuses on establishing a lightweight architecture that has the potential to be used on mobile devices. \n- The approach is mostly described well and the evaluation seems technically sound. \n\nWeaknesses: \n\n- The exposition of the paper could be improved: the title should be more specific to what the method is actually about; the abstract appears too convoluted: it is not clear what the paper is actually about. \n- The paper does not provide a discussion of the limitations. As is, it is not clear on which data or in which situations the method fails. I would encourage the authors to discuss corner cases and limitations. \n\nAdditional Comments: \n\n- As mentioned above, the title seems off and should be changed. The method is clearly aims to improve image-based GAN architectures instead of general GAN architectures. This should be reflected in the title. \n- Referring to 'geometric structure' is somewhat confusing. Although not wrong, I would suggest to rather use the term 'spatial structure'. \n- As stated above, the abstract does not clearly describe what the method is aiming to accomplish. E.g. it is not clear what is meant with 'focused sampling pixels' and 'negative relations'. \n- In Section 3 what is meant by 'friendly enough'?\n- n and F are not defined in Eq (1). \n- In Section 3.2. 'Why does the long-range module work better': please use more appropriate scientific language for the sentence containing 'the truth is'. \n- What is E in Eq (4)?\n- The caption of Figure 3 does not provide enough details to understand what images are shown. \n- Section 4.1.: '... can complete filter fine details' --> fix. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting business with metadata and non-local relations",
            "review": "Overall this paper makes an interesting read and the authors put forward two improvements: the use of metadata and an improved attention method. These are then tied to mobile devices to justify why smaller networks are important. This means there are no less than three ideas whose worth should be proven in 8 pages. The authors furthermore, possibly by accident, claim improvements in the limited data regime, which of course would be valuable on its own right. The key question is whether the ideas are sufficiently proven, and in my opinion there is a certain lack of clarity and depth in this regard. \n\nLet’s consider metadata first. The spatially averaged relu5_3 features from VGG16 are added to the latent code (the addition symbol failed to reproduce in my print, causing some headscratching), which implicitly specifies the latent space dimension as 512, I guess. Unfortunately the paper fails to discuss how the metadata is used in training. Do you perhaps pick a random real image as metadata per generated image, per minibatch, or something else? How does this relate to “categories or basic textures of objects” (Sec 3.3)? Through what mechanism does this speed up convergence and prevent overfitting? This isn’t discussed at all, but if the finding is true, it’s very interesting. It does make the latent components unequal, thus possibly leading to kind of pre-assigned roles for some components, but how this would cause the observed benefits is unclear to me. Did you sweep parameters separately for “w/o meta”? If the effect is indeed real, this topic alone would warrant a full a paper! It would add a lot of confidence if this benefit materialized using some well-known SOTA GAN as well.\n\nWhen we look at Table 1, the absolute results for e.g. FFHQ are not great. StyleGAN2’s FID-50k is 3.71** for FFHQ 256x256 and around 8.0 when the capacity is reduced by 3/4th. Your results are far from that, which is a bit of a concern. I do appreciate that training time is an issue. But if the key argument is that for a given cost (#params, inference speed) your design is better than SOTA, then I don’t think the paper manages to prove this. You could have taken e.g. StyleGAN2 and reduced the capacity in some sensible way and plotted the FIDs — this could have been a convincing baseline in the perf@capacity discussion. Also, in my personal opinion your arch with multiple discriminators and aux loss terms is hardly simpler than StyleGAN. While that’s of course just one opinion, I would nevertheless tone down the claims about removing complexity.\n\nYou claim that taking the negative correlations/relations into account is important. I suppose that might be, but for me the LRM —> SA comparison doesn’t feel sufficiently convincing. I would have wanted to see much more detailed comparisons and visualizations about the differences in the found relations. Did you separately search the optimal parameters for SA (or Meta —> Residual)? Because if not, we may be seeing a random effect, not the real one. In any case it’s important to specify such details in the paper/supplement. \n \nYou go on record saying that “ImageNet [is] too small to train a heavy model” (Sec 4.1)! This unimaginable statement might seem to imply that you train a model using _only_ the 2k church images. Do you really do that, or train the model using all of the 1000 categories?\n\nI’m somewhat undecided about this paper and willing to revise my recommendation based on the author response, and perhaps insights from other reviews as well. \n\n\nMinor:\n- How do you compute FID? 10k? 50k?\n- Sec 3.2 descriptions have matrix multiply and Hadamard in the opposite order compared to Fig 2b.\n- Sec 3.3 Meatdata\n\n**) https://arxiv.org/abs/2006.06676\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments are not convincing and the method does not seem to be well-motivated",
            "review": "This paper proposes a module to model long-range dependencies for images and applies it to image synthesis. The paper claims that the proposed module is more lightweight parameter-efficient than common long-range modules such as self-attention.\n\nPros:\n+ The proposed model is indeed very efficient parameter-wise and time-wise, compared with the popular approaches. The image quality is reasonable considering the small model size.\n\nMajor concerns:\n- I do not quite understand the intuition behind the spatial long-range module. This module uses a 3x3 convolution to produce a feature map with H*W channels. Each feature vector in the feature map encodes the contribution of the location of the feature vector to all other locations. The computation, however, only depends on a very limited (3x3) context. So the network basically has to predict the relationship between each location and other locations, **without seeing other locations at all**. I do not understand how it would be possible for the proposed module to learn \"long-range\" relationship. I also have a problem understanding the channel long-range module. For example, how do you produce a C ×C ×1 tensor from an (H * W ) × C × 1 tensor with 3x3 convolution?\n- The experimental results are not convincing enough. The paper claims to compare the proposed method with \"two state-of-the-art approaches PGGAN (Karras et al., 2017) and SAGAN (Zhang et al., 2019)\". First, these two methods are far from \"state-of-the-art\". The state-of-the-art methods the author should compare are StyleGAN2 for unconditional generation (e.g., FFHQ) and BigGAN for conditional generation (e.g., ImageNet). Authors justify this by saying that \"the purpose of our method is to achieve a lightweight architecture with fewer parameters\" and the approaches mentioned above are large. This is not very convincing to me. I do not see any incompatibility between the long-range module, which is the main novelty here, and large state-of-the-art models like StyleGAN2/BigGAN. I think the authors should try to add LRM to StyleGAN2/BigGAN or replace self-attention in BigGAN with LRM and report if there is a performance improvement. Second, this paper reports 18.41 FID for PG-GAN, while the StyleGAN paper reports 8.04 for PG-GAN (Table 1 of [1]). Is it all due to different resolutions (256 vs 1024)? Can you compare with PG-GAN/SAGAN on a dataset where they have officially reported results?\n- The paper is a bit hard to follow and has occasional grammar mistakes.\n\nMinor problems that do not affect my score:\n- For examples -> For example\n- meatdata -> metadata\n\n[1] Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}