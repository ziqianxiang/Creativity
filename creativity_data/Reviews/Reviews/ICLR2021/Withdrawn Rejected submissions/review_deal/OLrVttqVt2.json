{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper establishes an interesting relationship between poisoning and online learning. Instead of framing the poisoning problem as a bi-level optimization problem as what is done conventionally, the paper proposes reducing the poisoning attack design to an online learning problem in which the adversary decides on a poisoning data point in a sequential manner.  The data point that leads to the largest difference of loss between the current model and the target model is selected as the next point to inject into the training data.\n\nPros\n+ If the loss function is convex, the proposed algorithm is guaranteed to converge to the target, and the paper provides a lower bound on the number of samples needed to reach the target model from the current model. \n\n+ Experiments on SVMs show the advantageous performance of the proposed attack algorithm. \n\n\nCons\n- The reason why the proposed online learning method could outperform the KKT-based attack is not well justified theoretically. Experimentally, the paper would be stronger if evaluations are done on more diverse settings and data.  \n\n- Reviewers have expressed concerns on how practical the proposed algorithm is, since the guarantees are established on convex loss functions. The paper would be stronger if  the authors can further compare the attack with deep-learning poisoning algorithms on larger datasets.\n\n\nI truly believe that the paperâ€™s exploration of convex models is an important step towards understanding the poisoning attacks and is a step towards understanding attacks for non-convex models. However, I do think that the paper could make a more profound contribution and impact with stronger experimental evaluations. \n\nTherefore, I would classify this paper as borderline, toward weak reject compared with other papers. \n"
    },
    "Reviews": [
        {
            "title": "Interesting problem but weak evaluation",
            "review": "This paper presents an improvement on an interesting problem: poison a dataset to induce a machine learning process to a misleading model. This field of study has been trending in recent years, and this paper presents a neat increment over previous works. The basic idea is that the attacker iteratively generates new data points so as to minimize the difference between the poisoned model and the target model. This approach requires retraining the model in each iteration, which is a costly procedure. This work then propose to use an online learning approach to update the model with a small cost to accelerate the entire process, and thus provide an efficient data poisoning approach.\n\nThere are several issues with both the work itself and the presentation. The main issue is about the evaluation. The approach is only evaluated on one dataset (Adult dataset), considers only one model (SVM) and compared against one approach (KKT attack). This is not enough to justify the universal effectiveness of the approach applying to other machine learning algorithms. In particular, modern deep learning algorithms typically suffer catastrophic forgetting issues when applying online learning algorithms, and these models are of more interests in the context of poisoning attacks. So this work does not provide enough evidence to justify that the proposed approach is effective in dealing with them.\n\nMaybe I neglect them, but I do not find the description of the adult dataset's stats, i.e., how large is the dataset? The paper studies poisoning more 1500 data points, what percentage is these data to the entire dataset? It looks like, from the description, that these dataset might be 100% of the entire dataset. Isn't this too large to be feasible in the real setup?\n\nIn addition, it might be important to compare against Jagielski's 2018 paper [1], which is not cited, as well in the non-subpopulation setup, in addition to KKT, which should provide a better baseline. This work also presents a set of commonly used dataset and machine learning models, which can be experimented with in the context of this work.\n\nPart of the above is presentation issues, and there are more issues in the presentation of the theoretical part. For example, the presentation of Thm 4.1 uses the term of epsilon-close, but I don't find its formal definition above (like in Sec 2). Thm 4.2 uses a notion of c(theta) without a definition of what it is. It seems c indicates a clean dataset (when in subscription), but I find it difficult to read it when there is an argument theta associated with it. These issues make me hard to understand what the main theorems of this work want to tell us.\n\nI think the paper should be revised to fix the presentation issues first so that we can evaluate more accurately whether the theoretical results have enough merit to upgrade my rating; also it may be helpful if more experiments can be conducted to demonstrate that the approach is effective on more datasets and more machine learning models.\n\n[1] Matthew Jagielski, et al. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning, SP 2018\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An online algorithm for targeted poisoning, with theoretical guarantees",
            "review": "**Paper summary**\nThe paper proposes an algorithm, that works in an online fashion, for targeted poisoning attacks. If the loss function is convex, then the algorithm is guaranteed to converge to the target as the number of poisoned samples increases. The paper claims that this is the first model-targeted attack which has theoretical guarantees. The lower bound provided is interesting in the sense that it can give a lower bound on the number of samples needed to reach the target model from the current model.\n\n**Strengths**\n1. The paper proposes a new algorithm that works by progressively adding poisoned points to the dataset. Theoretical guarantee (Theorem 4.1) is also provided for the algorithm. The proof idea is well explained and I liked the connection with online learning and how the algorithm is reduced to follow-the-leader algorithm. \n2. The algorithm works by progressively adding more points and hence it can stop as soon as it is close to the target. This means that it does not need a predecided budget and if it is indeed optimal, then it would use the minimum number of poisoned points to reach the target.\n3. On SVMs, the experiments show that the attack is almost optimal in the sense that it matches the theoretical lower bound.\n\n**Concerns**\n1. Theorem 4.2 is a lower bound for the minimum number of samples needed to reach the target model exactly. Can we say something about reaching the target model approximately (say up to distance $\\epsilon$)? I think this can be achieved by the following optimization in Theorem 4.2: $$\\inf_{\\theta':\\|\\theta'-\\theta_p\\|\\leq\\epsilon} \\sup_\\theta c(\\theta,\\theta')=\\frac{L(\\theta';\\mathcal{D}_c)-L(\\theta;\\mathcal{D}_c)+NC_R(R(\\theta_p)-R(\\theta))}{\\sup(l(\\theta;x,y)-l(\\theta';x,y))+C_R(R(\\theta)-R(\\theta')))}.$$\n\nFurther, how much would the lower bound decrease (as a function of $\\epsilon$) if we are indeed interested in only reaching the target model approximately?\n\n2. In the experiments, the lower bound is computed for the number of samples needed to reach the model induced after poisoning. This can be different from the target model. Hence, shouldn't the lower bound be computed for the number of samples needed to reach the target model? I think that would be the true lower bound for poisoning. However, I believe that the two numbers should be close.\n\n3. The experiments are performed only for linear SVMs. It would be more interesting to see if the attack also works well on deep neural networks. This is also important because the paper claims that the existing algorithms get stuck in bad local minima. Thus, it should be checked if this algorithm also gets stuck in bad local minima.\n\n**Score justification**\n\nModel poisoning is a very real threat in modern machine learning. Strong attacks provide good insights for developing strong defenses. I believe the attack proposed in this paper is strong and theoretically backed. Further, the paper also provides lower bound on the minimum amount of poisoning needed to reach the target model. However, I have some concerns regarding the attack's success on deep neural networks. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Model-Targeted Poisoning Attacks with Provable Convergence",
            "review": "The paper studies model-targeted poisoning attacks during training time. The victim learner is assumed to be a convex learner that performs regularized empirical risk minimization. Instead of formulating the problem as a bi-level optimization as standard, the paper proposes reducing the attack to an online learning problem, where the adversary decides on a poisoning data point in a sequential manner. The data point that leads to the largest difference of loss between the current model and the target model is selected as the next point to inject into the training data. In doing so, the learned model will converge to the target model in the long run. The paper provided a theoretical analysis on how many data points are needed to achieve a pre-specified small distance to the target model. Experiments demonstrated the effectiveness of the proposed attack.\n\nThe main advantage of the paper lies in that it draws the first connection between the data poisoning attack and online learning. The techniques used in the paper very novel. While the proposed attack algorithm looks simple, it actually has a deeper theoretical reason for why the attack can be effective. The authors also give a rigorous theoretical result on how many poisoning points are needed in order to drive the learnerâ€™s model toward a target model. This result is elegant!\n\nAnother strength of the paper is that the proposed online learning-based attack can even outperform the traditional KKT attacks. This is surprising to me because the KKT formulation is in fact an optimal-attack formulation. The only place that could incur sub-optimality in the KKT approach is the nonlinearity of the attack optimization, which may not solve to the global-optima solution. I am wondering if the authors could provide some intuition about why the proposed attack results in superior attack performance as compared to traditional data poisoning attacks?\n\nI am also curious if the attack in this paper extends to the deep learning setting, where the victim learners are no longer convex. Empirically it would be nice to show some results on that, although the theoretical results definitely do not easily extend. Currently, the paper only experiments on simple tasks where the dataset is kind of small. It would be more convincing to study larger datasets and verify whether the proposed attack can outperform existing baselines across a variety of tasks.\n\nOverall, I enjoy reading the paper, and I think it makes significant contributions that push forward the frontier of data poisoning attack. In particular, the online learning approach is very appealing and gives a very nice formulation of the model-targeted attack.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "ill-defined problem, unjustified approach, and insufficient evaluation",
            "review": "Most existing data poisoning attacks are objective driven, e.g., the poisoned model is inaccurate for certain inputs (called subpopulation). The authors define model-targeting attack, in which the attacker aims to poison training data such that the learnt model is a given target model. The authors define distance between the poisoned model and the target model using their loss difference (this reduces to objective driven poisoning attack). Evaluation on linear SVM is performed and comparison with one existing attack is conducted. \n\n1. The model-targeted poisoning attack is an ill-defined problem. When measuring the model-targeted poisoning attack, one should use some distance between the poisoned model and the target model, e.g., L2 distance (it's good that the evaluation considers this). The loss based metric reduces the model-targeted poisoning attack to be similar to objective-drive attack. However, when measuring success using distance between the poisoned model and the target model, the attack is obviously unsuccessful when the loss function is non-convex. Many ML loss functions are non-convex. In such non-convex cases, you cannot get the same model parameters in multiple runs of the same algorithm and training data, even if there are no poisoned data points. So the attack could only be successful for strongly convex loss functions, which have a global optimal solution. I would suggest the authors to explicitly mention that the attack is limited to such setting. Moreover, redefine the success metric to measure the attack. However, once limited to such setting, the paper's contribution is also limited. \n\n2. Insufficient evaluation. The evaluation does not compare with subpopulation attacks. I understand you study model-targeted attacks. Since the evaluation is for subpopulation attacks. It is still interesting to know the comparison results. \n\n3. Only linear SVM is evaluated. I suggest evaluating other models and datasets. \n\n4. Why is model-targeted poisoning attack relevant? Objective-driven attacks are more relevant. In fact, the evaluation is on subpopulation attack, which is an objective-driven attack. \n\n\nTypos\n\nis the setting use in many prior works -- used\n\nalso allow arbitrary selectiong of the poisoning points -- selection\n\nclassifier that that has 0% accuracy -- that",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}