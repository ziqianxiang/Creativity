{
    "Decision": "",
    "Reviews": [
        {
            "title": "How much time in search are we actually saving?",
            "review": "Summary: \n\nThis paper presents a NAS technique focused on pareto-frontier search. It presents a technique which is primarily a modification of the Once-For-All method (OFA) by Cai et al. 2020. The main modifications are: \n\n1. It uses a customized rank-loss function for the 'neural twin' regressor (termed 'evaluator' here) which in this case predicts whether an architecture will dominate other architectures wrt the pareto-curve. This enters the loss function via a hinge loss which takes in pairs of architectures (16K sampled from the trained supergraph). \n\n2. Instead of evolutionary search in OFA, this paper uses reinforce modified such that the controller network (a RNN as used in NAS via RL by Zoph et al) also takes as input the desired latency and outputs architectures which do well with respect the neural evaluator's rank loss. \n\nThe claimed main advantage of this procedure is that as opposed to other methods like OFA which do an independent search per specified latency, here they can do a single search (since the controller RNN also takes in desired latency as input) for the frontier of interest by sampling latencies in the latency range. \n\nExperiments on ImageNet are presented comparing to other methods but most importantly OFA and its variants. \n\nComments: \n\n- The paper is generally well-written. Thanks!\n\n- Here is my central concern right now: the improvement over OFA and variants seems very small. For example in Figure 4, if each of the 5 architectures on the frontier for each method are plotted with standard error (due to stochasticity in training) will the plots be within experimental error? Also can we have standard error for the final training of each architecture on the plots so we can compare fairly? \n\nOne could argue that even if the frontiers are nearly the same, the search is much more efficient in PFNAS as it is searching once and not repeatedly as in OFA. But since OFA search is evolutionary method against a simulator (neural twin/evaluator) how expensive is the search in first place? Is it a few minutes or seconds per search on a CPU? If so how much compute time are we actually saving here? \n\n- Some relevant pareto-frontier literature is missing in related work and should be cited and discussed: \n\na. Elsken, Thomas, Metzen, Jan Hendrik, and Hutter, Frank. Efficient multi-objective neural architecture\nsearch via lamarckian evolution, ICLR 2019\n\nb. Efficient Forward Architecture Search, Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee Eric Horvitz, Debadeepta Dey, Neurips 2019\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "The paper proposes a method (PFNAS) to find neural networks along a frontier which makes a good trade-off  between accuracy and computation cost. In PFNAS, a controller (a MDP based agent) samples an architecture and receives reward to update its parameters. Instead of using the weighted reward of accuracy and computation cost, the authors build a reward evaluator/generator to produce an auxiliary reward for the controller. The auxiliary reward evaluator is pre-trained by appropriately ranking models along the trade-off frontier. The controller is not new. The new contribution is the reward evaluator, which encodes pair-wise model comparison along the frontier although it only takes a single model as its input.\n\nThe paper is generally well written. However, there are following issues in the paper:\n1. The paper claims its difference from previous methods is that PFNAS can “find multiple promising architectures to fulfill all considered constraints in the same search process”:\n\n  1.1. Predictor-based NAS [1][2] with random sampling can also achieve this goal. The authors seem to overclaim this part and should discuss the difference from the previous works;\n\n  1.2. OFA (Once-for-all) also claims a similar contribution. The difference should be discussed. The OFA is more efficient when the hardware of deployment changes. When the hardware changes, OFA only need to build a new computation cost table and run an evolutionary search (for each computation constraint) using light predictors, but PFNAS needs to build a reward evaluator (15 mins) and train the controller (2 GPU hours) again. The advantage of PFNAS over OFA is unclear.\n\n2. Experiments\n\n  2.1. It is weird that OFA-MO is worse than the original OFA, because OFA-MO performs a search using the true accuracy from the super-network but the original OFA uses a predictor to predict the true accuracy. Please explain. Is it related to input resolution? \n\n  2.2. the multi-objective function in Mnasnet is outdated. Use a better baseline of the L1 like function in [3].\n\n\nClarity:\n1. \"model performance\": please clarify if performance means accuracy or speed.\n2. In \"process takes about 40 GPU hours\", clarify if it is 40 single GPU hours\n3. In Table 2, the P_s in the third row (without using Pareto Dominance Reward) is better than the fourth row?\n\n[1] https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740647.pdf\n[2] https://arxiv.org/pdf/2007.04965.pdf\n[3] https://openaccess.thecvf.com/content_CVPR_2020/papers/Bender_Can_Weight_Sharing_Outperform_Random_Architecture_Search_An_Investigation_With_CVPR_2020_paper.pdf",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Review ",
            "review": "Paper Summary\n\nThe paper considers the problem of Neural Architecture Search (NAS) when multiple objectives needs to be optimized jointly. An approach called Pareto-Frontier-aware Neural Architecture Search (PF-NAS) is proposed for optimizing over two objectives (specifically latency and accuracy). The approach consists of sampling multiple latency budgets uniformly and finding a Pareto set of architectures that satisfies the budget constraints. To compute a single objective value for a proposed architecture with a given budget, a model (named 'architecture evaluator') is learned with pairwise ranking loss. Experiments are performed on three platforms of different latencies.\n\n\nDetailed Comments\n\n- The paper considers an important problem relevant in practice (where multiple objective optimization is the usual norm). \n\n- One key part of the proposed approach is 'formulating the optimization problem into a Markov Decision Process (MDP)'. However, the write up is confusing and there are many details not described properly. For example, state and action space description is given as follows: 'Here, we define the budget as a state, the decision to find an architecture satisfying any budget as an action'. Is the action space binary (whether we take the decision or not)? Similarly, the state transition function is not clear. Given a state and an action, which state does the agent land in?  These are basic questions that needs to be addressed clearly and explicitly. \n\n- There is a big assumption in the paper that by learning a Pareto set of architectures for a set 'L' of sampled latency constraints, we can capture any latency by considering a simple interpolation of the latencies from set 'L'. This assumes that the entire space of latencies can be captured by a small sampled set used in training. It is an important assumption that needs to be discussed and tested in much more detail. This is also at odds with the main motivation of the paper that a single utility function cannot be utilized for multi-objective optimization problems. \n\n- It is suggested that the main reason for good performance of PF-NAS is that the 'learned Pareto frontier benefits from the shared knowledge across the search process under different budgets'. However, the performance drops significantly and monotonically by increasing the number of sampled budgets (K variable) (Section G in supplementary). This is in contrast with the former statement. If the method leverages shared knowledge across search process under different budgets, the performance should ideally increase (or remain same at least). \n\n- The writing of the paper comes across as if the proposed approach is general enough for multiple objectives. However, the proposed solution is specific for two objectives. Please let me know if there is a straightforward extension of the approach to more than two objectives. \n\n- Please consider adding a comparison of the proposed approach with other baselines on training time as well. Although PF-NAS does better than the baselines on accuracy metric, but the improvement is within single decimal points and even that might not be statistically significant. Therefore, training time comparison is very important because a practitioner will prefer a method which requires less training time if the accuracy gain is limited by a superior approach but with large training time. \n\n- The writing of the paper can be substantially improved. For example, it is not clear what does 'learning the whole Pareto frontier mean'? \n\n- Some specific questions on the experimental section\n\t- What embedding is used for budget part in the architecture evaluator?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work",
            "review": "This work proposes a neural architecture search approach that can generate multiple architectures of different computational budgets. To learn the Pareto frontier, it employs a Markov decision process and solves it using a policy gradient method. Moreover, to find better frontiers this work suggests a Pareto dominance rule and takes it as the reward. Experimental results on three different hardware platforms show that the proposed method successfully achieves competitive Pareto frontiers and thus show its effectiveness.\n\nThe paper is well-written with a nice motivation and the problem seems well-formulated. But, my concern is that it probably entails extra training time because it searches for multiple different budgets. I wonder how much search cost does the proposed method requires compared to other existing approaches.\n\nIn Eq. (3), only the upper bound T exists without a lower bound. I wonder there is any possibility that this may lead to an architecture with a much lower budget than the other, i.e., c(b_1) << c(b_2) < T. But, in the experiments, it doesn’t seem so.\n\nBefore the experiments section, it looks unclear to me that “PFNAS only searches once and thus takes approximately 1/K search cost”. It would be better to elaborate on the reason in detail.\n\nThere are similar works achieving a Pareto frontier in a learning framework that can control computational budgets by adjusting width or depth, even though the work realizes learning a Pareto frontier with NAS:\n1. Vu et al., Any-Width Networks. CVPR 2020.\n2. Yu et al., Slimmable Neural Networks, ICLR, 2019.\n3. Kim et al., NestedNet: Learning Nested Sparse Structure in Deep Neural Networks, CVPR 2018.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}