{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #5",
            "review": "This work targets at solving the problem of domain generalization by learning domain-specific batch normalization layers at training and then mapping an unknown domain into the linear combination of known domains for the inference. \n\nTheir contributions can be summarised as three-fold: \n1) The method maps training domains in a latent space via BN statistics, where domains can be well separated in such a latent space. \n2) The method can be well generalized to both known and unknown domains at the inference stage, i.e. known domains can be solved naturally and unknown domains can be represented as a linear combination of multiple known domains. \n3) The method is plug-and-play in various CNN architectures.\n\nStrengths:\n1) The paper is well written and easy to follow.\n2) The introduced method is quite simple and easy to be reproduced.\n3) Performance gains on multiple datasets indicate its effectiveness.\n\nWeaknesses:\n1) The paper missed two important references [A, B], where [A] introduced domain-specific BNs for learning domain-dependent knowledge in domain adaptation tasks, and [B] proposed to simply adapt to the domain of test data via the BN statistics at inference stage. This paper shows significant relations with these two previous works, but unfortunately, I could not find any discussions. Compared to these two works, the major difference is that this paper adopts linear combinations of the known domains to handle the testing on unknown domains, showing that the novelty is somewhat limited.\n\n[A] Domain-Specific Batch Normalization for Unsupervised Domain Adaptation. CVPR 2019.\n\n[B] Adaptive Batch Normalization for practical domain adaptation. PR 2018.\n\n2) The paper claimed its plug-and-play function, so experimental results on more datasets (e.g. VisDA-17) and more backbones (e.g. deeper networks than AlexNet and ResNet-18 in the paper) are expected. \n\n3) It's a recommendation rather than a weakness. I am wondering that if training with more known domains could help the performance on the unknown domain at testing, since the linear combination may be more accurate.\n\n\nI recommend a rate of 4 for the initial comment since the paper missed discussing and comparing it with two important previous works, which are highly related to this submission.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper review from AnonReviewer4",
            "review": "Summary:\nThe paper aims to tackle the domain generalization task for image classification. The central idea is to learn domain-specific statistics from seen domains, and during testing on the unseen domain, the output prediction is calculated via a linear combination of predictions from seen domains. Specifically, the paper adopts domain-specific batch normalization (BN) layers to train the model. During testing, a similarity score is computed by measuring the Wasserstein distance in statistics of all BN layers between the testing instance and the known BN from each domain. Therefore, the output prediction is weighted by this similarity and is the linear combination of predictions from each domain. The paper also adopts a warm-up training scheme to learn domain-specific BN first, and then simulates this linear combination strategy among seen domains as an additional training signal (to be consistent with testing time).\n\nPros:\nThe paper is well written and is easy to follow. The idea of using domain-specific BN layers as the weight on predictions is interesting, simple, and technically sound.\n\nExperimental results on several benchmarks show good performance compared to some baselines. Also, the ablation study shows the importance of the strategies used in training the entire framework, i.e., warm-up training and distance weighting at training time.\n\nCons:\nThe main concern for this paper is about the novelty and effectiveness of the proposed approach. Specifically, one closely relevant paper using domain-specific BN for domain generalization is not properly discussed and cited [a], where it achieves 2% better performance on PACS. Also, it seems directly using instance BN during training could be more effective than the proposed method. The authors should comment on it with more analysis and comparisons.\n\n[a] Seo et al., Learning to Optimize Domain Specific Normalization for Domain Generalization, ECCV'20\n\nUsing the statistics of all BN layers to represent the similarity between domains may not be accurate. From Figure 1 and Table 8 in the supplement, it is difficult to know whether the distance measured by BN is meaningful. It could be nicer if the authors provide experiments on the digit dataset for better demonstrations.\n\nIn experiments, warm-up training seems more important, especially for Sketch on PACS (Table 3). The authors should comment on it and compare the used standard BN with other BN methods (e.g., IN in [a]). It is also better to provide the baseline using only the warm-up training (without DT). \n\nFor the baseline \"DeepAll\", it is not clear why it is worse than the baselines reported from other papers. In Table 1 of the supplement, there is even a larger gap when using AlexNet. The author should explain this and see whether adding the proposed method on the proper baseline still provides a similar performance gain, so that it can fairly compete with other SOTA methods, e.g., MASF, JiGen, and MetaReg\n\nOther suggestions:\nIt is also good to show performance on the more challenging Office-Home dataset\nOne missing reference [b] to discuss and compare with\n\n[b] Matsuura and Harada, Domain generalization using a mixture of multiple latent domains, AAAI'20",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The main idea is simple but interesting. However, its implementation seems quite ad-hoc.",
            "review": "--Paper summary--\n\nThis paper presents a novel method for domain generalization. In the training phase, domain-specific batch normalization statistics are stored in the model. These statistics are used to obtain predictions of domain dependent models and are also used to estimate weights to ensemble the predictions. Experimental results with several benchmark datasets validate the advantage of the proposed method. \n\n--Review summary--\n\nThe main idea is simple but interesting. However, its implementation seems quite ad-hoc, which should limit the capability of the proposed method. I vote for rejection.\n\n--Details--\n\nStrength\n\n- The main idea is interesting. Ensembling multiple models requires large memory consumption to store those models. The proposed method solves this problem by using shared parameters with domain-specific batch statistics to imitate multiple models.\n- The proposed method empirically work well.\n\nWeakness and concerns\n\n- Two concerns on Eq. (7).\n\t- It is not robust against the scale of the parameters of the model. For example, if we multiply a certain positive constant value to the parameters of the i-th layer, D_L should increase due to the increase of mu_i and sigma_i, though the functionality of the model would not change thanks to batch normalization. Therefore, the performance of the proposed method should heavily depend on how to initialize the model as well as how to train the model.\n\t- Since sigma_t are the statistics within single instance, they cannot be directly compared with sigma_d, which are the statistics across multiple instances. If the authors argue that the comparison is meaningful, its justification is required.\n- The authors argue that the unknown target distribution can be recovered in Eq. (8), but no justification is given on this point. Since w_d^t is computed only for single target instance, it should be impossible without any assumption to \"recover\" the target distribution by using this weight. \n- Several closely related studies are not referred.  \n[R1] \"Transferable Normalization: Towards Improving Transferability of Deep Neural Networks,\" NeurIPS 2019.  \n[R2] \"Domain-Specific Batch Normalization for Unsupervised Domain Adaptation,\" CVPR 2019.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reivew",
            "review": "This paper calculates the batch normalization statistics layer by layer for each source domain and generalize to a novel domain by expressing it as the linear combination of source domains based on their distances regarding the BN statistics. The paper is easy to follow.  The proposed method achieved some marginal improvements above the state-of-the-arts.\n\nSome Concerns:\n\n1. The effectiveness of domain-dependent BN statistics need more in-depth discussions. For domain generalization (DG), [1] shows that freezing all batch normalization (that is, using the statistics from ImageNet-pretrained model) with simple Empirical Risk Minimization has been the strong baseline to solve the problem of DG. Do the bn statistics obtained in this paper more powerful than just freezing the BN?\n\n2. Is there any theoretic or empirical supports for the main claim that a novel target domain will always be the linear combination of multiple source domains? When will this assumption be true and when not?\n\n3. Could the authors elaborate more details about selecting the model that is employed and evaluated in the target domain? Is the process of model selection consistent with the previous works?\n\n4. For those results using Alexnet architecture, do the authors also include BNs for the compared methods?\n\n\n[1] Gulrajani I, Lopez-Paz D. In search of lost domain generalization[J]. arXiv preprint arXiv:2007.01434, 2020.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}