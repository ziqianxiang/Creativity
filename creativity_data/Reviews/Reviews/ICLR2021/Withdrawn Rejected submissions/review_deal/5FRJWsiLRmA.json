{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers were split between accept (7) and borderline reject (two 5's). All three reviewers acknowledged that the proposed approach is simple and intuitive (but this paper follows, for the most part, the concept of reservoir operation and apply it to transformers). The main criticisms were insufficient experiments (R5) and the lack of a clear conclusion (R2). I found these concerns to be valid and did not find strong reasons to overturn their recommendations. More comprehensive experiments (especially on WMT) and clear conclusions (accuracy or efficiency) would make this paper much stronger."
    },
    "Reviews": [
        {
            "title": "Review #5",
            "review": "The authors demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. The authors have experiments on four types of reservoir: Transformer Reservoir , FFN (feed-forward layer) Reservoir, BiGRU Reservoir and CNN Reservoir. And the results show that the Reservoir can achieve competitive/better performance or than normal Transformer on machine translation, language modeling, and MLM pre-training. As pointed out by the authors,  deep reservoir computing networks (Scardapane & Wang, 2017; Gallicchio & Micheli, 2017) have been explored before. The main novelty of this paper is only the Reservoir exploration of Transformer structure. Although the method is interesting and the experiments are well-done, the work \"REDUCING TRANSFORMER DEPTH ON DEMAND WITH STRUCTURED DROPOUT\" seems more straightforward and effective. The authors should have some comparisons with this method. As there's still residual connections, how about simply adding some noise to the previous layer? Overall, although the experiments are interesting, it doesn't provide novel theory on it and misses some comparison to previous works. It's hard to evaluate the significance of this work.\n\nPro:\n1. The reservoir on Transformer is interesting and has not been explored before.\n2. The authors prove the idea by many different tasks.\n\nCons:\n1. The reservoir operation was explored on other structures.\n2. No strong baseline provided, such as \"REDUCING TRANSFORMER DEPTH ON DEMAND WITH STRUCTURED DROPOUT\" or some other structure pruning based methods.\n3. No novel theory to explain the method.\n\n\n######update\n\nI like the experiment added in the revision. However, it is only tested on a IWSLT which is a smaller dataset and can be influence by many hyper-parameters. It's not clear what frozen layers mean for LayerDrop and it need to be clarified with more details. I didn't find clear comparison with stronger baselines on WMT in the revision.\n\nAs pointed our by the authors, I think the theory mainly come from previous works. \n\nI have also read other reviews. Overall, I would like to keep my rating.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "randomisation in transformer networks",
            "review": "The paper explores the concept of randomization in Transformer architectures. Essentially, some of the layers in the encoder network are replaced by fixed layers, which makes the computation faster straightforwardly in the prediction phase. The paper also illustrates a method, called backskipping, to reduce the cost of backpropagating gradients through the fixed layers. The proposal is well supported by a number of experiments in the area of Machine Translation.\nPROs:\n- I find the paper very clear and well written & readable. I enjoyed reading it.\n- The research proposed in the paper seems novel enough to me, and goes towards a fruitful research direction, that one of randomized neural algorithms (although this is nowadays established in the ML community).\n- The experiments offer a nice perspective on the advantages of the proposed model\n\nCONs:\n- I find inappropriate the use of the term \"reservoir\" in this work. In my understanding, that is essentially a fixed (untrained) recurrent layer. The aspect of the dynamics is fundamental in this respect. In the paper, instead, the use of \"reservoir\" stands mainly for \"randomized\". All in all, I don't see reservoir layers in the proposed model, and I suggest to take this into account (perhaps changing the name and the title)\n- The experiments show that using untrained layers gives a positive trade-off between accuracy and compute times. In the experimental comparison, I think it would - however - important to see how this trade-off behaves/scales wrt to the number of trainable weights (when avoiding training in some of the layers the number of trainable weights is reduced, and the overall complexity of the system is reduced too, having a sort of regularization effect).\n- The experiments show that it is possible to avoid training in some of the layers of a Transformer. However, in randomized neural networks, a pivotal role is played by the scaling parameters of the involved weight matrices. Orthogonal matrices are used in the experiments, which is good, but I wonder how (and if) the scaling of such matrices affect the performance. Ideally such scaling should be hyper-parameters and be chosen on a validation set.\n\na few minor points:\n- please, if possible, clarify more explicitly on the concept of \"converging faster\"\n- please, clarify on the data splitting (in tr/vl/ts) for the used datasets\n- please, introduce the datasets before section 3.1 (where they are already mentioned without intro).\n\n-- EDIT:\nI would like to thank the authors for the nice work during the review process. I am pretty satisfied with that and I feel serene to increase my rating to acceptance.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and simple idea but the experiments need to be clearer",
            "review": "### Summary\n\nThe paper studies how transformers train when some layers are kept fixed at the randomly initialized parameters. The authors observe that transformers can be effectively trained with a significant percentage of their layers \"frozen\". It is also argued that as a result transformers can be trained more efficiently as for the frozen layers gradients with respect to the parameters need not be computed.\n\n### Strengths\n\n- The proposed idea is very simple and easy to understand and implement.\n- The authors perform thorough experiments on several datasets and tasks.\n\n### Weaknesses\n\n1. The AUCC metric is not measuring the efficiency independent of the time budget as it is claimed. For instance given a high enough T_hat the best performing model will always have the highest AUCC. Contrary to that, given a low enough T_hat the fastest to converge model will always be better. I agree with the authors that measuring the efficiency of a neural network is a hard problem but I don't think that the AUCC metric is a good solution. Time to X% of best score is probably much more informative. In addition, time to best score is also not very informative as it depends heavily on the learning rate schedule and the random oscillations of training.\n2. From the provided comparison with respect to the number of updateable layers we cannot deduce information regarding the efficiency. For instance in Fig. 2 for WMT how many \"frozen\" layers are used for the reservoir transformers? What is the wall-clock time per epoch for each model?\n3. From the comparison in the supplementary material with respect to all the layers in the model we observe that the performance is on par with standard transformers but what is the improvement in efficiency? Namely, how much faster are the reservoir transformers for a given number of layers?\n\n### Reasons for recommendation\n\nThe paper proposes a very simple idea and then evaluates it experimentally. Therefore, the experimental section should be thorough and should lead to clear conclusions. Due to the the newly proposed metric as well as the lack of comparison between training time and achieved performance lead me to propose rejection.\n\n### Miscellaneous\n\n- Figure 14 bottom right should probably read \"Validation PPL\".\n\n### Post-rebuttal update\n\nI would like to thank the authors for their additions to the paper. I believe that the extra metrics improve the reader's ability to extract conclusions from the experiments significantly.\n\nHaving said that, I believe that the extra experiments and numbers do not paint a clearer picture. For instance, in IWSLT and WMT indeed there is fairly consistent evidence that FFN Reservoir is more efficient to train than fully trainable transformers. However, for enwik8 the T Reservoir outperforms everything significantly when looking at figure 15 but judging from figure 13 we see that the best case scenario has been selected for T Reservoir (namely 32 or 48 layers). Even more importantly perhaps, the story is completely different when looking at the test set evaluation where FFN Reservoirs perform better and actually the T Reservoir performs the worst among all methods. Similarly on RoBERTa pretraining, fully trainable transformers seem to achieve the lowest validation perplexity and with the highest efficiency.\n\nTo summarize, I believe that the reservoir transformers could be a useful tool for improving either the efficiency or the generalization of transformers in low-data regimes or both. However, a distillation of the experiments and conclusions is required in order for this to be shown from the paper. Namely, even with all those numbers I still cannot judge which reservoir layer will be better, in what sense it will be better (accuracy or efficiency) and why is it going to be better.\n\nDue to the above, I tend to keep my score but because the additions of the authors provide significantly more information (and in my opinion value) to the paper, I will increase my score to 5.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}