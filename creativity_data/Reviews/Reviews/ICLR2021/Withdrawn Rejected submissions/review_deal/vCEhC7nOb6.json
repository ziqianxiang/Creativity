{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The main concern is that the results in this paper are based on strong asymptotic assumptions. (At least) more empirical results are needed.\n"
    },
    "Reviews": [
        {
            "title": "Technical issues",
            "review": "The paper studies the gradient flow dynamics over smooth homogeneous models with two types of weight normalized parameterization - standard weight normalization (SWN) and exponentiated weight normalization (EWN). Thm 1 shows the induced dynamics in the unnormalized parameter space resulting from gradient flow on respective weight normalized parameterization. This result is a good starting point that highlights the different dynamics arising from the two parameterizations.\n\nHowever, in the remainder of the paper, there are several technical issues/confusions, outlined below (p.s., please number the equations):\n\n1. In the proof of Proposition 3 and also Thm 2 (see e.g.,  last eq. in page 23 and corresponding equations for GD in Appendix E.1.2, and similarly, last eqn in page 20), the following equality is used which is not true in general. Please clarify if I missed something: ||w(t)||=||w(t_2)||+int_{k=t_2}^t ||dw(t)|| -- triangle inequality would show that the RHS is an upper bound but I do not see how we can get exact equality. \n\n2. In the proof of Thm 3 (page 20), why does Proposition 2 imply that w_u and its negative gradient are aligned in opposite directions? Specifically, why should there be a t_2 such that for all t>t_2, cos(-\\nabla_{w_u} L,w_u)<=\\epsilon?\n\n3. In Appendix D.2 (page 22) while bounding ||w_u(t)|| for SWN, along with the above two concerns, I am also not sure how the two terms in ||dw(t)|| from Thm 1 lead to the simplified bounds on ||w_u(t)|| in the first non-thm equation on page 22. \n\n4. Finally, although not a technical mistake, I believe that the discussion comparing between EWN and unnormalized GF (which I will simply call GF) is conceptually confusing. As the authors themselves note, EWN and GF both follow the *same trajectory*. EWN simply has a scaling factor of ||w(t)||^2 which affects the “speed” along the trajectory but the path itself if the same -- both have dw(t) = -s(t) nabla_w L(w(t)) for different scalar speeds s(t) and it corresponds to the same path in the space of w but with different time warping. Thus, if one solves the differential equations indefinitely both EWN and GF will trace the exact same path albeit at different times and will eventually lead to the same separator. But the plots and the discussion about Fig 5 for example suggest that EWN and GF leads to different asymptotic solutions, which is not correct.\n\nThus, when comparing EWN and GF, the message could be that EWN when discretized could lead to faster convergence - this is somewhat justified experimentally (from Fig 5) but not theoretically as to truly compare one needs to show analysis for the discretized algorithm. Also experimentally to provide correct comparison of the speed, in Fig 5, the number of iterations of the two methods (EWN and GF) should be matched which is not true in the current plots. On the other hand, it is simply wrong to phrase the message as “EWN and GF lead to different solutions asymptotically”. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "This paper analyzes weight normalization methods, including exponential weight normalization (EWN) and standard weight normalization (SWN), in contrast with unnormalized networks. Under a number of assumptions, the paper characterizes the asymptotic relation between weight norm and gradient norm at the node level (Theorem 2), which shows a distinction between SWN and EWN. Then it's argued that SWN leads to sparser solutions (Proposition 3), which is potentially beneficial for pruning. The paper also shows a convergence rate for SWN which is slightly faster than unnormalized and SWN from previous work, but under stronger assumptions. The paper verifies these results empirically on some toy examples.\n\npros:\n+ The exponential weight normalization method seems new.\n+ The paper has some interesting findings regarding the asymptotic behavior of weight normalization methods (if the results can be justified properly).\n\ncons:\nThe theoretical results are based on very strong asymptotic assumptions, which are not justified properly. The experiments are on very toy settings which are far below the bar. Either the theory or the experiments need to be stronger for this paper to be a solid contribution.\n\n- The assumptions (A1)-(A4) used throughout the paper are much stronger than those in previous work, such as Lyu & Li (2020). In particular, (A3) and (A4) are nonstandard. I'm not sure when these assumptions are expected to hold, and they are only empirically verified on an extremely simple dataset (4 examples).\n\n- In Proposition 3, which is where it is shown that SWN leads to sparsity, there is an extremely strong assumption that the ratio of two gradient norms at two nodes stays constant forever after some point in training. How can this possibly be true?\n\n\n---------- after rebuttal ----------\n\nThanks for the response and the updated manuscript. I'm raising my score from 4 to 5. I'm still leaning towards rejection since I still find the results quite subtle and I hope to see more empirical justifications.\n\nIn the updated Proposition 3, the sparsity-inducing property 3 assumes the existence of a time $t_2>t_1$ when the ratio between the two weight norms deviate from $1/c$. However, it seems entirely possible that this ratio will have already converged $1/c$ after time $t_1$; in this case the two weight norms grow at the same rate. It would be good to investigate this more carefully to see which cases are more likely to happen. I'm also concerned that the advantage of EWN for pruning only shows up in extremely small loss value (Figure 7), and therefore the practical relevance shown in the current paper is not very convincing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid results for analyzing normalization methods, but significance is not exactly clear and several results lack details.",
            "review": "### Summary \nThis paper studies the inductive bias of gradient methods with normalization on smooth homogeneous models. The focus is on two normalization methods, standard weight normalization (SWN) and exponential weight normalization (EWN). The authors show two main results. The first characterizes the trajectory of normalized gradient methods from which they provide theoretical evidence that EWN is biased towards sparse solutions. The second provides convergence rates for the normalized methods which shows the difference between convergence rates of normalized and unnormalized methods. The theoretical results are corroborated with experiments on several toy datasets.\n\n### Reason for score\nI am currently inclined towards accepting the paper because the results are novel, solid and should be interesting and useful for researchers working on theory of deep learning. However, the score is only marginally above the acceptance threshold, because I have several concerns regarding the clarity and significance of the results. I am willing to raise my score if the authors address my concerns in the rebuttal.\n\n### Pros\n1.\tThe theoretical results are solid, novel and the proof techniques might be useful in other inductive bias analyses.\n2.\tThe sparsity result for EWN is interesting and provides novel insights on pruning neural networks as the MNIST experiments show.\n3.\tMost of the paper is clearly written.\n\n### Cons (roughly ordered from major to minor comments)\n\n1.\tIt is not clear in which cases SWN and EWN are used in practice. The authors do not explicitly cite papers that use them. Therefore, it is not clear how to assess the significance of the results.\n2.\tAfter Theorem 2 it is claimed that ||w_u(t)|| is inversely proportional to ||grad_u L(t)||. I am not sure why this is correct. If ||w_u(t)|| = t and ||grad_u L(t)|| = 1/t^2 for all u, then the theorem result holds, but the claim after the theorem (mentioned above) does not hold. Am I missing something?\n3.\tIn Proposition 3, the assumption that the ratio of gradient norms is exactly c from some t onwards is very strong. The authors should comment on this. Does it hold in practice? Does the Proposition hold under weaker assumptions?\n4.\tMost of the experiments are performed on very simple datasets with few points in the training sets. I think that experiments on other datasets (e.g., with 1000s of points) can strengthen the results.\n5.\tIn Proposition 1, eta(t) is said to be a constant but it seems to depend on the loss which changes with time. What is the L in the denominator of the learning rate equation? Is it the loss?\n6.\tIn Figure 1, the neighborhood of a point for different geometries is not formally defined. The current figures are not clear.\n7.\tIn several experiments, it is claimed that the loss achieved values of order e^(-300). This seems like an unrealistic precision to get empirically. Is there a mistake here?\n8.\tThe presentation of the normalization methods in the equations in page 2 is not very clear. Specifically, why these equations result in a form of normalization. Can the updates be presented in a concise equation where the normalization is showed explicitly?\n9.\tI think that the authors should provide more context to the pruning results in Section 6. Specifically, say why the insights on EWN in previous sections can be useful for pruning applications.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work on the implicit bias of gradient descent with exponential weight normalization",
            "review": "This paper analyzes the implicit bias of gradient descent with both the standard weight normalization (SWN), which basically uses the gradients with respect to the radial part and spherical part of the weights, and the exponential weight normalization (EWN), which further parameterizes the radial part using an exponential function. Under a few convergence assumptions, it is shown that for SWN, given a node in the network, the norm of the input weight vector is proportional to the norm of the gradient with respect to this weight vector, while for EWN, the norm of the weight vector is inversely proportional to the norm of gradient. It is further shown that such an implicit bias implies that EWN induces sparse limiting directions, and empirical support is provided. \n\nI think SWN and EWN proposed by this paper are interesting, and it is surprising that they introduce opposite implicit biases. It is also interesting that EWN can find sparse or \"simple\" solutions.\n\nI have the following questions regarding experiments:\n1. Can Proposition 3 be verified on MNIST? For example, can you compare the distribution of norms of weight vectors for EWN, SWN, and unnormalized gradient descent?\n2. Can EWN also improve generalization or sparsity on more complicated datasets, such as CIFAR?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}