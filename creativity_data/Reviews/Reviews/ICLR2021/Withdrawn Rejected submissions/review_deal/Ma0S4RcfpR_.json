{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose a recurrent model of self-position, with a handcrafted expression of the rotational structure in terms of a matrix Lie group. As noted by the reviewers, this work strongly builds upon Gao et al (ICLR 2019). This really is mentioned too late and not prominently enough in the manuscript, and furthermore, the difference to this work is not clearly explored in the paper (there are just two sentences immediately prior to the conclusion and no experimental comparison). The reviewers pointed out that the phenomena observed here are handcrafted into the structure of the model, rather than being emergent. The reviewers raised concerns that it is not clear what conclusion to draw from this work.\nFor these reasons, I recommend rejection this stage."
    },
    "Reviews": [
        {
            "title": "An interesting approach but not clear what we learn from the result",
            "review": "The paper proposes a model of the grid cell system based on computing an embedding of position and head direction that allows matrix Lie algebras to translate and rotate the coordinate frame.  The results show that individual elements of the embedding have spatial response profiles resembling grid cells in entorhinal cortex.\n\nEquations 1 and 2 are a nice mathematical approach.  the justification for using Lie algebra is clear and its very elegant, and seems like a good idea.   The results are interesting too - very neat to see the hexagonal grid arrangement emerge.  But why?  Here I'm not sure we learn anything.  In fact the authors seem to state that certain parameter settings such as beta are required to bring about this result, but there's little insight provided as to why.  And how does it affect the result?  Also, the modularity - one of the most interesting facets of the grid cells system - is rigged in advance, rather than an emergent property.  So it leaves you wondering, what's the point here?  For example, what is the advantage conferred by embedding 2D position into a higher dimensional space?  why is this a good thing for the brain to do?  and why a grid system?  the answer seems to be \"that's what emerges from our matrix Lie algebra model.\"  ok, but that's not very illuminating.  Surely there should be some way to reason about this result and why its useful for brains to have this type of representation.  That would make the paper a lot more interesting in my view.\n\nThe authors do a nice job investigating the influence of different terms.  Ok, but it also seems like just hunting and pecking phenomenology, try this try that.\n\nAlso this sentence in the intro I find rather unsatisfying:  \"It is worth noting that our work is mainly concerned with representation learning. We do not seek to pursue biologically realistic modeling of neural dynamics..\" - so you are modeling an aspect of biology, but you aren't concerned with realistic modeling of neural dynamics?  I can understand perhaps leaving out spiking neurons and ion channels and all that, but it would seem to me that taking into account what is actually neuronally feasible is important.  For example, presumably having a local connectivity structure in B is important, and that could well affect the results.  Its not just about representation learning, but implementation and biophysical constraints are also important.\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting suggestion backed by numerical experiments but lacking analytical justification",
            "review": "The authors develop a model for learning the observed responses of grid cells (GC) in the entorhinal cortex from the animal movement vectors. Their key assumption is that the GC activity vector rotates with the movement magnitude according to the Lie group formalism and the corresponding Lie group generator is also rotated by the change in movement orientation. Their claim is supported by a numerical optimization of the objective function reflecting these assumptions as well as the projection onto the place cell representation in the hippocampus.\n\nI find the paper original, interesting and clearly written. The numerical simulations support the claim. However, given the current state of the field, I would like to see an analytical demonstration of this claim like in the work of Sorscher et al which the authors cite. Specifically, would it be possible to demonstrate that dropping the orthogonality constraint used in Sorscher et al and introducing the rotation of the Lie generator would still result in realistic GC responses as the authors claim? Such analytical demonstration would provide a much needed insight into the operation of the system. \n\nMinor comments:\n\nPage 3, second line after Eq(4): the closing parenthesis is missing after B(\\theta\n\nSorscher et al reference is listed twice\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nAdded after author response. My enthusiasm for the paper has diminished because it seems to be more of an incremental step over Gao et al 2019 and the authors did not provide additional analytical insight into their new results. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Path Integration Lie Algebra",
            "review": "Summary: The authors propose a simple recurrent network as a model of spatial navigation in the MEC/Hippocampal network. This model assumes that grid cells only regularly receive egocentric movement information, an important aspect for understanding the origin of these functional cell types in-vitro. Overall, this article should be of interest for any ICLR members interested in biological  spatial navigation. \n\nStrong Points: \nAs stated above, the model is an intuitive explanation of how allocentric-egocentric transformations might be performed. While the authors use a back propagation approach to training, the separated loss functions (eq 10-12) for each layer of the network mean that learning could be performed by predictive contrastive coding, as recent research suggests biological networks may be doing.\nThe learned receptive fields show many of the more nuanced aspects of grid cells found in experimental studies, such as discretized angle relative to the environment. The investigation of error accumulation as a function of time steps since encoding allows additional comparisons to the literature. These give additional confidence that the model is biologically plausible, at some level of abstraction.\n\nWeak Points: As a non-mathematician, it's unclear to me what the implications of the lie algebra presented in the beginning of section 2 are. My understanding is that this makes grid activities the product of two separable matrices (displacement, and rotation). While this is biologically plausible, perhaps the authors could explain any additional reasons why this is of importance.\n\nAdditional Comments: A possibly interesting future experiment would be investigating effects on error correction by only having place cells at a handful of spatial scales (A.2), especially if there is a \"block\" structure in u.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Elegant formalism but unclear utility",
            "review": "This paper proposes a simple recurrent model of how grid cells may perform path integration, which also recapitulates the well-known finding that grid cells exhibit hexagonal firing patterns. Their model consists of two primary components where self-position is represented by a population activity vector (which is rotated by a generator matrix of a Lie algebra whenever the agent moves in a given direction), and where self-motion is represented by the rotation of this vector (whereby when the agent changes its direction, this generator matrix is itself rotated by another generator matrix).\n\nStrengths:\n+ I think the model is a mathematically elegant formalism.\n+It is more explicit than the approach taken by task-optimized (nonlinear) RNNs.\n\nWeaknesses:\n-\tIt is unclear to me what scientific insight we get from this model and formalism over the prior task-optimized approaches. For instance, this model (as formulated in Section 2.3) is not shown to be a prototype approximation to these non-linear RNN models that exhibit emergent behavior. So it is not clear that your work provides any further “explanation” as to how these nonlinear models attain such solutions purely through optimization on a task.\n-\tFurthermore, I am not really sure how “emergent” the hexagonal grid patterns really are in this model. Given partitioning of the generator matrices into blocks in Section 2.5, it almost seems by construction we would get hexagonal grid patterns and it would be very hard for the model to learn anything different.\n\nWhile the ideas of this paper are mathematically elegant, I do not see the added utility these models provide over prior approaches nor how they provide a deeper explanation of the surprising emergent grid firing patterns observed in task-optimized nonlinear RNNs. For these reasons, I recommend rejection.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}