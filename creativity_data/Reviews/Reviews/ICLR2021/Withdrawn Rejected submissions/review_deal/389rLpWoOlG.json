{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper evaluates several different strategies for labeling of missing data, and recommend the best strategy in practice based on the empirical results on six data sets.\n\nThe reviewers agree that empirical evaluation is important for providing a good guideline for this practical problem. The concerns of the reviewers include the lack of motivation on the chosen strategies, the lack of novelty (the tools in the strategies are all pretty standard in the literature), the lack of reproducibility (by referring to the authors' own anonymous work for parameters), and the lack of breadth (e.g #data sets) and depth (e.g. metrics explored) in the experiments.\n"
    },
    "Reviews": [
        {
            "title": "A weak empirical evaluation",
            "review": "This paper aims to evaluate the performance of seven automated labeling algorithms in terms of accuracy. The authors conducted a set of experiments on six datasets from different domains under two typical settings where 10% and 50%of labels in the datasets are available. Experimental results show that the algorithms label spreading with KNN perform better in the aggregated results, the active learning algorithms  QBC and query instance uncertainty sample perform better when 10% of labels available.\n\nOverall, this paper cannot meet the high-quality requirements of ICLR.  First, active learning algorithms such as QBC and uncertainty sampling is not automated labeling algorithms. They are only strategies for the selection of unlabeled instances. The selected instance either can be labeled by human experts or automated labeling algorithms. Second, when evaluating an automated labeling algorithm, merely using accuracy is not enough. For example, when the underlying class distributions are imbalanced, the accuracy is not sufficient to characterize the generalization performance of a learning algorithm. Third, two settings of 10% and 50% of labels available are also insufficient. Many papers of the empirical study investigated the performance under more complicated settings. Finally, the number of investigated methods is two small and the paper should cover more state-of-the-art algorithms.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good idea, but flawed execution",
            "review": "The paper presents an empirical comparison of different approaches for data\nlabeling. The authors describe their experimental setup and findings, making\nrecommendations for when to use what approach in practice.\n\nThe authors reference their own anonymous work throughout the paper as\njustification for the presented investigation and its parameters. This is\nproblematic as the reviewers are now unable to confirm that the presented\ninvestigation is well-grounded.\n\nThe authors evaluate their approaches on only six datasets. It is unclear to\nwhat extent the results generalize, in particular as no detailed results per\ndataset are given. There could be significant differences between the different\ntypes of datasets, but not enough data is presented to judge. This matters in\nparticular with respect to the recommendations the authors make at the end of\nthe paper.\n\nSome details of the experimental setup are unclear. The authors say that they\nmeasure F1 score, but then refer to accuracy (e.g. in Figure 1). Which measure\nwas used? The experimental setup describes six datasets, but the results text\nrefers to seven. The results presented in Table 1 and Figure 1 seem to disagree\nwith Table 2 -- LabelSpreadingKNN is the highest-ranked algorithm, but\nUncertaintySampling performs better in terms of all the statistics presented in\nTable 1. The same is true for the second set of experiments (Tables 3 and 4).\nFor the first set of experiments it is unclear what fraction of labels were\nmissing.\n\nIt is unclear why the Bradley-Terry model was used here to compare outcomes.\nThere are multiple other methods to judge how and whether paired distributions\ndiffer. It appears that only ranks were used for this comparison and not the\nactual performance numbers.\n\nFinally, all methods evaluated by the authors have hyperparameters that need to\nbe set. It is unclear how the authors chose the particular values they used in\nthe experiments, and tuning them for best performance may have a major impact on\ntheir performance and the rankings. Conclusions from untuned methods are\nunlikely to generalize.\n\nThere are numerous typos and grammatical mistakes throughout the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An important topic (empirical evaluation) but it might require a major revision",
            "review": "In this paper, the authors present an empirical analysis of seven machine learning algorithms based on six benchmark datasets: Four graph-based semi-supervised learning algorithms and three active learning algorithms have been evaluated on image data sets (CIFAR10, Digits), texts (Fake and true news, 20news), and other data types (Iris and Wine). Based on the empirical performances of these algorithms, the authors provided a ranked list of the studied algorithms.\n\nEmpirical comparisons of multiple machine learning algorithms on real-world datasets are important as they help understand the strengths and weaknesses of different algorithms when they are deployed in real-world environments. However, I think this paper will benefit significantly from a major revision addressing the concerns listed below:\n1) Algorithm choices: all algorithms considered in the current paper were already extensively studied. Focusing on state of the art approaches (including recent deep learning-based approach) could significantly strengthen the practical relevance and impact of experiments and conclusions reported here.\n2) Data set choices: more challenging datasets can be considered: The datasets used in the current paper (including classical Iris and Wine) have up to only 20 categories.\n3) Presentation: I found it challenging to comprehend the exact experimental settings.\n- Section 3.1 states that 80% of data are allocated for training while the remaining 20% are used in testing. How does this setting apply for active learning algorithms? Did they actively select data points to label until when 80% are labeled or use only `50 instances’?\n- How is 80/20% decomposition applied to semi-supervised learning algorithms? Did they use the entire dataset with 80% of the entities labeled? In typical application scenarios of semi-supervised learning, only small portions of data instances are labeled. I was not sure if the experimental setting prepared in the current paper reflects well the real-world application scenarios of semi-supervised learning.\n- Also, in general, semi-supervised learning and active learning are different problems. I was not sure how the results reported in the current paper including Table 1 should be interpreted: I guess this does not suggest that semi-supervised learning algorithms are better than active learning approaches.\n4) Without having access to two `anonymous’ papers cited in this submission, it is hard to properly assess the contributions of this paper.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " The paper compares several approaches for labelling data, spanning active learning and semi-supervised strategies. The problem and choice of algorithms are not sufficiently motivated. In particular, it is unclear why the authors compare semi-supervised techniques with active learning techniques. The latter requires an oracle while the former doesn't. There is a range of further largely unjustified ad-hoc choices. This work is not sufficiently significant to warrant publication. ",
            "review": "\n\n## Detailed Comments\n- \"There are problems with supervised learning and machine learning in general.\" - This statement is so general, it is essentially vacuous.\n- \"machine learning requires huge amounts of data\" - unclear what this means. and whatever it means it's not true in general.\n- \"severe labeling issues were found\" - what issues?\n- \"we provide the an overview\"\n- \"(AL),\" - missing space. this happens at several places in the text\n- \"continue iterative\" - iteratively\n- \"other stopping criteria\" -> \"criterion\". also, why \"other\"? there was no stopping criterion mentioned so far. \n- \"If a learner does not choose his strategy\" -> their strategy\n- eq at bottom of page 2: what is \"u\"?\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}