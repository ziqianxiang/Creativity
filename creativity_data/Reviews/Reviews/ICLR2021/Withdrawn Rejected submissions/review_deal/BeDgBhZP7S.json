{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting paper on training with long tailed distribution",
            "review": "This paper studies the memorization effects in learning with long tailed data. It defined a notion of \"phase change point\" where the learning on common examples ends and learning on tail examples starts, which indicate \"when\" memorization happens. It also analyze \"where\" memorization happens in the network by analyzing different layer in the network.\n\nThis paper is relatively easy to follow and present some interesting findings that could potentially improve our understanding of deep learning. However, I think some of the claims need more empirical supports. Here are some questions and suggestions:\n\n1. The title is too general. Maybe consider at least add a subtitle to explain what this paper is about?\n\n2. The claim that learning and memorization happens in two stages with a phase transition at PCP is a very strong one. It could use more studies and supports. Currently the main evidence is Fig.1, which shows that for different ratio of outliers, the difference of accuracy always peak at the same time, which is indeed surprising to me. However, it is very hard to see what is actually going on by just looking at the difference curve. A 'phase transition' suggest there is no significant overlapping between the two stages. It would be very helpful to show the accuracies on good examples and outliers side by side with their differences to really see what is going on, especially to see if a clear phase transition is happening. Also, does the difference in accuracy means the actual learning curves (average accuracy over the entire training set, or over the clean examples and outliers separately) also remains almost the same profile regardless of the outlier ratio? The same applies to other plots as well, maybe it is good to put the results on one of the dataset to the Appendix and make some space to show the two curves separately, side-by-side with the difference curves.\n\n3. Please fix Fig.1. The y-axis says 'Accuracy', which might be 'Difference in Accuracy' instead? Also in the legend, you have 'Delta 0.1', etc. I'm guessing delta means the ration of outliers, but it does not seem to be defined anywhere.\n\n4. If PCP is the point where learning finishes and memorization starts, could you do early stopping at PCP and get optimal generalization performance? \n\n5. Fig.2 and Fig.3 show that purging / 'keeping' weights that changes after PCP has bigger impact on the long tail set. Would it be generally true for any subset of weights? Maybe it is good to show, as a baseline, what happens if you purge / keep a random subset  of weights (keep the number of weights purged / kept the same in every layer for a fair comparison).\n\n6. Section 4.4 shows that memorization mostly happens in lower layers. I have the following requests: a) can you also show analysis on ResNets? Because previous sections suggests resents have slightly different behaviors from AlexNet. b) (again) can you show the two accuracies along side of their difference? Some previous papers show that *learning* mostly happen at lower layers. I wonder if *memorization* happening at lower layer is a by-product or does it actually have a unique characteristics here. c) I wonder what role PCP plays here. Previous work (Zhang et al., \"Are All Layers Created Equal?\") showed that some layers can be rewinded (similar to the \"keep\" operation here) all the way back to the random initialization without hurting the performance while other layers are quite sensitive. For simple networks without residual connections, the bottom layers are sensitive and top layers are insensitive. It seems to be consistent with what is observed here. So I would like to see the experiment carried out with \"keep\" to different time step in the training course (similar to the setup mentioned above), and check whether the time step at PCP provide any unique behavior characterization or could the same observations be made at any time step.\n\n7. This paper uses artificially injected outliers to simulate long tailed examples. Can you discuss what can we possibly do to (maybe approximately) carry out similar analysis in real world data without artificial outliers? On the other hand, would any of the observations made here help us improve learning in practice, e.g. identifying outliers, deciding optimal early stopping timing, etc.?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Catching the Long Tail in Deep Neural Networks",
            "review": "###\nSummary:\nThis paper investigates the dynamics of deep neural network (DNN) training. It focuses on behavior exhibited when learning on frequent versus infrequent (long-tail) examples. Paper performs an empirical evaluation on MNIST and CIFAR10 and provides evidence that supports that:\n - DNNs learn pattern associated with frequent examples first, corroborating previous finding of (Arpit et al.).\n- DNNs allocate different set of weights to learn frequent and long-tail samples.\n- DNNs early layers are more likely to memorize examples, which was also illustrated in (Feldman & Zhang, 2020).\n\n###\nReasons for score: \nOverall, I think this is a borderline paper. I find interesting their experimental setting which focus on rare examples instead of noisy one. I also find the empircal observations interesting. However, I think some of the claims could be clarified or better supported (cf see below). In addition, I find that the experimental setting is a bit limited. It would be nice to ensure the robustness of the observations by running several seeds and show that the observations also scale to larger dataset and a more diverse set of architectures.\n \n###\n Pros:\n- Interesting setting (long tail examples).\n- Highlight interesting phenomena such as the allocation of different set of weight for frequent and infrequent examples.\n \n###\nCons: \n- Empirical evaluation is a bit limited: experiments are runs using only one seed, authors explore only MNIST and CIFAR10 datasets and only two architectures.\n\n- Authors claim that “we can associate weights that remain mostly the same as related to pattern learning and those that change as related to long-tail memorization”. However, in Figure 2 and 3, it seems that using the Keep strategy has a non-trivial impact on the samples in the frequent set. While this impact is bigger on the long-tail set, weights that  remain mostly the same still contribute in a non-trivial manner to the samples in the frequent set.\n\n- The connection between purge strategy and lottery ticket is unclear and could be clarified. Furthermore, no empirical data point is provided to back this connection\n\n### \nQuestions\n- Is the total number of training samples fixed for different percentage of long tail examples? If yes, increasing the number of infrequent examples change the distribution of frequent samples. It could explain why the efficiency of learning natural patterns is impacted, as some natural pattern might be rarer for dataset with higher percentage of infrequent examples?\n\n- Why does the difference in accuracy between the trained model and the model with the Keep strategy decreases on the long-tail set for high-percentage of long-tail samples (0.8)  ?\n\n- Could you elaborate on why Resnet-18 is learning patterns in a way that is more akin to memorization than pattern-learning? Could it be that the separation between weights allocated to frequent and long-tail examples is not as clear in this case?\n\n###\nMinor Comments:\n- Why figures 1 label are named  “delta 0.1” instead of just “0.1”?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting but not convincing viewpoint",
            "review": "##########################################################################\n\nSummary:\n\nThis paper studies the two different learning phases of the DNN models on the long-tailed datasets. It provides evidence that DNN models have two phases: mining frequent patterns and memorizing long-tailed instances. It also develops a method to find the Phase Change Point (PCP) where memorization is taking place the pattern learning. Based on the PCP and localization of weights, they found that memorization happens in the early layers rather than classification layers, and the longer-tailed dataset will reduce the efficiency of learning natural patterns.\n\n##########################################################################\n\nPros:\n\n+ This paper provides a new point of view  (yet, I don't think there is substantial evidence in this paper) to understand why the long-tailed problem exists in the DNN models. It is because the model can only memorize instances rather than learn general patterns from the tail categories.\n+ The phase change point (PCP) and the weight localization method defined in this paper provides a tool to analyze when and where the model starts to focus on tail categories.\n+ Some conclusions might be useful for future work in this field, e.g., it's the first few layers rather than the last classifier layers that memorizes the patterns.\n\n##########################################################################\n\nConcerns:\n\n- One of my greatest concerns is that the PCP seems not able to prove the existence of two phases, i.e., the pattern learning phase and the long-tail memorization phase. Because it cannot prove the tailed categories are memorized rather than learned. It only shows that the DNN model firstly learns frequent categories then learns tail categories after a few epochs.\n- The figures used in this paper are not clear, e.g., as you said, you use the first derivative (delta) of accuracy to find PCP, but in figure 1,2,3,4, what's the meaning of delta 0.1/0.4/0.8?\n- In Figure 2c and 3c, do you mean the percentage of convergent weights? Because I think DNN models tend to converge more weights when the training epochs increase. Why does the non-convergent weight increase instead?\n- Difference in accuracy is an ambiguous expression in the figure because it can represent either increasing or decreasing.\n\n\n##########################################################################\n\nReasons for scores:\n\nThis paper introduces an interesting viewpoint on long-tailed problems of DNN models. However, the figures and the experiment settings are not clear. Besides, I don't think this paper finds evidence for the existence of two phases as it claims. The conclusions are based on the assumption that the performance on tail categories are all achieved by simply memorizing instances, which itself needs to be proved.\n\n##########################################################################",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}