{
    "Decision": "",
    "Reviews": [
        {
            "title": "Handwavy motivation, technical issues",
            "review": "Many things are happening in this paper but the essence is to do a random search to compute the argmax in the greedy policy. That is, computing (approximately) $\\arg\\max_a Q_{\\pi_{\\theta_t}}(s, a)$ given $Q_ {\\pi_{\\theta_t}}$. The random search is performed by drawing actions around a nominal action using both normal and uniform perturbations. This nominal action comes from a network trained separately to \"armortize\" the search process. \n\nVarious notes: \n- bottom of page 2, transition dynamics: your notation implies that you consider only deterministic dynamics\n- page 3,  discounted cumulative rewards: the optimization problem is that of maximizing the expected discounted return, not the discounted return per-se.\n- page 3, \"learning the behavior policy\": your notation implies that you consider only deterministic policies. This cannot be the case in the usual policy gradient setting: we require stochastic policies.\npage 3, \"by timing up the Jacobian matrix\": I don't know what this means\n- page 4 \"which gives the descent direction\": is this truly a descent direction?\n- page 4 \"Proportion 1\" (proposition): I would like to see a proof of this. What happens if $Q_w$ is an approximation? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A well-written paper with clear motivations, but more clarifications are needed. ",
            "review": "This paper proposes a new algorithm: ZOSPI that combines the zero-order approach with a policy gradient based algorithm, to improve sample efficiency by increasing exploration. The paper is very well-organised with clear motivations and explanations. However, the empirical experiments on Mujoco tasks are not strong enough, and the bootstrap network part seems not very related to zero-order supervised policy without more clarifications. I'll explain my questions/concerns in the following four points:\n\n- The empirical evaluations on Mujoco tasks do not show a clear winner between TD3 and ZOSPI (ZOSPI is better on Walker while TD3 is better on HalfCheetah, and the performances on rest environments are pretty similar). Since ZOSPI is built based on TD3 with the twin delayed networks as shown in Alg2, these results cannot support the point that the global search helps. Actually I don't quite understand the second paragraph under Fig2, which argues for the success of ZOSPI under the high-dimensional setting; on both Ant and Humanoid, ZOSPI is not better than TD3, while they should be the same under the worst case that no global samples are useful; therefore I think these two results are discouraging for the effectiveness of global samples rather than supportive. \n\n- I'll use eq(4+) to represent the equation below eq(4). There should be more clarifications on section 5.1 about the choice of eq(4+) over the one from Agarwal et al., 2019. Could you elaborate more on why eq(4+) is the unbiased target for Bellman equation? It looks more like Bellman optimality equation. Besides, it will be challenging for Q(s,a), a~unif() to get large Q values when the action dimension is high, therefore I think on ablation study maybe helpful to demonstrate why eq(4+) is better than the one from Agarwal et al., 2019.\n\n- More details about how to combine Gaussian Process (GP) with actor should be added either in main paper or Appendix to make it more clear. Since the actor is a network with multiple output rather than a scalar, I'm confused how to naively apply GP under this setting. \n\n- The experiments on Four-Solution-Maze are not super convincing for me because it seems exploration would help significantly under this setting given all the four goals have same rewards. From the empirical performances, DDPG outperforms SAC and TD3 given the latter two explores more goals than DDPG. Besides, given the four goals have the same rewards, the only benefit of going to the nearest goal is because of the differences of the initial points (if using the same environment dynamic as Sec 5.1 in [1]), which can be overwhelmed by the noisy signals during training. I think a more persuasive setting would be giving higher reward to one particular goal, and show that ZOSPI and its variants can achieve higher cumulative rewards. \n\n\n[1] Haarnoja, Tuomas, et al. \"Reinforcement learning with deep energy-based policies.\" arXiv preprint arXiv:1702.08165 (2017).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insufficient Empirical Evidence and Novelty",
            "review": "**Summary**: This paper proposes a simple modification of deterministic PG actor-critic methods, where the actor update is modified to be non-local via random shooting. The resulting algorithm is competitive with SAC and TD3 on Gym benchmark tasks, and improves on a 2d navigation task.\n\n**Assessment:** It seems the core claim of the paper is that performing \"non-local\" policy improvement using ZOSPI should better \"exploit\" the value function, and thus lead to better solutions. Unfortunately, the experimental results provide little evidence that this is true -- ZOSPI does not really exceed the performance of TD3 or SAC on the Gym benchmark tasks (except for Walker2d). \n\nThe idea of \"non-local\" Q function optimization is also not specific to this paper. In fact, most stochastic PG methods (like SAC) optimize the Q-function \"globally\" (albeit still potentially converging to suboptimal minima), and in particular, the original Soft Q-Learning paper [3] has a similar zero-order method to ZOSPI for action selection.\n\nDue to the lackluster empirical results and novelty, in it's current form, I don't believe that this paper is ready for acceptance to ICLR.\n\n**Major Comments:**\n\n- The experimental results on the Gym tasks (except for Walker2d) appear unimpressive. Perhaps this is because TD3 and SAC have essentially \"solved\" the Gym tasks (and their hyperparameters are also heavily tuned for it), but in any case, the current set of benchmark results do not provide much evidence to support the hypothesis that ZOSPI actually improves on TD3/SAC. \n\n- There appears to be a general confusion about the capabilities and interpretations of policy gradient methods. I suspect that part of this is because the text uses PG interchangeably for true (stochastic) PG methods and deterministic PG methods.\n    - For example, the statement about local Q-function optimization made about policy gradient methods really only hold for deterministic PG methods, and does not necessarily hold for stochastic policy gradient methods. Stochastic policy gradient methods (e.g. SAC) roughly optimize $E_{a\\sim\\pi}[Q(s,a)]$, which queries the Q-function globally assuming that $\\pi$ has full support.\n    - The following statement is also made: that \"different from standard PG methods .... ZOSPI can be interpreted as sampling-based supervised learning\". Policy gradient methods have been closely related to supervised learning procedures in past work: for example, see [1] for a classic survey ([2] is a recent blog post on similar ideas)\n\n- One thing that could really help this paper is an experiment that shows that PG methods like SAC are indeed unable to globally minimize the Q-function -- this would help make the case for why something like ZOSPI would be necessary. As it stands right now, while I certainly agree that this issue may happen in theory, it is not clear to me that the phenomenon occurs in practice.\n\n- The original Soft Q-Learning paper [3] also sampled actions using a technique very similar to ZOSPI (a zero-order method sampling the Q-function). Comparing this to your formulation and algorithm would be enlightening. \n\n**Minor Comments:**\n\n- I think that Figure 1 is well done -- it conveys the idea of ZOSPI very clearly.\n- I found the Four-Solution Maze environment quite intriguing, especially how it captures the notion that many RL algorithms converge to clearly sub-optimal policies. The figure however is a little hard to read and understand without zooming in very closely.\n- It would have been nice to see the bootstrapped and GP algorithms explored in more details in the experiments, and not only on the one domain.\n\n**Typos**\n- \"Despite of their improvements\" -> \"Despite their improvements\"\n- \"virtue of its natural uncertainty capture ability\" reads awkwardly (Section 5.2)\n\n[1] Marc Deisenroth, Jan Peters, Gerhard Neumann. A Survey on Policy Search for Robotics\n\n[2] Benjamin Eysenbach, Aviral Kumar, Abhishek Gupta. Reinforcement learning is supervised learning on optimized data (BAIR Blog) https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/\n\n[3] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More analysis and rigour needed",
            "review": "**Pros**\n\nThe question this work addresses is interesting. I see potential in deriving actor-critic methods that do not rely specifically on a policy gradient update, rather relying on a \"self-supervised\" method of updating a policy towards specific actions or a distribution of actions.\n\nSection 4.1 clearly identifies the problem with methods that rely only on local information to update the policy. \n\n**Cons**\n\nThere is no discussion of how the reasoning in section 4.1 applies to estimated action values: is it actually important to do global optimization of the value function estimate if it is highly inaccurate? As far as I can tell, this pertinent question is not addressed in the work.\n\nThere is a lack of statistical significance in the experiments, which hampers answering the three claims at the beginning of section 6. The last plot in Fig 2 has uncertainties so large as to render the plot uninformative, undermining claims 1-2. More runs are needed (at least 20, preferably 30+, considering the high variance of RL; see Henderson et al., 2018). I also would have liked to see sensitivity plots to understand the robustness of ZOSPI to its hyperparameters (probably the most important are learning rate and sampling range).\n\nClaim 3 is not strongly supported either as the Maze does not seem to be a difficult exploration problem; it seems more to be a measure of maintaining probability mass over multiple goals. Evaluation on some more standard exploration benchmarks, like DeepSea, would have been helpful. \n\nExperimental details are lacking. Hyperparameter details are missing from the experimental specifications for both the MuJoCo environments and the Maze. I am not sure how a discrete maze accepts continuous actions. I don't think Fig 3 is that informative as the results could be radically different with different hyperparams and more seeds. I'm also unclear on what Figure 2 is actually measuring: is it the return averaged over a past few episodes? Is it an average of evaluation runs? More clarity is needed. \n\nThe theoretical analysis is lacking. I'm not clear on how local and global sampling should be balanced. There doesn't seem to be a proof for Proposition 1 (it's misspelled as \"proportion\"), and I'm not entirely sure what the proposition is saying; it seems to be saying something about the difference between the best and worst action as the number of samples goes to infinity?\n\nThe application of Gaussian Processes seems tacked on. I'm not sure of the value that GPs add; more space in the work needs to be devoted to it if it is relevant. \n\nEspecially given the discussion in section 5 about exploration with UCB and bootstrapped networks, I think there should be discussion included on whether one should even learn a policy. That is, why not just directly use the maximum action from globally sampling the action-value estimate?\n\n**Summary**\n\nWhile the problem setting is interesting, the lack of theoretical analysis and empirical rigour lead me to reject this work.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}