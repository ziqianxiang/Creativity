{
    "Decision": "",
    "Reviews": [
        {
            "title": "A learning rate adaptive method based on measuring distribution of singular values of layers",
            "review": "\nAdaptive optimisers (e.g. Adam and AdaGrad) offer fast convergence, however exhibit poor generalisation characteristics. Learning rate decay schedules mentioned in the paper have better generalisation properties, however, as stated by the authors: \"Unfortunately, little is understood about why such methods really work. It becomes more like alchemy rather than analytical/empirical reasoning.\"\n\nThis paper proposes an easy-to-use learning rate adaptive method which aims for good generalisation as well.\n\nI think the paper is very well written and easy to read. The knowlegde-gain function seems intuitively reasonable (measuring the 'importance' of the update by looking at the distribution of singular values of each convolutive layer).\n\nHowever, the reasoning for this method, feels a bit too heuristic as well. I have some critique regarding the two theoretical results given in the paper. Thm. 2 (convergence guarantee) seems like a bit too hasty application of the results given by Loizou and Richtarik (2020). For example, the meaning of the vector w^* included in the statement of Thm. 2 is not explained in the main text. In the proof given in the appendix, it is stated that \"w^* is some global optimal parameter set\". I think this would require more explanation. The proof of Thm. 1, given in Appendix, says that \"The condition strongly holds for the beginning epochs due to random initialization of weights ... By progression epoch training this condition loosens and might not hold. Therefore, the monotonicity of knowledge gain for p=2 could be violated in the interim process.\" I think here also more rigour is needed.\n\nThe experimental results seem good: the proposed method seems to beat state-of-the-art optimisation method by a small margin. However, this margin is not sufficient to convince me enough, taking into account the deficits in the theory.\n\nI believe the idea of knowledge function as presented in the paper is good. By careful rewriting and polishing of the theory, this can become a very good paper.\n\nsmall remarks:\n\n-I would suggest changing 'low-rank singular values' to 'singular values'\n\n-Remark 1 feels somehow out-of-place, could be in the appendix\n \nEDIT: The authors have answered many of my concerns and I think the revision has improved the paper. I raise my score by one.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting theories but limited applicability.",
            "review": "# Summary\n\nThis paper proposes Adaptive Scheduling (AdaS) for momentum SGD. The algorithm operates in roughly three stages:\n\n1. Optimize the parameters for $K$ timesteps using standard momentum.\n2. For each convolutional block, perform low-rank factorization of the block to compute the average \"knowledge gain\".\n3. Decay the learning rate globally by $\\beta$, then increment the learning rate of each block by the delta in knowledge gain between the previous iteration and the current iteration. This attempts to approximate via moving average the knowledge gain rate of increase over time.\n\nKnowledge gain across a channel is defined roughly as the size of the channel divided by the highest singular value in the channel, times the sum of all singular values. The authors approximate knowledge gain using a low-rank factorization of the true weights, and the AdaS algorithm itself uses a simple average of the input-channel knowledge gain and output-channel knowledge gain.\n\nConvergence results are provided, as well as an empirical analysis on CV tasks.\n\n# Strengths\n\nI really enjoyed reading the theoretical development leading up to AdamS in pages 1-5. It makes sense that we would want lower learning rates for ill-conditioned weight tensors and vice-versa, and this paper seems to propose a tractable method of determining that. The writing is generally clear and easy to follow.\n\n# Weaknesses\n\nThe empirics of the paper are not as comprehensive as most other \"optimization for DL\" papers in 2020, comprising CIFAR and Tiny ImageNet. Seed counts and measures of variation are not included. The reviewer is cognizant of the computational burdens involved with larger datasets but does not know the authors' computational budget. The authors are advised to include a \"computational details\" section in their appendix to outline their hardware resources and their software environment; as a reviewer, it helps me to understand how severely resource constraints are binding on experimentation. Given that \"knowledge gain\" is a novel concept, code would also help to understand the precise mechanics of the computation.\n\nThe comparison between AdaS and mSGD is somewhat unfair in the sense that the authors are sweeping over three values of $\\beta$ in Table 1, while only using one (canonical) configuration of mSGD. Despite this, most rows show only a marginal (if any) gain from AdaS. I'm left unable to identify the main point that the experiments are trying to make.\n\nIt's a bit weird to think about AdaS in the same context as Adam and cousins. This algorithm focuses on identifying a unique learning rate *per layer*, while Adam is all about per-parameter adaptation. The paper doesn't really discuss this difference which I believe deserves more attention.\n\nI suppose my biggest hesitation with this paper's empirics is that it focuses on image tasks. People in CV have predominantly been using non-adaptive algorithms, and the experiments in this paper do not constitute a compelling case for using adaptive algorithms in CV. Where adaptive algorithms have gained the most steam is in sequence modeling/NLP and reinforcement learning, but without comprehensive (or any) experiments in those domains it's impossible to assess the merits of AdaS in contexts where people are already using adaptive algorithms. (I understand that many models in these domains do not use convolutions, but there are perhaps some multimodal tasks or pixel-based RL environments that may be suitable as testbeds).\n\n# Overall\n\nI rate this paper a 4 primarily on the basis of an unclear utility for practitioners.\n\nUPDATE: I thank the authors for their response. I am still leaning toward rejection on the basis of the empirical weaknesses, which have not been fully addressed by the authors. Thus, I retain my original score.\n\n# Comments and clarification points\n\n- Please use \"Adam\" instead of \"AdaM\". The former is conventional and thus avoids confusion.\n- Relatedly, please cite the K&B 2014 paper upon the first mention of Adam in the introduction.\n- Capitalize \"Probing metrics\" on p.2\n- p.4: I don't see how Equation 3 as written actually normalizes the knowledge gain to fall in $[0, 1]$. Maybe something's multiplied that should actually be divided.\n- p.4: \"Note that we found the measure Gd on channels d = {3, 4} to be more statistically significant for training algorithm compared to the first (kernel height) and second (kernel width) dimensions.\" is vague. What is statistical significance in this context?\n- \"AdaS Introduced No Computational Overhead\" is almost surely an overclaim. The factorization step is *at least* linear time, which is the overhead accrued by Adam and cousins.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Writing and experimental setup need to be improved",
            "review": "In this paper, the authors aim to provide insight into the generalization characteristics of adaptive gradient optimizers using two metrics called \"knowledge gain\" and \"mapping condition\". A new optimization algorithm is also proposed, called AdaS, that adapts the learning rate during training by tracking the change in the knowledge gain across consecutive epochs. AdaS is compared with Adam, SGD and other optimizers on three different models.\n\nThis paper focuses on an important problem. Understanding the poor generalization of adaptive gradient methods for certain models is an important question to study.\n\nBut while the paper might have potentially good ideas, I found the paper quite hard to read because of the quality of the writing. There are plenty of statements made that I think are either vague, unclear, or sometimes simply wrong. To list a few examples:\n- \"It eventually converges faster and exhibits better generalization compared to vanilla SGD.\" - This is certainly not true in general (https://jmlr.org/papers/volume17/16-157/16-157.pdf) and needs to be made more precise.\n- Claiming that using momentum is a method to estimate the gradient, which is false.\n- Claiming that SGD with tuned learning rate schedules are universally used is also a false claim. For example, Adam is usually the optimization algorithm of choice for training GANs or transformers.\n- The authors frequently use the phrase \"well-posedness of convolutional layers\" without defining what it means.\n- Another phrase that's used without any explanation/definition is \"energy norm\" as far as I can tell.\n- The statement for theorem 2 needs to be much more thorough. The authors show linear convergence of AdaS, but I don't see any mention of the assumptions on the loss function/gradients. Further, the way the theorem is written, it is not easy to parse whether q(t) < 1.\n\nThe other major limitation of the paper is the lack of rigorous experimentation. As far as I can tell, the initial learning rates were swept only for AdaS, whereas default learning rates, which have appeared in previous work, were used for most of the other optimizers. This feels like an unfair comparison to me.\nIn addition, the optimal initial learning rates would vary quite a bit depending on what the epoch budget is. Therefore, it doesn't make much sense to me to compare the performance at the end of different epoch budgets while tuning the learning rate for a particular epoch budget.\n\nI would have liked experiments with bigger models, but even if doing those are not computationally feasible, there are additional questions that would be good to answer:\n- How is N_d calculated?\n- How sensitive is the performance of AdaS to the initial learning rates?\n- From the ablations on the beta parameter, it seems like performance can vary a lot with beta (from 2-4% in test set accuracy). AdaS, therefore, seems to have two hyperparameters (the initial learning rate and beta) to which performance is quite sensitive to. Is AdaS competitive with other baselines if all algorithms are provided the same hyperparameter tuning budget?\n\nI would encourage the authors to address some of my comments above, in which case I would be happy to increase my score.\n\nThere are a couple of typos.\n- Equation 3 -> It should be N_d' and not N_d?\n- Equation 7 should be eta(t+1) for this to make sense? The same change needs to be applied to algorithm 1 for it to make sense.\n\n-------\n\nUpdate after rebuttal: I thank the authors for their response. While the writing has improved, I think it still needs work to improve readability. I also do not really understand the \"beta is not a hyperparameter\" argument unless the authors can show that the optimal beta for \"speedy convergence\" or \"greedy convergence\" do not change for very different problems (and not simply ResNet image classification problems). I therefore am not changing my score.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Marginal Results",
            "review": "This paper proposes AdaS to mitigate the generalization degradations of existing adaptive methods. AdaS is motivated from two proposed notations, knowledge gain and mapping condition, which corresponds to something similar to stable rank and condition number in matrix, respectively. With additional hyperparameters introduced in AdaS, it does not improve SGD by much. \n\nOverall I do not think this paper makes enough contribution for an acceptance. In specific I have the following issues for the authors to address:\n\n- In abstract, \"new optimizer-AdaS-to...\". In LaTeX, you can use \"---\" to make an em-dash.\n\n- \"kth\" should be replaced by \"k-th\". Similarly for other notations.\n\n- Definition1. I believe you make some mistakes in Eq. (3) ??? To my understanding based on the following paragraph, this should be \n\\sum_{k=1}^N sigma_k     /    ( N * sigma_1  )\n\n- Also, I think your definition 1 and 2 are highly related to stable rank and condition number for matrix. I am not sure if there exist standard counterparts for tensor, but I would recommend to follow well-known inventions --- maybe with certain adjustments --- instead of inventing something totally new like knowledge gain and mapping condition.\n\n- Thm1. Is this theorem vacuous?? In Eq. (7) you already assume eta is positive, thus G (W^t) \\ge G (W^{t-1}) holds by assumptions. Do I miss something? Please explain.\n\n- Thm2. Again I find the assumptions not intuitive. Note that lambda_min can easily be zero or super small, then how can you assume a_1(t) + a_2(t) < 1 ??\n\n- Experiments. I do not see benefits of AdaS over SGD. (1) Based on Fig. 1, to achieve good generalization AdaS takes nearly the same amount of epochs to converge as SGD, thus it is hard to claim AdaS converges faster than SGD. (2) In CIFAR-10/100 experiments, AdaS can hardly outperform SGD. (3) In tiny-ImageNet, the improvement is less than 0.5% --- in my experience this can be easily achieved from other tricks like dropout, data augmentations, weight decay, etc. --- not to mention there are two more hyperparameters in AdaS. \n\nTo sum up, both theory and experiments do not sound satisfactory to me. I cannot suggest an acceptance for this paper.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}