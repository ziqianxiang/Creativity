{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work proposes a shortest path constraint for the reinforcement learning algorithm to improve efficiency in sparse-reward scenarios. The experiments are shown in navigation tasks in first-person maze and grid world. Reviewers found the idea interesting and the paper well-written but none of them championed the paper for clear acceptance. The authors provided a detailed thoughtful rebuttal. All the reviewers acknowledged the rebuttal followed by discussion. After considering rebuttal, review, and discussion, both AC and reviewers feel that experiments don't fully support and justify the algorithm. The main issue is that the results are shown only for the shortest pathfinding problems where the shortest path constraint is shown to be helpful. Hence, it is recommended to run it on diverse scenarios and standard benchmarks like the Atari games suite. Please refer to the reviews for final feedback and suggestions to strengthen the future submission."
    },
    "Reviews": [
        {
            "title": "A well written paper introducing shortest path constraint for sparse reward MDPs",
            "review": "**Summary**\nThis paper proposes a new constraint for constrained MDP, based on k-shortest path, which helps improve sample efficiency for (model-free) RL algorithms in sparse-reward MDP, while theoretically proving that the constraint retains the same optimal policy in the original MDP. Intuitively, for sparse (positive) reward setting, the optimal policy should reach the positive reward states with the shortest path (as it has the lowest discounting). The relaxed form of the constraint considers the that the distance between two states is less than $k$, rather than being optimal length (which the optimal policy still also satisfies). The constraint is then converted into its Lagrangian form as a cost term to the reward (i.e. a type of reward shaping). Practically, this requires a k-reachability network (RNet), which is a binary distance discriminator judging whether the distance between two states are reachable within $k$ steps. This network is trained with contrastive loss, similar to prior work SPTM by Savinov et al., 2018. However, SPTM uses RNet for graph-based planning (i.e. the local distance between states), while this paper uses RNet as the cost/constraint on the policy objective function. Experiments were conducted in several maze navigation environments (2D grid world MiniGrid, to first person 3D maze environments in DeepMind Lab), showing promising results compared to several baselines which use intrinsic curiosity. Several ablations were performed on the hyperparameters ($k$, and tolerance $\\delta t$ on the constraint), as well as some qualitative examples of the policies learned compared to novelty reward shaping.\n\n\n**Strengths**:\n- There has been a lot of work with goal-conditioned RL with sparse reward. The advantage of this paper is that this approach converts the notion of reaching goals into a constraint that is applicable to general MDPs. \n- The theoretical results (under some mild assumptions such as positive cumulative reward, mild stochasticity) appear to be correct, although I am not absolutely certain. \n- Empirically the experiments were designed well to study the effects of its hyperparameters ($k$, and tolerance $\\delta t$ on the constraint), and performs strongly as the task is harder (sparser reward / long trajectories) compared to its baselines\n\n**Weaknesses**:\n- It is still unclear to me about the potential performance gap between using an imperfect RNet versus an oracle ground truth distance discriminator. See my question below for some more detail\n- In the experimental sections, some sections are unclear about whether the RNet is used or not. For example, RNet is not used in 6.2 (MiniGrid), while is used in 6.3 (DeepMindLab) but required reading appendix to confirm. I am not sure for 6.4 and 6.5 whether RNet is used at all (I think it is not). Please clarify for me and in the paper. \n\n**Recommendation**:\n I recommend this paper a marginally above acceptance threshold. Overall I think that it is a well-written paper with thorough theoretical and empirical results. There are some clarification parts that can improve the paper even further.\n\n**Questions**:\n1. My main question is what is the performance difference between having the ground truth distance versus using the RNet? For example, can you compare the performance of 6.2 if RNet was used here? I understand RNet is not used in 6.2 for the purpose of understanding the objective. I would the oracle RNet would give the upper bound on the performance on SPRL, but how much worse is using RNet in those environments?\n2. Perhaps outside of the scope of this paper, but I am curious about how SPRL would perform if there was a curriculum on the $k$ and $\\delta t$ value, rather than a fixed value that is treated as a hyper parameter. Perhaps the authors have some intuition or have actually tried this as well. \n3. Please clarify about the use of RNet vs. Ground truth distance in the section 6.4/6.5. \n\n**After rebuttal responses**:\n\nI have read the authors’ response to my concerns, as well as the other reviews. I maintain my current evaluation with a weak acceptance of the paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting approach but might need to consider more diversified domains to justify its general applicability.",
            "review": "This paper proposed a k-shortest-path constrained reinforcement learning method for solving sparse reward MDPs. Under the assumption that the MDP is a single-goal task and by utilizing a novel cost function, the proposed method is demonstrated to outperform a few baselines in terms of sample efficiency.\n\nPro:\nThe technique along with its theoretical property are well developed.\n\nCareful numerical studies are provided to evaluate the effect of the k-shortest constraint in a tabular-RL settings  \n\nCons:\nIt is unclear how this method and its theoretical results can be generalized to other settings where the rewards are less sparse than single-goal MDP settings, but still very sparse.\n\nThe experiments for testing the proposed method are all based on navigation tasks, which give us an impression that the proposed method is only applicable to a specific problem. It might be better to consider a different type of domains.\n\nThe compared baseline methods are very limited and not necessarily ideal candidates for solving sparse reward problems. There are other approaches, such as hierarchical reinforcement learning which can also solve sparse reward problems , e.g., eigen-options [1] might also worth to compare. \n[1] Machado et al. A Laplacian Framework for Option Discovery in Reinforcement Learning, imcl2017.\n\nOther questions:\nIs there a convergence guarantee for algorithm 1? \n\nIt might be better to provide some computational complexity analysis of algorithm 1.\n\nWhat is the motivation of using k-reachability network to implement the binary distance discriminator?\n\n--------------after the rebuttal---------------\nI appreciated the authors' effort in addressing my comments and questions. I maintain my score of weak acceptance for this paper. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work",
            "review": "This paper proposes the k-Shortest-Path (k-SP) constraint to restrict the agent’s trajectory to avoid redundant exploration and thus improves sample efficiency in sparse-reward MDPs. Specifically, k-SP constraint is applied to a trajectory rolled out by a policy where all of its sub-path of length k is required to be a shortest-path under the π-distance metric. Instead of a hard constraint, a cost function-based formulation is proposed to implement the constraint. The method can improve the sample efficiency in sparse reward tasks and also preserve the optimality of given MDP. Numerical results in the paper also demonstrate the effectiveness of k-SP compared with existing methods on two domains (1) Mini-Grid and (2) DeepMind Lab in sparse reward settings.\n\nOverall, the paper is well written and clearly conveys the main idea and the main results of the work. The idea and motivation of the paper are very intuitive and very reasonable. The theoretical results are immediately following the ideas. The algorithm proposed has a clear structure and is easy to understand and implement. For experiments, the new algorithm consistently outperforms existing studies on a set of MDPs where there exists a goal state (as both the unique reward state and the terminal state). Some important discussions are highlighted to introduce the algorithm,. Moreover, the proposed mechanism seems to be an inspiration for future work considering the state space exploration related to sample efficiency.\n\nHowever, I wasn't fully convinced by the paper about relevance. Reinforcement learning aims to learn in an environment by trial and error without prior knowledge about the environment. As the problems considered by the paper are with episodic rewards (in both theory and experiments), *the problem themselves are shortest path problems*. Using the shortest path constraint to solve shortest path problems seems not fair to be placed among a set of learning algorithms. Armed with this prior knowledge, the algorithm outperforms marginally (though consistently) compared with pure learning-based algorithms, only with its best choice of k. I believe it would fall short if placed among search algorithms. Some strong justifications are needed for the work to be relevant.\n\nPros: \n1.\tThe paper considers a practical problem in reinforcement learning: sample efficiency in sparse reward tasks. The RL algorithm tends to fail if the collected trajectory does not contain enough evaluative feedback. The idea of using a constrained-RL framework and cost function to tackle the problem is natural and has been well motivated given some drawbacks in existing work mentioned in section 5.\n2.\tThe relaxation from the shortest path to k-SP is well explained. The novel cost function introduced to penalizes the policy-violating SP constraint can tackle some limitations of existing methods. For example, it can preserve the convergence and optimality of the policy. \n3.  This paper provides convincing numerical experiments to show the effectiveness of the proposed framework. The ablation studies are also helpful to show the effects of hyperparameters. \n  \nCons: \n1. The choice of k is not clear.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clean and intuitive method preventing over-exploration for sparse reward setting",
            "review": "The paper proposes a novel k-shortest-path constraint that prevents over-exploration by exploit the combinatorial structure (i.e. shortest path) of sparse reward tasks. It is proved theoretically that the k-shortest-path constraint maintains the optimal policy. Empirical evidence shows that the k-SP constraint indeed significantly improves the sample complexity over baseline algorithms in benchmarking environments. \n\nWhile I certainly agree that sparse reward tasks is important, one of the biggest challenges in sparse reward tasks is when the environment has a large state space. When the state space is finite and small, there is no doubt that many algorithms (even  algorithms with no deep learning method) can solve the task. However, for a large state space and sparse reward task (such as the robotics tasks in Mujoco environments[1], Montezuma's Revenge in Atari environments), I'm not sure whether the k-SP constraint is enough for efficiently solve the task. The k-SP constraint essentially tells the agent to avoid going to duplicated states. Even so, due to large state space, it is unlikely that the agent can observe non-zero reward without extra guidance. To be more specific, could the author(s) provide more discussion compared to HER (Hindsight Experience Replay) and hierarchical reinforcement learning?\n\nOverall, the paper is well-written, and the empirical results are clear. Therefore, I would recommend a weak acceptance.\n\n[1] http://gym.openai.com/envs/#robotics",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}