{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper is attempting to improve the OOD generalization performance of neural networks on relational reasoning tasks. This is an important failure point of general neural network architectures and important research topic. The results of the paper shows impressive improvements on a set of subject.\n\n* The paper is improved during the rebuttal, however, I do agree with the R5 and the paper is still lacking a lot in terms o clarity. The writing of this paper still requires some work. \n\n* As R2 also has written, the proposed idea is not so concrete to apply as practical solutions, and the presentation of the paper still requires some more work.\n\n* R3 pointed out some inaccuracies and it seems like authors have added some ablations in the direction that R3 has suggested.\n\nI am suggesting to reject this paper given that the majority of the reviewers are also leaning towards rejection as well. I would recommend the authors to improve the clarity of the paper, do more ablations for their models and resubmit to a different conference."
    },
    "Reviews": [
        {
            "title": "Good results and analysis but description requires work",
            "review": "This paper showcases three studies on relational reasoning where objects need to be compared on a certain attribute (like size). The experiments show large improvements in generalization performance over previous work. The authors attempt to formulate a general method from their experiments.\n\nMy biggest criticism is that the method needs to be described more clearly. There are lots of variations and add-ons from one experiment to the next. When you introduce notation, it is not clear what it maps to in your architecture descriptions (e.g. the choice of g and c). Is it fair to say that the main insight of the paper is the following: it is possible and desirable (in terms of generalization) to learn comparators between objects by feeding the difference between low-dimensional (or specifically one-dimensional) projections of the object representations, taken pairwise, into small MLPs?\n\nThe hyperparameters/architectural choices need to be summarized better. Could you perhaps make a table specifying the following for each experiment (I’ll refer to maximum of a set, dSprites, and RPM as experiments 1, 2, and 3 respectively):\n- the value of K (this is clear for experiment 1 but not others). Appendix E seems to suggest you use K = 512 for experiment 3. This is much larger than what I would expect.\n- the size of the low-dimensional manifold for each projection p. Is this always 1? Figure 10 seems to show 1 works best, which is a nice result. \n- the comparison summarizer g \n- the exact form of the comparator function c. Are you feeding in p(o_i) - p(o_j) into an MLP or |p(o_i) - p(o_j)| or something else altogether? This isn't clear especially for experiment 1.\n- how many objects are you ultimately comparing/learning a relation over. If I understand correctly, this is N, 2, and 8 for the three experiments respectively. Please do specify N.\n- what is the final output. This is clear for experiment 2 and 3. For experiment 1, I assume the output is a categorical probability vector with N dimensions? Or is it in fact the value of the maximum element?\n- what type of loss you are using. You defined a fairly non-intuitive loss L(d(a_t(o_i), a_t(o_j)), f(o_i, o_j)) in section 2.1. It is unclear where you’re using this.\n\nThe strengths of the paper include:\n- Solid results showing better strictly better generalization\n- Good attempts to analyze the method\n\nMinor: I do also take exception when you refer to your proposed comparator as “an inductive bias module.” (I realize there is no consensus on the definition of the term “inductive bias.” If we do define it–as something which changes the preferred solutions of a learning algorithm–then most deep learning architecture choices are inductive biases. So it is rather vacuous to refer to your architectures as inductively biased.) Moreover, an extra input (e.g. p(o_i) - p(o_j) which is fed to the comparator in your case) cannot be termed as an inductive bias, because it changes the solution space completely (consider learning the XOR function with a single linear layer when the difference is provided as an additional input versus when it is not).\n\nLastly, the inspiration from neuroscience is quite loose. The fact that human brains have high-level neurons which disentangle object position and size does not lead to your method directly; you could argue that all of deep learning attempts to learn higher-level features progressively with depth in the network. I would suggest revising the paper to focus on the insights, along with a clear description of the method, rather than the loose inspiration.\n\nQuestions and request for clarification:\n1) Could you please provide error bars for the RPM experiment? \n2) On your observations (section 3.4) answering why comparators on low-dimensional manifolds improve generalization:\n- I would suggest moving Figure 10 from the appendix to this section to answer why low-dimensional projects are important to learn good comparators.\n- I see that observation 1 holds for 2D projection spaces, but could you illustrate the linearity of the projection in 3+ dimensions? Alternatively, please change the phrasing to refer to 2D specifically to avoid overclaiming.\n- Under observation 3, your claim that the 2D comparator is “random [...] outside of the training points’ sub-manifold” in Figure 4b: in what sense do you mean that? Could you add similar plots over independent runs so the “randomness” is evident?\n3) I do appreciate your efforts to show systematic generalization. But your o.o.d algorithmic alignment metric is sloppy and arbitrarily defined. For instance:\n- What is V in the definition? I don’t see it defined or used anywhere in section 2.5.\n- What are the requirements on \\Tau, the truncation function? It seems to be constrained by \\beta and \\textbf{u} but this isn’t fully specified. I suppose you want to ensure the truncation function is not vacuous (e.g. the identity function)?\n- Are all dimensions in \\textbf{u} truncated by the same \\beta parameter? Why choose a subset of dimensions for truncation at all (rather than truncate all dimensions)? This needs justification. \n- How would the truncation work when the training data are images?\n- Given than D_s is only a truncation of D, it seems fair to say that the sample {x^s_i, y^s_i} has non-zero likelihood of coming from D instead of D_s. So strictly speaking, your definition of (M, \\epsilon, …)-learnability is not measuring o.o.d performance; rather it seems to be measuring performance on a “superset” of the training sample. Is this a correct reading?\n4) Your algorithmic alignment results (section 3.5) show that your comparator generalizes favorably over the baseline for various values of \\beta and the sample size. But what dataset are the scores based on? What is the truncation function? What is the baseline?\n5) Notation: could you please avoid overloading the use of \"a\" as the ground-truth mapping function (from an object's representation to its t'th attribute) as well as the attentional weights for pooling? Ideally, avoid defining the former unless you're actually using it.\n\nIn conclusion, I do consider this work substantial and important. I would be open to revising my score if the presentation is sharpened.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "initial review",
            "review": "Review: \nThis paper addresses an inductive bias for relational reasoning tasks to improve generalization performance on out-of-distribution scenarios (so called extrapolation), which the value ranges (continuous variables) or value itself (discrete variables) of the training/test dataset do not overlap. The proposed idea is to project high-dimensional representations onto low-dimensional space and compare those of the targets of relational reasoning. To validate effectiveness of the idea, the authors provide the improved results on three problems: (1) finding the maximum of a set of real numbers, (2) comparing the attributes of visual objects with dSprites dataset, and (3) visual reasoning with Raven progressive matrices (RPM) based on the PGM dataset. The experimental results of each  show better performance than other baseline methods. This research deals with one of important topics in AI/ML research. Out-of-distribution generalization is potentially extended to world models, common sense reasoning, learning and reasoning with small number of instances, and abstraction capability of human intelligence. As Section of related work in this paper also mentions these points, it is understandable to figure out the position of this work. \nOn the other hand, I think that the idea of projection and comparison is similar to the approaches of zero-shot learning and deep metric learning. So, the novelty of this idea is not so big and it would be better to explain them as related work. Considering the experimental results in Section 3.4, it is not sure whether the proposed method still shows similar efficacy regardless to the degree of difference between the training datasets and the out-of-distribution ones.  \nI recommend as ‘Marginally below acceptance threshold’\n\nPros:\n- In Section 3.4, it is interesting that the analyses of the projection space and low-dimensional embedding and why the dimension should be small. I like it.\n- The authors propose a generalized improvement strategy for relational reasoning tasks on the out-of-distribution scenarios, and they show that the key idea effectively improves the performance.\n\nConcerns:\n- The proposed idea is not so concrete to apply as practical solutions. I think it seems close to a guideline. If there is someone to use this approach, it needs lots of effort and time for the actual problems.\n- It seems that the distribution difference between training datasets and the out-of-distribution ones influences explicitly on their performance. Does the main claim still valid in that situation? What is the relationship between the difference and the validness?\n- How does the dataset for visual object comparison split? Overall, there are not much explanation on details of the configurations of comparative methods. It seems difficult to reproduce the results. \n- What is the baseline method of Figure 5 (b)?\n- Overall presentation is not so clear. That makes difficult to follow.\n\nMinors:\n- I think ‘Less defined’ is not so good expression.\n- Figure 4 (b) has no information on each axis.\n- The right end of Figure 5 (a) is covered by Figure 5 (b).\n- Figure 5 (b) shows poor readability.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper presents two neural network design ideas: low-dimensional projection and arithmetic comparator. By integrating these two ideas into CNNs, the model can solve a set of tasks that require recognizing 1-dimensional properties of objects (such as the size, the color, etc), and making a comparison of these properties. The authors show that their method has strong out-of-distribution generalization: such as generalization to larger objects.\n\nThe presentation of the paper is relatively clear. However, I believe the authors could make the paper stronger by strengthening their experiments with more results and ablation studies. Details follow.\n\n1. The numbers in Table 4 and Table 5 are suspicious. Can the authors verify that these two tables have exactly the same results?\n2. The last sentence of the first paragraph of Page3 says \"the vector distance p(o_i) - p(o_j) or absolute distance improves the generalization performance\". Are there any ablation studies for those?\n3. The sec 3.1 \"maximum of a set\" experiment is toy-ish: in some sense, only the proposed model takes comparison results as the model input, while all other models take numbers. The authors should consider other baselines that also take comparison results as their inputs.\n4. I suggest the authors discuss more the results in Table 2: what are the failure cases of the model? Also in Table 2, the author should make ablation studies to discuss the importance of 1) low-dim embeddings and 2) comparison operations. For example, what will the baseline perform if it also projects the input image into a very low-dim space?\n5. Table 3. I suggest the authors supply the performance on standard train-test splits for RPM as well. This will give us an idea of how the model performs on \"i.i.d.\" test splits. Also, what are the failure cases for Table 3?\n6. I have some concerns about the applicability of this proposed model. Learning meaningful low-dimensional embeddings can be hard. For example, how to extract the number of people in an image? Or in an even simpler task, how to count the number of vertices of an arbitrary polygon in an image? The authors should discuss the limitation of their method.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good idea, but the experiments don't really test the hypothesis",
            "review": "\"Projecting into a lower dimensional space, so that irrelevant dimensions are projected out, will help in extrapolation\" is an interesting hypothesis that should be tested. However, I was not convinced that the experiments gave convincing evidence for the hypothesis. It is standard in science to test a hypothesis by keeping as many of the extraneous variables as invariant as possible. However, in this case a different architecture (which projects onto a lower dimensional space) was used for each problem, and there was an impressive improvement. It wasn't clear to me whether it was the lower dimensionality or other architecture choices that made the difference.\n\nDetailed question. What is g in equation (2)? I can make sense if it if the output of f is a K-dimensional vector.  But then you say \"g is a function that combines....to make a prediction\" so. I interpret that as producing a value. So then f gives a number, but then I can't iWork out what is the input to the MLP in equation (2); I was trying to look for a, say,  d-dimensional input, but I can't work out what d is.\n\nThe results of finding the maximum (Table 1) are dramatic. Is it really just the lower dimensional embedding? I'm guessing that you can learn the correct hypothesis, but the others can't (or there are lots of hypotheses consistent with the data).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Why not low-dimension?",
            "review": "The paper proposes a simple low-dimensional projection and comparison module for extrapolatable relational reasoning. The proposed module first projects object embeddings into lower-dimensional manifolds, compare and concatenate them to form a vector, before finally making a prediction. The paper shows that on the maximum-of-a-set task, the visual-object-comparison task, and the IQ test, the proposed module, when combined with deep models, outperforms existing works in extrapolation.\n\nThe paper empirically investigates how the proposed module can improve relational reasoning in ood extrapolation tasks, which is considered an important aspect for deep neural networks. The paper is well-presented, providing intuitive sketches to ease understanding.\n\n------------------------------------\n\na. It looks like the major claim of the work is that low-dimension manifolds can help learning. But I wonder, for most of tasks, especially the representation used for classification, when will low-dimension manifolds hurt? Aren't the final layers of deep networks always low-dimensional? Low dimensions ease understanding, interpretation, visualization, and learning. When the true hidden representation is low-dimensional, why do we need to make it high-dimensional? I'm confused what the authors are trying to argue in this point.  \nb. From a technical point of view, the true lower-dimensional representation, if well-learned, will improve generalization without doubt. However, the work does not demonstrate the representation learned has this property. For example, if the authors are trying to say they learn the attribute, say size, can it be shown that learned low-dim representation is correlated with the actual size of it, independent of its position, during extrapolatory tests? And can the comparator be shown to act like a sign function. Especially in the IQ test problem? The first two tests look particularly simple, as one can even design transformations that show that. E.g., for numbers you can use identity transformation and do a subtraction and a sign function. For the black-and-white images, you can use a simple identity kernel to compute the number of foreground pixels.  \nc. The module design looks to be adopted directly from existing works like relational module, PrediNet, and CoPINet. Relational module and PrediNet have this object-pair and comparison idea, and the CoPINet mentions the contrast implementation. The low-dim comparator is a simple modification of the existing works.  \nd. While the argument is on extrapolation, I wonder what makes the module infeasible for interpolation. If the module learns well the true representation and knows what it means to compare, interpolation shouldn't be a problem either. Can it be shown that the representation is good for interpolation as well?  \ne. The authors, during experiments, use pre-trained models rather than training from scratch. It is possible that it's the pre-trained representation that helps the learning and extrapolation rather than the proposed low-dim projection? What if the models are not pre-trained?  \nf.  Finally, for Section 3.4. It looks like the low-dim representation is not well-learned. Test data representation is much clustered that the training data. Different ranges of training data are well-separated, but test data points are distributed across the test range. Also, for the comparator plot, response for training data is sure to be better than that for unseen data, but what kind of insight is there?  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}