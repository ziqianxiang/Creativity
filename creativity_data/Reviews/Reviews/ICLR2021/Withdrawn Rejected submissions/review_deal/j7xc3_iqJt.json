{
    "Decision": "",
    "Reviews": [
        {
            "title": "Mem2Mem vs vanilla attention mechanism vs other summarization frameworks?",
            "review": "The paper proposes memory-to-memory mechanism (Mem2Mem) for long-form summarization, which extends the encoder-decoder framework with encoder/decoder memory banks dynamically updated during text generation. Results show that a hierarchical RNN-based model with Mem2Mem is competitive with a Transformer-based summarization model in three different datasets.\n\nSome questions and comments:\n\n1. It is mentioned in the paper that one disadvantage of current models is the use of multi-step training or reinforcement learning. Why and in what aspect are these at a disadvantage? Is there a performance gap between these methods and the proposed one? It is hard to answer this question because there is no comparison shown in the paper regarding this statement.\n2. The paper emphasizes that the whole process of Mem2Mem can essentially be seen as extraction followed by generation. However, can't the same thing be said about attention mechanism in a hierarchical architecture, i.e. the attention weights can be seen as the probability of selecting the sentence. Isn't it more appropriate to say (in both memory and attention architectures) that the process is more ike a (soft-)clustering of the sentences into different topics?\n3. One advantage of the method mentioned in the paper is that it removes the necessity of using a pre-extractive stage, especially for longer documents. There are multiple frameworks that have been proposed before that replaces the extractive-then-abstractive summarization framework. For example, [1] has investigated on some of these frameworks, and in fact they showed results on PubMed and arXiv as well, which is significantly better than what is shown in this paper. The paper would be more convincing if it had shown these comparisons as well.\n4. Table 3 shows comparisons between unsupervised extractive methods and an extractive version of the proposed model using memory weights. In connection to question #2, how would using the attention weights to extract sentences perform compared to the proposed model?\n5. The paper provides human evaluation results using a subset of the arXiv dataset. They hired Amazon Mechanical Turk workers to read and annotate 20 random article-summary pairs. Based on the information given, there are a couple of issues. It is not ensured that the annotators are knowledgable enough to annotate research papers, especially since these papers tend to be very technical for a lay person to read easily. How are \"important points\" judged by the annotators when judging informativeness? How much time was given to read one very long paper and to judge its corresponding summary?\n\n[1] https://arxiv.org/pdf/2006.10213.pdf",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Mem2Mem Review",
            "review": "The paper proposed an abstractive summarization approach by leveraging external memory modules. The idea is to have a hybrid extractive and abstractive summarization approach without the supervision on the binary ground truth labels for the extractive step.\n\nThe method has a limited novelty with respect to the architecture of the approach. Memory networks have been already used in summarization (kim et al) and Benmalek et al previously proposed the application of external memories equipped with encoder/decoder methods for language generation and more importantly the model performance is not comparable with the state-of-the-art approaches such as PEGASUS ICML 2020  on the same datasets. So, considering that the architecture is not novel and having an implicit hybrid extractive and abstractive summarization with proposed architecture  cannot necessarily outperform the existing methods, I think the contribution of the paper does not seem sufficient.\n\nThey proposed an abstractive summarization approach by leveraging external memory modules. The idea is to have a hybrid extractive and abstractive summarization approach without the supervision on the binary ground truth labels for the extractive step.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A well written and well explained paper. ",
            "review": "The authors have clearly explained the paper and provided justifications about what problem they are solving. The paper proposes a Mem2Mem (memory-to-memory mechanism) for the task of abstractive document summarization. \n\nPros:\n* The work achieves comparable to better results even when compared with larger networks like GPT-2. \n* Ablation studies justifies the use of regularization methods over baseline HRED models. \n\nCons: \n\n* While the idea is well explained, the novelty of the paper is hard to justify. The work with memory networks and the design has been used in the past and this work seems like an extension of several works in the past. (Lin et al., Kim et al. ). Also, the comparison with Kim et al. MMN would be easier to understand the differences. Or fair differences needs to be pointed out. \n\n* The memory architecture seems like an engineering work over memory read/write mechanism for general seq2seq models proposed by Benmalek et al. It is sure an improvement over it, but lacks thinking from new angle. Would be interesting to understand the differences from the authors point of view. \n\n* \"The Mem2Memâ€™s memory compression can be generalized to other domains that require text generation guided by content selection\". No discussions has been done regarding this by the authors and the claims need to be justified with some proofs/experiments. It is unclear as of now how that is possible. \n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}