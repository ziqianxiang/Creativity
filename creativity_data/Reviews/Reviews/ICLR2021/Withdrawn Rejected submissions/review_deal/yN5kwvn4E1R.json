{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "All four reviewers expressed very significant and consistent concerns on this submission during review. No reviewer is willing to support this submission during discussion. It is clear this submission does not make the bar of ICLR."
    },
    "Reviews": [
        {
            "title": "This paper introduces a method that combines GCN and GAT to extract features from two views of each graph for semi-supervised classification. The framework makes sense in terms of learning comprehensive features, but the method is an incremental and straightforward development on existing methods.",
            "review": "This paper introduces a method on semi-supervised graph classification. For each graph, the method first constructs another view based on the cosine similarity between nodes' features, and from the two views (topology and feature similarity), GCN and GAT are applied to extract representations. All node representations are further combined via two layers of attentions. A diversity loss that encourages dissimilarity between the learned representations of GCN and GAT is introduced to the cross-entropy loss for joint optimization. The whole framework makes sense in terms of learning meaningful node representations for classification. However, the method lacks novelty, it is an incremental development on the existing graph neural networks. The choice of GCN and GAT as the building blocks are not well justified. It is also possible to try other kinds of GNNs. The statement on GAT \"ignores the inherent structure of the graph space\" on page 4 is confusing since it learns weights based on the graph structures. The experimental results show the better performance of the proposed method, but are not well analyzed. It may be better to compare with other multi-graph methods such as\nYu Shi, Fangqiu Han, Xinwei He, Xinran He, Carl Yang, Jie Luo, and Jiawei Han. \"mvn2vec: Preservation and collaboration in multi-view network embedding.\" arXiv preprint arXiv:1801.06597 (2018).\nAlso, it seems AM-GCN in the experiments also works on multi-view of graphs. The superiority of the proposed method compared to AM-GCN is not clearly described in the paper.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official review",
            "review": "This paper presents a dual complementary network framework for graph representation learning. Two graphs representing topology and features respectively are first constructed. Then, two branches leveraging the two graphs are proposed to explore different aspects of the original graph. Finally, a diversity loss is presented to capture the rich information of node features.\nTo me, the overall presentation is barely satisfactory, with many proposals not well-motivated. Also, the novelty is limited given the large body of existing work on exploring dual aspects of graphs. Moreover, the experiments are not convincing.\n\nDetailed comments:\n* The reason why two branches of GConv nets are used is not clear. I am especially not clear how the proposed DGCN differs from dual-channeled GAT, why the diversity loss is not employed on attention heads, and how does the embedding learnt by GCN supplement the information of that by GAT. More elaborations are needed.\n* Experiments are not convincing; the result analysis of this paper is rather superficial.\n  * Since DGCN uses two branches of GConv nets, large-scale datasets are necessary to evaluate the performance and efficiency.\n  * Inconsistency between GCN and kNN-GCN. It seems that on UAI2010, BlogCatelog, and Flickr kNN-GCN is significantly better than GCN, but the opposite holds for the other two datasets. It should be noted why the two methods show such different performance on different datasets.\n* Given the large amount of existing literature regarding dual networks, many related methods are missing. The authors should especially pay attention to network embedding techniques, e.g., [1].\n\nMinor:\n* Mathematical expressions are in chaotic forms, which makes the readability poor.\n* Page 5: CDAN -> DGCN?\n\n[1]\tZ. Meng, S. Liang, H. Bao, and X. Zhang, Co-Embedding Attributed Networks, in WSDM, 2019, pp. 393â€“401.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The work is incremental and straightforward",
            "review": "The paper presents a GNN model to jointly encode both topology and feature graphs to enhance node representations' quality. In particular, the model DGCN uses two GCNs to learn and propagate two different types of node representations on the topology graph, respectively. The model also utilizes two GATs to learn and propagate two different types of node representations on the feature graph, respectively. Finally, the model leverages attention mechanisms on these four types of node representations to produce the final node embeddings.\n\nPros: \n+ The model obtains promising results.\n\nCons:\n\n+ The motivation in the second paragraph of the introduction makes confusion. The quote sentence - \"Most of the traditional GNNs only consider the single connection between nodes and ignore other implicit information\" - leads to why not considering multiple connections between nodes as there are GNN works on hyper-graphs such as [2].\n+ The intuition in the third paragraph of the introduction is not clear as the paper does not have any ablation study for this intuition. \n\"Network performance is largely related to the quality of the graph, which usually emphasizes the relevance of an attribute of instances\", so which references for this intuition? What are the attributes of instances?\n+ Using v=1 for A_1 to denote the topology graph and v = 2 for A_2 to denote the feature graph makes the paper harder to read.\n+ The paper is not well written as it does not include any descriptions about model parameters in both the paper and the supplementary material. So it's hard to understand how to train DGCN and the baselines and how to analyze the model and ablation studies.\n+ The most important one is that, regarding the model architecture, DGCN is precisely similar to AM-GCN. In particular, DGCN changes from using two GCNs for the feature graph in AM-GCN [1] to using two GATs. Therefore, DGCN is straightforward and incremental (i.e., lacking novelty).\n\n[1] AM-GCN: Adaptive Multi-channel Graph Convolutional Networks. KDD 2020.\n\n[2] Hyper-SAGNN: a self-attention based graph neural network for hypergraphs. ICLR 2020.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Heuristic idea without much insight, unfair evaluation ",
            "review": "This work proposes a new method by combining GCN and GAT to perform node semi-supervised classification task. The new model uses node features to build another graph, uses GCN and GAT on the original graph and the new graph, and also adds a loss term to reduce the similarity of the final node representations.  \n\n1. The idea is very heuristic without much insight. The paper keeps arguing traditional GNNs only use one-side information, but they are actually leverage both node features and graph structures by propagating nodes features over the graph structure. So the statement is not correct. The paper claims that different node attributes contribute in different ways that should be sufficiently leveraged, but this is a very confusing argument if not paired with empirical justification. In the proposed model, it seems the authors try to resolve the above confusing issue via using another graph structure built only based on only node attributes. There is very unclear connection showing why this method resolves the problem they proposed. \n\n2. The experiment parts are not in a fair comparison too, as the paper does not use the standard way to perform dataset splitting. For this new splitting way, no hyperparameters are report for both the model here and previous models. Some benchmark datasets for semi-supervise learning are also not used, e.g., cora, pubmed,... \n\n3. There are quite a few errors in grammar. I suggest authors to perform some grammar checking. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}