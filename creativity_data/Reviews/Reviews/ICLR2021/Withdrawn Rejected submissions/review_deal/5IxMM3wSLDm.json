{
    "Decision": "",
    "Reviews": [
        {
            "title": "Contrastive learning for novelty detection",
            "review": "The paper considers the problem of novelty detection and proposes to utilise contrastive predictive coding for that. Moreover, the authors derive a novelty score based on contrastive likelihood function. Both training and novelty score is also augmented with additional rotation prediction of the image (loss and likelihood of it respectively).\n\nStrong points:\n* Well-written and easy to follow\n* Brings contrastive predictive coding to the novelty detection field (with caveats, see below)\n* Outperforms a range of baselines (with caveats, see below)\n* Combines ideas from contrastive learning and self-supervision (for novelty detection)\n\nWeak points:\n* Some important details are missing required for reproducibility (see below)\n* Some results raise questions (see below)\n* Rather incremental advance of the current literature, does not excite with radically novel ideas or unexpected combination of existing ideas\n\nDespite raised weak points, I recommend weak acceptance as the paper represents a good piece of work, it does provide better results that existing methods and it does present a novel (with caveats) idea of bringing contrastive learning + self-supervision to the novelty detection field. \n\nSupporting arguments (and mentioned caveats):\n1. “our work is the first to use CPC in the novelty detection setting” – it does not seem to be true anymore. There are at least 2 papers that are using contrastive training/learning for novelty/out-of-distribution detection. I appreciate that the papers do not seem to be published yet in a peer-reviewed venue and were released close to the submission date for ICLR 2021 and the authors might not have known about these works, but the claim does not seem to hold, at least some acknowledgements could be made: \nTack, J., Mo, S., Jeong, J. and Shin, J., 2020. Csi: Novelty detection via contrastive learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176.\nWinkens, J., Bunel, R., Roy, A.G., Stanforth, R., Natarajan, V., Ledsam, J.R., MacWilliams, P., Kohli, P., Karthikesalingam, A., Kohl, S. and Cemgil, T., 2020. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566\n2. The paper claims that “Results for all the metrics are copied from each corresponding paper” [regarding baselines’ results presumably]. However, for example, the original paper on DAGMM (Zong, B., Song, Q., Min, M.R., Cheng, W., Lumezanu, C., Cho, D. and Chen, H., 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations) does not seem to have results on Fashion-MNIST. Where then do the results for the baselines come from?\nIn general, comparison with other methods by copying the results from the original papers is questionable, because even the setting of the experiment with the same data may be different (e.g., which examples are sampled as novel), but more importantly, different papers would use different backbone architectures and training procedures. Therefore, in this case it is unclear whether the gain in performance is due to the proposed novel training procedure and novel novelty score, or just the consequence of the better architecture and optimisation set up.\nFor convincing comparison, I would suggest to take at least the closest model (for example, for CIFAR-10 it seems to be Rot + Trans) and set up training of this baseline and the proposed model as close as possible for fair comparison.\n3. No details on how a threshold was chosen for F1 score calculation in Table 4. Table 4 in general provides interesting results that worth further discussion. Why AUC is 1 (i.e. ideal), whereas F1 score is 0.999 (i.e. a little bit less than ideal)? Why the results of the proposed method are the same for all three very different settings?\n4. It would be better to see all results with +- std deviation with training with different random seeds as neural network training is rarely very stable\n5. It would be better to add further discussion on why the proposed method manages to deal with classes all the baselines struggle (8 and “airplane”)\n\nQuestions to the authors:\n1. Could you please clarify about baseline results that do not seem to exist in the original papers? \n2. Would it be possible to see a comparison to one of the strong baselines with close set up of both the baseline and the proposed method? On a small experiment\n3. Could you please explain the results in table 4? (see clarifying questions above)\n4. Do you have intuition / reasoning why the proposed method does not struggle with 8 and airplane as all the other baselines?\n\nAdditional comments/suggestions that may improve the paper but not critical for evaluation:\n1. Maybe to add into abstract some reasoning why the proposed methods utilises exactly rotations\n2. There is a mix in terminology about the problem setting in the literature, as some authors would call the setting in the paper – when we have a dataset of normal data – semi-supervised (e.g. Wang, S., Zeng, Y., Liu, X., Zhu, E., Yin, J., Xu, C., & Kloft, M. (2019). Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network. In Advances in Neural Information Processing Systems (NeurIPS 2019)). While unsupervised setting would be when a training dataset may contain abnormal samples and we do not know the labels for those cases. Although, the terminology used in the paper does exist, it would be better to acknowledge existence of this latter setting.\n3. The first line in section 3.1. I would suggest to change $X \\tilde p_{normal} (x)$ to $x_i \\tilde p_{normal} (x)$. Otherwise, strictly speaking $p_{normal}(x)$ is defined as a distribution of N-dimensional random variable\n4. Although min-max normalisation is used in other novelty scores (e.g., Park, H., Noh, J. and Ham, B., 2020. Learning Memory-guided Normality for Anomaly Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 14372-14381)), there is a problem in this approach such that one would guarantee to find an anomaly at least once in the test set (for the sample that maximises the unnormalised score). In this case though, it probably mitigated a little bit as a sum of two normalised scores is used and one may hope that the sample that maximises one unnormalised score would not maximise the second unnormalised score as well. It would be good to have a small discussion on this in the paper\n5. It would be good to add the note that protocol 1 is used for COIL-100 because the original dataset does not have train/test split. Or if there is another reason for that, then it would be good to state it\n6. It would be better to see all results with +- std deviation with training with different random seeds as neural network training is rarely very stable\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nThe paper proposes a novelty detection algorithm which uses contrastive predictive coding (CPC) and a pretext task to obtain a novelty score of a sample.\n\nStrength:\n\nThe paper provides a fairly extensive summary of related literature and attempts to include important baselines in the experiments.\n\nThe core idea of the paper is clearly presented.\n\nWeakness:\n\nThe overall novelty of the paper seems marginal. The model architecture is almost identical to CPCv2, and the pretext task of predicting rotation angle is used multiple times in previous work.\n\nThe paper does not provide good intuition behind the proposed approach. The first paragraph on page 5 provides such an explanation which is done in a very abstract level without concrete experimental evidence.\n\nThe introduction states that \"the intuition is that the representations learned with normal data should be less [predictive] of the future for anomalous data.\" However, this statement may not be true considering empirical evidence currently known to the community. PixelCNNs model a distribution of images by predicting future pixels autoregressively, but are known to assign unreasonably high likelihood to obvious out-of-distribution samples [2].\n\nThe novelty score should be computed independently for each test data. In equation 5, the proposed novelty score contains normalization operations over the test set, resulting in the dependency on the whole test data. This means the novelty score may vary upon the choice of the test set and is not a desirable property of a novelty score function.\n\nThe experiments section only includes one-vs-all settings of novelty detection and is far from being extensive. I encourage the authors to consider performing (at least a subset of) the following experiments:\n- Hold-out class detection: Instead of setting a single class as the normal class, use a single class as the novelty class and use the rest of the classes as normal. For example, use nine MNIST digits from 0 to 8 as normal data, and use digit 9 as novelty data. This setting is usually more difficult than the one-vs-all setting.\n- Out-of-distribution detection: Test whether novelty detection algorithms can discriminate normal data distribution from a different dataset. For example, test whether the proposed model trained on CIFAR-10 and differentiate between CIFAR-10 and SVHN. It is recently reported that deep generative models fail to discriminate certain pairs of datasets which are  obviously different from each other.\n\nMinor comments:\n\nAs the CPCv2 architecture itself is not the contribution of the paper, it could be illustrated in a separate section, such as preliminaries or background.\n\n[1] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).  \n[2] Nalisnick, Eric, et al. \"Do deep generative models know what they don't know?.\" arXiv preprint arXiv:1810.09136 (2018).",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper proposes a method for novelty/anomaly detection that combines CPC and rotation prediction as training objective, and use the objective as a proxy for novelty/anomaly score. The paper compares with both reconstruction-based and non-reconstruction-based baselines and show some improvement.\n\nPros:\n+ the paper is relatively well written and easy to follow.\n+ the method looks pretty simple (straight-forward), though there's still some complexity at combining both CPC and rotation prediction.\n+ compared to reconstruction based methods and limited non-reconstruction based methods, the proposed method performs well.\n\nCons:\n- the proposed method is a naive combination of two well known methods, in the sense that it looks pretty arbitrary/ad-hoc, and it's also not well motivated. Why CPC (among other contrastive methods)? Why rotation prediction? Why does it help to combine them? Why not combining more methods?\n- the baselines are somewhat sparse in non-reconstruction methods, missing some of important recent work, e.g. Tack, Jihoon, et al. that achieved a significantly better result.\n- the results are also not comprehensive enough, other than knowing the proposed method has somewhat numerical improvement, there's little understanding/insights gained from the proposed method and experiments.\n\nOverall, I think this paper does not meet the bar in its current form.\n\n--\nTack, Jihoon, et al. \"Csi: Novelty detection via contrastive learning on distributionally shifted instances.\" NeurIPS 2020",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}