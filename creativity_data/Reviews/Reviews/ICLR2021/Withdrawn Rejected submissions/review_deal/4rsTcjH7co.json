{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose a technique called Autoencoder Adversarial Interpolation (AEAI). The key idea is to train autoencoder architectures that explicitly \"shapes\" trajectories in the encoder (latent) space to correspond to smooth geodesics between data points. This is achieved by a combination of several loss terms that are fairly intuitive. The authors empirically justify each term via ablation studies on simple datasets.\n\nInitial review scores had wide variance. The reviewers liked the overall approach as well as the clarity with which the theory and experiments were presented, but raised several concerns. The authors provided succinct responses that seem to have satisfied the reviewers on average.\n\nUnfortunately, after having carefully read this paper (and the authors' responses), I have to go against the wishes of the majority of the reviewers, and recommend a reject. My two main concerns are as follows:\na) The synthetic pole dataset, as well as the COIL-100 dataset, are far too simplistic to evaluate performance. It is now standard to report results on considerably more challenging datasets.\nb) Echoing R2 -- the authors should articulate why a shaped latent space should actually matter in applications, beyond giving intuitive(I guess?) visualizations and reconstruction error curves. Results on downstream tasks may be one avenue to achieve this."
    },
    "Reviews": [
        {
            "title": "Interpolating Autoencoder",
            "review": "This paper focused on developing a new regularization technique for autoencoders, which shapes the latent representation to follow a manifold that is consistent with the training images and that drives the manifold to be smooth and locally convex. The authors suggest that the manifold structure of continuous data must be considered to include the geometry and shape of the manifold. The new interpolation regularization mechanism consists of an adversarial loss, a cycle-consistency loss, and a smoothness loss. So the architecture of the proposed model includes a standard autoencoder, a discriminator and the loss mentioned above. \n\nStrengths: The motivation of using four losses were clearly described and the architecture of the model was straightforward. The authors tested the proposed AEAI method on one synthetic plot dataset and COIL-100 dataset and compared the results with three other models. The effectiveness of the proposed technique is tested by both visual inspection and comparing the reconstruction error against the available ground-truth images. The authors also examined the transition smoothness from one sample to the other on both datasets. The comparison of the alpha values showed that the proposed technique did generate more smooth results than other methods. By visual inspection, the results looked pretty good for the proposed technique while other interpolations methods change abruptly between modes and introduce smallartifacts during reconstruction. The evaluation of MSE also showed the proposed technique achieved lower MSE. In general, I think the analysis of the proposed technique was convincing, and the authors also provided ablation study with smoothness and cycle-consistency losses missing, showing the necessity of adding these losses.\n\nWeakness: The proposed model is a mixture of known techniques, without a unifying theme. The authors can try to compare the latent space directly with manifold learning techniques or autoencoders that directly penalize using manifold learning. One example is the recent paper at ICML https://arxiv.org/abs/2002.04881 (Chen et al. \"Learning Flat Latent Manifolds with VAEs.\" ICML 2020) or https://www.sciencedirect.com/science/article/abs/pii/S1063520317300957 (Mishne et al, Diffusion Nets, Applied and Computational Harmonic Analysis.\") The authors also don't quantify the manifold learning capability of the latent space. A potential metric for this is the Denoised Manifold Affinity-Preservation Metric (DeMAP) proposed in Moon et al., Nature Biotechnology 2019. I would also suggest visualizations of the latent space using the PHATE technique from the same paper.  \n\nI also feel that the authors should try a more irregular/noisy dataset to further show the effectiveness of the model. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper has the potential to be a great paper but in its current form would really benefit from more results and another pass of writing",
            "review": "# Summary\nThis paper proposes new regularization terms (smoothness and cycle consistency) for more realistic latent interpolations in auto-encoders. It is experimentally tested by measuring interpolation error on two datasets: a new custom pole dataset introduced by the paper and the COIL-100 dataset.\n\n# Pros\n1. The method feels intuitively motivated and makes sense.\n2. The results appear convincing compared to the (limited choice of) baselines.\n3. Ablation study helps comprehend the contribution of each loss term.\n\n# Cons\n1. No direct comparison to state of the art baselines, namely GAIA and AMR.\n2. Results only on artificial tasks. For example, there’s no measure of the effects of the better conditioned latent space on downstream tasks such as a classification on SVHN or CIFAR10 like some other works in the domain checked.\n3. Novelty is not clearly identified: a good place would be the end of section 1, e.g. “Our contributions are …”. I deduced it was the cycle consistency loss (which seems inspired from cycle GANs) and smoothness loss since the other loss terms look like what previous techniques already do.\n4. Ablation should really be in the main section of the paper since it’s important. On the other hand Figures (3) and (4) which didn’t really add to my comprehension could be removed to save space.\n5. Without the appendix the paper is lacking essential information.\n6. The bibliography seems to be lacking (see questions and nits)\n\n# Questions and nits\n1. “This regularization ... can also be used as a general regularization technique to avoid overfitting or to produce new samples for data augmentation.” It would be good to see it demonstrated in results.\n2. In several places, you cite auto-encoders but really refer to variational auto-encoders, not crediting earlier works.\n3. “Researchers have demonstrated the ability to interpolate between data points by decoding a convex sum of latent vectors (Shu et al., 2018) ...”  I believe there’s prior art before that, for example, just to name one: https://arxiv.org/abs/1611.03383 \n4. “... interpolated points to look reliable as it is optimized ...” In several places the word “reliable” is used, I suppose you meant “realistic”. If not can you clarify what it means?\n5. “... while providing a convex latent manifold with a bijective mapping between the input and latent spaces.” It seems counter intuitive to me, the input space is much larger than the latent space, how can it be bijective? In addition you later introduce $L_c$ the cycle consistency loss which tries to map $f(g(z_{i,j,\\alpha}))$ to $z_{i,j,\\alpha}$, in other words it looks surjective only. I understand that perceptually you want $g(f(x))$ to be close to $x$, but they are not the same, or are they? If you are referring to the mapping from the latent space onto itself by $f(g(.))$, then the term `identity mapping` would seem clearer to me.\n6. “Credibility” I don’t understand why it is written in such a complicated manner. Why not simply write $P(\\hat{x}_{i,j,\\alpha}) \\geq 1 - \\beta$ for a constant $\\beta \\geq 0$. \n7. Concerning the smoothness loss $L_s$ it seems to be derived from the k-Lipschitz constraint (3). But in its implementation it’s really 0-Lipschitz, it would be great to clarify this aspect. \n8. In addition, concerning $L_s$, the formulation seems odd. I’d expect it to be written as $\\sum ||\\frac{d\\hat{x}_{i,j}(\\alpha)}{d\\alpha}(n/M)||^2$ since in its current form $\\alpha$ does not appear in the numerator.\n9. One last thing concerning $L_s$, it would be good to give the reader a simple explanation: if I’m correct you’re simply minimizing the pixel distance between successive images on the interpolation line. That would be really helpful to state it clearly - assuming I understood correctly.\n10. “2.2 JUSTIFICATION FOR THE PROPOSED APPROACH”. I found it was already justified by the intro to section 2 for the most part. One exception is the justification for the cycle consistency loss L_c which was not covered in the introduction of section 2. So you could simplify/remove this section by moving it to section 2.\n11. On the topic of L_c, it is justified as “the cycle-consistency loss L_c forces the encoder-decoder architecture to map linearly interpolated latent vectors onto the image manifold …”. I don’t see why it should. As I understand it, the loss term itself only seems to say that it should map to an image that projects to the same latent representation. This could use clarification.\n12. In the comparison to other methods, did you use the same number as training examples? I’m asking this question because the loss term L_s has an implicit batch size of M which could lead your method to see M times more samples than the methods under comparison (in particular AAE and ACAI).\n13. “The two images (xi, xj) are encoded by the shared-weight encoder…”. I didn’t follow what the weights are shared with.\n14. The loss terms are called $L_{a,c,s}$ but the weighting hyper-parameters are called $\\lambda_{1,2,3}$. Using the same letter as their corresponding loss term instead of 1,2,3 would make reading more friendly.\n15. “The GAIA method of Sainburg et al. (2018) is similar in spirit to the AMR framework.” The GAIA method came first, so this is the other way around (B is similar in spirit to A, if B came after A). Actually it should probably be presented before AMR. \n16. “but also ensure a diverse generation ... while avoiding mode collapse“. How does it ensure it? I didn’t really get what loss terms directly prevented modal collapse nor diversity? Or if the effect is indirect, could you give more insights on what causes it?\n17. The results section comes just after Related Work and it’s striking that there’s no comparison to the two methods yours is most similar to and that were covered sentences ago. \n   1. I understand you claim that “Comparisons with AMR and GAIA methods (Beckham et al., 2019; Sainburg et al., 2018) are analogous to the ablation study presented in the Appendix, where the smoothness and cycle-consistency losses are missing.” but it’s not clear whether you ran the experiment and observed it was indeed analogous or whether it should be analogous.\n   2. In addition, in the appendix, I didn’t see a curve in the ablation study with *both* smoothness and cycle-consistency loss removed.\n   3. Not presenting this result with the other methods and putting it in the appendix gives an incomplete and possibly misleading impression to someone quickly eyeballing results like many readers tend to do. I think it is highly detrimental to the results presentation.\n18. On the COIL-100 dataset, it is not clear whether during training you used interpolations only between objects of the same class or between classes as well.\n19. “Results on other datasets can be seen in the Appendix.” What other datasets? I seem to only see results for COIL-100 and the pole dataset.\n20. In addition it would be interesting to see what happens for interpolations between classes.\n\n=====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. I understand the authors view that some of my queries fall outside their desired scope for the paper, however I still think the paper could benefit from such contents.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clearly presented with compelling results.",
            "review": "## Summary\nThe paper presents a method of regularising the latent space of an Autoencoder in a way that pressures the data manifold to be convex. This allows interpolation within the latent space which does not leave the data manifold and results in realistic reconstructions as one moves from one point to another.\n\nThis is done through the introduction of adversarial and cycle-consistency losses, over and above the usual reconstruction loss and a smoothness loss. The adversarial loss ensures that the interpolated reconstructions are realistic, while cycle-consistency encourages a bijective mapping.\n\n## Quality & Clarity\n\nThe paper is clearly written and without typographical or grammatical errors. It is structured logically and the authors' arguments are easily followed.\n\nThe results are compelling and the proposed AEAI technique clearly outperforms other methods in the qualitative experiments, with quantitative results to substantiate it.\n\n## Originally & Significance\n\nThe contribution of the paper is clear in that it imposes convexity regularisation to the latex space. The approach is compared with modern competing techniques and the work is well positioned among recent literature in the field.\n\n## Outcome\n\nThis is a clear, high-quality paper with compelling results.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper, a bit limited contribution ",
            "review": "This paper introduces several autoencoder (AE) regularization terms that aim at reproducing continuous realistic deformation by interpolating latent codes of images. The authors assume there is a continuous process generating the data and introduce three novel loss terms (in addition to the standard AE reconstruction loss). The first term is a GAN loss for decoded interpolated latents $\\hat{x}(\\alpha)$ where this terms makes sure the interpolated latents are decoded to images similar to the train images. The second term is called cycle-consistency and enforce injectivity of the decoder. The last term is enforcing smoothness of the decoded image as a function of the interpolated latent. Combining these three losses with the original loss leads to natural interpolations of latents that enjoy both smoothness and realism.  The method is tested on a synthetic \"pole shadow\" example, and COIL 100. The method seem to improve upon several baselines on these datasets. \n\nMore details:\n\n- The method is simple and able to improve latent interpolations. I think the choice of losses and properties they enforce is well explained.  The results are convincing and seem to provide a good arrangement of the latent space. \n- In terms of contributions: the incorporation of a discriminator loss in training of AE to provide more realistic interpolations was done before, as the authors acknowledge (e.g., Beckham et al. 2019). This diminish some of this paper's contribution. However, as shown in Figure 12, discriminator loss alone does not solve the continuous latent problem.\n- In Figure 12, I cannot see the benefit in the smoothing term, that is, top-left and bottom-right images look almost identical to me. Can you state if there is some difference?  If not, did you encounter a benefit in incorporating the smoothness term somewhere else? \n- Are the corners in the square interpolation grids (e.g., Figures  5, 9, 10, 11, 12, 13) train or test examples? How does grid interpolation of test examples look? The last question is interesting both for in-distribution test examples, as well as out-of-distribution test examples. \n- How does the discriminator $D$ is trained? Do you use standard GAN loss? Missing info here. \n- How do you set the different parameters $\\lambda_i$ of the model?\n- How is the loss $\\mathcal{L}_S$ approximated? Do you use automatic differentiation, numerical differentiation? Do you use stochastic approximation?  \n- Figure 8 was not clear to me.\n- Why L1 distance of images is used in 4.2 and L2 in 4.3?\n- Minor: typo \"qualitatively and qualitatively\" ; distorted text in Figure 6\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}