{
    "Decision": "",
    "Reviews": [
        {
            "title": "A practical paper not supported by any theory",
            "review": "In this paper, the authors present a framework that translates images from various data sources to a unified representation. The goal is to establish a preprocessing step that translates data from different sources and of different modalities (real, synthetic) into a unified representation that can then be used to train networks for computer vision tasks. Since the generated image has a unified representation that is less complex, the downstream network can be trained with a less complex architecture. The image-to-image translation approach combines a state-of-the-art semantic segmentation algorithm with an edge detection algorithm to generate edge-plus-segmentation maps from arbitrary input images. A GAN uses the EPS maps to generate output images, which makes the approach to be agnostic to the style and details of the input images The proposed method has other bells and whistles such as using a progressive learning scheme to achieve high-resolution output images and reduced training time, ablation studies, etc.. The generator trained using YouTube videos is let loose on multiple datasets.\nThis paper heavily builds on existing literature. The novelty is low. Using edge plus segmentation steps to produce the intermediate representation which then guides the image generation process is interesting but lacks any theoretical claims. The term domain adaptation in the title does not truly reflect the current state of the art on domain adaptation. Also, just using edge + segmentation does not justify the term morphology. Morphology has more sophisticated implication tan used here.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Practical Idea but Limited Novelty",
            "review": "# summary\n\nIn this paper, the authors propose an intuitive and practical domain adaptation pipeline, leveraging morphologic segmentation as the intermediate representation to bridge the gap between input and output domains. Then GAN-based image2image translator could be introduced to render the same content in different low-level styles, which is helpful to train the source model with the samples in the target domain style. In this way, morphology segmentation could be a common representation to unify the samples from all different domains.\n\n# pros\n\n1.  The overall writing of this submission looks good to me. The storyline is well-motivated. The authors provide a lot of details of the proposed pipeline, which should be helpful for the reader to reproduce this practice.\n\n2. The ablation studies with both qualitative and quantitative results provide plenty of interesting comparisons. Specifically, figure 5/6/7 demonstrates the design choices with the corresponding effects in a visually straightforward way.\n\n# cons\n\n1. I wonder if the authors would like to evaluate their method in several established domain adaptation benchmarks. For example, the proposed pre-processing step should be easily applied to the specific problem, such as unsupervised domain adaptation for semantic segmentation. For example, the authors may want to follow the synthetic2real setting of FCNs in the wild [1]. Without the domain adaptation experiments for the downstream recognition task, it could be hard for the reader to determine how useful the morphologic segmentation is.\n\n2. The concept of morphologic segmentation as a universal representation looks practical but not novel enough for a scientific research paper. On the one hand, edge+segmentation is not a fundamentally different proposal compared to contour+segmentation. On the other hand, utilizing a semantic structured map as the bridge has already been explored previously.\n\n> P.S. The latex layout of this submission looks strange to me. The authors may want to use the original template if possible.\n\n# reference\n\n[1] Hoffman, Judy, et al. \"Fcns in the wild: Pixel-level adversarial and constraint-based adaptation.\" arXiv preprint arXiv:1612.02649 (2016).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lack of novelty and experimental verification for the benefits on the downstream tasks.",
            "review": "This paper proposes to tackle the domain adaptation problem by projecting all the images from different domains into a uniform output domain. The main idea is to extract the semantic segmentation and edge maps (based on DeepLabv3 and DexiNed-f networks respectively) of input images to produce the so-called edge-plus-segmentation map (EPS), where the domain-specific texture/appearance information is therefore removed, then use pix2pixHD image translation model to turn such EPS map into a photo-realistic output image (i.e. the uniform output domain). There are some clear issues of the proposed method:\n\n- As the semantic segmentation and edge detection models used in the proposed method for producing the EPS map are trained on other datasets, there is no way to guarantee that resultant quality of these two models on any arbitrary datasets. Once there exists errors on the EPS map, the image translation results based on the pix2pixHD will be unable  to well maintain the semantic segmentation and edge information of the input images thus likely leading to problems for the downstream tasks. \n- There is no comparison with respect to other domain adaptation methods on the downstream tasks (e.g. classification, semantic segmentation) for showing the benefits and the robustness of the proposed method. \n- The idea of extracting the semantic and edge information from the images from different domains are actually not new, there are already quite some related works attempting to maintain the domain-invariant \"content\" information for performing the downstream tasks, e.g. Kim et al., Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation, CVPR 2020 and Chang et al., All about Structure: Adapting Structural Information across Domains for Boosting Semantic Segmentation, CVPR 2019. It is better to have thorough investigation on the related works from high-level perspectives.  \n\nIn brief, as there is no significant novelty, lacks of proper experiments, and exists clear flaws of the proposed method,  this paper clearly does not reach the acceptance standard of ICLR.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lack of convincing results and limited novelty",
            "review": "The paper proposed a framework using morphology and semantics to unify representations in different domains. \n\n\nStrengths:\n1.The introduction of the EPS map as an intermediate representation is new.\n2. The paper is well written and easy to read.\n\nWeakness:\nThe major concern is the limited contribution of this work. \n1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2].\n2. The use of morphologic information in this paper is simple as the combination of edge detection and segmentation, which are both employed as tools from existing benchmarks (in this paper the author used DeeplabV3, DexiNed-f, employed as off-the-shelf tools for image pre-processing purpose as mentioned in section 4).\n3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didnâ€™t provide insight into this but assumed morphologic segmentation will be invariant.\n4. Results compared to other domain adaptation methods (especially generative methods) are missing. There is an obvious lack of evidence that the proposed method is superior.\n\nIn brief, the contribution of this paper is limited, the results provided are not sufficient to support the method being effective. A reject.\n\n\n[1] Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation\n[2] Image to Image Translation for Domain Adaptation\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Unclear motivation and insufficient evaluation",
            "review": "**Summary**\n\nThe paper proposes a framework to adapt images from various visual domains to a single domain using image-to-image translation methods. Instead of training the image-to-image translation model from each input domain to the output domain, the paper proposes to first convert the input domain into an Edge-plus-Segmentation (EPS) map followed using off-the-shelf edge detection and semantic segmentation models. One can then use the EPS map as input for an image-to-image translation model to convert it into a photorealistic image.\n\n**Strength**\n+ Training an image-to-image translation model often requires diverse samples. Using the proposed EPS map, one can train the model using samples from multiple source datasets. \n+ With the edge information (particularly inside object shape), the synthesized images are sharper and contain more details. \n+ The paper reported several ablation studies to investigate the effects of image smoothness, edge carving, and changing the intensities of the semantic segmentation maps.\n\n**Weakness**\n-\tIt's unclear what this paper's motivation is as I do not see a clear application from the proposed method. The paper showed results mapping one RGB image to another RGB image (with a different style). When do we need this domain adaptation, and how would this be useful? For example, it would have been better to demonstrate the methodology's use on some actual tasks involving domain adaptation, such as adapting a model trained on a synthetic dataset to a real dataset. \n-\tApart from the motivation, there are no comparisons against any other potential baseline approaches in the evaluation. For example, from the results shown in the paper and the supplementary material, I believe that a simple photographic style transfer method would achieve similar, if not better, effects. \n-\tWhen reporting the semantic segmentation IoU as a metric, it would be essential to show the baseline performance. What is the IoU when applying the segmentation model to the original visual domain? How much improvement can we get by mapping the images in the input domain to the output domain? Showing results only after the image-to-image translation is not informative.\n-\tThe technical novelty of the paper is somewhat limited as well. Existing work (Pix2PixHD) has shown that one can improve the visual quality of the synthesized images using an instance edge map. This paper extends that to use edges (which include both object contour and internal structures). However, from the image synthesis perspective, this EPS input needs to be obtained from some input RGB images in the first place. Since we already start with an RGB image, why should we resort to this somewhat complicated image synthesis pipeline (as opposed to simple color/style transfer)?\n\nIn sum, while the paper showcased improved visual quality on the translated images, I have concerns about this paper in its unclear motivation and limited evaluation. I would appreciate it if the authors could clarify and, if possible, provide comparisons with the baselines (e.g., style transfer).\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}