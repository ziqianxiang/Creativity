{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a method for learning intristic reward from demonstrations. The inartistic reward is computed as time-to-reach and generalizes to unseen states.  The reviewers agree that the method is novel useful, and of interest to ICLR community. \n\nAlthough the authors' significantly improved the manuscript during the rebuttal phase with new results, and addressed many of the reviewers' comments, the overall novelty of the paper is still somewhat limited, making it unsuitable for ICLR in its current form.   \n\nThe future  version of the paper should address the comments below and go through a detailed pass for clarity.\n\nAdditional comments that did not influence the final decision:\n\nThe idea of learning temporal distance to the goal is not novel [1], although the application as an intristic reward is. The authors should connect the temporal difference to the reachability theory and solving two-point boundary problem for systems with non-linear dynamics, as a theoretical foundation of the method [1].\n\nI am curious about the decision to use the time to reach as a reward directly, instead of delta between the states. Some empirical work provides evidence [2,3] that delta yield less side effects in behaviors. \n\n[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772207\n[2]https://arxiv.org/abs/1803.10227\n[3] https://arxiv.org/abs/2003.06906"
    },
    "Reviews": [
        {
            "title": "Insufficient evaluation for a paper that comes with no theoratical insights",
            "review": "Summary\n--------\nThe paper proposes a method for imitation learning for goal-directed tasks that uses a learned proximity function for computing rewards.\nAn ensemble of proximity functions is trained in supervised way to predict the time step (rescaled to the range $[0,1]$) for the expert's states. The ensemble is improved online based on the additional objective of assigning zero proximity to the states encountered by the agent. The agent is improved using PPO where the gain in proximity serves as reward function with an additional cost based on the uncertainty about the proximity (estimated by the standard deviation of the ensemble).\n\nStrong points\n-------------\n- Predicting time from states is an interesting idea and assigning higher reward to later states is sensible for goal directed tasks.\n- The paper is well-written and sufficiently clear  \n\nConcerns\n---------\n- Soundness\nThere is barely any theoretical justification for the approach. Furthermore, several hacks / hyperparameters have been added to achieve the reported results (uncertainty penalty, bonus reward, proximity \"discounting\", reward scaling).\n\n- Relevance\nThe approach is limited to goal-directed tasks and thus much less applicable than comparable methods.\n\n- Evaluation\nI think that the evaluations are not fair for several reasons.\n1. The competitors are not intended for learning under different initial state distributions. It is well known that behavioral cloning often performs very bad for out-of-distribution data. Adversarial methods like GAIL and GAIL-s are based on distribution matching which is in general not possible if the initial state is significantly different from the demonstrations. Especially GAIL-s can suffer here, since it would move to the initial state when starting at the goal position. Matching state transitions as GAILfO would be a bit more reasonable here. Methods that can guide the agent towards the demonstrations, such as SQIL (Reddy et al. 2019) might be even more suitable.\n\n2. The paper mentions that GAIL makes use of additional information by taking into account the actions, but completely ignores the fact that the proposed method makes use of time-labels which are much more relevant for tasks that are essentially defined by the state at the last time step. It would be easy to provide this information also to the adversarial methods by weighting the discriminator samples during training dependent on the time step (in the extrem case by only using the samples form the final time steps). Another naive baseline would be to use a non-parameteric reward function by placing radial basis functions at every expert sample (again weighted dependent on the time step). Instead, the evaluation does not seem to make any use of the strong assumption of goal-directed tasks for any of the competing methods.\n\n3. It seems that much more hyper-parameter tuning was involved for obtaining the results for the proposed method than for competing methods. Even for hyperparameters that are applicable to other methods, such as learning rates and reward scaling the competing methods share the same parameters during all experiments, whereas the proposed method has different hyperparameters depending on the environment. \n\n\nAdditional Feedback\n-------------------\nClarity could be improved at some points. Section 3.2. introduces $\\delta$ as a discounting but doesn't mention that it is typically set to $1/T$ which essentially scales the predicted time step to the range $[0,1]$. Without this information, it is hard to make sense of Eq. 1 because for a \"discount factor\" close to 1 (which is typical for the discount factor of the MDP) the proximity function would actually be negative for most time steps and by setting the proximity of agent states to 0 one would actually assign them very high goal proximity.  \n\n\nQuestions\n---------\n1. Why provide an additional reward for last time step? Is this really necessary to obtain good results?\n\n2. I don't see how the optimal proximity function for Eq.4 converges to $\\frac{1}{2} - \\delta \\frac{(T-t)}{2}$. This is not really relevant to the algorithm but still I'm wondering whether this claim is correct. Can you sketch a proof?\n\n3. Can you specify the initial distributions for the test and train experiments more precisely. For example, for the navigation task you mention that 50% of the possible initial states  and goals where used for collecting demonstrations. But, I can neither find the set of possible initial states, nor how the 50% where chosen (uniformly, or selected?).  \n\n\nAssessment\n----------\nThe proposed (heuristic) approach seems reasonable for the considered setting, however, overall the contribution is too small. Strong theoretical results or a thorough empirical evaluation (with suitable baselines) would both be fine for me. However, the current submission lacks quite severly on both sides and is therefore in my opinion not suitable for publication.\n\n\nReferences\n----------\nReddy, S., Dragan, A. D., Levine, S. SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards. ICLR. 2019",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Theoretical foundation is better to be clarified. ",
            "review": "To accelerate and improve imitation learning for goal-driven tasks, the authors introduce a goal proximity function that is learned from the observation of expert demonstrations and online agent experience.\nThe inferred goal proximity is used as an additional reward signal. The authors showed that heuristics efficiently improve the performance of imitation learning.\n\nThe method is simple and looks effective, as shown in the experiment. \nHowever, from the theoretical viewpoint, this proposal looks a heuristic method.\nIt is better to clarify the theoretical foundation.\n\nThe relationship with GAIL is mentioned several times. However, the explicit comparison between the proposed method and GAIL is not given.  \n(For example, \"First, we set the target proximity of states in agent trajectories to 0, similar to adversarial imitation learning methods (Ho & Ermon, 2016), and train the proximity function with both expert demonstrations and agent experience by minimizing the following loss.\" )\nDescribing the comparison (as an appendix) may help readers to understand the key idea.\n\nTo my understanding, the paper focus on LfO. However, the relationship between LfO and \"goal proximity\" is not clear. The \"goal proximity\" can be used for LfD as well?\n\nIf we consider \"goal proximity function\" as a goal-related reward function, the method is regarded as the integration of an imitation learning, e.g., GAIL, and a goal-driven reinforcement learning.\nFrom this view, this work looks related to the following paper. \n\n-Kinose, Akira, and Tadahiro Taniguchi. \"Integration of imitation learning using GAIL and reinforcement learning using task-achievement rewards via probabilistic graphical model.\" Advanced Robotics 34.16 (2020): 1055-1067.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising approach, but missing fundamental results",
            "review": "SUMMARY:\nThe authors propose a new method for imitation learning from observation that attempts to estimate and leverage a notion of goal proximity in order to help the learning process. The authors provide a framework for computing this estimate, and a technique for using that estimate -- along with a measure of uncertainty -- to perform imitation learning from observation. Experimental results for several domains are presented in which the proposed technique achieves better performance than the comparison methods. \n\n\nSTRENGTHS:\n\t(S1) The paper seeks to solve an interesting and relevant problem in IfO.\n\t(S2) The proposed technique -- estimating and using a notion of task completion proximity -- is (as far as I'm aware) a novel take on the IfO that seems to have the potential to advance the state of the art.\n\n\nWEAKNESSES:\n\t(W1) Fundamental experimental results are missing. The paper proposes a technique with two major components: (a) an estimated proximity function, and (b) a method to exploit uncertainty information in that estimate. However, its not clear from the results which if it is the unique combination of these components that leads to the good results, or just one of them -- especially given the results in Figure 5 which shows how critical (b) is. One way to get at that would be to apply (b) to some adversarial imitation learning techniques (eg, GAIfO) and see how they perform. Without something like this, one cannot tell if it is the proximity function, the use of uncertainty information, or the combination that truly leads to improvement. This is a fundamental question that must be addressed.\n\t(W2) As written, some of the details of the proposed method are not clear to where it would likely be difficult to reproduce. For example:\n\t\t(a) There appears to be an unstated assumption that all demonstration trajectories terminate at a known time $T$, which is not typically true. Meanwhile, it seems like such trajectories would necessitate different choices of $\\delta$, but the authors only discuss how to set $\\delta$ in general according to some parameter $H$ which \n\t\t(b) It seems as though the training objective for $f_\\phi$ articulated in Equation (1) is highly dependent on $\\delta$, but it doesn't seem as though the choice of delta is discussed.\n\n\nRECOMMENDATION STATEMENT:\nThe proposed method describes a novel approach to a good problem and seems to hold promise. However, I feel that important experimental results need to be added before the paper should be published.\n\n\nQUESTIONS FOR AUTHORS:\n\t(Q1) How would a standard imitation from observation algorithm perform if endowed with the same uncertainty information as the proposed method?\n\t(Q2) In Figure 4d, why are cells in the NW quadrant seemingly just as proximal as cells in the SW/NE? It seems as though they should be less proximal.\n\n\nMINOR COMMENTS:\n\t(MC1) The two legends present on Figure 5 are confusing while looking at (a), and its unclear where some of the curves in the legend at the top are in (b) and (c). The figure should be revised to be more clear.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice work, but missing crucial literature proposing temporal progress rewards",
            "review": "## Paper and Review Summary \n\nThis paper introduces a goal proximity approach to learning rewards from observations captured during demonstrations in goal-oriented tasks. This metric allocates rewards based on the temporal distance to a goal, relying on a model trained to predict this distance. The paper also proposes an adversarial learning approach to policy optimisation with this reward, which helps to improve policies in regions where demonstrations were not captured.\n\nI like the idea, but, unfortunately, this paper has missed key related work [Angelov et al.], [Burke et al.] on reward inference and policy scheduling using temporal progress metrics, which have been proposed previously. Although I do see potential novelty and additions going beyond the work in [Angelov et al.], [Burke et al.] (adversarial training, ablations the incorporation of uncertainty in the metric) - the primary contribution is severely reduced. As a result, I am inclined to recommend *rejecting* this work, unless the paper revisions can convince me that there are sufficient differences and novelty. However, this may require a substantial rewrite of the introduction and conclusions and major revisions. \n\n## Pros \n\n- The paper is well written, with detailed experiments and nice ablations\n- I like the idea of combining a goal progress metric with adversarial learning\n- The inclusion of the uncertainty metric is interesting, potentially allowing for risk-based policy optimisation\n\n### Cons\nLimited novelty: \n- A linear goal progress metric has already proposed by Angelov et al. in the paper [Composing Diverse Policies for Temporally Extended Tasks](https://arxiv.org/pdf/1907.08199.pdf). This paper trains a model to predict the normalised time to goal using observed demonstrations, and uses this to select sub-policies for long-horizon tasks, in a model-based setting. Angelov et al. do not optimise policies directly using the goal metric.\n- In follow on work by Burke et al. in the paper [Learning rewards for robotic ultrasound scanning\nusing probabilistic temporal ranking](https://arxiv.org/abs/2002.01240) this goal progress metric of Angelov et al. is extended to a probabilistic temporal ranking metric that allows for non-monotonically increasing goal progress in demonstrations to be captured. Here, the goal progress reward is used for policy optimisation (value iteration for grid world experiments) and online learning.\n\n### Questions\n\n- On page 3, the paper states \"*There are alternative ways to represent and learn goal proximity, such as exponentially discounted proximity and ranking-based proximity (Brown et al., 2019). But, in our experiments, linearly discounted proximity consistently performed better than alternatives*\" - I am curious that linear proximity models performed better as experiments in Brown et al. and Burke et al. show that non-linear temporal progress metrics are more effective. In particular, ranking-based methods allow for non-monotonically increasing progress metrics. Is this comment based only on the comparison between exponentially and linearly increasing progress, or was a temporal ranking approach considered? If not, I would recommend exploring a temporal ranking approach going forward.\n- In Equation (2), I see the progress metric is not used directly, instead a derived reward was used. Did you experiment with using the progress metric directly instead?\n- Page 7 FETCH-PUSH - Why can't the methods learn diagonal pushing policies?\n- Figure 5 - Why does the policy using offline only training fail completely?  Is this a function of the way/ amount the training data was collected or something else?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official review",
            "review": "**Paper summary**\n\nThis paper proposes a method for imitation learning from observations based on learning a goal proximity function from expert demonstrations and using it as a dense reward for training an imitator. The authors show that this method improves generalization to unseen states in comparison to several baselines. \n\n**Pros:**\n\n- The idea is simple, well-motivated and the paper is easy to read, clear and well-written.\n\n- The experiments are well designed and the results are explained with sufficient detail. \n\n**Cons:**\n\nWhile I find the idea interesting and the experiments well-designed, I have a few concerns about the method and would need some clarifications to evaluate it further:\n\n1. Figure 5 and 9 demonstrate that a key component of the method is the adversarial training of the proximity function. Without it, the method completely fails. In comparison, not using the uncertainty part, or not training offline, makes less of a difference. This seems to indicate that the adversarial part is more important than the temporal aspect of it. If we ignore the temporal aspect and replace the target for f, $1-\\delta (T-t)$, by its upper value of 1, we obtain a loss: $E_{expert} [f -1]^2 + E_{imitator} f^2$ which is very similar to the one used in GAIL: $E_{expert} \\log(\\frac{1}{1-D}) + E_{imitator} \\log \\frac{1}{D})$ for values of $D$ and $f$ between 0 and 1 (plotting the 4 functions recommended). Given this similarity, I think the paper would benefit from one more ablation of this form: removing the temporal part and keeping the adversarial component. \n\n2. Following from point 1, I wonder why are the GAIL results so different (e.g GAIfO). One concern I have is the final reward. The method proposed in this paper makes the assumption that the last state of the demonstration is the goal. Therefore the authors use an extra reward at this time step (Eq. 2). If I understand correctly, GAIL in general doesn’t make this assumption, so I wonder if this is at least in part responsible for the differences between these methods. I think the paper would benefit from clarifying this by performing an ablation that removes this final reward.\n\n3. One maybe more superficial concern is whether the name ‘goal proximity’ is appropriate. Given that the function $f$ is not only trained to follow time, but also adversarially to be zero on imitator data, this name is confusing to me. In fact, as shown in Fig 4d, the value of $f$ doesn’t end up corresponding to the time (or number of actions) that would be required to reach the goal in the general sense. \n\n4. Regarding Fig 4d. I’m not sure why the centers of all quadrants are light. Shouldn’t only the path to the goal used by the expert be light?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}