{
    "Decision": "",
    "Reviews": [
        {
            "title": "the novelty is not enough",
            "review": "In this paper, the authors presented an algorithm based on ADAM, which modified the choice of exponential decay rate as well as the batch size. The main issue is the lack of novelty. In my point of view, the convergence analysis has little difference with [1], unless a better convergence rate is achieved. Actually, the convergence rate is worse than SGD.\n\nI think many researchers do not even notice the issue in [2], the fist & second moments are not unbiased since limit and expectation are not able to switch unless be proved. As a result, I can hardly agree with the statement “Then, the unbiased local estimate of the gradient variance…” in Sec.3( page 3). \n\nThe third issue is the O(T) increasement of sample size. Practically, what if T greater than or equal to the training data size?  I think we are not able to construct a convergent sequence.\n\nThere are many typos in the paper, e.g. $\\mathrm{E}_t [g_t] = f(\\theta_t)$ in page 12.\n\nIn conclusion, I think the contribution is not enough to make the paper be accepted. \n\n[1] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. In NeurIPS, pp. 9793–9803, 2018.\n[2] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), ICLR, 2015. URL http://arxiv.org/abs/1412.6980.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of Adaptive Learning Rates with Maximum Variation Averaging",
            "review": "## Summary\n\nIn this paper, the authors propose to improve Adam with an iteration dependent averaging weight $\\beta_t$. A choice of $\\beta_t$ lead to a specific moving average, i.e. a weighting over the past iterations. The authors then compute the variance through time of the gradient $g_t$ with those weights. $\\beta_t$ is then chosen greedily so as to maximize the variance at each iteration.\n\nOn the theoretical side, the authors provide a weak convergence results (see review hereafter, requires a batch size that goes to infinity, and the bound is in $O(1 / \\epsilon^2)$ compared to the best proofs for Adam.\n\nOn the practical side, provides experiments on standard vision and NLP datasets, showing that their method is competitive and sometimes better than existing ones.\n\n\n## Review\n\nThe idea of chosing $\\beta_t$ in order to maximise the value of the historical variance is appealing. A key intuition is that if a large gradient is seen, it is going to be remembered for a longer time than with the standard Adam. This would make the presented algorithm MAdam somewhere between Adam and AMSGrad.\nIn fact, if there was no upper bound on $\\beta_t$, it is likely a large past gradient would never be forgotten, as assigning more weight to the following smaller iterations would decrease the variance.\n\nThe theoretical analysis is quite weak compared to the most recent proofs technique for adaptive methods in the non convex settings [1,2]. In particular, the bound obtained is proportional to $1/\\epsilon^2$. Given that $\\epsilon$ is typically 1e-8, this makes the bound uninformative. Besides, the analysis requires that the \\eta learning rate is smaller than $\\epsilon / L$, which is quite impractical, and the batch size to go to infinity, which is not required by standard proofs for Adam or Adagrad.\n\nIn fact, with the assumption that $\\eta < \\epsilon / L$, one can prove the convergence for any choice of v_t in equation (5), as long as it is bounded by G, as the effective learning rate for each coordinate will be between 1/L and 1 / (L + G^2), and the proof quickly boils down to SGD.\nThis means the authors bypassed most of the difficulties that arise for adaptive methods, in particular the correlation between the numerator and denominator that introduces a bias in the update direction. In Adam or Adagrad, the fact that $\\beta_t$ is controlled (Adagrad can be seen as taking $\\beta_t = 1-1/t$) means that the correlation is also controlled. With MAdam, this is no longer the case as the effective $\\beta_t$ could be as small as 1/2. Large gradients will likely lead to $\\beta_t = 1/2$ as this will maximize the observed variance. This will add an uncontrolled bias to the update direction and potentially prevent convergence.\nThis means that for a specifically crafted example like the one in Reddi et al. 2018 (On the convergence of Adam and Beyond), the method could fail to converge.\n\nNote that it is known that for NLP task, gradients can be normalized (which would be equivalent to taking $\\beta_t = 0$), which is done by optimizers like LAMB with good results. Therefore the problem I mentioned in the previous paragraph might not be a problem there. However, it does limit the impact of MAdam as an optimizer for general problems.\nThe practical results look encouraging even though they are not always significant. \n\nBecause of the strong limitations of the theoretical results in this paper I recommend rejection.\n\n## Remarks\n\n- Section 3, equation (3), (4) and (5): the notation w_t(\\beta_t) is weird as w_t also depends on $\\beta_{t - 1}$, in particular, $w_t(\\beta_t) = \\ldots w_{t-1}(\\beta_{t-1})$. A more correct notation would be w_t(\\beta), or make it depend implicitely on \\beta.\n- notation with ~ vs no ~ is a bit inconsistent: juse before equation (6), $\\tilde{\\sigma}$ should maybe be called $\\sigma$ as it works with unbiased estimates.\n- after eq (7), again confusion between \\tilde or no \\tilde (\"we have abbreviated $u_{t-1}(\\beta_{t-1})$ into $\\tilde{u}$).\n- Theorem 1: instead of taking the batch size going to infinity, one can take $\\eta = 1 / \\sqrt{T}$ for a known number of time steps T. This gives a $O(1/\\sqrt(T))$ rate, which matches the best possible rate for stochastic optimization [3].\n- In Section 5.4, the authors say that their methods allow to take a larger learning rate than Adam. This is in fact expected as the maximization of the variance will typically lead to larger values for $v_t$ for MAdam. This can then be compensated by taking a larger $\\eta$. However, this does not mean that the effective learning will be larger for MAdam.\n\n\n[1] A Sufficient Condition for Convergences of Adam and RMSProp, Zou et al. CVPR 2019\n[2] On the Convergence of Adam and Adagrad, Defossez et al. arxiv 2020\n[3]  Information-theoretic lower bounds on the\noracle complexity of convex optimization, Agarwal et al, Neurips 2007",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but the execution of the idea and results aren't sufficient for ICLR",
            "review": "This paper proposes a new adaptive algorithm for nonconvex optimization. The distinct feature of the algorithm is to use an iteration dependent second order momentum parameter $\\beta_t$, compared to standard constant $\\beta$, like ADAM. The authors analyze the convergence of the algorithm for smooth nonconvex optimization and show that with increasing mini-batch sizes, growing as $t+1$, the method has $1/T$ rate and $\\epsilon^{-2}$ complexity for squared gradient norm. The authors show that the method has competitive performance in different applications with neural networks.\n\nPros: General idea of using $\\beta_t$ depending on the estimated gradient variance is intuitive and interesting, the explanation of the idea is clear. Moreover, the practical evaluation part is extensive with diverse set of applications.\n\nConcerns: Even though the intuitive role of the choice of $\\beta_t$ is well-explained, I think the discussion is kind of vague. I would prefer to see either fully rigorous theoretical justification or some simple empirical evidence (toy experiments just to illustrate the effect) justifying the claims made regarding $\\beta_t$. For example, the paragraph after eq (9) can be much clearer if this effects is illustrated with simple experiments.\n\nIn addition, the theoretical analysis is quite rough: only the upper and lower bounds of $\\beta_t$, which are $\\underline \\beta$ and $\\bar \\beta$ are used. The specific form of $\\beta_t$ isn't really utilized in the proof, so it isn't clear if the choice of $\\beta_t$ brings any benefit in theory. Actually it seems to me the proof would work with any upper and lower bounded $\\beta_t$, is this correct?\n\nThe proof of Thm 1 follows Zaheer et al,. 2018 and uses increasing batch sizes (b_t = t+1). However, the result of Zaheer et al., 2018 shows that with increasing mini-batch sizes, Adam also works, whereas Adam fails when one uses a single sample (or constant mini-batch size). Therefore, I think that the setting that the authors analyze is too benign: even a non-convergent method (Adam) works with very large mini-batch sizes, therefore the new method converging in this setting doesn't give a complete story. I would suggest to include a more refined analysis using one gradient every iteration like Chen et al., 2019.\n\nIn page 4, it is written that \"we analyze the case $\\alpha=0$, but the analysis should extend to general case\". I don't agree with this remark. We know for adaptive methods first order momentum parameter can introduce difficulties  in analysis. For example, Sec 4.3 of [1]. Since the authors use nonzero $\\alpha$ in the experiments, I think they should analyze this case.\n\nEven though the experiments are extensive, the performance of the new methods isn't very promising. The method performs almost the same as the existing methods and some methods such as Adagrad and AdamNC are omitted. It seems Amsgrad is also omitted even though the sentence \"We also find AMSGRAD improves ...\" in Sect 5.1 is unclear. Does the authors mean applying Amsgrad to all the adaptive methods? In that case, what does it mean to apply Amsgrad to other methods? Amsgrad is a distinct optimizer itself.\n\nMinor comment:\nParagraph after eq. (10), why does bounded gradients imply smoothness of $f(\\theta)$, or do the authors want to say (10) implies smoothness of $f(\\theta)$? \n\nOverall, even though I find the idea of maximum variance weighted averaging for squared gradients interesting, the execution of the idea doesn't seem sufficient for a top conference. In particular, the theoretical justification for the new $\\beta_t$ is missing and the theoretical analysis is loose (not using the specific form of $\\beta_t$) without new insights. Moreover, the setting in which the analysis is done is not representative for adaptive methods. Lastly, the empirical merit is unclear. Therefore I vote for a rejection.\n\n[1] Chen, Liu, Xu, Li, Lin, Hong, Cox, ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization, NeurIPs 2019.\n\n\n\n=============== after discussion phase ================\n\nI thank the authors for their response. Unfortunately, I am still not convinced about the theoretical results. Analyzing the algorithm in large-batch regime is not very insightful as large (increasing) batch sizes basically makes the algorithm behave like a deterministic method. Moreover, with the mistake that Reviewer 4 identified, the theoretical results of the paper got even worse. The new algorithms behave worse than Adam in the worst-case large-batch case, which contradicts the motivation. Even though the algorithm looks promising in practice, I believe a clear principled reason is necessary to verify the practical observations. Therefore, I keep my score for rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviewer #4",
            "review": "I have read the main text as well as the appendix to validate the proofs and check the additional experimental results. \n\nSummary:\n\n•\tThe paper proposes a modification to first and second moment updates of ADAM and LAProp, namely MADAM and LAMADAM, such that instead of computing a uniform running average of gradients and their squares, a weighted average is computed. Specifically, instead of $\\beta_1$ and $\\beta_2$, a single parameter $\\beta_{t,i}$ for both moment estimations across each coordinate. $\\beta_{t,i}$ is chosen to maximize the variance estimate, $v_t,i$, along each individual coordinate. \n\n•\tThe paper claims to achieve a rate of $\\mathbb E \\lVert \\nabla f (\\theta_a) \\rVert^2 = \\mathcal O (\\log(T) / T)$, where $\\theta_a$ is randomly chosen over all iterations.\n\n•\tTo demonstrate the numerical performance of their algorithms, the authors provide extensive experimental results in comparison with SGD and other adaptive methods for image classification, machine translation and natural language understanding tasks.\n\n\nStrengths:\n\n•\tThe paper identifies practical shortcomings of adaptive gradient methods for different regimes of stochastic gradient behavior and proposes a new methodology that could yield improved performance for various tasks.\n\n•\tI think the authors motivate their adaptive learning rate in a detailed manner fashion. They provide intuitive explanations as to how the adaptive learning rate behaves with respect to the magnitude of deviation of stochastic gradients from the running average. In section 4, they argue about the convergence properties of their algorithms based on a simple 2-dimensional quadratic problem by presenting iterate trajectories and error values of Adam and MAdam.\n\n•\tThe paper offers comprehensive experimental evaluation for different tasks under well-known setups. Authors give different perspectives to performance and behavior of their methods in comparison with widely-used optimizers.\n\n\nWeaknesses:\n\n•\tIn equation (18) in the appendix, on the second equality, the authors write that $ \\mathbb E_t \\left[ \\frac{g_{t,i}}{\\sqrt{ \\beta_{t,i} v_{t-1, i} } + \\epsilon} | \\theta_t \\right] =  \\frac{ [ f( \\theta_t ) ]i }{\\sqrt{ \\beta_{t,i} v_{t-1, i} } + \\epsilon} $. However, computation of $\\beta_{t,i}$ depends on $g_{t,i}$ which makes expressions in the numerator and the denominator dependent random variables. One solution might be using lower/upper bound of $\\beta_{t,i}$ and only deal with the expected value of $g_{t,i}$. However, we do not know whether the expression inside the expectation $\\mathbb E_t \\left[ \\frac{g_{t,i}}{ \\sqrt{ \\beta_{t,i} v_{t-1, i} } + \\epsilon } | \\theta_t \\right]$ is positive or negative. This keeps us from picking the appropriate bound on $\\beta_{t,i}$. If I am not mistaken in my observation, this equality shouldn’t be valid, which creates a critical problem for the correctness of the theoretical analysis.\n\n•\tRegarding the practical performance, the proposed methods have comparable, sometimes better, performance in most tasks. Although the proposed methods show consistent performance across tasks of different nature, their improvements over other optimizers are not very significant. Comparison against SGD for Transformers might have been important to show instability of SGD for such tasks.\n\n\nAdditional Comments:\n\n•\t$\\mathbb E_t$ is missing in the first 2 lines of Eq. (21) for the summation with coefficient $\\eta_t G / 2$. For the same expression, you take $\\sqrt{ 1 - \\beta_{t,i} }$ outside of both expectation and summation, which shouldn’t be valid. You could lower-bound $\\beta_{t,i} \\geq \\underline{\\beta}$, then you could take the expression out.\n\n•\tIn expression (25), the factor in front of the parenthesis is missing a $G$. It should be $2 \\sqrt{\\overline{\\beta}} G + \\epsilon$.\n\n\nScore:\n\nI vote for clear rejection of this paper, mainly because I suspected there is a mistake in the proof of Theorem 1 as I indicated above in my comments. In my opinion, this paper is rather practically-oriented in the way that it highlights practical performance over theoretical properties. Experiments are extensive and I appreciate the effort that is put into them very much. However, I am also not totally convinced that MADAM and LAMADAM have significantly better performance than other optimizers. I am open for further discussion with the authors about my comments/concerns and their possible future updates/improvements.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}