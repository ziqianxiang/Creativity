{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The work focuses on detecting whether a certain data sample was used to train a deep network-based conditional image synthesis model. The key idea is not to rely on just reconstruction error but normalizing it via a proposed difficulty score. The reviewers found the problem statement important and the paper easy to follow. However, a common concern in the discussion was the approach is specific to image-to-image translation problems. Many other minor questions were answered by the authors in the rebuttal. Upon discussion post rebuttal, the reviewers decided to maintain their score. AC and reviewers believe that the paper will benefit from better analysis and description of difficulty score and its correlation with reconstruction error. It would be ideal to see how these ideas are useful for a broader problem than image translation. Please refer to the reviews for final feedback and suggestions to strengthen the submission."
    },
    "Reviews": [
        {
            "title": "Interesting method",
            "review": "This submission focuses on the problem of membership inference attack for conditional image synthesis. This is an interesting new application and a meaningful extension of the topic of MIA. It is straightforward to use reconstruction loss as a measure for membership error. However, the authors pointed out that on top of reconstruction loss, the difficulty for the sample should also be taken into account. With that the authors propose a novel method of measuring generation difficulty for each sample by training a regressor to estimate pixel value using the abstract representation by a pre-trained model. The regressor is trained using a portion of randomly selected pixels and tested on the rest. The error of the prediction is use as the indicator of difficulty. This is an intuitive method and from the results of the experiment, the proposed method seems to be quite effective. The reviewer would like the author to clarify a few points: 1) is it an assumption that the pre-trained network has never seen any data used in the testing? If not, what would happen if the data has been seen when training the “feature extractor” but not the victim model? 2) Wouldn’t it be more application/problem-specific to train a regressor using the data the victim model is conditioned on (e.g. the segmentation mask) to predict the pixel values of the ground-truth image in estimating sample difficulty? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Borderline paper with concerns on novelty and applications",
            "review": "This paper aims to solve the problem of membership attack, ie detecting if data samples were used to train a neural network for conditional image generation. The paper first proposes a simple but effective approach by using the reconstruction error. To address the issue that reconstruction error alone is less effective at discriminating between difficult images used in training and easy images that were never seen before, a difficulty score is further proposed, which can be computed for each image, and its computation does not require a training set. The eventual membership error, obtained by subtracting the difficulty score from the reconstruction error, is shown to be effective on several datasets.\n\nMerits:\n+ The paper is well written. I can easily get the main idea of the paper. And this work is well-motivated: some images are universally easy, and others are difficult.\n+ The proposed method looks simple and largely reasonable and is consistent with the motivation.\n+ Experiments have shown good results in terms of segmentation-to-image translation. The ablation study shows the effectiveness of the proposed difficulty score.\n\nIssues:\n- The novelty of the proposed algorithm is limited: the reconstruction error is quite standard, and the difficulty score is not well explained. I wonder why the difficulty score is designed as in (2). Please provide more thorough explanations and clarify how it differs from previous approaches.\n- While the title of the paper is \"conditional generative models\", the experiments are only conducted on segmentation-to-image translation. It will be important to show how it works in other related applications.\n- The membership error relies on a hyper-parameter $\\alpha$ in (3). Why is this parameter universally good for different datasets? Could the author provide more insights into tuning $\\alpha$ for different tasks?\n\n--------------------------------------------------------------------\nI have read the response, and my rating is not changed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple and effective difficulty scoring function for MIA, experimental comparisons are comprehensive",
            "review": "This paper explores the MIA in the scenario of I2I translation and proposed a difficulty scoring function by observing that simply borrowing the MIA techniques from classification task is less effective. The scoring function is to measures the success of a linear regression model trained on part of pixels to predict the unseen pixel values. It is mainly to distinguish difficult-trained vs. easy-untrained examples. The improvement on performance brought by this simple function is obvious and comparisons are comprehensive. Below are some of my concerns and suggestions:\n\n(1) The data used for training the regression model is the same with the one used for training the I2I model? Does it mean we still need to access the original data after I2I model pre-training?\n\n(2) Is the 70/30 partition of pixels for training the regression model working the best? It will be better to analyze this partition ratio and see how the MIA accuracy will be affected.\n\n(3) Generally if the pre-trained model is more generalizable (e.g., trained with more data), I feel the MIA will be less useful. An interesting direction is to explore whether it will be more useful in few-shot case, not only to detect whether overfitting happens, but also to explore how to use MIA to avoid overfitting and make the model trained with less data more robust.\n\n========\n\nUpdate:\n\nThanks for the author's feedback that clarifies my concerns. Adding the difficulty score into training looks like a more promising future direction. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Current version lacks a clear motivation and does not seem like a complete paper",
            "review": "The paper studies membership inference attacks (MIA) in the context of conditional image generation models, i.e., models learned for image-to-image translation. The problem aims to detect if a data sample is used to train a neural network model. The paper observes that some images are \"universally\" easy/difficult to reconstruct, and states using reconstruction error alone is less effective at discriminating between difficult images used in training and easy images that are never seen before. The paper designs a difficulty score to augment reconstruction error based method, and shows MIA results on multiple datasets.\n\n\nThe problem of MIA is meaningful in terms of detecting leakage of privacy-sensitive data, as stated in the introduction. But the motivation of this paper is not clear. The paper lacks an explanation why studying MIA in the context of image-to-image translation.\n\nIn the second paragraph of introduction, it is not clear either why MIA can \"provide insights on the degree of overfitting in the victim mode\"?\n\nSection 3.2: What is the rationale behind the designed function Eq. (2) for measuring the difficulty of an image? The paper resizes all images into 56x56. Does this introduce noises that influences difficulty measurement? What is the resizing methods? Does different resizing methods have different performance?\n\nEq.(3): The paper considers \"difficult\" training images and \"easy\" unseen images, but does not consider \"difficult\" unseen images. For the unseen images that are difficult to reconstruction, does Eq.(3) still work?\n\nTable 3 does not seem complete. Why are there not ROC and ACC for \"ours\"? What do the numbers mean under \"ours\"?\n\nAs the paper mentioned in related work that some prior work also studies MIA in the context of generative models (GANs and VAEs), it is natural to think what the unique conclusions through the study presented in this paper?\n\nFigure 5: How to tell if \"difficult score\" is better correlated to the reconstrunction error than \"supervised difficulty\"?\n\n--------\npost-rebuttal\n--------\n\nI appreciate that authors have provided rebuttal that addresses many of my questions. I've read the updated paper and other reviewers' comments. The rebuttal has largely addressed my questions. However I am still not convinced by the following. Without a clarification I cannot see the significance of this work. So I'd maintain my rating. \n\nI still don't get the motivation why studying MIA in image-to-image translation. I really wanted to solicit a motivational example. The authors merely say \"image-to-image translation has yet to be studied in the context of MIA.\" I'm not convinced by this.\n\nA.4 The authors simply draw curves of MIA performance vs. learning epoch. I still don't know where overfitting happens. From the context, it seems that the proposed method will fail if the victim model is not trained to be overfitting. In practice, surely victim models are not overfitting?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}