{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviews were a bit mixed, with some concerns on the novelty and experimental evaluation. While the authors' efforts during rebuttable were appreciated, the overall sentiment is that this work, in its current form, cannot be accepted to ICLR yet. Please consider revising your work based on the excellent reviews. Some more comments from the AC's independent assessment:\n \n(a) Further elaboration on the novelty is needed. Currently the main message appears to be that if we combine two existing approaches (AT and EntM or LS) then we get better results. This is perhaps not too surprising and more elaboration on the significance would be appreciated. \n\n(b) More comparisons in the experiments, including the SOTA performances and alternative defenses (some below).\n \n(c) The analysis in Section 6 adds more confusion than clarification. It is clear that EntM and LS would largely decrease M_f, but why would they also decrease the Lipschitz constant even more sharply? If this explanation is useful, why not directly regularize the Lipschitz constant and maximize the margin M_f? There is in fact a large body of work on this, see for example:\n\n1. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation\n\n2. Parseval Networks: Improving Robustness to Adversarial Examples\n\n3. L2-Nonexpansive Neural Networks\n\n4. and the many references since.\n\n"
    },
    "Reviews": [
        {
            "title": "Interesting combination of ideas to increase adversarial robustness but gaps in the experimental evaluation",
            "review": "The authors combine adversarial training with two methods that increase the entropy of the output distribution of neural networks (label smoothing and entropy maximization). The authors find that this combination of ideas increases adversarial robustness on standard benchmarks, especially in the regime of large perturbation budgets (e.g., 16/255 on CIFAR-10). The authors also investigate the effect of their methods on the classification margin to understand the increase in adversarial accuracy.\n\nWhile the authors provide a detailed experimental evaluation of their defense method, this part of the paper is still my main concern. In particular, I see the following issues:\n\n* It is not clear if the authors choose a sufficiently large steps size for the PGD attacks. Typical values are 2 * eps / k to 10 * eps / k, where eps is the l_inf perturbation budget and k is the step size. However, the authors choose smaller step sizes in at least some cases, e.g., eps / k for the perturbation accuracy curves in Figure 1. This is a concern particularly because Table 1 shows that the model accuracies still decrease substantially when going from 10 to 40 PGD steps.\n\n* Again on the note of step sizes, why did the authors choose max(1 / 510, eps / k) in the attacks with more iterations?\n\n* It would be good to present a single table with the smallest known adversarial accuracies for the various defenses. E.g., Table 1 does not seem to include the results from more adaptive attacks where TRADES is sometimes comparable to the proposed methods.\n\n* How did the authors choose random restarts? They mention sigma = 0.005 - is this for a Gaussian distribution? What happens if the authors pick a point from the appropriately scaled l_inf ball instead?\n\n* It would be good to see attacks with the Carlini-Wagner (margin) loss function.\n\nConsidering the well-known difficulties with evaluating defenses against adversarial attacks, I currently cannot recommend accepting the paper.\n\nAdditional comments:\n\n- The authors attribue Szegedy et al., 2013 with adversarial examples. The authors may be interested in https://arxiv.org/abs/1712.03141 , in particular Figure 8, for a more detailed history of this research direction.\n\n- Trade-offs between adversarial and standard accuracy have been studied before Rice et al., 2020. For instance, see https://arxiv.org/abs/1805.12152 and https://arxiv.org/abs/2002.10716 .\n\n- Could scaling the softmax temperature also work for increasing the entropy of the softmax distribution in a way that leads to increased robustness?\n\n- Could the proposed technique be combined with TRADES or training on additional unlabeled data? (e.g., see https://arxiv.org/abs/1905.13736 and https://arxiv.org/abs/1905.13725 ).\n\n- Is there a \"max\" missing in Equation 5?\n\n- Beginning of Section 6: \"deeper into the how\"\n\n- Equation 12: what is X + delta?\n\n---------------------------------------------------------------------------------------------\n\nThank you for the detailed response. I have updated my score from 4 to 5.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Need Novelty Clarification",
            "review": "Summary:\n\nThis paper tries to improve the model robustness by modifying the loss function with the EntM or LS term. Also, they give a further analysis to identify how uncertainty promotion works.\n\n\nStrength:\n\n-- The methods seem rational. And the experiments also demonstrate the effectiveness.\n\n\nWeakness:\n\n--The novelty is limited. Training the model without the one-hot label is not new. Also, it seems that the label smoothing is the only contribution proposed by this work, but it is pretty naive.\n\n--The analysis part is difficult to follow.\n\n\nComments:\n--There are many works that claim the distillation training can improve the robustness of the model, which also adopt the soft label in loss function. Please compare with relative methods.\n\n-- For uncertainty analysis, the explanation is not straightforward. First, I am not sure why Eq. (11) holds. Second, why the L2 norm of the Jacobian matrix is its largest singular value? Also, I found the analysis is finally conducted through empirical studies. I think maybe the equation descriptions can be modified easier to understand and make the empirical study more clearly if possible.\n\n-- I think the performance drop with adversarial training is still high, although the results show the accuracy is better than TRADE. I suggest the authors to provide some experiments that adjust the hyper parameters to identify the trade-off between the clean accuracy and the robustness of the model.\n\n--For Eqs. (7)-(8), what does the term ‘logC’ stand for?\n\n--This work only considers untarget attack. How about the performance under target attack?\n\n--The attack algorithms seem not state-of-the-art. Please try more algorithms if possible.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness \"",
            "review": "# Summary\nThis paper investigates the complementary mechanisms of adversarial training and uncertainty promoting regularizers. In the field of adversarial machine learning, adversarial training as proposed by Madry et al. 2017 has been the common method. In the field of uncertainty regularization, maximum entropy and label smoothing have been the accepted methods. However, the combination of both has not been investigated before. The paper provides extensive experiments on these methods. The final section gives insights in the theory behind adversarial training and uncertainty promoting regularization, where they show that the combined method increases a notion of normalized margin and a notion of adversarial robustness. \n\n# Strong & Weak points\n\n## Strong points\n\n  * The related work section contains an extensive overview of related and contemporary literature, where both literature in adversarial ML and uncertainty promotion is being discussed.\n  * Ablation experiments in figure 1 and 2 show that combining adversarial training and uncertainty promoting regularizers have better accuracy under a range of attack settings. \n  * Theoretical insights as to why Entropy Maximization would help improve adversarial robustness, complementary to adversarial training is provided in Section 6\n\n## Weak points\n\n  * The method is studied on small datasets (CIFAR, MNIST, SVHN) using small models (ResNet18). It remains to be seen how these insights translate to larger datasets (ImageNet) and larger models (ResNet50).\n  * In the abstract and introduction, the text speaks of “true” robustness, against “strong” attacks, but these terms are never defined.\n  * The increase in (approximate) normalized margin when using entropy maximization as shown in table 4 provides an important argument for the claim of this paper. However, the numbers were obtained using the CIFAR10 dataset and a ResNet18 model. A stronger case could be made with a larger dataset and a larger model.\n\n# Statement\n\nRecommendation: 6\n\nReasons\n   * The claims for a complementary benefit of adversarial training and uncertainty promoting regularization are backed up with both extensive experiments and theoretical insights.\n   * The fields of adversarial training and uncertainty regularizers have been evolving separately and this paper provides initial insights how these two lines of research can be combined. [Comment: I am not 100% up to date with the related literature, so I'll be looking to other reviewers if they are aware of existing work combining uncertainty regularization and adversarial training.]\n   * Ablation experiments in Figure 3 shows substrates for the complementary actions of Entropy Maximization and Adversarial training. Entropy maximization can be shown to increase a notion of “normalized margin width”, adversarial training can be shown to increase a notion of  adversarial robustness, and when combining the methods, both metrics increase. \n\n# Additional questions:\n\nTable 3a) misses the point cloud for normal training. How do the normalized margins and adversarial distances of normal training compare in this plot? Table 4 already shows that the normalized margin (0.19) is smaller than the three methods, but I miss the numbers for adversarial distances during normal training.\n\n# Minor feedback\n\nThese points are minor feedback and not part of the assessment.\n\n  * KL divergence can be decomposed as $KL(p|q) = H[q] + CE(p|q)$. Could this decomposition explain the differences between TRADES, PAT, and entropy maximization?\n\n  * Figure 1 & 2: please provide labels for the x axes.\n  * None of the derivations on eqn. 9 to 13 depend on $\\theta$. Consider dropping the $\\theta$ variables everywhere to focus the analysis on what really matters: the derivatives w.r.t. $x$. \n  * Equation 12: the $\\delta$ variable is only implicitly defined. It is not clear to me if its direction is parallel or orthogonal to the decision boundary. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Limited novelty and insufficient experiments.",
            "review": "1) First, this work studies how entropy maximization and label smoothing combined with adversarial training can improve adversarial robustness. Although these two techniques have been shown to prevent model from being over-confident, I still think it is an over-claim that \"rethinking uncertainty in deep learning\" as this work does not truly study uncertainty in deep learning or the correlation between uncertainty and adversarial robustness.\n2) Label smoothing alone does not provide stronger adversarial robustness, this is not a surprising result as pointed out by many existing work. As entropy maximization is similar as label smoothing, it is also in the expectation that they have similar performance without providing stronger adversarial robustness.\n3) \"Combining label smoothing/EntM with adversarial training can further provide stronger adversarial robustness\", this is the main conclusion of this work. However, I am not fully convinced this is novel enough as this has been observed by existing work. In addition, this work only tested the model on the PGD attacks, the same type of attack (although with smaller l infinity norm bound) is used during training. It is very necessary to test the model against different types of attacks, especially decision-boundary based attacks, to support this conclusion as this is the main contribution of this work. \nIn all, I vote for a rejection for this work.\n\n******** After Rebuttal ************\nI carefully read the authors' response and unfortunately they do not address my concerns. Based on my research background in adversarial robustness and uncertainty estimates, I would keep my original rating unchanged as this work has very limited contribution to these two areas.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}