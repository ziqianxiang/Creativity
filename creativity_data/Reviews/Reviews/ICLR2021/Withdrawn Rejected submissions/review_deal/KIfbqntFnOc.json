{
    "Decision": "",
    "Reviews": [
        {
            "title": "Scaling noise in Resnets to generate resilience",
            "review": "This paper presents a new class of Resnets, called Ito Ensembles. These Resnets are obtained by adding noise to original Resnets. The key point is that the amplitude of the noise depends on the depth of the layer, and the appropriate scaling with depth is chosen based on arguments from stochastic calculus. Extensive numerical experiments show that the Ito Ensembles vastly outperform state-of-the-art networks in terms of resilience to adversarial attacks, while keeping a very high accuracy on benign inputs.\n\nAltogether, this paper seems like a strong candidate for ICLR. I am however not at all an expert on resilience to advesarial attacks, and therefore not fully qualified to assess the main results (hence my low confidence score). My expertise lies closer to stochastic differential equations, and I mostly comment on this aspect below.\n\nStrengths:\n- addresses an important question, the resilience of Resnets\n- beyond state-of-art performance on adversarial attacks\n- extensive numerical experiments\n- possibly principled arguments for noise amplitude (but see below)\n\nConcerns:\n- I feel uncomfortable with the insistence on \"Ito ensembles\" and \"Ito calculus\". Ito calculus refers to stochastic differential equations obtained by taking a specific continuous limit (in contrast to Stratonovich), and this plays a marginal role here. The Ito Ensemble networks introduced in the paper are in fact simply discrete networks with additive noise, so  calling them \"Ito ensembles\" is a little misleading.\n- the Ito formalism is in fact only used to derive the scaling of the noise term with layer depth. I have however found that part difficult to follow (while it lies close to my expertise). In particular, the specific omega/(1+t) scaling is introduced without motivation, while presumably it should be explainable with a simple scaling argument. I am probably missing something, but it is not completely clear to me how this scaling leads to the final equation, where the depth t is absent. It would be important to make the arguments more transparent, as this is a key part of the paper.\n- It would have been interesting to see what happens numerically when the noise follows a different scaling with depth. How much do the resilience/performance results ultimately depend on the specific scaling?\n\n\nOther feedback:\n- the last paragraph before Section 4 is particularly hard to follow: \n\t* \"interpreting p(x,t) as a function that is constant along trajectories\" - not sure what this means, p(x,t) is the density of trajectories\n\t* \"p(x,t) corresponds to the following differential equation ...\" - the equation that comes next is simply the equation for the mean trajectory, so again not sure what this is supposed to convey.\n- is the argument for noise amplitude specific in any way to Resnets, or does it apply to any deep network?\n- Table 5 shows performance as function of w for Imagenet Results. It would have been nice to see it also for Cifar-10\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Ensembles of neural networks are more robust to perturbations of inputs",
            "review": "The authors propose a strategy to make the original ResNet more robust through adding a Brownian motion term to the ODE corresponding to the original ResNet model and taking ensembles of different disctretizations of the resulting stochastic differential equation. Greatly improved robustness is demonstrated on standard tasks, with a small decrease in accuracy of performance. The technique is on appearance simple, straightforward to use and works well in practice.\n\nOne crucial difference that the authors point out which sets their work apart from previous work in this area is not having to make the model robust to a specially selected set of adversarial attacks, or to limit the size of the adversarial perturbation that a model is able to withstand.\n\nSome comments. \n\n1. What about trying ensembles where the $\\omega$ term can be different for different members of the ensemble? I would predict that such ensembles would be more beneficial than ones that use a single value of $\\omega$. Like an ensemble with $\\omega = 0.2$ for 10 members and $\\omega = 0.4$ for 10 members.\n\n2. Can these techniques for robustness be generalized beyond ResNets? Are there equivalent techniques for other models that the authors know of?\n\n3. No details on the training or inference are provided, making reproduction of the results in this paper very challenging for the user. Could you please make it more clear how exactly the proposed networks are trained? As I can guess, each network in the ensemble uses a different discretization. This raises the question --- namely, how these different discretizations of SDEs computed and how the backpropagation procedure to find the weights of the network is done? You should mention what sort of packages can be used to accomplish this. Also, once each network in the ensemble is obtained, how exactly are \"independent inferences\" drawn from each model?\n\n4. The use of ensembles of several models to provide robustness has been widely used in the machine learning literature before. The authors should review or at least mention other approaches to constructing ensembles and comment on why their method provides an advantage versus existing ones. What is it about adding a Brownian motion term to an SDE that acts to help with improving robustness?\n\nOverall, however, the results seem pretty impressive to me and the technique seems simple enough to be used in applications of ResNets, so I would tend towards acceptance of this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "interesting paper that extends neural ODE",
            "review": "* quality\nThe paper present to the setting of stochastic ODE, which is a very interesting idea. The authors argues such neural nets are stable, but the argument is hand waving and lacks theory.\n* clarity\nThe paper is well written, although the part about robustness needs more details, and the theory should be more rigorous.\n* originality\nThe idea seems original.\n* significance\nAlthough the paper's idea is interesting, the idea seems very straightforward, and marginally incremental. If the authors can contribute more provable theoretical robustness guarantees, then the paper would be a good accept. Right now it isn't.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "I have many concerns on this submission. -- After reading the rebuttal, nothing new in this paper. The authors turn a deaf ear on the literature. The robustness is due to obfuscated gradient.",
            "review": "This paper proposes to do adversarial defense by adding noise to Resnets and model ensemble, which seems not new to me. I have substantial concerns from the model to experiments about this paper.\n\n\nMajor comments:\n\n1. My first major concern is what is the difference between this paper compared and the paper ``````\"Resnets ensemble via the Feynman-Kac formalism to improve natural and robust accuracies, NeurIPS, 2019’’. In this paper, the authors have already interpreted the adversarial vulnerability of ResNets from a transport equation viewpoint and proposed to ensemble the noise injected ResNets informed by the Feynman-Kac formula. The work in NeurIPS 2019 has been restated in the section ``Robustness of stochastic Ito resnets ensemble’’. Please justify this. Furthermore, the authors ignored all the existing works along the direction of injecting noise into neural networks to improve robustness, which are the most related works to this paper. Just name a few here. \"Towards robust neural networks via random self-ensemble, ECCV, 2018\". \"Resnets ensemble via the Feynman-Kac formalism to improve natural and robust accuracies, NeurIPS, 2019\". \"Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack, CVPR, 2019\". \n\n2. I am concerned about modeling Resnets as an ODE, why is the scale of different layers consistent; in particular, how can you consider the step size goes to zero? In ResNets, the dimension of different layers varies, how is this reflected in the ODE model? The derivation in section 3 is very questionable. The authors need to justify why making an ODE model of Resnet makes sense. \n\n3. How is the backpropagation of noise injected ResNet done?\n\n4. The experiment parts did not consider the obfuscated gradient at all! This makes me feel the reported robustness results are not trustworthy. Let me elaborate here: 1) Due to noise injection, the gradient used in the attack is not the true gradient, which makes an obfuscated gradient. To attack the model, the author should use the EoT attack. 2) The authors should attack models with IFGSM with more iterations, say 1000 iterations, and compare with PGD adversarial training in the same setting. 3) The ensemble of models increases the model capacity, which makes comparison unfair. As is pointed out in ``\"Towards Deep Learning Models Resistant to Adversarial Attacks, ICLR 2018’’, model capacity is crucial for adversarial robustness. 4) The authors may also attack their model with gradient-free attacks. \n\n5. In a nutshell, to my understanding, the algorithm proposed in this paper has been published before and the reported robustness is based on obfuscated gradient, which is well known in this community. Based on these, I have to give a very low score for this submission.\n\n\nMinor comments:\n\n1. Second paragraph: the authors state that preprocessing methods can improve robustness. This is questionable, as pointed in the paper \"``Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples, ICML 2018’’, these approaches did not really improve robustness.\n\n2. Second paragraph: the authors state that these theoretical guarantees on worst-case inputs hold only for small perturbation. This seems wrong, please read the certification papers carefully and check the certified robust radius. For instance, ``\"Certified adversarial robustness via randomized smoothing, ICML 2019’’. I did not see a noticeable advantage of the robustness of this paper over existing certifications.\n\n3. Typos still exist. Please check the paper carefully.\n",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}