{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The submitted paper contains interesting theoretical insights into common approaches for exploration and proposes a new way for deriving intrinsic rewards for exploration which is evaluated in several benchmark environments. While all reviewers appreciate these aspects, there are concerns about whether the paper is ready for publication. In particular, the authors’ response did not clarify all open questions and concerns (although the authors already improved the paper a lot by updating the submitted paper according to recommendations/questions of the reviewers). After discussions and author feedback, 3 knowledgable reviewers suggest (weak) rejection of the paper and 1 reviewer suggested acceptance of the paper. Considering this, I recommend to reject the paper but I would like to encourage the authors to consider the comments of the reviewers to revise their paper accordingly, as I expect the paper to then turn into a strong and impactful one."
    },
    "Reviews": [
        {
            "title": "Interesting work on exploration in RL, needs a clarification on transition function",
            "review": "This paper proposes a method for exploration in reinforcement learning by using the uncertainty over the value function as an intrinsic reward. It also offers an interesting theoretical analysis on the problem of estimating uncertainty over the value function. The paper is clearly written in general and has mentioned the related methods sufficiently. Moreover, the authors have compared their method with state-of-the-art models. In the experimental results section, the authors have shown that their model works well both in deterministic and stochastic environment. The noise in this stochastic environment, however, is very low (To be more precise the entropy of transition function is low). This is actually my main question/concern about the paper. In the beginning of section 3, the authors have mentioned that they \"fix\" the transition function in calculation of uncertainty over the value function. Does this mean that in that part, p(s'|a, s) is set to 1 for the most probable state and 0 for others? If this is the case, the method might not work well when the entropy of the transition function is high (e.g. the agent goes to one state 50% of the time, and to another 50% with an action), or in continuous state space environments where the actions are noisy. In the mentioned cases, sampling cases might actually work better because at least they consider those states in the future. I think it will improve the paper a lot if the authors add such environments to their analysis and discuss such problems.  \n\nUpdate: Thanks the authors for their response. Based on the other reviews and authors' response I decrease my score by 1 point.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Review for Reviewer 1",
            "review": "This paper proposes to use an intrinsic reward based on uncertainties calculated from temporal difference errors. The approach, called Temporal Difference Uncertainties (TDU), estimates the variance of td errors across multiple (bootstrapped) parameters, for a given state, action, next state and reward, where variability is due only to variance in parameters. The other addition is to learn a separate set of action-values that use this intrinsic reward, from the bootstrap set. Actions are then taken by randomly sampling an action-value function from the combined set. \n\nThe idea of using td uncertainties as an intrinsic reward is interesting, and should help avoid the fact that bootstrapping value functions alone likely provides insufficient exploration. However, the paper in its current form is not yet ready for publication for two main reasons. First, there are significant gaps in motivating and detailing the TDU technique. In sections 1 and 2, TDU is motivated as an exploration method deriving “an intrinsic reward from the agent’s uncertainty over the value function.”  In particular, it is heavily implied that TDU’s uncertainty estimate does not suffer the bias indicated by Lemma 1.  But it is not detailed how the quantity \\sigma(\\tau) estimates value function uncertainty, nor how it is unbiased. Why is it a better choice than, for example, directly using the bootstrapped set of action-values? There is some intuition provided, as well as an informal argument following from the distribution p(\\delta|\\tau) to uncertainty over value function parameters \\theta.  But this intuition and lack of formality is at odds with the formality highlighting that estimating value function uncertainty is hard.\n\nSecond, the empirical results appear to be obtained with 3 runs and do not provide significant evidence of improvements. Considering the apparent variance in all techniques, no meaningful conclusions may be drawn from comparison between the average of so few samples. The reported shading is also not explained, though I suspect it is standard errors. For only 3 samples, standard errors are not reliable, and in either case here are quite large. It would be better to run on few environments, and try to provide a stronger claim about the role of tdu. Even better would be to also highlight if results change significantly with changes to hyperparameters.   \n \nBelow, more detailed comments are given about the paper. \n\nIt would be useful to better discuss the use of Bootstrapped DQN (BDQN), and why this approach improves on BDQN. In Janz et al. 2019, there is a thoughtful and detailed analysis of one particular failing of BDQN methods (cf. section 5.3) including with a specific demonstrative environment (the binary tree MDP with randomized actions.) While this TDU paper does make a compelling argument that TDU addresses certain challenges of BDQN methods, TDU’s performance on the binary tree MDP (or one sufficiently similar) is not included.  This is a nontrivial omission.  If TDU succeeds on the environment, then a significant challenge is overcome. And if TDU does not succeed, then it is still worthy of publication, just with some discussion.\n\nThe theoretical contribution seems like it could be interesting, but it is not fully clear. Is this a statement about all methods that try to estimate some form of value uncertainty? Or those based on PSRL? Lemma 1 is summarized as “the harder it is to generalise, the more likely we are to observe a bias” in value function estimation, which is helpful, but warrants further detail.  A worked, demonstrative example may be edifying here.\n\nIt would be useful to discuss more why estimating TDUs is easy (as per the title of Section 3). It looks like the quantity in Equation 4 relies on p(theta), which was stated to be difficult to maintain. A bootstrapped distribution is used to get, as described in Section 4, but Section 3 did not make it clear to me why estimating sigma(tau) was straightforward. There are two additional statements here that could use clarification:\n1. Why does using the standard deviation put this bonus on the same scale as the reward?\n2. The paper says: “We compare TDU to (a) a version where σ is defined as standard deviation over Q and (b) where σ(Q) is used as an upper confidence bound in the policy, instead of as an intrinsic reward (Figure 2).”  These choices are very interesting, but should be explained in more detail.  Especially why their results indicate that “the key to TDU’s success is that the intrinsic reward relies on a distribution of TD-errors to control for environment uncertainty.“\n\nMinor comments:\n1. “neural networks, which are prone to overfitting and tend to generalise through interpolation (e.g. Li et al., 2020; Liu et al., 2020; Belkin et al., 2019)”  This is not an accurate representation of the cited results.  Those papers do not argue that neural networks are prone to overfitting.  Indeed, they argue the opposite: neural networks do not seem to overfit in the overparameterized limit, despite their tendency to interpolate the training set.  That is, generalization is maintained, where high generalization is characterized as per convention: low prediction risk w.r.t. an independently sampled test set.  While this is a significant mischaracterization, it does not appear to be critical to the paper, so we choose not to include it in the rejection justification.\n\n2. The colored bar chart to render parameter sweeps (Fig 2 left, center left, and center right) is a bit distracting. The use of color does not seem to be helping here. \n\n3. The related work section describes and cites some of the same work as the introduction, and is hence some of it is a bit redundant.  There is no need to duplicate that part of the survey in this section.\n\n4. There is an incomplete sentence: “While this can be effective in sparse reward settings, ... it can also lead to arbitrarily bad as the exploration (see analysis in Osband et al., 2019).”\n\n5. In appendix section D.1, there appears to be a missing citation: “Finally, instead of n-step returns we utilize Peng’s Q(λ) as was done in (Value-driven Hindsight Modelling: citation needed).”\n\n6. This work looks at computing variances directly, and using that variance for uncertainty, rather than using the bootstrapped action-values. This citation seems relevant: \"Context-Dependent Upper-Confidence Bounds for Directed Exploration\", Kumaraswamy et al., 2018.  \n\n------- Update\nThe response and update was a huge improvement. I now much better understand the goals, and the authors added some content that I think made the paper stronger and clearer. One outstanding issue is still that I do not understand why E_M[Q_M^pi] is the gold standard, and why bias is measured relative to that quantity. I explain this more below, but first mention some of the additions I really liked. For an upcoming paper, if this issue is remedied, I think this will be a good paper. \n\nThank you for introducing Proposition 2 and the explanation about bias beforehand. This very much clarifies the motivation for using TD errors. The result itself highlights that using TDU will have a lower bias to the true expected TD error if the bias for the action-values for (s,a) and (s’,a’) are both in the same direction (both positive or both negative). Otherwise, however, it looks like the bias of TDU is strictly worse. One issue though with this result is that the magnitudes of these quantities could be different. The comparison between bias for TDU and the bias for Q seem a bit like comparing apples and oranges, and I would in fact expect the TD error to be smaller in magnitude and so naturally have a smaller bias. How much is due to this and how much to relative bias reductions? I suspect there is a real bias reduction here, but clarifying this would help.\n\nFor the variance result, it seems better to directly report 47, and the discuss ramifications, rather than writing that they all need to be approximately equal. By the way, it is too bad that the result is a bit weaker for the variance, which is precisely the quantity you care about for defining your rewards. But nonetheless this formalization is helpful and provides solid insight.\n\nI like that the empirical work was improved, including adding some comments about significance. I also very much appreciate the ablation, where you use the variance directly from the bootstrapped action-values as an intrinsic reward. My only concern here is that the magnitude of rewards might be quite different, since the TD error should be smaller than the action-values themselves. This might mean different beta are needed.\n\nHowever, I remain unsure about the importance of this bias that TDU mitigates. You state: “Our analysis shows that biased estimates arise because uncertainty estimates require an integration over unknown future state visitations.“ It remains a strong statement that uncertainty estimates require integration over unknown future states. As one example where this does not seem to be true is the Kumaraswamy paper you have cited. They show that if you use LSTD to get estimates of the action-values for a fixed policy, then you can get an estimate of the variance of the parameters. Maybe this setting assumes too much, and so it does not invalidate your result. But, I do believe a more clear argument is needed in this section for this result, as I expand on below.\n\n“Methods that rely on posterior sampling under function approximators assume that the induced distribution, p(Qθ ), is an accurate estimate of the agent’s uncertainty over its value function, p(Qπ ), so that sampling Qθ ∼ p(Qθ ) is approximately equivalent to sampling from Qπ ∼ p(Qπ ). ” This is a strong statement. I do not see why it is true. Are you suggesting that the agent must have the true Qpi? Is it not enough to use uncertainty estimates (epistemic uncertainty) for a function approximator? This result seems to show: if we want to mimic uncertainty estimates over true action-values for different models, then this is not possible under function approximation. But, that is maybe reasonable. Instead, shouldn’t we ask: how can we mimic uncertainty over approximate action-values for different models? (i.e., relative to our function class)\n\nAdditionally, you call E_theta[Q_theta] an estimator and discuss it’s bias. But, isn’t that quantity not random? I presume E_theta[Q_theta] is the expectation, and the difference to E_M[Q_pi] is the bias. But, then what is the estimator that is biased?\n\nMinor comments:\n\n“However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself. “ What does this mean?\n“state-action transitions ” what is that? I think you mean just transition, since you condition on the whole thing\nIn the proof of Lemma 1, the Variance would remove the expectation over r term. Your result still holds, but the proof itself looks like it should be separated into the two cases.\n“While Proposition 1 states that we cannot remove this bias unless we are willing to maintain a full posterior p(θ), ” It is not clear how this result shows this. What if I maintained a full Gaussian posterior over theta? Would that solve the problem? What is a partial posterior?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting exploration approach",
            "review": "Summary:\nThe authors introduce the use of value function variance (conditioned on state transition) as auxiliary reward promoting exploration during training. The variance is estimated using the bootstrap DQN approach. The main difference with similar methods is that the value uncertainty is not used in a Thompson sampling scheme but it is instead use to provide exploration reward. \n\nRelevance:\nThe paper address a central problem in deep RL. \n\nOriginality:\nThe central idea is rather original and it elegantly combine ideas the value uncertainty estimation approach with the auxiliary exploration reward strategy. This new form of auxiliary reward is rather attractive as it automatically scales down during training as the model becomes more certain.\n\n\nScientific quality:\n- The new method is generally well motivated. However, while the author use a Bayesian terminology (e.g. prior, posterior), I am not convinced that the proposed bootstrap method for uncertainty estimation has a clear Bayesian interpretation as a form of approximate inference. Boostrap resampling can lead to proper Bayesian posteriors when the likelihood is used to weight the samples but this does not seem to be the case in this work. \n- The experiment section contain an interesting set of experiments and compare with several relevant baselines. \n-The paper is clear and very well written. However, I am not convinced by some of the claims. For example, the author claims that since reward uncertainty estimation is biased then networks cannot meaningfully generalize. While the claim can definitely be true, I do not think it follows from their premises. I would like to see the authors to expand this argument. \n\nPros:\n- Simple and elegant new method\n- Very good performance when compared with a large number of relevant baselines\n\nCons:\n- Some claims are not completely justified\n- It would have been informative to have a wider range of problems in the experiments section",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Focused and sound",
            "review": "Summary:\nThe paper proposes a novel heuristic approach to estimate the uncertainty of the value function of an agent. Since this heuristic for estimating the uncertainty can be applied not only to tabular RL, but also when using function approximation, it is an interesting approach.\n\nStrong points:\nThe proposed heuristic can be used to estimate the uncertainty not only for tabular RL, but also when using function approximation, without relying on the ability of the function approximators to specify a value for the uncertainty.\n\nWeak points:\nThe investigated benchmarks are essentially deterministic and cover a very limited range of the spectrum of RL in general.\n\nRecommendation:\nThe paper is well written and clearly structured. It introduces a new approach that is likely to be beneficial in a relevant number of problems. I recommend to accept the paper.\n\nAdditional feedback with the aim to improve the paper:\n„However […] obtaining accurate uncertainty estimates is almost as challenging a problem.“ As challenging as what?\nIt remains unclear, what is meant by \"diverse and deep exploration\".\nAt (\\beta >> 0), please use a proper LaTeX command for >>.\nPlease do correct missing capital letters in the bibliography: e.g.  bayesian, q-\n\n\n-----------------\n(Nov 26.) After reading Review1 I lower my Confidence.\n(Nov 29.) Taking into account the other reviews, the authors' responses to these reviews and the discussion, I now think the paper is not quite ready for publication and lower the score to 5.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}