{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Although all reviewers agree that the work is interesting and has potential, several issues in the presentation and the experimental section (especially regarding the ablation) need to be worked on before granting acceptance to the paper. "
    },
    "Reviews": [
        {
            "title": "A review on a simple unified information regularization framework for multi-source domain adaptation",
            "review": "**Summarize what the paper claims to contribute.**\nThe paper introduces a multi-source information-regularized adaptation network (MIAN). MIAN consists of three parts; information regularization, source classification and batch spectral penalization. The information regularization in MIAN provides an information-theoretic interpretation of multi-source domain adaptation. This interpretation seems to have lower information variance compared to the other multi-source domain adaptation technique relying on multiple binary domain classifiers. The performance of MIAN is evaluated on three benchmarks and it achieves competitive performance comparable to state-of-the-art methods.\n \n**List strong and weak points of the paper.**\n\n*strong points*\n\nThe proposed information-theoretic regularization is well-motivated and profound compared to the previous works on multi-source domain adaptation.\nGive a first attempt for theoretical explanation on multi-source domain adaptation by extending previous approaches dealing with only source and target domain cases.\n\n*weak points*\n\nA derivation, an ablation study and an analysis on the proposed method are missing.\nThe paper seems to be not finished yet.\n \n**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice**\n\nI give “Ok but not good enough - rejection (4)” to this paper. The motivation of the paper is plausible to multi-source domain adaptation and the proposed regularization technique seems to be profound. But the supporting evidence of the proposed method, such as appropriate derivation and proof, ablation study and analysis, are missing in the paper. I conjecture that the paper is not finished yet.\n \n**Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.**\n\n*variance of information*\n\n“In contrast, the variance of our constraint (5) is inversely proportional to $(N+1)^2$” I can’t find the derivation of this term, which is a crucial statement for the proposed method. In the supplementary materials of this paper and (Roh et al., 2020), i could not find the related covariance terms. Could you explain the variance of $\\hat{I}(Z; V)$ in eq (5) and $\\sum_{k=1}^{N}{\\hat{I} (Z_k ; U_k) }$ in terms of the number of domains $N$?\n \n*ablation studies on MIAN*\n\nWhich one is the counterpart of MIAN? I want to know the performance of the baseline architecture without MIAN regularization, which is an empirical evidence to check whether MIAN is working or not.\n \n**Provide additional feedback with the aim to improve the paper.**\n\n*paper editing issues*\n\nThe methods in the table are not cited in both the table and the main text. This makes it hard to read the experiment table.\n \n*Missing related works.*\n\nRefer [R1, R2] for multi-source domain adaptation. The paper [R2] achieves similar performance with MIAN on multi-source domain adaptation on office-31 benchmark, although it doesn't report standard deviation.\nThe major works on multi-source domain settings are dealing with domain generalization and larger scale benchmarks, such as DomainNet.\n \n*analysis on the method*\n\nI think that the motivation of this paper can be intuitively explained by using a figure description of the Venn diagram of information-theoretic measures. Refer to the Venn diagram figures on the page [R3].\nIs it possible to quantify the information shared among/between domains? I think that kind of measurement is very effective for visualizing multi-source domain adaptation & domain generalization.\n \n[R1] Boosting Domain Adaptation by Discovering Latent Domain, CVPR 18\n\n[R2] Domain Specific Batch Normalization for Unsupervised Domain Adaptation, CVPR 2019\n\n[R3] https://en.wikipedia.org/wiki/Information_theory_and_measure_theory",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "strong motivation, but limited contribution and unclear explanation",
            "review": "This paper proposes a simple unified information regularization framework for multi-source domain adaptation. Experimental results verify the effectiveness of the proposed method. \n\nThe paper is well organized. The motivation is clear and the idea of reducing multiple discriminators to single discriminator is very reasonable. The experiments are somewhat convincing.\n\nThe main weaknesses of this paper lie in the following aspects:\n\n- The authors claim that domain-discriminative knowledge from other domains is not fully leveraged by multiple individual discriminators, while their proposed MIAN can filter out domain-specific information while preserve domain-shared information. However, such a claim is not supported by solid theoretical or empirical evidence but only the authors intuition.  Since the motivation of the whole paper is based on this claim, it greatly undermines the basis of this paper. Hence, it is critical for the authors to provide sufficient evidence to demonstrate this claim.\n\n- The overall contribution is not that large, as  the proposed information regularization method is simply based on recently proposed related work.  And some aspects of the proposed method are not clearly explained. For example, why Eqn. (3) and Eqn. (6) are equivalent? It is not very obvious to me. \n\n- The experimental results are not good enough. The proposed MIAN exhibits good performance on small datasets like Digits-Five and Office-31. But its performance (an average of 73.12) on larger dataset like Office-Home is worse than other competitors like MFSAN (an average of 74.1). Furthermore, why not conduct experiments on DomainNet, the largest benchmark created specifically for DA?\n\n- The experimental analysis seems to be inadequate. The paper lacks hyperparameter sensitivity analysis, which is standard practice in UDA area. Furthermore, although there are some analysis on variance of stochastic gradients, there is no evidence to support that this will lead to better performance, or  increased computational-efficiency or improved stability.\n\n- The presentation can be greatly improved. Some typos exist and some mathematical equations are not formal enough. E.g., $\\Xcal$, $\\Zcal$ and $M$ are used without or before definition. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple, powerful multi-source information regularized domain adaptation framework ",
            "review": "This paper studies the multi-source domain adaptation problem. The authors examine the existing MDA solutions, i.e. using a domain discriminator for each source-target pair, and argue that the existing ones are likely to distribute the domain-discriminative information across multiple discriminators. By theoretically analyzing from the information regularization point, the authors present a simple yet powerful architecture called multi-source information-regularized adaptation network, MIAN.\n\nI vote to accept the paper.\n-This paper has a clear motivation, is well written, and establishes the final objective step by step with the theoretical supports. \n\n-The proposed objective is simple but powerful. I enjoy reading the analysis of advantages over the existing solutions, which is well reflected in the experiments. The reported performance is competitive, even compared to the missing reference ECCV’20 (see below).\n\n-The quantitative analyses validate the effectiveness of the model design. Particularly, the analysis on variance of stochastic gradients validates the technical benefits on optimization stability. \n\nA few comments/suggestions.\n-A few recent MDA works are missing, including but not limited to [ref-1, ref-2, ref-3]. Although not all of them using image datasets as testbed, but I would encourage the authors to include and discuss them under the same structure. \n[ref-1] Hang Wang et al., Learning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation, ECCV 2020\n[ref-2] Chuang Lin et al., Multi-source Domain Adaptation for Visual Sentiment Classification, https://arxiv.org/abs/2001.03886\n[ref-3] Haotian Wang et al., Tmda: Task-specific multi-source domain adaptation via clustering embedded adversarial training. ICDM 2019.\n\n-In section 3.3, the authors argue that their frameworks filtering out domain-specific information while preserving the amount of domain-shared information. The statement and the earlier discussion are intuitively correct to me. However, it would be great to see a quantitative or qualitive study on the effectiveness of the preserved domain-shared information and domain-specific separately.\n\n-Minors. In page 5, “It bias the representation towards …” -> “biases”\n\n=====\nUpdates: Thanks for the authors' response. I carefully read other reviewers' comments and responses. My concern on the missing study of empirical or theoretical support of claim \"framework can filter out domain-specific information while preserving the amount of domain-shared information\" was also raised by other reviewer. Overall, I still believe this paper provides new insights for this field.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper aims to propose a new method for multi-source domain adaptation with a strong theoretical flavour but the writing is hard to follow and the experimental comparison needs clarification. ",
            "review": "This paper proposes a Multi-source Information-regularized Adaptation Networks (MIAN) for multi-source domain adaptation. The presentation has a strong theoretical flavour. MIAN is evaluated on three benchmark datasets against other methods to show its superiority. At times, the paper is difficult to follow and lacks clarity. Please see my detailed comments below.\n\n1. The paper is organised with three pages of theoretical insights (Section 3) after Section 2 on related works, which could be difficult for many readers. It could be better to convey the intuition, high-level idea, or big picture with some visual illustration that can help readers appreciate the proposed idea(s).\n\n2. The paper reproduced quite some existing theories (and also a proof in the  appendix). I am wondering whether these materials can be presented in a more accessible and compact way. \n\n3. Although source code has been provided (which is good), providing pseudo-code can help readers better understand the proposed method and differentiate it from other existing ones. \n\n4. It will be better to perform some computational complexity analysis to give a fuller picture including the efficiency of the proposed method. \n\n5. At the top of Page 7, it says \"we reproduced all the other baseline results using the same backbone architecture and optimizer settings as the proposed method\". Have you tune the optimizer settings for the proposed method and baselines? How did you determine the hyperparameters reported in the appendix? Grid search?\n\n6. It is not clear why standard deviations are not reported for Digits-Five.\n\n7. On the three datasets studied, the models evaluated seem to be different for different datasets. For example, there is no result on Single-best at all for Office-Home. For \"single-best\", seven methods (besides source only) were reported for Digits-Five but only DAN and JAN were reported for Office-31. \n\n8. Repeating each experiment for only four times may not give a good estimation of the variance. In addition, the 100.00+-0.00 result of DAN on DSLR is impressive. I am wondering whether this perfect result will hold if you have more repetitions. \n\n9. On Office-31, MIAN actually only outperforms JAN's single-best by 2%, and the second best by 1.4%, which is far from a large margin claimed. \n\n10. Figure 2a and 2b (similarly 2c and 2d) are hardly readable on print.\n\n11. Minor issues.\nFigure 1 could be put at the top rather than in the middle of text. \nAcronyms are not always defined, e.g. FCN in Figure 1, although knowledgeable readers know what it means.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}