{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers' judgements, this is a good submission but hasn't reached the bar of ICLR."
    },
    "Reviews": [
        {
            "title": "Novel Policy Seeking with Constrained Optimization",
            "review": "Summary:\n\nThis paper attempts to solve the problem of seeking novel policies in reinforcement learning from a constrained optimization perspective. This new perspective motives two new algorithms to solve the optimization problem, which are based on feasible direction and the interior point methods. The authors provide empirical results on several Mujoco benchmarks. \n\nDetails:\n\nThe idea of formulating the problem from a constrained optimization perspective is interesting. This new perspective motivates new and better algorithms to solve the optimization problem. \n\nHowever, I feel like the presentation is poor and the writing should be improved.  \n\nWhat’s the exact problem setting? The authors should clearly describe the problem setting before presenting the methods, even one paragraph would be helpful.  \n\nA lot of algorithm details are missing: \n\nQ1. $\\bar{D}^q_W (\\theta_i, \\theta_j)$ is a metric for any state distribution $q$. What’s the motivation of using $q = \\bar{\\rho}$? \n\nQ2. When computing the policy distance, what is $\\rho_{\\theta_i}$ in (4)? Is it the current policy, or a reference policy?\n\nQ3. I assume $\\theta_i$ is the current policy. According to (4), the algorithm uses $\\theta_i$ to get samples, and compute an importance correction ratio $q/\\rho_{\\theta_i}$ to approximate the distance. How is the $q(s)=\\bar{\\rho}(s)$ computed? The authors propose to approximate $\\rho_{\\theta}$ using monte-carlo methods. Does it mean the algorithm need to approximate $\\bar{\\rho}(s)$ using the reference policies for each $s\\sim \\rho_{\\theta_i}$? Is there a computation issue?\n\nQ4. This goes back to Q1. Why just using the on policy samples to estimate the distance? Is there any potential advantage to use $q = \\bar{\\rho}$? \n\nQ5. Learning the stationary distribution is a hard research problem itself. See recent work for example: \n\nZhang, R., Dai, B., Li, L. and Schuurmans, D., 2019, September. GenDICE: Generalized Offline Estimation of Stationary Values. In International Conference on Learning Representations.\n\nI agree the stationary distribution can be approximated using MC methods, but it might need a lot of samples as the variance is very high. This makes me wonder how is the algorithm implemented in practice, and how does the stationary distribution estimation subroutine affect the algorithm’s performance. \n\nOther suggestions:\n\nIf I understand correctly, this paper tries to solve the problem of finding a set of novel polices that solve a given task while exhibiting different behaviors. This seems also related to the exploration problem, as some works try to make the current policy different with previous policies to encourage exploration. See for example:\n\nHazan, E., Kakade, S., Singh, K. and Van Soest, A., 2019, May. Provably efficient maximum entropy exploration. In International Conference on Machine Learning (pp. 2681-2691). \n\nIt might be worth to discuss how the novel policy seeking problem is related to the exploration problem. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting method but can be more convincing",
            "review": "Summary: This paper proposed a method to leverage the constrained optimization for policy training to learn diverse policies given some references. Based on a diversity metric defined on policy divergences, the paper employs two constrained optimization techniques for this problem with some modifications. Experiments on mujoco environments suggest that the proposed algorithms can beat existing diversity-driven policy optimization methods to learn both better and novel policies. Generally, the paper is well-written and easy to follow. Some concerns/comments:\n\n* The state distributions of proposed CTNB and IPD are different: In the CTNB method, the trajectories will keep rollout until they reach some termination conditions such as time limit or failure behavior. However, in the IPD method, if the cumulative novel reward is below some thresholds, then the trajectories will be truncated. It will be helpful to compare the CTNB with that extra termination condition. \n\n* Using the divergence of policies to quantify the difference between policies seems not a very innovative metric. Some related work could be:\n\nHong, Z. W., Shann, T. Y., Su, S. Y., Chang, Y. H., Fu, T. J., & Lee, C. Y. (2018). Diversity-driven exploration strategy for deep reinforcement learning.\n\nIt will be great if the authors can compare and explain the relationship between the proposed metric and some related ones.\n\n* The experiments can be more convincing if more locomotion environments are included, especially some higher-dimensional environments such as Humanoid and HumanoidStandup. Also, some other environments with a long-term/sparse reward setting can be more illustrative such as some mazes or Atari games. For some of those games, since it is stage-based, the IPD might terminate some rollouts if all reasonable policies are similar at the beginning of the trajectory. For a maze example, all good policies should choose to open the door at the beginning and then behave diversely.  \n\nOther/Minor Comments:\n\n* The choice of r_0 can affect the performance: When sequentially training the policy, should r_0 be adjusted when training each new policy?\n\n* It can be more interesting if some visualization of hopper policy diversity is included.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Seeking novel policy only after finding a good one ",
            "review": "This paper aims at novel policy seeking which incorporates curiosity-driven exploration for better reinforcement learning. This paper first propose to use  a Wasserstein-based metric to calculate the difference between policies, and use it to define  the policy novelty. With these, the authors modeled the novel policy seeking as a constrained markov decision process(CMDP) and solved it using CTNB and IPD.\n\n1. This paper allows to consider the novelity issue dynamically.  However, when training policy according to the proposed CTNB or IPD, there should be some pretrained policies as perconditions, in other words, the proposed method needs some prior knowledge rather than learning policy from scratch. This may be a limitation for its application.\n\n2. About the proposition 2, the single trajectory estimation is unbiased, however, the variance seems to be large, the influence about the estimation variance should be considered.\n\n3.  in fomula (5) and (6), is $r_{int, t}$ equal to $r_{int}$ ? If is,  why use t? and what does moving average mean since there are several kinds of moving averages?  \n\n4. in fomula (4) and (6), Are Ts the same?\n\n5. Fig.2  shows that in Waklker2d and HalfCheetah, the proposed CTNB has less novel than PPO, which doesn't match the purpose of CTNB.\n\n6. It seems not easy to tune the novelty threshold for different task, as it performs different on different tasks. Can the author provide some insight on how to tune this.\n\n7. Five random seeds is not sufficient for experiments. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "This paper proposes a novel constrained optimization based method, to optimize the expected return as well as encourage novelty of a new policy in contrast to existing policies. By modeling the problem as a constrained optimization problem, they can avoid excessive novelty seeking effectively, which is common in existing methods which model the problem with multi-objective optimization. \n\nTo be specific, they first propose a novel metric to measure the novelty of a new policy. To estimate such metric on sampled state with dense online reward, they propose an importance-based estimator for the proposed metric. With the estimation of the novelty metric, they propose to formulate the problem as a constrained optimization problem. The novelty is constrained to be larger than certain threshold r_0. In this way, the algorithm will only encourage larger novelty when the novelty is less than r_0, therefore avoiding excessive novelty seeking which may hurt the performance. They improve TNB proposed in (Zhang et al., 2019) with CTNB, where the ∇θg term exists only when the constraint is violated. They also propose another method based on Interior Point Method. Since IPM is computationally expensive and numerically unstable, they made an adaptation to RL setting, by bounding the collected transitions in the feasible region. \n\nOverall, the method is intuitive and reasonable. I have the following questions:\n\n1. The first contribution of this paper, is proposing a novel metric to measure the novelty of current policy in contrast to existing policies. Why propose a novel metric? Is existing metric for measuring the novelty not good? If so, can you verify your claim in experiments? \n\n2. The hyper-parameter r_0.  From Figure 3, we can see the different performance under different novelty thresholds r_0. The algorithm seems to be sensitive to r_0, which is of course reasonable. How did you choose r_0 for different environments? Did you consider a soft r_0 rather than a hard constraint(that is, maybe the constraint has different weight for different r_0)?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}