{
    "Decision": "",
    "Reviews": [
        {
            "title": "An intuitive improvement for active learning yet less innovative and convincing ",
            "review": "The paper proposed a semi-supervised active learning method for sequence tagging, which is an important, well-studied task formulation in natural language processing. The key hypothesis in this paper is that incorporating unlabeled data into the process of active learning is better than using only labeled data. This is an intuitive and straightforward idea and was investigated back to McCallum and Nigam (ICML 1998). \n\nSpecifically for sequence tagging, the paper does not propose a novel metric for determining the (least) confidence of model predictions. The use of unlabeled data is here is simply an additional loss for learning a more powerful tagging model, which suggests that a so-called \"semi-supervised active learning\" method here is actively learning a model with semi-supervised loss. The findings and experimental results are therefore less interesting to the audience as this incremental improvement (via using more unlabelled data) is very predictable. The key challenge of using unlabeled data for efficiently guiding the selection of next-batch examples is not properly addressed in this work. \n\nAnother key component of the paper is the locally contextual CRF (LC-CRF), which uses both previous token x_{t-1} and follow-up token x_{t+1} to create additional potential functions for modeling y_{t}.\nThe motivation for this modification of CRF is, however, less clear to me in principle. Doesn't $h_t$ already encode the information of x_{t-1} and x_{t+1}, if using bidirectional LSTM like other conventional sequence tagging models? \nAlso, I didn't see the description of the LSTM that was used in this work, so I assumed that the authors used a uni-directional LSTM. Why don't you use a bi-lstm here? Although shown in the ablation study, the performance gain of using LC-CRF here is not convincing to me. Can you compare the LSTM+LC-CRF method with BLSTM+NC-CRF? Can you show more case studies for the readers to know why LC-CRF is needed here? Is this only helpful for active learning?\n\nFor the evaluation of active learning methods (for sequence tagging), can you create a fair comparison with the comprehensive, existing reported results on the datasets of Siddhant and Lipton (EMNLP 2018, \"Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.\")? They have made efforts for using different active learning methods on some of the datasets that this paper is using.  \n\nA practical but less discussed factor for evaluating active learning methods is the batch-size and the length of the duration of each training step. As our key motivation for using active learning is to use less human efforts and thus increase the efficiency of learning a model in a new domain, how we can keep a balance between model training and user waiting is important for a practical active-learning based annotation framework (e.g., Lin et al., AlpacaTag ACL 2019 demo).  I hope the authors can show more results on different batch-size of queries and model-training costs in each step.  Also, since the paper argues that the proposed method is better than simple self-training, would you also compare with that in experiments?\n\nIn summary, I agree with the motivation of using unlabeled data to improve active learning, but the proposed method is not particularly innovative or convincing to me. I would suggest a weak rejection.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The proposed method combines semi-supervised learning with active learning and can achieve great performance with only 30% of the data. The method is based on locally-contextual CRF and uses the 'least confidence' strategy. The experiments are conducted on CONLL 2003, ONTONOTES, Chunking and POS tagging. \n\nThe paper shows consistent improvements over baseline models that are purely supervised learning and CNN-CNN-LSTM.  \n\nI have three questions regarding the choices of the paper :\n1. Why not leverage state-of-the-art pretraining models such as BERT? Would the performance improvements be complementary to the benefits of pretraining models? \n2. Does using a different active learning strategy matter in your method?\n3. When you have a lot of labeled training data, how much does active learning help when compared to randomly selecting data points?\n4. In Figure 3(d), why would the performance of CNN-CNN-LSTM goes down when the amount of training data increases?\n\nI am willing to raise my ratings if the response can address my questions. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "*Summary*\nThis paper applies semi-supervised learning in the active learning loop and claims that semi-supervised learning methods bring better active learning. The paper uses the \"uncertainty principle” (choosing the least confident sample) for active learning strategy and run experiments on sequence labeling tasks such as Chunking, NER, POS. \n\n\n*Quality*\nThis paper does not seem to be ready for a conference as it lacks materials for reviewers to judge its contribution for the following reasons:\n* The paper is not proposing a new method for active learning but is performing semi-supervised learning in the active learning loop. With semi-supervised learning generally working better in sequence labeling tasks, I don't find the improvement of semi-supervised active learning very surprising. \n* I do not think other possible baselines are not considered that utilize (unlabeled dataset + low-resource). Unsupervised learning is very successful with the BERT-kind of models. I wonder what would happen if we provide the same amount of labeled data (randomly chosen) to the large unsupervised model. Do you think semi-supervised active learning will work better than this baseline? If you have already tried this, how much better does your model work?\n* I am not sure why a particular framework was chosen, LC-CRF, and what else has been considered.  I think other semi-supervised learning methods (perhaps the ones in section 3) could be considered too? While it might be hard to compute marginal probability P(x) with those models, as there are multiple ways to run semi-supervised learning, why not consider them?\n* In the same line, the comparison of LC-CRF to NC-CRF is not sufficient baseline comparison.  It is a good sanity check but feels like it shouldn’t be provided as another baseline as many people can conjecture that LC-CRF won’t work as well as LC-CRF. It seems like an extensive experiment on LC-CRF is just serving to make NC-CRF look good.\n* If the paper doesn’t address theoretical aspects, then I think the paper should address practical issues by experiments. I am not sure if this paper does.\n  * To elaborate, practically speaking, how would the authors choose a set of unlabeled instances \"to consider\" for active learning? As there will be a vast amount of unlabeled data, evaluating each of the instances would be too expensive.\n\n\nQuestions & feedback;\n* In Table 3, What do you mean by the “final result”? I may have missed, but when do you stop your learning? \n* SSL is generally less prone to overfitting. But, claiming that the proposed method is less prone to overfitting given Figure 3(d) seems like an overclaim.\n* I am actually not sure how authors select their samples from their pool using the “least confidence” rule. Do authors predict labels for every possible instance? Which I think would be too expensive.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper studies the sequence labeling problem in natural language processing. The authors propose a new locally-contextual conditional random field model combined with an LSTM model to represent potentials and also design a new training procedure using semi-supervised active learning. In the experimental section, the authors showed that the proposed model can improve the pure supervised active learning approach and can further improve the performance by leveraging the more unlabelled data.\n\nMain comments:\n\nThis topic is interesting and the paper is easy to follow. However, the novelty of this paper is marginal and both model and training procedure seem straightforward extensions of previous approaches. The authors mentioned that the effectiveness of the proposed approach is from the use of a large amount of unlabeled data while from the experimental results (e.g., Figure 3) it seems that the major gain is from the locally-contextual design compared to the baseline method (CNN-CNN_LSTM). It would be nice if the authors had also discussed the impact of active learning. \n\n\nMinor comments:\n1. I suggest the authors compare the performance between the proposed locally-contextual CRF with the state-of-the-art contextual-aware approach (e.g., BERT) with or without CRF.\n2. In section 2.1, I suggest that the authors use a more formal presentation (e.g., algorithm box, bullets) to present how the active learning and training procedure work.\n3. In the experimental setup, it's not clear the setting of the semi-supervised approach.\n  -- How many unlabelled data are using for different tasks?\n  -- In section 4.2, it would be better to do an ablation study to show the effect of the constant prefactor of the unsupervised part and why the proposed two-stage setting is useful.\n4. For the table 3, it does not clearly demonstrate the advantages of the proposed model.\n  -- From the results, it seems like using the same amount of supervised data and unlabelled data, the proposed model is lower than the state-of-the-art model.\n  -- It would be better to simplify the best-publised part to make the comparison clear.\n  -- It's not clear the purpose to show the performance of the CNN-CNN-LSTM (100 data points per round).\n\nHaving said that, I believe this paper can be an important contribution if the authors invest more in understanding the novelty of the approach and if more thorough experimental analysis is conducted.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}