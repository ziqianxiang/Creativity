{
    "Decision": "",
    "Reviews": [
        {
            "title": "Paper clearly not ready for publication",
            "review": "On the positive side, the paper reports good results on two standard datasets that are slightly above other methods reported in the paper. Surprisingly, no paper from 2020 is cited in table 1 even though the filed is moving fast.\n\nHowever, the paper has several major issues\n\n1) the writing is clearly below standard for a conferences such as ICLR. Besides the fact that the English grammar needs major rewriting, the key contributions of the work are not clear and the related work section does not position the work properly in the landscape of the literature\n\n2) the only results reported are in table 1 and no ablation study or any other insight is given about which component does contribute what to the improved performance\n\n3) the description of the approach is very unclear - in fact the so called \"external knowledge bank\" (probably the authors mean \"external knowledge base\"?) is essentially just using GloVE vectors - an approach that many few shot learning approaches have been using before. Also, GloVE vectors are not really representing knowledge but just semantic similarity - this is something very different and thus the entire motivation given in the introduction does not make sense to me as a reader\n\n4) while the general idea of generating a cross modal graph seems not insensible, it remained entirely unclear what the key aspect of the approach is to make this graph useful for few shot training. The generation of the graph itself (that is sensible and useful) is obviously the key challenge to be addressed for such an approach and it remained entirely unclear what the novel or different component of the proposed approach is that may result in good performance. Without giving a convincing presentation and argumentation for this aspect of the approach the value of the paper is rather low to both researchers in the area and in the broader field\n\nSo clearly the paper is far too premature to be considered for acceptance imho\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The basic idea is well motivated, while some details are not clearly described.",
            "review": "Traditional meta-learning FSL assumes that the semantic information during inference is accurate. This paper proposes a cross-modal knowledge enhancement mechanism, which can loose the accurate semantic information restriction by indexing relevant information from auxiliary semantic knowledge. Experiments on two few-shot image recognition datasets are conducted to support the proposed method.\n\nPros:\nThis paper proposes a method which can relax the existing constrains that accurate semantic information should be provided. This basic idea is well-motivated.\n\nA cross-modality knowledge graph is devised, which combines different types of knowledge graph and is helpful to fuse external knowledge into data representation.\n  \nCons: \nThe paper states that the disadvantages of existing methods lie in the dependence of accurate semantic information and claims that this paper is designed to alleviate this constrains. However, the external knowledge employed in this paper is still word embedding of semantic labels. So is the semantic labels are not accurate, will they affect the final performance? Please clarify this issue.\n\nAuthors argues that graphs building in class-level is more stable. However, no explanation or other evidence are given to support this claim. \n\nThe overall loss function contains different components, the role of each component is not clearly demonstrated.\n\nThere are some parameters that are not well defined. such as \\lambda_{i}, whether is a learnable or pre-defined parameter? Since the optimization part in Subsection 3.2 does not involve this parameter. It’s better to explicitly introduce it. \\phi_{t} is similar. \\phi_{v} is clearly stated to be trainable, however \\phi_{t} is not introduced. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More experiment analysis, details explanations, related works.",
            "review": "The authors propose to utilize a semantic knowledge bank to assist few-shot image classification task. Two knowledge graphs are built, one for visual prototype one for semantic prototype, and merged to build better class representation for the metric-based classifier. \n\nI have a question about the knowledge bank. The authors state that we don't know the relationship between the target classes and the knowledge bank,  but seems don't elaborate more details on which set of classes are the assistant knowledge bank includes? Only training classes in the dataset? or both training classes and evaluation classes? or all classes? or additional classes from other datasets? I suggest the authors to include more details with respect to how to build this knowledge bank since this is the proposed key point in this paper.\n\nAlso, I would suggest the authors to add more comparisons (in experiments) to zero-shot learning / generalized zero-shot learning methods. Since this cross-modality few-shot learning setting is similar to the ZSL setting, which also has semantic information, but knows which class the semantic information belongs to.\n\nAs the model includes multiple components, such as two knowledge graphs, message passing models, reconstruction loss, etc. I would be better if the author can include ablation studies which show the importance of different component.\n\nI am also interested in the analysis of how the knowledge bank can help / benefit the few-shot learning. For example, by showing the before and after representations of every class, or how the intra-adjacent matrix learns the relationship between classes.\n\nI am curious why the authors use the transformer model in Eq.8-9. Do you use the exactly same structure as the Transformer? how many layers? Can the authors elaborate more on the motivation and the model details, please?\n\nMissing related works: I would recommend the authors to include some related works, i believe is highly relevant. As far as I know, [a] is the first paper introducing how to leverage a gnn model structure for few-shot learning. Thus, i would suggest to include it in related works. [b] also leverages the graph structure and message passing mechanisms over prototypes. So I suggest to include it too. While the authors include some works in the field of ZSL, [c] as far as i know, is the first paper that proposes to leverage the graph structure for ZSL. I also recommend including this paper in.\n\n[a] Few-Shot Learning with Graph Neural Networks\n[b] Learning to Propagate for Graph Meta-Learning\n[c] zero-shot recognition via semantic embeddings and knowledge graphs\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Experiment section needs to be polished and incomplete comparison with existing works",
            "review": "The goal of this paper is to address the few-shot classification, where only one to five samples per class can be observed during the training stage. The proposed method uses external information to enhance the performance of the few-shot classifier. With the proposed Cross-modality Knowledge Enhancement Mechanism (CKEM), the cross-modality knowledge graph is constructed based on visual information and semantic information, and it is claimed to be beneficial for the few-shot classification. In the experiment, they demonstrate 1-2% absolute accuracy improvement compared with the single-modal baseline (ProtoNet). \n\nStrengths and weaknesses are listed as follows.\nStrengths:\n1. The paper is easy to read, and the proposed idea is also easy to follow. Figure 1 can help the understanding of the proposed model.\n\n2. The proposed model does not need the manual labeled relationship between semantic knowledge and target categories, and this may further reduce the supervision for knowledge graph construction. \n\nWeaknesses:\n1. Incomplete comparison with existing works: the authors should list a more complete comparison table to compare the results of existing works. Some existing works can perform competitive or even superior results compared with the proposed model under the same feature backbone (ResNet-12).\n\n|            |                mini-ImageNet |                             |                        tiered-ImageNet    ||   \n|------------|---------------|---------------|---------------|--------------|\n|                     | 1-shot        | 5-shot        | 1-shot        | 5-shot       | \n| Proposed   |   63.29+-0.71%. |  80.12+-0.22%    | 66.69+-0.75%. | 83.04+-0.61% |  \n| CAN[1]       |  63.85+-0.48%. |   79.44+-0.34%    | 69.89+-0.51%. | 84.23+-0.37% | \n| CAN +T[1] |   67.19+-0.55%. |   80.64+-0.35%    | 73.21+-0.58%. | 84.93+-0.38% |\n\n2. Comparison with unitary modality methods is misleading, as these baseline methods do not use any external information for the training and inference. The proposed method should consider other cross-modal few-shot classification works as baseline methods. \n\n3. As the proposed model contains several modules, such as knowledge graph construction, graph neural network, and other few linear layers between components, this makes the reader hard to understand which module is crucial. It will be great if the authors could provide an ablation study to verify the effectiveness of each module in the next version of this paper. For example, the authors could provide studies like 1) what is the performance if the semantic information is removed in the knowledge graph, or 2) how the number of graph neural layers affects the overall performance.  \n\nOverall, the paper is easy to read. The idea of integrating semantic information in few-shot classification is interesting while it has been widely explored in existing works. Given the reported results of the proposed model and lack of analyses in the proposed model, I am inclined to the score \"Ok but not good enough - rejection\". \n\n[1] “Cross Attention Network for Few-shot Classification”, Hou et al., NeurIPS ‘19",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}