{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper builds upon hypergraph convolutional networks (HCN), extending them to time-varying hypergraphs in dynamical settings.  However, as some of the reviewers pointed out, it would be useful to explore other system variations to better justify the choices in this particular approach; perhaps an evaluation on a wider set of datasets would also strengthen the contribution of the paper,  as well as adding evaluation metrics that can be more appropriate for the application considered (stock market prediction). Also, concerns were raised by several reviewers regarding the somewhat incremental  improvement over the state of art, and the degree of novelty in the proposed approach. Overall, while the problem considered is important and the approach is promising, the paper in its current shape  is somewhat borderline and may require a bit of additional work to be ready for publication.\n"
    },
    "Reviews": [
        {
            "title": "Not ready enough for publication",
            "review": "Minor: Multiply typos and grammatical errors: (esp. section 5.1 onwards).\nFor example, see here: \"With the construction of dynamic hypergraph, we assign the nodes featuress with the hidden embedding of price and volume extracted by LSTM, and hyperedges featrues with the embedding represented by GloVe Pennington et al. (2014)\"\n\nClarity:  \n(1) Clear description of the problem statement and model overall. Images are well drawn\n(2) Reason for sigmoid in eqn(3) is not clear\n(3) How the initial node/hyperedge features are generated is not very clear . This is important given that the newly constructed Tiingo and Stocktwits in the paper are proposed as benchmarks for comparison with baseline models. And that there are no additional datasets apart from these 2.\n\nNovelty:\n(1) Similar problem statement addressed in https://dl.acm.org/doi/pdf/10.1145/3394486.3403389 - Needs to be added as a baseline.\n(2) The paper proposes a static hypergraph encoding technique and feed the encoding from snapshots at different timestamps into an LSTM for time -series  prediction. \n(3) The static hypergraph encoding technique in itself is not very novel. The paper uses attention mechanism for aggregation of node features into hyperedges (inner attention) and yet another attention mechanism (outer attention) for aggregating influences of hyperedges onto nodes .Finally an aggregation layer is used to update node embeddings. This setup is again similar to the paper mentioned above.\n\nExperiments: (1) The paper proposes a mechanism for encoding hypergraphs which evolve over time. However, the authors run experiments on only 2 graphs and compared with 4 baseines. \n(2) There are only two datasets both related to stock prediction. The procedure for node/hyperedge feature vector generation needs more explanation. Also, it’ll be good to run the model+baselines on additional graphs from other domains (pointer can be seen at paper linked above)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Dynamic hypergraph convolutional networks applied to financial data",
            "review": "This paper proposed a pipeline for dynamic hypergraph convolutional networks, with a two-fold component that handles both the hypergraph convolution and the temporal evolution. I think the paper is a nice contribution to the less-well studied area of time dependent networks, and to some extent, also that of hypergraph embedding. The application to finance and the data sets used are interesting, with hypergraphs/edges encoding a proxy for co-occurence behaviour of stocks.\n\nI think the experimental section is less strong, with only two data sets considered (albeit indeed the setting is less usual and less such data sets exist), and the authors only compared against a single other methodology. Regarding the performance metrics, the 3 ones currently used are less effective/appropriate when dealing with financial/stock price prediction data, where more relevant measures would be PnL and Sharpe Ratio, especially when weights are taken into account that relate to the liquidity of each of the instruments considered in the portfolio. The number of testing days is also very small, in order to be able to draw meaningful results, especially for a portfolio of 91 instruments.  \n\nThe paper is for the most part fairly straightforward to follow, though it is full of typos: a few examples being\nan financial, this work explore, an node, Spectral graph convolution transform features, outer at-tention, to name a few.\nThe paper should be proofread very carefully.\n\nSome of the notation could be better explained. I don’t see any mention of what y represents? (future price/returns)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A decent formulation for discrete-time dynamic hypergraphs with some improvement in prediction accuracy",
            "review": "This paper proposes a method called DyHCN for learning dynamic hypergraph convolutional networks where the hypergraph structure is allowed to evolve over time. The interactions within each hyper edge, that between nodes, as well as related are used to learn the hypergaph embedding. The evolution of the centroid nodes is then modelled using LSTM. DyHCN gives better modelling accuracy as compared to some existing ones.\n\nPros:\n- The formulation is clearly presented.\n- The inner and outer attention models adopted are reasonable\n- Empirical experiments have been carried out to verify its effectiveness.\n\nCons:\n- Only discrete-time dynamic hypergraph is considered.\n- The attention model and the modelling of the evolution of the centroid nodes are not particularly novel.\n- The performance improvement as compared with the SOTA method is incremental.\n- Only one particular prediction is adopted for the performance evaluation.\n\nSpecific comments\n- In Inner attention section, x_i^t is referred to as a nod but it should be th feature of node I\n\nQn:\n- Other than stock price prediction, what are the other possible prediction tasks? Can the performance comparison be carried out based on the additional predication tasks?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Proposal for a dynamic hypergraph neural network",
            "review": "The paper extends over hypergraph convolutional networks (HCN) by adding a temporal evolution module in order to solve prediction tasks in a dynamic environment. The main part of the paper is the description of the proposed system. It is composed of a HCN for computing node embeddings at each time step and a LSTM as the temporal module. Experimental results are provided for dynamic prediction tasks over stock datasets.\n\nThe proposed system seems to be a reasonable choice for solving prediction tasks on dynamic hypergraphs. But, in my opinion, some choices could have been justified and variations of the system could have been compared. The only considered system variation is the aggregation method and the best one is the only one using trainable parameters. Thus I think that there is room for improvement. Also the current version is difficult to read due to many typos, mispellings, repetitions, ... Therefore, in my opinion, the paper is not ready for publication.\n\nDetailed comments.\n\n* Too many typos, mispellings, ... to be given in this review\n\n* Introduction. It could be improved. Formal definitions are not necessary. I am not convinced that the two formulations for the evolution are useful and they are repeated in Section 3.1. The presentation of the system could be improved: the description of the \"two challenging toughs\" is not convincing enough; the description of the DyHCN system is unclear (for instance what is a node-hyperedge ?).\n\n* Section 2. §GCN. There is a huge recent literature on GNNs. Please give useful links and please explain why you choose to only present some GCNs. The definition is not easy to read.\n\n* Section 2. §HCN. Also many HNNs models have been proposed. Please justify your choice. The definition is not easy to read. Also definitions of hypergraphs should be coherent (definition in Section 2 wrt definition in section 3.1).\n\n* Section 3.1. It is already presented in the introduction. I am not convinced that the continuous-time presentation is useful because it is not used.\n\n* Section 3. Please introduce what is the meaning of a \"centroid node\"\n\n* Section 4. §GCN should be updated with recent references.\n\n* Section 5.1. I would like to know why it is useful to model the task with hypergraphs. Why is it important to know for a stock event in which news or which comment it appears?\n\n* Section 5. It is not clear to me how the authors model the task when using time series prediction. Also for RSR and DHGCN.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}