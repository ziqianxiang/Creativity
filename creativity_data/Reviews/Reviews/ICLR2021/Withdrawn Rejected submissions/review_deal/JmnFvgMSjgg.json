{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #3",
            "review": "Summary\n\nThis paper proposes a new regularization method for conditional GAN to improve the multi-modal mapping in the generator. Based on MS or DS regularization, the authors propose to incorporate additional constraints that encourage the generator to preserve the changing ratio between latent and image spaces. The proposed method is evaluated on several multi-modal image-to-image translation tasks and demonstrates comparable or better performance than vanilla cGANs and the ones with MS or DS regularization.\n\nPros\n- The paper is generally well-written and easy to follow.\n- The proposed idea is simple, easy to implement but seems to be effective in several multi-modal image-to-image translations.\n\nConcerns & suggestions\n\nAlthough I agree with the high-level idea of the paper, the paper made several vague/overly-strong claims that require further clarification/justification.\n- The paper argues that it can fix the mode-override and mode-fusion problems in MS- or DS-GAN. Despite their claims, however, most arguments are based on a conceptual level, while it is not clear how the proposed method actually improves on this challenge. Most experiment results are based on summary statistics, such as FID, LPIPS, NDB, JSD, where none of these measures corresponds to the direct measurement on mode-override or fusion. To make it clear, it would be better to employ some synthetic datasets where we can actually simulate and quantifies the modes in the data.\n-  It seems that the MS or DS regularization tends to be underestimated in the paper. The regularization in the DSGAN (Yang et al. (2019)) has the upper-bound of the regularization, which can prevent the regularization from seeking arbitrary distant mode. With the proper choice of this bound, the regularization can mildly encourage the generator to seek local modes as in the proposed regularization. \n- It is not clear if preserving the changing ratio between latent and image spaces is reasonable,  as the ‘mode’ in image space would not be quite informative. \n- The analysis in Section 2.3 does not guarantee any meaningful relationship (i.e., bound) between Eq.(2) and Eq.(3). i.e., maximizing Eq.(2) does not guarantee the maximization of Eq.(3). It is because the higher-order terms in Eq.(4) and Eq.(5) in the Appendix are simply ignored in the proof. This is contrasted to the analysis in DS-GAN (Eq.(4) in Yang et al. (2019)), which proves that the DS regularization is the “lower-bound” of the generator Jacobian norm.  \n- Numbers reported in Table 1 for both DRIT and MSGAN are somehow different from the ones reported in Mao et al. (2020). Could you elaborate why?\n- The results in Table 3 show that the impact of RVC is comparable to the one with only DR in many cases. However, the authors made very strong arguments that their regularization clearly beats the DR, which seems to be an overclaim. \n-  Please consider rewriting Eq.(2): ‘s’ in Eq.(2) is defined as it takes three arguments, but the description in the text is based on two arguments. Also, it is unclear what l_1 norm in the description below Eq.(2) is referring to.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "DivAugGAN review",
            "review": "In this paper, authors propose a set of additional losses for conditional generative networks that enforces diversity of output images and minimizes mode collapse, which is a common issue of conditional GANs. \n\nThe main idea of this paper is using a triplet of latent vectors to enforce generation diversity instead of one as suggested in the previous works. the diversity loss is based on the ratio of the distances between the latent vectors and the corresponding generator outputs.\n\nThere are a few issues in this paper:\n1. The figures 1 and 2 include the text that is too small to read. In Figure 2, d_z and s are not introduced in the caption. \n2. Page 5. section 2.2. The distance metrics d_I and d_z are mentioned multiple times but, authors never mentioned which metrics are used in practice (e.g. L1 distance, LPIPS etc).\n3. Page 2, last paragraph. Authours claim they use LPIPS metrics to measure diversity, but how exactly did they do this? To my knowledge, LPIPS is just a perceptual similarity metrics that measures how similar the two images are. Did you use LPIPS on all pairs of generated images?\n4. Figure 3, 4, 5: it is not clear that the outputs of the proposed method are more diverse than those produced by the baseline methods. In fact, the results of DivAvgGAN on figure 4 look less diverse than the results of other methods. \n5. Having in mind my thoughts on LPIPS metric for diversity estimation (2), the only metric that shows the diversity of the produced results measured in this paper seems to be the NDB metric since JSD only reflects the diversity of real data. Looking at the tables, it is not clear that the proposed method provides stable and significant improvement over the previous methods in terms of output diversity. \n\nGiven that the main contribution of this paper is claimed to be the improvement in diversity of the generated images, and my remarks above, the advantage of the proposed method remains unclear. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited novelty, missing theoretical analysis, confusing denotions",
            "review": "The paper provides a regularizer to conditional generative adversarial networks to achieve high diversity in multimodal image-to-image translation. The regularizer helps to further suppress the mode collapse problem and reduce the possibility of bringing about unexpected mode fusion or mode override issues.  Some experiments verify the effectiveness of the proposed method. \n\nMy major concerns are following.\n\n1) The novelty seems to be limitted. The idea of injecting noises and regularizing the corresponding distances is intuitive and have been explored by some previous works, such DSGAN and MSGAN. The modifications over these works are marginal.\n\n2) It is suggested to add more theoretical analysis to illustrate why the proposed regularizer is helpful to improve the diversity of image-to-image translation.  \n\n3) Some denotions are confusing, like the last three terms in Eq. (2). $s$ is described to represent the image distance betwee two generated samples. However, there are three sub-terms in every one of the last three terms in Eq. (2).\n\n4) In Table 1 and Table 2, it seems to be that the proposed regularizer only achieves consistently better results on some datasets. It would be helpful to analyze the failure cases with more details.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and effective method. But I have some concerns.",
            "review": "[Summary] \nThis paper proposes a new regularization method to enhance the diversity of images generated by GAN in multimodal image-to-image (I2I) translation. diversity augmented conditional GAN (DivAugGAN). Unlike the previous regularization apporaches for diversity such as mode seeking [Mao et al. 2019] and diversity sensitivity [Yang et al. 2019] methods, the proposed DivAugGAN consider relative variation between latent codes as well as images together. Also, the authors provide some theoretical analysis on their diversity augmentation regularization. They evaluate their method on two-domain datasets such as cat-dog and summer-winter datasets and multiple domain datasets including Alps, Image weather condition, AFHQ, and WikiArts. They compare their model with DRIT and MSGAN for two-domain and MDMM and StarGANv2 for multidomain datasets by employing their regularization. The results look competitive with ablation study. \n\n[Overall recommendation]\nOverall, this paper looks competitive but has some concerns (the details in Weakness). So my score is borderline.\n\n[Strength]\n- Diversity enhancement is crucial issue in multimodal I2I translation\n- The proposed method is simple yet effective and is described with theoretical analysis\n- The authors provide extensive experiments including ablation studies and the results look competitive in terms of quantity and quality\n\n[Weakness]\n- The authors argue that DivAugGAN address mode override and mode fusion. However, there is no reference or detailed analysis with data on mode override and fusion. As I know, mode override and fusion is less known problems compared to mode collapse, the author need to analyze mode override problem with data in more details to enhance the contribution. I think it is not enough to argue with Figure 1 only. If there is a suitable reference, the brief summarization is ok. \n- In preliminary, core related studies were missed such as StarGAN v2 [Choi et al. 2020] and DMIT [Yu et al. 2019]. Also, DMIT is one of SOTA multimodal I2I method. Because DivAugGAN is a regularization term, DMIT with DivAugGAN experiments can improve the contribution. \n- The organization can be improved. For example, Figure 3 can move to the latter to be closer to the Experiment section. \n- The authors insist that the proposed method can improve the performance \"without any computational overhead\". Considering the definition of the proposed regularization, the variation consistency calculation might require some overhead. For verifying this, could the authors compare the training time?\n- It seems that the goal of the proposed method is to enhance the diversity while keeping fidelity. For effectively verifying this, how about using two-side metrics such as precision & recall [Kynkäänniemi et al. 2019] or density & coverage [Naeem et al. 2020] as an additional metric?\n- It is not clear whether DivAugGAN(S) used diversity sensitivity loss for StarGANv2 or not. If used, How is the results of not using DS term and DivAug term only?  \n- Many papers on I2I use face datasets such as CelebA-HQ for evaluation. Is there any reason why the experiments are not included?\n\n[Minor]\n- Overall, it is not easy to read due to too compact space configuration. By discarding the references less related to the paper, the space might be available to improve the readability.\n- DR and RCV in Table 3 can be defined in Eq (2).\n- There are some typos.: in p4, ignore --> ignoring, encounter --> encountering. in Figure 1 caption, is appeared --> appears\n- The font is too small to read in Figure 2.\n\n[Refs]\n- [Mao et al. 2019] Mode seeking generative adversarial networks for diverse image synthesis. CVPR 2019.\n- [Yang et al. 2019] Diversity-sensitive conditional generative adversarial networks. ICLR 2019.\n- [Choi et al. 2020] StarGAN v2: Diverse Image Synthesis for Multiple Domains. CVPR 2020.\n- [Yu et al. 2019] Multi-mapping Image-to-Image Translation via Learning Disentanglement. NeurIPS 2019.\n- [Kynkäänniemi et al. 2019] Improved precision and recall metric for assessing generative models. NeurIPS 2019.\n- [Naeem et al. 2020] Reliable Fidelity and Diversity Metrics for Generative Models. ICML 2020.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}