{
    "Decision": "",
    "Reviews": [
        {
            "title": "More clarity is required, and marginal improvement in experiments.",
            "review": "- Summary\nBased on XLNet, this paper proposes SRXLNet, which is a syntactic-aware language generation method for Machine Translation Tasks. The authors proposed 1) syntactic relevance factorization sampling, and 2) two-stream self-attention, also 3) introduced fragment loop mechanism. The proposed method was demonstrated on machine translation tasks from low-resource agglutination languages to Chinese.\n\n- Strong\n\t* The motivation of syntactic relevant language generation is interesting. \n\n- Weakness\n\t- I strongly recommend proofreading to revise grammar errors, typos, and writing structure. \n\t- Also, it was hard to recognize the novel points compared with the XLNet.\n\t- In equation 8, how is z_t encoded and fed into the network as an input of $g_\\theta$?\n\t- In section 3.4, what is the default configuration?\n\t- From the experimental results, the improvement of the proposed method seems to be marginal compared to Transformer+XLNet. Also, it could show more efficacy, when it is demonstrated on other benchmarks and compared with previous state-of-the-art methods.\n\t- I would like to recommend the qualitative analysis of the sampling results, and attention dynamics. Also, when sampling among DFS, BFS, and Sequential order, which is effective? \n\t- I wonder about the performance of the dependency parser and its influence on the final performance.\n\t\n- Questions\n\t1. It is not clear the relation between agglutination languages as a source and generation target sentences with syntactic tree information. Are the proposed methods limited to agglutination languages? \n\t2. Please address and clarify the cons above.\n\n- Additional feedback\n\t* In section 3, the definition of z is not presented.\t\n\t* In eq.10. “$KV = h^{(m-1)}_{(z <= t)}$”\n\t* Which pretrained XLNet did you use?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "The main problem of this paper is that it is unclear what it is presented as main innovation.\nThe introduction is poorly written as there is not clear distinction among existing ideas and novel ideas.\nThe first suggestion is to clearly introduce a paragraph starting with a signaling expression like: in this paper. It is very interesting to attract attention with lists of items, but these lists should be properly introduced.\nIn any case, the main contribution of the paper seems to be the use of XLNET for solving the problem of translation in low resource language pairs. However, it is not clear what is the real innovative part of the paper as it seems to be only an incremental work.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Effective syntactic relevance search used in XLNET for low-resource MT",
            "review": "This paper proposes to apply syntactic relevance factorization sampling in XLNET and propose a syntactic relevance two-stream self-attention mechanism to learn both factorization and context features. The learned word vectors by pre-trained XLNET are then used in downstream NMT encoders for low-resource MT and yield improvements over previous models. \n\nStrength:\n\n1) The idea is intuitive and seems to work.\n2) Works effectively and shows adequate improvements on low-resource MT datasets compared to other embeddings. Although the comparisons are performed on light model structures and only uses the learned word embeddings from different pretrained models.  \n3) Thorough ablation study.\n\nWeakness:\n\n1) The writing can be better, it's hard to follow the math. \n2) The comparisons are limited to word embeddings and are inadequate. This paper only uses the learned word embeddings of the pre-trained models. Normally both the token embeddings and the attention heads of the pre-trained models are used to initialized downstream models.  Moreover, many cross-lingual pre-trained models such as XLM, XLMR, multi-lingual BART have been proven to be effective in low-resource MT.  It's better to compare with those models as well. \n3) The case study is not intuitive, it's better to give some examples and the take-away is a little superficial. It's unclear what's the take-away of the analysis of sentence length and polysemy. How does your model perform compared to XLNET?\n4) Better to use clearer figures. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well motivated,  but needs substantial revision ",
            "review": "This paper presents a model that combines dependency parse trees and an improved version of XLNet for X-Chinese translation, where X is a low-resource agglutinative language. Experiments show that the model outperforms several baselines. I particularly like the fact that the paper works on translation that involves low-resource languages. \nThere are, however, a nontrivial number of mistakes in the math. Another drawback is that the author(s) only compare their models with baselines that do not need dependency trees; that is, their model has access to more supervision which is really expensive to collect in scale. On the other hand, the author(s) did not compare to other models with the same data availability.  In addition, I believe that the writing of this paper can be improved a lot (see my  detailed comments below). \nTherefore, I don't think this paper is in a good shape for publication at ICLR. \n\n## Detailed comments and questions: \n- The math in this paper is confusing, though people in the field could probably work it out by substantial guess, for example: \n\n  i. Eq 3: $J$ and $h$ is never introduced in the paper. \n\n ii. Eq 5 and 15: how can $Loss(\\theta)$ be $\\arg\\max_\\theta \\ldots$? If so, the loss term is just the parameter $\\theta$ itself and not necessary to be a scalar -- I believe the equations should be something like $\\theta^* = \\arg\\max_\\theta \\ldots$.\n\n iii. Eq 12: $Encoder(\\cdot)$ is never introduced explicitly -- how does it work and what's the shape of $h^\\textit{mtencoder}$? \n- Figure 2: no (English) translation of the Chinese terms provided in the figure, but they are referred in English translations in the main text (Sec. 3.2, first paragraph), which makes it difficult to follow for people who don't know a lot about Chinese. \n- If I understood correctly, according to Figure 3, the dependency trees are associated with sentences in the source language; however, in Sec. 4, the author(s) claimed that they used dependency trees of Chinese (CTB). Is Chinese the source language or the target? If it's the source language, then it should be \"Chinese-X\" (rather than \"X-Chinese\") translation by convention. \n- I would suggest the author(s) to be more careful when using the term \"semantic understanding\": different people have different understandings about such a vague term. To me, mapping sentences to high-dimensional vectors, which can be hardly interpreted, is not \"semantic understanding\" at all; and a model does not necessarily to \"understand\" natural language  in order to achieve high performance on downstream tasks [1]. \n- As far as I know, word2vec is proposed by Mikolov et al. (2013) [2], while the author(s) cited Church (2017). \n- If a model is mentioned in your paper, it should be cited formally as references instead of just providing links in footnotes. \n\n## Minor comments: \n- The author(s) use the \\citet command (or something equivalent, which generates \"Author (year)\" style citations) for every citation in this paper, but many of them should be \\citep. Fixing this could improve the reading experience a lot. \n- agglutination language -> agglutinative language\n- Glove -> GloVe\n- Some citations appear in very strange positions, e.g., in page 1, both Salazar et al. (2020) and Liu et al. (2019) are later than the original BERT paper (Devlin et al., 2019), and I cannot understand why the author(s) would like to cite them when introducing BERT's training objectives. \n- (above Eq 2) What do you mean by \"similar to\"? Is it \"proportional to\"? \n- Two-stream self-attention: I would prefer not saying that it is a \"distributed\" calculation method, as \"distributed computing\" has another widely-accepted meaning, which means that components of a software are shared among multiple computers.\n\n## References\n[1] https://www.aclweb.org/anthology/2020.acl-main.463.pdf\n\n[2] https://papers.neurips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}