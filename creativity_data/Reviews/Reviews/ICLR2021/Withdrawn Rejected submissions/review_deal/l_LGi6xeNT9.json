{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers had a number of concerns:\n\nnot state of the art\nrecommend analysis and comparison with [1] Temporal Shift Module\nwriting needs to be improved\nappreciate the motivation for the paper, but needs more extensive\nexperimentation.  Need larger scene-related datasets.\n\nWe hope you find the reviewers' comments helpful as you revise the work."
    },
    "Reviews": [
        {
            "title": "The interpretability of the 3TConv",
            "review": "This paper proposes the temporally factorized 3D convolution (3TConv) as an interpretable alternative to the regular 3DConv. In a 3TConv, the 3D convolutional filter is obtained by learning a 2D filter and a set of temporal transformation parameters, resulting in a sparse filter requiring less parameters. Temporal parameterization provides a novel way of visualizing and understanding the temporal dynamics learned by a 3TConvNet.\n\n1.\tThe difficulty of training. With the increase of the temporal dimension l of the kernel, the nonlinearity in Eqs. (4)-(5) increases rapidly. Is there a stable way to train the whole network? Will the training strategy have a significant effect on the performance of the whole network?\n2.\tThe interpretability of the temporal parameters. Each slice \\Theta corresponds to an affine transformation matrix that is derived from the affine transformation parameters scale, rotation and translation. Cannot get the point to understand the kernel in Eqs. (3)-(5). How do the scale/rotation/translation work in the temporal dimension? Maybe some background knowledge can help readers to understand the core idea of the paper.\n3.\tCompared with the SOTA performance on the Jester and UCF101 datasets, the proposed method’s performance is not so good. How much does the training strategy effect the performance?\n4.\tTSM[1] implements the 3D Conv using a fantastic way and achieves excellent performance. The authors can do some analysis and comparison with such networks. \n[1] J. Lin, C. Gan, and S. Han, \"Temporal Shift Module for Efficient Video Understanding,\" in ICCV, 2019, pp. 7083-7093.\n\nInteresting research, but deeper researches are needed to make the core idea more interpretable and effective.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes a temporally factorized 3D convolution (3TConv) as an interpretable alternative to a regular 3D convolution. The authors then conduct several interpretability and action recognition experiments on Jester and UCF101 datasets. The main claims of the paper include: 1) 3TConv is more interpretable than the 3D Conv, and 2) a 3TConv based network achieves better action recognition accuracy than a 3D CNN in the low-data regimes.",
            "review": "Strengths:\n- The paper addresses an interesting problem. 3D convolutional networks are complex, they require lots of computation, and they are also difficult to interpret\n- The proposed approach is easy to understand.\n\nWeaknesses:\n- The writing of the paper could be significantly improved. At the moment, many parts of the paper feels like a documentation to the code, which is not how a conference paper should look like. The formatting, and the notation could be improved, e.g. 3TConv is a cumbersome term that is used continuously throughout the paper. Many variables are bolded and they also contain overly many subscripts. Some of the paragraph \"subheaders\" are weirdly formatted. There are also many grammar mistakes, typos, incoherent sentences. Overall, the paper looks unprofessional.\n- The action recognition experiments are not convincing. It is not surprising that 3TConv based models perform better than the 3D CNNs in the low-data regime, i.e., due to their large number of parameters, 3D CNNs require lots of data to work well. Even the 2D CNNs would most likely outperform 3D CNNs in such cases. The more interesting experiment would be to show how the performance of 3TConv and 3D CNNs differ in the large-scale regime, i.e., on the commonly used datasets such as Kinetics or Something-Something. The lack of such large-scale experiments make the current draft look incomplete. Most modern action recognition models are evaluated on these standard benchmarks.\n- The complexity analysis should include not only the number of parameters but also the FLOP counts. Most current action recognition models use FLOPS to assess the efficiency of their model.\n- I also found the interpretability experiments in the current draft unconvincing. The authors claim that their proposed model is more interpretable than 3D CNNs. However, even after reading the draft, and studying the figures, I couldn't fully understand the basis of such claims. The authors present a few qualitative visualizations, which are very difficult to interpret objectively (as admitted by the authors in the paper). Furthermore, the figures are labeled and described so poorly, that it makes it even more difficult to understand what exactly they depict. Overall, I was confused by these experiments. This is unfortunate because the entire story of the paper is tied to these experiments.\n- Instead of comparing their approach to 3D CNNs, the authors should consider R(2+1)D as their main baseline. R(2+1)D models are simpler, more effective (in many cases), and much easier to interpret than 3D CNNs.\n\nOverall:\n- The paper is poorly written, and the current experimental section is inadequate. Therefore, I recommend rejecting the paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a new operation 3TConv for video understanding, which provides interpretation for the model that 3DConv lacks. It is good to see the research that investigates the explainable and interpretable operation in deep neural networks. However, this work lacks extensive experiments and I have some technical concerns about this work. The demonstrated work here is not ready for ICLR.",
            "review": "This paper proposes 3TConv for interpreting 3D CNN when operating video tasks i.e., video action recognition task discussed in this work. The motivation of this paper is good but lacks extensive experiments to validate the hypothesis.\n\nConcerns:\n1. The authors only conduct experiments using two datasets: UCF101 and Jester, which are not sufficient. UCF101 is more scene-related dataset but is kind of small that may easily cause overfitting when training from scratch on this dataset. Jester contains sufficient samples to train deep neural nets but is more temporal-related. So a direct comparison on these two datasets is not representative. I think larger scene-related datasets e.g., Kinetics-400 or even mini-kinetics should be used in the experiment. I also suggest adding more experiments using other common large-scale termporal-related datasets e.g., Somethig to something and Egogesture to validate the performance.\n2. Section 4.3 on page 6, the authors draw the conclusion ''This is likely due to 3DConv being a more complex and ﬂexible model and better at exploiting the availability of more data'', which I think is not appropriate. In table 1 (1)  we can see that UCF101 has very low accuracy \n (caused overfitting), which means the model has not been trained appropriately. How can authors compare results with a model that not trained properly? And even with pretrained model, I suspect results on UCF101 is still very low; (2) Regarding the Jester dataset, I do not think it would have such a big difference with training from scratch and pretrained as jester contains sufficient samples that allow you training from scratch (e.g., R(2+1)D performs well). The authors have not shown pretrained R(2+1)D and pretrained 3DConv that are all necessary in order to validate the performance.\n3. On page 7,  ''This discrepancy is explained by the fact that models with more parameters can learn from bigger datasets better than smaller or more restricted models.'' There is no actual experiment  to validate this conclusion. If authors would like to validate this result, I think extra experiments with deeper model using 3DConv and 3TConv should be added e.g., using ResNet-34 backbone.\n4. On page 7, ''We can only compare the model trained from scratch due to time limitations involving the download of the Kinetics dataset for a full comparison and we reserve this for future work.'' I am afraid this reason is not acceptable. If the authors have not prepared for this, this work is not ready for ICLR.\n5. Figure 3, as I mentioned before, I do not think training from scratch on Jester will be different from pretrained on Jester. I suggest authors to check if the experimental setting for training from scratch. I suspect the model is not fully converged.\n\nQuestions:\n1. Figure 1 (1) what is the rule of picking a channel? why not do an average across all channels? Is this a cherry-pick? (2) I think it is doable for using CAM for visualization which might make more sense (3) Can authors explain that if compare saliency map on the first row, I think 3DCOnv visualization makes more sense than 3TConv.\n\nSummary:\nThe motivation of this work is good (try to give an interpretation for 3D CNN). However current lacks enough experiments to validate the performance and I also suspect the experimental setting as the issue I mention when using the jester dataset.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Temporally Factorized 3D Convolution (3TConv)",
            "review": "In this work, the authors propose an interpretable 3D convolution, namely 3TConv, by using temporal factorization. Specifically, they use scale, rotation and translation to generate affine transformation matrix that is multiplied with the filter in the previous step to obtain the one in the current step.\n\nThis paper considers an interesting and important topic in video classification. However, the paper is not well written, especially in the method and experiment sections.\n\n1 The motivation to introduce 3TConv is not well claimed. Why it should be designed in the formulation of Eq(3)? Moreover, I think that the way to generate Theta should be further explained in the main text, instead of in the Appendix.\n\n2 Section 3.2 is not quite clear for me.  The parameters are not well defined. As a result, the explanations in this section are hard to understand. Furthermore, how can 3Tconv be used to explain temporal dynamics? I do not get it.\n\n3 Fig.1: After checking plots and explanations, I am not quite sure about what is the conclusion. For me, those visualization plots are not straightforward to tell the story.\n\n4 The results in Table 1 are not convincing. In fact, the accuracy is far from SOTA. Also, please show GFLOPs to validate computation efficiency.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}