{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper has been discussed by the reviewers that have acknowledged the rebuttal and the authors’ responses. However, the reviewers still had the following weaknesses and concerns (not solved post rebuttal):\n\n* Expensive procedure (e.g., exhaustive enumeration before finding Pareto frontier)\n* The experiments should be more rigorous, with more realistic real-world problems.\n* Missing comparison with baselines (unanimously acknowledged by the reviewers).\n* No explanations and insights provided as to why the method should work well\n* Clarity of the presentation\n\nAs a result, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission.\n"
    },
    "Reviews": [
        {
            "title": "Relevant topic, paper appears a bit preliminary",
            "review": "Summary\nThe paper addresses the proposes a solution for multi-task multicriteria hyperparameter optimization. The solution hinges around Pareto optimality. The technique appears to do an exhaustive enumeration in the space of hyperparameters (Equation 4). Experimental results are provided for two scenarios with two and three hyperparameters. \n\nPros\nThe paper addresses an important problem.\n\nCons\n1.\tThe paper appears to use exhaustive enumeration before finding Pateto frontier. Exhaustive enumeration is computationally expensive.\n2.\tThe experimental results are sketchy. The final performance of the models as a result of hyper-parameter selection is not provided.\n3.\tComparison with baselines is missing.\n\nClarifications needed\nThe paper can be improved by adding the following information.\n\n1.\tSection 4.1: Provide details of the tasks.\n2.\tSection 4.1: “Neural networks trained on ten TPUs v2, which took several days.”: How many days?\n3.\tSection 4.2: “sample mean/variance of the epoch number at which convergence is achieved in the test sample.”: What is meant by convergence on test samples?\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Half-baked, needs more work",
            "review": "This paper proposes MTMC, a method that solves the pareto optimization problem for multiple tasks and multiple criteria.\n\nPro:\n1, concise.\n2, solution seems interesting from a technical perspective.\n\nCon:\n1, No explanation on why this proposed method should work well.\n2, No comparison to state-of-the-art method (or any baseline methods). Several related methods were discussed in related work, but why not compare to them, e.g. (Igel, 2005)?\n3, Experiments are not convincing. Please include descriptions of this problem in (Akhmetzyanov & Yuzhakov, 2019) and add more realistic real-world problems. Please include specific description of the metric to evaluate different methods including the proposed one.\n\nSome minor points:\n1, It's unclear what N_combination is exactly and why it's different from N_parameter.\n2, Typo: in Eq (4), x_sise -> x_size\n\nOverall I think this paper is half developed and can benefit from some intuitive explanations, theoretical analysis of the proposed method and more convincing experiment.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "clear reject",
            "review": "\nThis paper proposes a multi-task multicriteria hyperparameter optimization method. Experiments fail to demonstrate the performance of the described method. \n\nStrong points:\n\n- The multi-objective selection procedure appears sensible\n- Experiments run on state of the art models\n\nWeak points: \n\n- Experiments don't say what dataset they used, don't make me look at the citation when you have about 4 pages left of space\n- Real values for objectives never provided\n- No baseline / comparison \n- No justification for method choice\n\nJustification: \n\nThe paper makes little sense to me. A lot of things are poorly explained, although somewhere in there is the core of a good idea. The whole paper needs to be rewritten with an emphasis on properly defining terms, objectives and research questions.\n\nI strongly recommend that this paper be rejected. \n\nSome suggestions: \n - I am not sure if hyperparameter optimization is the right term to describe this method. One cannot speak of hyperparameter optimization, as there is no hyperparameter optimization process taking place. In fact, this is more akin to a selection procedure.\n - Equation 9 is introduced as the vector of the optimal solution, whereas it is one of many optimal solutions. It is incorrect to say that the optimal solution is putting an equal weight on all objectives, as far as multi-objective optimization goes. \n - Add some figures explaining the whole procedure (from hyperparameter evaluation to multi-objective selection)\n - Evaluate against baselines\n - Provide some justification for the steps of the method (i.e. in 3.2)\n - Have someone not familiar with the work revise the paper before submitting.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Multi-Task Multicriteria Hyperparameter Optimization",
            "review": "This article bases hyperparameter optimization over multi-objective by aggregating them with weights. An illustration is provided on a grid search to select Pareto optimal solutions.\n\nAggregation of objectives to get scalar values has been studied at length in multi-objective optimization, so it is hardly novel. See, e.g.,  Miettinen, K., Nonlinear multiobjective optimization,  Springer, 1999, 12.\nBesides , choosing weights is known to be quite difficult in practice.\n\nConcerning optimization of hyperparameters, there are many works applicable that would be much more efficient than a crude grid search. In the case of Bayesian optimization, there are works like, e.g.,:\n-Swersky, K.; Snoek, J. & Adams, R. P., Multi-task bayesian optimization, Advances in neural information processing systems, 2013, 2004-2012\n- Paria, B., Kandasamy, K., & Póczos, B. (2020, August). A flexible framework for multi-objective Bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence (pp. 766-776). PMLR.\n- Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).\n\nSince no comparison is provided with methods from the literature, that the proposed method is not novel, I thus recommend rejection.\n\nTypos:\nEq. 4: sise → size\nP2: the nearest Pareto front to the origin?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}