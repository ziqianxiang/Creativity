{
    "Decision": "",
    "Reviews": [
        {
            "title": "Gap between motivation and application; not placed well in existing body of literature; further comparison with prior work is needed; frequent typos and inconsistencies",
            "review": "The manuscript presents novel joint descent (JD) algorithms for simultaneously training a machine learning model and tuning its hyperparameters by randomly alternating between first order gradient methods for training and zero-order methods for tuning. Their accelerated joint descent algorithm for smooth, strongly convex objective functions $f(x,y)$ ($x$ are model weights, $y$ are hyperparameters) achieves an iteration complexity of $O(n_y \\sqrt{\\kappa} poly\\log(n/\\epsilon)).$ This is a potential improvement over the $O(n_y \\sqrt{\\kappa_x \\kappa_y} \\log(n_x /\\epsilon) \\log(n_y /\\epsilon))$ complexity of the classical approach of first training the model (approximately minimizing $x$) with Nesterov's AGD and then taking a step of zero order (gradient estimation) method in $y$. Here $\\kappa_x, \\kappa_y$ denote condition numbers of $f(\\cdot, y)$ (for all $y$) and $f(x, \\cdot)$ (for all $x$), respectively, while $\\kappa$ is the condition number of $\\kappa.$ The idea behind the algorithm is interesting, but the algorithm is not well-placed in the literature. This makes identifying the contributions of the paper very difficult. Below are some of the concerns of the reviewer:\n\n\n— There is a significant gap between the motivation and the problem formulation of the paper. The paper is motivated by the idea of joint tuning and training. However, it ends up talking about coordinate descent type algorithms. When talking about tuning and training, one of the major considerations is overfitting. This is the reason that in many practical scenarios “over-tuning”  is not recommended. In addition, tuning is typically done on a different dataset than the training procedure, which leads to different objective functions for each task. Hence what happens in practice is more similar to a bi-level optimization where at the first level, out-of-sample loss is optimized in tuning and in the second level training is optimized. This is mainly due to concerns about overfitting. If both of them are done jointly considering the same objective function, then I would consider it the training phase (What is the definition of training in your paper? What is the definition of tuning?). This gap between the motivation and the formulation part is one of the major concerns of the reviewer. \n\n— The major idea of the algorithm is randomized coordinate descent. This type of algorithm and its variations have been studied heavily in the last decade starting by the pioneering work of Nesterov “Efficiency of coordinate descent methods on huge-scale optimization problems”. However, the paper does not compare with the literature of randomized (gradient) coordinate descent. In addition, the idea of approximately solving one block and solving w.r.t the other block has also been studied in the literature: \n see “Razaviyayn, Meisam, Maziar Sanjabi, and Zhi-Quan Luo. \"A stochastic successive minimization method for nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks. Mathematical Programming 157.2 (2016): 515-545.” and  “Mairal, J. (2015). Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM Journal on Optimization, 25(2), 829-855.” Notice that when the algorithm updates the variable y, it plays the role of estimating the objective function in the aforementioned works. Then when update x, the variable is updated. The algorithm and contributions need to better be placed in the context of existing body of literature. \n\n\n\n— Explain more about hyperparameters and the importance of tuning them in machine learning in the Introduction in order to better motivate the problem at hand.\n\n— p.1 second paragraph: should read ``$\\beta$-smooth, $\\mu$-strongly convex\"instead of ``$L$-Lipschitz, convex\". $\\beta$ instead of $L$ is a matter of notational consistency. But no such rate is possible for gradient descent without strong convexity and Lipschitz \\textit{gradient} (i.e.$\\beta$-smoothness).\n\n—  p.2 first and second paragraphs (and elsewhere): use $\\beta$ instead of $L$ for consistency \n  \n—  p.2 Devoting a full paragraph to discussion of Bayesian optimization seems a bit unnecessary and tangential, since this technique is not used at all except as a baseline in the experimental section\n\n—  p.3 ``unrealistic\" would probably be a more appropriate word than ``impractical\" in this context. Also, use ``a\" instead of \"an\" and ``notably\" insetad of ``notable\" in that same sentence. \n\n—  p.3 The last paragraph in Section 1.1: I believe the rate achieved by the classical approach should in fact be $O(n_y \\sqrt{\\kappa_x \\kappa_y} \\log(n_x /\\epsilon) \\log(n_y /\\epsilon)),$ not $O(n_y \\kappa \\log^2(n/ \\epsilon))$. In the last sentence of the paragraph, to be correct and consistent with what's been said before in the paper, ``runtime\" should be replaced with ``iteration complexity\" and $\\log(1/\\epsilon)$ should be replaced with $\\log(n/\\epsilon).$ Also, the assertion that the resulting iteration complexity is worse when $\\kappa$ is small seems false. Since $\\kappa \\geq 1$ always, simply using zero order methods would in fact be worse than the classical approach when $n_x$ is large.\n    \n—  General critique related to the above bullet: be more careful about distinguishing between $\\kappa, \\kappa_x, \\kappa_y$. First of all, you need to define $\\kappa_x$ and $\\kappa_y.$ Since your main selling point is saving a square-root factor of some condition number in the complexity, it is crucial to understand exactly what that factor is: is it a factor of $\\sqrt{\\kappa_x}, \\sqrt{\\kappa},$ ...? \n    \n—  p.3 Sec 1.2 first paragraph: For theory, you must assume that $f$ is strongly convex and $\\beta$-smooth. The condition number is not even defined for a function that is merely convex. \n    \n—  p.3 Sec 1.2 second paragraph: the iteration complexity you claim to achieve is not exactly right--should be poly log, not log. Thus, you are only saving a factor of $\\sqrt{\\kappa}$ at most; in fact you might be losing if $n$ is exponentially big compared to $\\kappa$ due to the log terms.\n    \n—  p.4 Sec 2.1: first inequalities are incorrect: $\\|\\nabla f(x)\\|^2$ should be replaced by $\\|(1/\\beta) \\nabla f(x)\\|^2$ and the RHS of the last inequality should be $f(x) - (1/2 \\beta)\\|\\nabla f(x) \\|^2.$\n    \n—  p.4 Sec 2.1 par. 2: tell reader which class of zero order methods (ZOM) you're considering--is it gradient estmation, local randomlized descent, or Bayesian optimization? In the same paragraph, ``Many algorithms...\" requires a citation. The estimate of the expected value of the absolute value of the inner product $\\langle \\nabla f(x), u \\rangle$ requires explanation and/or citation. \n    \n—  p.4 Sec 2.1 par. 3: The wording ``stepsize must be chosen to be $h$...\" should be clarified: why must it be? Is this the only choice that ensures convergence? Why? The sentence ``... we can still accelerate to achieve an iteration complexity of...\" needs citation and ideally more explanation/detail. Also, the last word in that sentence (``iterations\") is redundant. \n    \n—  p.5 Section 3.1: The entire discussion preceding Theorem 3 seems to be more relevant to Section 3.2. I don't see you using the techniques discussed there in Algorithm 1. Algorithm 1 uses the mehtods from Golovin et al (2019), which should instead be discussed in Section 3.1. \n    \n—  p.5 Section 3.1: clarify which kind of ZOM step: gradient estimatino? \n    \n—  p.5 Section 3.1: in the equation defining $g_\\eta^p(z),$ you need to define $u, \\eta$. In what follows, you need to define $z^{+}.$\n    \n—  Thm 3: State all assumptions clearly. For example, are we assuming $f$ is $\\beta$-smooth as function of $z = (x,y)$ or just $x$? Strong convexity? \n    \n—  Explain why is Algorithm 1 useful if it yields the same iteration complexity (Theorem 3) as the classical approach? \n    \n—  p.6 Section 3.2: first paragraph-- explain why the two issues mentioned are issues, e.g. why can we only approximate $\\nabla_y f(x,y)$?\n    \n—  Section 4: Why not compare against classical approach with gradient descent followed by the gradient estimation approach or the approach of Golovin et al. (2019) say? This would allow us to see the benefit (if any) of your joint/alternating descent approach against the classical approach, since essentially your methods and those methods would then be the same except for how many steps of gradient descent you perform before taking a step of ZOM. Also, can you compare against other joint training and tuning algorithms? \n    \n—  Appendix A.1: $\\beta$ should be replaced by $\\beta_x$ and $\\beta_y$ (Lipschitz constants of $\\nabla_x f$ and $\\nabla_y f$, respectively) for more clarify and precision (and in particular, to ensure that bounds have correct ``units\"). The statement of Theorem 3 (and other instances of this issue throughout the paper, including Theorem 4 and proof I think) should be modified accordingly. \n    \n—   Appendix A.1: the $\\mathbb E[f(x, y^+)]$ bound seems to hold with probability $1/2$ over the distribution of $u$ (not almost surely). Then how does the bound on $\\mathbb E[f(z^+)]$ follow (apparently with probability 1 over $u$)? \n    \n—   Appendix A.1: What is ``AM-GM\"? \n    \n—  Appendix A.2: It would be nice if you restate Lemma 5 of Nesterov & Spokoiny (and in general restate results that you use in your proofs) so the reader doesn't need to link to other papers in order to understand your paper. \n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Comments on Joint Descent: Training and Tuning Simultaneously",
            "review": "In this paper, the authors proposed a joint descent method for training and tuning simultaneously, where the problem is to minimize a bivariate function with two optimization variables x and y. In this work, the gradient w.r.t x is assumed to be known while the gradient w.r.t. y is estimated through value function evaluation. \n\nThe convergence rates of a gradient-based or black box algorithm to optimal points are both known. Here, the new contributions are the joint descent algorithms and corresponding convergence results for a mixed case (gradient w.r.t. x is known while w.r.t y is estimated). The significance of this work seems not high.\n\n1. Alg. 1 involves a non-trivial inner loop. The complexity of solving the subproblem should be also included in Th3.\n\n2. The unbiasedness of gradient estimate g_eta(z) by choosing g_eta=2g_eta^0.5(z) would be the only new contribution in this work. It should be listed as a lemma.\n\n3. What is the relation between r and \\eta?\n\n4. The theorems only cover the convex case, while in practice the loss function would be nonconvex as shown in sec. 4.2. The current theoretical result is still limited.\n\n5. Both theorems say that the dependence of log(n/\\epsilon) is polynomial. Is it possible to have an approximate order of the worst case?\n\n6. Regarding the final convergence rate, is it surprising? Maybe the authors can list the relevant state-of-the-art convergence rate in this setting for a clearer comparison. \n\nMinor comments:\nz should be introduced before Th1.\nThere are many grammar mistakes in the paper.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper presents an alternating optimization method to solve the problem of jointly minimizing a bivariate function. The paper develops the theoretical foundations with an analysis to balance the first and zeroth order steps. \n\nThe derivation and the flow of the paper are clear and sound to me. In fact, the analysis is mostly based on the combination of the existing first-order and zeroth-order results. Therefore novelty is rather minor, which is my major concern. Perhaps the authors could find an explicit example where such an algorithm (first-order update on x and zeroth-order update on y) is indeed useful (like, when the first-order alternating methods are not applicable). The experiments provided does not suggest so. \n\nTheorem 3 shows the bounds when the probability p is chosen between 1/(n_y+1) and 0.5. It would be great to add a discussion to suggest the reason for that, and the performance when p is chosen out of the range. It is also of great interest how the probability p plays in the role of the final convergence rate. The experiments show a fairly large discrepancy for different p, which should deserve a detailed discussion. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Clean and promising algorithmic idea; could use more experimental follow-up",
            "review": "Summary:\nThis paper considers jointly optimizing over parameters x (over which gradients are available) and hyperparameters y (over which only zero-order queries are available). The authors propose Joint Descent, which flips a coin at each step to take a gradient step on x, or a search step on y with a logarithmic grid of radii times spherical Gaussians. An accelerated variant is derived, which gets the Nesterov convergence rate times the dimension of y. Experiments on synthetic jointly convex functions and toy MNIST hyperparameter setups are shown, where JD performs favorably.\n\nPros:\n- The work is well-motivated; indeed, it is interesting that ML optimization workflows consist of alternating between training a model and adjusting the hyperparameters, and it would be interesting to study joint optimization of differentiable and non-differentiable hyperparameters.\n- The formulation, algorithm, analysis, and writing are exceptionally clean.\n\nCons:\n- This is not such a realistic formulation of the difficulties of hyperparameter optimization; the theory and practice are too decoupled, even in the convex experiments in this paper (the worst-case convergence rates are not very helpful to decide on the parameter p). The claim that the convexity and well-conditionedness of f \"seems unnecessary for fast convergence in practice\" may be partially supported in these cases, but are not reflective of contemporary hyperparameter optimization. For example, even when the problem is a convex quadratic in x (with stochastic gradients), the time-varying learning rate is a subtle matter [\"The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares\", Ge et al. '19]. See detailed comments below for some suggestions.\n- Though the theoretical part is clean and (to my knowledge) not published before, it does not carry enough novelty to stand independently of the experiments. Thus, in my opinion, a deeper experimental study is needed for this nice algorithm before publication.\n\nDetailed comments:\n- It would be interesting to demonstrate some class of realistic hyperparameter optimization problems that benefit from (accelerated) joint descent. Larger-scale experiments would have plenty of search spaces to try; I do believe that joint descent is an excellent idea for a heuristic, and should find non-trivial hyperparameter settings in some of these spaces.\n- The discussion on cheap function evaluations vs. full Hessian-vector products in meta-learning would also be a good direction for experimental follow-up.\n\nSmall typos:\n- \"apriori\" -> \"a priori\"\n- \"a few notations\" -> \"some notation\"\n- \"Theorem 1\" statement should have \\mathrm{poly}",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}