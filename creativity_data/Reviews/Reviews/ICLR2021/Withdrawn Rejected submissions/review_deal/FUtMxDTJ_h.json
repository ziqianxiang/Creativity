{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes to learn symmetries of a physical system jointly with its Hamiltonian from data by learning a canonical transformation that render some of the coordinates constant.\nThe Hamiltonian dynamics and \"canonical\" transformation are softly enforced via loss terms.\nA few experiments are performed demonstrating that the idea works and can learn a few approximate invariants, as well as some improvements over baselines agnostic to the symmetries.\nThe idea is interesting, but the experiments are limited in scope. It is not clear how to extend this idea to more complex systems where we do not know the number of conserved quantities in advance. It is also not clear how good are the learned invariants, as the results showing errors in conserved quantities (Fig 3) suggest that it is not very precise beyond a few time steps.\n"
    },
    "Reviews": [
        {
            "title": "an application of NN to learning symmetries in physics",
            "review": "This paper presents the results of a NN trained to learn symmetries in physics, specifically, to learn and preserve quantities that are preserved (e.g., energy, angular momentum). The input is a sequence generated from a Hamiltonian dynamics. Results of experiments on 2 and 3 body problems and a harmonic oscillator are presented. The training networks are small, shallow feedforward networks. There is some customization of the training networks to incorporate \"cyclic\" coordinates. Results indicated empirical conservation up to small error of physically conserved quantities. The paper is fairly easy to read, with much relevant background provided.\n\nIn the early days of NNs, this might have been a very interesting paper. With today's advances, and NN finding success in almost every area with data, it is not clear what the contribution of this paper is. Perhaps the main innovation is the design of the networks. Unfortunately, there is little explanation provided of the experimental results.\n\n-- Why is this result interesting? Given that the output is a simple function of the input, why is this result surprising in the least?\n\n-- Given that the model for data is explicit and the training model is simple, can you say anthing rigorous to explain the results obtained empirically?\n\n-- the outputs are close but do not perfectly periodic. What parameter/model changes could possibly explain this? Would a different representation do better?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Symmetry Control Neural Networks for ICLR 2021",
            "review": "**Summary.** The authors propose using a neural network to learn a canonical transformation of the data coordinates before learning a Hamiltonian. This is a novel contribution in that previous work has shown how to learn Hamiltonians with neural networks but it has not shown how to learn the proper canonical transformation. In the course of learning this canonical transformation, they show how to project out other symmetries (linear and angular momentum) and hence improve upon HNNs while also learning these other symmetries to a good approximation.\n\n**Strong points.** The authors have a strong grasp of Hamiltonian mechanics and they lay out the theory in an accurate and easy-to-follow manner. The core idea is a good one and it leads to promising (although entirely qualitative) results. The experimental setup is reasonable and the behavior of the trained models is visualized effectively. The symbolic regression applied to the learned models shows that they learned physically relevant quantities, and this is an excellent empirical validation of the authors’ approach, eg. Equation 16.\n\n**Weak points.** Some sections of the paper were very hard to follow. For example, the experimental setup which was introduced in the form of “Model 1, Model 2, Model 3…” was very confusing. First of all, the authors never gave intuition for what each of these models was specifically aiming to test. Here are my best guesses, although I would like to see the authors’ definitions as well:\n* Model 1: This appears to be the generic Symmetry Control NN. They enforce a Poisson loss in latent space and then train an HNN on it. Since they set \\beta to zero, H ends up being invariant to the transformed coordinates (P, Q) only implicitly, by way of the Poisson loss and the HNN loss. Since the fourth loss term is regularizing P_i to be constant, it appears that they are regularizing *all* P_i in the model to be constant. I don’t quite understand this. This presumes that all momenta are stationary, which is not the case for the 2-body system -- we only expect two of them to be constant. Is the idea that this regularization will hopefully force two to be constant while the other two are not?\n* Model 2: This enforces only half of the transformed coordinates to be cyclic. But the authors make another change: they force the learned canonical transformation to be linear by construction. So to summarize, they are changing two things: 1) now we're using a linear transformation for (p,q)->(P,Q) and 2) now we're using a different number of cyclical coordinates. A proper ablation of the experiment should just change one of these, and the authors should describe what they are trying to ablate.\n* Model 3: This resembles Model 1 more closely, but here we are explicitly defining the first two symmetries (x and y momentum conservation) but not the third (angular momentum)\n* Model 4: This is complementary to Model 3 in that they are fixing the angular momentum and learning the linear momenta\n* Model 5: They fix all the known symmetries (both angular and linear momenta). This corresponds to adding maximal domain knowledge to the model.\nThe remainder of the experimental results likewise need more explanations. There should be a table of quantitative results. There should be more discussion regarding the system of coupled oscillators. There should be quantitative measurements of error-of-fit for the symbolic regression models\n\tThe 3-body results are quite good and it was interesting to see that these models excelled in this context compared to baseline NNs and HNNs. The authors provide a nice, intuitive discussion of these results.\n\n**Recommendation.** 5 : Marginally below acceptance threshold\n\n**Reasoning.** This paper tackles a significant problem, presents a novel and useful method, and achieves promising empirical results. Its weaknesses were 1) that experimental methods and results were not sufficiently well explained and 2) there were not enough quantitative results. The paper cannot be accepted to ICLR as-is. I would consider changing my recommendation if these two core issues were addressed in a substantial way.\n\n**To improve the paper.** The authors should make their explanation of methods substantially clearer, with special attention paid to explaining why they design the experiments and models the way they did. The authors should provide qualitative results for all three tasks.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "an exploration of learning symmetries in the HNN paradigm ",
            "review": "This submission explores the question of identifying conserved quantities in Hamiltonian dynamics for physical systems by attempting to learn canonical transformations. The approach closely resembles previous work on \"Hamiltonian neural networks\" but the loss is augmented with a term enforcing the invariance of the dynamics under the transformation and with a term that ensures the resulting transformed coordinates satisfy the constraints of the algebraic relations that emerge from the Poisson bracket. Together these two additions allow the authors to train a network that performs a change of coordinates which is subsequently optimized to bring it closer to a canonical transformation. Perhaps the main observation is that some of the cyclic coordinates identified by the network have a clear relation to the underlying conserved quantities. \n\nOn the whole I was not particularly impressed by the numerical results---despite imposing fixed numbers of conserved quantities, the performance is not meaningfully superior to the established HNN framework. I did not understand why the authors would use an integrator that is not symplectic---this seems like a clear route to bettering the performance over longer timescales. What is more, the weight of the additional terms in the loss was not systematically varied, so was not left with much sense of the extent to which it was affecting the optimization. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting research on finding conservative quantities to improve prediction, however not yet convincing",
            "review": "The paper proposes a set of loss functions that can make neural networks learn Hamiltonian dynamics with conserved quantities.\nThe research builds on Hamiltonian networks and adds the search for conserved quantities as an auxiliary task.  The loss functions are a straight forward translation of equations describing Hamiltonian dynamics with conserved quantities.\n\nFor the machine learning audience, the paper could be written more didactically, such that it can be easier followed. In some parts, information is missing or misleading and the graphics need some improvements. Otherwise, the presentation is adequate.\n\nThe proposed method is novel, as far as I can tell. Although fascinating, its impact will probably not be very high, as real-world systems, for instance, robot dynamics are not a closed system and thus energy conservation is not given. Nevertheless, I would love to see more progress in this direction.\n\nMy main concern with the paper, as it is mostly empirical, does not provide enough evidence that the method would work in non-toy settings.\n\nConcrete problems:\n- the number of conserved coordinates has to be known\n    - here I think it would be possible to show what happens when the number of conserved quantities is scanned through. I guess a breakdown in performance would be observed when the number is too high. I think something like this would strengthen the paper\n    \n- the different models are confusing: If I understand correctly, then Model 1 is your method in its general form and is the only real model to be compared to. It only has the knowledge about how many conserved quantities are the system. The other models are some kind of ablation, an additional way to check which parts perform how well. I appreciate those models, but they should not be called model 1...5 but rather Sym-Net and Sym-Net + Oracle X Y Z, because they get quite a bit of extra information.\n\n- already for 3 bodies the new network does not perform much better (sometimes worse) than the baseline on its own terrain -- the conserved quantities:\n    - after 8 steps HNN is better or equally good on all conserved quantities (Fig 3). Still, the prediction is a bit better than the HNN, which puzzles me a bit. What is the reason for the better performance? Also, the qualitative predictions in Fig 3 are not really convincing.    \n- using data from a non-synthetic dataset that you are not generating yourself would strengthen the paper\n\nAs you are also after finding concise analytical expressions, your approach could be potentially nicely combined with the \"equation learning\" architecture [1]. In any case, it might also be a good baseline. It would be valuable to know whether the inductive bias of a functional form or the conserved quantities is more effective.\n\nDetails:\n - below Eq 3: unpack and explain a bit why the {conserved quantity,H} vanishes. (I think this will increase readability)\n - Eq 4: maybe Q and P could be introduced before they appear in EQ 4.\n - page 3: 2. state what you need from the Poisson algebra here: {p_i,p_j} = 0,  {q_i,q_j} = 0, {p_i,q_j} = \\delta_ij\n- p4 check last sentence\n- Fig 2 and 3 Font size is an order of magnitude to small. A proper legend would also help.\n\n[1] http://proceedings.mlr.press/v80/sahoo18a.html\n\n--- Post rebuttal update\n\nThe authors clarified and removed problems in the paper and did additional experiments. The changes to the paper are quite substantial and I cannot make a full review again.  (Upvote from 4->5)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}