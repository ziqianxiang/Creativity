{
    "Decision": "",
    "Reviews": [
        {
            "title": "Important Questions, Insufficient Answers",
            "review": "Full disclosure: I reviewed a previous version of this manuscript for another conference. While some of my previous criticisms have been addressed in this update, significant ones have not. I will try to restate these, but they will be familiar to the authors.\n\nSummary:\n--------\nThe main goal of this paper is to address the following questions (paraphrased) from the introduction:\n- Has AP started to saturate or is further progress possible on object detection with current methods?\n- What is holding us back from making progress on this problem?\n\nTo this end the authors propose an \"empirical upper bound\" for object detection (Section 4). To measure this upper bound, it is assumed that localisation is solved and that classification is the bottleneck. A ResNet152 network is trained and evaluated (as an image classifier) on the pre-localised and cropped bounding boxes and the resulting AP is taken to be the \"empirical upper bound\" on detector performance (or UAP). The paper then considers whether the UAP can be increased e.g. by adding context (enlarging the boxes), or by adding boxes to the evaluation set that don't perfectly overlap with the ground truth. \n\nAdditional experiments return to the detection setting. The part of the image surrounding objects of interest is modified to further examine the effect of context on detection (Section 5), and an error analysis a la Hoiem et al. 2012 is conducted to characterise the errors of recent detectors (Section 6).\n\nDiscussion:\n-----------\nThe questions this paper aims to answer are certainly very important ones and need addressing. My main issue with the paper however remains the following: I am unconvinced by the proposed upper bound. \n\nI do not see how this is meaningfully different from image classification results e.g. on ImageNet. We know that neural networks perform fairly well on the task, e.g. the ResNet-152 considered for experiments here. The one difference is perhaps that here, classification performance is measured on different datasets. The numbers here are better than top-1 error on ImageNet (with the exception of OpenImages), but this isn't really surprising as e.g. MSCOCO contains fewer categories (80 vs 1000) with larger inter-category diversity compared to ImageNet, where the differences between many categories are more fine-grained. This proposed upper bound will also constantly be changing as people develop better DNNs for image classification, and there's no reason to believe that ImageNet performance won't simply correlate with classification performance on the datasets considered here. In fact, Figure 6 shows that classification accuracy strongly correlates with the proposed UAP.\n\nThe authors explicitly address this criticism in this version of the submission (Sec. 4, paragraph 1):\n\n\"Beware that we do not mean to undermine the importance of the localization component. What we intend to convey is that assuming no further progress in object recognition and investing all of our efforts in solving the localization problem can lead us to this upper bound and not beyond that.\"\n\nThis is exactly the problem though: the assumption that one can meaningfully decouple localisation from classification in a detection context. The challenge with detection is ensuring that the response of the network is well-localised and peaks on the center of the object of interest. This is difficult due to the way object detectors are trained. Training a classification network on pre-cropped objects obscures this challenge and doesn't really tell us anything we don't know from studies on image classification. \n\nTake for example Fig. 2a in (Jiang et al., ECCV '18) Acquisition of Localization Confidence for Accurate Object Detection. Due to the imbalance between positives and negatives, object detectors are typically trained with loosely defined positives (e.g. in this case, IoU > 0.5 with a GT bbox). As a result of this training scheme, as this diagram shows, there is not much correlation between classification confidence and bbox precision, which naturally has an effect on localisation ability/detection quality. \n\nA more interesting \"upper bound\" would consider the trade-off between learning features for recognition and the effect on localisation. The experiments in 4.2 start to go in this direction by sampling boxes with different IoU values with the ground truth, but these experiments still only consider the classification setting. As a result of the focus on classification, this oversampling of boxes is described as a \"new data augmentation technique\" (p5, end of section 4.2). From a detection standpoint though, it isn't really a new technique; this is very similar to how any anchor-based bounding box detector is trained: A mixture of boxes with varying overlaps with the ground truth are considered as positives.\n\nAdditional remarks:\n- \"Object recognition is believed to be solved in computer vision witnessed by the below human-level error rate of the state of the art models (∼3% top-5 error on ImageNet vs. ∼5% human error rate\"\n\nI doubt that this is a widely-held belief given the wealth of recent research on adversarial examples and out-of-distribution detection which demonstrate that a lot remains to be done on the task of pure recognition/classification.\n\n- The error diagnosis in section 6 is nice, but this is a straightforward application of an existing error analysis tool to recent detectors. There is not much discussion and the results aren't related to the rest of the paper or placed in context. Are there new conclusions here with respect to prior work? \n\n- There are a few references in the bibliography that need updating with the proper venue (e.g. Hall et al., Hendrycks & Dietterich, Recht et al.).\n\nConclusion:\n-----------\n\nI fully agree that the stated goals of the authors are important and worthy of pursuing, I just don't believe that this analysis is appropriate for the detection setting. What do we learn about a DNN's or in this case a ResNet-152's ability to recognise objects that we do not know from the image classification literature? There are some detection-related experiments/analyses towards the end, but the proposed empirical upper bound (the paper's namesake) is inadequate.\n\nThere is a trade-off between recognition or even more broadly detection performance and localisation given the way object detectors are currently trained. An analysis that aims to uncover the limitations of current methods or determine how much room for growth there is cannot really sidestep this issue and revert to a pure classification setting.\n\nI would encourage the authors to continue to pursue this line of work but to more closely examine this trade-off, e.g. by trying to find out how well-localised a CNN's response is and how correlated it is with various things such as (i) receptive field and or depth of the network, (ii) classification performance, (iii) amount of clutter in the scene, (iv) context, etc.\n\nUPDATE:\n------------\nThe authors did not provide a rebuttal, so I will maintain my rating. R1 and R3 agree that using classification performance as an upper bound is questionable given the dependence between classification and localisation in this setting. R2 makes nice suggestions on restructuring the text and slimming it down to the core points.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simplification of the detection task to recognition is interest but not accurate",
            "review": "The paper tries to present an empirical upper bound of object detection by trying to simplify it as essentially a recognition problem.\n\nStrengths of the paper:\n1) The idea of the paper is simple and clear.\n2) The paper is well written\n3) The experiments are well laid out.\n4) The reference section establishes the field well.\n\nThere are several issues with the above approach:\n1) By simplifying the object detection problem as a recognition problem, we assume an interdependence of the two which many recent studies have shown not to be the case. \n2) Various Multi-Task learning frameworks in the past have shown that jointly solving related problems, that are well formulated, actually perform better than a single task. So assuming that with localization essentially replaced by ground truth and training just a classification part is an upper bound is not correct. [1, 2, 3]\n3) The upper bound found using the assumptions of 1 is trivial, and does not add anything to the conversation\n4) The sampling based strategy detailed to circumvent some of the issues is also lacking. Just because we train an object recognition model with bounding boxes sampled to have a certain overlap, it says nothing about how a network might evolve when we jointly train the task of classification and localization together.\n\n[1] Taskonomy: Disentangling Task Transfer Learning\n[2] Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics \n[3] End-to-End Multi-Task Learning with Attention",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Post-rebuttal update:\n\nAs the authors did not provide any rebuttal or engage in a discussion my score and verdict remain the same. I think I am a bit more positive about the method than the other reviewers but the paper is not ready for publication \"as is\".\n\n\n\n1. Summary\n\nThe paper presents a method to determine an upper bound for object detection performance by using a strong classifier to classify cropped bounding boxes. This way all problems with object localization are automatically avoided and performance given a strong classification model is evaluated.\n\n\n2. Strengths\n+ I think the idea is very interesting and the measure makes sense and gives a better intuition how good object detection models should be.\n+ The additional studies on the influence of context on detections are very interesting.\n+ It is great that the difficulties of the AP metrics are discussed.\n- The write up is at times hard to follow and the paper feels crammed with information. Figures and line of argumentation should be clearer. I'll summarize my suggestions below.\n\n3. Weaknesses\n- The Fashion Dataset does not really add much to the story, Pascal VOC and COCO are interesting enough.\n- It is sometimes not clear if Accuracy, AP, AP50 or something else is reported in a Table.\n- The paper lacks a deep discussion of the findings and suggestions what area object detection researchers should focus on. There is some of that sprinkled through the text but is kind of buried and easy to miss.\n\n4. Recommendation\n\nWith the current write up the paper is very much borderline. While the idea is very relevant and the reported experiments are interesting the paper is hard to follow.\n\n5. Questions/Recommendations\n\nI think the paper could (and should) be accepted but it would require a significant rewrite that may be beyond the scope of the discussion period. Here are my suggestions:\n- Focus on the upper bound and context measures.\n- Restructure:\n-- Introduction: Why do we need an upper bound\n-- Related work: How are detectors analyzed, how is AP computed (move Appendix A here)\n-- Methods: How is the upper bound computed\n-- Results: What is the upper bound on COCO and Pascal VOC, how are the upper bound and object detection performance influenced y context\n-- Discussion: What recommendations can be made from the new insights?\n- Remove:\n-- Fashion Dataset: Make it a separate paper, e.g. a workshop submission.\n-- Sampling of boxes vs Choosing just the correct box: Move to the appendix and mention just briefly that you tested this and found selecting the ground truth box to be the better strategy. It seems a lot of work wen into this comparison but describing the process adds little to the generated insights.\n-- Section 6 - Error Diagnosis: This is outdated with Bolya. Rather compare if their findings regarding the main problems correspond with yours\n-- Object detection performance on object crops: I don't this experiment tells us much as typical objects in COCO only cover a fraction of the image area.\n- Shorten:\n-- Introduction: Remove any results ans simply state your goal. This can be done in a few sentences.\n-- Related work: While the citation list is extensive the discussion of the related work was hard to follow. A better sorting of the references would be helpful.\n-- Figures 4 and 5: Show per class results for just one example and rather use the space to expand figure 1\n- Expand:\n-- Figure 1: This is the main Figure and could be larger including Open Images and the two ways to compute Pascal VOC AP\n-- Discussion of AP: Move Appendix A into the main paper and discuss the difference between COCO and VOC AP more in depth\n-- Discussion of results\n\n6. Additional feedback\n- In Figure 2 it is not clear that Accuracy is reported\n- Figure 1 is too crammed\n- Figure 6 is hard to understand. In general the figure and corresponding section should either be longer or move to the appendix.\n- Remove crop condition from Figure 7 and replace it with baseline performance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This authors investigated the empirical upper bound AP (UAP) for object detection. Concretely, the paper approximated UAP using the score of the best object recognition model trained and tested on the ground truth bounding boxes. They also studied the efficacy of visual context surrounding the target object and characterized the sources of errors in object detectors.",
            "review": "Strengths\n- The experiments are comprehensive and extensive. The paper used existing popular object detectors and datasets: more than 15 models over 4 datasets.\n- The study on the role of visual context in object recognition by enlarging or shrinking object bounding boxes is very interesting. It gave insights for understanding how deep learning models learn to recognize objects and the conclusion that enlarging or shrinking the object bounding boxes lowers the performance is an interesting finding.\n\nWeaknesses\n- The way the paper formulate the problem is to train an image classifier on cropped object regions from the original images, and use the classification performance as the UAP. However, this is still not equivalent to the object detector’s classification performance assuming the localization is perfect. When training object detectors, each object ’s receptive field is large than the actual object region, meaning that spatial context information has been utilized for recognizing the object category. Thus it is questionable that whether the model trained on ground truth boxes can be a valid upper bound for object detectors.\n- The study of the visual context showed that the canonical object size leads to the best recognition accuracy. However, in object detector, visual context is important in recognizing the category of the object. Due to properties of convolution operation, the spatial context is used to infer the object category. So the conclusion that context does not help the object classifier may not be true for object detector.\n- One of the conclusions of the paper is that “models frequently miss small objects, more often than medium and large ones”, which is known knowledge and has been studied extensively in many other works.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}