{
    "Decision": "",
    "Reviews": [
        {
            "title": "Need more theoretical or experimental support for the claims",
            "review": "Summary: This paper focuses on studying the contribution of visual information in the context of multimodal machine translation. The paper motivates the utility of explicit approaches for conditioning visual information in the form of two methods a) gated fusion of visual information, and b) an image retrieval module augmenting a standard multimodal machine translation model. In both cases, the model learns to condition visual information when the model deems it necessary for translation. The experiments are conducted on the multimodal machine translation shared task datasets and IWSLT datasets. The paper emphasises the difference in BLEU as an indication of the performance of the systems. The degree to which gating allows for visual information and difference of BLEU between learned and static retrieval-based models are highlighted as indicating the underutilisation of visual information. The paper then hypothesises that conditioning the visual information acts as a regularizer and proceeds to empirically validate using experiments where the models are conditioned with random noise instead of visual information. The paper also tries to contrast and show that the effect of conditioning on visual information for the current multimodal machine translation models is similar to regularisation using weight decay. The paper concludes with this final observation stating that the visual context is beneficial as a regulariser than as previously assumed as ``'informative' set of cues.\n\nComments: The paper is well written and looks at an interesting problem of visual conditioning and its utility. As the paper suggests, it is generally not clear where and when does visual information help the process of machine translation. The proposal of investigating the visual contribution using a gated approach and a retrieval-based approach (both are not new proposals but are used here for interpretation) is interesting. \n\nHowever, there are several problems that I came across when perusing the paper. The paper makes an assumption that the translation of the samples in the dataset will improve with visual information. Perhaps this may be a case for a proportion of samples as is indicated in Barrault et al, 2018.  A similar observation regarding a small proportion of samples that could exploit visual information was reported in Caglayan et al, 2019. The results in both gating and retrieval-based models experiments seem to indicate that it is indeed exploiting visual information over a fraction of examples. \n\nWhile using BLEU to compare systems is usually sufficient, in the context of this paper, however, I wonder if better qualitative analyses of the translated samples would help uncover the contributions of images. \n\nThe training dynamics in the proposed models is also unclear. It would also be worthwhile looking deeply into the training dynamics --- understanding how visual information is exploited during training. The only observations that the paper seems to make are by looking at performance over various testsplits. \n\nThe experiments with random noise and weight decay show that the models seem to have better aggregate BLEU scores. While this is interesting and likely has some correlation with experiments where the conditioning is done with images, I don’t think this is sufficient to claim that the visual information is acting as a regularizer. I think going deeper into qualitative evaluation would be helpful. Perhaps BLEU as a metric is unable to elicit the difference. At this moment, it lacks both theoretical and experimental support. \n\nMinor concerns: \n\na) Regarding “to ensure fair comparison and minimize training/environmental differences, all the above baselines are implemented by ourselves based on FairSeq and with the same set of hyper-parameters”. I wonder if experimenting with same set of hyper-parameters is a good idea as some of these are also involve platform changes.  Perchance the hyperparameters are suboptimal in the new implementation with PyTorch backend? \n\nb) As the experiments are conducted with adam optimiser, I wonder if the weight-decay experiments might have similar artefacts as demonstrated in Loshchilov et al, 2019. \n\nc) The experiments with IWSLT dataset in the Appendix is unclear. It is very difficult to comprehend the setup. What was the set of images here? \n\n\nDecision: Rated 4 \n\nReferences: \nBarrault, Loïc, et al. \"Findings of the third shared task on multimodal machine translation.\" 2018.\n\nCaglayan, Ozan, et al. \"Probing the need for visual context in multimodal machine translation.\" 2019.\n\nLoshchilov, Ilya, and Frank Hutter. \"Fixing weight decay regularization in Adam.\" 2019\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting work yet there is space for improvement.",
            "review": "Summary:\n\n\nThis paper tries to provide a better understanding of whether visual features are helpful for Multimodal Machine Translation (MMT). It is not new in the literature that visual features are found to be actually not that useful. However, the contribution in this paper is to provide another interesting analysis of this problem. In their work, they propose to use the so called fusion model, which mixtures two embedding spaces of text and images with a weight learned by an attention mechanism. The another interesting contribution of the model is to use the so called retriever component, which is basically an inner product between embedding of text and embedding of image. The goal of the retriever component is to generate more diverse images during training (say top_K), not only the ground truth. These techniques are definitely not new in the literature. These are a combination of different methods proposed before in the literature (for different NLP problems). However, the work claims that this is the first time it was used for the purpose of understanding the contribution of visual features in MMD. And I agree with that, to the best of my knowledge.  Finally, they claim that what visual features do is all about regularization.\n\n\nOn the positive side, this work is nice with interesting findings.  I personally learnt useful things from this work. \n\nOn the other hand, however, I have several concerns regarding the writing/presentation as well as questions regarding experiment results.\n\n1. Writing/presentation: Writing could be improved regarding the contributions of the paper and structure of experiments to validate them. The current version is written in such a way that is not clear what the paper ultimately aims for. If part of it is about that the proposed model that achieves SOTA result, it is definitely not clear where the improvement comes from (e.g. whether it is because of the use of pretrained models including ResNet-50, BERT, and Text-to-Image-Retrieval task, or because of your proposed model itself). If part of it is about analyzing the effects of visual features, while it is interesting, I feel not totally satisfied in the experiment results (see below for the detail reason why). \n\nWriting could be also improved regarding the presentation. For instance, the work has different baselines but it is not totally clear to me what the baselines are for. Are they simply for reference? Or are they there because the paper tries to show that that the model achieves SOTA results? This make me confused. \n\nAnother different example is the presentation of decoding. “Fast Decoding” algorithm is mentioned quickly without any explanation. I understood this algorithm belongs to other work, but since decoding is very important in your model, it is not satisfied to see it was presented quickly. Finally, I have a difficulty with the section \"``What does the retriever learn\" because the explanation is not clear to me. If you present a gradient formula from the paper, you should make it easier for readers to know at least why it derives that way. I understood this is proposed by another work and you try to apply it into your model, but at least some explanation might help.\n\n2. Experiment results: First, if visual features are ignored, why does fusion model is better than the balance model? If it is because the fusion model discards visual features, then why does the fusion model is better than the Transformer Tiny? I feel the contribution is very nice but the problem is evident are not clear in the paper. \n\nSecond, if you found regularization help the baseline achieve similar or better visual features, it is not convinced with the conclusion that what visual features do is about regularization. What if they learn different things but somewhat on-par results? I think readers are convinced if the paper provides much more solid argument than what is in the current version.\n\nIn summary, the paper is interesting. However, I suggested it should be revised significantly to improve readability as well as make arguments more solid.\n\n\nOther minor notes:\n\n\n- The baseline isn’t -> is not\n\n- Following standard practices, we use a pre-trained BERT model2 to obtain the “pooled” representation of the sequence (denoted as BERTCLS(x)). -> I am curious by poor do you remove the 0 token?\n\n- RETRIEVAL-AUGMENTED MMT - I understand the formula but at least there is a reference of p_{\\theta}(y|x, z)) using the gate fusion MMT. \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper is well motivated and well written. The method is simple yet effective. The analysis is insufficient to support the claim.",
            "review": "\nThis work revisits the recent development of neural multimodal machine translation. The authors split the current MMT models into two categories, among which both conventional MMT and retrieval-based MMT models are evaluated. Experimental results on Multi30K show that the small models achieve SOTA. According to the observation, the MMT models tend to ignore the multimodal information, and the analysis indicates that the benefits from multimodal models might result from some regularization effects. The hypothesis is further verified by adding random noise and using large weight decay.\n\n--------------------------------------------------------------------------------------------------------\nStrengths:\n\n1. This work investigates MMT from an interpretable perspective. This study is well motivated and the paper is well written. \n\n2. The method for the MMT task is simple yet effective, especially the small model shows competitive results on the Multi30K dataset, which overcomes the over-fitting problems in previous larger models.\n\n3. Detailed analysis of the experimental results highlights the effectiveness of regularization on the Multi30K dataset. \n\n--------------------------------------------------------------------------------------------------------\n\nWeaknesses:\n\n1.  A concern is whether the regularization effects stand generally. The conclusion drawn from only one dataset would not be convincing enough, especially for the reason that the Multi30K dataset is quite small, and the sentences are short and repetitive.\nSince there is a retrieval-based method in this work, how about the performance on larger scale WMT tasks? The analysis of different lengths of sentences from Multi30K would also be helpful. The analysis is weak to support the claim that the visual features work as regularization. \n\n2. The overfitting issue on Multi30K was disclosed before. Using images for regularized training was also not new. It is not clear how the findings in this work could help future research. \n    Reference: Caglayan, Ozan, et al. \"LIUM-CVC Submissions for WMT17 Multimodal Translation Task.\" Proceedings of the Second Conference on Machine Translation. 2017.\n\n3. There is no comparison on how the visual information affects different sizes of models, i.e., the base, small, and tiny models in the paper, which would be potential as supportive evidence to analyze whether the visual information could alleviate the overfitting in larger models. Otherwise, there would be a concern that the improvements would come from the less overfitting on the tiny model, i.e., a very competitive baseline. \n\n4. The two “proposed” models are very similar to the public ones. The gated fusion method is actually used in previous MMT models, as clarified in the paper. The retrieval-based method, including cross-modal image retrieval, was also studied before. A clarification would be beneficial.  Therefore, the statement “proposing two interpretable MMT models” would suffer from overclaiming to some extent. I would suggest to rephrase it as “providing” or “presenting” the two methods instead of “proposing” ones.\n    Reference: Zhang, Zhuosheng, et al. \"Probing contextualized sentence representations with visual awareness.\" arXiv preprint arXiv:1911.02971 (2019).\n\n\n--------------------------------------------------------------------------------------------------------\n\nMinor Comments:\n\nLine 7 in the introduction, form -> from?\n\n--------------------------------------------------------------------------------------------------------\n\nOverall Recommendation:\n\nThis paper is well written and presents good insights. However,  the experiment part is weak and lacks some important comparisons to support the hypothesis.  Also, it is not clear how the findings in this work could help future research. Hopefully, the authors can address my concern in the rebuttal period. \n\n--------------------------------------------------------------------------------------------------------\nPost rebuttal\n\nThank you for your detailed response. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting results for adversarial evaluation but weak for interpretability",
            "review": "This paper investigates approaches for multimodal machine translation (MMT) and their adversarial evaluation. The contributions are mainly twofold: (a) a conventional and a retrieval-based MMT systems which marginally improve over existing methods, and (b) an analysis of gains given by two regularization approaches in text-only MT and two MMT systems, showing that these techniques achieve similar gains to the visual modality.\n\n---\n\nStrong points:\n1) An interesting study that highlights how text-only baselines can perform similarly to MMT systems. I think this result is the main contribution of this paper\n2) Rigorous experimental setup, leading to a controlled study of several MMT systems\n3) It is interesting to see confirmed that tiny transformers perform better in Multi30k (including multimodal ones)\n\n---\n\nWeak points:\n1) Misleading title: From the title, I expected a thorough analysis of several MMT systems, while findings are basically based on the proposed systems\n2) Forced interpretability direction: I find that interpretability is a major point in the motivation of this work, however, this only corresponds to a gate vector in the Gated Fusion model. As also written in the paper, similar gates have been used in the literature. In addition, the gate used in this paper -- used to weight the image representation -- is a function of both image and text representations, and not of the image only\n3) The end of paragraph 1 in page 2 claims that images lead to smaller network weights but I do not see this claim supported in the experiments\n3) The paper also claims SOTA performance of the new models, while (a) they achieve minimal gains in most of the test sets, and (b) the results in Fig 1 make this claim even harder to believe. I find the results in Section 5.2 really informative, and, as the aim of the paper is not SOTA, I think this should be toned down\n4) Section 5.1 hints at multiple MMT systems but interpretability results (Table 2) are only given for the Gated Fusion model\n\n---\n\nMinor points:\n1) How is statistical significance evaluated in Table 1?\n2) Given the (nice!) small size of most models, I would really be interested in seeing the variance of these models when trained multiple times\n3) It would be great for reproducibility if you released code for the models and experiments\n\n---\n\nTo sum up, I think that this paper contains interesting results (especially about adversarial evaluation) but I don't personally find the general interpretability of MMT systems motivation strongly backed up by the current results.\n\n---\nPost rebuttal:\nThank you for the thorough responses. I still believe that presentation should be improved to effectively convey the findings in this paper. Nevertheless, the results are interesting for the community (although I would deeper analyses for claiming interpretability).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}