{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting, but experiments should be improved.",
            "review": "This paper proposes a probabilistic method for point clouds instance segmentation. The method extends the traditional deterministic embedding method to the probabilistic one so that uncertainty can also be considered in the subsequent clustering process. The main advantage of this method is that it is simple and can be plugged into existing embedding methods. Enables the network to take into account the uncertainty.\n\nUnder the Effect of the spatial embedding section, the authors said that 6D deterministic embedding is better than the 3D one. So what if continue to add more dimension to the deterministic embedding. Can it achieve the same performance as the probabilistic method of 6D? I think this baseline is quite simple and also effective, which may endure a lesser computation burden as computing similarity between distribution is heavier than the deterministic one. \n\nI am also wondering about the effect of spatial embedding used in Occseg, which modeling object center as a gaussian distribution and being more simple. If authors can add this comparison, it would improve the paper I think.\n\n In the appendix, the authors claimed that the method can be integrated with any embedding-based method and any backbone network. However, the experiments are based on a relatively low baseline. I'm wondering about the effect of integrating Probabilistic Embeddings into the PointGroup pipeline (a higher baseline).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Point Cloud Instance Segmentation using Probabilistic Embeddings ",
            "review": "**Summary:**\nThis work looks at the problem of instance segmentation of object parts in point clouds.  Their method is composed of four main contributions.\n1.\tThey extend the notion of pointcloud instance embeddings (e.g. Wang 2018)  to be probabilistic, that is, predicting both an embedding (the mean) and an uncertainty for each point.  This idea of capturing the uncertainty of predictions has appeared in prior work ( Kendal & Gal), but this appears to be the first use in instance segmentation.\n2.\tThis requires extending the similarities used in prior embedding work to capture the uncertainty of the embeddings.  Since they model the embedding probability density with a gaussian, they can use a simple closed form of the Bhattacharyya kernel here.\n3.\tThen they show how to incorporate this similarity into the training with a tweak from Wong 2018 to deal with imbalanced datasets.  This probabilistic approach also requires some effort to avoid degenerate solutions by assigning high uncertainties.  They regularize the entropy in this case (other Euclidean based embeddings have to solve a similar problem, but do so in different ways).\n4.\tFinally, they extend the idea from Neven et al. 2019 for initializing instance cluster centers used during inference.  Both works predict a per-class instance center map that predicts points that are likely to be the part’s “center”.  But in this work, they use center to mean most similar in embedding to the center, rather than spatial distance.  This new interpretation of center is made possible by the probabilistic embedding.\n\n**Positives:**\n-\tThe experiments cover everything that I would hope to see:\n  - Benchmark against existing state of the art on PartNet (appears to be the only dataset available for such a task)\n  - Ablation of each of the contributions (center-aware loss, full probabilistic embeddings)\n - Some visualizations to give an intuition for the uncertainties and similarity/dissimilarity  of the embeddings across the object.\n - Convincingly improves over the state of the art.\n-\tOverall the work is clearly written (see negatives for one exception)\n-\tOverall, this work extends a number of existing ideas in a clear way to achieve a high performing model that’s not overly complex. \n\n**Negatives:**\n- One of the biggest shortcomings of this work is the lack of motivation.  What’s the intuition for adding an uncertainty to the embedding?  Why do we expect this to improve accuracy of the prediction?  It would be great to see a short discussion in the introduction.\n- Section 3.4 is the only section I had trouble understanding.  I think it would benefit from a little bit more introduction – it jumps straight into the steps you take, without explaining why – e.g. what do you mean by an instance center?  I think this is exacerbated by part of it being relegated to the appendix.\n- I don’t see any reason why this method is specific to pointcloud instance segmentation.  It would have been nice to see a quick experiment on image-based instance segmentation as well for a direct comparison to Neven 2019, but I understand that’s probably a bit beyond the scope of this work.  \n\n**Recommendation:**\nI think this paper is worth accepting.   While none of the contributions are completely new ideas, the extension of the uncertainty estimation to instance embeddings isn’t trivial and has a clear benefit in the pointcloud domain.  Plus it passes the test of \"would I try this method if I were working on this task?\".  However, I am a bit worried I don’t have an intuition for *why* it helps. \n\n**Minor comments:**\n-\tIs section 3.4 semantic classification?  It seems like this section is more related to instance extraction.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "============================\nSummary\n\nThis paper proposes to learn a 3-dimensional probabilistic feature embedding space for tackling the task of 3D point cloud instance segmentation. Instead of using a deep neural network to learn 128/256-dim long but deterministic embedding space for each point in the point cloud, the authors propose to model uncertainty for each point feature by modeling it as a Gaussian random variable. So the job is to learn the mean-vector and the variance-matrix for the Gaussian distribution for each point feature. The authors also propose new losses to train such probabilistic embedding space. Experiments are done on the PartNet shape part instance-segmentation benchmark and achieve 3.1% performance improvement over previous works.\n\n\n============================\nPros\n\nThis paper proposes a new perspective of dealing with the task of 3D point cloud instance segmentation. All previous works either use detection-based pipelines, or use grouping-based pipeline. The method proposed in this paper falls into the category of grouping-based pipeline. However, all previous works along this thread consider using long and deterministic feature vectors, learned by deep neural networks. This paper is the first paper, to my best knowledge, introduces the concept of learning probabilistic embedding for the task. Also, ablation studies and superior performance on PartNet demonstrate the effectiveness of the method.\n\nTo train the network and do the grouping in the inference time, new ways for designing the network losses and performing clustering under this probabilistic framework are introduced: 1) using Bhattacharyya kernel to measure the point similarity; 2) using the sum of gaussian to describe a part instance center; 3) using  Anisotropy and  Heteroscedasticiy version of the gaussian distribution; etc. I feel that these technical contributions are strong and highly non-trivial.\n\n\n============================\nCons\n\nMy biggest concern is on experiments: \n1) I feel it is necessary to also show experiments on scene object instance segmentation. I noticed that the authors mention experiments on ScanNet in the appendix. But this is not referred in the main text at all. Also, the performance seems to be much worse than previous works. It is ok to claim that scene object segmentation is easier than shape part segmentation due to the fact that parts are not entangled while objects in scenes are quite scattered. But in other aspects (e.g. scenes can be partial, non-aligned canonically), scenes may be harder. I would expect you to achieve at least comparable performance on scene object segmentation tasks. Why the performance is much worse? We need more explanations on this.\n2) why not compare to ASIS, as mentioned by the authors in Sec 2.2?\n\nAlso, I feel the paper writing is not mature:\n1) For Eq (10), the loss function for training the network, the authors say \"See appendix for a detailed explanation of the loss function\". I feel that explanations on the loss function is very important, so you need to do so in the main text.\n2) in Sec 3.1, the sentence \"Thus we propose to model the embedding with a (symmetric) stable distribution. This leaves us two options, Gaussian and Cauchy\" is really unmature. Why only two options? Aren't there other symmetric stable distributions?\n3) in the introduction, paragraph 2, the final sentence \"We remark that the current state of the art methods (Neven et al. (2019); Wang\net al. (2018)) follow this approach.\" seems to be duplicate with the opening sentence which cites the same two papers.\n4) etc.\n\nOther questions:\n1) why using 3D feature vector? Can you try bigger dimensions? How would that compare to 3D?\n2) can you argue/show why Bhattacharyya kernel is a better choice than computing Frechet distance?\n\n============================\nOverall Rating\n\nI have a mixed feeling for this paper. On one hand, I really like this paper since it brings in a brand-new perspective of modeling per-point feature as a probabilistic embedding considering uncertainty. On the other hand, I'm not fully convinced of the effectiveness of the method due to the much worse performance on ScanNet scene object instance segmentation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The main contribution is the new similarity metric in embedding, but I was not convinced why it’s better than Euclidean distance and cosine similarity. Also, the performance in ScanNet is worse than previous work.",
            "review": "This paper introduces a novel technique for semantic and instance segmentation of 3D shapes. The main idea is to map each point in the input point cloud not to a point in the latent embedding space but to a Gaussian distribution so that the similarity across points can be defined as Bhattacharyya distance between the distributions. The loss functions for the semantic and instance segmentations are defined based on the property of Bhattacharyya distance, but the ideas mainly came from previous work. The experimental results show the StoA performance of the proposed method in PartNet benchmark (segmenting objects), but the performance in ScanNet benchmark (segmenting scenes) is inferior to that of recent previous work.\n\n*** Strengths ***\nThe proposed method outperforms previous work in the PartNet benchmark.\n\n*** Weaknesses / Comments ***\nThe biggest concern for me is that there is no explanation/discussion about why the Bhattacharyya distance of Gaussian distributions is better than the typical similarity metrics such as Euclidean distance and cosine similarity. I cannot see any theoretical reason behind the outperformance of the Bhattacharyya distance, and the authors also only show some empirical results.\n\nThe representation of Gaussian distribution in 3D space has six variables (three from the mean and three from the diagonal of the covariance matrix). Thus, for comparison, what if one maps points to a six-dimensional latent space and uses Euclidean distance or cosine similarity? The Bhattacharyya distance has numbers in [0, 1] range, and one can also obtain the same range of numbers by converting the Euclidean distance $d$ to $\\exp(-d^2/\\sigma)$ ($\\sigma$ is a parameter) and converting the cosine similarity $t$ to $1 - \\frac{t}{2}$.\n\nAlso, I could not find any experimental results for semantic segmentation. Does the proposed method perform better than softmax? \n\nI also think the uncertainty is also overclaimed. There is no guarantee that the points on the boundary of segments have a larger variance. They can have a small variance and just be located in the middle of two clusters of Gaussians.\n\nEq 3 looks ugly theoretically. Since the Bhattacharyya distance is used as a similarity metric, ideally the center of Gaussians should be the Gaussian minimizing the sum of Bhattacharyya distances to all Gaussians.\n\nLastly, this paper aims to solve the 3D instance segmentation problem, and scene datasets such as ScanNet are the most common target for that problem. But, the performance of the proposed method in ScanNet is far lower than that of StoA techniques (in supplementary). It would have been better if the authors had posed this work as introducing a novel similarity metric in general latent embedding problems.\n\n\n*** Justification ***\nI’m a bit against supporting this paper for acceptance. When seeing this paper as a 3D instance segmentation paper, the inferior performance in ScanNet sticks out to me. When seeing this paper as introducing a novel similarity metric in embedding problems, I was not convinced why the new metric is better than Euclidean distance and cosine similarity; in the other embedding problems, I would still use the common similarity metrics. I’ll be happy to discuss more after the authors’ rebuttal.\n\n*** Minor Comments ***\n- Section 1: In the first paragraph, instance segmentation does not mean predicting class labels. The combination of semantic and instance segmentations is called panoptic segmentation.\n- Section 3.1: In the fifth line, $e_i \\sim p_i(e)$, what is $e$?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}