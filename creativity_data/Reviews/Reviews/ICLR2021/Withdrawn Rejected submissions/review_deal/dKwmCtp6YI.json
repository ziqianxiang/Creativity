{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper study to what extent languages are hard to model by a conditional language-model based on information-theoretic measurements. \n\nOverall, the reviewers value the systematic and extensive controlled experiments present in the paper. However, the presentation of the paper makes it very hard to follow and reviewers all still complain that it is hard to understand the take-home message of the paper.  \n\nDespite the reviewers also appreciate the authors' effort in improving the paper, submitting the revision, responding to the feedback, they still conclude that significant reorganizing and revising of the paper is needed before it can be published. \n\nIn particular, the paper may be able to improve by backing up the empirical study with some linguistic phenomena or by a more careful rewriting in explaining and discussing the empirical results. \n\nSome other strong arguments such as \"Our application of statistical comparisons as a fairness measure also serves as a novel rigorous method for the intrinsic evaluation of languages, resolving a decades-long debate on language complexity.\" may need to be carefully revised. In this particular example, it is unclear how this paper \"resolve\" the debate on language complexity by demonstrating a few experiments.  Several sentences like this one should be revised.  \n\n"
    },
    "Reviews": [
        {
            "title": "Good premise; Unclear Paper Focus",
            "review": "Summary: The authors attempt to investigate to what extent languages are hard to conditionally language-model. They do this by using some information theoretic measures. Claims:\n- There are no statistically significant differences between source language representations, but there are significant difference between pairs of target language representations.\n- There is no complexity that intrinsic to a language except its statistical properties concerning sequence length and vocabulary (unless word-based methods are used).\n- They also observe phenomena such as Double Descent and erraticity.\n\n----\nStrengths:\n- The Experiments are extensive.\n- The relative similarity of source language representations is interesting and worth exploring further.\n\nWeaknesses: \n- The diagrams are difficult to read\n- The paper is hard to follow and would benefit from a clearer focus rather than the broad range of topics covered here. For example:\n    - It is difficult to understand what the methods/terms (the information theoretic measure used, double descent) are - little time is spent explaining these.\n    - Double descent is discussed in the paper but it is still made not clear why this is relevant in the paper.\n    - Several portions of text are repeated - with some editing, space can be made to discuss concepts important to the paper\n- The authors make recommendations for modeling (Eg. using char level or byte level models for certain models - which have been extensively studied for this): this is not followed up with any concrete results on translation/downstream tasks or pointing out relevant work.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but raises questions",
            "review": "This paper is trying to answer an important question: How does representation play a role in carrying meanings? In doing so, the authors experimented with 6 languages in  3 + 5 kinds of representations. The authors concluded that the different performances among language pairs maybe be the result of word segmentation in different ways.\n\nThis is an interesting step towards understanding meaning representations, especially in languages that do not have an alphabet. However,  as much as I agree with some of the final conclusions, the soundness of the experiments appears to be in question.\n\nMy main concerns are about the additional experiments with Chinese.\n\nThe authors claimed that \"On the character level, target language ZH (ZHtrg) shows a different learning pattern throughout.\" There are two types of character-level representations used: Wubi and Pinyin. Wubi was originally invented for professional typesetters so that they can type fast. The segmentation may not be correlated with the meaning of the word at all, as claimed by the papers cited by the authors. Pinyin, on the other hand, is highly ambiguous. One pinyin may representation dozens of words and the authors did not take tones into considerations at all. \n\nThe author also mentioned that \"After filtering length to 300 characters maximum per line in parallel for the 6 languages, we made 3 subsets of the data with 1 million lines each\". Each language carries meaning differently and the information density is drastically different. 300 characters in Chinese carry much more information than 300 characters in English. This is an unfair comparison.\n\nIt would make a lot more sense if the authors treat each language differently because of their orthographic differences.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper and experiments, but lack of evidence to support its claims.",
            "review": "The paper investigates whether languages are equally hard to Conditional-Language-Model (CLM). To do this, the authors perform controlled experiments by modeling text from parallel data from 6 typologically diverse languages. They pair the languages and perform experiments in 30 directions with Transformers, and compare 3 different unit representations: characters, bytes, and word-level (BPE). \n\nI appreciate the authors' effort for their systematic controlled experiments. However, I'm leaning towards rejecting this paper since I think some of the claims made in the paper are too strong and not really backed up by their experiments. \n\nSome comments:\n* The term \"Conditional-Language-Model\" can be misleading, since this paper model a target language conditioned on a source language, so more like in a machine translation setting rather than standard language modeling setting where you can also condition on the previous history.\n* I'm also not sure if comparing perplexity by conditioning on another different language (source language) is correct. The experiments would be clearer if done with standard LM with Transformers encoder model like BERT for example.\n* At the end of Section 2, the authors mention about \"generalizations\", but I couldn't really find any discussion about this in the paper. Maybe this can be clarified?\n* I found that claiming script bias in character models is too strong, if the experiment only shows bias in ZH (and this is expected since its character-level has different notion with languages with Latin script).\n* Byte-level: I found that there is still some \"erraticity\" in Figure 3(b) especially when the data size increases (which is more practical in real world application), so this is not entirely resolved?\n* I also think the summary in Section 1.2 stating that linguistic typological information is not necessary given \"statistical properties concerning sequence length and vocabulary\" is not necessarily valid since these two properties are the results from linguistic typology information.\n\n\nMissing references:\n1. From characters to words to in between: Do we capture morphology? Clara Vania and Adam Lopez. ACL 2017.\n2. Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. Barbara Plank, Anders SÃ¸gaard and Yoav Goldberg. ACL 2016. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting and thorough exaimination of an important problem. Writing is sometimes too complicated and some more high level analysis could help.",
            "review": "The paper provides an empirical investigation of an important problem: the transferability of language modeling signals across languages in the transformer model. This is an important question because it can teach us both on the relations between languages and the properties of the transformer model (although it is not easy to tease the two effects apart).\n\nThis is a thorough paper with a large number of experiments and with interesting conclusions that nicely generalize the low level patterns observed in the experiments. These conclusions are likely to be useful for the research community as part of its on going investigation of language transfer and the transformer model.\n\nI have several comments though:\n\n1. The language of the paper is often very complicated. Just as a couple of examples: It was very hard for me to follow the abstract, the first paragraph, the (very long) sentence that start with \"in order to eliminate\" (1.1), item 3 in the list of contributions and this is just a partial list.  I ask that if the paper is accepted the authors will try to improve this aspect.\n\n2. The writing is often over pedagogical and I often got the feeling that the authors try to educate their readers (but not in the positive sense of the word). I would try to avoid this style. \n\n3. This work seems highly relevant to the following paper:\n\n\"Towards Zero-shot Language Modeling.\" Edoardo Maria Ponti, Ivan Vulic,Ryan Cotterell, Roi Reichart and Anna Korhonen . EMNLP 2019\n\nI think the discussion parts can gain form comparing the conclusions of the two papers, when relevant.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}