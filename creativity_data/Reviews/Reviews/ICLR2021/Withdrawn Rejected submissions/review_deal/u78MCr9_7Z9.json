{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #5",
            "review": "Summary: In the context of the popular research direction of multi-objects VAEs, the authors wish to address the known problem of repeated objects in the model explanations of scenes by introducing a correlation prior to be minimized. This should force the model to produce factorizations with lower correlation and help it reason about relationships between objects. The authors evaluate their prior by comparing several state of the art multi-objects VAE with and without the proposed prior and show little improvements in terms of MSE and factorization accuracy over the baseline. The authors also propose a novel measure of VAE models improvement, based on estimating the difference in ELBO between different models.\n\nGood:\n* The problem approached is important to the community and would certainly benefit it if solved. The qualitative examples of segmentations and reconstructions are promising.\n* Interesting novel approach to evaluating VAE models improvement. \n\n\nCould be improved:\n* The results  table has some issues: it is not significant to  report a standard deviation with multiple significant digits. While not impossible, a standard deviation of 0 is at least suspicious over 5 different runs of any deep model. Multiple of the results are highlighted even though they are exactly the same as the control group (MulMON MSE on CLE-VM and Monet MSE on Dolphin).\n* Section 4.3, if the performance gets busted with higher values of sigma, doesn’t that mean that any value could work and so that the prior isn’t that useful?\n* One of the main strengths of the paper to me is the qualitative examples of how your prior prevents the duplication of objects. Is there an objective metric that would measure that? That would be a great contribution to the community too.\n* You claim that current unsupervised scene representation methods don’t reason about object relations, but I would argue that this direction is currently being explored, for instance with graph neural networks to model such relationships, see Contrastive Learning of Structured World Models, Kipf et al.\n* Why is CLE-VM used instead of the benchmark CLEVR? The authors should justify the choice of getting away from the established benchmarks. Regarding comparisons with benchmarks, the proposed metric would be more valuable if presented together with a throughout evaluation of existing models and evidence that improvements in the metric do correlate with improvements on other established tasks.\n* Figure 5: actually reporting the correlation coefficient would be more convincing. And what about the other models?\n\nMinor:\n* Figure 6 looks like it was squashed horizontally, please fix the proportions\n\nIn conclusion, the proposed approach shows some promising qualitative results but the reported evaluation doesn’t yet support the claimed improvements. I wouldn’t recommend accepting it in the current state.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple approach with effective results",
            "review": "Summary:\n\nThe following work solves a shared issue in compositional scene generation models that aim to factorize images into up to K individual components, each with their own encoded representation $z_{k}$. The issue with existing works is that no constraints are placed on the predicted indivudal $z_{k}$ representations to prevent them from being correlated with each other, thereby making the decoder's task of reconstructing the scene with clear delineation between visual components much harder. The proposed solution is to compute the correlation matrix between all $z_{k}$ vectors and to penalize this correlation matrix by mapping it to a decorrelated prior.\n\nStrengths:\n\n-Simple technique to solve a systemic issues in a family of methods\n\n-Interesting analysis on the variational gap -- specifically on the use of the lower bound to evaluate the improvement when the true likelihood of the data is unavailable.\n\nWeaknesses and concerns:\n\n-In general, I felt that the explanation was much more complicated than it needed to be. If not read carefully, the introduction was also easily misinterpreted to seem as if the goal were to relax the decorrelation assumption, when in fact, as the authors had to explicitly point out, it is the exact opposite.\n\n-Based on my understanding, the following work is in line with contrastive representation learning works such as [1] [2]. The application of the decorrelation prior is mathematically equivalent to maximizing the cosine distance between any pair of feature representations. Unless I am mistaken, I think it would have been much clearer to explain it as such.\n\n-I believe there is an unstated assumption about this work, which is that it assumes each visual component appears at most once. If that is the case, I think it would be best to state that somewhere. \n\n\n[1] Chen et al. A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020\n[2] He et al. Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020\n\n\nOverall, simple technique with solid results, decent analysis, but the writing seemed much more complicated than it needed to be.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The results are interesting, but I think that the authors misunderstood the methods they are trying to improve.",
            "review": "Compositional VAEs (MONET, IODINE, MultiMON) represent an image by a set of latent variables, where every latent variable represents a separate component of the scene. This paper is trying to improve comp-VAEs by penalising correlation between latent variables inferred from a given scene. The authors state that this improvement is motivated by the observation that \"[current models] do not reason about scene objects' relations\" and the fact that, sometimes, several latents are used to explain the same scene component. The additional penalty is applied to the three aforementioned models and is tested in terms of ELBOs and segmentation metrics on two datasets (CLE-MV and Dolphin). The results with the added penalty are generally improved with respect to the baselines, which seems to support this approach.\n\nWhile the results are interesting in the sense that the performance is clearly improved, the explanations and intuitions presented in the paper are clearly wrong and some of the evaluations might be incorrect, which I will discuss shortly. I also do not see sufficient connections to non-maximum suppression (NMS, further discussed below) to include it in the title and the method name. As it stands, the paper requires a significant rewrite, and I am afraid it is not fit for publication.\n\nHad the motivating issues (lack of relational reasoning, multiple latents used to explain the same scene component) as observed by the authors were true, I would agree that fixing them is a reasonable goal for a paper. However, this is simply not true in case of  of the mentioned models (perhaps beside MultiMON, which I do not know, and that is cited as \"Anon, Anon, Neurips, 2020\").\nFirst, all of the models do some sort of relational reasoning:\n* MONET infers objects' masks autoregressively and conditions appearance variables on the masks.\n* in IODINE, different components are modelled as conditionally-independent given the previous inference steps. Once the previous inference steps are integrated out, the latents are clearly dependent.\n* GENESIS (not mentioned by the authors until related works) infers latents autoregressively AND introduces an autoregressive prior that allows modelling dependencies between latents/objects.\n\nSecond, since all of the models use per-pixel gaussian mixtures to explain the images, it follows that it is almost impossible to use two or more mixture components to explain the same object in the scene, and especially not in a way that the reconstructions from different components overlap with each other (as is the case for the red objects in Fig3, reconstruction for iodine(0) and multimon(0)). This is obvious when one inspects how samples from such models are created: we first sample a component, then a reconstruction from that component. For the same reason, every pixel in the scene **has to** be accounted for by at least one component, so that apparent holes that are visible in the third column of Fig 3 should not exist. Those two issues (overlapping reconstructions, holes) suggest a bug in the implementation, that I imagine could have led the authors to believe that their method is related to NMS. More specifically, monet and genesis use a stick-breaking construction of the masks, and iodine uses softmax (see alg. 1 in the iodine paper). The reported results look as if the generated masks were independent, and perhaps a sigmoid was used for individual masks instead of a softmax?\n\nThird, we can see that the reconstructions from monet(0) and multimon(0) in fig3 are far from perfect. Since CLEVR-like datasets are quite easy to reconstruct (and model, in general) this suggests that the models in question were not trained long enough.\n\nFourth, one of the main metrics used in the paper, dubbed \"suboptimality measure\" is the difference of ELBOs between two models.  This is used instead of the approximation gap (measured as the KL between the optimally-trained approx posterior and the true posterior), which cannot be computed due to the inaccessibility of the true posterior. On the surface, this is ok, because as demonstrated in (eq 5), we can subtract the KLs and be left with a difference of ELBOs because the log marginal probability cancels out. In practice, this assumes that the log probability p_theta(x) is the same for both models. Since p_theta(x) is a function of the prior and decoder parameters, it will be different in every training run (even for the same model, not to mention different models), and therefore the difference of ELBOs cannot be used as a proxy. Instead, I would suggest estimating the marginal probability using an IWAE bound with K \\in {100, ..., 5000} particles and subtracting the ELBO value from that--this would give the estimate of the KL(approx_posterior || true_posterior).\n\nAll of the above points are significant and each point by itself would raise my suspicion towards any paper. Taken together, I believe they completely justify my score, and therefore I recommend rejection. This is NOT to say that the paper is not valuable. I find the results interesting, but perhaps a better line of inquiry would be that of learning disentangled representations? Disentangled representations are often thought to be independent (see BetaVAE of Higgins et. al. or FactorVAE of Kim and Mnih), and I do not know many works that consider disentanglement of several multivariate latent variables. The approach here seems to work, and I would be happy to see a rewrite of this paper at a future venue.",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}