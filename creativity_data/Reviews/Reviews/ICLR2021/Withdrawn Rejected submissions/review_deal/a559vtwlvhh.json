{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper investigates the effect of the dirichlet hyperparameter in alphazero style architecture. They argue that the hyperparameter has an important effect on exploration and its value should be understood with regards to the typical branching factor of the environment.\n\nI think the significance of the paper is weak - it's hardly surprising that the exploration hyperparameter has a significant impact on performance; it's well known that exploration hyperparameters play an important role in pretty much any RL application. The additional claim - that it can be understood more intuitively than others, is believable, but somewhat poorly substantiated. \n\nThe toy problem the authors study is only weakly related to the way alpha is used in alphazero. Because it is used only at the root, the direct effect is to drive exploration in the real environment (instead of the simulator); then indirectly in the search (as the policy prior learns to imitate the behavioral policy). The authors instead apply exploration at all nodes; moreover, they branch by sampling the policy prior instead of using an PUCT-like scheme. The authors acknowledge these limitations, but it remains that the scheme is different enough from alphazero than it's unclear what knowledge can be acquired from it.\n\nThe connect 4 environment only show that alpha is indeed important (and arguably should be parametrized as log alpha), but does not really demonstrate the parameter could be set a priori through inspection of the branching factor.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Results are interesting but idea is not novel enough",
            "review": "##########################################################################\n\nSummary:\n\n \nThe paper motivates the importance of alpha, a hyper-parameter used for defining the dirichlet distribution used in the tree search exploration of alpha-zero. Motivated by hyper-parameter results in the alpha-zero family papers, this paper carries out ablation studies on alpha with some small games and observes that the best choice of alpha correlates with the difficulty of the game (e.g. branching factors). Finally, the paper shows experiments on the Connect-4 game and show how the performance changes as alpha changes, corroborating previous claims.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. The paper carries out a detailed study on the importance of the hyper-parameter alpha, but I do not think overall the contribution warrants a publication at ICLR. The hyper-parameter alpha is tied to exploration, which is intuitively reasonable, and it is interesting to confirm this hypothesis by running detailed and comprehensive ablations; however, as solid as this contribution might be, this alone does not seem novel & useful & comprehensive enough.\n\n##########################################################################\n\nCons: \n\n \n1. Idea is not novel. The paper confirms that alpha/beta are critical for games with varying degrees of difficulty. However, this alone is not useful enough. It would be nice if one could design an automatic tuning scheme for alpha/beta as the training evolves.\n\n2. Figure 5 shows two plots with the same results but alpha vs. log(alpha) on the x-axis. It seems that the win rate change is not as sharp as shown in Fig 2. Could the author explain the reason? From Figure 5 the change in win rate <0.2, throughout such a big range of alpha; yet for Fig 2, the change could be from 0 to 1.\n\n \n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above \n\n \n#########################################################################",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Too imprecise and contributions not convincing",
            "review": "Summary\n---\n\nThis paper investigates how the \"alpha\" hyperparameter used in the AlphaZero line of work for the Dirichlet noise applied to prior probability distributions over actions in the root node can be tuned. This includes investigating the relation between performance, alpha, and branching factor in a toy problems, as well as evaluating an AlphaZero-like system with various values for alpha in Connect 4.\n\nStrong Points\n---\n\n1) Paper is well-structured\n2) Relatively easy to follow/understand what point the authors are trying to get across\n3) Nice to see a deep, detailed focus on something as small as a single hyperparameter\n\nWeak Points\n---\n\n1) The toy problem seems to be structured in a highly specific way which is quite different from the domains we're presumably interested in when talking about AlphaZero-style approaches (games), and to me this highly specific structure looks like it would crucially affect the behaviour of specifically this Dirichlet noise for exploration stuff. More concretely; the toy problem has a completely uniform structure, with every internal node having exactly the same number of children, all leaf nodes being at exactly the same depth, and every leaf node having exactly the same probability of having the non-zero reward of 1, independent of where it is in the tree. This structure is clearly very different from what we normally expect in \"interesting\" search trees, where we would typically expect much less symmetry in all these aspects. Due to the specific structure, it looks to me like the results can be easily explained for this particular case; if the branching factor is low enough, the search can reach sufficiently many leaf nodes to have a reasonable chance of encountering the nonzero reward even with uniform search, so it's better to draw a uniform noise from the Dirichlect distribution. If the branching factor is too high, the search has to simply randomly pick a smaller number of paths in the tree to \"zoom in\" on and hope to get lucky that one of them has the nonzero reward, because the tree is too big to uniformly search it all. Unfortunately, I don't see this explanation being generalisable to any problem other than this specific toy problem.\n2) I don't feel that some of the claims in the paper about it providing new insights that can help us better tune this hyperparameter based on intuition (as opposed to just doing a hyperparameter sweep) is supported by the contents of the paper. As mentioned above, I don't see the conclusions from the toy problem generalising to other problems. Except maybe the intuition that the optimal alpha probably somehow relates to a game's branching factor, but this is not a new insight; this insight was also already described and used in https://arxiv.org/abs/1902.10565 (which is not cited here), and other \"non-academic\" sources such as https://medium.com/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5. Aside from the toy problem, there is only an evaluation in Connect 4. This is just a single game, and therefore again really not enough to generalise conclusions more broadly to other domains. A thorough evaluation on many different games would have been nice!\n3) Many cases of imprecise language:\na) In abstract: \"the alpha parameter governing a search prior\" is not accurate, since the policy head output is often already referred to as the \"prior\" for the search. Something like \"governing exploration noise in the search\" may be more accurate.\nb) \"exploration dictates the final action distribution\" under 1.1, but clearly that's not true, exploration is only a part of the algorithm and exploitation also plays a big role.\nc) \"The hyperparameter alpha influences the balance between trusting the networks predicted policy or the experience and exploring the problem space.\" is not accurate. Alpha dictates what kind of noise distribution we add (a uniform one vs. one with most probability mass centered on a small number of actions). There's a different hyperparameter, usually epsilon, which controls the weight given to whatever noise distribution is sampled from the Dirichlet distribution, and the role of epsilon seems much closer to what's described here.\nd) \"In the above formula c_{puct} is a hyperparameter to weight the trade-off between recent simulated experience and the neural network trained policy\" is not correct. It provides the trade-off between the \"exploration\" term and the \"exploitation\" term. The exploitation term contains the estimated value, whereas the exploration term contains the policy head output (already mixed with Dirichlet noise) as well as the term with the visit counts.\n\nOverall Recommendation\n---\n\nIn particular points 1) and 2) listed under the Weak Points make me recommend rejection at this time.\n\nQuestions for Authors\n---\n\nCould you provide more details on the experimental setup? For example:\n\n1) Which values of alpha did you sample when you write \"we sampled several different values of alpha\"?\n2) What neural network structure was used?\n3) What kind of hardware was used? For how long? How many self-play games? How many epochs? Which optimiser? Learning rate? Experience buffer capacity? Other training hyperparameters?\n4) Page 6 mentions \"trained to convergence\". How was it establish that training really converged?\n\nMinor Comments\n---\n\n- The (Wang et al., 2019) citation right at the start, in front of the (Silver et al.) ones when discussing the AlphaZero line of work looks out of place; both in terms of formatting (in front of the other ones, rather than in the semicolon-separate list of other references), and in terms of relevance (it's not one of the original works in the described line of work).\n- When discussing related work for AlphaZero hyperparameter tuning, should probably cite and discuss https://arxiv.org/abs/1812.06855 (it's technically about AlphaGo instead of AlphaZero, but there seems to be enough obvious overlap there).\n- I'm not sure what \"We demonstrate the strong flexibility afforded by introducing alpha as a hyperparameter\" in Introduction refers to, since there is no evaluation of what happens when there is no alpha (which I assume would imply no Dirichlet noise) in the paper and how that would be less \"flexible\".\n- No references to literature at all when first mentioning MCTS (and no description of what the acronym stands for).\n- Page 3 references Table 2.2, but there is no Table 2.2.\n- Describing the results in Figure 5 as a \"150% improvement in win rate\" looks a bit strange. Much more useful to just talk about the percentage points.\n- The two subfigures in Figure 5 should probably have the same range of values for y-axis.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "AlphaZero Hyper-Parameter Sweep on Small Games",
            "review": "The AlphaZero algorithm uses a Dirichlet distribution to inject exploration noise into the search. The optimal choice of shape parameter alpha for this distribution depends on the game being learned -- intuitively one would expect an approximate dependency on the effective branching factor of the game. The authors of this paper show that the choice of alpha greatly affects performance both in toy games and in Connect-4.\n\nThe experiments on toy games and real games are competently done.\nThere is no theoretical discussion of the role of the Dirichlet noise distribution or the optimal choice of alpha, and overall there is not much novelty or insight in the paper. \n\nThe Dirichlet exploration noise has two distinct effects. Where the prior is close to uniform, it concentrates exploration on a single action, increasing the depth to which the search progresses by narrowing the width. This effect is what is measured in the toy game, where performance depends on finding the reward at depth 5. \n\nOn the other hand, where the prior is concentrated on a small number of actions, the noise will serve to increase width (by introducing another action for exploration) while decreasing depth. This is helpful in exploring alternative actions rather than concentrating excessively on the principal variation. As an extreme example, if the prior is greatly concentrated on a single action at the root, then it is possible that all search iterations select that action, meaning that the search provides no useful information about the relative merits of different actions at the root. Injecting Dirichlet noise effectively prevents this failure case, and is helpful in less-extreme cases also. This effect is not discussed in the paper; it is also possible that the experiments performed, which use partly-trained networks, do not uncover this effect because the prior has not trained sufficiently.\n ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}