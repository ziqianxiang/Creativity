{
    "Decision": "",
    "Reviews": [
        {
            "title": "A new multi-step off policy RL algorithm",
            "review": "**Summary:**\n\nThis paper presents a new multi-step RL algorithm called Greedy Multi-Step Value Iteration (GM-VI). The new algorithm is able to adjust the backup length at each time step. Some theoretical results are provided that show its convergence to the globally optimal value function, as well as some other desirable properties. Finally some experimental results are presented on 2 toy benchmark domains.\n\n**Strengths:**\n\nThe paper presents a novel multi-step algorithm that could be applicable to a wide variety of problems. It appears to be theoretically sound, and the theoretical statements are presented relatively clearly. There is some experimental support showing its utility.\n\n**Weaknesses:**\n\nI think the major weakness with this paper is the experiments. The abstract states that the paper demonstrates: “state-of-the art performance on a series of standard benchmark datasets,” and the introduction says something similar. This statement is a bit misleading. First, only two domains are evaluated: Mountain Car, and Acrobot. This is not really an extensive series of tasks. Second, achieving “state-of-the-art” performance on these tasks is not necessarily a significant achievement; they have been carefully studied for a long time, and they are relatively small compared to many of the challenging problems being tackled by RL researchers today. However, these domains are very useful for understanding how an algorithm performs, and evaluating the relative differences between comparable algorithms. But I do have some additional reservations.\n\nOn mountain car specifically, a good policy can achieve returns of about -110 per episode, which is better than the performance of any of the algorithms presented in the paper. Does GM-Q learning ever achieve this level of performance? (Also, the reward is Mountain car is in no way sparse — the agent gets -1 reward at every step.)\n\nGM-Q learning is compared to 1-step Sarsa, but GM-Q has a significant advantage in that it is able to use a multi-step return. I think it would be fair to compare n-step Sarsa, either on-policy, or off-policy with importance sampling.\n\nEspecially for these small domains, 10 independent runs for each algorithm is far too few. I would recommend at least 30. More runs would greatly strengthen the conclusions about the relative performance between the algorithms.\n\nShowing confidence intervals on learning curves is usually a good way to get a sense of the variance in the performance samples and to try and judge if the means are actually different. However, I don’t see the utility in plotting 40% confidence intervals; that seems far too low to be useful. Along with adding more runs, I suggest increasing the confidence (to at least 90% maybe?).\n\nAlternatively, in the low run regime, sometimes it can be more informative just to plot each of the learning curves for all the runs on the same plot, along with the mean. This gives a really good sense of the variability between runs. Presenting the results this way assumes less about the distribution that the performance samples are coming from.\n\n**Recommendation:**\n\nI think this paper is below the acceptance threshold; I argue to reject. I think expanding the experiments section to include more domains, comparing to some additional algorithms (e.g. n-step Sarsa), and/or increasing the number of independent runs (seeds) for each, would increase my score.\n\n**Questions:**\n\nSee questions and suggestions in weaknesses.\n\n**Minor comments:**\n\nI think the term “back-propagation” is used incorrectly in a few places in the paper. For example at the beginning of Section 2.2: “VI approach aims to approximate the optimal value function by back-propagating the optimal value.” In the RL book, Sutton and Barto use the the phrase “backing up” for this concept. And rephrasing the statement with this term would give: “aims to approximate the optimal value function by backing up the optimal value.” This distinguishes the operation clearly from the backpropagation algorithm used for training neural networks.\n\nThe paper would benefit from a careful proofread. There are some grammatical mistakes throughout that can be distracting. For example:\n\nEnd of page 1:\n\n“As mentioned in the next section, **very few research** (Horgan et al., 2018; Barth-Maron et al., 2018) in **literatures address** these issues”\n—> “very little research … in the literature addresses…”\n\nBeginning of page 2:\n\n“The core idea of our method is to greedily **chooses** the largest value among various-step bootstrapping **estimation**, so as to approximate optimal value as quickly as possible.”\n—> “The core idea of our method is to greedily **choose** the largest value among various-step bootstrapping **estimates**, so as to approximate the optimal value as quickly as possible.”\n\n**Minor questions:**\n\nBottom of page 3:\n\nThe text states: “our GM-optimality operator also looks forward for multiple steps but greedily chooses the optimal one among all these bootstrap values.”\nIn what sense is the operator choosing the “optimal” bootstrap target? Isn’t it just choosing the maximum?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "## Summary\nThis paper investigates multi-step value iteration. It shows the contraction rate of a greedy multi-step algorithm and compares this with the contraction rate of the n-step Bellman Operator. The paper concludes by demonstrating its algorithm performance on Mountain Car and Acrobot.\n\n## Review\nThis paper is a clear reject for me. The literature review leaves a lot to be desired. N-step methods have been discussed at length in the community, but this paper cites only Sutton and Barto 2018 with regards to n-step learning? And only one paper cited for eligibility traces? Almost all other cited papers are largely irrelevant or---at least---are orthogonal to the point being investigated (a few deep RL systems that happen to incorporate [but do not investigate] n-step learning, prioritized experience replay, double q-learning, etc.). The writing and grammar in this paper make it difficult to read to the point of hindering my ability to accurately comprehend and review the paper. The experiments show no improvement of the proposed algorithm over Q-learning. The theory section is largely trivial or already known, except perhaps the contraction rate proof for the novel algorithm.\n\nFor the literature review, some useful papers (this list is not exhaustive, merely long enough to demonstrate my point that the literature review is not satisfactory): \n* Tomar, Efroni, and Ghavamzadeh 2020 \n* De Asis, Garcia, Holland and Sutton 2017\n* Mahmood, Yu and Sutton 2017\n* Efroni, Dalal, Scherrer and Mannor 2018\n* White and White 2016 (A greedy approach to adapting the trace parameter ...)\n\nUsing 40% confidence intervals with 10 random seeds is not sufficient statistical evidence that the proposed algorithm performs any different than n-step q-learning. On Acrobot, the algorithms overlap as it stands with 40% confidence intervals, on Mountain Car the separation is larger but still not convincingly so (with the statistical interpretation being that there is at best a 40% chance that this result is meaningful anyways). All of this ignoring that 10 random seeds is likely leading to an underestimation of the variance of these algorithms. I should note that both n-step methods and the Mountain Car domain are known for having high variance, so a more careful statistical investigation should be used for these experiments.\n\n### Some nitpicks\n* I wouldn't call $n$ a stepsize. At the very least, this confused me significantly on my first pass through the paper. Controlling $n$ does not control the \"size of the update step\".\n* Figure 2 is unreadable and I'm not sure what it was meant to communicate.\n* Are you *certain* that you don't need importance sampling ratios? The paper mentions that the goal is to find the optimal value function, not a specific policy. However, the algorithm takes the max action at each step which is implicitly defining a greedy policy at every step. The algorithm will converge to the optimal value function, under a deterministic optimal policy. While I believe that the algorithm is still convergent to the optimal value function, even in absence of the importance sampling ratio, I believe that each update step will be biased. This should likely be discussed in more depth as it is not an obvious result and its implications are likewise not obvious.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to multi-step value iteration, although with some theoretical issues",
            "review": "Summary of paper\n\nThis paper introduces a new operator for value-iteration-style learning in RL, based on forming a Bellman target by taking a maximum over observed n-step returns of varying lengths. \n\nReview summary\n\nOverall, I think there are some promising ideas in this paper, and the approach the authors propose may be an interesting algorithm to add to the toolbox of methods for deterministic MDPs. However, I think there are some issues with the theory in the paper, and additional details in the experiments section will aid the comparison of the newly-proposed method with the baselines considered there.\n\nSignificance / Originality\n\nMulti-step Q-learning methods are increasingly popular in deep RL, and often seem to outperform single-step methods empirically. This is still an area where our theoretical understanding is lagging behind what these methods can empirically achieve, and so further work in this area will likely be valuable to the broader RL community. The specific idea proposed in this paper is straightforward, as far as I am aware is original, and may be of interest to the deep RL community. However, I believe there are some issues with the theory and experiments that mean the paper is not ready for acceptance at ICLR.\n\nClarity\n\nOverall the writing is generally clear. There are some typos that should be cleared up, but I think the authors do a good job of describing the crucial background material, and clearly describing their new proposed operator. I appreciated the inclusion of a worked example at the end of Section 3, although think that this could be further improved to make its point more clearly.\n\nQuality\n\nAs described above, the authors introduce a straightforward variant of value-iteration-based learning. Whilst this idea is attractive in its simplicity, and seems to perform well in the experiments run in the paper, I believe there are some problems with the main theoretical results, which are expanded on below, which reveal that the method may be unsuitable for stochastic environments. In addition, the experiments section is currently lacking important details, which make it difficult to compare the newly-proposed method against their baselines. I hope the authors can provide these details in their rebuttal.\n\n\n\nMain technical comments:\n\n\n\nI have checked the proof of Lemma 1 and believe it to be correct.\n\nThe statement of Lemma 2 should be updated: it states \"given any value function q\", but only states a result for Q^\\pi.\n\nI don't think Theorem 1 is correct, and I think this problem feeds into the results in Section 4.2 as well. In general, I think the fixed point of \\mathcal{G}^N_\\mathcal{P} will be different from Q^*, and will not necessarily yield the optimal policy when acting greedily with respect to the fixed point. This essentially stems from the \"max\" in the operator being inside the expectation.\n\nThere are several problems with the current proof sketch in the main paper. The first is that currently, Lemma 2 applies only to Q-functions Q^pi corresponding to a policy pi. Therefore, if we initialise our Q-function such that it is not equal to Q^pi for any policy pi, the reasoning does not hold. Further, even if we do initialise at Q^\\pi, the second inequality does not follow from Lemma 2, since G^N_P Q^\\pi may not be the Q-function for any policy pi. Finally, I can't see the logic behind the final clause - even if it were true that the sequence (G^N_p)^k Q^pi converges monotonically to \\hat{Q}^*, I don't see why it follows that \\hat{Q}^* should be equal to Q^*. We can conclude that it must be greater than or equal to Q^*, but not equal to it in general I believe.\n\nI expect a version of Theorem 1 might be salvageable under restrictions on the MDP under consideration, such as deterministic rewards and transitions. The environments tested in the experiments section have this form, which may explain the good performance of the proposed method on those environments. However, I would expect the model to perform less well when there is significant stochasticity present in the environment (in which the argmax over which n-step length to pick will be dominated by high variance rewards, rather than better performance in expectation).\n\nExperiments:\n\nAt a high level, the experiments seem to indicate positive performance for the proposed method relative to the baselines considered. This is promising, but I have a couple of comments that would make these comparisons more convincing. At present, it is difficult to evaluate these empirical results due to a lack of information about the training procedure, and how hyperparameter sweeps were carried out. Some details that are unclear include:\n - The precise architecture of the neural network function approximation. Are these multi-layer perceptrons? What activation function was used? How were the weights initialised? Was a target network used?\n - Which optimizer is used?\n - The hyperparameter table mentions a \"start exploration rate\" epsilon - was this decayed throughout training?\nThe manner in which hyperparameters were selected is also unclear:\n - Were any hyperparameter sweeps performed (e.g. on learning rate, exploration epsilon, buffer size, network architecture)? Why were different architectures used for the two different environments?\n - How was the lambda hyperparameter in Retrace chosen?\nLastly, as described in sections above, I would expect qualitatively different performance on stochastic environments, and would encourage the authors to consider comparisons on such environments too.\n\n\n\nMinor comments:\n\n\n\n\"For a method like [value iteration], the good side is that it can safely use data from any behaviour policy without off-policy correction\". I think that this is only true for single-step value iteration methods (and is also the case for single-step policy evaluation methods too), and technically speaking under function approximation one should make an off-policy correction for the state visitation distribution in any case.\n\nI think the diagram at the end of Section 3 could be significantly improved. I appreciate the authors' inclusion of a worked example, but found the diagrams quite cluttered, and difficult to extract insights from without spending a lot of time studying the details of the charts.\n\nSeveral typos to be tidied up throughout:\nAbstract: \"algorithms\" -> \"algorithm\", \"resulted\" -> \"resulting\".\n(Hasselt, 2010) -> (van Hasselt, 2010).\nConsider expanding contractions such as \"it's\" to \"it is\".\nConsider tidying up references (capitalization in titles for e.g. Q-learning), referencing conference versions of papers rather than arXiv etc.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theoretical results are not convincing",
            "review": "The paper deals with the question of $n$-step bootstrapping and how to dynamically choose it. Specifically, the proposed method is to iterate on all possible $n$'s and to simply select the best computed sum over all possible $M$ values of $n.$ \n\nIn general, this method does not rely on the measured quality of the value approximation, even though it is portrayed this way in the introduction. Instead, it relies on choosing the maximal numerical value over multiple choices of $n$; I don't see how this makes 'mathematical sense'. The paper doesn't provide such an explanation (at least not a convincing one, to me). It seems to me, instead, that simply taking the maximum over several steps will actually have an adverse effect which will impair performance: the maximal approximate value over $M$ possible future states is expected to be highly sensitive to over-estimation errors and outliers. I also find the naming 'adaptive step-size' wrong, since the horizon length is what's optimized here and no step-size is involved. \n\nAdditionally, I'm having difficulty with the claim that in the paper's setup, one does not need to estimate a value function, which obviates the need of off-policy corrections.  The authors base it on the fact that they deal with a value-iteration and not a policy-iteration algorithm. I tend to disagree and believe that if no off-policy correction is used, one needs to either optimize over the initial $n$ steps, as is done in the h-step greedy VI/PI in [1], or alternatively work with on-policy samples. I don't see how the offered maximization over $M$ possible $n$ steps addresses the issue that the trajectories are generated from arbitrary actions from various policies. So in other words, I'm not sure the theory here is correct. In the same spirit, Lemma 3 gives a stronger result ($\\gamma^N$ contraction) than Lemma 1 ($\\gamma$ contraction), even though the operator in Lemma 1 is a 'stronger' generalization of the one in Lemma 3. How is that possible? The proof, by the way, is missing and points to Lemma 1; but I don't see how it follows. Also, I find it hard to believe that a $\\gamma^N$ contraction is attained without the multi-step optimization done in [1]. It is believable for evaluation, but not for maximization. Regarding the latter point, BTW, I'm especially curious and it should be relatively easy to find a reference and prove me wrong.  \n\nAn additional qualm is that the algorithms analyzed here are classical planning variants that have full access to the model and go through all possible states at each iteration, hence the monotonic improvement in Lemma 2 is guaranteed. These algorithms do not compare with sample-based Q-learning or others. I find it problematic that the authors do not mention it at all in the paper. The authors are also oblivious to this important distinction in Alg. 1, in Table 1, and in the experiments section. \n\nRegarding Theorem 2 -- item 1 is trivial so I suggest dropping it. And regarding Theorem 4 -- the condition on $\\pi$ there is extremely strong, making the result almost vacuous. It basically says that we already have the optimal policy and are just re-applying the Bellman recursion in one way or another.\n\nLastly, the paper is poorly written and grammatical errors are abundant. Several examples are:\n\np.1, last line: \"...very few literatures address...\"\n\np.3, below (3): \"Classical implementation includes:...\"\n\np.3, GAE operator: \"the ... operator is defined as ..., which assign exponentially ...\"\n\np.3, one-before-last sentence: \"looks forward for several steps and take the corresponding...\"\n\np.4, first par.: \"Otherwise if the trajectory is worse than it has explored before, ..., which only involve the ... but not concern about a ...\"\n\np.5, first par.: \"the Q-function is update ...\"\n\np.10, proof of T2: \"we proof 2\".\n\n\n\n[1] Efroni, Y., Dalal, G., Scherrer, B., & Mannor, S. (2019, July). How to combine tree-search methods in reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 3494-3501).",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}