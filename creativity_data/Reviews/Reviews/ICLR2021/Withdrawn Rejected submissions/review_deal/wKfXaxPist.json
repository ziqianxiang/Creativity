{
    "Decision": "",
    "Reviews": [
        {
            "title": "Important baselines missing making utility unclear. Loss in quality may be unacceptable.",
            "review": "# Summary\nAdapts DACT (Eyzaguirre & Soto) for Transformer (BERT) models to reduce in number of Transformer (BERT/ROBERTA) layers/blocks that need to be computed. It is applied in a fine-tuning stage to BERT/ROBERTA and evaluated on some GLUE tasks with some drop in quality while reducing approximately 50% of layers that need to be computed. DeeBERT (Xin et al) is the main baseline compared.\n\n# Pros\n1. Presents a method for reducing computation to consistently improve compute by ~ 50% in BERT/ROBERTA models.\n\n# Cons\n1. Unclear how does it compares  (quality-wise) to following baselines: (a) distillation; (b) training BERT with 1/2 the layers. Those are obvious alternatives for saving 50% compute, which is approximately what this method does, while also saving 50% parameters.\n2. Interpretability results are not convincing. Integrated Gradients can also be applied to BERT, and it is not clear whether the attribution is better in this adaptation. The only evidence provided is two samples.\n3. The DeeBERT results in Table 1 do not seem to agree with Table 1 in the DeeBERT paper. In the latter, there is minimal quality loss, whereas in the former quality-loss is very significant. Can you explain this?\n4. Quality-loss in some tasks is significant, e.g. RTE. It is difficult to trust if the quality-loss is unpredictably large. The claim in the abstract is not true: \"Our experiments demonstrate that our approach is effective in significantly reducing computational complexity without affecting model accuracy\"",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel approach but lacks comparisons to appropriate baselines",
            "review": "The authors propose introducing a differentiable module that can control when the execution can be stopped during inference. Their approach is able to reduce computation by ~50% during inference for the tasks they consider while sacrificing some accuracy. \n\nThe proposed technique is a novel modification of prior work (Graves et al) to transfer learning in NLP.  Since the proposed approach is fully differentiable, it allows the model to learn when to halt execution which can be very beneficial. The proposed approach causes significant loss in accuracy in some cases (RTE task).  The adaptive mechanism does add some interpretability but other static approaches like DistillBERT also make the model smaller which can make it more interpretable. \n\nHere are some thoughts to help improve the paper:\n\n*  How does the proposed approach compare with other static techniques like ALBERT, DistillBERT etc? Is the complexity of introducing an adaptive computation module worth the tradeoff in terms or accuracy or compute?\n* Why have you considered only three tasks from the GLUE benchmark? It would be good to report results for all the tasks. It could even be worth adding harder tasks from the SuperGLUE benchmark to see how to accuracy changes with your approach. \n* Some more analysis would be interesting. How does the adaptive computation change with examples in the dataset? Does the length of the input matter? What about different tasks? \n* How far can you push the savings? Is it possible to save ~90% of the computation without significant loss in accuracy?\n\nOverall, I think this work is interesting. However, the lack of comparison to appropriate baselines and a full suite of tasks make it hard to accept the paper. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting research problem, insufficient experiments to justify the claims",
            "review": "Summary: This paper proposes Differentiable Adaptive Computation Time BERT (DACT-BERT) for efficient inference with BERT. The approach equips the BERT model with halting classifiers after every transformer layer. The predictions of these halting classifiers are used to determine when to stop applying additional transformer layers and make a prediction, similar to the approach presented in [2] for machine translation. The training process involves fine-tuning a pre-trained BERT model in two stages: (i) Standard fine-tuning of the BERT model on the downstream task, followed by (ii) Jointly fine-tuning the DACT classifiers with the rest of the BERT stack. The DACT classifiers are trained on the end-to-end objective function for the downstream task and regularized with an ACT loss similar to [1].\n\nThe key difference between DACT-BERT and previous approaches that utilize ACT for BERT inference ([3,4,5,6]) is that DACT-BERT trains halting classifiers end-to-end with the model, instead of training them separately once the BERT parameters are frozen. The only other recent work that trains control classifiers end-to-end with BERT is [7], but they use a different conditional computation formulation.\n\nDACT-BERT is compared against DeeBERT [3] on the MNLI, QRPC and RTE tasks from the GLUE benchmark and the quality of the two models seems comparable. The authors also analyze the attention entropy distributions to demonstrate that the model learns to utilize the final layers adaptively for harder tasks.\n\nStrengths:\n1. The key problem being investigated in the paper is of broad interest. ACT and halting classifiers are very promising from an efficient inference perspective, and training them end-to-end with the model has the potential to improve performance over fine-tuning classifiers in a separate post-training phase.\n\nWeaknesses / Questions for authors:\n1. The empirical results lack a proper comparison against existing approaches and baselines. The results presented in Table 1 are hard to draw conclusions from given that the models being compared (DeeBERT and DACT-BERT) are not using an equivalent amount of computation at several data points. Using a wider range of thresholds for DeeBERT for these comparisons might help present a more complete picture. Additionally, comparison against distilled BERT models of varying sizes will further strengthen the results.\n2. The comparisons are presented on 3 tasks from the GLUE benchmark. It's not clear how well the results generalize to other NLU tasks.\n3. During training, the output of the DACT classifiers are treated like continuous variables, and the final classifier is applied to a soft combination of the outputs of all the layers ($a_N$). What is the exact formulation of the output fed into the classifier during inference? Is it a soft combination of the outputs of all layers before halting (i.e. is $a_n$ or $y_n$ used as the final output)?\n4. Given that the key difference of DACT-BERT from previous approaches is training halting classifiers jointly with the model, performing ablation experiments to demonstrate that joint training improves over separately fine-tuning the halting classifiers will go a long way towards supporting your claims.\n5. The attention entropy distributions suggest that the DACT-BERT might in fact be pruning away the higher order layers in the model. Did the authors compare against static pruning approaches like LayerDrop? [8]\n6. Missing References: [2,5,7,8]\n7. There are a few other typological and grammatical errors: (i) Section 1, line 4: finnetuning, (ii) Last paragraph before Section 4: adaptatively, (iii) Section 4.2, paragraph 3: sensible -> sensitive\n\nRecommendation: While the paper studies an interesting problem, the experiments presented in the paper are not sufficient to justify the claims. I would be willing to update the rating if experiments on additional NLU tasks and more comprehensive experiments including comparisons against additional baselines and proper ablations are added to the paper. However, without additional experiments the paper is not good enough for acceptance.\n\nReferences:\n\n[1] Adaptive Computation Time for Recurrent Neural Networks, Graves\n\n[2] Depth-Adaptive Transformer, Elbayad et al.\n\n[3] DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference, Xin et al.\n\n[4] FastBERT: a Self-distilling BERT with Adaptive Inference Time, Liu et al.\n\n[5] The Right Tool for the Job: Matching Model and Instance Complexities, Schwartz et al.\n\n[6] BERT Loses Patience: Fast and Robust Inference with Early Exit, Zhou et al.\n\n[7] Controlling Computation versus Quality for Neural Sequence Models, Bapna et al.\n\n[8] Reducing Transformer Depth on Demand with Structured Dropout, Fan et al.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}