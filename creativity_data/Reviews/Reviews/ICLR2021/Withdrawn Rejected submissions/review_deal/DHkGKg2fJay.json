{
    "Decision": "",
    "Reviews": [
        {
            "title": "A nice paper, but what happens with the experimental part",
            "review": "Partially labeled problem, also called ambiguously labeled problem,\nrefers to the task where each training example is associated a set of\ncandidate labels, while only one is assumed to be true. This paper\nintroduces a family of loss functions named Leveraged Weighted (LW)\nloss function. The family relies on \"the leverage parameter\" to\nleverage between losses on partial labels and residual labels\n(non-partial labels).\n\nThe paper is really interesting, the ideas are nice and overall well\nwritten until section 4 (experiments). Maybe I missed something but\nexperiments are not really meaningful and the conclusion is very\nsimilar to the abstract. Were the authors short in time ? The ideas of\nthe paper could be strengthened with tailored experiments. \n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unclear significance of theory results + cherry-picked experiments",
            "review": "In this paper, the authors propose a new family of loss functions for the partial label learning (PLL) setting where each data point has several candidate labels, including (by assumption) the true label. The proposed loss is the weighted sum of loss terms corresponding to the classifier's confidence on labels (weighted by $w_z$), in and not in the provided label set $\\textbf{y}$ with a weighting between these two groups called the \"leverage parameter\" $\\beta$. The authors establish a correspondence to a set of \"ordinary losses\". They then propose an iterative approach to learning the $w_z$ and true parameters inspired by prior work.  Finally they show empirical results compared to prior state of the art.\n\nHigh level comments:\n* (W1) The significance of the connection between the partial label losses and the \"ordinary\" losses in Thms 1 & 2 seems to be unclear- first, because these \"ordinary\" loss forms are still defined in terms of the parameters of the PLL setting, and also because it's unclear what conclusions this connection to this \"ordinary\" form can actually let us draw about the PLL loss.\n- To start, in Thm1, the \"ordinary loss\" is defined in terms of the partial label sets $\\textbf{y} \\in \\mathcal{Y}$, as an expectation over $P(\\textbf{y}|y, x)$ of the partial label loss... it's unclear at this point what this \"ordinary loss\" is and why we should care about it, because this does not seem like a standard non-PLL loss of any kind...\n- In Thm 2, eqn (5), there is a q_z term which expresses a probability of a label being in the partial label set- again, this loss is not an \"ordinary\" non-PLL loss, since it directly depends on parameters of the PLL setting...\n- In Corollary 1, we see three sub-cases considered:\n    - In the $\\beta = 0$ setting, there doesn't seem to be much we can conclude other than in a trivial setting\n    - In the $\\beta = 1$ setting, it's unclear if the stated conclusion is true if we also have to learn the $w_z$, which practically we always would?\n    - In the $\\beta = 2$ setting, it's again unclear what (under limited conditions) the connection to the OVA loss from the cited paper means, i.e. why we should be interested in this connection?\n- Overall: I am confused by the terminology \"ordinary loss\" and unclear what this theoretical exposition actually lets us conclude about the PLL loss\n\n* (W2) Table 1 shows very small to moderate gains over a single prior approach- but for arbitrarily picked $\\beta$ terms.  Were the best performing $\\beta$s on the test set picked?  This seems to be the case, in which case these are cherry-picked and don't let us conclude anything.\n- Additionally, given the breadth of other approaches covered in the beginning, more methods should have been compared to",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "OK paper, but link b/W theory and practice not completely clear.",
            "review": "This paper proposes a new loss to learn from partial labels, that generalises into a parametric form a number of other losses proposed in the literature. Some results connect the used loss to a corresponding loss over the actual, true precise labels, showing under which conditions it is consistent. The proposed loss appear interesting, as it generalises a number of other proposals, and it provides good performances during learning. \n\nI have a number of comments and questions on the paper, that maybe the authors can discuss/clarify in subsequent iterations:\n\n* It is not clear to me to which extent Theorem 1 and 2 actually provide guidelines to the practical choice of a loss function over partial labels. More precisely, both theorems and ordinary loss need to estimate the \"coarsening\" process leading to partial losses in the form of q_z (or q_s and q_t) quantities, which cannot be estimated from data. So their connection with the actual practical proposal and the estimation of weights w_z is actually a bit blurry. Could some additional elements be given? \n\n* P5, Section 2.4.: \"our assumptions are much weaker and closer to the reality\". The first item seems a bit unclear, as the assumptions become as strong as the other once a value of Beta is chosen. A more accurate wording would probably say that the model is more general (whether it makes weaker/stronger assumption is debatable, as once it is instanciated, it makes as much assumptions as the others). Similarly, since Beta is not estimated but chosen, how does it make the model \"closer to reality\" and what is meant by reality here? \n\n* Basically, the model is adjusted by re-estimating the weights w_z associated to instances, however I miss a connection between those weights and the corresponding coarsening models (basically summarised here by q_s, q_t, q_z probabilities)? While EM-like approaches typically try to estimate the observational process in their disambiguation, with some conditions under which such an estimate can be statistically consistent, I miss here some connections between the learning process and the identification of the probabilities q_s, q_t, q_z. This echoes my first comment, as one would expect some connection between the estimated w_z and the two proposed theorems. \n\nMinor comments and typos:\n=========================\n\n- P1: \"as follows are our contributions\" --> \"our contributions are as follows\"\n- P1: \"is introduce\" --> \"is introduced\"\n- P4: In equation (1), \\psi is supposed to be a function of two arguments, but takes only one. \n- P5: \"failed to\" --> \"do not\"?\n- P5: \"that consistent in risk\" --> \"that are consistent in risk\"?\n- P6: \"to conquer this problem\" --> \"to solve this problem\"?\n- p7: \"we accept the occasion that\"\n- P7: \"with $\\beta$ with\"\n- Appendix, P1, proof 1: q_z in last line should be q_y\n- Appendix, P2: this makes two lemma 1, as there is another lemma 1 in the paper. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new loss for partial label learning",
            "review": "This paper discusses the setting of partial label learning, where supervision arises in the form of a set of candidate class labels instead of a single label. For this setting, the authors make some assumptions w.r.t. independence of labels, resulting in several theoretical results that connect partial label loss functions with more traditional multi-class classification loss functions. In addition, a novel algorithm for a specific partial label loss is introduced, and experimentally compared to existing algorithms on four classical benchmark datasets. \n\nBefore giving comments, let me say that I am not an expert on the topic of partial label learning, and I am not familiar with the existing literature, so my review report should have a low score in the overall decision for this paper. \n\nThe paper contains quite a lot of typos and incorrect sentences, which make it sometimes hard to follow. As a reader you also need to digest quite a lot of notations, so reading this paper takes time.  However, the setting is formally introduced in a correct way, so that's a positive point. In Section 2.3 a connection is made to the existing literature. That's also positive, because the developed theory departs from general loss functions. \n\nLemma 1 and Theorems 1 and 2 automatically follow from Assumptions 1 to 3, so the theoretical results are not very surprising. Assumptions 1 and 2 come from the literature, because references are given, whereas Assumption 3 is proposed by the authors. I am not so convinced of this third assumption, because I don't think it holds in practice. The assumption says that the two decisions to put two classes as plausible labels in a bag are independent decisions if those labels are incorrect. However, in many practical scenarios those decisions won't be made independent of each other. For example, in object detection, a labeler might not be sure which animal is present in an image. He/she will most like choose a set of candidate labels among animals that look similar. If the animal is a bird, the labeler might give different types of birds as candidate labels, but it is unlikely that the class \"dog\" will be in the bag. So, different types of birds will often end up together in a label bag. \n\nThe theoretical analysis is very general, but the algorithm presented in Section 3 is developed for a specific loss function. In the experiments this algorithm is compared to some baselines on four datasets where partially labeled data is created in an artificial way. I found these results a bit underwhelming because I was missing a real-world application (although I have to admit that other papers in this area also create partially labeled data in an artificial way). \n\nOverall I think that the impact of this work is not so high, because Assumption 3 is not realistic, the presented algorithm is very specific and the work is missing a real-world application. \n\n\n\n\n\n  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}