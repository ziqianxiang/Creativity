{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a variational family of distributions for posterior estimation in sequential latent variable models. The paper does so by extending variational recurrent neural networks so as to use a variational-mixture posterior and capture more realistic multi-modalities in the data. \n\nDuring the review process, it was suggested to improve the clarity of the paper, provide results on an additional dataset and a visualization of the latent distributions. I commend the authors for addressing these issues satisfactorily.\n\nOverall, the paper is well-motivated and well-written. However, when considering the novelty of the paper, although none of the reviewers raised this issue, I believe the paper heavily relies on previously proposed ideas and therefore, its contribution can be seen as incremental. Additionally, something important to highlight is in section 2, with regards to deep state-space models (SSMs). The authors make a rather strong claim with regards to assumptions on the variational distribution. However, one can find out-of-the-box implementations where this is not the case, see e.g. https://pyro.ai/examples/dmm.html that implements deep Markov models with posteriors based on inverse autoregressive flows. A comparison with such approaches may be also required. \n"
    },
    "Reviews": [
        {
            "title": "Good paper on interesting topic, some clarifications needed.",
            "review": "**Variational dynamic mixtures**\n\nThe paper extends the VRNN to cater for multi-modality in the probability distribution governing a dynamic system. This is particularly important when average trajectories are highly unlikely or even physically impossible. To achieve this, the authors start from VRNN and alter the inference model so that it uses stochastic recurrent states and a mixture variational posterior distribution (with 0/1 weights to trigger only the most likely mixture components encouraging multi-modality).\nAs a minor contribution they propose a new evaluation metric for measuring the diversity of generations based on Wasserstein distance. This is important in their case when the likelihood evaluation may favour generations from a single mode - a situation their model shall prevent.\n\nThe paper is very well motivated (with taxi trajectory prediction as a running use case) and well positioned with respect to the state of the art.\nThe experimental evaluation is convincing, using both synthetic and real experiments to support the claims and shows advantages over baselines. Ablation studies examine the importance of some more ad-hoc choices (showing these help but are not critical).\nThe paper is well written and structured to help the reader follow the main thoughts.\nHowever, there are some points in the mathematical formulation of the model which raise questions and deserve to be explained better - see below.\n\nFor this reason I recommend not accepting the paper for now but I'm am very much willing to improve my score significantly once these will have been clarified.\n\n* From Fig2 it seems that the stochastic states s_t do not exist in the generative model, they only live in the inference model. Right?\n\n* From eq. (1) and Fig2a we have $h_{t} = \\phi^{GRU}(z_{t}, h_{t-1})$ and the generative distribution of $p(z_{t+1} | z_{\\leq t})$ is a function $\\phi^{tra}(h_{t})$. Is it correct to think about this as the prior distribution for $z_{t+1}$?\n\n* My understanding of eq. (3) is that we have $\\widetilde{q}(s_{t-1} | x_{<t}) = 1$ if $s_{t-1} = h_{t-1}$ and zero otherwise. Is that right?\n\n* You say $q(s_{t-1} | x_{\\leq t}) = \\omega(s_{t-1}, x_t) \\, \\widetilde{q}(s_{t-1} | x_{<t})$.\n    * How can you condition $s_{t-1}$ on the future value $x_t$? (Also in comparison to equation (4), where you assume $q(z_t | x_{\\leq T}) = q(z_t | x_{\\leq t})$ for all $t$ thus avoiding dependence on future values of $x$. \n    * If my understanding of eq (3) is correct (see above), this is trivially 1 or 0 irrespective of $\\omega$.\n\n* In eq (4) you say $q(z_{1:T} | x_{1:T}) = \\Pi_{t=1}^T q(z_{t} | x_{\\leq t})$.  Is this correct? I would expect $\\Pi_{t=1}^T q(z_{t} | z_{\\lt t}, x_{\\leq t})$.\n\n* Again in eq (4) you say the final result is $\\Pi_{t=1}^T \\int q(z_{t} | s_{t-1}, x_{t}) q(s_{t-1} | x_{\\leq t}) d s_{t-1}$. Is this correct? I would expect $\\Pi_{t=1}^T \\int q(z_{t} | s_{t-1}, \\mathbf{x_{\\leq t}}) q(s_{t-1} | x_{\\leq t}) d s_{t-1}$. Does this mean you assume the conditional independence $q(z_{t} | s_{t-1}, \\mathbf{x_{\\leq t}}) = q(z_{t} | s_{t-1}, \\mathbf{x_{t}})$?\n\n* samples $s_{t-1}^{(i)}$ in eq (5) are constructed by sampling $z_{t-1}^{(i)} \\sim q(z_{t-1} | x_{<t})$ and passing it through the recurrent net $s_{t-1}^{(i)} \\gets h_{t-1}^{(i)} = \\phi^{GRU}(z_{t-1}^{(i)}, h_{t-2})$, right?\n\n* eq (5) and annex A: How come $\\omega$ is a function of $x_t$ only and not of $x_{<t}$. Given your importance sampling argument, $q$ conditions on $x_{<t}$ as well. Or do you assume the conditional indpepence $q(x_t | s_{t-1}, x_{<t}) = q(x_t | s_{t-1})$?\n\n* what is $p(x_t | s_{t-1})$ used in eq (6)? This looks like a generative distribution ($p$) but has not been defined before and $s_{t-1}$ do not exist in the generative model (figure 1a).\n\n* ELBO proof in annex B: can you please provide details (equations, detailing also the conditional independence assumptions you take) for getting from eq (7) to eq (8)?\n\n* Wasserstein distance in eq (14) - the differences are calculated between the generated trajectories and the corresponding true sample (the *group sample*) or all the n true samples?\n\nAFTER REVIEW UPDATE: I find the revised version much improved explaining the inference model much more clearly. The lack of clarity was for me the main reason for evaluating the paper as below the acceptance threshold despite the fact that otherwise I found the paper to be good and useful for the community. As the lack of clarity has now been, in my view, resolved, I increase my score to 7 - Good paper, accept.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New inference model for capturing multi-modal dynamics",
            "review": "Summary\n\nThis paper introduces variational dynamic mixtures (VDM), a new variational family, and demonstrates that using VDM to model the approximate posterior in sequential latent variable models can better capture multi-modality in data. VDM includes a distribution over recurrent states in the inference model, such that a sampling-based marginalization of this distribution reduces the approximate posterior to a mixture model. Setting the weights such that only the most probable mixture component is selected allows other mixture components to capture other modes. The authors validate VDM on both synthetic and real multimodal datasets, which outperform baselines with respect to negative log-likelihood and a new empirical Wasserstein distance.\n\nPositives\n\n+ This paper tackles an important and well-motivated problem: capturing multi-modality in data. This is very practical and I believe will be of interest to the ICLR community. \n\n+ Paper is well-written and easy to follow. I appreciate that the authors highlight their design decisions in the main text, while also providing alternatives and/or ablation studies in the appendix.\n\n+ VDM improves performance while also using a non-autoregressive generative model, compared to baselines with more powerful generative models (e.g. VRNN). This highlights the effectiveness of their inference model, also illustrated in the synthetic experiment in Figure 3. The inference model in VDM is also quite general, as a single-sample approximation in their inference model is equivalent to the inference model in VRNN.\n\nConcerns \n\n- I think this paper would benefit from one additional dataset where the multimodality is inherent in the data. Taxi trajectories are multimodal but also highly structured (trajectories must be on roads), so a different dataset you can consider can be pedestrian or sport trajectory datasets, where the data is also inherently multimodal but also less structured. I think some baseline models can do better in this setting (at least qualitatively), so I’m curious if VDM still convincingly outperforms them. My main concern is that the pollution data is synthetically multimodal (because I think that with contextual information the data is more periodic) and the Lorenz attractor experiment only highlights that VDM can handle stochasticity, which is also present in trajectory data.\n\n- The related work is missing a section about other methods that try to capture multimodality. For instance, VRNNs are known to not capture multimodality well, and there have been extensions along this direction such as in [1]. There’s also another line of work that introduces mutual information between trajectories and latent variables in the objective, such as in [2]. The “sequence forecasting” paragraph can be omitted/combined with “neural recurrent models”.\n\n- The results on the taxi dataset look good. It would be great if you can also provide analysis on the resulting latent space, similar to what was done in Figure 3.\n\n[1] Goyal et al. Z-Forcing: Training Stochastic Recurrent Networks \n\n[2] Li et al. InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations\n\nMinor Comments\n\n- The bold/thin lines in Figure 2 can be hard to distinguish. I recommend using a dotted line instead.\n\n- In Figure 3, how many timesteps are there in total? Are the blue/orange trajectories in the left plots corresponding to the blue/orange clusters in the middle/right plots?\n\n- Tables 1,2 and 3 all cut through paragraphs in the middle, which can be distracting.\n\n- Some quotations on page 5 use close quotations ” on both sides.\n\n---------\n\nPost-rebuttal comments\n\nThank you for adding the additional experiments and analysis. The results with the basketball dataset (Table 4, Figure 6) and the visualization of the latent distribution (Figure 5) address my inital concerns and showcase the versatility of VDM. I've increased my score from 6 to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A deep probabilistic model for time series forecasting is proposed.  Detailed description for mathematics is required.",
            "review": "This paper presented the variational dynamic mixtures as a deep probabilistic model for time series forecasting. The research issue, called the taxi trajectory prediction problem, is addressed. Some comments are provided.\n\nPros:\nA new solution to mixture density network as a kind of generative model with latent states and multinomial observations was proposed. The detailed experiments were addressed. New evaluation metric was introduced.\n\nCons:\nThere are a number of notations and variables which were not clearly defined. This matter made the reading to be easily confused. A clear algorithm or working flow for complicated system was missing. Some descriptions were not clear.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}