{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed a new optimization framework for pruning CNNs considering coupling between channels in the neighboring layers. Two reviewers suggested acceptance and two did rejection. The main concerns of the negative reviewers are (a) limited novelty, (b) limited performance metrics and (c) limited baselines. The authors' response did not fully clarify the reviewers' concerns during the discussion phase, and AC also agrees that they should be resolved to meet the high standard of ICLR. Hence, AC recommend rejection.\n\nHere is additional thought from AC. The authors propose ours-c and ours-cs. The latter is reported to outperform the former in terms of FLOPs, but AC thinks the former may have merits in other more important performance metrics, e.g., the actual latency and/or memory consumption on a target device. More discussions and results for this would strengthen the paper."
    },
    "Reviews": [
        {
            "title": "Writing is good but with limited novelty.",
            "review": "This paper mainly improves the idea of \"PRUNING FILTERS FOR EFFICIENT CONVNETS\" by encouraging the pruning with a {0-1} optimization instead of a greedy manner. Experiments validate the effectiveness of the proposed method. \n\nPros: \n+ Writing is good, and the technical details seem sound and clear.\n+ The motivation makes sense.\n\nCons:\n- The novelty is limited. The formulation of the 0-1 optimization for pruning is simple and intuitive. Concretely, it leverages the pre-trained weights for the unpruned network and tries to select the kernels with the maximum magnitude. For me, I am not sure whether the novelty is up to the standard of ICLR venue. \n- The objective is to maximize the norm of selected filters. However, magnitude-based pruning is already challenged for it is not accurate to indicate the selection. \n- Authors claim that current pruning papers can not reach a strict constraint for FLOPs during pruning. However, it is not true for recent pruning methods, such as AutoSlim, TAS and MetaPruning. Necessary discussions are needed. \n- Pruning on recent compact networks is favoured, such as MobileNetV2, which is also a routine network for many pruning papers. \n\n[ICLR2017] PRUNING FILTERS FOR EFFICIENT CONVNETS  \n[2019] AutoSlim: Towards One-Shot Architecture Search for Channel Numbers\n[ICCV2019] MetaPruning- Meta Learning for Automatic Neural Network Channel Pruning.pdf\n[NIPS2019] Network Pruning via Transformable Architecture Search",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper introduces an optimization method for pruning channels in networks. The authors first motivated the proposed approach by showing that current pruning methods will result in \"inactive weights\" for the following layer.  Then the authors introduce a QCQP optimization method that can constrain the exact amout of resources during the optimization process. Extensive experiments are conducted on different benchmarks with different backbones. And the authors also performed spatial pruning to further reduce resource usage.\n\n\n####### Strengths######\n+ The motivation is clear and the presentation is generally good.\n+ The  idea of mitigating the effect of inactive weights is interesting.\n+ Extensive studies have been conducted in terms as different datasets/backbones.\n\n####### Weakness######\n- The term \"the inherent quadratic coupling\" used in the abstract is a bit confusing without any explannations.\n- I didn't quite follow section 2.2, where the authors discussed the quadratic coupling effect. In figure 2, I understand the prunned channels for the greedy part. But I don't quite get how the proposed approach is able to prune the last channel of the 2nd layer. It would be nice to discuss this when the optimization is introduced?\n- Following the previous point, the authors basically are saying QCQP is better than greedy pruning. I couldn't find experimentd comparing these two methods. I mean I understand previous methods are greey based. But it would be nice to have the same implementation by the authors for apple-to-apple comparisons.\n- Missing references\n[1]Rethinking the Value of Network Pruning\n[2] Channel Gating Neural Networks\n[3] Dynamic Channel Pruning: Feature Boosting and Suppression\n\n\n##############Post Rebuttal###############\n\nMy concerns are addressed by the authors. I'm keeping my original rating.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good motivation and more metrics should be considered",
            "review": "Summaryï¼š\nIn this manuscript, a new pruning method is proposed by considering the inherent quadratic constraint between consecutive layers. Without this constraint, inactive weights cannot be safely removed. Even with the same objective function, the optimized result is different, as shown in the motivation section. Based on this observation, the pruning task is models as a QCQP optimization problem. And a faster algorithm to solve this problem is proposed. Moreover, the pruning on filter size can also be modeled as the QCQP problem, making the pruning on both channel and filter size feasible.\n\nStrengths:\n-\tThe paper is well-written and well-motivated. The motivation is reasonable and the proposed method does alleviate the overlooked issue. \n-\tThe results on the CIFAR10 and ImageNet surpass some previous methods.\n-\tThe proposed method does not need iterative pruning procedure as other methods, making it simple to use.\n-\tThe proposed motivation may inspire following works in this area.\n\nWeaknesses:\n-\tI am not an expert in the area of pruning. I think this motivation is quite good but the results seem to be less impressive. Moreover, I believe the results should be evaluated from more aspects, e.g., the actual latency on target device, the memory consumption during the inference time and the actual network size.\n-\tThe performance is only compared with few methods. And the proposed is not consistently better than other methods. For those inferior results, some analysis should be provided since the results violate the motivation.\n\nI am willing to change my rating according to the feedback from authors and the comments from other reviewers.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good submission focusing on a valuable topic",
            "review": "The authors proposed a pruning method that aims to reduce the parameters and heavy computational cost of large convolutional neural networks (CNNs). According to the comparison experiments performed on several widely-used network fashions, the proposed strategy could help to efficiently reduce the numbers of parameters while maintaining less performance decreasing. I think this study is valuable in both theory and applications. \nHowever, several issues may be further emphasized to make the submission improved:\n\n(1) Were the CNN models deployed in the real resource-constrained environment such as automatic drive hardware with less computational capability? Or could this proposed strategy be applied to reduce the parameters of the CNN models to the level of MobileNet?\n\n(2) Authors determined that the proposed method is useful to tackle several classification tasks, did this method also perform well on the CNN models aimed at segmentation, detection et al. Performance of this kind of models may decrease more than the classification. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}