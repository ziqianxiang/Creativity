{
    "Decision": "",
    "Reviews": [
        {
            "title": "Main contributions not clear enough.",
            "review": "Summary: This paper proposes a budget-aware BO method for HPO. The authors propose to use an additional GP model to fit the error term that takes into account budgets. New initialization schemes are also proposed based on optimal design that are proved to have outperformed random initialization.\n\nPros:\n- The problem formulation and hypotheses that involve budgets in the GP model make sense (even though I am a little bit skeptical about the motivation in practice).\n- The new optimal initial design has been carefully described and is shown to outperform random initialization.\n\nReasons for the score:\nDespite the strong points I have listed, I have several concerns which lead to my current score. The major reason is that the real contributions of the paper are not clearly stated in my opinion. For me the only contribution that has been more or less sufficiently grounded is the new initial design. Meanwhile, the benefits of involving the budgets are not that clear. In Section 4.2 and 4.3, the authors only compared GPB to classical GP with random initialization. If the authors claim to propose a good HPO algorithm, then it would probably be useful to compare to other SOTA algorithms other than BO. In particular, I think it is relevant to compare, in a certain way, against multi-fidelity methods (BO or not) that use budgets as the measure of fidelity (or at least a discussion should have been provided). Then, if the authors want to emphasize more on the initial design part, than it would probably be appropriate to compare with other initialization methods for BO. I'm not familiar with the literature on that part, so it would be nice if the authors could provide some discussions on that. I would probably increase the score if the authors could deliver a more clear message on the contributions. \n\nOther technical questions and remarks:\n- Could you please elaborate a bit more on the Fréchet derivative when we consider the M-optimality? Precisely, what does d(x,ξ) look like when \\ell = \\infty?\n- In Section 4.1: \n  - What is h in the objective function?\n  - I don't really understand what do b = 1, b = 2/3, b = 0.5 represent, could you please explain a bit more?\n  - Does the choice of N = 6 affect a lot the performance? For that, I know that the authors have provided some results with N = 10, 20 in Table 2. However there are two problems here, first of all, what did you actually report in Table 2? I guess it's the integral loss with some chosen b, but it is written nowhere, and we don't know which b you have chosen. Second, I would suggest plotting some figures that report the evolution of the integral loss in terms of different N, N from 1 to 20 for example.\n\nMinors:\n- Section 1 paragraph 3: It seems that the acronym HPO first appears here, it would probably be better to mention that it means Hyper-parameter Optimization.\n- Section 1 paragraph 4: The papers cited for the fourth aspect, namely multi-fidelity methods, are mostly not BO methods. I understand the point, but it's a bit misleading I think.\n- Section 2 paragraph 1: \"Instead, we observe y(x) = f(x) + ε, where ε is a noise and follows N(0, σ^2_noise) (Falkner et al., 2018)\" <- I don't understand why you would cite a particular paper here, it's just a very common formulation for black-box optimization and Falkner et al., 2018 is clearly not the first paper to use that formulation.\n- Section 2, in the standard algorithmic procedure of BO, step 2: What is \\mathcal{B}? I don't think it has been ever defined before.\n- Section 4.1, paragraph 1: Algorithm 2 outperforms other designs since its uniformity <- 'since' here is used as a conjunction, thus cannot be followed directly by a noun I think, maybe use 'due to' instead.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Useful and significant method to allocate budgets to hyperparameter configuration with GPs",
            "review": "Nice paper regarding a regression on budgets of configurations in HPO problems. Theoretical and empirical work. The methodology is valid and sound and I think that is an useful method. Although, the paper has some issues and clarity is not its best point in my opinion. Nevertheless, I give observations to help with this point. Novelty is not the best feature as it is a small incremental contribution to the BO framework. Despite this, it is significant and useful, as it is not very complex and can be understood easily. I also have some questions that can be found in this review. I would recommend acceptance of this paper if my questions are answered and the comments and observations are covered by the authors, specially, concerning the results of the empirical work. Right now and until I read the response letter, I cannot recommend this paper for acceptance until my observations are answered by authors and I see the opinion of other reviewers.\n\nObservations, questions and comments:\n\nAbstract:\nI have to read the abstract 2 or 3 times to understand what is the paper offering. I do not think that the idea is clear at all. The conclusion is better exposed. I would suggest to add phrases like \"automatically set the evaluation resources for each hyperparameter configuration and take the variance caused by different budgets into account.\" The idea is clear them. It would be a pity that some interested reader skips this paper as it does not found it interesting because of the abstract. \n\nIntro:\n-> The intro is too focused on ML and AutoML, BO has much more applications, I would give a pair of citations talking about them. \n-> \"machine compute time\" -> \"machine computation time\" or \"computation time\".\n-> Formalization of the BO problem : put \\boldsymbol{} to the x's.\n-> I disagree with the four aspects for improving BO methods with a given search space. The first major abscence is to avoid myopic Bayesian optimization. Changing BO from the myopic to the non-myopic scheme (as far as possible, not benefiting for immediate regret but to the regret in n iterations or throughout all the process) is a critical aspect [1].\n-> I would also add enabling parallel evaluation of configurations in an iteration. Multiple papers addresses this, being parallel PES maybe the most formal way to do it in my opinion. [2,3]\n-> Formalize the fidelities paragraph after the list of BO aspects with some equations.\n-> \"A theorem for judge whether a design\" sounds bad.\n\n2. Gaussian process with budgets.\n\n-> Properly define b. \n-> Why do you fit the error with a GP and not with another prob. surrogate model (like random forests)? What are the assumptions that the GP encodes about the error?\n-> More importantly, do g(x) and h(x,b) have dependencies? Because you model them using independent GPs... I would think that costly configurations will be correlated with good results... Would a multi-output GP obtain better results here modelling possible dependencies between the functions?\n-> Please, put vectors in bold to differentiate them from escalars in equations, it gives them clarity, now it is a bit difficult to interpret them.\n-> \"However, in our cases, we just need to measure the correlation between two configurations with this RBF kernel\". Why RBF and not Matern for example? Please justify these decisions with arguments.\n-> Put the list of \"The standard algorithmic procedure of BO is stated as follows.\" in an algorithm Latex environment not in a list.\n\nSection 3.1 -> OK.\n\nSection 3.2\n-> Jump from section 3.1 to 3.2 is to abrupt. Please provide cohesion with an introductory paragraph, it is very technical right now and difficult to follow if you are not an expert in the particular methodology. \n-> \"This approximate design\" sounds bad.\n\nSection 3.3\n-> The \"é\" of Frechet appears to be in another typography.\n\nSection 3.4\n-> \"this optimal design guarantees to have models with minimum variance\" Reference proof or arguments here.\n-> \"For the construction of LHD, threshold algorithm\" -> a threshold algorithm or the threshold algorithm (with reference).\n-> \" threshold algorithm is used because the main step of the construction is permutation, which can be viewed as integer programming.\" develop this idea.\n-> Why have you used a derived simulated annealing and not other opt. technique? Could we apply here other opt. technique such that it can be a hyperparameter of your methodology?\n\n\nSection 4.\n-> IMPORTANT: Regarding results: It seems that the methodology does not provide a significant enhancement wrt random in real experiments. How do you justify these results? For me, they have been a bit disappointing, but I would like to see your explanations about them.  Specially concerning the cifar experiment.\n-> \"For more applications, auto data augment and neural architecture search (NAS) are applied.\" Applied is redundant. I would put are considered.\n-> Regarding tables: please put the best result in bold to improve clarity.\n-> Why do you employ a SGD optimizer?\n\n\n\nReferences:\n[1] González, J., Osborne, M., & Lawrence, N. (2016, May). GLASSES: Relieving the myopia of Bayesian optimisation. In Artificial Intelligence and Statistics (pp. 790-799).\n[2] Shah, A., & Ghahramani, Z. (2015). Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems (pp. 3330-3338).\n[3] Garrido-Merchán, E. C., & Hernández-Lobato, D. (2020). Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints. arXiv preprint arXiv:2004.00601.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Reject",
            "review": "Summary\n=========\n\nThis work proposed 1) a GP model that incorporate budget, thus recommending budget during the BO; 2) another way for recommending initial hyperparameters based on minimizing on the variance on two unknown parameters and 3) empirical evaluations of the proposed methods.\n\nReason for score\n==============\nI found the idea of choosing initial design (hyperparameters) by minimizing the variance of parameters quite interesting. But I am not sure I understand the method due to lack of clarity. In addition,  I think this work ignores many previous work and lacks of rigor. I will give more detailed comments below.\n\nPros\n====\nI found the idea of choosing initial design (hyperparameters) by minimizing the variance of parameters quite interesting. I hope the author can give more motivations, explanations and study more the property of the proposed condition.\n\nCons\n====\n\n1. Lack of coverage of prior works. There are already several previous works trying to suggest the budget at the same time with hyperparameters. For example the following:\n\n* Section 3.1 in Klein, A., Falkner, S., Bartels, S., Hennig, P. & Hutter, F.. (2017). Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, in PMLR 54:528-536\n\n* Klein A, Tiao L C, Lienart T, et al. Model-based Asynchronous Hyperparameter and Neural Architecture Search[J]. arXiv e-prints, 2020: arXiv: 2003.10865. (not recommending budget but use a fixed schedule)\n\n* Falkner, Stefan, Aaron Klein, and Frank Hutter. \"BOHB: Robust and efficient hyperparameter optimization at scale.\" arXiv preprint arXiv:1807.01774 (2018). (not recommending budget but use a fixed schedule)\n\nNotice, even in the last two works are not suggesting budget, but if the fixed schedule is enough to have good performance, why we need to have dynamic one?\n\n* The author mentioned \"We introduce Latin hypercube design (LHD)\" in the end of Section 3, but for me it is well known in the BO community that start with low discrepancy sequences is beneficial. For example, Section 4 of the following paper:\n\nBergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. The Journal of Machine Learning Research, 2012, 13(1): 281-305\n\n2. Lack of clarify. I will list several questions I had during the review:\n\n* In the beginning of Section 2, the author introduced G (not small g), H (not small h), f1 f2, beta1 and beta2; what are those and what's the role of them?\n\n* What is the beta in Section 4.1? For the results in Table 1&2, is it only after he initial design are evaluated or there are some continuation in the BO phase? If yes, how many BO iteration are used?\n\n* For the experiments at Section 4.2 and 4.3, where is the switching point between initial design and BO? If GBP suggest budget dynamically, why \"The GPB model is run with alternative budgets of 1, 10, 20, 30, 40, 50 epochs\"?\n\n3. Lack of rigor. The effectiveness of the methods can not be judged from the experiments.\n\n* The acquisition function is standard EI and it can not be counted as a contribution.\n\n* The proposed the GPB model is not compared to the previous work that I mentioned above. \n\n* The experiments setting in Section 4.2 and 4.3 are hard to believe. Picking the best from 20 or 10 configurations seems unrealistic for me.\n\n* If the TA method is based on Latin Hypercube (LH), should the comparison at least include LH? And if the author want to show the usefulness of this initial design, I would suggest to start with standard BO setting (can be any open source packages such as scikit-optimize/spearmint etc) and only change the initial design, instead of coupling the initial design to GPB.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}