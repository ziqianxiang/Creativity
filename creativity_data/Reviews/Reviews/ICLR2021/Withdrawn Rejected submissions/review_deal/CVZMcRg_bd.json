{
    "Decision": "",
    "Reviews": [
        {
            "title": "No clear advantage, trivial theoretically",
            "review": "The paper proposes a method to divide a high-dimensional space into a set of disjoint sub-spaces, and to approximate a target function as a polynomial of a predetermined degree in each subspace. The approximation is  done through interpolation. The polynomial coefficients are linear functions of the training data. The paper outlines experiments with the aims of signal denoising and image segmentation which rely on the proposed technique.\n\nThe paper has significant problems with notation. First of all, a function cannot belong to the real space of dimension n, as written in the definition 1. Next, it is never written that [n] denotes a set of natural numbers from 1 to n. The forward pass, defined right after definition 1, is formulated incorrectly: the authors subtract a vector from a function. I assume, they were willing to write summation over x, and h(x) instead of h. However, it is only my guess. The space {\\cal F}_d^k is undefined.\nEssentially, as far as I understand, the authors propose to approximate a function as a sum of functions defined on the subspaces obtained by selecting certain indices of coordinates from the initial space. For example, one can select certain pixels of an image, and define a function as a sum of functions on disjoint sets of pixels. The authors consider an example of applying their technique to image segmentation. However, it is unclear where does the division of the image into the disjoint set of pixel groups come from? If it comes from ground truth, then the proposed technique is simply averaging the pixel-wise predictions using the ground-truth segmentation. In this experiment, it is not clearly explained, how was the baseline trained.\nAnyway, as far as I see it, the paper provides a rather trivial technique and does not report any practical improvement in relevant metrics for the considered problems, e.g. image segmentation. Moreover, theoretically the proposed technique looks trivial. Mathematical notation is incorrect in several places.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but insufficient evaluation",
            "review": "This paper introduces a method for differentiating through the solutions to a piecewise polynomial approximation of a vector. Results are shown for a 1D signal denoising task, as well as image segmentation. \n\nPiecewise polynomial fits seem like a potentially useful operation to have in the differentiable optimization toolbox. The expressions given for the Jacobian are also highly efficient, making them an easy thing to add to new architectures. The experimental results show some benefits to using the new layer. \n\nMy main concern is the need for more rigorous experimental evaluation. \n\n-For the denoising task, it seems like the inputs are generated from a distribution which obeys exactly the kind of piecewise structure which the layer enforces, so it is not surprising that it provides some improvement. What happens when the input signal is not exactly piecewise?\n\n-Are any quantitative results available for the denoising task? The example images show small visual improvements, but it's important to show the actual accuracy on the test set. \n\n-For image denoising, the improvements over the base convnet are extremely small (and the proposed model is actually outperformed by the baseline, by a very very small margin, in one of the two settings). The purported improvement in the visual quality of the segmentation is possibly present, but really not easy to make out.\n\n-What does it meant to use the new layer as a regularizer?\n\nI would also suggest to the authors to clarify the presentation of what a piecewise polynomial function is, in this context. It's not clear that they mean a polynomial function of the set of indices 1...n, which leads to all sorts of confusion about why a function is a point in R^n, etc. I saw that one of the other reviewers had asked a question about this, and seeing the answered helped me but it still took a little bit to figure out what was going on (the examples in the experiments made it clear). ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review --- Differentiable Programming for Piecewise Polynomial Functions ",
            "review": "The authors are attacking the problem of differentiable programming for piecewise polynomial functions. The main contribution of the paper is the novel approach for computing the  weak Jacobians for piecewise polynomial functions,  \n\nThe authors claims that they are extending gradient based methods to piecewise smooth models. However, by definition, a piecewise smooth function is continuous, then we can naturally apply sub-gradient based methods. Thus why do we need the more complicated approach in the paper?\n\n\nThe notation is hard to understand. In Def.  1, a piecewise polynomial is defined as the sum of a series of polynomials, and the is very strange, since sum of polynomials is still polynomial.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A hard to read article, with too light experiments",
            "review": "Summary\n\nThe authors consider the approximation of a 1-d signal x and derive the Jacobian of the function that maps x to a piecewise polynomial approximation. This jacobian has a convenient block sparse structure which allows for fast computations. Then, the authors use the polynomial approximation as a layer in a machine learning pipeline, and show that it helps in denoising tasks.\n\nMajor comments\n\n- The article is hard to comprehend, mostly because the notations and definitions are not rigorous. For instance, the first definition of piecewise polynomial is hard to understand, because the definition of the $h_{B_i}$ is not given. The definition of a “partition of [n]” should also be given, since I believe that here the authors do not use the usual definition (in the standard definition, ({1, 3}, {2}) is a partition of [3]). In the first formula in page.4, what is ‘i’? We assume that it is the index such that $j\\in B_i$, but it is not specified. In the same page, the definition $t = [0, 1,\\dots, |B_i| -1]^{\\top}$ contradicts the immediate next explanation that it indexes the elements in $B_i$, and the fact that $t$ is indexed by $j\\in B_i$: if B_i = {2, 3}, then $t_3$ should be defined.\n\n- The experiments are poorly explained. Many important information are deferred to the appendix, e.g. “Denoting z as a Gaussian noise vector as input and $G_{\\theta} as a generative model…”: what is $G$ ? how is it trained? In the figures, the authors mention that the other method is “straight-through gradient backpropagation”, what does it mean? Since both backpropagation and the chain rule applied by hand with the Jacobian are exact methods, why do they lead to different results?\n\n- The experiments evaluation is mostly based on visual inspection,  only one experiment shows quantitative results. This is not enough to claim that the proposed method leads to superior performances.\n\n\nMinor comments\n\n- The authors propose a novel algorithmic method, yet do not provide code for reproducing their results.\n\nMisc\n\n- In def.1 it should be $\\mathcal{F}^k_d$.\n- “We conduct four different generators”: this should be rephrased.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}