{
    "Decision": "",
    "Reviews": [
        {
            "title": "Causal discovery using visible and invisible factors",
            "review": "The paper deals with causal discovery in the clinical setting to aid explainability and human understanding. The authors present an eps-ICP algorithm to handle both visible and invisible variables. They also integrate this into a vision-related task. \nThis is extremely important in the setting of randomized controlled trials to investigate the causal dependence between treatment and recovery.\n\nThe paper is written clearly and the motivations are clearly stated. Prior literature is also discussed in detail.\n\nComments:\n\n*In Section 5, and Table 2, it seems that eps-ICP(GT) performs better than eps-ICP in most comparisons. It will be worth discussing why this is the case and how the proposed model can be made to perform better than models with ground truth. In many medical settings, ground truth is not available or costly to obtain and so it is crucial to show better performance in the proposed model without ground truth. \n\n*Another aspect of this work is the number of invisible factors and its role in the proposed model. \na) Have the authors considered a nested analysis of their model where they consider one invisible factor at start, and then keep adding invisible factors to quantify change in model performance? \nb) Again, in a clinical setting, there is no way to know all the confounding invisible factors. Would there be an optimum number of invisible factors required for the proposed model to work. \n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The paper has some interesting ideas, but is too disorganized and incomplete to provide a thorough evaluation",
            "review": "# Summary\n\nThe paper proposes a relaxation of ICP-style algorithms for causal feature discovery. The authors incorporate this algorithm into a program synthesis workflow for producing predictive models in scenarios where there are structural and non-causal variables, and in addition, some variables have the form of an image while other variables are categorical or numeric. The authors evaluate this algorithm on two datasets: first, a synthetic dataset built on MNIST, and secondly, a new Swiss CRC dataset of tissue microarray images from coleo-rectal cancer patients.\n\n# Feedback\n\nThe method proposed here could be promising, and the dataset presented could be an interesting benchmark, but the paper itself is too disorganized to make this point compellingly. In my opinion, the authors try to highlight too many aspects of this study at once, and this results in none of these ideas being communicated clearly. After reading the paper, I was unsure what the core problem was that the authors were trying to solve, and whether the metrics reported actually aligned with that goal. Certainly, it is important to be able to combine data from image and tabular data, to create models that generalize well across environments, to provide predictive algorithms that can be interpreted by practitioners, and to do so with tools that are well-suited to the purpose. But the authors do little to contextualize each of these things (aside from comparisons to ICP-based methods), and this makes it difficult to parse what the important aspects of the method are, or even what the method is. On this basis alone, I would recommend rejection.\n\nFor example, I think the discussion of type-safe program synthesis feels completely off-topic. It may be the case that this is an important aspect of the solution, but based on the current writing it seems completely disconnected. Certainly, flow-control modifications made to the HOUDINI language seem completely out of scope.\n\nRegarding what seems like the core method in the paper, the epsilon-ICP approach, this seems to motivate a reasonable regularization method, although it is unclear to me whether the connection to ICP is well-defined. Some potential holes that should be addressed. First, is the set S^*_{\\epsilon} unique? It seems like there could potentially be many distinct combinations of non-causal variables that could be included in this set for any epsilon > 0, so it is not clear that the analogies to the objects in ICP methodology are actually rigorous. Secondly, is there a guarantee (or at least heuristic argument) that the gradient-based approach will actually converge to S^*_{\\epsilon} or be a proper subset of S^*_{\\epsilon} with some probability? ICP methods satisfy this latter requirement (the authors rightly point out that this guarantee may not be useful for model-building), and I think the authors need to make a similar argument is they want to use ICP as a motivation.\n\nThe authors should also make connections to the literature on regularizing predictive models with interpretations or explanations. For example, Ross et al 2017 (https://www.ijcai.org/Proceedings/2017/371) present a method making use of gradients to constrain models. I imagine there are many other similar methods in this vein. It may also be worthwhile to connect this method to meta-learning algorithms such as MAML (https://arxiv.org/abs/1703.03400), which also makes use of global and within-environment updates.\n\nThe connection between the epsilon-ICP algorithm and combining visual and tabular data is also unclear. Are these two orthogonal aspects of the problem, or is epsilon-ICP supposed to resolve some issue with current approaches to problems with this sort of mixed data?\n\nIt also seems to me that there would be some non-trivial tuning of the scale of the gradients from each data source: the gradient norm would depend pretty strongly on the number of pixels in an image or the representations of colors in each channel. Most methods that make use of norm-based regularization of coefficients (consider LASSO methods in linear regression) include some kind of standardization step to remove this degree of freedom (although the causal implications of these standardization schemes are unclear).\n\nThe descriptions of the experiments are also incomplete. Some concerns:\n * The MNIST example seems overly contrived.\n * The notion of “ground truth” is not adequately explained. From the paper, I am unable to understand exactly how the GT methods differ.\n * It is not clear what environments correspond to. What is allowed to change between environments and what remains the same?\n * What does it mean for the baseline to be trained without environments? Does that mean that it trains on the data from all environments without doing environment-specific gradient updates, or that it is only trained within one environment? Does the baseline get access to an indicator of which environment each datapoint came from? \n * On what population is the model evaluated? In a specific environment? Across a uniform sample of cases from all environments? Usually, causal representations only matter in cases where we expect that the model will be tested in an environment distinct from the one it was trained in (otherwise, simply optimizing predictive performance is sufficient).\n * Relatedly,  it seems that simply training models that perform better in validation yields better performance metrics. Why is a causally-aware method necessary here?\n * How are environments created in the Swiss CRC experiment?\n * How were inputs scaled in these experiments?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper Not Clear Enough to Evaluate Major Claims",
            "review": "This paper considers the problem of performing \"invariant causal prediction\" on what seems to be multi-modal data, in which one of the modes is images/visual data. The authors propose an extension to invariance causal prediction that they claim provides reliability and interpretability.\n\n**I found the paper incredibly confusing and thus it was hard to evaluate the validity of many of the claims about efficacy, performance, etc. The best advice I can offer, from a general standpoint, is that the paper could do a _much_ better job of laying out the definitions of key terms in language familiar to the causal inference community. This might entail reorganizing the paper to provide a clearer introduction that clearly describes 1) the goal of ICP/why it might be used, abstractly 2) a revised discussion of the CRC example to demonstrate the prospective real world efficacy of ICP 3) an intuitive explanation of why ICP is not sufficient and what epsilon-ICP buys you. To my reading, if the above items are present, they're neither clear nor laid out in a logical way that makes the authors' contributions easy to understand.**\n\nAs an example, section 1.1 seems to frame the main problem of study as one in the causal discovery domain. The goal, it would seem, is to learn a causal graph (or a sub-model of a full causal graph) of some sort. The authors later discuss ideas of prediction performance -- the connection between search and estimation is not clear. Is there one all-encompassing method that achieves both?\n\nItemized comments follow.\n\nFollowing the ** comment above, the authors use non-standard verbage without providing formal definitions until much later in the paper. For instance, they use the expression \"identifiable causal variable\" in the introduction. Are these \"causal\" because they are causes of the outcome of interest? The definition of a \"plausible\" cause in section 3 doesn't really clear this up.\n\n\"On the other hand, how to accurately predict the outcome remains an open question. Besides, the experiments were mostly conducted on numerical data sets...\" -- again, why is prediction relevant here? Obviously, eventually the goal of discovery is to use the learned model for post-selection inference. Is that what e-ICP is doing?\n\nIn the review of causal visual discovery, \"In many of these instances, it is common that a visual object extracted from an image is sufficiently explained by tits textual counterpart\" -- how do we know? Is this an algorithmic fact? Is there a formalism to describe \"is explained by\"?\n\nBy the time I was in the middle of reading section 2, I found myself asking \"what IS the task the authors are trying to accomplish?\". If the authors could describe for instance \"image + numerical non-visual data goes in and XYZ comes out\", in non-jargony terms, that would go a long way to improving the readability of the manuscript.\n\nThroughout, the authors should be clear about whether, when they use the term \"invisible\" they mean latent (as in unobserved) or simply non-visible. This makes a huge difference in the identifiability of model parameters and the authors seem rather inconsistent in their use of the term.\n\nSee above comments on using terms before they're adequately defined. In the section on contributions (end of S2), the authors state that they \"efficiently discover e-plausible causal variables\". What are these? Shouldn't this be given an intuitive definition (if not a formal one) before it's used. I was totally lost at this point, 3/8 the way into the paper, and the main method (and notation) hadn't even been introduced yet.\n\n\"Concretely, we learn the gradients to reveal the e-plausible causal variables...\" -- Gradients of what? How do we know the function in question is differentiable? This seems to be addressed in section 4 but at this point in the paper there's no context.\n\nWhat are the experimental 'environments'? Are these different \"worlds\" under intervention? Is there a formal definition (or an intuitive one)?\n\nTheorem 1: I had trouble trying to understand the substantive meaning of this theorem. It seems the authors are saying that if there exists a function of X such that X is a plausible cause of Y with Y=f(X) + delta then the subsets of X for which that statement is true make up S*...?\nWhat, intuitively, is S? What about S*? S(U) is a set of identifiable variables wrt U? How does this notion of 'identifiability' match the conventional statistical and causal notions?\nThe authors should add text to explain the meaning and implications of the theorem in the paper. They should also consider providing intuition for why it might be true. \n\n\"One key observation is that if f is differentiable, then the gradient w.r.t. an input variable can reflect the importance of the variable\" -- how does this relate to the notion of influence functions in (semi-/non-)parametric statistics?\n\nFollowing assumption 2, \"The epsilon condition seeks for instignificant gradient norm ||\\nabla_S..|| to exclude the non e-plausible causal variables\" This seems to imply the definition is somewhat self-referential?\n\n\"Consequently, the final regularized loss function is (...)\" -- where does this loss function come from?\n\nAlgorithm 1 -- what is the output here? It appears the goal is to learn theta, which parametrizes f, the function that maps X to Y? How does that correspond to learning a causal (sub-) graph? Or to identifying candidate causes of Y among some superset X?\n\nSymbolic language -- I found the entire discussion of the symbolic language HOUDINI to be impenetrable. It apparently relies on a symbolic language from the literature and the authors claim it provides interpretability to clinicians. Given that the language is not clearly and completely described in the main draft, it's impossible to evaluate how it ensures these properties. Additionally, the authors continue to use terminology without defining it, such as \"top-down iterative refinement strategy\". I am not familiar with the programming languages literature, but given that this paper has been submitted to an ML conference and primarily focuses on causal topics, it's incumbent upon the authors to provide those details which the present draft lacks. In any case, I can scarcely believe that such a language provides MORE intuition to a clinician -- the authors should clarify how they envision this language providing that benefit.\n\nEvaluation -- as mentioned above, it's not entirely clear whether the authors' main task is discovery, prediction, or both (jointly or sequentially). As such, this makes the evaluation discussion murky. How, for instance, can discovery be evaluated by an \"accuracy\" metric. Similarly, the authors say that they adopt a variant of Wasserstein distance, known as Frechet inception distance, to measure distributional differences between prediction errors. It isn't clear how this helps evaluate whether a learned causal model is far from the truth since post-selection inference pre-supposes (to a degree) that the causal model is correct.\n\n\"We compare the proposes e-ICP to e-ICP (GT) with availability of ground truth visible data\" -- how is the ground truth available here? Particularly in the CRC example? For the method to be causal there would typically need to be a notion of intervention and a known ground truth of the intervention. In the CRC data is the intervention via an RCT in the cancer cohort?\n\nIs it not possible that some of the \"non-visible\" variables are not actually necessarily causes, but rather serve as proxies of causes. The authors don't really define the clinical meaning of variables like pM, preOP, and postOP, but it's not uncommon for clinical variables to serve as proxies for other unmeasured clinically relevant features. For instance, in medical disparities research, race is not typically a cause of some outcome of interest but it's often included in non-causal risk models since it's a proxy for many co-morbidities that are not always measured (and so the risk prediction model has better performance). Is it possible that something similar is going on here and that these are not actually \"causal\" variables as the authors have defined them?",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}