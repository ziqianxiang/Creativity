{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper introduces a notion of distributional generalization, which aims at characterizing aspects of underlying distribution that are learned by a trained predictor. Authors make several interesting conjectures and support them with empirical evidence. Reviewers agreed on the novelty of the ideas; however, the work seems to be preliminary in its current form. Unfortunately, I cannot recommend acceptance at this time. "
    },
    "Reviews": [
        {
            "title": "Recommendation to Reject",
            "review": "This paper introduces a new notion of \"distributional generalization\" as a tool to quantify the difference between the outputs from training and testing data sets using a certain machine learning algorithm. The authors formulate two conjectures for the so-called \"Interpolating classifiers\": the Feature Calibration Conjecture and Agreement Conjecture. The conjectures are supported numerically by some real data experiments and are proven for the 1-nearest-neighbor classifier under some technical conditions. \n\nStrengths: \n1. Two conjectures are interesting and have the potential to explain some unexplained phenomena for some existing machine learning algorithms.\n2. Numerical experiments are quite thorough, using different data sets and different machine learning algorithms.\n\nWeaknesses:\n1. The justifications of the conjectures are almost purely empirical (except in the 1-nearest-neighbor (1-N-N) classifier case). However, the 1-NN classifier is just a toy example that has limited use in machine learning literature. In fact, one even does not need to train the 1-NN classifier given a training data set. This is probably why it is easy to analyze 1-NN under the considered setting.\n2. While observations from data experiments are interesting and I enjoyed reading them, the paper fails to directly demonstrate how these observations can be used to improve performances or our understandings of existing algorithms. For example, how Conjecture 2 is useful? can we use it to evaluate the uncertainty (or confidence interval) of the test accuracy?\nHow can we find distinguishable features in Conjecture 1 in practice? using trees?\n\nConclusion:\nAlthough I find the proposed conjectures interesting and may have potential values, the current formulation, and justification for these conjectures are a bit too superficial and loose. It is difficult for me to envision a scenario where these conjectures can make a meaningful impact.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting observations",
            "review": "This paper proposes a new notion of generalization called distributional generalization which states that the outputs of the classifier for train and test are close as distributions not just their corresponding accuracy numbers. They propose conjectures about their the distributional closeness that they expect and how it depends on the model, number of data points and the algorithm. This paper gives experiments to support their conjectures for different model classes including neural networks, kernel methods and decision trees.\n\nI find the notion of distributional generalization interesting. The first experiment of this paper is that the label noise introduced in the training set in one subgroup of a particular class is also localized in the same subgroup in the test set although the classifier does not have any explicit information about the subgroups which is interesting. The paper formally states that the distribution of train and test outcomes are similar in distribution with respect to tests which themselves can learned by that model class with the current number of samples.\n\nI recommend accepting this paper because the notion of distributional generalization that this paper introduces is both interesting and surprising. Moreover, this paper has shown that it holds widely across different interpolating classifiers like decision trees, neural networks and kernel methods.\n\nThe paper includes an extensive set of experiments that are easy to follow.\n\nOne concern is as itself mentioned in the paper that this work does not talk about what conditions on the distribution, algorithm and the model class are necessary for this conjecture to hold either empirically or theoretically and the conjecture is not precisely defined. Any insights on this part would be good to include. \n\nAnother concern is that although this is an interesting observation, the paper does not talk about why and how studying this form of generalization would be useful for understanding interpolating classifiers and generalization in general. \n\nQuestions:\n1) The authors suspect locality to be the underlying reason behind distributional generalization and find this observation to be true for kernel methods also. Do the authors know if there is some work on the locality aspects of kernel methods arguing that some particular kernels are more local than others and how this relates to the observations in this paper?\n2) Regarding the agreement property in section 4, I find it a little surprising that the conjecture says that the correlation and the accuracy are the same. Does this relate to the number of classes present?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of Distributional Generalization",
            "review": "> Summarize what the paper claims to contribute\n\nThis paper generalizes the classical notion of generalization to the notion that the output of the classifier should be close when applied to the training data and the testing data. Two conjectures were made to predict the behavior of the distributional generalization for a few models and data. The first conjecture, Feature Calibration Conjecture, asserts that the distribution of the outputs are similar up to “distinguishable features”. The second conjecture, Agreement Conjecture, asserts that test accuracy matches the classification stability for interpolated classifiers. A number of experiments surrounding these conjectures were conducted to illustrate the Distributional Generalization\n\n> List strong and weak points of the paper. Be as comprehensive as possible.\n+ Great idea to look into distribution of the output beyond the error. I think this is long overdue and this paper has a lot of potential.\n+ Comprehensive numerical studies which show some interesting findings.\n- Many vague and ambiguous notations which make the paper hard to read.\n- Barely any theoretical justification. \n- \"Too many\" ideas. I would suggest the authors to divide the paper into 3 papers and dig deeper in each topic. \n\n> Provide supporting arguments for your recommendation.\n\n1. Notation. While the display on the bottom of page 2 is supposed to be an informal conjecture, it may be too informal. Does x∈TestSet indicate a distribution or a set? What does ≡ mean here? It was mentioned that the joint distribution of X and Y is D, and D^n denote n iid samples from D. But is D^n a sample of training data points, or a distribution? Later on it was mentioned that \"S ∼ D^n\" which does not make sense if D^n is a sample. \n2. There are only two theoretical results in the paper with rigorous proofs. But they are established for nearest neighbor classifier only and some assumptions are imposed which warrants the results straightforwardly. However, most of the conjectures are made for many classifiers beyond nearest neighbor. \n3. Indistinguishability and interpolated classifier: in the Indistinguishability meta-conjecture on page 2, the second equality is clearly due to interpolated classifier. I wonder if the closeness indicated in the first approximation (between trainset and testset) is a merit of the interpolated classifier, or the general good generalization performance of a classifier. It is not clear to me why interpolated classifiers will induce this closeness.\n4. The role of L. From what I can see, this intermediate feature L is introduced because \\mathcal{X} can be a large high-dimensional space which can make visualization or quantification of the closeness difficult. This function L is introduced so that it is easier to see how (L(x), f(x)) is distributed. For constant partition (L(x) = 0), the distributional generalization reduces to the marginal distribution of \"f(x)\". In my opinion, if a classifier is true generalizable, the distribution of (x, f(x)) itself, without L, should be the same between the test and training domains. From the statement of Theorem 1, it seems to me that  Conjecture 1 holds for nearest neighbor BECAUSE some distinguishable features exist, which may already be a strong assumption. So strong that the very existence of the distinguishable features warrants the generalizability. Unfortunately because there are only conjectures and no rigorous proofs, this part is still not clear to me.\n\n> Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. \n\nWhile the numerical studies have convinced me that distributional generalization is really a thing here, I am not sure that I am that impressed. After all, the precision was only two decimal points. Can the total-variation distance be calculated? Is there a counterexample in which one classifier does have classical generalizability but not distributional generalizability? Focus on a narrower set of topics but dig deeper may go a long way here.  \n\n> Minor comments\n\nPage 3: “do we this same procedure” is a typo\nPage 4: how is “learnable” defined?\nPage 5: f←Train F (D^n) is not defined, esp. given that it is unclear if D^n is a distribution or a set. Should D^n be replaced by S?\nPage 5: Indistinguishably is a typo\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an extended notion of generalization. The new proposed notion asks that for a family of tests $T: X \\times Y → [0,1]$, $T(x, f(x))$  will be similar for train/test examples. The paper proposes three interesting conjectures that are related to distributional generalization. The paper proves Conjecture 1 for nearest-neighbor classifiers. The paper also gives empirical evidence supporting their conjectures.\n\nI think this is a nice contribution. It raises some new questions on generalization, and points to some interesting properties that interpolating classifiers satisfy (at least empirically) that deserve to be studied in more detail. In particular, Conjecture 1 seems to suggest that there is a form of hierarchical learning that interpolating classifiers are doing without being told to explicitly do so. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}