{
    "Decision": "",
    "Reviews": [
        {
            "title": "A review for invariant batch normalization for multi-source domain generalization",
            "review": "**Summarize what the paper claims to contribute.**\n \nThe paper introduces an invariant batch normalization technique, which adopts invariant risk minimization framework into batch normalization layer and its consecutive convolutional layers. The design of invariant batch normalization is based on the empirical experiments which measure the contribution of components in neural networks with accuracy. Based on the proposed method, we can easily boost the performance of domain adaptation by substituting batch normalization layers in a modern neural network. The performances are shown in three domain adaptation benchmarks.\n \n**List strong and weak points of the paper.**\n \n*strong points*\n \nThe motivation of the proposed method is based on the evidence. I found that the design of the proposed method is consistent with the explanation on figure 1.\nThe proposed method shows state-of-the-art performance on the three benchmarks for domain adaptation without bells and whistles\n \n*weak points*\n \nThe reasonings behind invariant risk minimization (IRM) are not explained well, even though the paper relies on the concept of IRM.\nThe analysis on the proposed method is not done properly.\n \n**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice**\n \nI give “5: Marginally below acceptance threshold” to the paper. The proposed normalization technique is simple and applicable to many modern neural networks for domain generalization. It gives an analysis in figure 1 to explain the effect of batch normalization in depth. The proposed method shows good performance on three benchmarks for domain generalization. However, the merits of the proposed method compared to the previous IRM works are not explained well in the paper.\n \n**Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.**\n \n*minimax formulation*\n \nAlthough the comparison with the other IRM methods are left as future work as described in section 3.5, I think this part is crucial for the contribution of the proposed method. I want to see the reasoning behind the proposed minimax formulation compared to the previous IRM works (Arjovsky et at., 2019 and Ahuja et al., 2020).\n \nWhat is the advantage of minimax solver over gradient norm penalty (Arjovsky et at., 2019) and classifier ensemble (Ahuja et al., 2020)? Is there any restriction on the other IRM methods not adopting the current baseline? I think it should be explained in the paper and can be explained without experiments on the benchmarks introduced in (Arjovsky et al., 2019).\n \nIf the proposed minimax formulation is valid, then it should be also applicable to benchmarks in IRM paper (Arjovsky et al., 2019). I conjecture that the proposed method is better than a naive baseline. How about the comparison with the other IRM methods? Why do you leave it for the future work?\n \n*stability*\n \nCould you give an analysis on the stability of the proposed method, not the final performance? In the abstract, you mentioned that “the improved stability”.\nThe paper lacks an analysis on the proposed method. Rather it only gives a performance table.\n \n**Provide additional feedback with the aim to improve the paper.**\n \nFigure 1 explains good intuition behind normalization for domain adaptation & generalization. Could you extend this plot into cover other normalization techniques, such as instance normalization which appears in the experiment section?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new solution to the domain generalization problem based on batch normalization",
            "review": "The paper studies the problem of domain generalization and inspired by previous works shows empirically that batch normalization parameters learned on different domains are a serious source performance gab between models trained on multi source domains and models trained on a  specific target domain. \nA such a new solution is proposed that aims at learning invariant batch normalization layer. The proposed solution minimizes the difference between the empirical risk on each domain using the general batch normalization parameters and that when using finetuned batch normalization layer arguing that if the representation layer is invariant then the shared classifier is optimal in each domain. \nExperiments on multiple domain generalisation benchmarks show minor improvements over existing methods. \n===========================================================================================\nI am confused with considering the batch normalization parameters as the classifier (text after eq2)? does that mean there another classifier for each domain? is that shared? why is this naming?\nEq3 looks weird the reached minima of minimizing the ERM on all domains should implicitly provide the lowest possible ERM on each domain without the need to the constraints, i might have missed something though.\nIn the text after Eq4, it is said that the equation maximized the Risk w.r.t to the specialized parameters I believe it is the opposite? the maximization is of -R.\nIn Algorithm1, after updating specific domain parameters, only the encoder parameters are updated, so the batch normalization parameters w are not updated?\n\nThe improvements are not always consistent and are minor, not sure it is worth the computational cost.\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but the paper is not ready for publication yet",
            "review": "$Summary$\n\nThe Authors face the domain generalization problem, assessing the role of batch normalization and domain statistics in out-of-domain performance. They introduce a method that relies on Invariant Risk Minimization, whose goal is obtaining normalization parameters that are robust across different domains. The method is tested on standard domain generalization benchmarks.\n\n$Pros$\n\n- While batch normalization is a key ingredient in deep learning, its usability is severely compromised under domain shifts, because the learned parameters and the data statistics are no longer reliable. The design of methods to address this point is hence very relevant for the community. The idea of relying on Invariant Risk Minimization [Arjovski et al., 2019] to cope with this problem is interesting.\n\n- The results suggest that the proposed method has a positive effect, with respect to standard Empirical Risk Minimization.\n\n$Cons$\n\n- The main weakness of this work in my opinion is the current presentation. For what concerns the content, the contributions with respect to the prior art are not extensively detailed: In Sec.1 (Distribution matching) the difference between the proposed approach and the published ones is merely simply as \"Our approach is orthogonal to them. Moving to Sec. 3.5, the difference with respect to Invariant Risk Minimization is very synthetic. Comparing the results in Sec. 3.2 with the literature review in Sec.2.2, I could not understand the impact of the reported results (Figure 1): if other works have assessed the vulnerability of batch normalization under domain shifts, what is the additional contribution brought by the reported numbers? What additional information do they carry? This should be properly clarified in the manuscript. The Authors should expand more in these points. For what concerns the writing itself, this paper requires extensive proof-checking, as there are typos in most of the paragraphs. \n\n- I have difficulties understanding the proposed method: from Algorithm 1, it seems that the whole model is trained by Invariant Risk Minimization, whereas from the contributions it seems that the proposed approaches only concern normalization layers (for example, see the second bullet-point in the Introduction). Could the Authors clarify how the training stage is performed, and its relationship with Invariant Risk Minimization?\n\n- In the ResNet-50 experiments (Tables 1/2), the improvement from Empirical Risk Minimization to InvarNorm seems rather marginal (within 1%). Could the Authors report some statistics around the results, to assess the statistical significance of the results? Furthermore, given the relatedness of the approaches, it would be valuable having the comparison with Invariant Risk Minimization in every protocol (also with ResNet-50).\n\n$Review$ $summary$\n\nWhile I like the original idea behind this paper, I believe that it requires extensive re-work before being considered for publication. I look forward reading the Author response and iterating the discussion.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited theoretical or empirical support",
            "review": "Summary:\n\nThe authors posit that domain-generalization error in neural nets comes from a shift in the means/stdevs of hidden features, as well as the optimal parameters of batch normalization layers within the network.\nThey propose InvarNorm, which is a minimax training objective which seeks to penalize the risk difference between per-domain optimal BN statistics and parameters, and shared ones.\nThe authors present experimental results on PACS, VLCS, and Office-Home. \n\nReview:\n\nI have several concerns about the paper which prevent me from recommending acceptance in its current form.\n\nCorrectness:\n\n- The InvarMax objective is minimized when a model using batch statistics from data pooled across environments yields optimal risk (Sec 3.3 \"Marginal Distribution Shift\"). This is distinct from saying that the batch statistics are invariant across environments, which supposedly is the goal (Sec 3.2). The paper could be strengthened by providing a formal argument that the batch statistics are environment-invariant when the InvarNorm objective is minimized.\n\n- The Fig 1 AdaBN experiment isn't direct evidence that the BN statistics shift over distribution; the paper could be strengthened by referencing (or replicating) the tSNE plot of the BN statistics in the AdaBN paper, which directly supports this claim.\n\n- The authors claim that a shift in the optimal BN rescaling parameters is caused by a domain shift in conditional distribution P(Y|X), but this isn't justified either theoretically or by experiment. The experiment of Fig 1 shows that fine-tuning the rescaling parameters improves accuracy, but that doesn't imply any connection to the conditional P(Y|X).\n\n- Related to the previous point: There's a big difference between \"marginal distribution\" and \"feature-wise mean/stdev\"; likewise there's a big difference between \"conditional distribution\" and \"BN rescaling params\"; these differences aren't explained in the paper.\n\nNovelty:\n\nThe main conceptual insights are the observation of shift in BN statistics, and the expressivity of the BN rescaling parameters for domain shifts. The observation that BN statistics change over distribution isn't novel; see e.g. AdaBN. Likewise the expresivity of the BN rescaling parameters has been shown before in e.g. https://arxiv.org/abs/2003.00152 (albeit not specifically in a domain-shift setting). \n\nSignificance:\n\nThere isn't much theoretical justification given, so the significance is evaluated empirically.\nOn VLCS, PACS, and OfficeHome (ResNet-50), InvarNorm outperforms the authors' ERM baselines by only about a point. (A slightly larger improvement is seen with ResNet-18, but I argue that an improvement which diminishes for larger networks isn't very useful.) Moreover there seems to be headroom for better tuning of the ERM baselines; e.g. see https://arxiv.org/pdf/2007.01434.pdf whose ERM baselines outperform these ones. \nFinally, since no error intervals are given for these numbers, we don't know whether the small differences are significant or not.\nIn conclusion, the experiments don't establish the algorithm's usefulness.\n\nClarity:\n\nThe writing was somewhat hard to follow. I'd recommend having the text proofread for grammar.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}