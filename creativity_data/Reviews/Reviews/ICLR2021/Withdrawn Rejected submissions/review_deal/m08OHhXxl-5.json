{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work considers the problem of calibrating a multi-class classifier while preserving differential privacy. It proposes a method Accuracy Temperature Scaling, that aims to achieve consistency rather than calibration. The method is particularly easy to implement under the constraint of DP. The paper then evaluates  the calibration algorithm in the context of domain perturbation/shift and, as the authors demonstrate it outperforms adaptations of other technques to DP.\n\nThe strong sides of this work are \n* the first work to study calibration in this setting (albeit that is also a result of the setting being of a relatively narrow interest)\n* proposes a new algorithm\n* evaluation on multiple benchmarks\n\nThe weaknesses\n* The method is not justified either by theoretical analysis or clear intuition\n* Evaluation of performance in the context of domain shift makes the the presentation somewhat confusing and experiments much more involved but is largely orthogonal to the problem of calibration\n\nOverall the work has merits but also significant issues."
    },
    "Reviews": [
        {
            "title": "This paper studies the problem of recalibrating a classifier under the presence of domain shift and the constraints of differential privacy.  They show how to adapt several algorithms for dealing with domain shift to the paradigm of differential privacy, giving mechanisms that achieve both goals.",
            "review": "Summary:\n\nThis paper studies the problem of recalibrating a classifier under the presence of domain shift and the constraints of differential privacy.  They show how to adapt several algorithms for dealing with domain shift to the paradigm of differential privacy, giving mechanisms that achieve both goals.\n\nStrong Points:\n\n1. Addresses a new, interesting, and practically relevant problem.\n2. Technically strong, good insights and clearly bridges the gap between two distinct areas.  \n3. Well organized and written, very clear for the most part. \n\nWeak Points:\n\n1. The sensitivity of f cannot be bounded.  What the authors propose (Section C.2.1) to address this --- i.e., using a sufficiently large value based on the empirical values --- is not generally accepted.\n\nOther Notes:\n\nI think the ideas are cool.  The reduction to a 1-dimensional minimization problem over T makes sense.  The technical insight to use golden search to reduce queries/noise is clever.  Experiments show that the proposed approach actually works as it is expected to.  My one weak point is a bit concerning however.  Hopefully authors can address it adequately in the rebuttal.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The empirical results seem complete. I have several concerns about the technical part.",
            "review": "This paper studies the problem of privacy-preserving calibration under the domain shift. The authors propose ''accuracy temperature scaling'' with privacy guarantees.  \n \nThe empirical results seem complete. I still have several concerns about the technical part.\n\n \n1. The proposed algorithm is not described very clearly in section 3 and section 4. After spending considerable time reading sections 3 and 4, I still feel hard to follow how they address the domain-shift issues.\n \n2. The privacy part seems like a plug-and-play of the Laplace mechanism. Hence, the technical novelty might be limited. Note that the privacy computation in section 3 based on a naive composition --- ` each $M_i$ satisfies $\\epsilon/k$, the total privacy cost follows $\\epsilon$. I would suggest the authors use recently advanced composition [1,2] for better privacy and utility tradeoffs. Moreover, the calculation of sensitivity seems to be wrong. As the authors claim in Section C.2.1 in the appendix, the sensitivity is technically infinite. They set $\\triangle_f=10$ based on empirical observation, which violates the privacy definition.\n\n[1] The Composition Theorem for Differential Privacy.\n[2] Renyi Differential Privacy.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Neat use of accuracy temperature scaling for differential privacy",
            "review": "The paper tackles the problem of privacy preserving calibration under domain shift, which is an interesting combination of 3 separate problems that may often occur together. The main contribution is the use of accuracy temperature scaling for calibration, which is a good match for differential privacy. Extensive experimental validation is given.\n\nStrengths:\n- The use of accuracy temperature scaling is neat in combination with DP. Whilst not a startling novelty, it is a nice observation that their method Accuracy Temperature scaling (Acc-T) combines so well with differential privacy, and yet does not lose out in terms of utility (and in fact does better under more stringent privacy settings, since fewer DP noise iterations are required.\n- Strong experimental section. Although only on image data, the experiments cover a multitude of shift types, and the baselines are apt for the task.\n\nWeaknesses:\n- In terms of methodology, the paper is only tangentially related to domain shift, since there is a form of fine-tuning on the target dataset (using the overall accuracy) (see §2.2). There are extensive experiments on recalibration under domain shift (using perturbed image datasets). However, there must also be an interplay between the degree of shift, the effectiveness of transfer learning (e.g. fine-tuning) and calibration. This is not really covered\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\n \nThe paper studies the problem of classifier recalibration under differential privacy constraints. They propose a framework with a calibrator and several private data sources, and it works as follows. At each iteration, the calibrator queries each source, and the data source sends back the private answer, which will be used to optimize the calibration. They also provide a recalibration technique, accuracy temperature scaling, which is effective under the privacy constraint for the reason of low sensitivity. Rigorous experimental results are provided.\n\n\nReasons for score: \n\n Overall, I am positive about this paper, but I have a few concerns.  I've listed the strengths and weaknesses below. Hopefully, the authors can address my concern in the rebuttal period. I'd be happy to raise my score if I am wrong.\n\nStrengths: \n\n\n1. The problem is well-motivated, giving the rising privacy concern and the importance of recalibration.\n\n2. The choice of the query function is novel for privacy constraint, as it has lower sensitivity compared to the log-likelihood function.\n\n3. They provide extensive experimental results to demonstrate the effectiveness of the proposed method.\n\n \n Weaknesses:\n\n1. I don't see how the algorithm addresses the domain shift problem. But they claim that ''We also fine-tune on the target domain'' in section 2.2.\n\n2. According to Tables 3 and 4 in the appendix, the Acc-T works well compared to others without privacy constraints. As it is expected to have higher biases, I would appreciate it if the authors could provide more details on how they evaluated the error and explanations.\n\n3. The framework seems like federated learning with differential privacy, where a central server only gets private local updates from users and takes the average to optimize the parameters. The framework doesn't seem to be novel, but it can be a novel use of this setting for recalibration. It would be good to add a few sentences discussing the connections to federated learning.\n\n4.. Truncating the log likelihood function can also lower the sensitivity. It would be interesting to see the comparison.\n\nMinor comments:\n\n1. In algorithm 1, the query functions for all d data sources are the same. But the general framework states that they can be different. I just wonder how to design a customized query function for each data source. Or maybe it would be more clear to remove the subscript.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}