{
    "Decision": "",
    "Reviews": [
        {
            "title": "An OK paper that extends two previous works for MIL.  ",
            "review": "Methodology:\nThe paper presents an approach for MIL. Two previous pooling operators (attention and distribution) are combined to capture more information with the attention scores and raw instance-level score. Some theoretical results were provided to support the claim of information gain in the proposed operator. Experiments on two benchmarks show that the attention-distribution operator outperforms four baseline MIL pooling operators.       \n            \nPros:\nThe paper combines two pooling operators in a reasonable way to achieve better empirical results.  \n\nCons:\n- The overall conceptual and technical novelty is limited.\nThe work basically extends two existing MIL pooling operators (attention and distribution) in a straightforward and trivial fashion, which does not provide much more insights into the problem itself. The marginal contribution does not justify the significance well.     \n\n- Three propositions are presented to justify the information superiority between pooling operators. The statements and proves are always based on conditions that “f attention weights are accessible and pooling filters θfilter and θfilter are perfect,”, so what is a perfect filter exactly in this context? \n\n- Empirical study should be enhanced\nFor one thing, several other interesting popular pooling operators or strategies are not tested. , To name a few, the linear softmax and exponential softmax pooling operators from “A Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling” ICASSP 2019, the top-k instance pooling operator from “Multiple instance learning for soft bags via top instances”, CVPR 2015. These are also shown to outperform conventional max/mean/median pooling for MIL too.\nAnother concern is on the benchmarks. The two datasets used in the paper are not large enough (<3k examples) given today’s deep model context as adopted by the method. It would be great if the comparison on some more realistic benchmarks (E.g., MS COCO) can be presented too. Will that change the observations in the paper? Will a stronger / better-tuned attention affect the results and conclusion. Since the conceptual and technical novelty is limited, extensive empirical justification is needed for a more convincing conclusion.  \n\n\nMinor comments:\nIt looks to me that some notations in the theoretical analysis need to be reconsidered. E.g. Proposition 2 mean h_X = E[dist h_X] does not make much sense to me. What is the expectation of a pdf (dist h_x) here? It looks like it should be E[v]~p(v) with p(v) from dist h_X.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper is an easy read, however, it lacks a good theoretical foundation",
            "review": "**Summary of the paper**\n* The paper proposes a categorization of Multiple Instance Learning (MIL) criterion into point estimate (mean, max etc.) and distribution based representation of bags.\n* A distribution based representation for a bag is supposed to be superior in some sense and hence, the paper claims that it should provide a better performance than the point estimate based representation.\n* The paper extends the previously proposed kernel density based MIL criterion in Oner et al, 2020 with the concept of attention using weights. It also includes theoretical analysis of some of the properties of the distribution based filters.\n* The experiments are performed on a lymph node metastases dataset where the images have to be classified as being normal, metastases or boundary/mixed. There are also experiments performed with classical MIL datasets on drug activity prediction and animal image annotation. The results are mixed in terms of the superiority of the proposed distribution with attention representation.\n\n**Strengths**\n* The inclusion of attention in the distribution filter is a nice way to extend and round out the work of Oner et al. 2020.\n* The experiments with the lymph node metastases dataset are performed for five different types of MIL tasks while comparing five different pooling filters on each task. The results provide valuable insights into the strengths and weaknesses of the different MIL filters on the different tasks.\n* The code is made available.\n\n**Weaknesses**\n* The distribution filter as proposed in this work uses a kernel density estimator to sample the distribution uniformly along the different dimensions. I am not really sure if we can still call the sampled representation a distribution function. Consider an alternative viewpoint to the framework depicted in Figure 1 where \\\\(\\theta_{transform}\\\\) absorbs the max, mean, attention pooling. This can be achieved as part of the \\\\(\\theta_{transform}\\\\) by making the first layer be max-pooling, average pooling or a single linear layer applied on the feature dimension. The \\\\(\\theta_{filter}\\\\) would then just be an identity mapping after feature extraction for the max, mean and attention pooling filters, whereas it would be the density based representation for the distribution filter. The module \\\\(\\theta_{transform}\\\\) has the freedom to incorporate attention pooling on the new features and then follow it up with a multi-layer perceptron for final classification.\n* The theoretical analysis as provided in the propositions 1-3 is mostly about the properties of distributions. I was not able to understand why these serve as a theoretical justification for the “superiority” of them over point estimates. The theoretical analysis did not provide insight into why such a “superiority” leads to better performance on the task at hand. The statement in contribution 2 - “distributed with attention pooling filter is theoretically the best” is not fully justified because the term “best” is not defined and its relevance to the final classification performance is not provided.\n* In the proof for proposition 3, the density function in subsection (ii) is expressed in terms of \\\\(g_{x_i}\\\\) which is a weighted feature value \\\\(w_i f_{x_i}\\\\). I was not able to understand why the distribution filter was written in terms of this. If I understand correctly the default distribution filter works directly on the features \\\\(f_{x_i}\\\\). If the features are multiplied by those “attention weights” then it is incorrect to refer to it as \\\\(p_X\\\\) anymore. The set \\\\(X\\\\) is now different due to the weighting. Hence, I believe the proof is not valid. Please correct me if I am missing something.\n* If the distribution based bag representation is the best and superior according to the theoretical analysis, the experimental results don’t reflect that. It seems like the mean pooling can be better in a multi-task setting as shown in Table 2.\n* It isn’t clear if the Distribution-Net as labeled in Table 3 uses attention or not. Table 3 should probably also include the results from all the five MIL filters. It will be good to see information about the differences in the feature extraction network between Distribution-Net and the baselines like Wang et al., 2018 and Ilse et al., 2018.\n* Similarly, the results in Table 2 on the lymph node metastases dataset are missing comparisons to a baseline method from literature.\n\n**Questions/Minor comments**\n* It is easy to follow the paper, however, there are some minor issues in the writing which can be improved. Ex: Second paragraph in page 2: The sentence starting with “We theoretically showed” should probably be in present tense. The second half of “However, they don’t go beyond one specific application and lack of theoretical analysis” needs some grammatical correction.\n* Are there any insights into the performance of the distribution filter when the bag size is small? Why was it fixed to 64 in the experiments? Would a large number help in better estimation of the distribution?\n\n**Justification of rating**\nThe paper’s claims of a novel categorization of MIL filters, incorporating attention into distribution based representation and the theoretical analysis fall short in various ways as mentioned in the weaknesses. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Gaussian distribution pooling for MIL",
            "review": "**Summary:**\n\nThis paper analyzes pooling functions for Multiple Instance Learning (MIL). The authors presented and compared several point estimate and distribution pooling functions. They also showed some theoretical properties of some point estimate and Gaussian distribution based poolings. They extended the Gaussian distribution pooling with an attention mechanism. Finally, they analyzed empirically 5 pooling functions on a lymph node metastases dataset.\n\n**Reasons for score:** \n\nOverall, I feel it is quite difficult to evaluate the quality/impact of this paper because a lot of parts are not well motivated. For example, the authors show some theoretical results but never explain why these results are important. The comparison to some existing pooling functions is missing.\n\n**Pros:**\n\n- The introduction section to the MIL framework is easy to understand.\n- I think the idea of using a Gaussian distribution pooling for MIL is novel.\n- I like the idea to compare the pooling functions in several contexts.\n- MIL models are an interesting direction because they allow to reduce the annotation complexity.\n\n**Cons:**\n\nFirst, I think the term distribution pooling is quite confusing because it is specific to a Gaussian distribution pooling. I think the authors should rename it to something like Gaussian distribution pooling. The authors used the term distribution based pooling in the title and abstract so the reader can think that it will present results for generic distribution based pooling but the paper only shows results about Gaussian distribution based pooling. The authors should be clearer about what is in this paper. \n\nThe technical contribution of the distribution with attention pooling is very limited. It is a straightforward extension of the distribution pooling where the attention is computed with a softmax. The authors did not explain the motivation of the distribution with attention pooling. Attention models are trendy but it is not enough to justify a new pooling  function. For example, the authors should explain what problem the distribution pooling cannot solve but the distribution with attention pooling can. It will help to understand the importance/impact of this contribution.\n\nIn the abstract and the contribution section, the authors claim that  they “theoretically showed that the distribution based pooling filters are superior to the point estimate based counterparts.” I did not see where it is proved in the paper. In general, it is not a good idea to say that something is better than something else without giving detail. The authors should specify the metric or for what property. There are a lot of ways to evaluate/compare a model: metric (accuracy), stability, convergence speed, robustness (noise, adversarial attack), fairness, runtime, etc. \n\nThe authors did not explain the importance of their theoretical results. For example, it is difficult to understand why the property 1 (“given two feature matrices, whenever ‘max’ pooling filter produces two different representations, so does ‘distribution’ pooling filter”) is important. If the goal is to classify the bags, this property does not seem important. In a perfect world, you only need 2 representations: one for the positive bags and one for the negative bags. The authors should also explain what is a perfect filter (proposition 2 and 3).\n\nI think some comparisons to pooling functions are missing. The authors did not compare their model with [1, 2, 3, 4, 5]. I know three are a lot of papers about MIL and it is not possible to compare with every approach but I think the authors should at least compare with [5] because it is a generalization of a lot of pooling functions and the deep set paper [6].\n\nIt is a bit difficult to understand some results of table 3. For example in the 3-class scenario, there is almost a 3pt difference between the distribution and the attention poolings but the difference is not significant (p>0.05). The authors show results on a new dataset so it is quite challenging to understand the difficulty of the task. There are a lot of existing MIL datasets so the authors should justify why they need a new dataset or why they cannot use existing MIL datasets. However, the idea to compare pooling functions for different tasks is interesting  but it will be better if it is on well-known datasets.\nThey show results on very old MIL datasets but I do not think they are the right datasets to evaluate the model because they are very small datasets and can be sensible to the random seed. For example, it is quite difficult to interpret the results of 0.923 on MIUSK1 because the result was 0.874 20 year ago when the mi/MI-SVM paper was published. I think this paper will be stronger if the authors show results on bigger and more recent dataset like MSCOCO (see [6]). \n\n**Minor comment:**\n\nI think there is room to improve the structure and some notations in the paper. For instance, I do not think that the distribution with attention pooling should be in the MIL framework section because it is a technical contribution. It should have its own section with its motivation. I think it is possible to replace $f_{x_i}^j$ by standard matrix notation $F_{x_i, j}$ because F is a feature matrix. Also the notation $[...|...]$ is not consistent because it creates a vector and a matrix. \n\n[1]  Lee C., Gallagher PW., Tu Z. Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree. In AISTATS, 2016.\n\n[2] Sun C., Paluri M., Collobert R., Nevatia R., Bourdev L. ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks. In CVPR, 2016.\n\n[3] Girdhar R., Ramanan D. Attentional Pooling for Action Recognition. In NeurIPS, 2017.\n\n[4] Kulkarni, Jurie, Zepeda, Perez, Chevallier. SPLeaP: Soft Pooling of Learned Parts for Image Classification. In ECCV, 2016.\n\n[5] Diane Bouchacourt, Sebastian Nowozin, M. Pawan Kumar. Entropy-Based Latent Structured Output Prediction. In ICCV, 2015.\n\n[6] Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan R and Smola, Alexander J. Deep Sets. In NeurIPS, 2017.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}