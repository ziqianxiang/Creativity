{
    "Decision": "",
    "Reviews": [
        {
            "title": "incremental work in both technical approach and accuracy improvement.",
            "review": "This work explores the use of anomaly detection techniques to identify in-domain and out-of-domain data instances, and uses the anomaly scores as a domain relevance weight to the common domain adaption in NLP. A set of six anomaly detectors is evaluated w.r.t. its ability to detect out-of-domain data instances, and iForest is found to be generally better than the other five detectors and is therefore selected to provide the domain relevance weights. The iForest-enabled domain adaption method called DANA is evaluated on nine NLP tasks and compared with five relevant competing methods.\n\nOverall, the paper is well written and easy-to-follow. The idea of using anomaly detection methods to enable domain adaption is interesting, and is shown effective in the presented extensive results. However, one main concern here is that the distance-based method, Distance (Wang et al., 2017a), actually has a similar working mechanism as several well-known anomaly detectors, such as svdd (\"Support vector data description.\" Machine learning 54, no. 1 (2004): 45-66.) and the centroid-based method (\"Security analysis of online centroid anomaly detection.\" The Journal of Machine Learning Research 13, no. 1 (2012): 3681-3724.). All these methods generate centroids, prototypes, or class centers of the in-domain data, and identify data instances that are far away from the centroids as anomalies. The d(v_f , C_F_out) term in Distance (Wang et al., 2017a) may be removed, and the Distance method should still work well as this term is largely redundant to d(v_f , C_F_in ). This means that Distance may also work effective even without using any out-of-domain data. Therefore, the proposed method shares similar motivation and technical approaches as the previous work Distance (Wang et al., 2017a). Further, the proposed method and Distance also achieve similarly good performance, as shown in Table 4. The performance difference between these two methods are very marginally, showing no obvious advantages. One possible way to further enhance the performance is to advanced anomaly detection methods (such as \"Deep anomaly detection with deviation networks.\" In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 353-362. 2019) that can leverage a very limited number of out-of-domain data during the training stage to achieve significant anomaly detection improvement.\n\nIn summary, the overall idea is interesting, and it represents an important way to address the commonly-used domain adaption problem, but the manuscript is weak in terms of novelty and the advancement to the area.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Domain adaptation technique by leveraging Anomaly Detection (AD) as an uncertainty quantifier",
            "review": "### Summary:\nThe paper proposes a domain adaptation technique by leveraging Anomaly Detection (AD) as an uncertainty quantifier when domain-specific large corpora are not available. Authors filter the domain-specific data from the general corpus based on confidence scores of AD. By using this extra data, authors achieved better performance than a domain-specific fine-tuned model pre-trained with general data only.\n\n### Strong Points\n 1. A well-motivated work with extensive experimentation over multiple domain and multiple tasks.\n 2. Suitable for domains with little task / domain specific data available\n\n### Weak Points\n 1. Although authors claim that proposed approach works better than fine-tuned models, the work does not compare DANA with well-known models like SciBERT, BioBERT, ClinicalBERT, etc. Experiments on this front will be a valuable addition.\n 2. In section 3, under heading “Ranking anomaly detection algorithms”, any particular reason why the training for anomaly detection classifier was done only on in-domain train data i.e., during training why out-of-domain data has not been considered?\n 3. I feel this approach can be thought of addition of one more pretraining of the general pre-trained model with the filtered data (which can be obtained by any uncertainty quantification method). I wonder how the performance will compare in such a scenario. Below are the steps:\n    1. Pre-train a model with general data\n    2. Filter domain-specific examples from the large general corpus using any uncertainty quantification methods\n    3. Pre-train model with filtered domain-specific data (Extra addition from current approaches)\n    4. Fine-tune model with small task-specific data",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A framework to extract examples from the general domain data to train a language model for the target domain",
            "review": "This paper presents a framework to extract examples from the general domain data to train a language model for the target domain. The motivated observation of this work is that examples of the target domain will also occur in the general domain data. Thus, it proposes to identify these examples from the general domain data and train the language model on them only. Specifically, suppose that there is a little task data in the target domain. Based on the data, this work formulates the above task as an anomaly detection problem, where the task data is treated as the labeled data and the general domain data is treated as the unlabeled data.\n\n**Pros**:\n\n* The motivation of this work is reasonable and the studied problem is important concerning about the wide application of language model pretraining.\n* The writing is clear and easy to follow.\n* The experimental study presented in Figure 2 is interesting and inspiring.\n\n**Cons**:\n\n* According to my understanding, the proposed method in this work is a direct application of the popular instance-reweighing (sampling) framework in the domain adaptation field to the language model pretraining task. Thus, the novelty of the proposed method is incremental for me.\n\n* Moreover, I do not think the proposed method is a reasonable solution to the studied problem. The reasons are follows:\n\n    Let $p(x)$ denote the data distribution of the target domain and $p(x|\\mathcal{D}_{in})$ denote the data distribution modeled based on the target data $\\mathcal{D}_{in}$. According to the setting of this work, the size of $\\mathcal{D}_{in}$ should be small. This means that there are quite a few regions $x \\in \\mathcal{X}$ that $p(x)>\\delta$ while $p(x|\\mathcal{D}_{in})<\\delta$, where $\\delta > 0$ is a threshold described in the following. Note that the anomaly detection method will only extract examples $x \\in \\mathcal{X}$ that $p(x \\in \\mathcal{X}|\\mathcal{D}_{in})> \\delta$ as the target data. This means that the method is still not able to address the long-tail distribution problem introduced by the small size of the target data. In addition, the distribution of the selected data is determined by the general domain data but not the target data. This means that the method is sensitive to the selection of the general domain.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work on an important problem. However, comparison to important previous work is missing and some of the ideas appear in previous work that is not mentioned in the current paper.",
            "review": "The paper addresses an important problem. While large scale pre-trained \"language encoders\", trained with LM style objectives, have lead to substantial progress in NLP, they are still sensitive to domain shifts. That is, when the a model based on these models is trained on data from one domain and applied to data from another domain, it suffers from performance degradation. The authors \nalso note that we cannot expect to solve this problem through fine-tuning on large corpora from the target domain, because such corpora are not always available - a statement that  I surely agree with.\n\nThe authors propose a method based on identifying source domain sentences that are similar enough to the target domain, and fine-tuning on these sentences (after they have been re-weighted). The similarity computation is based on methods from anomaly detection which also yield the weights used during re-weighting.  Experimental results with several domains demonstrate some improvements over baselines.\n\nOn the positive side, the problem considered in this paper is important and I appreciate the will of the authors to avoid large unlabeled corpora from the target domain. Also, the connection to anomaly detection is interesting and the authors explored that connection quite thoroughly.  However, I think the paper lacks in three important dimensions: (1) Awareness of related work that should be compared to; (2) Reference to important previous work; and (3) Statistically sound evaluation of the results.\n\n\n1. Related work that should be compared to:\n\nThe authors does not discuss and compare to a very relevant method that aimed to solve the exact same problem:\n\nEyal Ben-David, Carmel Rabinovitz, Roi Reichart.  PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models. Trans. Assoc. Comput. Linguistics 8: 504-521 (2020)\n\n\nWithout a comparison to this method, which achieves large improvements on similar setups, it is very hard to evaluate the quality of the proposed method with respect to the current state-of-the-art.\n\n\n2. Reference to important previous work:\n\nImportant ideas in the paper are discussed in previous wok.\n\n- The idea of identifying the similar aspects of the source and target domains are the heart of domain adaptation with pivot-based learning. This goes back to the seminal method of:\n\nJohn Blitzer, Ryan T. McDonald, Fernando Pereira:\nDomain Adaptation with Structural Correspondence Learning. EMNLP 2006: 120-128\n\nJohn Blitzer, Mark Dredze, Fernando Pereira:\nBiographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. ACL 2007\n\nAnd has been studied in the context of deep learning by the above paper of Ben-David et al, 2020, but also in earlier work by Ziser and Reichart, 2017, 2018. \n\n- The idea of integrating a domain classifier into the domain adaptation process is the heart of the following well known work:\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor S. Lempitsky: Domain-Adversarial Training of Neural Networks. J. Mach. Learn. Res. 17: 59:1-59:35 (2016)\n\nInstance re-weighting is also a well-known idea in domain adaptation that deserves a proper citation.\n\n3. Finally, in only 2 of 9 cases in table 4 the proposed method provides an improvement of over 1% compared to the BASE method. Such small gaps require proper consideration of statistical significance, and also taking into account the multiple comparison setup (there are 9 setups and many comparisons between algorithms). Please see recent work by Dror et al., (2017, 2018) for more details.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}