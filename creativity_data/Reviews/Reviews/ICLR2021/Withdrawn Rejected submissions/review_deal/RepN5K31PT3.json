{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "All the reviewers questioned the significance of the result, in the sense that the qualitatively it is not clear how much of an improvement it is to replace \"min(S_T,C_T) with Lipschitz assumption\" by \"min(S_T,C_T,G_T)\". The authors' response on this point did not convince the reviewers. If the authors were to resubmit this work to a future conference, we encourage them to significantly expand on this point."
    },
    "Reviews": [
        {
            "title": "Questions on the significance of the result.",
            "review": "For strongly convex & smooth functions, this work considers regret against the best sequence of points in hindsight, and gives an algorithm that achieves regret that is linear in path length and square path length while making multiple gradient queries every round.\n\nComments:\n+ While discussing related work, the paper conflates two notions of \"dynamic regret\" which are quite distinct both in the nature of guarantee and techniques needed to achieve them. The first is the one paper explains -- regret against the sequence x_t^* typically scaling with the path length of x_t^*, where x_t^* are minimizers. The second is regret against ANY arbitrary sequence z_t^* with regret scaling as path length of z_t^* (again, z_t^*'s are arbitrary). A guarantee of the second kind is strictly stronger in that it implies the first (and not vice versa). Indeed, Zinkevich and Jadbabaie et al consider the second notion. Hence, the picture the paper paints of the state of prior work is not entirely accurate.\n+ Now, let us just focus on the first objective as this paper does. Here, a very simple strategy -- choosing x_t = x_{t-1}^* to be the minimizer of the last function -- works. Indeed, this immediately gives regret bounds of L (Lipschitz constant) * path length, and (smoothness) beta * squared path length. Obtaining these with single step of GD offer a challenge, but allowed multiple gradient calls, one can use them to compute x_{t-1}^* offline. In this view, I don't find the results compelling.\n+ The work mentions that previous works required a Lipschitz constant bound. In presence of smoothness, isn't a Lipschitz constant bound of beta * Diameter implied?\n+ The first experiment seems to constrain iterates to positive orthant (for regression) via update 6. Is there a reason for this choice?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Natural generalization of OMGD path-length regret; unclear justification of quantity G_T",
            "review": "Summary:\nThis paper studies the dynamic regret of online multiple mirror descent, which is online mirror descent with M repeated steps on each of T sequential loss functions. The authors show three bounds for the dynamic regret of OMMD, which generalizes OMGD [Zhang et al. '17]: C_T (the path length of the minimizer sequence), S_T (the sum of squared segment lengths), and G_T (the squared dual gradient norm of the points played).\n\nPros:\n- This work fills a gap in the literature left by [Zhang et al. '17], which considers the analogous gradient descent algorithm, and derives the min(C_T, S_T) bound that this paper generalizes. It is natural to ask whether to corresponding mirror descent algorithm enjoys the benefits of mirror descent in classical online learning, in which the choice of regularizer allows for a statistical rate to adapt to the geometry of the decision set.\n- The paper is well-written, and the analyses are crisp. There is a great deal of effort placed in including intuitive discussions along with the key results.\n\nCons:\n- I'm not quite convinced by the motivation for G_T. Multiple points below in the detailed comments, where clarifications would be appreciated.\n- Thus, overall, given my current understanding, I don't think this paper fully answers the research question in a way that is a sufficiently large delta compared to [Zhang et al. '17].\n\nDetailed comments:\n- Unlike C_T and S_T, G_T depends on the learner's decisions. So, if a learner makes very bad decisions and thus experiences large gradients (this is necessarily true by strong convexity), the G_T term becomes vacuous, and the regret bound is dominated by min(C_T, S_T).\n- The discussion at the top of page 3 (\"G_T can be smaller than both C_T and S_T, especially when the cost functions fluctuate drastically over time\") is confusing: if cost functions fluctuate drastically over time, shouldn't the gradients experienced always be large? Seems like the opposite statement is the usual motivation for path length-based regret: \"C_T and S_T can be smaller than G_T, if the functions fluctuate a lot but it's not worst-case because the minimizers are stable\". A clarification, example, or mathematical statement would be helpful.\n- Remark 4 is also confusing: if f needs to be strongly convex with a constant lambda, then its gradient dual norm can't shrink uniformly. Could the authors clarify?\n- The fact that the Lipschitzness of the loss functions isn't assumed is also not necessarily a feature. At points where f_t is highly non-Lipschitz, the gradient is large, and thus G_T (the upper bound claimed to be the novelty) is large. While it's true that the regularizer r can be chosen more freely than the losses, a poor choice leads to large L_r, so it's not clear what claim that discussion (at the end of 2.1) is making.\n\n*** post-response ***\n\nThanks for the clarifications and revisions in the manuscript.\n\n- A2: Lemma 1 (and its M-step repetition) show that the outputs of mirror descent x_{t+1} are guaranteed to be close to the minimizers of f_t. But the loss of x_{t+1} is measured on f_{t+1}; this is what I meant by \"bad decisions\". Thus I'm confused about the statement \"the learner is not expected to make bad decisions\"; Lemma 1 does not imply this. It only implies this when the consecutive minimizers are close (which is captured by C_T and S_T being small).\n- A3: This example makes sense (though in this example, the function is L-Lipschitz, not L-smooth). I also noticed that the example in Remark 5 isn't both smooth and strongly convex in any norm.\n- A4: Understood; thanks.\n- A5: I now understand (from the addendum in the paper as well) that this \"user control of L_r\" is basically referring to the standard selling point of OMD that the user can specify a regularizer that adapts to the geometry of the decision set and loss functions; I buy this point. This is separate from the ability to handle Lipschitz constants being large (which gradient descent can also handle, by having a smaller learning rate).\n\nAdditionally, I'm confused about the response to R2's \"simple strategy\". The convergence rates of GD and MD are stated in incompatible ways. The right comparison would be to pick M so that the RHS of Lemma 8 in the appendix to be smaller than \\eps. I agree that this work strictly generalizes [Zhang et al. '17], but I don't agree that this work bypasses computational difficulties of approximately minimizing a smooth & strongly convex f_t; as the authors point out themselves in a different reply, when r is quadratic, OMMD reduces to running OMGD.\n\nIn my opinion, this algorithm and analysis are potentially worthy of publication at a top venue, but the manuscript evidently needs an overhaul in its justification of the setting and G_T. Thus my overall score is unchanged.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The significance of the contribution is unclear",
            "review": "This work derives a new upper bound on the dynamic regret for online convex optimization in the restricted setting where the comparison sequence is made up of the minimizers x^_1,...,x*_T of the loss sequence. There are three main parameters that control regret in this case: the path length C_T = ||x*_2-x*_1||+...+||x*_T-x*_{T-1}||. The squared path length C_T = ||x*_2-x*_1||^2+...+||x*_T-x*_{T-1}||^2. And the sum G_T of squared loss gradient norms evaluated at x*_1,...,x*_T. \n\nPrevious work by Zhang showed that when the loss functions are simultaneously strongly convex and strongly smooth w.r.t. the Euclidean norm, then Online Gradient Descent with multiple updates per step (OMGD) achieves regret of order min{C_T,S_T} whenever G_T < T G^2 for some G > 0 (i.e., the gradient norm of the loss is uniformly bounded). Note that this algorithm invokes the first-order oracle a constant number of times per step.\n\nThis work extends Zhang's results in two main directions. First, OMGD is replaced by OMMD (which is Online Multiple Mirror Descent). Accordingly, strong convexity and strong smoothness are now computed w.r.t. the Bregman divergence induced by the mirror map. Second, the condition G_T < T G^2 is dropped. The resulting bound is of order min{C_T,S_T,G_T}. Experiments with quadratic losses compare OMMD against OMGD and Dynamic Mirror Descent (DMD) by Hall and Willet. Both these algorithms have worse upper bounds than OMMD.\n\nI have some concerns about the significance of the results. I wonder what are interesting examples of loss functions that are simultaneously strongly convex and strongly smooth with respect to a Lipschitz continuous Bregman divergence induced by a regularization function which ---in turn--- is simultaneously strongly convex and strongly smooth with respect to some norm? We know that quadratic losses and regularized logistic losses satisfy these assumptions w.r.t the squared Euclidean mirror map (as shown by Zhang). I suspect that the generalization proposed in this work does not lead to include additional interesting losses.\n\nSince OMMD improves on the OMGD regret bound, it would be interesting to know whether OMMD reduces to OMGD when the regularizer is Euclidean.\n\nThe experiments use a decision set (the simplex) where OMD with entropic regularization is known to work better than OGD. This advantage appears to be simply transferred to the dynamic setting. Indeed, OMMD performs only marginally better than DMD, which also uses the entropic regularization.\n\nIt is not even clear whether the theory applies to the setting used in the experiments. The quadratic losses used in the experiments are strongly convex and smooth only with respect to Euclidean regularization and not with respect to entropic regularization (the regularization used by OMMD in the experiments).\n\nThe assumptions require Lipschitz continuity of the Bregman divergence. Does this condition apply to KL divergence? What is K in this case? \n\nIn the ridge regression experiment, z_i is defined as a scalar but treated as a vector in the definition of the loss.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}