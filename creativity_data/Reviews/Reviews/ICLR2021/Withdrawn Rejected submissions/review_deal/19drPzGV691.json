{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers found found the paper well motivated and well written, they found both the theoretical contributions limited in novelty and the experiments too rudimentary to be insightful."
    },
    "Reviews": [
        {
            "title": "Convergance and Novelty",
            "review": "First I wanted to thank authors for putting the manuscript together, I enjoyed reading it. \n\nSummary : Authors proposed using DRL to find a good (optimal) CVaR policy, by pointing that the optimal CVaR policy is non-stationary, and DRL can be leveraged to learn and execute this kind of policies. In addition they showed just taking a max over CVaR rather than E will not result in an optimal policy. \n\nStrength : \n1. Paper is well written, it's easy to follow and it provides necessary background for the reader to follow. \n2. Important Issue : I believe finding a scalable way to optimize for CVaR is an important problem to tackle, as most of the previous work are not scalable (e.g. Chow et al)\n\nWeakness/ Concerns:\n1. Convergence: I have a concern about the convergence of the algorithm, by applying the proposed algorithm, is there any guarantee for convergence? Even in the case of policy evaluation and not control (not taking max, picking action on \\pi) is there a convergence guarantee? Or if not can authors provide intuition/ reason why is that the case?\n\n2. Novelty: Reading the paper, I challenge the novelty of current algorithm/ manuscript. Or maybe I had a hard time pinpointing it, to the best of my knowledge most of the claims have been already known, can authors please explain what they think their main contribution is? (I'm happy to change my score given the explanation)\n\n3. Experiments: The premise of using DRL for CVaR is mainly \"scalability\" so that we can solve larger problems (state space mainly). However, this is not reflected in the experiments, and experiments are in small domains. I think the paper can benefit from an experiment in larger state space. \n\nScore: \nAt this point I think the manuscript is not ready for a publication, however, I did enjoy reading it, and I think it's a great work so far with potentials. I am happy to change my score given authors response to my concerns. \n\nThanks.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of the paper",
            "review": "This paper is about risk-sensitive RL based on the CVaR risk measure. This paper is mainly based on the work presented in Dabney et al. in 2018 which is about distributional RL for a family of risk measures which includes CVaR as well. The main motivation for this work was the point that the method presented in Dabney et al. 2018 overestimates the dynamics and could be excessively conservative in certain scenarios. Authors have proposed to use static CVaR instead and have developed algorithms to do that.\n\nThis paper has solid theoretical results (propositions 1 and 2). Authors have identified a problem in Dabney et al. and proposed an algorithm to resolve it.\n\nThe major issue though, is about the evaluation and experimental results. authors have provided results on a synthetic dataset and a real dataset related to options trading. Even on the real data results, for larger values of alpha, Dabney et al. 2018 has outperformed the proposed approach. In order to make this paper ready for a venue such as ICLR, authors should provide a more comprehensive evaluation of their methods. At least, it is expected to show the performance of their approach and its comparison vs Dabney et al. on several Atari games. Otherwise, the contribution of this paper would be limited.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Marginally Reject",
            "review": "This paper consider the problem of learning a risk-averse policy base on CVaR measure using distributional reinforcement learning. The main contributions of this paper are twofold. First, they show that the standard distributional RL algorithm overestimate the dynamic, Markovian CVaR, which might be too conservative. Secondly, they propose a modified algorithm that can learn a proper CVaR-optimized policy based on static, non-Markovian CVaR. \n\nOverall this paper is well-written and easy to follow. The problem is well-motivated, and the proofs of the main propositions are clean and easy to check. \n\nHowever, I have two main concerns on this paper. First, the theory part of this paper (proposition 1 & 2) are quite straight forward, and the modifications to the existing algorithm in [1] are mild, thus the novelty of this work is somewhat limited. Second, the option trading experiment train on a mixture of real stock prices and simulated stock prices (the authors use simulated data to allow training on unlimited data), and we don’t know the exact size of training set, this seems a bit wired to me. Is there any sampling complexity guarantee for the proposed algorithm? How does the performance of the algorithm scale with the training sample size？\n\n[1]  Will Dabney, Mark Rowland, Marc G. Bellemare, and R´emi Munos. Distributional reinforcement learning with quantile regression.  In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Distributional RL for optimal static CVaR policy versus markovian/dynamic CVaR",
            "review": "This paper addresses the problem of learning the optimal policy for static CVaR with distributional RL.\nThe authors underline the difference between static CVaR which is the CVaR on the whole cumulative discounted rewards and the dynamic CVaR which is a Markovian alternative proposed by (Ruszczynski 2010).\nAs pointed out by the authors, the optimal policy for static CVaR must take into account the accumulated rewards to take decisions: the optimal policy is hence not stationary.  A nice property of dynamic CVaR is that its optimal policy is stationary.\nA key result of the paper is to show that optimal solution for the dynamic CVaR is suboptimal for static CVaR (Proposition 1).\nLeveraging on this remark, the authors propose an algorithm based on distributional (quantile regression) to solve the static CVaR problem. The obtained policy is stationary on an augmented MDP where the states are decorated with he reward collected so-far.\nThe experiments on synthetic and real data underline the relevance of the proposed approach.\n\nI really enjoyed reading this theoretical paper. It is very well written and easy to follow. The ideas presented are interesting.\nMy only criticisms 'or questions' are about the state augmentation:\n1. As mentioned on page 4 it would be sample inefficient but one could solve the augmented MDP directly. Why not provide this approach in your experiments ?\n2. As mentioned on page 5 you are using the distribution estimates to \"store the information needed\", instead of using state augmentation. But estimating a distribution is a costly task. Do you have any argument to justify that this approach is indeed more \"sample efficient\" than a naive state augmentation version without distributional estimates ? Any convergence bound ?\n\nMinor remark:\nI would be nice to have at least a small conclusion or perspective\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}