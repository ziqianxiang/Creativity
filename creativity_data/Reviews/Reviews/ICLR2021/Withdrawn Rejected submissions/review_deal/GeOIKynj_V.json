{
    "Decision": "",
    "Reviews": [
        {
            "title": "An empirical paper but the experiments and the analysis of the results is not substantial enough.",
            "review": "This work presents experimental results on Atari games comparing ConvNet vs CapsuleNet architecture for approximating Q-Value in the Deep Q-Network RL algorithm. The empirical results presented in this work can be of interest to the community as a positive indicator of the benefit of using CapsNet on a benchmark that many people can easily understand. However the suite of experiments and the results obtained on those experiments are not very convincing, nor very illuminating. \n\nMy main concern regarding the paper is about the importance of the work, CapsNet based architectures have been shown to be competitive with CNNs as a classifier. Using them to implement an agent itself does not seem to be a very novel contribution. Especially since everything else about the RL algorithm, such as the training method, is used  directly from previous work.\n\nA secondary concern that I have is that the since the paper's main contribution is a comparison of CapsNet based agents to CNN based agents in ATARI environments therefore those evaluations should be carried out more rigorously. But, the main comparison between CapsNets and ConvNets shown in Table 1 and Figure 5 do not show the variance in the performance so it is hard to tell how significant the bolded higher numbers are. These tables and figures are the main results of this paper therefore emphasis on such rigor is warranted I think. Similarly Figure 5 seems to shows the average score over 4 runs but no indication of the variance.  \n\nBecause of these two reasons I do not think that the results presented in the paper are ready for publication in ICLR.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nThe authors proposed utilizing CapsNet for vision-based reinforcement learning tasks for improved sample efficiency. The proposed CapsNet model has two capsule layers and two following fully connected layers for action-value prediction.  They trained the model by the standard routing algorithm and DQN algorithm with prioritized experience replay. Empirical results on nine selected ATARI games demonstrate that the proposed CapsNet based model is much more parameter-efficient and achieved competitive game scores.  \n\nPros: \n\nThe paper aims at addressing the data efficiency challenge from a model perspective for deep reinforcement learning algorithms for vision-based task domains. The proposed model is much more compact compared to the CNN alternatives while achieving competitive performance. \n\nCons: \n\nMy main concern about this paper is its novelty. The training algorithms are standard. The proposed CapsNet for RL tasks resembles the one by (Andersen 2018) except that they added additional fully connected layers for potential negative-value treatments. \n\nThe empirical results are also selective. An extensive evaluation of ATARI's full suite would help demonstrate the effectiveness of the proposed method. Given the current learning curves, the data-efficiency story is not fully delivered as the CapsNet curves mixed with CNN ones initially. Additional results against more recent DRL baselines or those closely related ones, such as the mentioned A2C+CapsNet approach, would be informative and meaningful. \n\nThe authors could improve the presentation of the paper. The capitalization of DL and DRL is not consistent. Some details of the evaluation score computation are not specified, such as the number of trajectories used in the average score computation.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "CapsNets-based Deep Q-Networks for Games - Good work but falling short",
            "review": "Authors present an empirical study comparing CapsNets - and CNN- based Deep Q-Networks using Prioritised Experience Replay.\nIt is true that not much work has been done in Deep Reinforcement learning and CapsNets in this context; the paper provides a nice overview of how the two different approaches compare against each other when employing simple architectures. \nThe evidence show that CapsNets-based DQN outperforms CNN-based one, achieving this with about 8-9% of the number of trainable parameters. \nThere is no excessive hyperparametrisation involved, nor is it intended to propose a new architecture or so, as the authors use the vanilla CapsNets with Dynamic routing. I think results would have been different should different CapsNets approaches have been used; but that is not the main point of this paper. Nevertheless it is something authors could have touched upon to some extent. My thinking is that it could produce considerably different estimates and hence drifting the conclusion and discussion accordingly.\nThe hypothesis that CapsNets learn better the states is interesting as well but it would be prudent to evaluate this further.\nI am not sure whether the paper's originality and significance is at the level required for acceptance at ICLR.\nIn my opinion it is a good empirical work but lacks novelty.\n\nMinor:\nFigure 4: top is Capsnets not the other way round",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper uses CapsNet to replace CNNs in the area of reinforcement learning. The quality e.g., screenshot figure, the novelty of this paper is far from being considered in publishing at ICLR.",
            "review": "This paper discusses the use of CapsNet for replacing CNNs in reinforcement learning (RL). The quality of the paper needs to be substantially improved e.g., avoid screenshot figures and the novelty of this paper is not enough for ICLR.\n\nConcerns:\n1. Contributions of this work are not enough. The authors just replace CNNs with CapsNets and simply compare these two different set-ups in RL. It is too simple. I think it needs to propose some modifications on either architecture or learning strategy for CapsNet, which makes CapsNet is more compatible in RL tasks.\n2.  Experiments are not enough. The authors only compare the performance between CapsNet and CNN. More ablation studies are required e.g., different learning strategies, different architectures.\n\n\n\nPaper quality:\n1. Figure quality. Lots of figures are screenshots. These are not acceptable. e.g., Fig.2, Fig.3.\n2. Related work is not enough\n3. Background should be a part of the introduction.",
            "rating": "2: Strong rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}