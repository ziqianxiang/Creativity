{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Reviewers were concerned about the technical novelty because the two-stage sampling strategy is similar to BBN and the decoupling of features and classifiers. Rebuttal addressed some concerns about the experiments, but Reviewers' major concerns remained. "
    },
    "Reviews": [
        {
            "title": "Good paper with some concern on the novelty and experiments.",
            "review": "This paper proposes that decoupling feature representation and classification is not the optimal solution for the long-tailed recognition problem. They propose to train the model with instance-balanced sampling first, and switch to reverse sampling at the last epochs with a small learning rate, so that the tail classes could be better represented than decoupling approaches (Kang et al. 2020) while the whole framework is much simpler than BBN (Zhou et al. 2020). The results are promising on CIFAR and iNaturalist.\n\n#### Pros:\n1. This paper is well written and easy to follow. It presents the ideas clearly.\n2. The proposed approach is simple and effective on CIFAR and iNaturalist, and the ablation study and analysis is very comprehensive.\n3. It brings some insights into the long-tail problem.\n\n#### I have some concerns:\n1. The novelty is not enough for me. This paper seems to be a soft version of the decoupled approach (Kang et al. 2020), by combining the reverse sampling strategy proposed by BBN( Zhou et al. (2020)). Since it first trains the network normally, and then switch the sampling strategy and trains the whole network with a small learning rate, actually, the feature representation part is almost done in the first instance-balanced sampling stage, and is just fine-tuned a little with reverse sampling.\n2. About the experiments, \n(1) I wonder why the authors did not conduct experiments on ImageNet-LT, which is considered to be one of the most challenging datasets. It will be more convinced if they show that the performance boosts on ImageNet-LT.\n(2) All the experiments are based on ResNet-50. Will we still get improvement on stronger networks? Or the improvement will be absorbed by the larger backbone? Please let me know if I take it in the wrong way.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThis paper aims to address the problem of long-tail classification by revising the data resampling strategy. The main contribution is that it considers the representation trade-off between the head and tail classes. To this end, it combines instance-balanced sampling and class-reversed sampling, by first conducting training using instance-balanced sampler for several epochs and then switching to class-reversed sampler to continue training for the last several epochs. The approach is tested on three long-tail classification benchmarks, including CIFAR-10, CIFAR-100, and iNatualist 2018, and compared with state-of-the-art results.\n\nStrengths:\n\n1. The authors tackle an important and challenging problem of long-tail classification.\n2. The proposed approach is simple and interesting.\n3. Experimental evaluations demonstrate the effect by introducing the mixed strategy of instance-balanced and class-reversed sampling.\n\nWeakness:\n\t\n1. While the authors claimed that they challenged the hypothesis by Kang et al. that the learning of feature representation and classifier should be completely decoupled in long-tail classification, from my perspective this paper is a nature extension of Kang et al. Similar to Kang et al., this paper further demonstrates the importance of progressive learning to address long-tailed distribution, which first mainly focuses on head classes (first stage) and then on tail classes (second stage). Different from Kang et al., a) this paper uses more aggressive sampling strategy at the second stage, by replacing class-balanced sampler with class-reversed sampler, and b) this paper shows that the feature can be further fine-tuned at the second stage.\n\n2. This paper mainly reported the overall performance. It would be more convincing to provide the per-class performance to show the improvement on the tail classes.\n\n3. This paper mainly focused on the comparison with Kang et al. Since Kang et al. extensively conducted experiments on the long-tailed ImageNet and Places datasets, how does the proposed approach perform on these two datasets?\n\n4. While the time to switch from instance-balanced sampling to class-reversed sampling is a hyper-parameter as the authors mentioned, I was wondering if there is any principle to guide this design choice. Also, if this is dataset/distribution dependent or sensitive.\n\n5. Consider an even smoother transition, from instance-balanced sampling to class-balanced sampling and finally to class-reversed sampling. Will this combination outperform the strategy in the paper?\n\n6. In the discussion section, the authors claimed that the near-optimality of their method and that little room left for the improvement of the successive resampling strategy. This seems a very strong argument. I was wondering if there is any formal theoretical guarantee on this.\n\nPost Rebuttal:\n\nI do appreciate the efforts and additional experiments and theoretical analysis that the authors made in the rebuttal. While this paper proposed an interesting approach to long-tail recognition, some connections, distinctions, and comparisons with related work and thorough experimental analysis were missing in the original manuscript, as mentioned by other reviewers as well. Some of these concerns were addressed in the rebuttal, but not fully clarified. For example, the new comparison on the more challenging ImageNet-LT dataset (Table 3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited Novelty",
            "review": "##########################################################################\n\nSummary:\n\nIn short, the main contribution of this paper is to jointly combine the instance-balanced sampler and the class-reversed sampling in the training, where the latter is only applied to the last few epochs. It also provides a viewpoint of the long-tailed problems from the memorization-generalization mechanism.\n\n##########################################################################\n\nPros:\n\n+ The proposed method is indeed very simple.\n\n##########################################################################\n\nCons:\n\n- From my point of view, this paper is just a special case and a simple version of BBN [BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition, CVPR, 2020]. The main contribution of this method is jointly combining the instance-balanced sampler and the class-reversed sampling in training, which is exactly how the bilateral-branch network works.\n- Since BBN and Decoupling have already thoroughly discussed the trade-off between instance-balanced sampler and class-reversed sampling, I don't think this paper provides any new ideas for the later researchers.\n\n\n##########################################################################\n\nQuestions during the rebuttal period:\n\nOne piece of advice: try to make the introduction more concise in your future work. The exact same sentence \"We only switch from instance-balanced sampler to class-reversed sampler for the last several epochs of training.\" shows three times in the introduction, which makes the audience question the limited contribution of this paper.\n\n##########################################################################\n\nReasons for scores:\n\nSince the novelty is limited, and the points made by this paper have already been well discussed by previous work BBN CVPR 2020 and Decoupling ICLR 2020, I decide to reject this submission.\n\n##########################################################################",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #1 ",
            "review": "This paper investigates long-tailed visual recognition from a memorization-generalization point of view.  A simple yet effective combinational sampling method is proposed to tackle long-tailed learning trade-off via overexposure of tail samples in the late stage of training. Specifically, switching instance-balanced sampler to class-reversed sampler for the last several epochs of training can help networks learn tail classes of low-regularity. Experiments show this method can reach sota performance more efficiently compared to baselines. \n \nPros: \n\n+ This method is simple yet effective, and can be plugged into almost any existing methods to improve the robustness of low-regularity classes.\n\n+ Compared to previous sota methods (BBN & Decouple) which adopted a 2-stage training strategy, this method is trained end-to-end.\n\n+ This paper empirically identifies that low regularity of tail classes is the primary hurdle for long-tailed learning and can be alleviated by appropriate memorization mechanisms.\n\n+ This paper provides complete experiments and ablation studies to demonstrate the effectiveness of their methods.\n\n+ The overall writing is clear, and the method is easy to understand.\n \nCons: \n\n- Though effective according to the experiments, this method seems incremental and to some extent lacks novelty. The idea of feeding more tail-classes shares similar spirits with BBN, which uses class-reversed sampling to re-balance the feature extractor. Although authors of this paper may adopt a simple architecture design compared to previous methods.\n\n- Regarding the experimental results, there is a 2% improvement on Long-tailed CIFAR-100 with imbalance ratio of 100 and 50, but considering other settings, especially long-tailed CIFAR-10, the gain is far from “unreasonable effective” as indicated by the author. Meanwhile, this new sampling-switching method may perform even or worse compared to baseline in some settings.\n\n- This paper relates the idea of adopting CR sampling in later periods of training to recent research on memorization mechanisms, but the authors didn’t provide some theoretical explanations that can more densely link his proposal into memorization. A deeper analysis of the proposed method would have been nice, such as a theoretical lower bound of decision boundaries of tail classes after adopting the reverse class sampler compared to the instance sampler. In general, the authors need to provide more insights into this problem as well as his proposal. This method seems somewhat incremental now.\n\n- Hyperparameters such as S seem to be sensitive.\n\nMinor comments: \n\n* More comparisons against recent research on long-tailed problems could be added, as well as the references.\n\n**Post-Rebuttal**\n\nI appreciate the authors' efforts to make this submission much more solid. The investigation of the tail sample memorization phenomenon is insightful and would be beneficial to the community. However, the experimental results on standard benchmarks (e.g. ImageNet) are still not convincing enough. Thus, I will only increase my score to 5 (marginally below acceptance threshold). I definitely encourage the authors to further polish the manuscript and submit to future venues.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}