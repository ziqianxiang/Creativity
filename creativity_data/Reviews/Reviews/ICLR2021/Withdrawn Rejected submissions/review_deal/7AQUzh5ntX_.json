{
    "Decision": "",
    "Reviews": [
        {
            "title": "Paper achieves good results on the chosen tasks. Why were these tasks the ones chosen?",
            "review": "Summary\n==========================================\nThis paper describes a way learn an attention mechanism for objects in a 3D environment without explicit supervision. This attention mechanism allows the proposed network, ROMA, to outperform existing approaches without having access to the ground truth state of the object.\n\nScore\n==========================================\nOverall, I recommend accepting this paper. My biggest concern is that the tasks performed in the paper lack variety in that they are all kitchen based tasks moving small objects. I am curious about tasks that have further dependencies where both objects may not be visible from the same viewpoint. The paper is well written and it does give results on 8 different tasks even if they are similar. The approach gives empirically good results on the chosen tasks with low sample complexity. I would like to see results on 1 similar, but distinct category of tasks (like simple, single object navigation).\n\nPros\n==========================================\n- This paper is able to learn to perform these tasks without imitation learning\n- The approach is more sample efficient than the baseline\n- Able to learn these tasks with rich object designed feature\n- The figures are very clear and readable.\n- The paper demonstrates a strong baseline that outperforms unexpectedly OCN.\n\nCons and Questions\n==========================================\n- The AI2Thor scenes are small enough that the object is often almost always visible. I wonder if ROMA would be able to perform as well in environments where the ground truth desired objects aren't always visible. Would ROMA learn well if it needed to explore in larger environments?\n- How do these tasks compare with those that networks like OCN were intended to be used? I am very surprised OCN does worse than the baseline. Are the authors confident its not a hyperparam problem?\n- All of the objects are small hand-held objects, would it be possible to add some experiments using larger objects? I wonder if the attention component of ROMA is so helpful because of the relative size of the objects. Would ROMA perform just as well when one of the objects saturate the agent's field of view? Or is the attention component only helpful when the objects are small as they are in the constructed tasks.\n- \"We report results on the 8 tasks that had the highest optimal length.\" What other tasks were evaluated? Why was choosing tasks that had high optimal length important? Does ROMA perform poorly on tasks that have a low optimal length? Is it that ROMA is designed on tasks with high optimal length? \n- This paper clearly demonstrates where ROMA outperforms other techniques (with these compositional object tasks). Which tasks does ROMA under perform on compared to these methods, if any? Do these other techniques perform well when no composition of objects is required?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "- Summary\n    - This paper presents an architecture for learning object interaction tasks in AI2 Thor with sparse rewards.  The architecture utilizes attention to select actions with an object centric representation.  To handle reward sparsity, the authors propose an auxiliary objective that predicts the visual state of a given object in the next frame given an action applied as formulate it is a contrastive loss.\n    - They evaluate their method on 8 tasks in one scene in AI2 Thor and outperform the baseline methods while also performing similarly to an agent equipped with ground truth object information\n- Strengths\n    - Proposed method is able to learn under sparse rewards\n    - Proposes a novel auxiliary objective for object centric RL\n    - Evaluated in a reasonably complex setting\n- Weaknesses\n    - The evaluation is performed on a single scene and the same scene is used for both training and testing.  The work would be considerably stronger if a train/test split was used and ideally all scene types.\n    - \"We assume no access to supervision.\"  I disagree with this claim.  The object detector, while only providing object proposals, was trained on some supervised data.  On what data was the object detector trained (I couldn't find these details in the paper or supplement)?  If it was trained on COCO or similar, an argument could be made that this shouldn't count.  Further, can the authors please confirm that all methods compared benefit from equally from this source of supervision?\n    - I would also like to see an ablation as to how the method performs as object detector performance is degraded\n    - One weakness with this work is the premise that we should learn with sparse rewards.  In the case of learning in reality, shaped reward may not be available, but when learning in simulation, there is a wealth of information available for reward shaping.  It is unclear to the reviewer if the effort to design an effective shaped reward is any more than the effort to design a model that doesn't need one.  It is also unclear if methods trained with a sparse reward transfer to reality better than those trained with a shaped reward.  Given the limited number of scenes the method was evaluated on, I also cannot assess whether the method is likely to work well in reality.  Given all this, I am left wonder why one should use a sparse reward.\n- Overall\n    - While the proposed method seems to be effective, the evaluation is limited and there are issues that need to be addressed before publication.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not bad method, but weirdly easy self-made tasks for an embodied agent setting",
            "review": "## Summary\n\nThe authors are trying to solve single-task object interaction problems in a 3D environment (e.g. \"find and slice the lettuce and the tomato\"). The proposed method involves learning a Q function for object interactions, a self-supervised attention mechanism, and an auxiliary loss for an object-centered forward model.\n\n## Strengths & Weaknesses\n\n#### Strengths\n\n1) The proposed model performs well in the experiments and reaches a high success rate.\n2) I'm not an expert in this specific sub-field of embodied agents but upon skimming the baseline papers, they look like sensible choices.\n\n#### Weaknesses\n\n1) It came as a bit of a surprise to me that the authors created their own tasks for this and how simple the tasks are. I thought in the embodied agent community we had moved to task-/goal-conditioned policies (see e.g. ALFRED [1] and their top contributions on their leaderboard). In this paper, the authors created multiple tasks, but any single agent only has to solve one task, from different starting positions. Maybe I misunderstood something about the task definition, but a simple DQN can learn most of the tasks in 0.5 mil steps with over 80% success rate. I appreciate the contribution in the sense that your network does seem to learn to pay attention to relevant object patches - I had just hoped you would be able to take that and apply it to a new setting, e.g. test if the attention generalizes to the same object in a new apartment/environment.\n2) The are some clarity problems that aren't a big deal in themselves but add up to a slightly opaque method. I think these are easier to overcome than my other concern: (a) a network diagram would've been nice. I know there's a table in the appendix, but I think it's crucial that the main paper can stand on its own and it's not entirely clear from the writing, how many networks there are and how they work together. (b) Is Eq.6 the full loss? You're only training on the Q values and the encoding of the objects in the next step? (c.) You removed the task description from 5.1 and put it into the appendix. I don't think this is good practice. If you were using a standard method like OpenAI Gym's HalfCheetah-v2 or something that's common in RL literature, then I would agree that the task description could go into the appendix but since you created these tasks from scratch, I think *the main paper needs to contain information about the tasks that you're evaluating your model on.* (d) Clarity question: The objects are always in the same position and the main difficulty stems from the fact that the agent starts in different starting positions, right? If that's the case, in order to assess the difficulty of the tasks, would you mind adding the success rate of a random agent, please? And what's the average starting distance of the agent from the first required object (in terms of actions, not grid spaces)? (e) Figure one has a bit of a misalignment between the drawing and the caption. The drawing is good, but the caption talks about reusing $\\mathcal{R}$ in $f_{model}$ but how that's done is not depicted, $f_{int}$ is only explained in the text later, and you mention computing $Q$ values, but it's not highlighted in the diagram \n\n**TL;DR how to improve the paper:** Please add more implementation details, as outlined in (2) and if possible, and please comment on how goal-conditioning would work in this setting or report test scores, where you test your trained policy on the same task but in a different configuration (different apartment/different positions of objects).\n\n## Impact & Recommendation\n\nAs mentioned above, the method seems fundamentally sound but I have 2 concerns about the impact and novelty: (a) goal-conditioned tasks (i.e. environments where the agent receives varying instructions) seem to be the standard for embodied agent works these days and this paper takes a step back from that and (b) the main selling point of the paper is that it doesn't require expert demonstrations or object supervision but that data is available, so why not use it (see [1] - ALFRED can generate expert solutions for any given task, and there's ShapeNet of course, as well as several other 3D and 2D object segmentation datasets that allow for pretraining)? How is it more realistic to have an embodied agent run the same environment 10,000 times than to pretrain the agent on objects or seed it from a handful of expert demonstrations?\n\nI think that a lot of work has gone into this work. For now, I'd recommend rejecting this work but I hope the authors can shed some light on how they are positioning themselves and how to better motivate this work so that I can raise my score.\n\n\n## Questions, Nitpicks, and Comments\n\n- I appreciate that you start with an example. Always great. But maybe take a more interesting example than warming up a single potato. :D Do the toast for example. It's a reasonably complex problem: find bread, take bread, find a toaster, insert bread, wait for the bread to finish toasting.\n- I don't know if it's clear to everyone that the reward of +1 is only given at the very end of that episode when the bread is golden brown or the potato is hot. There's no intermediate sparse reward for finding the potato/pot/bread/toaster, etc. I think it'd be great to highlight this in the introduction a bit more.\n- What does \"discriminative across object-states\" mean in the intro?\n- In section 2: You mention \"In contrast, we do not provide the agent with any ground-truth object information\" - but you do, you give the agent patches/bounding boxes.\n- In section 5.0, you're already giving away the conclusions of your experiments. I'd wait with that until the reader has actually seen the results.\n- Fig. 3 you write \"Percent Million Frames\" on the x-axes but you can skip the \"Percent\", right? It's just 0 - 0.5 \"Million Frames\".\n- couple of small typos like \"...mean success rate success rate\" top of page 7.\n- Table 1 why does \"fill cup with water\" performs so well on DQN and why is your method beaten by the most naive baseline here? \n- Section 5.3: \"We suspect that this is due to its ability to bootstrap object-attention.\" - could you explain this, please?\n\n\n## References\n\n[1] https://askforalfred.com/",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting direction, but poor experiments",
            "review": "**Summary:**\nThe paper introduces an approach for performing tasks in interactive environments. The goal of the paper is improving sample efficiency by incorporating information about multiple objects into Q-value estimation and also discriminating object states according to the interactions. The paper considers 8 types of tasks in the AI2-THOR environment and reports success rate for each of the tasks. The paper also provides the results for a number of baselines and ablation experiments.\n\n**Strengths:**\n- The paper explores improving sample efficiency for long-horizon interactive tasks, which is a very interesting direction.\n- The ideas for incorporating object relations and states are also interesting.\n\n**Weaknesses:**\nThe major issue of this paper is the experiments:\n\n(1) The same environment has been used for training and testing. Train and test environments should be separate from each other.\n\n(2) The generalization of the method has not been evaluated since the same environment has been used for train and test. Sample efficiency is important but not without generalization. A large capacity model probably overfits much faster than any of the proposed models and baselines. The issue is that it might not generalize.\n\n(3) Only 1 scene out of 120 scenes of THOR has been used. The paper should consider a larger set for reporting the results.\n\nOther comments:\n\n- The paper claims that the method is unsupervised but a pre-trained model is used for extracting bounding boxes for objects.\n\n- The paper contrasts the method with imitation learning, but it uses groundtruth location information across the trajectory. That is also considered strong supervision.\n\n- I recommend using datasets such as ALFRED (Shridhar et al., CVPR 2020) which includes a variety of tasks and separate train/val/test splits.\n\n- I also recommend using more sample efficient RL techniques such as PPO to see if the proposed method provides any improvement over those or not. \n\n**Score justification:**\nThe paper explores a very interesting direction, but the experiments are flawed as explained above. So I choose a reject rating.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}