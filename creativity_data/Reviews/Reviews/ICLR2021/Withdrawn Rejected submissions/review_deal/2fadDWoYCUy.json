{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting motivation, but experimentation setup doesn't align with the goal.",
            "review": "The paper's primary contribution is a framework to fine-tune an on-device model in a streaming fashion by maintaining a buffer of unlabeled data samples. The authors present a data replacement strategy called Contrast Replace, which seems reasonable as it is smarter than the other two heuristics (random replacement, and FIFO replacement) which do not take into account how well the encoder has learnt to represent the given image.\n\nHowever, I have a bunch of questions around the experimentation setup:\n\n* The authors mention that this technique would be useful for additional improvement on top of an existing model already deployed on-device, however the experimentation seems to demonstrate training from scratch. How does this method improve a baseline model with a certain accuracy? Regardless, what is the upper-bound for a model trained with the same network with labeled data?\n\n* \"When training, the encoder is kept frozen and the linear layer is trained with standard cross-entropy loss.\". It seems that the encoder is trained first, and then the linear layer is trained next. However, when deployed on-device the framework will only be able to tweak the encoder. Are there results simulating that process?\n\n* Why is the max buffer size limited to 256? The network seems to improve with increase in the buffer size, and we don't see a plateauing of the results yet with the sizes reported. Since typically images are down-sized before providing as an input to the network, they can directly be stored as such in the buffer even on-device. At what stage do the results stop improving?\n\n* They mention that \"If strong data augmentation such as random crop, random color distortion, and random Gaussian blur were used here, the score will change due to the randomness introduced in these augmentation techniques.\". Isn't that the point of augmentation: to learn encodings that are robust to such augmentations? It is not clear why such augmentation techniques are helpful in regular image classification, but do not perform well in their setup.\n\n---\n\nOverall, while the motivation is interesting (\"deploy a model and tweak it with unlabeled data to improve the accuracy\"), I am not convinced by the experimentation setup. It does not accurately simulate what the authors claim the usecase to be. The idea has promise, but I encourage the authors to consider addressing the feedback.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an approach for on-device contrastive learning by selecting samples from input streams. In particular, the authors proposed a contrastive scoring function which computes scores for all samples in a given batch. If the image representation is similar to its horizontally flipped variant,  such sample will be considered as bad samples. On the other hand, if representations from the two views  are different, the corresponding samples will be considered as good samples, which are further used to update the model. Experiements are conducted on CIFAR/ImageNet.\n\n####### Strengths######\n+ The idea of enabling on-device learning in an online learning setting for streaming data is interesting.\n+ Good results are shown and the proposed approach offers better performance compared to FIFO and random replacement baselines.\n\n#######Weakness######\n- The training details are confusing. Currently, the training details are shown in the appendix. However, I think they are too important to be put in the appendix. I'm not sure what is training objective for the whole framewok. With the constrastive scoring, you would have a batch of samples that are used to update the model (as shown in the right part of Fig 1). Yet, it is confusing to me what's being updated. Are you updating the encoder for contrastive learning (SimCLR) or are your updating the linear classifier? \nIf they are used for contrastive learning, how would you train the linear classifier?\nThe training details in Appendix A.1 is really vague. What is \"We train the encoder from scratch with the SimCLR\". Based on what set of data the encoder is trained?\n\n- Following the previous point, if the selected samples are used for updating the encoder to achieve contrastive learning, what is the motivation for doing contrastive learning in an online manner?\n\n- Comparions with online learning methods are missing. The authors mentioned \"Kindly note that learning from streaming is a challenging task and existing works (Aljundi et al., 2019; Lopez-Paz & Ranzato, 2017; Nguyen et al., 2017) are usually\nevaluated on labeled small-scale datasets such as variants\" Comparisons with these methods are needed.  The authors claimed that they focus on \"unlabeled streaming from subsets of large-scale ImageNet\". But how would you linear classifiers if you don't have labels?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper investigates how to selectively choose subsets of data to train a contrastive learning-based self-supervised model. The main contribution is a scoring strategy to determine whether or not to keep samples for training. The score is determined by the distance in embedding space between a sample image and a horizontally-flipped version of the same image. More useful training samples will have a greater distance as the network hasn't yet learned that the semantic content in both images is the same.\n\nExample settings that motivate this work would be robots or UAVs that constantly stream in new images but cannot save them all. To benchmark in a similar way, standard image datasets are used like CIFAR but samples are presented in a non-IID way: consecutive images are artificially correlated by presenting sets of images that belong to the same object class. This is to simulate the stream of temporally-correlated data that would occur with a robot in the wild. The network must identify which of these samples to save in a small data buffer to then train on.\n\nThe two baselines used for comparison are to either 1) take in all images as they are provided or 2) randomly select subsets of the images to update the current buffer of samples. Using the proposed contrastive score to choose samples leads to faster training and better convergence than the two baselines.\n\nStrengths:\n\n+ the contrastive score is a good idea for picking out training samples. It is simple to implement (only requiring inference with a normal and flipped version of an image), and it makes sense why it would be informative for indicating useful training samples in contrastive learning\n\n+ from the provided CIFAR and ImageNet results choosing samples based on this score clearly leads to faster training than the more naive baselines\n\nWeaknesses:\n\n- no existing active learning approaches are used as baselines. Outperforming the two naive heuristics (that do nothing) is not as strong a case for this method, and there is so much literature in active learning with simple strategies to identify good training samples. Many of these methods could be applied in this context, I find it surprising that not one existing method could be used for comparison.\n\n- the experimental setting feels a bit contrived. For example, I don't understand why the buffer is so small. Only being able to save a single batchsize worth of samples (8-256) seems unnecessarily over-restricted and it is unclear that the behavior of the system would be the same given a much larger buffer.\n\n- is the correlation of samples (STC) the only reason that choosing samples based on the contrastive score should help? It is surprising, it seems as though this method should be useful even with IID samples from the full dataset.\n\n- when STC is low (Table 2), this should approximate normal pretraining on the full dataset. Shouldn't the numbers be much closer to those of (Chen et al)? Is it the difference in training epochs?\n\nOverall:\n\nI think that the proposed strategy seems like an interesting, simple technique to identify useful training samples in contrastive learning, but I find the lack of comparison to any active learning literature concerning and am uncertain about the particular evaluation setting. It seems that there should be evidence of the merits of this technique with fewer artificial constraints (e.g. larger data buffer, no temporal correlation of samples).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}