{
    "Decision": "",
    "Reviews": [
        {
            "title": "Initial Review",
            "review": "[General Review] \nIn this paper, the authors focus on open-domain question answering and examine whether the retriever is just a fast approximation for the reader. Some previous works find when the reader model in an open-domain QA system reads more top-k documents, the system accuracy somewhat decays. The authors give an explanation of this phenomenon that the retriever observes more negative examples than the reader. They conduct experiments to demonstrate the retriever and the reader are complementary to each other. Based on this conclusion, this paper proposes a method to distill the knowledge of the reader into the retriever. Experiments show such a method improves retrieval performance and leads to better accuracy for the question answering system.\n\nHowever, I think the motivation for this paper is questionable. The experiments and explanations in Section 3 are not persuasive for me. Besides, there is nothing different of the distillation method proposed in this paper than the method in other tasks. The results in Table 1 and Table 2 show the improvement of this method is slight. So I think the contribution and novelty of this paper are not enough for ICLR publication.\n\n[Strengths] \n1. This paper starts with a good question of whether the retriever is just a fast approximation for the reader.\n2. This paper is well-written and well-organized. \n\n[Weaknesses] \n1. In Figure 2a of Section 3.2, the authors compare the recall of the retriever and EM of the reader, which is not reasonable. The recall and EM are different metrics and should not be compared directly (or the authors should elaborate the appropriateness for this). For the retriever, a higher k obviously leads to a higher recall. For the reader, when we feed more candidate documents to the reader, it is more likely that the answer contained in these candidates documents. However, more candidate documents also introduce more distractor documents. It is a tradeoff of recall and accuracy of the candidate set for the reader. Anyway, it is incorrect to directly compare the recall of the retriever and the accuracy of the reader. Such experiments can not support any conclusion.\n2. The results in Section 3.3 are somewhat trivial. It may be well-known for other open-domain QA researchers that negatives retrieved from retriever lead to better reader performance. Besides, I don't understand why such experimental results can demonstrate the retriever and reader are complementary.\n3. Due to the discussion above, I think the motivation of this paper is questionable.\n4. The authors said the two-tower architecture enforces the information in the question or the document to be bottlenecked by their embeddings, which can cause the loss of information. However, the drawback of two tower architecture can not be eliminated by the method proposed in this paper.\n5. The distillation method used in this paper is widely-used. Besides, this paper lacks the necessary analytical experiment to demonstrate why the method works and what knowledge is transferred from the reader to the retriever. \n6. The improvement in Table 2 and Table 3 seems to be marginal. In table 3, I think it will be more persuasive if the authors can provide the results of RDR and original DPR under different top-k setup.\n\n\n[Questions for Authors] \n1. If the retriever and the reader are complementary, why not conduct an experiment that distills the knowledge from the retriever to the reader?\n2. Can you explain or empirically demonstrate the retriever and the reader are complementary in what aspect?\n3. Can you explain what kind of knowledge is transferred to the retriever? Can you conduct an experiment to prove this?\n4. The paper claims that the distillation method is model-agnostic. However, I still have no idea how to adopt this method to other works. Can you provide more experimental results that adopt this method to other retriever models?\n5. The results in Table 1 and Table 2 are confusing. Why the DPR with RDR shows significant improvement but slightly decrease after removing the distillation method in Table 2? Why are the results of DPR-Single worse than the results of the original DPR?\n\n[Minor Comments] \n1. What is the difference between the $D_{ret}$ and $D_{read}$ in Section 4.1?\n2. What is the difference between the DPR-Single and BM25+DPR? As far as I know, the original DPR is trained with negatives retrieved by the BM25 method.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea and results but lacks rigorous experimental details and comparisons",
            "review": "Summary:\nThis paper studies the effectiveness of the retriever component in the context of open-domain question-answering. More precisely, the paper explores the setting where the retrieval score is modeled as the dot-product from the dense representations of two separate BERT models: one for the query and one for the context. The authors first propose an hypothesis arising from the fact that an efficient retriever based on dot-product of dense representations isn’t just an approximation of the reader component, which is based on the full attention ($O(N^2)$) between the query and context. Second, they present a knowledge distillation approach, where the reader is the teacher network and the retriever is student network. By distilling the knowledge from the reader to retriever, they show improvements in the retriever recall@k metric which further improves the reader’s performance.\n\nPros:\n - The distillation approach presented in the paper looks interesting and well-motivated. From the experimental results, the distilled retriever substantially improves over the performance over its non-distilled counterpart on the Natural Questions and TriviaQA dataset,s especially in the setting where took is small such as 1 or 20.\n\n- The distilled retriever is able to retrieve more relevant top-k documents and due to this the performance of the reader model also shows consistent gains.\n\nCons:\n - In Sec 3.2, there can be many reasons why the accuracy of the reader falls when the number of retrieved examples are increased. One reason is might be due to the ambiguity in the question and the ground-truth answers. Given the context of the paper, this observation in Fig 2(a) needs an in-depth analysis. It feels that currently the paper lacks qualitative examples of error analysis, which could explain why there is a QA accuracy drop.\n\n- The distillation objective function is not clearly explained in the paper. For example, it is not defined how the similarity function is calculated for the reader model and it is referred to the original DPR paper. Such critical details should have been included in the main paper itself.\n\n-  The experimental training setting details of the distilled retriever is not included in the main paper and it is unclear how it was trained. There is also no discussion of hyperparameters, it’s tuning, sensitivity of the model to different hyper parameters etc.\n\n- The performance comparison with the state-of-the-art models and associated discussion should be kept in the main text.\n\n- It seems that the distilled retriever performs much better compared to the baseline retriever because it’s ability to do document ranking has improved. This concept has some familiarity with BERT’s full-attention-based document reranking, where first a larger number of documents are retrieved which are later reranked. One reference which describes this in detail is: https://arxiv.org/abs/2005.00181. It would have been very useful to include the comparison of the distilled retriever with full-attention reranking to fully understand the reason for it’s improved performance.\n\nSome writing issues\n\nIntroduction Section:\n- (minor issue) Why is the Knowledge Graph in camelcase? \n- I feel that the comparison of one-tower and two-tower models with kernel SVMs is rather vague and not well-motivated. Instead, the authors should cite works (if exists) which show that in the context of information retrieval, the one-tower BERT model has the capacity to better preserve document rankings compared to the two-tower BERT model.\n- References/Citation Issues:\n  In the submission, the REALM paper has been reported to be accepted in ICML 2019. However, REALM was accepted in ICML 2020 conference.\n This paper is very relevant and can be cited in the related works: Pre-training Tasks for Embedding-based Large-scale Retrieval, ICLR 2020, https://arxiv.org/abs/2002.03932\n\nRelated Work Section:\n- Open domain QA: Why do you think the answers of pertained LMs unreliable and uninterpretable? Is there a reference which you can quote here?\n- Section 3.1, 2nd paragraph: Why is this a classic challenge?\n\nResults Section:\n- Section 3.2, text: The red graph in Fig 2a shows the accuracy of the reader at k=1,…,2000. However, the plot shows the accuracy from 1,…,1000. \n- The results of NQ-dev and NQ-test appears to have correlation. And as such, only comparing the performance on NQ-Test would have been sufficient.\n- In the light of Table 1, Figure 3 doesn’t really add to more information.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting but not convincing",
            "review": "Interesting but not convincing\n\nSummary: \nThis paper investigates whether the current two-step retriever-reader approach in open-domain question answering can be simplified into the reader only approach. They find the two-tower retriever has its own effect in open-domain QA – replacing it with the one-tower reader model harms the overall QA performance.\nThen the authors propose to distill the reader knowledge into the retriever using a standard teacher-student approach. Results show that the retriever recall and subsequent reader accuracy can be improved significantly when the number of retrieved passages is extremely small (such as 1) while the improvement is marginal when the number of retrieved passages increases to 10 or larger.\n\nStrong Points:\n1.\tIn general, it is interesting to investigate the functionality of the retriever and reader in open-domain question answering. Although there are some end-to-end models in open-domain QA, it is still unclear whether the reader can take place of the retriever completely.\n2.\tThe idea of distillation the knowledge of reader into the retriever is reasonable. In some sense, it can mitigate the error propagation problem in two-step training of the retriever and reader.\n\nWeak Points: \n1.\tThis paper is more like a case study of DPR other than an analysis of the functionality of the retriever and reader in open-domain QA. The authors restrict the retriever as the Dense Passage Retriever used in the DPR paper and the reader as the extractive multi-passage model in the DPR paper. But there are some other ways of retrieval such as BM25 and reading such as Fusion-In-Decoder. If taking these approaches into consideration, some claims in the paper may not be valid. For example, in the Fusion-In-Decoder model, the overall performance increases as the reader reads more passages. Therefore, the conclusion from section 3.2 and 3.3 comes from the limitation of extractive based reader models but it does not imply anything meaningful to the problem investigated in this paper.\n2.\tThe overall improvement for the distillation experiments is marginal. Since the reader model includes a ranker and a span predictor, the distillation process takes the knowledge of the trained ranker into retriever. There is no surprise that it can improve the top-1 retrieval recall. But it does not introduce any new knowledge into the two-step model, and thus the overall performance remains comparable to previous DPR results.\n\nQuestions:\n1.\tIn section 4.1, it is said that “Along with the scores of each token being the starting or ending positions of the answer, the reader outputs a ranking score”. How is the starting and ending score used in the distillation process? In my understanding, the ranker score from the reader model is used for distillation but not the starting and ending score.\n\nSome typos in the paper:\n1.\tIntroduction: “For instance, Clark & Gardner (2017) and Lewis et al. (2020) both demonstrate in Figure 3 that as the reader reads more top-k documents” -> Figure 2\n2.\tPage 6 - Section 4.1 - Teacher: Reader: “DPR reader described in Section 6.1 of the work of” -> Section 2?\n3.\tPage 8: I cannot find footnote 8.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting and exciting direction. Great results and valuable analysis",
            "review": "The paper explores the question of whether the _two-tower-model_ of the retriever (for the setting of open-domain QA), that was intended for fast and approximate search, has other benefits over the more complex _one-tower-model_ re-ranker/reader. \nThe paper claims that the retriever is not merely an approximation and that the retriever is complementary to the reader and may have make the system more robust against negative examples in the search space.  The authors claim that this is evident by showing that the reader's scores peak after reading a few number of retrieved passages (k) and as k is increased the reader's score drops. They further verify this claim by trying to train the reader in a similar fashion to the retriever and are still able to observe the early peaking, i.e, the reader is still not able to distinguish negative passages as well as the retriever. \n\nMotivated by this, the authors propose using knowledge distillation to imbibe the retriever with some of the knowledge of the reader. They show that this technique improves the efficacy of the retriever which can further improve the reader leading to an overall boost in the end-to-end performance. \n\nPros\n- The knowledge distillation method described seems technically sound and boasts impressive results\n- The analysis and discussions comparing the reader and retriever for retrieval are insightful and would be valuable to the community\n- The paper is well written and easy to understand. \n\nClarifications (minor)\n- Trying to improve the retriever using the reader seems to be at odds with the claim that the reader and retriever perform complementary functions. It might be beneficial to have some experiments that shed light on what aspects of the retriever are improved by the knowledge distillation\n- The claim \"our guess is that the former (retriever) is good at finding passage embeddings relevant to a\nquestion embedding from a large vector space, and the latter (reader) is good at distinguishing between difficult examples in a relatively small search space\" might benefit from some qualitative analysis\n\n\nOverall, the paper provides some good insights and would be valuable to the community. I recommend acceptance. \n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited new insights",
            "review": "\n\nThis paper has an attractive title and studies an interesting and important problem in retriever-reader solutions for open-domain QA. The paper consists of two parts. The first is an analysis section to show that the retriever and the reader are complementary. The second is a new distillation based training objective for the retriever.\n\nAlthough the paper covers several interesting findings, I feel that the substances are not enough for an ICLR paper.\n\nThe analysis section mainly gives empirical results showing that the reader has a peak performance with relatively smaller top-k retrieved passages. Additionally, the analysis shows that this conclusion is consistent when the negative sample set is doubled. Similar empirical results have been observed in prior works, but the paper does not provide new insights about the possible reasons for this observation. From the analysis, the only conclusion we can draw is that the ranker and retriever have different behaviors. But since they are not trained with the same amount of data and not evaluated with the same metric, this analysis cannot reveal any potential reasons in depth. In my opinion, an analysis of the attention heatmaps will provide more information compared to the existing section.\n\nThe distillation approach uses a standard KL-divergence objective. This approach does improve the recall of the retriever significantly. However, it does not help much in the overall QA performance. In fact, one lesson we can learn from the DPR and fusion-in-decoder papers is that, there is a great potential to make use of large number of passages with clever QA system designs. In this perspective, they actually point out that the improvement of retrievers is less critical, especially when inputting the top-50/100 passages to the reader. And the reader can play a more important role in filtering and aggregating important information from noise retrieved passages. Plus that for top-50/100, the advantage of the proposed distillation method is small, this explains why the proposed retriever training does not help the QA system that much.\n\nGiven the limited substances and new insights, I think the paper does not meet the bar of ICLR. Since the analysis is not informative and there is a lot of redundancy in the writing, I think the length of this paper can be largely reduced to fit a short paper if the writing is revised properly.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}