{
    "Decision": "",
    "Reviews": [
        {
            "title": "Some trivial analysis for the general and stability of GANs",
            "review": "In this paper, the author provided some theoretical analysis that connects Lipschitz continuity, robustness and generalization, discussed sources of instability, and related data augmentation to the penalization of the Jacobian norm of both players, of GANs.\n\nIt seems that the theoretical findings and discussions are trivial.\n\nThe connection between data augmentation to penalization of the Jacobian norm of both players might be interesting. The authors should investigate it more thoroughly with better experiment design, if it is correct.\n\nEquation (7) and (8) does not seem to be right.\n ",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting perspective, more thorough and rigorous analysis needed for analyzing the generalization of GANs",
            "review": "**Summary**\nBased on a theory developed in (Xu & Mannor (2012)), this work analyzes the connection between robustness and generalization in the context of GANs. The analysis leads to a conclusion that the Lipschitz continuity of the loss function provides guarantee for \"generalization\" of GANs and data augmentation with a small perturbation noise could help impose the Lipschitz constraints on $D$ and $G$. Overall I think the work offers some potentially interesting ideas, but more thorough and insightful analysis are needed for the purpose of analyzing generalization, generative performance and stability of GANs.\n\n**Pros**\n- The paper can be considered an interesting application of the theoretical bound in (Xu & Mannor (2012)) to partially explain why Lipschitz continuity is so important for GANs, which is consistent with previous works and offers an alternative way to justify the Lipschitz constraint.\n- From this perspective, it also emphasizes that the loss function should also be Lipschitz w.r.t. $G$ and $D$ (not only $G$ and $D$ should be Lipschitz w.r.t. their inputs).\n- The observation on how data augmentation can help impose Lipschitz constraint is also interesting and partly explains some recent empirical success of using data augmentation to train GANs.\n\n**Concerns and Questions**\n- Although I think the proposed perspective has the potential to offer an insightful analysis for GANs, I think currently this work is more like a direct application of previous work and needs more in-depth analysis to explain the complex GANs framework.\n- The adaptation of the previous theory (Theorem 1) to the training of generator and discriminator seems natural and correct. However, the GANs consists of a complex minimax game (probably with single-step alternating gradient descent) and how does such individual generalization finally affects the whole training of GANs (e.g., stability and convergence of minimax optimization, the final generative performance)?\n- As a result, I think the work only addresses the first level of generalization in GANs, i.e. fixing the opponent and reducing it to a standard learning setting such that the previous bound directly apply. Such a generalization notion may not directly be meaningful in the context of GANs (e.g. leading to a distribution that fits better to the data). Ultimately, when we consider the generalization of GANs/generative models, we usually expect to explain how does the learnt generator generalize to new data points, etc, (instead of simply memorizing the training data).\n- From this work, is the Lipschitz continuity of the loss function a necessary and sufficient condition for the final performance of GANs? For example, in previous works e.g. Wasserstein GAN, the Lipschitz constraint naturally emerges when we want to minimize the dual form of Wasserstein distance, while in this work, it seems what it can guarantee is only the generalization error of the individual loss functions of $G$ and $D$. Hence I am not sure if this level of result will really transfer to the success of the whole framework, which is complicated by minimax optimization, etc. \n- For the data augmentation section, are you considering the augmentation to be something like a simple Gaussian or general augmentation like translation, cutout, rotation? If the augmentation is general, I don't think the augmented sample is within the neighborhood of the original sample, at least not in the sample space. Thus the following taylor expansion analysis may not be applicable? Also the overall data augmentation for regularizing the Lipschitz constraint for GANs is a bit confusing to me. It seems data augmentation is a noisy version of original loss and will thus introduce bias to the original objective. Why not directly use other methods such as GP or SN to achieve the same goal, without introducing bias?\n- Some missing proofs: are Eq (7) and (8) well-known results? It seems they may not be generally true and should be stated formally. For example, if the function space for $G$ is expressive enough, it is always an equality. If the function space is limited, there may be counter examples? Taking Eq (8) as an example, if the function space that the minimization is over only contains $p_{d}$, then $JS(p_{d+\\epsilon, p_{d+\\epsilon}}) < JS(p_{d+\\epsilon, p_{d}})$? Furthermore, in page 8, reducing JS may not necessarily reduce squared distance? For example, in model-misspecified case the minimizer of different distances/divergences can be different?\n- Minors: the $\\sigma$ and $o(\\sigma)$ are not defined in Lemma 4?\n- More thorough discussion on related work: I think the topic is a well studied one in recent years and a more thorough discussion to related works are needed. For example, [1] also focuses on discussing the importance of Lipschitz constraint and is only briefly mentioned. Also the data augmentation part could be closely related to [2].\n\n\n[1] Lipschitz generative adversarial nets. ICML 2019\n\n[2] Spread Divergence. ICML 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "review": "This paper studies the generalization properties of generative adversarial networks (GANs). The paper's goal is to demonstrate that the generalization error of GANs correlates with the Lipschitz constant of the discriminator and generator players. To show this, the paper bounds the generalization error with a function of the Lipschitz constants in Theorem 3 and Corollary 1. While understanding the connections between Lipschitz regularization and generalization in GANs is definitely an interesting topic, I don't find the paper's results addressing this problem. Indeed, it seems to me that the conclusions of Theorem 3 in Equation (1) and Corollary 3 will trivially follow from the definition of Lipschitz constant and hence do not provide any understanding of generalization in GANs.\n\nTo see why Equation (1) holds trivially, please observe that the first term in the upper bound, LB, will provide an upper-bound on the difference f(A,z_1) - f(A,z_2)  for any pair of samples z_1,z_2 in support set \\matchal{Z}, because B is defined as the diameter of \\matchal{Z} and L is defined to be the Lipschitz constant with respect to the same distance measure used for defining the diameter (as defined in Lemma 2). Therefore, the average of f(A,z) under the empirical and underlying distributions are also at most different by constant LB. This shows that Equation (3) will trivially hold where the upper-bound can be even improved to LB after removing the second term. In general, a valid generalization bound should go to zero as the sample size grows to infinity which is not the case in either Equation (1) or Corollary 3. I recommend redoing the generalization analysis by optimizing the bound in the last line of Theorem 3 over \\lambda. Please note that the particular case \\lambda = B which is used in Equation (1) and Corollary 3 leads to trivial bounds; however, after optimizng the upper-bound over \\lambda it may be possible to show non-trivial generalization error bounds. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice motivation with some incoherent theoretical results",
            "review": "This work studies an important problem of discovering the generalization (and robustness) ability of Generative Adversarial Networks (GANs) with a theoretical take. Using recent advances in connecting Lipschitzness of losses to generalization and robustness, this work applies such a framework to understanding GANs and establishes Lipschitzness (in both the discriminator and generator) as a key element for generalization in the form of a generalization bound. A connection to data augmentation is also made.\n\nPros\n- Motivation of the problem is well-explained and clearly written.\n- Connection to data augmentation is nice. \n\nCons\n- The theoretical results lack sufficient clarity in their derivation, which makes them difficult to judge.\n- Comparison with an existing method such as in [1] for the Gaussian noise analysis is missing which makes it harder to judge the significance.\n\nWhile the introduction and related work is well written, there are two main concerns I have with the presentation of the theoretical results, which can potentially nullify the findings. My first concern is with the generalization bound in Corollary 1 and its non-uniformity over $D$ and $G$. I believe it would be make the dependence on $(D,G)$ in $V_d$ and $V_g$ more explicit and especially note that Corollary 1 holds *only* for a fixed $D$ and $G$ with probability $1- \\delta$. This is important since you would typically want it to hold for all $D \\in \\mathcal{H}$ and $G \\in \\mathcal{G}$ (some hypothesis set $\\mathcal{H}$ and $\\mathcal{G}$), which would make sense since the notion of complexity of the hypothesis should come into play, as is typical in uniform generalization bounds. This would then have some form of Rademacher complexity included and changes the bound and/or adds discussion regarding how generalization error is not only linked to Lipschitzness but the complexity of discriminator/generator set. Moreover, it would be helpful to give examples of how large $B_x$ and $B_z$ is just to understand the bounds better. In light of this, I believe Corollary 1 is a simple application of known results and makes a careless analysis of the GAN objective -- however I encourage the authors to correct me if I have misunderstood this and that a non-uniform bound is still useful for this application.\n\nMy second concern regards the derivation of data augmentation, especially since I think this is one of the more stronger and core contributions of this work. Firstly, I believe whats going on in Eq (6) and the discussion that follows is a Gaussian convolution and there is much simpler notation to denote this. In particular, instead of $p_{d+\\epsilon}$, it is more appropriate to remark this as a $p_d$ after a convolution with a Gaussian distribution/kernel. This is quite a standard way of writing this and may increase accessibility. This is especially important since a key reference [1], which precisely analyzes the GAN objective under Gaussian noise and presents the derivation in a much more intuitive way. I ask of the authors to explain how this section of the paper differs from this work, especially given that they perform a higher order Taylor expansion (than the one in the Appendix) and derive a penalty as well. Furthermore, it is claimed \"By using the same argument as Goodfellow et al. (2014), one can see that training $G$ is equivalent to minimizing $E_{\\epsilon}[ \\operatorname{JS}(p_{d+\\epsilon}, p_{g + \\epsilon})]$. I believe there may be a misunderstanding here since the standard argument from Goodfellow (of the GAN objective minimizing the Jensen-Shannon divergence) would allow us to conclude that training $G$ would be equivalent to JS between the convolution of $p_d$ under a Gaussian kernel and the convolution of $p_g$ under a Gaussian kernel, which is different to $E_{\\epsilon}[ \\operatorname{JS}(p_{d+\\epsilon}, p_{g + \\epsilon})]$. In particular, a standard result of f-divergences (referred to as the 'Conditioning increases f-divergences' property) would allow us to conclude that $E_{\\epsilon}[\\operatorname{JS}(p_{d+\\epsilon}, p_{g + \\epsilon})]$ is indeed an *upper bound*. Perhaps I am missing something with regards to notation and which derivation from Goodfellow explains this properly and encourage the authors to explain this conclusion. \n\nRelated work:\nWhile a nice survey of generalization results are detailed, I think an important reference to discuss is [2], since not only does it discuss generalization of GANs but rather studies Lipschitzness of discriminators and their relationship to Autoencoders (which are known for having better stability in training) - both concepts which are central themes in the submission.\n\nMinor:\n- Can you provide a bit more intuition as to why a \"a small change in the input leads to a small change in the output of the learnt hypothesis\" when explaining Definition 1? As I see it, this definition only concerned inputs in the learning algorithm and does not say anything about the 'output of the learnt hypothesis'. Moreover, it is then mentioned 'the hypothesis $\\mathcal{A}(S)$ will generalize well over the areas around $S$' -- what exactly are 'areas' around $S$?\n- Page 5. \"There is a line of works\" -> \"There are lines of work\"\n\n[1] Roth, K., Lucchi, A., Nowozin, S., & Hofmann, T. (2017). Stabilizing training of generative adversarial networks through regularization. In Advances in neural information processing systems (pp. 2018–2028).\n\n[2] Husain, H., Nock, R., & Williamson, R. (2019). A Primal-Dual link between GANs and Autoencoders. In Advances in Neural Information Processing Systems (pp. 415–424).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}