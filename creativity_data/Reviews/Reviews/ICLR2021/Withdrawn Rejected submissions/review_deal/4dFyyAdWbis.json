{
    "Decision": "",
    "Reviews": [
        {
            "title": "A deeply flawed attempt at addressing several important problems",
            "review": "This paper attempts to tack several aspects of the unsupervised adaptation problem in the hope of presenting algorithms capable of handling more practical settings. In particular, the authors are concerned with the setting in which there may be one labeled source domain but then many target domains. In these cases the authors are concerned with both selecting the best \"collaborator\" by which they mean which classifier (either the baseline classifier or one previously adapted to a different target domain in the sequence. They are also concerned with learning in a privacy preserving fashion. \n\nUnfortunately the treatment is sloppy, the representation of the prior work misleading, key concepts badly confused, and the resulting methods at best dubious. In the first place, the authors seem to confuse the problem of domain adaptation (a general problem) with one specific class of heuristic methods proposed for addressing these problems (the domain adversarial approach). Many of these approaches are themselves problematic, unstable, and guaranteed to fail even if they succeed at their optimization objective, in particular in the case when label distributions shift, making the choice of example (disease prediction from CT scans) especially problematic.\n\nThe proposed approach includes a theorem, however the theory is poorly presented, of dubious utility, and its contribution misrepresented. The authors present an bound on target distribution cross entropy that depends on the lipshitzness of the networks and the Wasserstein distance between distributions. The authors propose using this bound to \"estimate\" the error from using a particular collaborator, confusing a bound for an estimate, and giving no indication of whether this bound would ever in reality be non-vacuous, not to mention the omission of key details of how they estimate the Wasserstein distance. More fatally how they plan to handle the propagation  of errors across multiple steps. Perhaps most fatally, I expect the bound itself to be wrong. All we need is for the supports to be non-overlapping and the underlying labeling function to be non-smooth. \n\nThe authors would be better served to address just one question rigorously rather than forcing too many immature components into a paper. In its current form, this paper is not fit for publication.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing an important baseline",
            "review": "- Summary and Contributions\n    - In this paper, the authors initialized a domain adaptation setting to meet practical applications where 1) the target domain can benefit from not only the labeled source domain, and 2) the target domain does not access to the data in the source domain. I quite like the motivation for this setting, and agree with the authors that this practical application of domain adaptation is significant. Meanwhile, I have several questions to be answered by the authors, listed in the following.\n- Strengths\n    - The problem investigated in this paper is of significance.\n    - The authors conducted extensive empirical studies, including adaptation to different settings, to validate the effectiveness of the proposed framework. \n    - The paper is well written and easy to follow.\n- Weaknesses\n    - The technical novelty of this paper, mainly lying in the DILS part in my opinion, is limited. The bound for choosing the optimal collaborator has been well introduced and studied in a bunch of domain adaptation works. \n    - The related works on multi-target domain adaptation are missing, including [1]. This line of works also leverages the knowledge sharing between unlabeled target domains. \n    - Additional empirical details and results are expected. Please kindly see the questions part.\n- Questions:\n    - How many steps are needed for the optimal collaborator selection? I guess that different step \\tau will give rise to different selection results? Is the result stable across different steps? If the convergence requires a large \\tau, the computational is quite inefficient, which is unexpected and impractical.\n    - Can the bound be generalized to multi-class classification beyond binary classification which Theorem 1 depends on?\n    - How do you estimate the Lipschitz constants?\n    - Regarding to the baselines in Table 2:\n        - I guess the superiority of the proposed over Proxy A-distance is completely due to the estimation of error rate on the source domain. Therefore, I expect to see the performance of choosing the collaborator via extending Proxy A-distance with the estimation of error rate on the source. If it is possible,  considering the error rate on the source and the MMD distance simultaneously would be another choice.\n        - The authors missed a very important baseline [2], which can obviously be adapted to solve this problem. Moreover, the performance on RMNIST by [2] is much better than this reported in Table 2.\n        - Why don't you compare with Proxy A-distance with different DA algorithms in Table 3?\n\t\t\n\n[1] Gholami, Behnam, et al. \"Unsupervised multi-target domain adaptation: An information theoretic approach.\" IEEE Transactions on Image Processing 29 (2020): 3993-4002.\n[2] Wang, Hao, Hao He, and Dina Katabi. \"Continuously Indexed Domain Adaptation.\" ICML (2020).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review3",
            "review": "This paper proposes to two algorithms for practical DA settings which includes multiple target domain and data is privacy-sensitive. For the first setting, the authors introduce an algorithm that selects the optimal domain based on the Wasserstein distance between the target domain and each of the candidate domains. For the second setting, this paper uses a lazy way to exchange gradients of domain discriminators.\n\nConcerns:\n\n1. It is unclear why the authors bring these two different DA settings (i.e., multiple target domains and privacy-sensitive adaptation) together in a paper? While the authors show the two proposed algorithms can be combined, however, the reviewer cannot figure out what the strong motivation is to discuss two topics in a paper.\n\n2. While the authors strongly argue the practical values of these two settings and two algorithms, however, no experimental results are shown from some more realistic DA datasets (such as OfficeHome and DomainNet). \n\n3. Why selecting the optimal domain collaborator is important? How about combining all the previous domains that have been seen (resulting in a single source domain) and then transfer to the novel target domain?\n\n4. Does the proposed algorithm scales up to other DA algorithms? For example, this paper use Wasserstein distance to distinguish domains. How about using other divergences such as MMD in other DA algorithms?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}