{
    "Decision": "",
    "Reviews": [
        {
            "title": "The proposed method is limited novelty, and the experiments are not enough.",
            "review": "Pros.:\n\n1. The proposed method is simple and reasonable.\n2. The paper is clearly presented and well-organized.\n3. The authors conduct many ablation studies to validate the different modules of the proposed methods.\n4. The proposed method could widely be applied in many applications, such as object segmentation, saliency detection, and WSOL, and achieve comparable performance.\n\nCons.:\n\n1. Limited Novelty:\nThe proposed method seems a combination of the previous methods. However, this combination is too straightforward, and I don't see any novel insight or interesting design. \n\nSpecifically, first, the idea of \"background removal\" is from Voynov & Babenko (2020), and the authors only replace the supervised BigGAN with any existed unsupervised methods, such as BigBiGAN and StyleGAN2. \n\nSecond, the proposed methods include two stages, including synthetic data generation and U-net training with synthetic data. These two stages are commonly used in the current unsupervised saliency detection, such as (Wang et al., 2017b; Zhang et al., 2018; 2017; Nguyen et al., 2019). In these methods, the authors first try to generate as clean pseudo ground truth as possible and then train any network with pseudo ground truth.\n\nTherefore, The proposed method is built based on the currently existing methods. The proposed methods incremental and has limited novelty. \n\n2. Wrong statements\n\nIn Sec. 2, for the second paragraph, \"Existing unsupervised approaches,\" the statements seem wrong. In  (Wang et al., 2017b; Zhang et al., 2018; 2017; Nguyen et al., 2019), all of the noisy ground-truth are generated from the unsupervised and handcrafted methods, so these four methods are purely unsupervised. I don't know why they require pre-trained classification or segmentation networks. This makes me confused, and I may misunderstand. Could the authors explain more?\n\nBesides, for these unsupervised saliency detection methods, I think their method is also simple, and they require fewer hyper-parameters. \n\n3. more hyper-parameters\n\nI agree with the claim, \" generative models are unstable and sensitive to hyperparameters,\" but the proposed method contains many hyper-parameters. The authors only use the default hyperparameters of (Voynov & Babenko, 2020) for the synthetic data generation. The proposed method should be unstable and sensitive to hyperparameters. Besides, for the three schemes to improve saliency masks, there are also other hyperparameters. I think the proposed method required more hyperparameters. Authors should try to prove the proposed method isn't unstable and sensitive to hyperparameters, but I don't see any experiments related to this.\n\n4. not enough experiments\n\nThe proposed method is similar to the unsupervised saliency segmentation methods, and they should be suitable baselines. However, in all experiments, I don't see the comparison between the proposed method and the unsupervised saliency segmentation method. The authors could use the public unsupervised saliency segmentation codes or implement them for object segmentation and WSOL tasks. For the saliency detection task, the authors could directly compare the results from their papers, but  I also don't see the comparison. After comparing the results of the unsupervised saliency segmentation methods in their papers, I find the performance of the proposed method is worse.  Besides, it is not suitable to put the ablation study and synthetic data quality in the appendix but in the main paper, because these two experiments are essential.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and effective method, but missing evaluations",
            "review": "This paper proposes a new method for unsupervised foreground object segmentation, saliency detection and object localization. The key idea of the paper is to use publicly available GANs trained without object classification labels to produce synthetic images equipped with segmentation masks. The authors' propose to \"discover\" disentangled latent space directions in GANS, which can be used to generate segmentation mask labels for synthesized images. The authors then train supervised networks for object segmentation, saliency detection and object localization with the synthesized images and masks while using an independent synthetic set and ground truths as the validation set for hyper-parameter tuning. \n\nPros:\nThe proposed technique is simple and shown to be effective, in spite of its simplicity. The authors show superior results in comparison to the SOTA methods for segmentation and saliency detection and slightly worse results for object localization. The fact that the authors show that GAN networks trained without supervision can be used just as effectively to extract segmentation information for synthesized images as GANS trained with object class labels, is an interesting insight of this paper.\n\nCons:\n1. The paper is missing references to an important SOTA work on WSOL: Ren et al., Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection, CVPR 2020.\n2. All the evaluations considered in the paper are on object centric datasets containing only one main foreground object roughly in the middle of the image. Such datasets are not fully representative of situations encountered in real world images, where multiple objects may be present in each image, which may not be in the center. How well does the proposed method perform for such images? For example, what is the performance of the authors proposed approach on the benchmark COCO [1] and VOC 2007 and 2012 [2].\n3. Several elements of the overall algorithm seems quite heuristic and brittle. For example, I imagine the choice of filter for greater than 50% of image pixels in the mask and of the optimal value of $h_bg$, to be highly dependent. Similarly, the manual procedure to identify the segmenting direction seems not very elegant. How would the authors address the case when no such direction can be found for an existing GAN network?\n\n[1] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence ´\nZitnick. Microsoft coco: Common objects in context. In Proc. ECCV, 2014.\n[2] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. In Proc. IJCV, 2010.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Big GANs Are Watching You: Towards Unsupervised Object Segmentation with Off-the-Shelf Generative Models\"",
            "review": "The paper provides an interesting way for unsupervised object segmentation. Specifically, it proposes to exploit off-the-shelf powerful generative models to create pseudo masks by controlling the “saliency lighting” direction in the latent space. Besides, a set of post-processing is used to improve the quality of pseudo masks. The created paired masks and images are then used for training models, which show promising performance on several benchmarks. \n\na.\tPros. \ni.\tOverall, the paper is well written and easy to follow. \nii.\tThe paper provides comprehensive experiments on object segmentation, saliency detection and weakly-supervised object localization tasks with commonly-used evaluation metrics. \niii.\tThe proposed approach shows promising improvements compared with previous works. \n\nb.\tCons. \ni.\tThe difference in experiments between object segmentation and saliency detection is not clear. Since the models only need to segment foreground objects out without predicting labels in the setting of object segmentation in the paper, the goal of which seems the same as the task for saliency detection. I suggest the authors clarify the differences and the necessity of these two tasks.  \nii.\tWhat is the motivation of ‘completely unsupervised’ object segmentation? Recent works of unsupervised deep saliency detection (DeepUSPS, Nguyen et al., NeurIPS 2019; Zhang et al., CVPR 2018) leverage pre-trained classification networks but obtain a much better performance than this paper (e.g., 87.83 VS 79.7 in terms of F-score on ECSSD). Their improvement is significant, and the supervision is cheap and acceptable. \niii.\tSome important baselines are missed in the experiments. I suggest the authors include (DeepUSPS, Nguyen et al., NeurIPS 2019; Zhang et al., CVPR 2018) in Table 2 to provide a reference level for different methods. Moreover, the authors should also compare them by removing their “pre-trained weights” for fair comparisons.  \niv.\tThe finding in Section 4.4 is interesting. However, I expect the authors to further explain why the performance of BigBIGAN is better than BigGAN. The result could be not convincing enough since BigGAN has remarkably higher generated image quality than BigBiGAN, which may influence the generation of pseudo masks.  \n\nI would appreciate the authors, if they can address and clarify the cons above.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results and very interesting research direction; however the presentation of the approach needs to be reworked",
            "review": "Strengths:\n\n- Empirical results, both quantitative and qualitative are appealing. Evaluation is performed on several tasks, comparing on relevant benchmarks, and to the best of my knowledge, with relevant competitors.\n- The final approach proposed by the authors seems indeed simpler and more lightweight in terms of training resources  than existing unsupervised segmentation approaches.\n- The approach brings substantial evidence of the potential usefulness of the latent space learned by generative models. Such empirical findings can promote further investigation of the latent space, and lead to more principled methods to leverage it. In short, the authors address a very interesting research direction.\n\nWeaknesses:\n\n- the approach is based on a number of heuristics; additionally, these heuristics seem to have sometimes been tuned to the dataset (eg. the mask thresholding procedure for the Flower’s dataset.) It is  not clear to me whether the labels were used in the validation procedure of this sequence of heuristics. This also makes it hard to seek more general principles, whose performance might not be capped by the inherent flaws that generally come with heuristics.\n- While the main paper is generally well written, important parts of the conceptual approach (explaining the method for identifying potential segmenting directions) are relegated to the appendix (B), and are unclearly presented. Throughout the main paper, one wonders how much the proposed approach is dependant on the actual published weights of the BigBiGAN model. Appendix B provides some intuition how the analysis could be re-conducted for other generative models (or even for other weights), but this should be much more central in the presentation of the approach, and this part should be rewritten, with improvements to the structure and clarity.\n- while the evaluation is quite extensive, the paper hardly provides any interpretation of the results. \n\nMore comments:\n- the presentation of Voynov and Babenko is not clear and should be more formally presented, for the paper to be self-contained.\n- the evaluation datasets seem in general to contain images with a single object centered in the image. This limitation should be discussed in the conclusion.\n- the validation set of the different datasets could be used to increase the size of the training set, instead of being left out. Alternatively, it could be used as the support the proposed empirical distribution for the validation set.\n- For reference, it would be good to report the supervised method’s results on these benchmarks, to show the remaining gap with supervised approaches.\n- although it seems to be quite recent, the authors should refer to the recently introduced and improved MaxBoxAccV2 metric and explain why the analysis has not been conducted using this metric.\n- just a note that the top of p5 really looks like a caption, which makes the reader look for the rest of section 3.4, until they eventually realise that it’s at the top of the page…\n- It’s not clear to me what the first line of the ablation study (+ImageNet embeddings) refers to.\n- Equation (6) of Appendix B is unclear to me. In general equations (1)-(7) should be reworked to improve simplicity and conciseness of the presentation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}