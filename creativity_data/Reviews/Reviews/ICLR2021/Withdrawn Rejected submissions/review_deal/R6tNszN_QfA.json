{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed a new family of losses for GANs and showed that this family is quite general and encompasses a number of existing losses as well as some new loss functions. The paper compared experimentally the existing losses and the new proposed losses. But the benefit of this family is not clear theoretically, and this work did not also provide the very helpful insights for the practical application of GANs.\n"
    },
    "Reviews": [
        {
            "title": "Interesting framework but with several limitations",
            "review": "**Summary of contributions:**\nThis paper proposes a new framework to design new loss for GANs. The authors show that their framework is quite general and encompass a number of existing approaches (e.g. the original GAN formulation, hinge loss, etc..), they also propose a categorization in three different classes and derive new loss function. They then compare experimentally the different existing loss and the new proposed loss that fall under their framework.\n\n**Main comment**:\nThe framework proposed in the paper is interesting since it's quite general and the authors are able to derive a large number of existing as well as new loss from it. However, I think the framework has several limitations:\n1. The formulation is based on the likelihood ratio which is only defined if the support of $g$ and $f$ match, this is known to not be the case in the context of GANs.\n2. The benefit of the framework is not clear, while it provides a way to derive new loss it's not clear what are the advantages of the new loss. Theoretically the author argue that it is a hard question to answer, and I agree. The authors try to answer this question through experiments but I find the experiments not very convincing. In particular, the authors argue that subclass A objectives are more stable based on the CelebA experiment, however it's not clear to me that the instability is due to a specific choice of objective function, it might just be that the hyper parameters where slightly off for the other objectives. I believe it would be interesting to understand better the results on CelebA, in particular maybe to show that some objectives are indeed more stable, they can vary several hyper-parameters and compare how often each objective is better than the other, that would make the results and conclusion much more convincing.\n\n*Minor comment*:\nThe paper is overall clear but the clarity of some sections could be improved. I think theorem 1 would be more clear if stated a bit differently simply saying that $D=\\omega(r)$ maximize $\\phi(D)+r \\psi(D)$ and that $r=1$ minimize $\\phi(\\omega(r))+r \\psi(\\omega(r))$. Section 3 is a bit dense, the subclasses also seem a bit arbitrary. I believe section 5 could be improved by stating more clearly the different observations, right now it looks more like a description of the figures than a clear statement of the question that the experiments try to answer and how they answer it.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The theoretical results are good, but not surprising",
            "review": "This paper generalizes the min-max problem of GANs to form a richer family of generative adversarial networks. Interestingly, most of the well-known variants of GANs can be found in the spectrum of formulations covered by the family proposed in this work.\nIn terms of modeling, it is evident that the family proposed in the paper is richer than that of f-GAN. The family in this paper is shown to have a connection to WGAN except that the Lipschitz condition is omitted. \nHowever, under the light of existing works including f-GAN and other relevant works, the obtained theoretical results are not surprising to me. In addition, apart from providing a richer family, this work does not significantly influence the practical aspects of GANs.\nI have some following questions:\n1. If we solve the min-max problem in (2) subjected the fact that \\phi and \\psi satisfy Eq. (9), is it equivalent to minimizing any divergence between two distributions with pdfs f and g?\n2. D(x) is not a typical discriminator whose values between [0;1] providing the probability to distinguish true and fake data, is not it? D is more similar to a critique whose output values are real-valued, is not it?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas, relationship to previous work unclear",
            "review": "Summary\n========\nIn this paper, the authors set out to find what scalar functions will make for a “max” part of the “min-max” GAN objective. They then find such a class of functions, and show that only a ratio between two equal probabilities will be admitted as a solution.\n\nPros:\n====\nThe paper nicely introduces a different way of seeing GANs, not as a difference between the generated and real data, but as a an integer of the ratio between generated and real distribution times the discriminator. Only if the ratio is 1 everywhere is the discriminator unable to maximize the max part of the GAN objective.\n\nFurther, I liked the idea that the discriminator shouldn’t just decide what class data belongs to, but also estimate the probability ratio. Specifically, in the formulation here, the max part is maximized when $D(X) =\\omega(r(X))$, so maximized iff $\\omega^{-1}(D(x))$ doesn’t just classify, but says the probability ratio between the two classes. If this idea is expanded upon, I think the authors could make a novel contribution.\n\nCons:\n=====\n\nUnfortunately, the authors have neglected to carefully explain how their contribution relates to previous work. It’s telling that the paper cites only two papers from 2018, one from 2019 and none from 2020. All other citations are from previous years, even though 2018-2020 has been a time of much GAN research.\n\nA key way in which the author’s work hasn’t been sufficiently compared to previous work is with their main claim “We propose a simple methodology for constructing such [min-max] problems assuring, at the same time, consistency of the corresponding solution.” In [Liu], they show a class of of functions where consistency is also guaranteed, and the class shown by the authors here is a subset of the class in [Liu]. The details are at the bottom of my review\n\nFurther, many of the techniques in this paper seem very similar to [Song], where they also investigate the f*-gan divergence. Specifically, the claims they make in Theorem 1 seem very similar to Prop. 2 in [Song]. Also the change of measure trick in the introduction can be found in [Song]. A detailed comparison of this work to that work would also be helpful.\n\nSince when reading this paper one simply doesn’t know what is previous work which has already been done by others and what is the author’s novel contribution. Once the authors address this, and one is confident the contribution is indeed novel, then the submission would be worth considering.\n\nDetails of why this is a subset of what’s already been shown in [Liu]:\n\nThere, they examine the difference between the target density $d$ (in this paper $d$ is $f$, but Liu uses $f$ for something else) and the generated density $g$ via $\\sup_{f\\in\\mathcal F}\\mathbb E_{x\\sim d,y\\sim g}[f(x,y)]$, so we find the function $f$ in a class $\\mathcal F$ which maximally separates the classes from $d$ and $g$.\n\nNow this work proposes to do the same thing, but with $f(x,y)=\\phi(D(x)) - \\psi(D(y))$ where $\\phi(z) = -\\int_{\\omega^{-1}(0)}^z \\omega^{-1}(t)p(t) dt + C_1 $ and $\\psi(z)=\\int_{\\omega^{-1}(0)}^z p(t) dt + C_2$.\n\nIn [Liu] they then split f(x,y) up into two functions m and r, such that f(x,y)=m(x, y) - r(x,y) where m(x,y) has the form m(x,y)=v(x)-v(y). This can be done in your case too, resulting in (here we drop the constants C_1 and C_2 for simplicity)\n\n$v(x) = \\int_{\\omega^{-1}(0)}^{D(x)} p(t) dt$, $v(y) = \\int_{\\omega^{-1}(0)}^{D(y)} p(t) dt$\n\nand \n\n$r(x,y) = \\int_{\\omega^{-1}(0)}^{D(x)} (\\omega^{-1}(t) + 1) p(t)dt$\n\nSince D(x) must be in $\\mathcal J_\\omega$, this integral has an infimum, and theorem 4 from [Liu] can be applied to achieve the same results as in this paper.\n\n[Song] Song, Jiaming, and Stefano Ermon. \"Bridging the Gap Between $ f $-GANs and Wasserstein GANs.\" arXiv preprint arXiv:1910.09779 (2019).\n\n[Liu] Liu, Shuang, Olivier Bousquet, and Kamalika Chaudhuri. \"Approximation and convergence properties of generative adversarial learning.\" Advances in Neural Information Processing Systems. 2017.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Impactful paper for further improvements on generative models with adversarial optimization problems.",
            "review": "Overall, this paper provides impacts on understanding the core of generative models with adversarial optimization problems.\nThis paper shows the diverse possibilities of formulating the generative model optimization problems that the researchers can further investigate for better performances. \nAlso, this paper shows that generative models with unexplored losses achieve the best results in various datasets which demonstrates the possibilities of future improvements of generative models.\nOverall, this paper is valuable to the machine learning community (especially for generative models and adversarial training). \nThe below are some concerns for this paper but those concerns are not bigger than the advantages of this paper.\n\n1. Quantitative experiments\n- Although the authors provided two tables (Table 2 and 3), there were not much analyses about the results.\n- I understand that it is not an easy problem to understand \"when\" should we use \"which\" function. However, it would be great if the authors can discover some trends in the results to demonstrate which type of functions work well with which type of datasets.\n- I think it would be great to use some synthetic data with known characteristics of distributions as the target distribution to analyze for understanding this point.\n\n2. Other types of dataset\n- Generative models are widely utilized in computer vision. \n- However, there are various other types of datasets that can get benefits of generative models such as tabular data and time-series data.\n- It would be good if the authors can provide some simple experiments to demonstrate its generalizability.\n\n3. Minor points\n- It is not clear to transform between equation (3) and (4). I think this is a critical part in this paper; thus, it would be good to explain a little bit more for this part.\n- The authors explain the differences between f-GAN and this paper. However, it is not super clear to understand. It would be good to clarify this point to highlight the novelty of this paper.\n\n--------------------------After reading other reviews are rebuttals---------------------\n\nAfter reading all the reviews from other reviewers and corresponding rebuttals, I think this paper is a good paper and enough to be accepted in ICLR. \n1. I think it has a clear difference from f-GAN. It can provide a new loss function for the generative models which can further extend the success of generative models in the future.\n2. Experiments are not super interesting but at least it has some intuitions corresponding to the authors' claims.\n3. General theoretical results for the generative models (such as when should we use which loss) is a very difficult problem to solve. Maybe this paper can provide some intuitions for solving that large problem. But it seems too much to ask this thing to the authors of this paper. Without that, I think this paper is still worth to present to the ICLR readers and participants.\n\nTherefore, I am standing on my original score (7).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}