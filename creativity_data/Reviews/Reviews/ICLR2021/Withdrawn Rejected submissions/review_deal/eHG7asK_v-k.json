{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "There was some slight disagreement on the paper, but the majority of reviewers agree that although some answers of the authors on questions brought good clarification, other issues still remain problematic. Some of the assumptions remain unclear (w.r.t CDTE), and reviewers still have doubts about the global convergence and weak stable fixed point concept, that lack clear math details.\nThe experiments are also still a bit too immature, more comparison is needed, as well as an evaluation on other domains."
    },
    "Reviews": [
        {
            "title": "Clearly written, sufficient number of experiments, but some possible points of improvement",
            "review": "MULTI-AGENT TRUST REGION LEARNING\n\nSummary\n\nThe authors establish a trust-region method for multi-agent RL. The idea is to apply TRPO to each agent separately, but with a trust region whose properties for agent i, depend on those of agent not-i. The authors propose, at iteration k, to find a weak stable fixed point for a game M_k, defined by a matrix of payoffs given by the (expectation value of) advantages. They show that solving for the equilibrium of M_k yields policies that monotonically improve performance, i.e., each agent's payoff at least does not decrease. \n\nThe authors addresss the extra cost introduced by having to solve a game at each iteration, which does not scale well as the number of agents N grows. \n\nThey compare with independent learners (IL), and evaluate 3 synthetic experiments, multi-agent variations of Mujoco sims, and Atari pong. For games have fully cooperative rewards, but feature adversarial dynamics/rewards, the authors compare convergence rates (faster convergence in synthetic games), pair-wise win-rate (MATRL beats IL agents on pong), and overall reward-growth (MATRL improves reward faster in cooperative games). \n\nStrengths\n\n- The paper is written clearly, and the technique is relatively straightforward. \n- The empirical evaluation seems sufficient in terms of showing MATRL improves over IL. \n\nWeaknesses\n\n- I would have liked to also see some comparison with techniques like LOLA, symplectic gradient adjustments, etc, as it's unclear what equilibrium MATRL converges to. Alternative (higher-order) methods don't necessarily converge to the equilibria of the original game -- does MATRL do so? Are MATRL solutions still a 'true' Nash equilibrium of the underlying game? \n- I would have liked to see more reporting on the runtime of solving the meta-game. As the authors mention, this is a crucial issue in scaling this up, and it would be good to see some quantitative benchmark of run-time vs number of agents. \n\nQuestions\n\n- Eq 2 has a typo? D(pi_i, pi_i)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Blind review",
            "review": "This paper presents a new trust-region method for multi-agent reinforcement learning (MARL). This approach extends ideas from single-agent trust-region methods to construct a smaller meta-game representing possible policy changes for each agent. The meta-game can then be solve to provide policy updates for the agents. Theory is provided for the meta-game (and corresponding restricted underlying game) and experimental results are shown with a number of baselines. \n\nThe idea of extending trust region methods to the multi-agent case is an interesting idea. As the paper points out, the multi-agent case is more complicated since the other agents are also learning, making the environment appear non-stationary from a single agent's perspective. The idea of using a meta-game with limited (i.e., 2) policy update choices at each step greatly simplifies learning, while at least somewhat taking other agent changes into account. Also, the method is based on theory for converging to a (weak stable) fixed point. \n\nThere are a lot of interesting ideas in the paper and the experiments are promising, but many of the details in the paper are not clear.\n\nFor example, the general assumptions and algorithm become clear in Figure 3 and Algorithm 1, but these should be explicitly stated much earlier (e.g., at the beginning of 3). Even so, it still isn't clear exactly what is assumed to be known about other agent policies. It appears that the process is fully centralized, where all agents know all other agents policies at all times. If this is indeed the case, why is it reasonable to use a centralized approach in a self-interested setting? In MARL, even assuming other agent actions are observed is a strong assumption, so assuming other agents policies are known is very strong. Furthermore, the paper says \"MATRL also provides fully decentralized execution and only requires a centralized mechanism to adjust the step size rather than a centralized critic or communication.\" but it appears that the training phase would need much more than that (to solve the meta-game) while the execution could be decentralized. A more detailed description of the algorithm should be given along with the assumptions needed. \n\nIt also isn't clear how useful the theory is since it only applies to the restricted underlying game rather than the full game. Since it appears to assume the other agents are fixed, it would be useful to at least speculate what could be said about the full case (e.g., when other agents are not fixed and when it can't be assumed the other other agent policy updates are known). Also, the game values in 5 would just be estimates in the MARL case (due to sampling and other agent changes) so it doesn't seem like the theory would apply in that case. These issues should be clarified in the paper. \n\nThe experimental results are impressive and show the proposed method works well. The method (MATRL) converges faster or to better values in all the domains. Nevertheless, it is not clear how the domains were chosen and many of the baselines are weak. In terms of the domains, the paper should discuss why a subset of domains is chosen (i.e., why these particular domains). In terms of baselines, there are stronger methods in each case. For example, many other MARL methods would apply in the Checker, Switch and MuJoCo domains (e.g., COMA and MAAC). Why use only VND and QMIX (which are very different than the proposed method)?  Why not use any other method (besides independent learners) in multi-agent pong? Lastly, several of the domains are partially observable, but the method is only defined in terms of full observability and nothing is said about how it was adapted to run in the partially observable case. It is assumed that they pretend the observation is state, but this should be made clear. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel algorithm addressing an important problem.",
            "review": "---- Summary ----\nThis paper proposes a multi-agent learning algorithm where after each policy improvement step, a small meta-game is analysed to propose a corrected step. The paper performs experiments in various MARL environments to test the approach.\n\n---- Reasons for score ----\nI recommend accepting this paper. The paper addresses the important problem of the non-convergence of independent learning in MARL. The algorithm proposed is novel and well justified, and the experiments show that it yields an improvement over independent learners.\n\n---- Pros ----\n1. The fundamental idea of the paper - taking independent policy improvement steps, and then analysing a local metagame to decide how to update policies - is interesting and novel.\n2. The practical algorithm this leads to helps with the problems of independent learning in MARL, without a large increase in algorithm complexity.\n3. The experiments demonstrate the advantages of the algorithm, in a variety of domains and with a fair comparison to the IL methods MATRL is being based on and other relevant baselines.\n4. The paper is situated well with respect to the existing literature.\n\n---- Cons ----\nIt seems to me that the effectiveness of the proposed method is likely to be heavily influenced by the underlying IL algorithm employed. In particular, an IL algorithm making very large policy updates is likely to need more correcting. For this reason, it would be interesting to investigate how the performance of MATRL and IL varies with a range of policy update sizes, and it would also be useful to clarify how the hyperparameters for the various experiments were selected.\n\n---- Typos and other minor comments ----\n1. Please clarify the meaning of the error bars in Figure 5.\n2. In the sentence before section 3.1, steps->step and details->detail.\n3. I didn't understand this in section 3.1: \"We set agent i’s to make a monotonic improvement of its policy.\"\n4. Near the bottom of page 3, \"conflict interests\" -> \"conflicting interests\"",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but has a number of holes",
            "review": "Summary:\nThis paper proposes a modification of the Independent Learners trust region policy optimization method in general sum games. The modification consists of first forming a “meta game”—ie. the matrix game in which each agent’s options are his previous policy and his independent trust region optimized policy—-and then interpolating between the two for each agent according to a Nash equilibrium of the meta game. The paper shows that this algorithm results in each step generating a “weak stable fixed point”. The paper concludes by showing a number of experimental results indicating the convergence and overall performance of the method as compared with relevant baselines in a number of different games.\n\nStrengths:\n* good motivation of why independent learner algorithms may not converge due to missed coupling\n* clever construction of meta game\n* good choice of variety of experiments\n\nWeaknesses:\n* the notion of “weak stable fixed point” is extremely weak. For example , Mazumdar 2020 (who is cited) and many others consider much stronger criteria like local Nash or differential Nash, or even quasi-Nash. As I understand it, “weak stability” is just requiring that iterates don’t ever find a local minimum in the coupled strategy space of all agents. Not only is this artificially introducing a coupling requirement between the agents, but it is basically just saying that the agents shouldn’t ever find themselves all wishing to change collectively.\n* the preliminaries are sometimes stated in a way that indicates only two players and other times N players\n* figures 1 and 2 are confusing\n* I find theorem 1 to be hard to parse (much notation is not defined clearly), based on very strong assumptions (alpha coupling, discrete spaces), essentially pointing out a fact about prior work on independent methods (not the current proposed one), and overall unnecessary in the present paper. I would advocate removing it.\n* it should be clarified that the rho are really distributions corresponding to mixed Nash strategies in the meta game, rather than deterministic strategies (which may not even exist)\n* in the experiments, it is not statistically useful to show results for only 5 seeds. I understand that these things are expensive and I have this comment for the vast majority of work in RL I have ever read, but showing plots like figure 5 is misleading, esp. without an immediate caution in the caption and remain text\n* I found figure 6 and the related text very confusing and not really useful in understanding what is going on\n* only validating convergence in matrix games is very limited. I was missing a more complete discussion of convergence, if only experimental\n\nNitpicks:\n* there were numerous typos and syntax/semantic errors throughout the paper. I would recommend employing the services of a copy editor in future\n\nOverall:\nI like the main concept of this paper, but I really can not recommend it for publication at this time, I have listed a number of directions in which I feel this paper could be improved in rebuttal or even in a later submission if it goes that route. My main concerns are: (1) the weakness of the theoretical property of “weak stability”, (2) lack of a more complete evaluation of convergence, (3) general confusion throughout reading the paper.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}