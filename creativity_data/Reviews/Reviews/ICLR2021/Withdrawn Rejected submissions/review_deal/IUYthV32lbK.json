{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "I thank the authors and reviewers for the lively discussions. Although reviewers agreed the work is interesting, there are some concerns about the significance of the results and experiments. None of the reviewers were strongly supportive of the paper while majority of them suggest that the paper needs a bit more work before being accepted. Also, reviewers suggest that the paper is not easy to follow and its writing should be improved. Given all, I think the paper , at the current stage, is below the accept threshold. I encourage authors to edit the paper according to the suggestions by the reviewers. "
    },
    "Reviews": [
        {
            "title": "Official Review 1",
            "review": "This paper studies the following problem: How to train a certifiably robust classifier with ensemble methods? The authors considered two types of ensembles: the weighted-average ensemble and large-margin ensemble. They first derived theoretically sufficient and necessary conditions for robustness under two types of ensembles, with the conclusion that large confidence margin and diversified gradients are two factors which contributes to the robustness of ensemble models. Diversity-regularized training, a method of designing loss functions for training ensemble models, is proposed motivated by their theoretical findings. They applied this methodology to randomized smoothing, performed extensive experiments and showed non-trivial improvement over single model methods.\n\nThe paper is very well-written and provided extremely detailed discussion about many different aspects of the problem, both theoretically and empirically. From the reviewer's point of view, this is the strongest part of this work. I am very impressed by the level of detail in the appendix, which covers many interesting questions like under which scenario is WE better than MME, why is ensemble before smoothing better than ensemble after smoothing, to name a few. I wish more papers in the community are written in this way.\n\nHowever, my current evaluation to this paper is a weak accept - it is a bit conservative, but I think it's based on some valid concerns, detailed below. \n\nThe experimental results, while showed non-trivial improvements over single model baselines, may not be very strong. In most cases, the improvements are like 3~4%, sometimes a little over 5% in smaller radius settings comparing to smoothadv. This improvement is much smaller than some of the earlier works like smoothadv vs gaussian. Also in certain settings, the improvement over other single model baselines becomes very small (e.g. <2% over MACER in Table 12). The performance is also overall very similar to recent ensemble baselines like Liu et al. 2020. My understanding is that this shows a limitation of ensemble-based methods in certifiable robustness.\n\nTo summarize, despite of the limitations mentioned above, I think this work is overall good enough for recommending acceptance and thank the authors for their effort.\n\nMinor comments:\nI don't quite get the point of appendix D.3, in particular, the reasoning that trying to justify the uniform distribution assumption of confidence scores. The concentration of measure in high dimensional Gaussian does not imply the uniformity of confidence scores. Although I do understand this assumption as a concrete example  trying to get more interpretable results.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theory, limited experiments with no benefit for large-scale data",
            "review": "The paper offers a novel idea regarding improving robustness of ensemble models with a rigorous mathematical background. A comparison between two types of ensemble models (Weighted Ensembles and Max-Margin Ensembles) and to single models offers very good insight into the theoretical dynamics of certified robustness.\nUnfortunately, the presented methods for practical applications are two simple regularization terms (of questionable form, see below), and, more importantly the authors are not able to conclusively show any benefits empirically - only MNIST and CIFAR10 show improvements vs baselines, ImageNet is evaluated as the only large-scale dataset, but with negative results.\n\nPositive:\n* a proof is offered why certain regularization is beneficial for robustness, while other works are only based on empirical results\n* the work consists of several proofs, greatly extending the theoretical background of ensemble model robustness\n* even if you ignore the proposed regularization term, theoretical insights of the robustness of Weighted Ensemble vs Max-Margin Ensemble vs single model are of value for further understanding the topic. (Figure 3 in the appendix is great)\n* the regularization term seems to work consistently well for CIFAR10 and MNIST\n\nNegative:\n* The paper has too much information in too little space - “related work” and “conclusion” seem artificially short to fit the 8 pages.\n* The paper reads like two papers glued together (one would be chapter 2 + chapter 4 + appendix E about the regularization term DRT, the other is chapter 3 + appendix C + D about the theoretical certified robustness of the ensemble types with smoothness). The paper has two main pillars (like mentioned above: chapter 2 and chapter 3), but chapter 3 offers no results/conclusions for chapter 4 (the experiments), it seems strangely misplaced between ch. 2 and ch. 4.\n* some notations are a bit sloppy, some assumptions are questionable\n* the GD regularization term (Gradient diversity loss) should have a different form: In the previous section the authors state that “..the magnitude of gradient sum could be efficiently reduced as long as their directions are diverse.”, but the GD term is still the L2 norm instead of cosine similarity. \n* Experiments are extremely limited and the proposed method does not work on large-scale data\n\nDetailed comments:\n* In Sub-Section “Key factors for the certified robustness of an ensemble” (p. 5): The authors write “Though reducing the gradient magnitude [something positive], it may hurt the model accuracy significantly” and continue showing how the L2 norm of the sum of two vectors contains the cosine similarity, which is critical for model diversity. Furthermore they argue that by reducing the L2 norm the cosine similarity is also reduced and continue using L2 as their regularization term called Gradient Diversity Loss. But if reducing the base model gradient norm is potentially so bad, then why is the regularization not exclusively the cosine similarity between the two gradients? This should be evaluated.\n* based on the analysis, the authors argue the confidence score margin has also important influence on the robustness, so it is artificially increased through another regularization term called Confidence Margin Loss. This opens two follow-up questions:\n    * naturally cross entropy already tries to reach high confidence scores, so the question is if the Confidence Margin Loss achieves anything that cross entropy implicitly does not, or if it maybe only stabilizes training convergence. I would like to see some results on that\n    * By increasing the confidence scores further, any improvement in regarding robustness may come at the cost of an increased expected calibration error - this would be interesting to evaluate\n* Ablation study: an additional evaluation of how beneficial each loss term is for the robustness is missing\n* Theorem 4 and onwards: The random variable “epsilon” cannot by element of R^d, since a random variable is a function.\n* “Definition 1” has an important typo: “epsilon” should be “r”, otherwise the term “r-robust” makes no sense, if no r is present in the definition\n* section 2.1: The whole paper is about Weighted Ensembles and Max-Margin-Ensembles, but their formal definitions are moved to the appendix, please move to main text\n* Proposition 1: In “min”, “y_i =/= y_0” is not required because if y_i == y_0, the term is equal 0 (this is not an error, but less clutter is preferable)\n* Definition 2: the second gradient is w.r.t. x, but the argument is y\n* Theorem 4 and 5, and Corollary 2 assume the confidence scores across several base models are i.i.d. and symmetric random variables. I am not usure if this assumption holds in any case and would appreciate some discussion here: How can the output be seen as symmetrical? Confidence scores are between 0 and 1, so exact symmetry is almost always impossible as long as they are close to 1 (which they are due to the training and if they are in-domain)...\n\n\n######Post-rebuttal####\nThe authors have addressed some of my concerns and I have raised my score accordingly \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lacks non-trivial theoretical extension or demonstrated empirical improvement ",
            "review": "Certified robustness approaches have been studied for single models based on interval propagation as well as randomized smoothing. The use of ensembles for empirical robustness has also been studied in the literature. This paper attempts to theoretically study the certifiable defense achieved by ensembles. The paper analyzes the standard Weighted Ensemble (WE) and MaxMargin Ensemble (MME) protocols, and proves the necessary and sufficient conditions for robustness under smoothness assumptions. The key idea is to show and utilize the diversification of gradients and large confidence margins. \n\nPros:\n\n+ The paper addresses an important challenge of adversarial robustness via ensembles and attempts to develop theoretical bounds for these defenses. \n\nCons:\n\n- Some theoretical discussion is rather straight-forward formulation of consequences of the definitions of robustness and ensemble methods. Proposition 1 and Theorem 1 follow directly from the definition of WE and MME, and r-robustness. \n\n- There are several notation lapses which make the paper difficult to read. In definition 2, it appears the intent is to take the partial gradient of the model at two different points x and y and then bound the ration of the difference of the gradients and the distance between two points (which would be the curvature). x,y are being used as values and the variable. The boldface font is not used consistently.  Another example is the missing relation between r and epsilon (which is described in the appendix). These make simple results difficult to parse, and negatively effect the readability of the paper and obscure the identification of novelty. \n\n- For Theorem 2, let us set N = 1 (that is the degenerate case when the ensemble is a single model), we see that the rate of change of difference between the top prediction and other predictions are now bounded by a linear spread of the difference divided by the robustness radius r and then we add or subtract (depending on sufficiency/necessity) the impact of curvature. Rearranging the terms so that first and third are on the same side, the theorem will read:\nr ( gradient term for prediction difference ) +/-  r^2 ( curvature term for prediction difference ) <=  prediction difference.\nIsn't this just a Taylor series approximation of the prediction difference using bounds on the curvature term (assumed as part of the definition)? \nNow, if we bring back any arbitrary N, the same would be applicable by the definition of how decisions are made by WE (the \"prediction difference\" term is now changed). \nTheorem 3 terms can be similarly rearranged to make the statement more obvious. Is the reviewer missing some non-obvious observation or challenge in proving Theorem 2/3?\n\n- Despite several other papers also using the general term of \"certified robustness\", it is important to note that this robustness is against a rather benign perturbation model. Typical perturbations (in particular adversarial attacks or other natural perturbations such as fog, rain for vision) do not fall into this class. But the reviewer is not concerned with this aspect heavily given the prevalence of the rather generic name of \"certified robustness\" for such approaches. \n\n- The empirical enforcement of diversity in ensembles has been studied in literature such as Pang et al 2019. https://arxiv.org/abs/1901.09981 uses cosine similarity.  Experimental evaluation with these methods is crucial to understand the value of the proposed approach. The current evaluation is very limited. \n\nQuestions and suggestions to the authors:\n\n1. It might be a good idea to bring the relationship between r and epsilon in the main text from the appendix. r-robustness is the fundamental definition and in its current presentation r does not occur anywhere. \n\n2. Could you help understand the concern in Weakness bit 3? This will help the reviewer better appreciate the theoretical novelty of the paper. Currently, the theoretical statements appear to make obvious statements once we get through the rather clunky notation and presentation choices. \n\n3. Why is cosine used for diversity? There are several ways to enforce diversity. Was cosine selected arbitrarily or is it motivated by some theoretical insight? \n\nThe paper addresses an important challenge of mathematically well-motivated defenses against perturbations. In its current form, the paper appears to formulate rather simplistic observations as theoretical results with a rather dense presentation. The empirical evaluation is significantly incomplete. \n\nAfter discussion with authors\n----------------------------------------\n\nThe reviewer thanks the authors for clarifications. With the confusion from the typos resolved, the paper is easier to follow. The technical correctness of the paper is not in doubt any more. But two major concerns still remain: \n\n1.  The diversity enforcement has been reported before, and more comprehensive discussion of related work would be useful.  A detailed empirical analysis would also make the paper balanced and not heavily reliant on the novelty/depth of theoretical contribution. \n\n2.  The main claimed theoretical contribution summarized in Lemma B.1 is tedious but a derivation of an obvious statement (as sketched out in the original review). \n\nThe reviewer is raising the score to encourage this work, but still holding to the recommendation of not accepting the paper in its current form. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Difficult to Follow",
            "review": "\nThis manuscript provides proofs for certified robustness for ensembles and proposes a new approach called Diversity Regularized Training (DRT) based on the theoretical findings. In addition to the standard loss term, DRT contains two regularization terms: (i) gradient diversity (GD) loss, and (ii) confidence margin loss (CM). These loss terms encourage the joint gradient difference for each model pair and large margin between the true and runner-up classes for base models. The authors discuss some theoretical findings in detail and demonstrate the performance of DRT on several datasets. I find the work valuable in the sense that it nicely combines ideas of ensembling and certified robustness. However, it was difficult to follow all the theoretical results and how they motivated the main finding (DRT) was not clear. Below are my comments/questions:\n\n- I want to thank the authors for nice summary in Appendix B1 and B2 on certified robustness and their map to ensembles.\n\n- Theorem 1, which serves as the foundation for the paper, assumes that either best prediction or the runner-up prediction is true class for any base model. I wonder how realistic this assumption is.\n\n- Also in theorem 1, I am confused that both f and y have index i. The number of base models does not need to match the number of classes.\n\n- In proof for Theorem 2, it could be helpful to mention each step. For instance, it was not clear to me where Lagrangian reminder was applied. Also, I recommend proving necessary and sufficient conditions separately.\n\n- In theorem 3, N is assumed to be 2. Is it possible to extend this to N > 2? So, discussion on such this could be helpful.\n\n- In the first paragraph on page 5, I do not follow which equation was meant in RHS.\n\n- These statements on page 5 are not clear to me: \"... and leads to higher certified ensemble robustness.\" and \"Thus, increasing confidence margins can lead to higher ensemble robustness.\"\n\n- For GD loss, why only pairwise summations were considered? Isn't it easier to look at overall diversity of gradients?\n\n- All pair-wise computation of quantities in regularizer terms should come with some computational complexity. It would be nice to include a discussion on this.\n\n- Results for Salman et al., 2019 on Table 2 do not match the paper of Salman et al., 2019 and imagenet results are not convincing. I understand ImageNet data can take too long but should we worry that DRT requires more hyper-parameter tuning? Some discussion on this would be helpful.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}