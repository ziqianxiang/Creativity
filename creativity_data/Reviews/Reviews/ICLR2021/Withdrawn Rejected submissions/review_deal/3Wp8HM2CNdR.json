{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper received borderline recommendations (5, 5, 6, 7) but even the two slightly more positive reviewers were lukewarm (R1 and R2). While the reviewers acknowledged the heavy computational requirements to do an apples-to-apples comparison with existing baselines, they remain underwhelmed with the lack of experiments. I agree with their criticism; even though the proposed idea seems promising, without comprehensive experiments, it is difficult to judge the significance of this work. R1 commented after the discussion period that an earlier version of this paper actually had ImageNet results. R4 made excellent suggestions to improve the paper further. The authors are strongly encouraged to incorporate them into their future submission.\n\n(I am copying R1's comment below in case it is invisible to the authors after the notification.)\n\nSorry for the late update -- I have read the rebuttal earlier. I would like to keep my acceptance rating but after the rebuttal I am fine either way. The paper first appeared in March on ArXiv, so indeed it is a concurrent work (actually an earlier work compared to BYOL or SwAV). We have actually tried to reproduce the results in the paper a while back but it did not go well (could not reproduce it), but this time the submission also includes the code. While I haven't run it, I trust the results are reproducible (maybe there are some tricks that I am not aware of).\n\nRegarding running experiments on toy examples -- I can understand that this research is resource-constrained for ImageNet, but the earlier draft actually had some results on ImageNet (60+ top-1 accuracy) (see appendix of https://arxiv.org/pdf/2007.06346v1.pdf), and for some reason this submission removed that. So this is not a positive sign. Overall, my experience for CIFAR vs ImageNet is that it is easier to make things work on CIFAR, while it is much harder to do so on ImageNet. So maybe some trials are indeed done by the authors, but they choose to not report it in the submission for some reason. On the other hand, one can argue that results on toy datasets are good enough contributions for an early develop of something and they are just not ready for larger and more challenging datasets yet.\n\nTherefore, this paper is quite a struggle. I hoped to see a better-than-this submission as this paper actually had all the time from March to October to improve its quality of experiments (actually even for ImageNet, one can to dozens of cycles on it during this time), but it did not for some reason."
    },
    "Reviews": [
        {
            "title": "A well-established method for unsupervised learning with contrastive loss functions with not very impressive results.                                            ",
            "review": "This paper proposes the mean square error loss with whitening operation to project positive pairs closely to each others while projecting the different positive pairs far away from each other on a unit sphere. This way, similar to BYOL, this paper removes the construction of negative pairs while improving the MoCo-V2 slightly on not very challenging benchmarks. The authors can find my questions/comments in the list below.\n\n1. What does gray and blue squares represent in figure 3? I believe figure 3 can be revisited for the sake of better understanding the method. For example, I was not able to understand why v_i and v_i+8 represent the same image. In this current shape, it creates confusion rather than helping for clarity.\n\n2. In the Batch Slicing section, the authors mention that the size each of sub-batch should be close to the size of the embedding x 2. Does this make sense? Is the size of embedding 512? If yes, do you have 1024 samples in a sub-batch? I would be happy if the authors can comment on this.\n\n3. I think my biggest concern with the paper is the lack of extensive experiments. All the experiments are done on small-scale datasets which is highly questionable when they are used for unsupervised learning. It would be much more convincing to have experiments on ImageNet which is a standard experiment for unsupervised learning.\n\n4. The improvement on relatively less challenging benchmarks are very marginal. Even in some cases, CIFAR100, contrastive loss does better than the proposed one. And the proposed method only outperforms BYOL in CIFAR10.\n\n5. My impression is that the contrastive method in table 1 and 2 represent the MoCo-v2. I would replace it with MoCo-v2 in these tables as there are many other methods that uses contrastive loss functions.\n\n6. Another problem with the experiments is that, they lack experiments on different downstream tasks such as object detection. \n\n7. The authors try the Euclidean and MSE distance to project positives closely. Did they consider the cosine similarity loss? I did not understand the point of using Euclidean distance experiments without normalization? What do they exactly prove? And we can always normalize the embeddings as it is done in the other methods. I would be happy to receive some comments on this from the authors.\n\n8. Finally, it would be nice to the advantages of the proposed method (removal of negatives) in terms of training complexity. Does it, as expected, reduce the complexity in the training time? If yes, quantifying it would increase the strength of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple and clean method, with very weak experiments. ",
            "review": "This paper proposed an MSE loss function with whitening (W-MSE) for self-supervised representation learning. The motivation is to reduce the demand of negative examples in contrastive representation learning. The proposed W-MSE loss function is compared with popular contrastive loss on a few benchmarks.\n\nContrastive learning is a popular topic in the self-supervised learning domain. Most of the existing methods rely on negative samps to avoid trivial solutions. This paper proposed a simple and clean solution to tackle this problem, by using whitening in the loss term. This paper is also well explained and illustrated.\n\nThe main weakness of the paper is the experiments. Based on the results, I am not convinced that the proposed W-MSE is effective. Here are more comments:\n\n(1) The experimental results are pretty close to the existing contrastive/BYOL baselines. On CIFAR-10 and STL-10, results are saturated thus the diff is very minor. On more challenging CIFAR-100 and Tiny ImageNet, the results are mixed when compared to BYOL. \n\n(2) Given that the results are very close, what are the other benefits of using W-MSE loss? For example, is the training speed faster than the other methods (contrastive, BYOL) with negative sampling? It would be nice to include such results to demonstrate the effectiveness of W-MSE.\n\n(3) This paper claims that without negative sampling, W-MSE loss encourages the use of more positive pairs in the batch. I'm wondering if the authors have tried more positive pairs beyond 4 in the paper.\n\n(4) I cannot understand the motivation of ablating the popular methods with Euclidean distance. Cosine similarity is just simple and it costs nothing compared to Euclidean distance. I think ablating this from existing contrastive loss is not sufficient to show the effectiveness of BYOL. \n \n(5) It would be nice to include a few comparable numbers from literature directly, instead of reproducing their methods. For example, this paper used ResNet18 while the BYOL paper used ResNet50. This paper reports results on Tiny ImageNet while the existing methods report on ImageNet. Note that I do not penalize this paper for this point, as computational cost is high for using larger architecture and larger dataset. But it would be good to have a comparable baseline in the middle ground, e.g. ResNet50 architecture on CIFAR-100 dataset, where BYOL reports 78.4% top-1 accuracy with linear eval. \n\n======Post Rebuttal Update======\n\nI would like to thank the authors for their rebuttal, which has addressed part of my concerns. After reading the authors' rebuttal and other reviewers' comments, I'm still concerned on the weak baselines and mixed results in this paper. Unfortunately, I will keep my rating.\n\nConcrete suggestions to improve this paper in the future:\n\n(1) Strongly recommended: use ResNet50 instead of ResNet18 for the small scale experiments, in this way you get directly the numbers from the literature (e.g. BYOL on CIFAR-100);\n\n(2) Nice to have: for the expensive ImageNet experiments, it would be nice to get comparable results using the smallest comparable architecture (e.g. ResNet50) from the literature. SimCLR claims that \"With 128 TPU v3 cores, it takes ∼1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs\" and MoCo claims that \"For IN-1M, we use a mini-batch size of 256 in 8 GPUs, ..., train for 200 epochs ..., taking ∼53 hours training ResNet-50.\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting method ",
            "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides a simple self-supervision loss function which is computed using only positive samples. In particular, the proposed loss function is based on the whitening of a latent space-features. Results show competitive but not better results than other methods. \n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for accepting. The paper proposes a simple loss function based on whitening and the MSE loss. The method does not need negative samples to contrast with the positives since the whitening process shrinks the overall samples to satisfy the spherical-distribution while the MSE loss attracts the positive ones. Although I have some concerns with the validation I really like the method.\n\n \n##########################################################################\n\nPros: \n\n \n1. Interesting and simple method.\n\n2. Clarity of the paper.\n \n \n##########################################################################\n\nCons: \n\n1. The validation \n \n-- The evaluation has been carried with small and simple datasets (CIFAR,STL and Tiny ImageNet). BYOL authors presented an extensive experiment setup with several datasets. Why not use the same (at least some) databases and compare them with their reported results on the paper (and not use your own implemented version of BYOL)? In fact, some of those datasets are already used in this paper but as you use ResNet-18 instead of ResNet-50 (used in BYOL paper) the results are not comparable. \n\n-- Why not ResNet-50 instead of ResNet-18. Validation would be easier with other published methods\n\n-- W-MSE with d=4 seems to work better than d=2. What about larger d?\n\n-- Although the authors point out that the proposed MSE loss does not need negative samples, negative samples are needed in the whitening transformation. \n\n-- It would be great to decouple the system performance. What is the system performance using Withening with the classical contrastive loss? Which is the improvement provided by the MSE loss over the classical contrastive loss?\n\n-- The authors say that Contrastive Loss needs large numbers of negative samples. How many negative samples were used in the experimental setup? The proposed method is evaluated with d=2 and d=4. Which is the positive/negative proportion of samples in the batches using the contrastive loss? How was this proportion selected?\n\nMinor Comments:\n- Please check: \"On the other hand, Hjelm et al. (2019) have shown that the contrastive loss needs a large number of negatives to be competitive\"\n\n\n \n##########################################################################\n\nQuestions during the rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\nSome typos: \n\n(1) Page 4: matrix and (z_i,z_j) correspond -> corresponds\n(2) Page 7: jitterering --> jittering\n\n\nI think that the paper\n\n\nUPDATE AFTER REBUTTAL:\nThe authors have covered most of my concerns about the paper and I think that the paper has been substantially improved. However, my biggest concern was about the experiment results and  I think the paper still lacks on validation comparison with other methods. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting attempt to remove negative examples by whitening, however experiments are not too satisfying",
            "review": "The paper proposes to first do representation \"whitening\", so that the representations are scattered in the space and not collapsing to a single data point; then compute distance metric on top of that (e.g. Euclidean, cosine similarity). A nice thing about explicit scattering is that it does not require large numbers of negative examples to pull the features apart. Experiments are done on several toy datasets like CIFAR.\n\n+ The paper is a nice, alternative attempt to remove negative examples in contrastive learning. Indeed, large number of negative examples is annoying and this direction is both exciting and significant.\n+ The approach proposed in this paper intuitively makes sense. There are several works already explaining that contrastive learning is essentially doing some kind of scattering in the space. \n+ The proposed approach seems pretty simple. The whitening code is only a dozen lines in PyTorch. I haven't run the code to verify the results though.\n\nI think the experiments are not too satisfying though. \n\n- The comparison is less fair between BYOL and multi-crop version of W-MSE. In SwAV, it shows that with multiple crops, the performance can be boosted quite a bit. I haven't tried on BYOL but I believe it could also be helping there. So the most apple-to-apple comparison between BYOL and W-MSE is the 2-crop version (d=2). In this case, BYOL is outperforming in most entries. Though it can be viewed as concurrent work (I think W-MSE is actually even earlier than BYOL), but the experiment session in the paper is not clear about this.\n- Overall running experiments on these toy datasets are less satisfying, not only because it lacks comparison to other major approaches (like MoCo on ImageNet), but also because the signal we get from smaller datasets may not transfer well to more real-world images. \n- I would like to see a comparison in terms of timing -- maybe BYOL (because of its momentum encoder and it needs 4 forward pass of the network to compute a single pair of losses) is running much smaller in training. W-MSE can run much faster because it only needs 4 (or even 1?) forward pass. This is a potential advantage that W-MSE has, but it is not clear from the paper.\n\nOther than experiments, I am also not too satisfied with the writing. The paper mentions another paper when talking about the key method (how to do whitening, e.g. which is back propagated, which is not) when describing the central technique of the paper. I would like to see the paper more self-contained in the next version. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}