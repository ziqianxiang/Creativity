{
    "Decision": "",
    "Reviews": [
        {
            "title": "It is good to show the convergence rate of MDA, but the theoretical advantage over SGD is not strong.",
            "review": "[Summary]\nThis paper proposes a new variant of the dual averaging method (MDA) developed in the convex optimization literature. The authors adapt the dual averaging method to learning deep neural networks. A convergence rate of O(1/\\sqrt{T}) is also provided for nonconvex optimization problems. Moreover, an interesting property of MDA is shown; MDA is a regularized SGD with time-varying regularization parameters. Experimental results show the better adaptivity of MDA to specific tasks; MDA exhibits comparable performance to the momentum method on CV task and to ADAM on NLP task.\n\n[Strength]\nThis paper is well written and easy to read. To the best of my knowledge, this is the first work that provides the convergence rate of a variant of the dual averaging method for nonconvex problems. Moreover, the better generalization ability of MDA observed in the experiment is attractive.\n\n[Weakness]\nThe derived convergence rate of MDA is as fast as that of SGD, so it seems to be difficult to theoretically explain the advantage of MDA. Although the connection between MDA and regularized SGD with decaying coefficients is certainly interesting, its benefit to the generalization ability is unclear.\n\n[Improvement]\nIt would be nice if the authors could explain the theoretical advantage of MDA over SGD or the role of induced regularization to SGD.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The assumptions used in convergence analysis are strong, and the experiments are weak.",
            "review": "This manuscript introduces Modernized Dual Averaging (MDA), which extends the traditional dual averaging from convex case to nonconvex case. Convergence analysis is provided. Experiments seem to justify its effectiveness in training deep neural networks for CV and NLP tasks. \n\n1. The assumptions are too strong. For example, Assumption 1 (A2) requires boundedness of second order moments, which is not needed for the analysis of SGD in nonconvex case. Assumption 2 is also too strong, how to justify the boundedness of domain without doing projections in the algorithm?\n2. The convergence result in Theorem 3.1 is even worse than SGD, let alone strong assumptions are made. The convergence rate of MDA is actually $O(\\log T/\\sqrt{T})$ instead of $O(1/\\sqrt{T})$ as in SGD.\n3. Why the induced $\\ell_2$ regularization smoothes the optimization landscape? The logic about this argument is seems ungrounded.\n4. In experiments of image classification Figure 1(a), why the performance of SGD+M drops arounds during epochs 200-250? It is not usually the case in practice. I think it is probably because the learning rate schedule is not well-tuned.\n5. In the ImageNet experiment Figure 1(b), the performance of MDA does not outperform SGD+M.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This is an interesting work but there are many aspects need to be improved.",
            "review": "This paper proposes a variant of dual averaging method called modernized dual averaging (MDA). The basic idea is to incorporate momentum (in the averaging form) into the dual averaging scheme. The authors show that dual averaging can be reformulated as SGD with time-varying regularization. Based on this reformulation and the Lyapunov analysis result in Defazio (2020), this paper establishes the convergence guarantee of MDA in the non-convex setting. Empirical evaluations on various deep learning tasks are provided. \n\nPros:\n- The paper is generally well-written and clearly presented. \n- The empirical evaluations cover a wide range of learning tasks such as CV, NLP, NMT, MRI reconstruction.\n- The way that the authors adapt the results in Defazio (2020) is interesting.\n\nCons:\n- The experimental results do not really show the benefits of MDA. Most of the improvement looks marginal. In many cases, Adam still works better. It is thus hard to convince practitioners to use MDA. Also, details for the \"grid-search over step-sizes, weight decay and learning rate schedule\" are missing.\n- If I am not mistaken, MDA is equivalent to SGD+M with time-varying regularization. Then an empirical analysis on the effect of this regularization term (or the choices of $\\beta_k$ and $\\lambda_k$) is necessary (e.g., how much regularization gives the best performance). The authors could also consider choosing different regularization center.\n- The contribution of the theory is described as \"the first convergence proof for a dual averaging algorithm in the non-convex case\". However, in the unconstrained Euclidean setting that the authors considered, dual averaging is not so different from gradient descent. The full potential of dual averaging (constrained, possibly non-Euclidean geometry) is not studied. \n- In this case, the analysis is just a general convergence guarantee, which is not quite related to the deep learning experiments. It would be much better if the authors could provide some insights on the bound (10) as in Defazio (2020).  \n\n\nOther comments:\n- It seems that $x_0$ is missing at many places (e.g., Eq.(3) and the proof of proposition 2.1).\n- Inequality (27) is wrong, the denominator should be $4\\eta (k+2)(k+3)$.\n- No code is provided for reproducibility.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lack of novelty, and missing analysis",
            "review": "This paper proposes a variant of dual averaging algorithm that includes a momentum component. Under a certain choice of hyper-parameters, the algorithm is shown to converge to a stationary point at a rate the same as SGD.  Experimentally, the algorithm is shown to have competitive performance compared to SGD+Momentum and Adam on several tasks.\n\nComments:\n\n> The proposed “modernized” dual averaging is essentially the dual averaging with momentum, which can be seen by simply checking the update rules of the algorithm. I think the authors should make this clearer in the paper.\n\n> As pointed out by the authors that dual averaging resembles SGD in in unconstrained optimization, I think the proposed method should be closed related, and highly similar, to the SGD+Momentum in the same way. I would like to see a discussion of the differences between MDA and SGD+M. Given the high similarity to SGD+M, I don’t think the proposed algorithm is novel. Furthermore, it makes sense to compare the proposed method to SGD+M, instead of Adam.\n\n> There is no analysis on the choice of hyper-parameters $\\beta_k$ and $\\lambda_k$ in the paper. There is no discussion on why the chosen hyper-parameter setting is preferrable and resulting in better performance over other choices. As stated by the authors, the choice of hyper-parameters is “motivated by a careful analysis of our theoretical convergence rate bounds”. However, the analysis provided is only based on the chosen hyper-parameter setting, as in Theorem 3.1.  \n\n> It is not clear why the choice of hyper-parameters of the proposed method is suitable for non-convex deep learning, which is claimed as one of the key contributions. An analysis supporting this claim is totally missing.\n\n> The experimental results are impressive, since they are quite competitive compared to the ones achieved by SGD+M and Adam. \n\n\nQuestions:\n\n> In the experiments, do you use the same hyper-parameter setting as in your theoretical analysis, such as in Theorem 3.1? Or just fine tune them and report the best performance?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}