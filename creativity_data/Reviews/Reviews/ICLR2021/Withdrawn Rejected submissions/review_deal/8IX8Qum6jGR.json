{
    "Decision": "",
    "Reviews": [
        {
            "title": "Many problems need to be elaborated.",
            "review": "#### Originality\nThis paper focuses on the optimization of GNNs and introduces critical representations in GNNs. Learn from DNNs, design a pyramid-like skeleton for GNNs. This is the first work that focuses on the optimization problems in GNNs. \nFor link rewiring, add new edges into the Laplacian matrix increasingly to make this framework more robust. This method is similar to C-GAT[1], which aims to solve the over-smoothing problem in GNNs.\n\n#### Pros\nThis paper shows the critical points in GNNs, and propose a new method that focuses on the optimization problems on GNNs, the saddle point problem can be overcome with the proposed pyramid-like skeleton. Combine with modularity consensus, rewire links for the graph and introduce a robust topological design.\t\nThis is the first work that focuses on the optimization problem in GNNs and provides solid proofs on this problem.\n\n#### Cons\n1. This paper proposed a three-pipeline training framework. However, it lacks a good motivation of this framework which focuses on global model contraction, weight evolution, and link's weight rewiring. In GNN literature, the expressive capability is related to whether a model can discriminate subgraphs [2]. What is the connection between the proposed framework and [2]?\n2. From the perspective of the reviewer, the proposed framework only provides one guideline to design GNN architecture in terms of width, based on vanilla GCN, whose contributions looks inadequate.\n3. What is the difference between Banach Fixed Point Theorem (in section 2) and Schauder fixed point theorem (in section 3)? More explanations should be provided.\n4. In terms of GNN architectures, the paper only discusses GCN, GraphSage, message passing, while ignored some important GNN models, like GIN [2], GAT [3], SGC [4], etc. \n5. In the second paragraph of Section 4, “An intuitive understanding of the topological robustness is to provide path redundancy between vertices. When one path fails, communication can continue through other alternative routes.” Any reference for this argument?\n6. In Section 4,” For weights in GNNs, there are both positive and negative values.”, what does the “weights” mean? The eigenvalues of the Laplacian matrix?\n7. The presentation of the paper can be improved. Especially, the references formats are wrong. The authors should refer to the instructions of the ICLR submission guidelines in the website. Besides, it is not clear to me in experiments. \n\n    a) The descriptions for the figures are too few to understand. In Figure 2, what does the legend “2,3,4” means? In figure 3, what does the y-axis mean? For Figure 2-4, too few descriptions to understand these subfigures.\n\n    b) In section 5.1, the experiments use 2-layer or 3-layer architecture, what is the standard when choosing layer numbers in Figure 2?\n\n    c)\tBesides, what is the difference between width and feature dimension in GNNs. For Cora, CiteSeer and PubMed, a small feature dimension like 64 is enough for GNNs. The width is too large for these 3 datasets\n\n[1]: Wang G, Ying R, Huang J, et al. Improving graph attention networks with large margin-based constraints[J]. arXiv preprint arXiv:1910.11945, 2019.\n\n[2] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks?,” in International Conference on Learning Represen- tations, 2019. \n\n[3] P. Velicˇkovic ́, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” in International Conference on Learning Representations, 2018.\n\n[4] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. In ICML.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting theory, insufficient experiments",
            "review": "Overall, the paper provides an interesting approach for GNN architecture design.\n\nHowever, to me, the overall framework is not coherent. I am confused by the overall contribution of this paper. The paper seems to combine three different techniques together and claiming it as a framework. \nConcretely, the paper mainly proposed two innovations, \"3 THE TRAINING ISSUES AND GLOBAL SKELETON IN GRAPH NEURAL\nNETWORKS\" and \"4 ROBUST TOPOLOGICAL LINK REWIRING\".  Why these 2 aspects are crucial for GNN architecture design? Other aspects, such as the GNN layer used (GCN, GAT, GraphSAGE layer) should be important for GNN architecture design as well.\n\nThere is barely any discussion for the experimental setting. This is bad for the main manuscript, as reviewers are not required to read the appendix, and the main manuscript should be at least self contained.\n\nFigure 2 and 3 are confusing as well. The captions are too brief. What do different subfigures mean?\n\nOther comments:\n1 Typo. \"we Fig. 2\" below Section 5.1.\n\n2 \"Table 1: Performance comparison of different graph classification methods\". This is really confusing. The paper is using node classification datasets. How do they work for graph classification?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes an approach for rewriting the graph structure of GNN during its learning. However the proposed approach is totally heuristics and it is hard to see the idea behind each step.",
            "review": "\n1. I think entirely the description in this paper is rather too abstract, and maybe more mathematical clarification would be needed. \n\n2. The proposed algorithm is explained by just one paragraph with only seven lines, and the description in this paragraph is again too abstract and so the procedure is very hard to reproduced. Some more detailed description, such as a pseudocode, would be useful.\n\n3. Eventually the proposed approach is totally heuristics, and it is hard to understand what kind of idea behind each step (I guess maybe each step is connected to some part of the description done before the method is shown, though). This is  hard to evaluate. \n\n4. The evaluation is empirical. I think even from empirical experiments, it should be clarified what part of the algorithm is most useful to boost the performance.\n\n5. Here is a minor point. The pyramid-like shrinkage property can be explained through illustrations.  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}