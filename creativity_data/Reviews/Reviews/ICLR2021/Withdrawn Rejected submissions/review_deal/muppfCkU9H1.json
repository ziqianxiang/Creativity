{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nThis paper has been reviewed by four knowledgeable referees. Two of them slightly leaned towards acceptance, whereas the other two suggested rejection. The main issues raised by the reviewers were (1) limited novelty [R1,R2], (2) missing baselines and ablations [R1,R3], (3) limited insights on the spectral analysis [R2], and (4) missing motivation behind modeling choices [R1,R3]. The rebuttal included a number of experiments requested by the reviewers (e.g. ablation with diffusion only [R1,R3], extended Diffusion GCN [R1], APPNP baseline [R3]), and adequately motivated some of the modeling choices. \n\nThe central question of the reviewers' discussion was whether the contribution of this paper was significant enough or too incremental. The discussion emphasized relevant literature which already considers multi-hop attention (e.g. https://openreview.net/forum?id=rkKvBAiiz [Cucurull et al.], https://ieeexplore.ieee.org/document/8683050 [Feng et al.], https://arxiv.org/abs/2001.07620 [Isufi et al.]), and which should have served as baseline. In particular, the experiment suggested by R3 was in line with some of these previous works, which consider \"a multi-hop adjacency matrix \" as a way to increase the GAT's receptive field. This was as opposed to preserving the 1-hop adjacency matrix used in the original GAT and stacking multiple layers to enlarge the receptive field, which as noted by the authors, may result in over-smoothed node features. The reviewers acknowledged that there is indeed as slight difference between the formulation proposed in the paper and the one in e.g. [Cucurull et al.]. The difference consists in calculating attention and then computing the powers with a decay factor vs. increasing the receptive field first by using powers of the adjacency matrix and then computing attention. Still, the multi-hop GAT baseline of [Cucurull et al.] could be extended to use a multi-hop adjacency matrix computed with the diffusion process from [Klicpera 2019], as suggested by R3. In light of these works and the above-mentioned missing baselines, the reviewers agreed that the contribution may be viewed as rather incremental (combining multi-hop graph attention with graph diffusion). The discussion also highlighted the potential of the presented spectral analysis, which could be strengthened by developing new insights in order to become a stronger contribution (see R2's suggestions). \n\nTo sum up, this was a very discussed paper, where the reviewers ultimately reached a consensus to reject, with no strong opposition. I agree with the reviewers' assessment and therefore must reject. I encourage the authors to follow the reviewers' suggestions and consider the multi-hop baselines as well as the hints provided by the reviewers about the spectral analysis to strengthen their work.\n"
    },
    "Reviews": [
        {
            "title": "Important problem but the proposed solution may be not powerful than existing models ",
            "review": "Summary:\n\n     Conventional Graph Neural Networks (GNNs) learn node representations that encode information from multiple hops away by iteratively aggregating information through their immediate neighbors. Self-Attention modules have been adopted to GNNs to selectively aggregate information coming through the immediate neighbors at different propagation stages. However, current self-attention mechanisms are limited to only attend over the nodes' immediate neighbors and not directly over their neighbors that are multiple hops away. Here in this work, the authors intend to address this issue and propose a means to obtain attention scores over indirectly connected neighbors. \n\n    The message passing paradigm is commonly adopted in GNNs because directly computing the higher powers of an adjacency matrix is not scalable. The same scalability concern is present for this work, which tries to obtain attention scores for indirect neighbors directly. Thus in order to solve this issue, the authors propose to diffuse the learned attention scores from their 1-hop neighbors to neighbors that are multiple hops away, thereby providing a means to directly obtain attention scores over indirect neighbors that are reachable from the nodes. \n\n——\nPros:\n\n\tThe paper is well written.\n\tThe paper provides experimental results for both homogenous and multi-relational graphs.\n\n——\nConcerns:\n\t(i) Proposed methodology being more powerful than GAT is arguable:\n\n\tWhen the attention scores for indirectly connected neighbors are still computed based on the immediate neighbors' attention scores, it is not convincing enough to be argued as more powerful than GAT, which learns attention scores over contextualized immediate neighbors.  Also, the approximate realization of the model described in Eqn: 5 follows a message-passing style to propagate attention scores. Suppose it is to be argued that standard message-passing-based diffusion is not powerful enough to get a good immediate neighbor representation that encodes neighbors' information from far away. In that case, it is not immediately clear how a similar diffusion, when used for propagating attention scores from immediate neighbors to neighbors multiple hops away, will be more powerful. \n\n(ii) Experimental results are not conclusive: \n\n\t\t(a) Effect of Layer-Norm and FeedForward \n\t\t\tOne of the important ablation model that is missing is MAGNA without feed-forward and layer-Norm components. Currently, it is not clear how much of an improvement is achieved because of these standard two components.  \n\t\t(b) Disentangling the effect of page-rank from the attention diffusion\n\t\t\tSince Diffusion-GCN is also based on Page-Rank based propagation, it would be helpful to compare with the Diffusion-GCN model with these two components appended to them, along with residual connection if already not present. This would help us clarify how much of the gain in performance depends on the page-rank-based propagation compared to the attention propagation. The teleport probability of Diffusion-GCN should also be similarly experimented with and the analysis should be compared with the plots in Figure 3. \n\n \t\t(c) Comparable or not significant gains achieved in Node classification tasks. \n\t\t\tIt is amendable that the authors have reported results for both single-relational and multi-relational graphs. However, the node classification results are not significantly better than GAT or Diffusion GCN on the reported smaller datasets (Table 1) with a single train/val/test split. And on OGB Arxiv dataset, GAT and Diffusion GCN numbers are not reported. Hence, it would be helpful to analyze additional datasets.\n  \t\t\t\t\tIgnoring the benefits of LayerNorm that the MAGNA can leverage, comparing its No-Feed-Fwd version with Diffusion-GCN, which is also based on a page-rank formulation, MAGNA gain ~1% improvement on Cora and Pubmed dataset whereas it falls behind by ~1% in Citeseer. \n\n\t\t(d) Disentangling the effect of Multi-scale diffusion from attention diffusion \n\t\t\tSince MAGNA uses a multi-scale diffusion at each layer, a comparison with a similar non-attentive multi-scale diffusion model like Lanczosnet that is also referred in the paper would be helpful to disentangle\tand understand the importance of the attention mechanism. \n\n\t\t(e) KG Completion: Missing baselines and model variations \n \n \t\t- Missing comparison with Self-attention (GAT) based knowledge graph embedding model, KBGAT. \n\t\t  Nathani et al., Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs, ACL 2019\n\t\t- Additionally, it would be helpful to have similar model ablation studies of MAGNA model as in Table 1 for KG Completion. \n\t\n\t\t(f) Depth Analysis:\n\t\t\t  Diffusion-GCN comparison missing. The performance stabilization over GAT might be arising because of the restart probability. MAGNA only has weights associated with 3 layers, unlike with GAT, which has weights for every propagation step. \n\t\t\t\n\n——\nQuestions during rebuttal:\n\n\t- Kindly clarify concern (i)\n\t- Check experimental concerns above for additional ablation and baseline variants that is required to disentangle and appreciate the usefulness of the primary contribution, the attention diffusion component. \n\t- Comparison with the KB-GAT model that is based on GAT for KG completion task, will strengthen the results on KG completion task.\n\t- Comparison with GAT and Diffusion-GCN with LayerNorm and FeedFwd components on multiple train/test/val splits for smaller datasets or for other datasets from OGB will strengthen the results for node classification. \n\n--- Post-rebuttal\nI thank the authors for responding to all the questions and getting back with additional experiment results.\n\nMajor concern: While I understand the motivation and how having attention scores over nodes multiple hops away can be powerful, I'm still not convinced with the approximate realization. It is not clear how diffusing attention defined over 1-hop neighbors is powerful over attention methods defined over immediate neighbors that contain k-hop information aggregated from diffusion.\n\nAlso, the performance drop and overfitting issue with GAT or diffusion-GCN can be combated similarly by sharing weights across GNN layers and also using a higher-order diffusion matrix at each GNN layer.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Novelty in the Spectral Analysis",
            "review": "Summary:\n\nThis paper mainly proposes to learn attention-based edge coefficients by incorporating information from farther away nodes by means of their shortest path (powers of the adjacency matrix). Furthermore, the authors show the spectral properties of the proposed algorithm and its equivalence to personalized page rank.\n\nStrong points:\n\nThe numerical experiments show slight improvement.\n\nThe spectral analysis is potentially interesting.\n\nWeak points:\n\nThe multi-hop attention network has been done before (see below). The novelty thus resides only in the spectral analysis and the page rank equivalence.\n\nRecommendation:\n\nThe paper is ok, but I do not consider it to have enough novelty to be considered for publication in ICLR.\n\nMajor comment:\n\n1) Assuming there is only a scalar assigned to each edge (i.e. \\mathcal{R} = \\mathbb{R}), then (1),(3),(4) are a particular case of (39)-(41) in Isufi et al, 2020, where A_{k} = alpha I for k<K and A_{K} = H (the H matrix in eq. 4). This renders (1), (3), (4)'s only novelty to be the potential for vector-valued edge weights. Please elaborate on the comparison with Isufi et al, 2020 in the paper.\n\nE. Isufi, F. Gama, and A. Ribeiro, \"EdgeNets: Edge Varying Graph Neural Networks,\" arXiv:2001.07620v2 [cs.LG], 12 March 2020. [Online]. Available: http://arxiv.org/abs/2001.07620\n\n2) Why is R in N_r x d_r? Shouldn't it have N_e as a dimension? So if we have 5 edge types (like in the QM9 molecule data set, with 4 bond types and an extra no-bond type, encoded as one hot vectors), that means that R has only 5 rows? And what does the number of columns represent? It's just that X is very straightforward: number of nodes x number of channels. But R is hard to interpret.\n\n3) The LHS of eq. (2) shows dependence with i, j and l. The RHS shows dependence with i, j, l, and also k. Where does k appear in the matrix indexing? Is there a different attention score matrix for each value of k?\n\n4) I believe the paper would benefit for an increased elaboration on the usefulness of proposition 2, since this is the main novelty. Let us say we are given a graph, and we choose to describe that graph by means of some matrix (either the adjacency or the Laplacian). The choice of graph description fixes the frequency interpretation of the graph. Different matrix choices lead to different frequency interpretations. Now, the attention mechanism changes this matrix description by learning a new matrix description. However, this new, learned, matrix description will most likely not even share the same eigenvectors as the original matrix description of the graph. So how do we know that the filters learned are actually low-pass filters in the graph? They maybe low-pass filters in the learned graph by attention mechanisms, but they may not be in the actual graph that was given by the problem. Please, clarify what's the interpretation of Proposition 2, since matrices A and \\mathcal{A} are being learned from data.\n\nMinor comments:\n\nDue to Cayley-Hamilton theorem, there is no need for (3) to go to infinity. It suffices for it to go to N_n-1.\n\nReference to Sandryhaila and Moura, 2013 is missing at the beginning of Section 3 when referring to discrete signal processing on graphs.\n\nA. Sandryhaila and J. M. F. Moura, \"Discrete signal processing on graphs,\" IEEE Trans. Signal Process., vol. 61, no. 7, pp. 1644–1656, 1 Apr. 2013.\n\nFootnote 2 on page 5 is missing a proposition reference.\n\n--- UPDATED SCORE ---\n\nFirst of all, I would like to thank the authors for carefully addressing my comments.\n\nIn light of the authors' response, and after a thorough and careful discussion with the other reviewers, I have decided to update my score to 5 (five).\n\nIn summary, I appreciate the authors' effort to signal the differences between their work and Elvin et al. While I agree with these, I still think this is only an incremental contribution. For further reference, after carefully discussing the paper with other reviewers, these two published papers were also pointed out where multi-hop attention is addressed. Namely,\nhttps://openreview.net/forum?id=rkKvBAiiz\nhttps://ieeexplore.ieee.org/document/8683050\nI apologize for not finding these papers in my first round of reviews, but it does not really alter my evaluation of the paper.\n\nI think that the most novel contribution is on the spectral analysis. But this is only stated, with no real insights developed, and not emphasized enough. More insight on this would definitely bring a novelty. More specifically, novelties that would have made the paper more interesting: (i) a different way of computing the attention coefficients, that would be more parameter efficient (as opposed to eq. (1)) in the paper, (ii) actual useful insights into what the frequency response of the learned filters look like in the attention matrix as opposed to the given support matrix of the graph, (iii) a comparison between the spectral basis of the learned attention matrix as compared to the support matrix.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #4",
            "review": "The authors propose a novel attention-based GNN called MAGNA. The main contribution consists in considerably increasing the receptive field by considering a multi-hop neighborhood instead of the standard one hop. The technical challenge consists in obtaining attention scores for all relevant nodes in an efficient way. MAGNA solves this by using a diffusion-based technique combined with a geometric distribution. The authors show that the latter further allows for approximations, and also give interesting theoretical insights (e.g., show a relation to page rank). \n\nThe paper is overall well written, related work is considered adequately, the idea is interesting, and the results are convincing. The evaluation comprises several different datasets, domains, tasks, and competitive baselines. The ablation studies give interesting insights in the effects of different parameter choices. Altogether, I suggest to accept the paper.\n\n----------------------------------------------\nSmaller comments:\n- p.3: \"degenerate categorical distribution with 1 category\": I think this should be explained.\n- p.3: \"effectively creating attention shortcuts between nodes that are not connected (Figure 1).\": I assume you mean \"directly connected\" since the whole approach seems to consider only connected nodes?\n- p.4: \"as well as good model generalization.\": How does the diffusion process ensure good model generalization?\n- p.5: Footnote 2: ??\n- 4.2. Baselines: The paragraph mentions few what is not in the table, so maybe you can just drop it and use the space for more descriptions.\n- 4.2. Results.: The last sentence is unclear to me.\n\n----------------------------------------------\nUpdate after Rebuttal: I have read the other reviews and authors' responses. \n\nWhile I do think the novelty of the contribution is sufficient, given that  the paper referenced in another review has not been peer-reviewed yet, the new ablation results in Table 1 show that the paper's contribution is not outstanding. I adjusted my score.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting combination of graph attention and graph diffusion",
            "review": "==== Summary ====\nThis paper proposes MAGNA, a multi-hop self-attention mechanism for attention based graph neural networks. The proposed method increases the receptive field at each layer, requiring less layers to achieve a large receptive field. Also, with the proposed method the attention coefficient between two nodes is not just a function of the two nodes but also of their neighbourhood. The proposed MAGNA method is an extension of GAT networks that introduces a diffusion step on the computed attention coefficients, following a similar approach (Diffusion-GCNs) that has been used for GCNs.\n\n==== Pros: ====\n* The proposed method extends GAT layers to have multi-hop receptive fields without increasing the number of model parameters.\n* Even though the two main building blocks (Graph Attention and Graph Diffusion) of the paper are not novel, their combination is novel and it achieves state of the art empirical results in two different tasks.\n* The evaluation of MAGNA and comparison with previous approaches is well done, using two different tasks and the standard benchmarks for those tasks.\n* The paper is well written.\n\n\n#### Cons:\n* The proposed method MAGNA seems to be similar to the APPNP method proposed in [Klicpera 2019a] that also uses diffusion to increase the neighbourhood around each node in a GCN layer (without attention). However the two models are not compared and the later is not included in the Related Work section, even though it is cited previously.\n* The ablation study is a good idea, but it isn’t very clear how it is done.\n* The motivation behind including layer normalization and deep aggregation or why they are useful isn’t entirely clear.\n\n#### Questions:\n1) MAGNA has 3 main differences compared to GAT: layer normalization, diffusion and deep aggregation. In the ablation study in Table 1, which components are removed for each row? When it says “No LayerNorm”, does that mean that it uses diffusion and deep aggregation but no LayerNorm? If so, is there an ablation test where MAGNA just uses the diffusion step but without the other two components? That would be very relevant, since it would be like a GAT network with a larger receptive field at each layer, and would show how much improvement the increased receptive field brings without the other components.\n2) Since the paper puts a lot of emphasis in the multi-hop capability of MAGNA, and related to my previous question, it would be quite interesting to compare MAGNA against a GAT network that has a larger receptive field, for example by using a multi-hop adjacency matrix computed with the diffusion process from [Klicpera 2019b] (called sparsified matrix in that paper) instead of the 1-hop adjacency matrix used in the original GAT paper. With this comparison we could see the benefit of using the diffusion of the attention values instead of just increasing the receptive field of GAT by allowing each node to pay attention to nodes up to K hopes away.\n3) In section 4.1 the authors say that “Since MAGNA computes many attention values, layer normalization is crucial in ensuring training stability”. Doesn’t MAGNA compute the same number of attention values as GAT and it then diffuses them? Any ideas why layer normalization is crucial when diffusion is used but not without it? (as seen in the ablation study, when MAGNA doesn’t use diffusion the scores are on par with GAT, which suggests that layer normalization is only useful with diffusion).\n\n#### Minor comments\nIn the Edge Attention Computation paragraph in section 2.2, the authors say “To compute representation of” and probably meant “To compute the representation of”.\n\n$W_o$ in Equation 6 is not explained in the text.\n\nAcross the paper and in the caption for Figure 2, the text talks about MAGNA blocks, but the text in the Figure says “DAGN” Block. Is that the same? Same in the 2nd plot in Figure 3, it says DAGN, should it be MAGNA?\n\n#### Reasons for score\nThe authors give a solid justification for using a diffusion process to increase the receptive field of GNNs with attention, and along with previous papers on graph diffusion provide a good motivation and explanation of their method. Also, the experiments section shows that MAGNA achieves state of the art results in two different tasks. Because of these reasons, I vote for accept. My only concern is that some components don’t have a clear justification besides the empirical improvement in performance (layer normalization and deep aggregation) and that comparisons with other techniques (APPNP and GAT with k-hop adjacency matrix) would make the experiments section stronger.\n\n----- Post Rebuttal Update -----\nAfter the author's rebuttal and the discussion with other reviewers, I have decided to lower my score to 6.\nThe reason is that during the review and discussion process it was pointed out that the main contribution of the paper is more incremental than novel, as both multi-hop GATs and diffusion for GNNs have been explored before in similar ways. Additionally, even though the authors provide some theoretical grounding for their work, some of it is a bit disconnected from the rest of the paper, like the relation with PageRank introduced in section 3.2, which is not referenced or discussed in any other section.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}