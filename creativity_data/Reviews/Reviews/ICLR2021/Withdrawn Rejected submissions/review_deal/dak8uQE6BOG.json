{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new network architecture that implements higher order multivariate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables, and can be applied to a broad range of tasks.   However, unifying discrete and continuous conditions and network without activation function are both well studied in literatures. The inappropriate discussions on the prior works and the advantages of the proposed method over prior approaches are not clearly justified.  Although outperforming SOTA is not necessary, the compared methods need to be well chosen which can provide convincing evidence on why MVP is needed under common settings."
    },
    "Reviews": [
        {
            "title": "Inappropriate experiments",
            "review": "Summary: This paper proposes a framework for generating conditional data using multivariate polynomials, which treats both discrete and continuous conditional variables in a unified way. From my perspective,  neither the effect nor the actual use is very clear.\n\nBelow are my major concerns:\n\n1. The author compares the proposed method with multiple methods on the class-conditional generation and image2image translation tasks, but none of the comparison methods are state-of-the-art. The choice of dataset and task is not very appropriate. \nWhy compare with SPADE on the class conditional generation and super-resolution tasks? SPADE is the SOTA of  **semantic image synthesis** in 2019. It is unfair to compare specific methods for inappropriate tasks.\nIn addition, the selected dataset is also weird. Why not use a standard super-resolution dataset or use downsampled FFHQ and CelebAHQ images for face super-resolution tasks. \n2. The author mentioned that the disadvantage of the traditional encoder/decoder architecture (pix2pix Isola et al., 2017) is that it ignores noise,  but in fact, this problem was solved to a certain extent as early as 2017 e.g. (BicycleGAN zhu al., 2017). In fact, in Figure 4, I did not see enough of the variability caused by the noise claimed in this paper, even though the task is so simple.\n3. Although the method proposed in this paper is novel, I am not sure it really makes sense in actual complex input scenarios. Besides, the results mentioned in this paper do not show any superiority over traditional architectural design.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Clear and polished but more evidence is needed to justify the importance of MVP",
            "review": "This paper proposes a conditional generation framework (cGAN) that bridges the gap between discrete and continuous variable used in the generation. They do so by proposing a new network architecture that implements higher order multi variate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables and has good expressivity even in the absence of activation functions.\t\n\nPros:\nThe figures are succinct and informative, giving a clear picture of the points the authors want to illustrate. I appreciate the multiple evaluations with mean and standard deviation reported. \n\nThe methods section is well-written and the intuition provided to readers who are not familiar with the concept of polynomial networks is very useful.\n\nCons/Comments:\nSection 2.1 Discrete conditional variable: the authors claim that conditional normalization might be an obstacle towards generalizing to unseen classes. They do not show their method is able to generalize. \n\nSection 2.1 Continuous conditional variable: The authors do not cite or mentioned papers that do multi modal image generation [1,2,3] that display diversity and do not require pixel-wise loss such as l1 or perceptual. They single out one-to-one translation models that are not designed for such tasks. \n\nTable 2: Why is there a huge disparity between IS and FID scores of SNGAN-CONC compared to SNGAN? The IS is slightly better but the FID is significantly worse. Furthermore, the FID of the original SNGAN paper [4] is 21.7 and the FID of BigGAN [5] is 14.73. What causes the disparity the results between the papers? In my experience, BigGAN performs much better than SNGAN yet the FID of SNGAN quoted in this paper is 14.7, virtually the same as BigGAN.\n\nSection 4.1 Resnet-based generator: The authors claim that inter-class interpolations have been done for other datasets but not CIFAR. Is BigGAN capable of doing inter-class interpolations for CIFAR since it is able to do for ImageNet?\n\nThe experiments include conditional generation based on discrete variables and continuous variables. One of the main comparisons is with SPADE. However, SPADE was designed for image generation from semantic maps, a task not tested in this paper. Super resolution was chosen as the one of the two tasks for continuous variable. This is a task not traditionally done with multi-modal output in mind. In my opinion, this is not the most useful task to base the experiments on. A more interesting task could be image-to-image translation or semantic image synthesis.\n\nThe experiments are all done at 64x64 resolution and the datasets are relatively simple compared to those used in SOTA models (512x512 ImageNet on BigGAN and 1024x1024 FFHQ on StyleGAN [6]). The quantitative improvements on the most commonly used ResNet architecture is minimal. While MVP remains expressive without activations and improves significantly using a Î  net architecture, those to my knowledge are not settings widely used now in literature.  \n\n\nIn general, this paper provides a clear unified framework for conditional image generation. The method is well explained and illustrated. However, their improvements on a commonly used architecture (ResNet) is minimal. It is my opinion that more justification and evidence is needed on why MVP is needed under common settings.\n\n[1] Choi, Yunjey, et al. \"Stargan v2: Diverse image synthesis for multiple domains.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[2] Huang, Xun, et al. \"Multimodal unsupervised image-to-image translation.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n[3] Lee, Hsin-Ying, et al. \"Drit++: Diverse image-to-image translation via disentangled representations.\" International Journal of Computer Vision (2020): 1-16.\n\n[4] Miyato, Takeru, et al. \"Spectral normalization for generative adversarial networks.\" arXiv preprint arXiv:1802.05957 (2018).\n\n[5] Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" arXiv preprint arXiv:1809.11096 (2018).\n\n[6] Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "**Post rebuttal (round #3)**\n\nThanks to the authors' effort on the rebuttal. Despite the extensive efforts, I feel the review/rebuttal iteration is not satisfactory, possibly due to some miscommunication.\n\nTo be clear, I want to re-emphasize that significant parts of my concerns were about **misleading claims** on prior work, and comparison to them was the next step.\n- For example, I just wanted to clarify that the claim \"type A ignores the noise and cannot learn the stochastic mapping\" is wrong. The paper could simply fix the claim instead of including a massive related work section. Maybe my review also has some responsibility: I could simply say **fix** the wrong claims, instead of indirectly delivering by pointing them out.\n\nAlso, as I explicitly mentioned the concerns A,C,D,F, the rebuttal could address them point-by-point.\n- In particular, I'm not convinced that cBN/sBN is **not applicable** to continuous conditions, as sBN predicts the BN parameters from the continuous latent variable.\n\nDespite the remaining concerns, I raised the score (from 5) to 6 as the architectures with multiplicative interactions are an important and timely topic. However, I think the paper is on the borderline, and the rebuttal and revised paper could be much stronger.\n\n------\n\n**Summary**\n\nThis paper extends $\\Pi$-Net, deep polynomial neural networks, to a multivariate setting. The proposed network, MVP, is applied to various conditional GANs, including discrete, continuous, and mixed condition scenarios.\n\n\n**Pros**\n\n- Extending $\\Pi$-Net to multivariate setting is a natural and necessary research direction.\n- Good experimental results on class-conditional, image-conditional, and mixed (class + image)-conditional scenarios.\n\n\n**Concerns/questions**\n\nI. Novelty over $\\Pi$-Net is not significant\n\n- $\\Pi$-Net has shown that deep polynomials can be useful for an unconditional generation. Extending it to conditional generation (using two variables) is quite straightforward. While the paper compares with other design choices (e.g., SICONC and SPADE), it is natural that MVP performs the best since it has the strongest expressive power.\n- While the paper claims that \"unifying discrete and continuous conditions\" is a key property of MVP, standard conditional GANs can also handle those cases and are already discussed in the literature. For example, [1] considers image (face) + class (quantized age) conditions.\n- While the paper claims that \"network without activation function\" is one of the main contributions, it was originally investigated and heavily discussed in $\\Pi$-Net.\n\n[1] Antipov et al. Face Aging With Conditional Generative Adversarial Networks. ICIP 2017.\n\n\nII. Wrong claims on the (drawbacks of) prior work\n\nThe paper claims that prior conditional GANs: (Type A) \"encoder network to obtain representations that are independent of the conditional variable\" and (Type B) \"directly concatenate the labels in the latent space\" have several drawbacks. However, the claims are incorrect, as stated below:\n- Type A does not ignore the noise and can learn the stochastic mapping (with proper training) [2,3]\n- Type A can be successful for discrete conditions [1]\n- Type B can scale beyond 10 class, especially with nonlinear mapping such as conditional BN [4,5]\n\nThe paper also claims that \"inter-class interpolations in CIFAR10 have not emerged\", but there is no backup for the claim. To verify this, the paper should compare the generated samples of standard GANs and MVP in Figure 2.\n\n[2] Zhu et al. Toward Multimodal Image-to-Image Translation. NeurIPS 2017.\\\n[3] Huang et al. Multimodal Unsupervised Image-to-Image Translation. ECCV 2018.\\\n[4] Miyato & Koyama. cGANs with Projection Discriminator. ICLR 2018.\\\n[5] Brock et al. Large Scale GAN Training for High Fidelity Natural Image Synthesis. ICLR 2019.\n\nIII. Why polynomial should work better than standard GANs?\n\nThe paper claims that MVP performs better than standard GANs in various conditional generation setups.\n- Is there any intuition that MVP should work better than standard GANs?\n- How about the number of parameters or sampling speed? Is the comparison (in Table 2-4) fair in terms of complexity?\n\n**Rating**\n\nDue to the concerns above, I currently recommend a rating of 5.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}