{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Though the observation regarding the importance of the low end of the spectrum is interesting in its own right, the paper would be better substantiated by experiments on more datasets and a more thorough characterization of the paper novelty/contrast to state of the art."
    },
    "Reviews": [
        {
            "title": "Novelty is limited",
            "review": "Brief summary\n\nThis paper studies the contribution of low-frequency eigenvalues and high-frequency eigenvalues of graph Laplacian empirically. The authors show that high-frequency eigenvalues are less informative. In their work, they propose an MLP model that can achieve equal or better performance than GCN by using eigenvectors as part of input features.\n\nPros:\n\n1. MLP fed with node features and spectral coordinates of a given node is competitive with a GCN and more efficient than GCN.\n2. The empirical study of high-frequency and low-frequency eigenvalues provides great insights on whether we should build high-pass filters for graph signals.\n3. The authors show that high-pass filters tuned properly can lead to some minor performance boosts on GCNs.\n\nCons:\n\n1. Although it can be done in the pre-processing step, the computation cost of eigenvalue decomposition is heavy. Besides, as the proposed method assumes the corresponding graph topology is fixed, it can not be used in an inductive setting.\n2. The idea of feeding node features and spectral coordinates to MLP is not novel. It is a kind of feature engineering.\n3. The impact of high-frequency eigenvalues on graph-level tasks is not studied. Graph-level tasks are equally important as node-level tasks for a powerful GCN.\n4. The authors said larger eta emphasizes higher frequencies, and when eta=2 or eta=3 it marginally boosts the performance of GCN on CiteSeer and Pubmed. Larger eta not only emphasizes higher frequencies but also lower frequencies because when eta goes to infinity, it leads to an all-pass filter which includes both high frequencies and low frequencies.\nOriginality\n\nThe paper provides new insights about low-pass filters and high-pass filters on graphs. Since the paper essentially proposed to use new features for an mlp model. The novelty of this paper is limited.\n\nMinor comments:\n\n1. About Equation 7 and Equation 8, the illustration is not clear to me. What is the range of eta? going from 0 to infinity? or going from negative infinity to 1? It seems quite ambiguous. In addition, if the range of eta varies from 0 to infinity, I think Equation 8 balances a trade-off between a smoothing low-pass filter and an all-pass filter. It is better to plot the spectrum of Equation 7 to verify you argument.\n2. How is Equation 9 derived?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #4",
            "review": "This paper aims to study how GCN will behave under spectral perturbations/manipulations. The empirical numerical analysis on three benchmark datasets (cora, citeseer, pubmed) show that most of the necessary information is contained in the low-frequency domain. Based on that, the author propose to expand the node feature matrix with the eigenvectors corresponding to low-frequency domain and apply MLP on this new feature matrix. Experimental results show that the proposed method outperforms vanilla GCN and achieve comparable results on pubmed with other baselines.\n\nI think the paper has a pretty good start point: understanding how the spectrum of adjacency matrix will affect the behavior of GCN. But I think the manuscript is loosely written and I can hardly follow it. I do have a lot of confusion about this paper and hope that the authors can clarify them.\n\n- In general, there are too many typos and grammar errors which make the manuscript hard to read.\n- Sec 1: in the 1st paragraph, what does 'graph principles' mean? I cannot recall a clear definition of this term. I think the authors could clarify it before using it formally.\n- Other contributions in Sec 1:\n  * (a), I think the empirical results in the paper show that retaining a small portion of low-frequencies is enough for achieving good classification accuracy. From the results and your analysis, I can hardly find a clear conclusion that links to it (i.e., the very first eigenvector is most informative); \n  * (b) and (e) are somehow connected since both are mentioning about GCN's behavior over manipulating high frequencies; \n  * (c), I actually did not quite understand where the clear link is. I would suggest the authors clarify it more clearly in the manuscript;\n  * Just a minor thing, what exactly does 'informative' mean, greatest change in loss function or greatest change in test accuracy? Better make it clear.\n- Sec 3:\n  * Notations are quite messy. A matrix can be denoted using italic lowercase letter, italic uppercase letter, calligraphic uppercase letter, bold italic uppercase letter. It is very confusing when reading the paper, especially when some scalars are also denoted using the same convention.\n  * I did not find any framework in this section, which contradicts the last paragraph in Sec 1.\n- Sec 4:\n  * Pubmed should have only 1 connected components. In the following sentence, what does 'not necessarily fully supported on ...' mean? \n  * In Sec 4.1, is there any intuition or theoretical justification for the projection operation used in band-pass filter?\n  * In Figure 2, it seems that Cora has a very sharp performance drop (~0.82 -> ~0.5) when x-axis is nonzero, is there any insightful explanation on that?\n  * Does the content of 'MLP ablation' correspond to your proposed method? This is the only place I can find clues about your method. I understand that the goal of this paper is show a MLP can perform better or comparable with GCN that perform message passing. But I believe you still need topology information to get those eigenvectors. Is there any insight or theoretical concern about simple concatenation instead of linear transformations like $e^T e x$?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty  is limited",
            "review": "Summary:\n The article analyzes GCNs from spectral viewpoint, and discusses the performance of GCNs with respect to spectral filtering. The paper shows by experimentation, that the performance of GCNs mainly depend on low frequencies (lower end of the spectrum/eigen-pairs). It then shows that an MLP with low frequency information (Eigen-pairs) performs very well in graph tasks. Aspects such as smoothness and high frequency ablations are also studied.\n\n--------------------\nStrengths:\n1. Paper presents a study of GCNs from spectral perspective.\n2. Paper makes few interesting observations about the influence of low and high frequency components of the Graph Laplacian on GCN performance.\n3. MLP with spectral information is investigated for graph tasks. \n--------------------\nWeakness:\n1. Novelty in the study is limited.\n2. Experimental results are inadequate.\n3. Presentation is poor.\n--------------------\nDetails: \nI have the following comments about the paper:\n\n1.  Novelty is limited: The novelty of this study is unclear to me. The fact that the GCN model of (Kipf and Welling, 2016) is based on low pass filtering is well-known. Note that, their model is simply a degree-one approximation of the low pass Chebyshev filter approach of (Defferrard et al., 2016). The original objective of spectral GCNs have been low pass (localized) filtering as proposed by (Bruna et al., 2013). The study in (Li et al., 2018) also further establishes these spectral results. Hence, the novelty of the paper is limited. \n\n2. The experiment results are inadequate. The paper considers just 3 datasets (all three have similar spectrum), and it is hard to make conclusions with the few results presented. The results in Figure 2, do not actually support the conclusions. The effects of retaining more LF components is not clear, especially for deeper networks.\nResults on more datasets with different spectral distributions would make the study more conclusive.\n\n3. The paper is poorly written, and needs to be checked for language consistency throughout. I recommend having the paper proof-read by a native speaker.\nNotation is inconsistent. Both upper and lower case letters are used for matrices.\n\n4. Cost: Note that GCN is a very simple model that is computationally inexpensive. It requires just few matvecs (# of features) at each layer to implement.\nThe proposed MLP requires eigenvectors, i..e, requires a complete Eigen-decomposition of the Laplacian, that has a cubic K^3 cost. Computational complexity of the compared methods should be discussed.\n\nMinor Comment:\ni. Page 2, \"one of the first paper\" --> one of the first papers\n\"litterature\", \"overs-moothing\" and many more incorrect spellings throughout",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice analysis of GCN models",
            "review": "Summary:\nThe work presents an interesting analysis of GCN models under spectral manipulations and relates the performance of GCNs through bandpass filtering. The authors demonstrate that GCNs mainly rely more on low-frequencies rather than high-frequencies which is contrary to what is observed in signal processing. For this, the authors use band-pass filters which allow only a portion of the spectrum to be utilized by the GCN model. The major findings are as follows:\n1. The high-frequency eigenvalues are less informative and perturbing them does not have much effect on GCN’s output. This supports the over-smoothing phenomena empirically observed by Li et al. 2018\n2. A simple MLP with few initial eigenvectors as additional features outperforms several existing GCN models. Moreover, including higher eigenvectors degrades performance as it adds noise which could not be well filtered by MLP. \n\n\nQuestions:\n1. The reason why GCN model with 8 hidden layers gives poor performance is not properly justified. It would be great if authors could provide more intuition behind it. Can over-parameterization can be a factor for such poor generalization?\n2. Can the authors provide some explanation behind the sudden jerks observed in Figures 2 and 4? \n\nTypos:\n1. In the related work section (para 2 and 3) change ‘overs-moothing’ to ‘over-smoothing’ and ‘litterature’ to ‘literature’. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}