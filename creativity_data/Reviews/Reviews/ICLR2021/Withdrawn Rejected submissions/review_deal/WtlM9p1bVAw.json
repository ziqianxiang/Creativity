{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a continual learning method based on a novelty detection technique. All reviewers are concerned about various issues, especially, motivation, experiment, and presentation. One of the reviewers was initially positive about this paper but downgraded his/her score due to unresolved problems in the proposed method. Considering all the comments and communications with the authors, AC believes that this paper is not ready for publication yet."
    },
    "Reviews": [
        {
            "title": "interesting idea to tackle the unsupervised class-incremental learning but needs more experiments",
            "review": "This paper proposes to tackle the problem of unsupervised class-incremental learning, where the training data is composed of a sequence of \"exposures\". Each exposure is comprised of a set of images that pertains to a single class, where the class label is unknown while the boundaries between exposures are known. The key difficulty in such unsupervised class-incremental learning is to determine whether an arriving exposure belongs to what the classification model $L$ has learnt previously or is a novel one, thus relating to the problem of novelty detection. The proposed method address the novelty detection by an interesting idea: they always treat the current exposure as a novel class and use it to train the copy of classification model $\\hat{L}$ together with the training exemplars of previously-learnt classes, if the current exposure actually belongs to one of the previous-learnt classes, the confusion occurs to make the classification accuracy significantly decrease (over a threshold) on that specific class, where the accuracy is computed based on the validation exemplars. Moreover, a technique of introducing class-imbalance into such confusion-based novelty detection is proposed and helps to boost the robustness of novelty detection. \nThere are some pros and cons of this paper as listed below.\n\nPros:\n+ The idea of using confusion to address the novelty detection is novel and interesting, where the corresponding threshold is easier to be determined and contributes to better out-of-distribution performance in comparison to other related works of using static distance-based threshold. \n+ The introduction of class-imbalance works well with the confusion-based novelty detection and its contribution is experimentally verified on various datasets.\n+ The overall performance of the proposed method on unsupervised incremental learning is better than an unsupervised baseline (IOLfCV) and comparable to a supervised one (BiC).\n\nCons:\n- The figure.2 is a little bit difficult for understanding the properties of seen and unseen classes with respect to class-imbalance ratio $\\lambda$ at the first sight, e.g. why the curve of unseen classes would go up along with larger $\\lambda$? Perhaps it is better to replace the terminology of \"seen\" and \"unseen\" classes by \"repeated\" and \"non-repeated\" classes?\n- There is another closely-related type of incremental learning: unsupervised continual learning. Although its setting is more difficult than the unsupervised class-incremental learning which is tackled in this paper, it would still be nice to have the baselines of unsupervised continual learning for providing more insights to the readers. \n- As the mostly-related work of this paper is Stojanov et al., CVPR 2019 (also addressing the unsupervised class-incremental learning problem), why the CRIB dataset proposed by Stojanov et al. is not used for evaluation here in order to have more direct comparison?\n- Moreover, as indicated by Stojanov et al., the repetition of classes (e.g. how frequent a learnt class arrives again for learning) plays an important role for the model performance, there should be clear description on the experimental setting of repetition as well as the investigation on it in this paper. \n- Furthermore, in the paper of Stojanov et al., they experiment with the classification models of having pre-trained feature extraction or being learnt from scratch. However, in this paper only the classification model pretrained on ImageNet is adopted. There should be experimental results and corresponding discussion on having the classification model trained from scratch for better understanding how the proposed confusion-based novelty detection behaves. \n- Lastly, it is also important to investigate on the forgetting effect. When updating the classification with predicted label, are the techniques for avoiding catastrophic forgetting used (e.g. knowledge distillation)? If not, how the proposed method prevents the catastrophic forgetting from happening? If the forgetting does happen, will the confusion-based novelty detection still be working?\n\nIn brief, this paper proposes interesting idea of having confusion-based novelty detection approach to tackle the unsupervised class-incremental learning, but it needs more experiments and discussions to make the paper more complete and ready for ICLR. I would expect to see the concerns listed above being well addressed in the rebuttal.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty detection via re-training",
            "review": "The paper studies an unsupervised class-incremental learning setting where a single class appears in each exposure, the classes can repeat and remain unknown during episodic training.  A set of exemplars is used to evaluate accuracy changes, based on which novelty is determined. The ideas is novel, but I is less scalable and the approach currently lacks key analysis and comparisons with the incremental learning methods and open-set approaches. \n\nPros`:\n+ An novelty detection approach that considers the changes in accuracy of the previous tasks as a new task is learned by assigning a new label to the incoming episode. A threshold value is then used to detect novelty.\n\nCons:\n\n- The basic intuition is that if a previous class is observed again, and the performance on old similar class will go down significantly. I feel this assumption is weak and can only be relevant in specialized cases, e.g., what if a very similar confusing class is observed? The currently evaluated datasets (SVHN, MNIST, CIFAR) do not consider such fine-grained cases. \n- The evaluations in comparison to SOTA incremental learning methods is insufficient. Only a single approach, BiC, is considered for comparisons. \n- The class imbalance based approach looks like a practical hack and is sensitive to the hyperparameters. \n- The propose approach will incur a high computational cost with training the model at each episode to detect novelty. The computational cost comparison is not performed in the experimental section. \n- The open set literature solves the same problem of OOD detection, however no comparison with SOTA methods is shown. I would recommend authors to check a nice survey on this topic: \"Recent Advances in Open Set Recognition: A Survey\"\n- The paper is not well-written. Fig. 2 comes before class-imbalance discussion.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A paper that I found difficult to read.",
            "review": "This article proposes a method for predicting whether a batch of data is of the same class as one of the classes already seen by a classifier or whether it contains data from another class. The idea is to then be able to incorporate this batch to the previous training set, in an unsupervised learning context. It is assumed that each batch contains data from only one class. Experiments are there to show the interest of this method for anomaly detection or incremental learning.\n\nI find it difficult to formulate an opinion on this paper because I don't think I have managed to understand the detail of what is actually done. For example with regard to the detection of out of distribution data, the classic problem is whether a data is out of a distribution. Here it is not a data but a batch of data that is considered. I don't really see, under these conditions, how to compare to classical OOD methods. \n\nAs far as incremental classification is concerned, I don't understand the definition of the metric given in section 4.3 and therefore I'm not sure I understand what the task is really about. The fact that it's unsupervised makes it away from standard problems.\n\nIt seems to me that the paper lacks a clear definition of the tasks addressed and the means to evaluate performance. ",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Lacking proper motivation and missing fair comparison",
            "review": "The authors propose a novelty detection module to help unsupervised class-incremental learning. The novelty detection relies on the percentage of accuracy drop during a model update when treating incoming data as a new class. If the model maintains high accuracy, then the module treats the incoming data as familiar, thereby choosing one of the existing classes as the correct label. The paper investigates the effectiveness of the proposed method on MNIST, SVHN, CIFAR-10, and CIFAR-100.\n\nThe main weakness of the submission might be that the proposed novelty detection is not well motivated. The bottleneck of the proposed pipeline comes down to whether the accuracy drop on the selected subset is a good indicator of out-of-distribution (OOD) detection. The submission does not provide theoretical insights nor direct references that show it is actually the case. In fact, in the experimental section (Sec 4.1), the authors have to use several different accuracy threshold settings (0.46, 0.63, 0.57, 0.62) for different datasets, demonstrating that accuracy drop might not be a reliable indicator.\n\nBesides, the direct competing method CURL (Rao et al.) is cited but not compared. iCarl [R1] should be quite related as well. The authors also use quite a different backbone network (ResNet-18) than other competing methods. Therefore, it is hard to justify whether the proposed approach is more effective than other baselines. \n\nThe method described here is also quite similar to the field of active learning. It would be great to discuss the relationship between the proposed novelty detection and other active learning literatures.\n\nSec 1 first sentence “continually learning systems remains to be a major obstacle in the field of artificial intelligence” is quite a strong statement. I believe there are other major obstacles in AI and they should be discussed as well.\n\nSec 1 paragraph 3, “an agent must conduct two procedures successfully”. The authors do not clearly define what is an “agent” in the context. It is hard for the readers to follow through the manuscript.\n\nSec 3.4, “After obtaining the correct label”. Actually the label technically is not “correct” but assumed to be correct for the class-incremental learning.\n\n[R1] Rebuffi et al. Icarl: Incremental classifier and representation learning. In CVPR 2017.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}