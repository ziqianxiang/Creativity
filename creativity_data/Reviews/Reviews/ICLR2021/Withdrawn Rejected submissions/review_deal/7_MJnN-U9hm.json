{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting empirical finding",
            "review": "Summary: This paper first shows 1) the advantages of forming input-output paths over pruning individual connections to avoid bottlenecks in gradient propagation, and 2) Paths with Higher Edge-Weights (PHEW) at initialization have higher\nloss gradient magnitude, resulting in more efficient training. They empirically validate the effectiveness\nof the proposed approach against pruning-before-training methods on CIFAR10, CIFAR100 and Tiny-ImageNet for VGG-Net and ResNet., and also evaluate the structural similarity relationship between PHEW networks and pruned networks constructed through Iterated Magnitude Pruning (IMP), concluding that the former belong in the family of winning tickets networks.\n\nPros: \n+ This paper presents an interesting way to construct \"winning tickets\" without the need for training data\n+ This paper provides a decent amount of empirical experiments to validate their hypothesis \n+ The writing is easy to follow\n\nCons:\n- The authors only provide results and observations based on relatively small dataset like CIFAR-10/100 and Tiny-ImageNet\n- There is a lack of discussion on the practical use of their technique\n\nQuestions to the authors:\n1. Could you provide information regarding the computational cost to find PHEW-based subnets? \n2. Could you comment on the practical use of your method? For example, compared to the early-bird ticket paper (https://arxiv.org/abs/1909.11957), where does PHEW stand in terms of the total training costs (including the cost to identify the subnets and retraining the subnets) and the achieved FLOPS-Accuracy tradeoffs? \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting and novel approach to pruning at initialization",
            "review": "## Summary\n\nThe authors propose a new method for pruning individual weights of deep neural networks (unstructured sparsity) at initialization, which leads to superior sparse networks in terms of test accuracy after training compared to existing approaches for pruning at initialization. There method is based on a biased random walk to construct paths through the network. At each step the random walk picks an edge in the next layer at random with probability proportional to the edge weight. Edges that are not included in the sampled paths are then removed before training the resulting sparse network. Notably, the method does not require any input data to construct the paths, yet it still results in networks that contain high-gradient paths. \n\n\n## Score\n\nI like the intuition behind the method and it seems to perform better than existing methods for pruning at initialization. The intuition is also backed up by some simple and easy-to-follow examples that show analytically why paths with higher Edge-Weight-Product (EWP) result in better prune performance. Experiments are mostly there and contain multiple repetitions. \n\nSome of things I am concerned about include the heavy usage of comparing PHEW networks to lottery tickets without providing sufficient evidence in my opinion. It is also not clear to me how this approach would generalize to other types of architectures. \n\nMore detailed feedback is below. \n\n\n## Ways to Improve My Score\n\nI would like to at least see the claims about PHEW networks being \"winning tickets\" clarified. If they are winning tickets in the original sense then I would like to see full lottery tickets experiments with iterative magnitude pruning (IMP) that show that the test accuracy of the resulting sparse networks is the same as with PHEW networks. If that's not the case, I would suggest to remove the repeated comparison to lottery tickets. The core method on its own has enough merit to be published. \nIn any case, I would remove the experiments in Section 6.2 or significantly tone down the writing. \n\nOther important feedback that I would like to see addressed are mentioned in the \"Weaknesses\" section. \n\n## Strengths\n\n* Excellent writing and well-structured paper. \n* Intuitive and novel method that is really interesting to read about with good intuition and analytical examples to motivate the method. \n* Experiments are carried out well and the results are presented in a coherent manner. Each experiment is repeated multiple times. \n\n## Weaknesses\n\n* As mentioned before, I would suggest to tone down the lottery ticket comparisons. I would assume that PHEW does not perform quite as well as the classic lottery tickets derived from IMP, which would not be surprising since IMP requires numerous training iterations. But in any case, I would like to see actual comparisons in terms of the resulting final test accuracy as main comparison tool instead of the comparison metric (Jaccard index, IoU) presented in Section 6.2. \n\n* The method seems to be limited to convolutional and fully-connected layers. Could the authors discuss potential generalization to a larger class of network layers? Since unstructured pruning methods (like PHEW) usually do *not* enable faster training or inference times, I find their main advantage to be that they are network-agnostic. This can thus enable the easy deployment of  unstructured pruning to any desired layer or architecture. \n\n* As a follow-up to the previous question. How are non-sequential network architectures handled? The experiments include ResNets, which contain residual connections, but to me it is not clear from the paper how this is achieved with PHEW. Also, what about recurrent architectures? Can PHEW handle those? As far as I am aware, the competing methods should not have a problem handling those architectures as well. \n\n* How about (at least a few) experiments with the full ImageNet data set? It shouldn't be too computationally expensive to train a ResNet on the full ImageNet data set especially since pruning at initialization does not require multiple retrain cycles like IMP.  \n \n\n## Other Minor Feedback\n\n* typo, page 4, below Fig. 4: \"...even more better...\"\n\n* I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs.\n\n* Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Yet Another Method for Pruning at Initialization; Overclaiming About Reasons for Purported Success and Nonsensical Claims about Finding Winning Tickets",
            "review": "# Summary of Paper\n\nThis paper is yet another proposed method for pruning neural networks at initialization. The proposed technique is PHEW, in which paths with the higher weights are probabilistically selected and removed from the network. The paper claims that this technique is successful because it leads to a higher gradient at initialization. The technique performs on par with many previous methods for pruning at initialization (SNIP, GraSP, and SynFlow), slightly outperforming these methods at the extreme sparsities that the authors specifically chose to highlight in the paper (Table 1) on very overparameterized networks.\n\n# Evaluation\n\n## Pruning Technique\n\nThe style of pruning itself is interesting and novel: pruning entire paths rather than individual weights. This has potential pros, but there is also one big drawback: it fixes the layerwise pruning proportions (since one weight is removed from each layer each time a path is pruned). The paper does not compare these layerwise pruning fractions to SNIP, GraSP, and SynFlow.\n\nThe pruning heuristic itself is not especially novel: SynFlow (Tanaka et al.) keeps those weights that have the highest total EWP (edge weight product - the product of the magnitudes of the weights on a path) summed over all paths through a particular weight.\n\nThe method of actually pruning weights with higher EWPs is probabilistic: greedily sample paths with higher weights from the beginning and end of the network; select weights with higher magnitudes with higher probability. The paper proves that this selects paths with higher EWPs than doing so uniformly at random, but it does not show that this method does particularly well at pruning weights with the highest EWPs.\n\n## Justification\n\nThe paper justifies this method in two ways:\n(A) The claim that pruning paths avoids creating \"stub neurons.\" The paper does not show that stub neurons are actually a problem for other pruning techniques that it compares against.\n(B) The claim that keeping paths with high EWPs increases the size of the gradient at the first step of training and that doing so is somehow good for the performance of the network.\n\nI will discuss (B) further, since the paper spends the most time on it: The paper supports this claim as follows:  (1) Section 3 includes a sketchy proof supporting this claim (see notes below) and (2) Section 4 includes a couple of small experiments that show some correlation between techniques that have a higher gradient magnitude at initialization and higher final accuracy of the network. Neither is convincing: these justifications do not rigorously substantiate the claim that \"paths with higher initial loss gradient form better sparse networks.\" To strengthen this claim, the authors will need to provide evidence that these two are causally connected and to attempt to falsify this claim (e.g., to try to find cases where a high initial gradient does not lead to good accuracy). I'm skeptical that the gradient magnitude at the first step of training says anything about the final accuracy of the network, and it will take a substantial amount of evidence to convince me otherwise.\n\n## Results\n\nThe paper only examines two incredibly overparameterized networks for CIFAR and TinyImageNet (VGG-19, which has 20M parameters vs. 270K for ResNet-20 for similar performance on CIFAR-10, and ResNet-34, which was designed for ImageNet - not CIFAR - and has 22M parameters). In the pruning literature, such overparameterized networks are a commonplace way to get flashy-looking numbers that aren't substantiated by reasonable-sized networks.\n\nThe actual performance of the method is not impressive. I am focused on sparsities where the methods can match the accuracy of the unpruned network; I think comparisons where the pruned network has lost 5%, 10%, or more in accuracy are not relevant since the network has been crippled to the point where it is not practically useful. (If the authors disagree with this claim, they can convince me by providing evidence of a practical use case for these heavily pruned and accuracy-crippled networks.) Focusing on these lower sparsities, in Figure 5, the method performs little different than other methods for pruning at initialization. In Table 1, the paper includes only three hand-picked sparsities (all on the extreme end). The table does have some bold numbers in the PHEW row, but I'm not convinced that these slight improvements of around half a percentage point over SNIP/GraSP/SynFlow are meanintful. The paper doesn't include a baseline of the aspirational accuracy that one might hope to reach (e.g., of magnitude pruning after training or of lottery ticket rewining (Frankle et al., \"Linear Mode Connectivity and the Lottery Ticket Hypothesis\")), so it's hard to tell whether these methods are all clustered near a \"bad\" accuracy far from this aspirational goal or whether half a percentage point is a big jump.\n\nIn general, it is typically possible to get a bold number for any pruning method by cherry-picking a sparsity, a network, and a dataset. This is a huge problem in the pruning literature: every paper claims to be SOTA by finding a place where it can get a bold number. To address the concern, the authors should include more networks and datasets (especially of the less overparameterized variety as mentioned above; ImageNet would be nice but I understand that - for academics - doing so may be out of reach) and show many sparsities across the spectrum (similar to Figure 5, but zoomed in enough that it's possible to make out distinctions between the methods if there are any).\n\nFor example, SynFlow clearly outperforms PHEW at extreme sparsities in Figure 5. Can I argue on this basis that PHEW is worse than SynFlow and so isn't valuable? If you are focused on wins on a particular network and dataset at a particular sparsity, you need to justify why these wins are meaningful and address why I shouldn't be concerned about places where PHEW does not perform well.\n\nThe paper is also missing two other baselines: uniform random pruning (which PHEW will likely outperform) and magnitude pruning at initialization (which Tanaka et al. show to be competitive with these methods). It would be disconcerting if PHEW did not substantially outperform these trivial baselines, and the authors should show that this is the case.\n\n## Claims of Finding Winning Tickets\n\nSection 6.2, which claims that PHEW finds winning tickets, is complete nonsense. Frankle & Carbin define a winning ticket as a subnetwork \"that is initialized such that - when trained in isolation - it can match the test accuracy of the original network after training for at most the same number of iterations.\" Emphasis: \"match the test accuracy of the original network.\" In Section 6.2, the paper falsely claims that this standard is that the subnetwork \"performs almost as well.\" In Table 1, none of the PHEW networks match the full accuracy of the VGG-19 and ResNet-34 baselines. They are not winning tickets by the definition of Frankle & Carbin, and that's not debatable.\n\nTo substantiate this claim, the authors should show that there exists a sparsity where PHEW subnetworks can reach the accuracy of the unpruned network; they should also show that random pruning can't do the same (otherwise, doing so would be trivial); ideally, they should show that PHEW can do this at sparsities where SNIP, GraSP, and SynFlow can't (I'm skeptical PHEW can do this, hence my concern about how the performance of PHEW isn't meaningfully higher than these other methods).\n\nAfter this incorrect definition of a winning ticket, the paper then claims that, to determine whether the PHEW networks are winning tickets, one should \"compar[e] the structural similarity of IMP networks [the method used by Frankle & Carbin to find winning tickets] with PHEW networks.\" The claim is that the subnetworks found by PHEW are more similar to those found by Frankle & Carbin's method than to those found by randomly pruning, meaning that \"PHEW networks belong in the class of winning ticket networks.\" This is nonsense. Nowhere in the definition of a winning ticket is structural similarity mentioned. Similarity of pruning masks says nothing about similar behavior of two networks unless the masks are identical.\n\nIf the authors think this pruning mask comparison is valuable, they should state that outright rather than making an argument that this somehow has anything to do with being a winning ticket. And I don't think this comparison is valuable: random subnetworks will assuredly have different structure than methods that prune different layers by different proportions. Moreover, Jaccard similarity of pruning masks says very little about the relationships between the actual behavior of two networks. What does a Jaccard similarity of 0.12 mean in comparison to a similarity of 0.02? How should one interpret that in terms of the behaviors of the network?\n\nFinally, all of these Jaccard similarities are very far from 1. The highest number comparing IMP and Phew is 0.25, and numbers are frequently closer to 0.1. These subnetworks aren't especially similar. Also, this section uses a network on MNIST rather than the networks in the rest of the paper, which is suspicious.\n\n## Ablations\n\nNote: the papers referenced here were released close enough to the ICLR deadline that the authors are not responsible for their content. However, I think it is important to bring this up, as it provides the authors with a great opportunity to strengthen their paper and convince me of the merit of their technique. I will not hold it against the authors if they do not include these comparisons, but - should they choose to include them and the results look promising - it would improve my view of this paper.\n\nRecent work (Su et al: \"Sanity-Checking Pruning Methods: Random Tickets Can Win the Jackpot,\" and Anonymous (ICLR 2021 submission) \"Pruning Neural Networks at Initialization: Why are we missing the mark?\") show that SNIP, GraSP, and SynFlow don't actually determine which weights are important to prune at initialization - just how much to prune each layer by. These papers show this by taking the SNIP/GraSP/SynFlow pruning masks, randomly shuffling them per-layer, and then showing that accuracy is the same (or, in the case of SynFlow, better). Moreover, Su et al propose a trivial way to generate good layerwise pruning fractions that matches the performance of SNIP, GraSP, and SynFlow. In other words, SNIP, GraSP, and SynFlow aren't doing anything very impressive - they're just fancy ways of figuring out how much to prune each layer of a network. I am concerned that PHEW is no different in this respect.\n\nThe authors of PHEW can run this ablation (shuffling the per-layer pruning masks before training) and show that their method is not affected to convince me that their method is an improvement over SNIP, GraSP, and SynFlow.\n\nAgain, this is optional for ICLR: the authors are not responsible for these papers by the ICLR concurrent work policy. I will not hold it against the authors if they do not make this comparison. But it is an opportunity to strengthen their paper and my impression of their paper.\n\n# To Increase My Score\n\nTo increase my score such that I will consider the paper for acceptance, the authors will need to do the following:\n1) Either show evidence that stub neurons are actually a problem at reasonable sparsities for the comparison pruning techniques used in the paper or drop the claim.\n2) Either add substantial evidence to Sections 3 and 4 supporting the claim of causality between higher gradient magnitude at initialization and higher final accuracy in neural networks or drop the claims.\n3) Clarify whether Section 4 uses PHEW as described in Section 5 or uses some other technique. It's not readily apparent.\n4) Study additional networks, specifically those are not so wildly overparameterized (e.g., ResNet-20 or ResNet-56 for CIFAR-10).\n5) Add the missing baselines as mentioned above and make the case that PHEW improvements are meaningful in the context of these baselines.\n6) Address cases where PHEW does not perform the best.\n7) Show many sparsities, not just three, in Table 1, so that a reader can be confident that you are not hiding anything by picking those three sparsities.\n8) Fix the problems with Section 6.2. (A) Either show that PHEW finds winning tickets by the proper definition or drop the claim (and remove mention of winning tickets from the title). (B) Either argue that the mask comparisons are independently valuable (I'm skeptical) or remove them from the paper.\n9) Optional: Run the ablations of Su et al./Anonymous and report on the findings.\n\nOnce you go down this list, you'll see that I'm skeptical about the content of Sections 2, 3, 4, and 6.2. The success of the paper rests on Sections 5 and 6: the novelty of PHEW and the actual results. The evaluation needs to be a lot more rigorous and convincing if the entire paper needs to rest on that alone. And right now, it is, given that I think Sections 2, 3, 4, and 6.2 undermine some potentially promising results by mixing in other sketchy claims.\n\n# Score\n\nThe idea of pruning paths is interesting, but the performance is lackluster (and the evaluation is not rigorous enough to convince me otherwise), the justification for the method is sketchy, and Section 6.2 is just plain wrong. The literature already contains three papers claiming to prune networks at initialization with lackluster performance and sketchy justifications (SNIP, GraSP, and SynFlow); it doesn't need another one.\n\nAs such, I argue strongly for rejection, although the authors are encouraged to address my points of concern and convince me otherwise. I am very open-minded to improving my score if presented with convincing evidence,  and I will take a close look at the response from the authors during the discussion period. I look forward to discussing this interesting paper with the authors over the next couple of weeks and hope we will have time to go through multiple rounds of interaction. The authors are encouraged to respond early (even if they respond to my review piecemeal over time) so that we can get a conversation going.\n\nScore: 3 - clear rejection\n\nConfidence: 4  - I am an expert on this topic and I am intimately familiar with the relevant literature, but I want to give the authors a fair chance to convince me I am wrong.\n\n# Notes\n\n## Intro\n\nThe term \"fully-connected network\" typically refers to a fully-connected multilayer perceptron (MLP). The term you should use is the \"unpruned network.\"\n\nThe paragraph before the contributions doesn't provide any useful information about PHEW itself. How does it work? How are paths selected? Most readers won't read past the intro, so it's important to provide that information there.\n\n## Section 2\n\nNeed to define \"layer collapse\" formally. Only someone who has read Tanaka et al. will know what you're describing.\n\nI'm confused about Figure 1: why does pruning random paths perform better than pruning random connections even before any weights have been pruned (the leftmost point)? Also, shouldn't there be error bars?\n\nHow do you perform path pruning in the face of overlapping paths? Do you simply keep pruning random paths one by one until you reach a target sparsity? Is there a faster way to do this?\n\nHow many stub units show up at \"matching sparsities\" (those where pruning with a method can still reach full accuracy) for random pruning? Is this actually a problematic phenomenon at sparsities that are relevant, or does this only occur at extreme sparsities where the network has already lost a substantial amount of accuracy? It's important to substantiate the claimed phenomenon.\n\n## Section 3\n\nSection 2 does not demonstrate the advantages of conserving input-output paths. It shows that it appears to perform better than pruning weights in the random pruning case, but that's far from a demonstration of efficacy (at least so far in the paper).\n\nThe assumptions in the proof of Figure 3 don't seem to make sense. Is it reasonable to assume that one path has a very large weight while one has a weight close to zero? This analysis doesn't take into account any other weights in the network; their effect on the gradients through any given weight will be much greater than the single weight you examine (since there are so many other paths from the output to any given weight).\n\nAlso, what does it mean for a path to have a loss gradient magnitude? Isn't that an attribute of individual weights?\n\nThe proof is correct if we assume the network has no other weights besides these two paths, but that's an unreasonable assumption and I don't see what it means for a path to have a gradient.\n\n## Section 4\n\nThe argument made in the beginning of Section 4 is identical to the argument made in the SNIP paper: weights that have a greater affect on the loss are better to keep when pruning. Please cite that paper accordingly.\n\nWhat does it mean for a path to have a \"higher gradient magnitude\"? It's never defined in the paper. Presumably it's the products of the gradient magnitudes for each weight along the path? How are the paths pruned? Is the path with the highest gradient magnitude pruned, and then recomputed? Is it not recomputed? Does this occur iteratively? This experiment is not reproducible.\n\nShowing that one method reaches higher accuracy and maintains a higher initial gradient magnitude is not evidence that one causally leads to the other.\n\nIn Figure 4, the three methods look identical until the network has lost more than 20 percentage points of accuracy. Why should we care about sparsities in this extreme range where the network has been crippled in this fashion?\n\n## Section 5\n\nThis section seems to assume that there is no known method for finding and pruning paths with higher EWPs. (Also, what happened to using higher gradient paths instead of weight paths?). But these techniques were both used in Section 4. This seems to be out of order: shouldn't an algorithm be described before a technique is used?\n\nThe argument in the beginning of Section 5 is completely hand-wavey. The notion of \"balance\" cuts against the analysis that just appeared earlier in the paper that argued to strictly prune individual paths with the highest EWP or gradient magnitude product. Where is this notion of \"balance\" empirically substantiated?\n\nHow close is this method to pruning the _highest_ weight paths in practice?\n\nHow does this method fare in situations where all weights receive the same initial magnitude (e.g., all weights are set to +/- sigma?)? Is this method sensitive to initialization schemes with specific properties? If so, which ones?\n\n## Section 6\n\nThese networks are wildly overparameterized for these tasks. VGG-19 and ResNet-34 have more than 20M parameters each, while networks that get similar performance (e.g., ResNet-20) have 75x to 100x fewer parameters. These networks are a way to get gaudy-looking sparsity numbers, but they aren't representative of real-world or larger-scale settings. The paper should use some less extraordinarily overparameterized networks.\n\nTable 1 only studies these methods at very extreme sparsities where none of the methods can find winning tickets. What do these numbers look like at less extreme sparsities? In general, it is a best practice to consider many different sparsities across the spectrum (as you do in Figure 5) - why not replace Table 1 with a comparison like this? In general, it is easy to cherry-pick networks, datasets, and sparsities where any pruning method looks like it is the best. The lowest sparsity in table 1 is 90%, which is already an extreme sparsity past the point where any of the methods can match full accuracy.\n\nHow many replicates were used to compute the mean and standard deviation?\n\nTable 1 is missing IMP with rewinding or pruning after training (some kind of pruning method that is known to get good performance as a baseline against which to compare the performance of this method).\n\nTable 1 is missing a comparison to random pruning as a lower baseline.\n\n## Section 6.2\n\nThis is not the definition of a winning ticket. See the beginning of this review. None of this analysis actually has anything to do with the definition of a winning ticket. If the authors wish to argue that the networks are structurally similar to IMP winning tickets, that could be an interesting discussion. But it has nothing to do with the definition of a winning ticket, and the argument currently being made is nonsensical.\n\nWhy does this section use a network on MNIST rather than the larger networks used in the previous section?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Need more justifications for the claim.",
            "review": "Summary:\n\n- This paper proposes a new algorithm for pruning neural networks at initialization without using any data. The authors motivate their method by first showing that existing pruning algorithms can result in `stub units`, then to overcome this issue,  they propose to preserve those paths that have higher edge weights than the others. To find such paths and also encourage more uniformly distributed connections at each layer, the authors adopt a bi-directional random walk algorithm. Empirically, the authors demonstrated that the proposed method can outperform existing algorithms by a big margin.\n\n\nPros:\n\n- This paper is easy to follow, and also the proposed method is well-motivated, and especially the existence of `stub units` is a big issue in existing methods. The proposed method can avoid such `stub units` and hence achieves improved performance.  \nThe experiments are well-executed and the corresponding results are statistically significant.\n\nCons & Questions:\n\n- Although I can follow the paper well, the writting can be greatly improved, especially the choice of some of the words.\n\n- The claim in section 3 may not hold for neurla networks with ReLU activation function. Consider the following case: for two paths p1 & p2, the edges along p1 has a lager magnitude than p2, however there exsist a dead unit in p1 (i.e., the activation is 0), then the gradient flow at that unit is zero, and thus no matter how large the weight maginitudes of the connected edges are, the gradient is always 0. In this case, p1 should be less important than p2, instead of p2 is more important. So, I doubt the the validaness of the claim.\n\n- Why do you use `bi-directional random walk` to cosntruct the sparse subnetwork? Based on the claimed motivation (in section 3), I believe selecting the top-k paths should be the right way. The current way of constructing the sparse subnetwork seems does not correspond to the claimed motivations? \n\n- Why data independent is good feature of pruning algorithm? I believe the pruned architecture should depend on the training data distribution.\n\n\nOverall, I am incling to rejection at the current stage. I will increase my score if the authors can address my concrens raised in `Cons & Questions` during the rebuttal period.\n\nTypo:\n- Further analysis of pruning has showed --> Further analysis of pruning has shown",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "New technique of pruning is intuitive and performs well",
            "review": "Summary: This paper comes up with a new technique for pruning at initialization without training data that keeps paths of higher weights rather than individual weights. They use gradient norms to explain why this method makes sense, demonstrate on a few datasets that it performs better than existing methods that prune before initialization, and compare the network structure to that of iterative magnitude pruning.\n\nPros\n* Pruning at initialization is an interesting and useful area of research. This paper develops a novel technique and does some analyses as to why it works.\n* Pruning by paths instead of edges to avoid stub units is simple and intuitive. Pruning by EWP for higher gradients also makes intuitive sense.\n* PHEW consistently performs better than other methods of pruning at initialization (table 1).\n* The paper is written clearly, apart from some minor questions listed below.\n\nCons\n* Section 4 has good comparisons; I would like to see more comparisons in section 6 as well, most importantly comparisons to random pruning (i.e. weights are shuffled within each layer) and inverted pruning (i.e. keep lower EWP paths). A recent but relevant paper https://arxiv.org/abs/2009.08576 shows how the other pruning methods don’t always do better than those baselines.\n* Table 1: while PHEW is consistently better than other methods and the random paths, the increase is only very small.\n* Figure 5: does not perform as well as SynFlow at high sparsity levels\n* Section 6.2: Whether a sparse structure is a “winning ticket” should just depend on the final performance of the trained sparse model. It is interesting that PHEW is more similar to IMP than it is to random, but having some shared properties does not necessarily make them winning tickets. Also, I would like to see experiments in this section done on larger datasets and conv networks, not just MLPs on MNIST.\n\nAdditional questions:\n* Figure 4 and section 4: what are “lower EWP paths” and how were they found? How are the “higher EWP paths” here different from those in sections 5-6 (how do you select the paths if not using the random walk)? What are the EWP of the random paths?\n* Section 5: do you need paths through every input/output unit, or just most (to obtain a “more uniformly distributed across all the inputs and outputs”)? Also, do “input units” and “output units” mean all units in the network (all layers), or just input units in the first layer and output units in the last layer?\n* Section 5.2: why use eq. 3 to select a kernel and then select a weight, rather than using eq. 3 to directly select the weight?\n* Section 6.1: how do you handle skip connections for ResNet?\n* Figure 5: error bars?\n* (Minor) Section 6.1: what are the “standard hyper-parameters”?\n\nAdditional suggestions:\n* Section 5: it would be great if you proved or empirically showed the importance of balance, and how you should weigh the importance of balance vs. high EWP. How much EWP (and gradient norm) do you end up sacrificing in order to obtain the balance?\n* What are some properties about the PHEW networks? For instance, how much do paths overlap? How much does each layer get pruned?\n* How do the gradient norms compare between the experiments in section 6.1?\n* I am curious how PHEW compares to pruning with data or pruning after training - I don’t expect to beat them, but it would be interesting to see how far off PHEW is.\n\nOverall: I recommend acceptance because this paper proposes a novel technique that is intuitive and performs well. While there are some missing pieces, I believe the ideas in this paper will be valuable to the community.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}