{
    "Decision": "",
    "Reviews": [
        {
            "title": "Study of early fusion of different modalities, with questionable experimental settings ",
            "review": "This paper studies early fusion with the introduction of a convolutional LSTM network architecture and compares it to intermediate and fully connected layer fusion. Experiments were conducted on a data set of combining two separate image and audio data sets, in addition to adding white noise to the data. Results show the benefit of early fusion.\n\nThe problem addressed is a classical one namely fusion of different modalities at different stages of data processing. In particular, early fusion, intermediate fusion and fully connected layer fusion are compared. The motivation of the study comes from neuroscience findings, namely that brains perform multimodal processing almost immediately, which is in contrast with often applied multimodal fusion strategies today. Whether multimodal data fusion of artificial systems should start immediately is certainly an interesting research question. \n\nAiming at audio-visual fusion, a convolutional LSTM network is introduced for this study. Experiments were conducted on a data set of combining two separate image and audio data sets, in addition to adding white noise to the data. Results show the benefit of early fusion. \n\nThe biggest concern with this study is the use of data sets. MNIST is used as the visual input and the Free Spoken Digit data set as the audio input, and the inputs with the same digit labels are paired. With this design philosophy, one can combine CIFAR-10 with the Free Spoken Digit data set, and it makes no difference for the machine. The advantage of early fusion is to exploit the correlation among different modalities in an early stage; for example, an audio-visual data set with time alignment between the modalities would have the property of correlation that an early fusion can exploit and benefit from. This, however, does not exist in the artificially constructed data set used in this paper. This weakens the experiments and the resulting conclusions. \n   \nAnother observation in the results is that if we compare unimodal results (e.g. Fig. 5, Train Visual SNR=0.5) with audio-visual results (e.g., Train Visual SNR=0.5 and any of the Train Audio SNR values from 0.25 to 2), the unimodal method performs far better. This means the audio modality does not help and does harm the performance. Then the conclusion would be no fusion is better. \n\nThe stronger performance of the vision modality also plays a role in how different fusion strategies are implemented, namely, more vision information used, better performance. \n\nIn the introduction of the convolutional LSTM, g_t=tanh() is mentioned as a gate, but it is in fact the new information rather than a gate; otherwise, it will end up with a multiplication of two gates i_t and g_t. This is about terminology only. \n\nThe spectrograms plotted in the paper are unclear, making it hard to see the details. \n\n\"This [combining two benchmark data sets] allows us to combine them to create a multimodal task which we know will be solvable\"\n- It is unclear what 'solvable' means. Also, a truly multimodal data set would have all the properties that the mixed data set has and further is more suitable for data fusion study of such. \n\nIn audio-visual speech processing, often more layers are used for both the unimodal processing and joint processing stages. It would be interesting to see how this impacts the behaviours of different fusion strategies. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting question being addressed; needs more exploration",
            "review": "This paper examines the impact of early vs late audio-visual fusion on robustness to (white) noise. It uses a convolutional-LSTM (C-LSTM) architecture and creates a multimodal dataset by pairing MNIST images with recorded audio of digits (the Free Spoken Digit dataset). It perturbs each modality independently to varying degrees of signal-to-noise ratio (SNR) and illustrates the resulting accuracies and predictions over time (i.e. as the audio plays in time). The work is motivated by both applied settings and neuroscience-based observations.\n\nOverall, I think this is an interesting paper and I appreciate the systematic exploration of the focused question that it addresses regarding the impact of audio-visual fusion point on robustness to white noise. \n\nGiven the question being explored is about importance of location in the architecture at which fusion happens, and given that the authors focus on one particular architecture, then I would like to see more of (some/any of) the following:\n\n(1) Numerical results. What are the actual accuracies? what are the exact baseline accuracies for, e.g MNIST visual only? etc. what are the exact differences between early and late fusion points for the non-noise cases? Comparing shades of yellow is excellent for seeing a trend in one image (some very nice figures!), but is not helpful for comparing exact values that are not visually adjacent. For example in Fig 7, the corresponding text says \"the correct class comes to dominate the activations\", but for SNR=(0.5,0.5) (4th row in Fig 7), it looks to me like the 3 might be getting classified as an 8. In Fig 5, I am curious about the exact accuracies for different amounts of training noise, etc. I don't require all the numbers that went into each figure, but just enough to be able to draw the important conclusions by looking at those tables themselves. This would help the clarity and quality, and depending on the numbers, it may also impact the significance of this work. (And could be provided in an Appendix)\n\n(2) Datasets. I am OK with having one “main” thoroughly-manipulated dataset (i.e. SNR ratios for audio and visual MNIST), but why not show how these ideas (i.e. an early vs a late fusion point) generalize to one or two other tasks? E.g. even SVHN might be a good start (?), and could match naturally with the existing recorded audio numbers. This would help the quality of the analysis and could support the significance of this paper. \n\n(3) Noise. Did the authors try other kinds of corruption, e.g. masking? more structured noise, e.g. \"coffee shop\" noise over the talking? structured visual 'glare'? This would help the quality of the analysis and support the signifance of the paper. Note that structured audio noise may best be added at the audio stage, before computing the spectrogram.\n\n(4) Discussion/Background. A bit more about limitations of this analysis, possible insights/intuitions, context/relationship to other audio-visual tasks (whether AVSR, captioning, etc). For example, the authors mention that Baltrusaitis et al articulate various thoughtful considerations in fusion approaches: how does this work address or ignore some of them? More examples of early vs late fusion in applied settings? etc\n\nSome other questions and observations arose as I read this:\n\n* How sensitive are the results to the exact alignment of the audio and the visual, e.g. what if the audio is \"centered\" rather than front-padded?\n* What \"slice\" (width) of the spectrogram is passed in at every time step? \n* Note that an MNIST image and a short spectrogram as the ones here could also be processed quite naturally by (non-recurrent) CNNs with the same # of params as the C-LSTM. Would this be a simple relevant baseline? (If yes, I'd like to see it; if not, why not?)\n* The C-LSTM is motivated as maintaining modality-specific inductive biases of each of audio and visual inputs, while still allowing easy manipulation of the fusion point to occur earlier/later.  Note that while conceivably an architecture with\nmultiple attention heads may handle fusion well in terms of final accuracy by dealing with alignment etc, the choice made here seems reasonable and well-suited for controlling for the various factors they are studying.\u0000 That said, I would appreciate an appendix that provides detail about exactly how the early vs late fusion is implemented. I am pretty sure I get it from Fig1 and Section 3, but it seems so critical for this paper, that there would be no reason not to get into full detail about it in an appendix.\n* I found the difference between audio and visual tasks shown in Fig 5 and Fig 6 to be quite interesting!\n* In Fig 7 and in all the appendix figures, the visual model appears heavily biased towards 3’s and (even more so) 6’s regardless of the input. The audio input seems helpful in counteracting this.\n* I do not find the results overly surprising, i.e. providing more information to more layers earlier on should just “add” to the information-- this is not a negative point, I still appreciate having intuitions confirmed-- but if the authors have any further insights to add in the discussion about this, I would be interested to read, e.g. was this their intuition as well? or is there a reason this *would* be surprising? (and would that intuition lead to examples where a later fusion point is indeed better?)\n* Why is there so much front padding on the audio, i.e. why not just make the audio clips shorter? or is it because there were one or two very long audio examples in the dataset?\n* Overall the presentation is clear and well-organized!\n* The exploration is indeed systematic, and I appreciate that!\n\nI like the problem and the approach and the focus of the paper. Further digging & details would be helpful as suggested above (e.g. it helps, but by exactly how much in this case (beyond the trends shown in the figures)? how does it generalize? what might this mean for future task? etc). In terms of work/benefit ratio, I believe that adding numerical results and a more detailed discussion would improve the paper somewhat, but it would still likely not be enough for me to recommend an accept unless the numbers+discussion provide a surprising amount of new information/insight. To improve the paper more significantly, I believe that at least one new dataset would be essential and perhaps another type(s) of noise included in the analysis. I would be very interested in seeing any revised version of this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unorthodox choice of fusion between image of digit and spoken digit spectrogram.",
            "review": "The paper is written well and fairly clear. A primary issue with the paper is that it makes a broad conclusion that early fusion is better than late based on a limited study. The audio-visual task is not what one typically expects for audio-visual speech recognition, that of a video of the moving lips uttering the synchronized audio of spoken word, as in the LRS2 database used in the study in the paper’s first reference Yu et al. 2020. Here, the authors present a training example comprised of an MNIST digit image with a spoken digit from the Free Spoken Digit corpus. They use an unorthodox combination of visual features with audio, by synchronizing the columns of the digit image with the time frame spectrogram filter outputs, which must be zero padded to yield the same number of frame in time as columns in the image. Since there is no meaningful relationship between image columns and a corresponding spectral coefficients, this introduces significant inductive bias in the model. Perhaps this could be overlooked if this were a novel study, however the history of multi-modal audio-visual learning is much older than the paper would indicate. The finding in [1] examined feature vs decision fusion in an GMM-HMM-based recognizer over 20 years ago and concluded the five different decision fusion approaches are all significantly better than the concatenated visual and audio features contrary to what this paper concludes. It may be that discriminative approaches like in this ICLR work better with early fusion, and generative ones like in [1] work well with late fusion due to the interpretable probabilities; however to make such a wide-ranging conclusion would require extensive theoretical and experimental validation, but would be a worthy area of research.\n \nQuality\n\nThe paper is well written and clear however the experimental results are questionable. With the accuracies being reported using a color gradient rather than actual numbers, it is hard to tell if the unimodal visual and audio baselines are correct; for single digits the accuracy should be >99%. In figure 6, it appears the unimodal accuracy of recognizing one out of ten digits with no noise is around 75% which is very poor and indicates a broken system. It is unfortunately hard to believe the rest of the results which incorporates this audio representation.\n\nClarity\n\nThe paper is clearly written. The figures, esp 1 and 2, are helpful in understanding how the authors combine the visual and speech modalities. The authors could specify how the spectral features are produced; one would assume a 10ms frame rate, 25 ms analysis window with a hanning or hamming function applied, but it would be better if one did not have to assume.\n\nOriginality\n\nThe combination of visual and audio signal in this manner: image columns treated as time varying features is novel, however in the field of multi-modal learning early-vs-late fusion is question that has been examined for at least 20 years.\n\nSignificance\n\n\n\nReferences and Relevant Work\n\nThe references omit far earlier work in multi-modal learning. It is unclear how the author’s chose their (recent) references to A/V speech recognition and multi-modal learning. Understandably, there is a lot of work in the field, so the choice of reference can be somewhat arbitrary, but the citation should go to earlier pioneering work. Some of these older references are:\n\n[1] Chalapathy Neti et al. LARGE-VOCABULARY AUDIO-VISUAL SPEECH RECOGNITION: A SUMMARY OF THE JOHNS HOPKINS SUMMER 2000 WORKSHOP. Proc. Workshop on Multimedia Signal Processing (Special Session on Joint Audio-Visual Processing), pp. 619-624, Cannes, 2001\n\n[2] S.L. Oviatt, A. DeAngeli, and K. Kuhn. 1997. Integration and Synchronization of Input Modes During Multimodal Human-Computer Interaction. In Proceedings of ACM Conference on Human Factors in Computing Systems, (CHI'97), pp. 415-422. \n\n[3] Dupont, S. and Luettin, J., “Audio-visual speech modeling for continuous speech recognition,” IEEE Trans. Multimedia, vol. 2, pp. 141–151, 2000.\n\n[4] Bangalore, Srinivas / Johnston, Michael (2000): \"Integrating multimodal language processing with speech recognition\", In ICSLP-2000, vol.2, 126-129.\n\nPros:\n- small scale experiments on public corpora that should be easy to reproduce\n- clear figures that demonstrate the main ideas of the paper\n\nCons\n- flawed experimental design treating image columns as temporal features synchronized with audio spectra although they are not meaningfully related.\n- the feature concatenation strategy and modeling has significant inductive bias; a generate model may have lead to different conclusions cf [1]. \n- poor references to prior work especially the field of audio-visual machine learning\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review -- AnonReviewer3",
            "review": "**Summary**\n\nIn this work, the authors systematically study the role of early fusion in multi-modal learning across visual and audio perceptual tasks. The authors investigate this topic with a convolutional LSTM architecture using the MNIST image classification dataset and the Free Spoken Digit datasets. The authors find that early fusion outperforms late fusion and improves model performance in the presence of noise in each modality.\n\n**Major Comments**\n\n*1. Generality of the results.*\nI am concerned that the lessons learned in this work apply only to the 1 problem examined. This concern is further amplified by the fact that the authors used the MNIST dataset which is well known to be saturating in accuracy (i.e. the problem is largely 'solved') thus any differences in the accuracy classifying the digits might not generalize to other, more rich problems. This has been the case for a large number of proposed image classification architectures and correspondingly, many authors have switched to more complex input data such as CIFAR-10, ImageNet, etc. \n\n*2. Convolutional LSTM.*\nAlthough these works did not use multi-modal problems, the authors should discuss and compare their architecture to other papers that have used convolutional LSTM architectures. Here are a few but quick literature search would uncover many others.\nhttps://arxiv.org/abs/1711.10151\nhttps://arxiv.org/abs/1506.04214\nhttps://arxiv.org/abs/1306.2795\n\n*3. Comparison with other multimodal problems.*\nThere are several applications of multimodal learning in the literature and the authors should compare their approach to these. For instance, the literature on lip reading provides a prime example.\nhttps://arxiv.org/abs/1806.06053\nhttps://arxiv.org/abs/1802.06424\nDid their results accord with what was seen in that literature as well? Or do the results differ? This discussion is missing from their work.\n\n*4. Multimodal sensory processing in neuroscience.*\nI would take issue with the characterization that \"only recently\" do neuroscientists appreciate the multimodal aspects of sensory cortices. The theme of \"cortical area == single dedicated sense/goal\" has long been a theme of a constant tension in neuroscience as evidence both for and against have been compiled across the years. The 'recent insights' are just on-going evidence in a long standing discussion that will probably continue for many decades hence.\n\n**Minor Comments**\n1. Figure 2 is not very informative. \n2. Justification for noise source. Additive white noise is the simplest form of noise degradation, however it would be great if the authors offered more discussion and justification for this form of noise. For instance, what are the distribution and sources of noise inherent in audio data? What about the MNIST data?\n3. Presentation of the accuracies. Although the heat map provides a visually salient means of summarizing the result, it would be good to pair this work with Euclidean graphs to quantitatively get a sense of how strong the gains/differences are.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}