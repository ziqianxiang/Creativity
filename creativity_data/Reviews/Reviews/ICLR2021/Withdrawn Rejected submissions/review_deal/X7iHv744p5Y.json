{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "Summary: \n\nThis work proposes an unsupervised paraphrase and abductive reasoning model inspired by the distributional hypothesis for words. The underlying assumption for their framework is that semantically similar sentences (or phrases) also should appear in a similar context. The proposed model helps in narrowing the gap between unsupervised and supervised paraphrasing.\n\nStrengths:\n\n1. Unsupervised approach - Doesn't require parallel data or bilingual data.\n2. Novel approach for auto-regressive text infilling, without length constraint. (\\alpha NLG)\n3. Clarity of the paper.\n\nWeaknesses:\n\n1. Computationally expensive training (GPT2 with large corpora) as well as inference (RD). For paraphrasing, source sentence needs to be sent to the model, contexts need to be generated, weights (w's) need to be optimized, ranking needs to be decided, to finally obtain a set of paraphrases for a sentence.\n2. Unrestricted generation of paraphrases: The quality of a paraphrase is dependent on meaning retention, lexical or syntactical diversity as well as grammatical correctness. While this approach might be helpful in meaning preservation (assuming semantically similar long texts do appear in a similar context) and grammatical correctness (if trained on the grammatically correct text), there seems to be no restriction (al least before re-ranking and sentence selection) on diversity (during training and inference). \n3. Experiments are conducted on a single dataset. \n\nComments:\n\n1. The strength of the method (for paraphrasing) is only realized during subset selection of paraphrases from a set of candidates (RD_30, RD_45). An alternative approach might be to just train an autoencoder model (with the same source and target sentence - thereby not needing any parallel data), inferring multiple sentences using beam search and selecting candidates during inference (using Novelty, Beam Search Scores, Subset selection, Diverse Paraphrasing, Diverse Beam Search). This will be computationally inexpensive during inference.\n2. In an ideal case, the approach basically is like an autoencoder approach with (partial) access to intermediate states. This would imply that during inference(RD) if the weights (w's) are learned correctly, the same source sentence should be generated. Empirically if that doesn't happen then it probably is because of some noise/suboptimal selection of w's. This boils down to saying that the strength of this approach lies in inducing (semi-controlled) noise in the system. If I am correct in interpreting this, then it seems like a computationally expensive way to induce noise.\n3. Please evaluate the generations on BERTScore, BLEURT, for paraphrase similarity.\n\nQuestions:\n\n1. What is the difference between k_c described in weight pruning and n_c defined in parameters (section 2.3)?\n2. Is the training data for CMGH, RD, R-VQVAE the same?\n3. While it makes intuitive sense why semantically (to some degree) similar words appear in a similar context, I am unsure if the extension to longer text makes sense. Please comment on this.\n\nPlease let me know if I have misunderstood something(s) (weaknesses/comments). I am open to revising the scores based on your inputs.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Using preceding/following sentences to represent the meaning of given texutual condition",
            "review": "Given a textual condition, the proposed method in this paper first generates preceding/following sentences by sampling from pre-trained neural language models (LMs), and then generates a sentence that is requested by the given task.  The method is used for two purposes: one is paraphrase generation, where a paraphrase sentence for a given sentence is generated, and the other is abductive text in-filling, which generates a sentence that fills in the blank between a given pair of sentences.  This approach tries to represent the meaning of given textual condition (a single sentence or a pair of sentences) with a set of preceding/following sentences, which is a natural extension of classic methods based on distributional hypothesis, i.e., representing words and phrases with set of their surrounding words and phrases.  As such, the proposed method has a rationale, and experimental results support that the method can achieve better performance than existing approaches and methods that do not use any task-specific training data.\n\nI like the presented idea and acknowledge its novelty and potential impact.  However, I cannot say that the paper writing and experiments are mature enough.  The most confusing thing is the naming of LMs.  The term \"left-to-right\" is typically used to refer to a sentence generation process, which starts from the leftmost token (e.g., word and phrase) in the sentence, generates one token at a time in an auto-regressive manner, and ends with generating the rightmost token.  For the languages that are written from right-to-left, such as Arabic and Hebrew, this term refers to the process of generating from the sentence end to the sentence beginning.  Unlike this convention, in this paper, \"left-to-right\" with $\\overrightarrow{\\text{LM}}$ is used to refer to the prediction of next sentence, and so does \"right-to-left\" with $\\overleftarrow{\\text{LM}}$ the prediction of preceding sentence.  These inappropriate notations mislead the readers and deteriorates readability of the paper.  I strongly recommend the authors to reconsider the terminology. \n\nOutputs of various methods were evaluated using automatic metrics, such as SARI and BLEU, as well as by human annotators.  However, I am not convinced that the human evaluation was properly done.  The protocol for human judgment has several unclear points that directly affect the quality, such as (a) proficiency of annotators, (b) whether they were carefully screened, (c) whether the outputs of different systems were anonymized and shuffled, and (d) whether the annotators were allowed/encouraged to repeatedly check their assessments in order to maximize self-consistency.  In my experience, a task with Fleiss's kappa below 0.4 lacks substantial regulation, even though coefficient values (0.20-0.40] are interpreted as \"fair agreement\" according to Landis and Koch (1977).  Please do not forget to cite Fleiss (1971) and Landis and Koch (1977).\n\nI wonder how one can draw human scores of XX.X (not XX) from only 100 examples evaluated manually.  The performance gap between models is not easily significant based on the number of samples, even though both 61.8 and 61.4 are marked.\n\nFrom a technical point of view, while the authors chose nucleus sampling in order to ensure the collectiveness of sampled sentences, it is not effectively emphasized.  I'd suggest to include its concise explanation.  Deleting Figure 2 (since Figure 1 can play the same role) saves the space for it.\n\nIn the end of Section 5, the authors insist that \"A true paraphrase should be novel.\"  However, this is not precise enough, in my opinion.  Paraphrase does not require this by definition; two sentences with one synonym replacement should be regarded as paraphrase of each other as long as they have the same meaning.  However, only a particular subset of paraphrases are eligible depending on application.  For instance, useful paraphrases in text summarization tend to refer to those into a shorter string, and paraphrases in text simplification cover only those from complicated -> simplified texts.  In these scenarios, the notion of correct paraphrases is narrowed down into a particular subset of all correct paraphrases by application-specific constraints.  If the authors assume a situation where \"novelty\" of paraphrases is indispensable, they should explain it rather than distorting the original definition of paraphrase.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting work",
            "review": "In this work, the reflective decoding is proposed for specific NLP applications as a unsupervised approach. This is an interesting idea, and the results are encouraging. Also this paper is well motivated. However, I think both the organization of the paper and the experiments can be improved. My concerns are as follows:\n\nIn algorithm1, how is the w learned?\n\nDid you specify what n_c is used? And how does it affect performance?\n\nI find B.3 hard to understand, what do you mean by \"We set this to 4 to 6.\"?\n\nAfter w is learned, how exactly do you sample from RD?\n\nI'd like to see some ablations study about whether the learning of w is essential, as opposed to just using a uniform w. \n\nAlso I think it's important to show some real examples of the reflective decoding, when does it work, and when it doesn't.\n\nThe paper's writing needs to be improved, there are quite a few places where I get lost of the author's intents. For example, I think it's better that sec2.1 and sec2.2 to be combined, it's hard for me to grasp what are the steps in reflective decoding.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}