{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Review of Paper",
            "review": "This paper proposed an adversarial training pipeline by integrating the pixel level samples and feature level information. The experiments conducted on three tasks/benchmarks and the results show the effectiveness of the method. \n\nThe work is interesting and the empirical results are impressive. However, there are some concerns. First, how the feature level adversary can help robustness is not clear. Previous work only shows it can help generalization. Secondly, based on my experiences, the robustness of some datasets (CIFAR) is too high. I doubt there might be some bias in the model/evaluation. Have you tried more strong attacks instead of PGD? In addition, the paper claimed that the pixel-based method is efficient but has not empirically demonstrated that. \n\nAnother concern is from the related work part. The paper misses several important works. Ref. a is the work using both pixel/feature adversary to improve model robustness. Ref. b and c both applying feature adversary in NLP and V+L. Without comparing it with them, it is hard to access the novelty of the work.\na. Regularizing deep networks using efficient layerwise adversarial training, AAAI 2018.\nb. Freelb: Enhanced adversarial training for natural language understanding, ICLR 2020.\nc. Large-scale adversarial training for vision-and-language representation learning, NeurIPS 2020.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Evaluation is not strong enough ",
            "review": "Summary:\nThe authors propose an adversarial defense strategy using a GAN based Image-translation framework. Particularly, the approach aims to modify the pixel values of an adversarial image to convert it into a non-adversarial input. This is achieved by imposing several loss functions at the pixel-level and at the feature-level to align the distributions of clean and adversarial samples. The authors consider three image based tasks - classification, detection and semantic segmentation. The method is observed to yield good performance on the considered evaluation settings.\n\nStrengths:\n1) One of the strengths of this work is that several baselines are considered for evaluation and an extensive ablation study is performed. This is especially useful when there are several loss functions at play. The losses considered in this work are shown to work cooperatively and improve the performance.\n2) The method achieves good performance in the considered evaluation setup. \n3) Evaluation under model transfer (Appendix Sec. A.2) suggests that the robustness is transferable across models, which could be a useful result.\n\nWeaknesses:\n\nThe major concerns are as follows:\n\n1. Regarding Novelty: The idea of the paper doesn't seem very novel as Defense GAN [1] has already been tried to provide robustness through GAN (which was broken) and there is follow up work which shows that adversarial training is required to make  Defense GAN robust [5]. Although the way of using GAN is novel.\n\nMoreover, several ideas have been proposed by other works (Eq. 3, Eq. 4) . The authors should clarify what is novel in their approach, and why such a complex training strategy is proposed (10 loss functions).\n\n\n2. Concerns regarding the evaluation: \n\na) A major  issue is that the attacks considered are not allowed to backpropagate through GAN for finding adversarial examples. Hence the white box attacks aren't truly white box. Also there are no adaptive attacks which are important for evaluation [2].\n\nb) There are no sanity checks for gradient masking (see [3]).\n\nc) The authors propose to use JPEG non-differentiability in between GAN and classifier to protect itself from attacks which use gradients for the GAN. Introducing non-differentiability is not a defense (see BPDA). This is an example of obfuscating gradients which produce a false sense of security [4].\n\n\n3. Regarding key training details: \n\na) Several key implementation details are missing: The hyperparameter values ($\\lambda_{1}$ through $\\lambda_{9}$) are not mentioned. The method employs too many hyperparameters and loss functions seems to be very difficult to train in practice (especially, considering that there is an adversarial distribution alignment objective in addition to adversarial training). However, neither there is any discussion on choosing the hyperparameter values, nor a sensitivity analysis. What are the number of training epochs? How much compute is required (in terms of both training time, memory requirement and #FLOPs). What is the Train/val split considered? \n\nb) It is unclear how adversaries $x^a$ are generated during training. Are the gradients propagated through generator $\\mathcal{G}$ as well (i.e. is the adversary generated at the input of the generator)? If not, then how is $\\mathcal{G}$ guaranteed to be robust against attacks? (One can always generate an attack by backpropagating the noise generating loss through $\\mathcal{G}$).\n\nc) $\\mathcal{L_o}$ (Eq. 5) is undefined! Is it a cross-entropy loss at the output or a class-wise alignment loss at some intermediate feature space? Also, what are the feature levels considered for Eq. 4-8?\n\nd) Line 6 in Algo. 1 is not scalable for large number of classes (e.g. ImageNet, 1000 classes). Each mini-batch would require samples from all classes, which introduces computations worth several orders of magnitude for each mini-batch. Further, in losses such as $\\mathcal{L_{F_{inter}}}$ , the gradient backpropagation would require thousands of samples in each mini-batch in cases like ImageNet (for computing the cluster center). It is unclear how such a large batch size would be handled. Could authors please clarify whether they only consider different classes in the batch or all classes present in the dataset for calculation?\n\ne) It is unclear what norm is used in Eq. 6,7,8 etc. More importantly, a direct L-p norm assumes that all classes (clusters) are equally important since it does not consider the spread (e.g. covariance). Could the authors clarify why it is appropriate in this case, since intuitively adversaries are likely to exhibit a larger spread as compared to the clean samples?\n\nf) Please clarify the notation for $\\hat{z}^a$ (Eq. 6 vs Eq. 8). Furthermore, is there only a single $\\hat{z}^{a(k)}$ for each $\\hat{m}^{a(k)}$ in Eq. 8?\n\n\n4. Minor comments:\na) Regarding the argument in Sec. A.3 (Theoretical Analysis): The statement that the pixel level constraint has a smaller solution space since $\\hat{x}^a$ and $\\hat{x}^c$ fall in the same output space of $\\mathcal{G}$ is not strong enough. Note that, $\\hat{x}^a$ must eventually match with $x^c$ for successful alignment. However, In the current formulation, the minimization of || $\\hat{x}^c - x^c$ || could get stuck in a local minimum, and therefore, the solution of || $\\hat{x}^a - \\hat{x}^c$ ||. So, this doesn’t eradicate the issue altogether. However, if this indeed is true (i.e. if the proposed constraint is better than the traditional constraint) it should be investigated further - are $\\hat{x}^c$ and $\\hat{x}^a$ really closer in the $\\mathcal{G}$-output? Is it akin to a curriculum where the difficulty of the task is gradually increased?\n\nb) t-SNE (Fig. 6): One should not entirely depend upon t-SNE to evaluate distribution alignment. It would be a good idea to also measure the Proxy-A distance between the distributions [6]. Also, Fig. 6c, 6d seem to be exactly the same (please cross-check if the Proxy-A distance is close to 0).\n\nc) typo on page 3, “This primarily dues to” -> “This is primarily due to”\n\nOverall Comment:\nAlthough the paper shows good results on the settings under consideration, the evaluation is not strong enough (e.g. missing sanity checks for gradient masking, and lack of evaluation on adaptive attacks). The method seems complicated and difficult to train in practice (consists of 10 loss functions). Moreover, the presentation must be improved by addressing the key issues above (e.g. missing details regarding the implementation and hyperparameter search). Considering these, I am leaning towards a rejection. I will be happy to increase the score if my concerns are addressed.\n\n[1]Samangouei, P., Kabkab, M., & Chellappa, R. (2018, February). Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models. In International Conference on Learning Representations.\n\n[2]Tramer, F., Carlini, N., Brendel, W., & Madry, A. (2020). On adaptive attacks to adversarial example defenses. arXiv preprint arXiv:2002.08347.\n\n[3] Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., ... & Kurakin, A. (2019). On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705.\n\n[4] Athalye, A., Carlini, N., & Wagner, D. (2018, July). Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In International Conference on Machine Learning (pp. 274-283).\n\n[5]Jalal, A., Ilyas, A., Daskalakis, C., & Dimakis, A. G. (2017). The robust manifold defense: Adversarial training using generative models. arXiv preprint arXiv:1712.09196.\n\n[6] Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2007). Analysis of representations for domain adaptation. In Advances in neural information processing systems (pp. 137-144).\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting work; not intuitive why it works; evaluation not convincing",
            "review": "### Summary\nThis paper presents an adversarial defense using generative networks. The design is based on aligning the distribution of clean and adversarial samples in the feature spaces, by using both pixel-level constraints and feature-level constraints. The paper claims superior robustness gains compared to previous methods.\n\n### Strengths\n+ The idea of combining generative networks with constraints on different levels seems novel and unexplored.\n+ The authors perform comprehensive evaluations across different tasks and different datasets.\n+ The writing is clear and easy to follow.\n\n### Weaknesses\n1. __Novelty and motivations.__\nThere have been many works on using generative methods to \"improve\" adversarial robustness (e.g., [4, 5]). However, they are all shown to be vulnerable to white-box attacks, and are nearly completely broken under stronger/adaptive attacks [1, 2]. It seems to me that there is no intrinsic difference between this paper and previous works using generative models. The overall pipelines are similar, despite here more losses are added. I don't see reasons why the proposed method would work given the others have already been broken. Can the authors provide intuitions or empirical analysis (such as loss landscape around inputs)?\n\n2. __The evaluations.__\n* Although the authors have performed large-scale studies, both on different tasks and different datasets, the robustness evaluation protocol seems not convincing, and at this moment I tend to believe it's a result of lacking strong attacks (see [1,2] for case studies on defense methods). From my understanding, the attack is not performed over G+O, but only on O --- which significantly reduces the attack strengths. If the model is end-to-end differentiable, one should always try to attack the whole framework.\n* Similar to the point mentioned above, there are no adaptive attacks used to evaluate the method. One should always be careful with a newly proposed defense, and ensure that adversarial robustness is evaluated in the worst-case [6]. Otherwise, the results are not conclusive.\n* The method is only evaluated on one type of black-box attack, e.g., transfer-based attack, which is relatively weak. To check whether the proposed method causes potential gradient masking, it is important to also try against gradient-free black-box attacks, such as decision-based attack (e.g., Boundary attack) and score-based attack (e.g., SPSA [3]).\n* For attacks, only 8 steps are used for PGD adversary, which is too small and not enough (similar issues on ImageNet). Try up to 100/1000 to at least ensure convergence, and may also try using multiple restarts as well as different step sizes. It is important to ensure the defense is evaluated in the worst-case [2].\n\n3. __Other issues.__\nMention the threat model early, that you're trying to defend against l-infinity perturbations.\n\n\nSince the claim of this paper is somewhat unexpected given previous works on defending against adversaries, the experiment results have to be very solid. With the above issues with the experiments I don't believe the current paper is ready for publication yet. \n\n\n### Reference\n1. On Evaluating Adversarial Robustness. 2019.\n2. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. 2018.\n3. Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. 2018.\n4. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. 2018.\n5. Defense-gan: Protecting classifiers against adversarial attacks using generative models. 2018.\n6. On Adaptive Attacks to Adversarial Example Defenses. 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Challenging problem with promising results",
            "review": "Summary: This paper utilizes deep generative networks with a novel training scheme to eliminate the distribution gap between adversarial samples and clean samples in the feature spaces. The training strategy includes both the pixel level and feature level. The results on different tasks and different datasets demonstrate the effectiveness and generality. \n\nClarity: This paper is clearly written and easy to follow. In particular, the abstract is clear and concise. The contributions in the introduction section allow the readers to quickly understand what the paper is claiming. \n\nOriginality: In general, the work is mainly standing on the shoulders of giants, the theoretical innovation is a little insufficient. I think the overall distribution alignment proposed by this work is interesting. \nPros: 1. The results are good. Comparisons between your approach and existing methods on the different tasks show the effectiveness and generality of your work.\n\n2. The paper is clear and well written. It motivates the idea well and provides a decent background explanation.\n\n3. The theme is worth studying as there still exist spaces to improve adversarial defense quality.\n\nCons: 1. When using your defense method, the accuracy of the clean dataset decreases. I know almost all existing defense methods would reduce the accuracy of the clean dataset, this weakness of existing defense methods has not been overcome by your method either.\n\n2. When comparing with the previous works, you had better put the results of the previous state-of-the-art work near your method (e.g. Table 3) so that the readers can easily compare the results. Why do you put the good results by other methods so far away from yours and put the bad results near?\n\n3. In general, you use much more hyper-parameters than the previous works. However, the results only increase a little compared with state-of-the-art work. Therefore, I think the cost effectiveness of your method is low.\n\n4. I suppose you could do more detailed experiments in the ablation study. The ablation study in the paper seems a little insufficient in comparison to your ten losses and nine hyper-parameters. Can you do the ablation study for every loss? It is important because maybe you can reduce some unimportant hyper-parameters and the readers can conclude which losses are the most important for defense for future study.\n\n5. I suppose you had better explain some evaluation indicators before you put it in the table. For example, you could explain the meanings of the evaluation indicators such as mIoU and mAP in table 9. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}