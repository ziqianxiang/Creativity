{
    "Decision": "",
    "Reviews": [
        {
            "title": "Simple trick to improve collapse in DARTS.",
            "review": "The paper proposes a small fix to the Differentiable Architecture Search (DARTS) algorithm. DARTS has been observed to favor residual connection operations leading to performance degradation (or \"collapse\") in the final model. The current paper adds random noise to the output of this operation to try mitigate this effect, and let the model explore other operations more thoroughly.\n\nStrengths:\n- The algorithm is very simple to implement on top of DARTS.\n- The noisy fix does not add computational cost to DARTS.\n- It seems the gains aren't very sensitive to the noisy hparams (mean and std). The hparams values don't seem critical.\n- Results on Imagenet (Table 1) look good (some questions below, however).\n- Source code with hparams will be open sourced.\n\nWeaknesses:\n- Section 3.2 feels handwavy (approx in eq 6 followed by \"must\" statement), and unnecessary to explain the intuition that unbiased noise may allow exploration of other operations without systematically changing the residual operation.\n- The std (only sometimes) reported in result tables seem to be ignored in the analysis. For example, in Table 1, Cifar 10, if we compare DARTS (97.0 +/- 0.14) and Noisy DARTS (97.30 +/- 0.23), the confidence intervals (say 2stds) greatly overlap, so it's hard to conclude anything statistically \"significant\". In many cases, the stds are simply not reported (say, Table 1, Imagenet). This makes reaching empirical conclusions hard.\n\nExtra comments and questions:\n- Notation for (1) is not properly introduced or discussed I think.\n- In (3), is f() the same as \\sigma()? The softmax of the architectural weights?\n- In (6), maybe explicitly say the expectation is wrt to injected noise z.\n- In Table 1, what's the difference between Noise DARTS a, b, A-t, A, A square?\n- Can you add stds to the Imagenet Table 1? Otherwise it's hard to assess the significance of the results.\n- Why is Noisy DART 24 times more expensive than DARTS? (from 0.5 GPU days to 12).\n- It's great to have so many experiments and baseline comparisons in the paper, but sometimes it makes feel the paper a bit overwhelming, and hard to know what are the key results to look at. Also, it leaves way less space for result analysis.\n- Can you add stds to Table 4 left? Imagenet results look convincing, but Cifar-10 would benefit from having stds.\n- In 4.4, the \"zero-mean noise vs biased noise\" conclusions aren't obvious to me. My take looking at Table 4, right, would have been that the noisy hparams don't seem to matter much. The confidence intervals really overlap! N(-0.5, 0.1) and N(0, 0.1) is a great example: the difference in mean performance is less than half a std... So maybe the unbiasedness of the noise is not so important, as long as we add some noise. Any comments on this?\n- The results comparing NFA and OFS look convincing to me (whereas NFA \\sigma=0.4 has a huge variance).\n\n\nOverall the paper proposes a simple and cheap trick to improve on DARTS, and empirical results suggest this may help. If the experimental section is properly described (NoisyDarts-a, -b?), updated, and results are rigorously analysed, this could be a useful paper for practitioners in the AutoML front.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of Noisy Differentiable Architecture Search",
            "review": "This paper proposes a new neural architecture search (NAS) method to address an issue in the gradient-based NAS method (i.e., DARTS). It is known that there is a performance gap between the search and evaluation phases of the gradient-based NAS due to a large number of selections of skip connections. To address this issue, this paper injects Gaussian noise into either skip connection or all candidate operations. The experimental results show that the proposed approach is simple yet effective. \n\nPros\n- The proposed method is simple yet effective and easy to follow.\n- The proposed method and competitive methods are evaluated on appropriate datasets.\n\nCons\n- There are some unclear points about the experiments. First, there are different kinds of the proposed methods (e.g. NoisyDARTS-a, -b, -A-t*), but there is no description of them in the paper. Please elaborate on them. Second, does the result in Table1 come from the OFS or NFA? Also, it would be better to show the difference between them on the experimental settings used in Table1. Finally, SDARTS is a competitive method to this paper, then it would be better to add the result of SDARTS to Table 1, not only Table 2. \n- Recent some papers [1] have reported that training protocol affects the final performance and it is not negligible. Does this paper report the performance of the competitive methods and the proposed method based on the exact same settings? To clarify the contribution of the proposed method, the competitive methods (e.g., DARTS, SDARTS, RDARTS, PDARTS) and the proposed method should be trained using the same experimental settings.\n- The figures are too small to understand.\n[1] A. Yang+, NAS EVALUATION IS FRUSTRATINGLY HARD, ICLR, 2020\n\nOverall, although the proposed method is simple yet effective, there are some unclear points to be clarified for publication.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple but limited fix to the DARTS performance collapse problem.",
            "review": "Summary:\nA method that injects noise into intermediate representations passed to skip-connect operations is proposed to ameliorate the skip-connect-collapse issue of DARTS. The authors evaluate this algorithm on a variety of NAS benchmarks. As discussed below, my concerns about the practicality/generality of the method and about the presentation of the results make me lean against acceptance.\n\nStrengths:\n1. Performance improves upon DARTS and avoids the latter catastrophic results on benchmarks such as NAS-Bench-201.\n2. Evaluation on a variety of search spaces and testing several noise injection settings.\n3. The method is a simple modification to DARTS.\n4. Code is released.\n\nWeaknesses:\n1. The mathematical discussion was confusing and did not seem particularly compelling/necessary. For example, the Section 3.2 discussion on design of mu and sigma comes down to “set mu=0 and tune sigma”, which does not really need much justification. For choosing mu=0 there seems to be some sort of linear approximation occurring in Eq. 4, but this is not formalized. Similarly, the Section 3.3 argument that the injection of noise smooths the loss landscape does not explain how Eq. 8 shows that noise “controls the loss landscape”; a simpler approach would be to just argue that injecting noise is equivalent to convolving with a Gaussian, which is a well-known function-smoothing technique.\n2. The method seems limited to search spaces employing skip-connects (the setting in which noise is injected to the outputs of non-skip-connect operations does not suggest its usefulness beyond skip-connects). Apart from the generality issue, it is thus not clear how this approach is any simpler or better than other skip-connect-specific methods such as DARTS+, to which there is no experimental comparison.\n3. The authors claim state-of-the-art results on the DARTS CIFAR-10 search space, but their average performance (97.30) is worse than that reported by PC-DARTS (97.43). It seems the authors could not reproduce the latter result, but this is not discussed. Similarly, the authors claim state-of-the-art results on the DARTS ImageNet search space, but it is difficult to compare their reported performance (76.1) to that of PC-DARTS (75.8) given that the latter subsampled ImageNet during search (which thus also took much less time). In any case this result should be included in Table 1.\n\nNotes:\n1. “However, the one-shot network is generally not well converged if halted too early, which gives low confidence to derive the final model.” - what does this mean?\n2. “Such a residual structure is generally helpful for training deep networks, so for the supernet of DARTS.” - incomplete sentence.\n3. What is y^\\star in Eq. 4?\n4. “interleavely” - not a word\n5. Fig. 4 is difficult to understand and the text is too small. What are the axes in the plots?\n6. “We run the pipeline across 7 seeds and plot the calculated values in Fig. 2” - what pipeline?\n7. NAS-Bench-201 results for non-weight-sharing methods should be included in Table 2.\n8. “our method stands out of the existing framework and no longer relies on any human designed rules” - injecting a specific amount of noise only for skip connections seems like a human-designed rule.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An empirical method to manually discourage the training and selection of skip connection  ",
            "review": "This paper aims to resolve the performance collapse from an excessive number of skip connections in the resulting architectures from differentiable architecture search (DARTS). With the belief that this peculiar phenomenon is caused by \"unfair\" training of operations, authors propose to add noises to the output feature maps from operation candidates, particularly for skip connections, to boost \"fairness\". They report results on extensive experiments and ablations. \n\nPros:\n+ The proposed method is extremely simple and easy to implement\n+ Extensive ablations on the hyperparameters of the proposed method are reported\n\nCons:\n- The proposed method is of limited technical contributions since it is only an empirical trick. In terms of adopting a probabilistic setup to maintain the sampling process in differentiable architecture search and encourage stochasticity, a prior work, SNAS (Xie et al., ICLR 2019) has provided a generic and theoretically elegant framework which formulates the NAS objective as a stochastic optimization. Even though built upon DARTS, SNAS is distinct in its Gumbel random variable for the sampling of sub-networks and the update of architecture parameters. The very same framework is also adopted by some later works aiming at larger scales or higher efficiency, e.g., FBNet (Wu et al., CVPR 2019), GDAS (Dong and Yang, CVPR 2019), DSNAS (Hu et al., CVPR 2020). I am aware that this work differentiates from these prior works in that noises are injected into the output of operations instead of the softmax over operations, but that only renders this work unprincipled, see below for the comment on the analysis provided. \n- The analyses provided by the authors are far off from the standard of ICLR. Specifically, in the derivation of the approximation in Equation 6, the authors argue for its validity with Equation 4. However, even if we regard Equation 4 as acceptable, it does not necessarily lead to the claim that \\frac{\\partial L}{\\partial y*} equals \\frac{\\partial L}{\\partial y}. Since there is strong nonlinearity in L=F(y), neither the loss L nor the gradient \\frac{\\partial L}{\\partial y*} is guaranteed to perturb only locally. \n- Even though adding noises to neural networks may empirically robustify the latter, the proposed method still occurs strange to me since it only adds noises to the skip connection, (so the name OFS: Only For Skip), which is the only candidate authors want to discourage among all. That is, the authors seem to encourage \"fairness\" by discrimination. The authors did provide an ablation study on the genuinely \"fair\" noise injection setup, NAF, which is an abbreviation for Noise For All in Table 6. However, it only makes me more suspicious.  Note that on NAS-Bench-201, the NAF setup only achieves 91.60±1.74 for CIFAR-10, 68.26±1.59 for CIFAR 100 and 41.57±2.59 for ImageNet, which is much worse than the aforementioned counterpart, GDAS's 93.61±0.09 on CIFAR-10, 70.70±0.30 on CIFAR-100 and 41.71±0.98 ImageNet according to Table 3. Although the result of SNAS was not reported in the original work of NAS-Bench-201, there is a recent reporting from a third-party (Chen et al., 2020) using the code released by the authors of SNAS: 92.77 ± 0.83 for CIFAR-10, 69.34 ± 1.98 for CIFAR-100 and 43.16 ± 2.64 for ImageNet. It is thus apparent that injecting noise to feature maps of all candidates does not provide extra robustification over the principled counterparts. In contrast, probably due to its deviation from the principled framework, we can see a degradation in the performance. Combining this observation and the aforementioned problems in the analyses, I would like to request authors to answer: Is it possible that skip-connections are not selected in the proposed methods simply because the training of subnetworks containing skip-connections is hindered deliberately by the noise injection mechanism? \n- With the above said I am quite concerned about the generality of the proposed method. Let us look at a concrete example. Actually, apart from the universal dominance of skip-connection, there is another speculative phenomenon in the original search space of DARTS, which is initially reported in Xie et al., ICLR 2019 and later systematically studied in Xie et al., 2020: Though most recent NAS cell-based methods directly exclude Zero/None from the search space, this operation was originally introduced by DARTS for the purpose of topology search. Counterintuitively, most edges select Zero/None at the end of DARTS, lots of edges select Zero/None at the end of SNAS. If the proposed method is general enough, can authors provide some primitive thoughts on how to address this phenomenon with the proposed method? \n\n\n\nXie et al., ICLR 2019. SNAS: Stochastic Neural Architecture Search\n\nWu et al., CVPR 2019. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\n\nDong and Yang., CVPR 2019. Searching for A Robust Neural Architecture in Four GPU Hours\n\nHu et al., CVPR 2020. DSNAS: Direct Neural Architecture Search without Parameter Retraining\n\nChen et al., 2020. DRNAS: Dirichlet Neural Architecture Search\n\nXie et al., 2020. Understanding the wiring evolution in differentiable neural architecture search",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}