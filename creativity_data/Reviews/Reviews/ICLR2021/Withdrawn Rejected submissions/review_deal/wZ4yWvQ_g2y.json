{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Compressing BERT is a practically important research direction. Our main concern on this submission is on its practical value. Comparing with MobileBERT in the literature, NAS-BERT does not show advantages on any aspect: latency, prediction performance, or model size (less important), while being much more costly to build because of NAS. MobileBERT just simply narrowed the original BERT models (8x narrower than BERT large). So it is hard to convince the readers that adaptive-size or NAS is interesting or matters. On the research side, this paper have some interesting points on designing the search space, but overall the novelty of this paper is limited, as all of the reviewers pointed out. It is also worth noticing that the claim of \"task agonistic\" in this paper does not fully hold: in the downstream tasks, the soft labels of the teacher model are required to train the compressed model. To be fully \"task agonistic\", the results on downstream tasks should be solely based on training with the ground truth labels, as in the MobileBERT paper. Once following the exact task agnostic experimental protocol, the reported performance in this paper may be significantly lower. "
    },
    "Reviews": [
        {
            "title": "Reasonable techniques and decent performance but with quite some complication",
            "review": "First of all, I believe the paper is looking into a very important question that attracts lots of attention recently. The set of techniques proposed in the work are also reasonable and practical, where the proposed progressive space pruning seems to work very well. Empirically, the obtained models do perform better compared to standard Transformer baselines. As for the comparison with previous methods, since there are too many implementation details that can affect the fairness of the comparison (e.g. length of pretraining, batch size, teacher performances, etc), it's hard to judge the actual scale of the gain.\n\nThere are also a few concerns. \n- Firstly, when the block-wise search is used, it feels like the NAS-BERT is trained in a way that is more similar to a variant of distillation that additionally utilizes intermediate hidden states. As this signal is not used in the standard BERT baseline, some improvement could actually come from this factor besides a better model (architecture+param). A better baseline could be a Transformer trained in a similar way.\n- Secondly, in terms of novelty, this work is more like the combination of existing ideas, namely \"once-for-all\" and \"block-wise search\". One general issue with \"once-for-all\" is after the search, although you obtain multiple models of different sizes, the excellence these models are tied to the (1) specific set of shared parameters obtained and (2) the pre-training task. So, whether this is really a good way to obtain the desired task-agnostic compressed models is still questionable.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Review ",
            "review": "Summary\n\nThe paper develops a new method to compress the BERT model with varying model sizes depending on the underlying usage. They use block-wise neural architecture search to choose the best set of submodules for each of the blocks. To reduce the size of the exponential search space they progressively remove the architectural configurations that yields high loss. Over-all the paper is well written and nicely presented.\n\n+ve\n\n- The NAS-BERT can produce pertained models with varying model sizes which is better than DistilBERT and BERT-PKD that requires pre-training every time the number of layers are varied.\n- The paper conducts rigorous experiments to demonstrate the effectiveness of NAS-BERT. The baseline methods used for comparison covers most of the state-of-the-art methods used for model compression for BERT.\n\nConcerns:\n \n- Can the authors clarify if they save the model parameters corresponding to all the possible architectural choices? Or they find out the best configuration matching the model size and latency requirements and then do the pre-training again with those architectural choices.\n\n- In appendix A.7 the paper demonstrates some of the architectures used by NAS-BERT. For lower model size ( < 20M ) it can be observed that the NAS-BERT ends up choosing SepConv layers most of the times. Do the authors do any analysis on why SepConv layer works better than the self-attention layer and the feed forward network. How does the network perform if it is composed of all SepConv layers? Have the authors tried to use only SepConv layers and see if that itself gives good accuracy rather than doing the architecture search.\n\n- In terms of original ideas, although the concepts of block-wise architecture search, using SepConv layer for NLP tasks and using block-wise knowledge distillation are not novel by themselves but this paper has efficiently made use of the available techniques (along with efficient engineering work like progressively reducing the search space) to develop a method that gives good performance on NLP tasks.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting work on NAS for BERT",
            "review": "Summary:\nThis paper proposes to search architectures of BERT model under various memory and latency contraints. The search algorithm is conducted by pretraining a big supernet that contains the all the sub-network structures, where the optimal models for different requirements are selected from it. Once an architecture is found, it is re-trained through pretraining-finetuning or two-stage distillation for each specific task. Several approaches (block-wise training and search, progressive shrinking, performance approximation) are proposed to improve the search efficiency. Experiments on GLUE benchmark shows the models found by proposed methods can achieve better accuracy than some of the previous compressed BERT models. The paper (together with the appendix) is clearly presented, and the idea is new and interesting to me. The experiments are detailed and comprehensive.\n\nPros:\nThe paper is well presented. The architecture of the superent and the candidate operations are carefully designed and selected. It seems that the SpeConv operation is particularly effective when the model size is small. The search algorithm including the block-wise training, progressive shrinking can remove less-optimal structures quickly and significantly reduce the search space. The performance of NAS-BERT models are generally better than those of the compressed BERT models with similar model size, although the comparisons may not be completely fair.\n\nConcerns:\n1. The organization of the paper can be further improved. The paper may not be easy to follow if the appendix is skipped, especially for the readers who are not familiar with NAS or related work. Many of the important information can only be found in appendix.\n2. The novelty of the paper is unclear to me. Although this work may be new on search BERT-like language model, it seems many of the ideas such as block-wise search and distillation are borrowed from existing work. Please the author clarify the main novelties and technical contribution of this work, especially to the field of neural architecture search or more broadly, AutoML . Moreover, some of the proposed techniques such as progressive shrinking are merely empirical practices and are lack of theory or insight showing how accurate the approximation would be.\n3. It is usually more illustrative (and also space saving) to plot accuracy versus latency/#parameters of different models in the same figure. Some of the well noted models such as MobileBERT and TinyBERT are not included in comparison. For DynaBERT, there are multiple configurations but only one is included. AdaBERT, which adopts NAS for each specific task, should also be included if possible. Again, since there are of many models with different size and latency, it may be better to have a plot for clear comparison. \n4. HAT (Wang et al. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. ACL 2020.) is not mentioned in the paper, which share similarities (training supernet) and differences (search algorithm) with this work from technical point of view. It will be better if the author can explain and compare the proposed search algorithm to evolutionary search.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper on applying NAS to pre-training tasks",
            "review": "This paper presents an effective NAS method for pre-trained language models at the pre-training stage, so the selected models can be applied to various downstream tasks with fine-tuning. To achieve better performances the widely used two-stage distillation and data augmentation are applied to the selected models from super-net.  \n\nThe main contribution of this work lies in the designed search space and the proposed three strategies (block-wise search, progressive shrinking and performance approximation) for improving search efficiency and accuracy. Although the novelty of this work is quite limited, training a big supernet for BERT at the pre-training stage is not trivial, which is useful for industry applications. The authors evaluate their approach on GLUE datasets and compare it to other state-of-the-art models.\n\nThe paper is well-written and organized, the experiments are thorough. However, I have several concerns:\n\n1)  In the Table 1 under the KD setting, “two-stage distillation” is conducted on the selected models from supernet to further improve the performances, it would be interesting to add another two settings: a) only conducting the distillation at the pre-training stage, b) continuing to pre-training on large scale unlabeled data to finally obtain better task-agnostic models. \n\n2) The models are evaluated on the GLUE dataset, more experiments on challenging QA tasks should be added. \n\n3) In the Table 1, the comparison to MobileBERT and TinyBERT should be added, and the FLOPs or the inference time on CPU/GPU can be provided.  \n\n4) Some important related work should be included, [1] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing (although this work focuses on the machine translation task) [2] Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}