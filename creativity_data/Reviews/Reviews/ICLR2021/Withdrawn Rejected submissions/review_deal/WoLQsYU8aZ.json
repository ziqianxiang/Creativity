{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper represents the PettingZoo library of multi-agent environments, providing a common API and benchmark for multi-agent learning. The library has high potential for impact and is likely of interest to a wide range of people in the ICLR community. However, in its current form the paper could be significantly improved by actioning the many pieces of constructive feedback provided by all reviewers.\n\nWe have also been made aware of two highly related papers \"Multiplayer Support for the Arcade Learning Environment\" and \"SuperSuit: Simple Microwrappers for Reinforcement Learning Environments.\" Together all three papers could be one comprehensive manuscript, but appear to have been unnecessarily split into three separate short papers."
    },
    "Reviews": [
        {
            "title": "Yet another testing framework for RL",
            "review": "The paper introduces yet another testing environment for RL, PettingZoo, as a logic evolution of OpenAI but, this time, for multi-agent RL systems, a feature not supported by the former.\n\nReasons to Accept:\n•\tVery nice engineering effort. The platform includes popular environments in a user-friendly way as well as detailed documentation and baselines for comparison.\nReasons to reject:\n•\tContents: I am confused by the nature of the work and assume it is meant as a demo. Actually, the paper reads like a product/company brochure, not as a scientific paper. This is mostly due PettingZoo is a great engineering effort (involving the use/incorporation of SW/packages from outside) with little science behind. Sections 2 to 4 gives an abstract overview of the main mechanisms in the API, environments and the interaction with the user, but all basically at the level of interaction at code level. The paper itself is not interesting from a research point of view, for me, because it does not provide any AI-like content apart from summarising the environments/APIs. I think sections 2-4 could have been compressed to one section introducing PettingZoo, after which an actual research contribution using the system could follow, to create a proper research paper. Further explanations, discussions and insights are thus needed (e.g., further comparisons with SOTA MARL approaches/models for different environments).\n•\tRelevance/Novelty: I also wonder whether the paper really represents a significant advance in the AI field, and I guess its relevance may be below the ECLR threshold. Even though I personally don’t see any particularly novel insight in this paper (it is a logic incremental evolution of OpenAI's Gym), the software coding/integration methodology followed is not wrong and the whole environment seem promising and may lead a mass proliferation of MARL research. And I happy to let the noisy process of science (and reviewing process) figure out the value here. I am fully open to change the score if the authors and the rest of reviewers convince me of the usefulness and relevancy of the contribution.\n•\tSuitability: Finally, I like the whole system (I actually think it is great and has great potential to be used as testbed for MARL system), but I feel this could have been better put forward at a dedicated workshop, as an overview, as an unpublished introduction paper, or as a white paper or technical report to briefly describe a system.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting contribution, could use some more detail",
            "review": "Summary\n---\n\nThe paper describes a new framework, \"PettingZoo\", which is proposed to play a similar role for Multi-Agent RL research as OpenAI's Gym framework does for single-agent RL research. The paper describes various lessons learned from Gym and other frameworks, and how these were taken into account in PettingZoo's design and implementation.\n\nStrong Points\n---\n\n1) These types of frameworks for standardised, easy-to-use, and varied benchmarks are important contributions.\n2) Framework looks easy to use, familiar API for Gym users.\n3) Paper well-written, easy to follow.\n4) Nice detailed listing of all hyperparameters in Appendix for the included baseline results.\n\nWeak Points\n---\n\nThe paper could use a more thorough review of existing work and a more detailed comparison to make a stronger case for this framework satisfying a need in the research community that is not already satisfied. The primary point seems to be the different \"Agent Environment Cycle\" model from (Terry et al., 2020b). Since this is from a very new paper (this year), not widely-known already (to the best of my knowledge), and seemingly a primary core point of this paper's contribution, it would probably be useful to elaborate on exactly what this model is and how it's different from others a bit more. There is a very short description, but from that one I can't see how it's any different from Extensive Normal Form games (with imperfect information). If it is indeed very similar / identical to the standard Extensive Normal Form games model, it would probably also be worthwhile to discuss other frameworks that already use these kinds of models in related work. Especially OpenSpiel comes to mind, since it has a wide variety of Multi-Agent RL algorithms built-in (and many different kinds of games), but possibly even other general game playing systems may be worth comparing to (such as Polygames, Ludii, GDL, etc.). Many of these have different focuses, but may still be worth comparing to them.\n\nOverall Recommendation\n---\n\nCurrently I'm leaning towards marginally above the acceptance threshold, because I do see potential and feel like the contribution is interesting and there's nothing really \"wrong\" with the paper. More details (see above) could definitely be useful to add though.\n\nQuestions for Authors\n---\n\nCould you elaborate on how this Agent Environment Cycle games model is different from Extensive Normal Form games? I understand that this is not the contribution of this paper per se, but it does seem to be a core motivation point for why this new framework is better than existing ones?\n\nMinor Comments\n---\n\n- In abstract: \"wrapper\" --> \"wrappers for\" or \"wrapping\" (I guess)\n- Variable names should be kept consistent between python code in Figures 1 and 2. Now Figure 1 has \"obs\" but Figure 2 has \"observation\". Figure 2 also explicitly assigns the name \"agent\" to the variable it loops over, but this variable isn't used, so can just stick to \"_\" like in Figure 1 I suppose?\n- Section 4 starts naming a bunch of acronyms (MPE, MAgent, SISL), but I don't really know what any of those mean.\n- Inconsistent formatting for the \"Classic\" paragraph in Section 4 (text starts immediately after paragraph header instead of below it)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Overall an ok paper with an important contribution to the ecosystem of MARL frameworks, let down by focus on the wrong areas.",
            "review": "### Quality\nThe paper is overall a good quality but it has some deficiencies. The authors need to do a better job comparing PettingZoo to other alternatives. What's the selling point?\n\n### Clarity\nThe paper was straight forward and easy to read.\n\n### Originality\nWhile obviously taking inspiration from OpenAI Gym, others have also thought about MARL. However the drivers which led to the development of PettingZoo are original with a focus on quality of life improvements and configurability. These are all positives.\n\n### Significance\nThere is an obvious interest in MARL approaches and hence PettingZoo can be a potentially significant contribution\n\n## Comments on specific sections\n\n#### Abstract: \n80% of the abstract is talking about OpenAI Gym, not about PettingZoo. The last sentence then mentions PettingZoo almost in passing. I recommend you start with Petting Zoo and say that it was inspired by OpenAI Gym and focus your abstract on what your paper is about - Petting Zoo.\n\n#### Introduction\nI have a similar issue with the introduction. You spend a lot of time extolling the virtues of OpenAI Gym, which is great because I can see that Petting Zoo has taken inspiration from Gym, however this could be put into a background or related work section.\nYour introduction should focus on introducing PettingZoo, describing the scope of the paper, an overview of the contributions made and giving the reader an overview of what is coming in the rest of the paper.\n\n#### Background\nAs mentioned above, the paper is missing a dedicated background or related work. You mention RLlib as an alternative multi-agent RL library but you don't go into detail. How about other MARL libraries/frameworks? What is the difference between PettingZoo and these? Advantages/Disadvantages of other systems? Why not contribute to OpenAI Gym?\n\n\n#### Design Philosophy\nI liked the design philosophy described. This list can also be thought of as motivation or key development drivers. This potentially could be your development methodology. However what is missing is evaluation criteria. Once you've developed PettingZoo what are the measurable qualitative or quantitative criteria that you will/can use to evaluate if you have done a good job.\n\n#### API\nI feel you should have gone into more detail into your MARL API. I had a look at the code and there is a bit more of the API which would be good to talk about especially the parallel API. A design or block diagram showing the architecture of \n\n#### Environments\nIts good to see a comprehensive set of environments available with the PettingZoo library.\n\n#### Documentation: \nWhile this is good you probably don't need to spend 3/4 of a page on it.\n\n#### Baselines: \nThe baselines are good, but it would be nice to compare them to other MARL libraries as well.\n\n### Other Comments\nThe paper is missing a discussion/evaluation section where the authors critically evaluate the PettingZoo library.With some of the changes suggested the paper will be much stronger.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful library for interacting with multi-agent environments",
            "review": "Thanks for time and effort on writing the library.\n\nThe paper introduces an API and library for multi-agent reinforcement learning along with simple installation of a very diverse set of environments along. Each environment has clear documentation of inputs/outputs, etc. \n\nPros:\n- The majority of the paper is clear and easy to understand\n- Significant effort on the framework \n- Clean API\n- Integration of very diverse set of environments\n- The documentation of individual environments is great\n\nCons:\n- Abstract is very weak. OpenAI Gym is overemphasized, and PettingZoo is underemphasized. Make sure it highlights why PettingZoo is great\n- Baselines are weak. Ape-X is far from a good baseline in my opinion. There are simpler more data efficient agents available and also policy gradient based agents. (I realize that these are not the main points of the paper.)\n- The paper states it has been shown that AECs are equivalent to POSGs but there is no reference. I assume this is just executing the individual actions of the agents and then getting the observations of all of them instead of interleaving getting observations. Wrt. implementation, a POSG environment used as AEC's may be in invalid states when not all agents have executed an action?\n- Paper could be polished more in general.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}