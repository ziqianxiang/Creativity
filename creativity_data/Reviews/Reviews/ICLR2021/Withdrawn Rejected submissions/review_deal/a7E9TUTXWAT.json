{
    "Decision": "",
    "Reviews": [
        {
            "title": "A method for embedding into Minkowski Space-Time. Nice idea, experimental setup might need some rework",
            "review": "This paper presents a novel method for learning causal representations.\nThe authors propose to embed data into a Minkowski Space-Time, in which they describe a succession of events being causal if each event is in the intersection of the light cones of all previous events.\nThey demonstrate the properties of the Minkowski Space-Time by training a VAE in this space. They show that as they move away on the time axis, the samples become more diverse. They also show that they can use light cones in the embedding space to constrain the generated samples to be causally plausible.\n\n\n################################################\n\nStrong points:\n\n- Non-euclidian embedding is a topic of interest and can be very useful to inject prior knowledge.\n\n- The method is grounded in theory and might be interesting for embedding spatio-temporal data.\n\n\nWeaknesses:\n\n- Using a Minkowski space-time for representation learning is not entirely novel and have been used before (in a work cited by the authors).\n\n- It is difficult to draw any conclusion from the presented results. \nThe authors state the transformations observed in Figure 3 are \"probable\" without having explained what is to be considered probable and what wouldn't be. For instance, on Fig. 3b, 3rd row, the model adds a shadow that wasn't present before, is that still acceptable or not? \nAlso, the causality relationship in the presented sequences is only post-hoc interpretations.\nActually, I believe it would be possible to present very similar results simply using a VAE in euclidian space.\n\n\n################################################\n\nScore motivation:\n\nI believe the paper is not ready for publication. Mostly, the experiments lack proper framing. It would help if the paper was able to demonstrate with less room for subjectivity that it works as intended. It should also provide a comparison with other methods, that works in euclidian space for instance.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The manuscript needs significant revisions",
            "review": "The paper addresses the problem of causal future prediction by embedding spatio-temporal information on a Minkowski spacetime. The authors propose to model the latent variables and time of sequential problem as coordinates in a pseudo-Riemannian manifold, in order to utilize the light cone idea in special relativity theory, which naturally introduces a causal dependency between subsequent samples.  By constraining the samples with the light cones of previous samples, they are able to ensure that the subsequent latent representations are not distant.\n\nThe problem of finding causal and physically meaningful representations will certainly be of interest to the ICLR community. In my opinion, incorporating ideas from theoretical physics into machine learning  is a promising venue for causality research. \n\nOn the other hand, the manuscript needs some significant revisions. First of all, although there are certain analogies between the problem and special relativity, the connections established between them are very loose and they weren’t discussed thoroughly. For example, the question of why the latent variables should obey relativistic laws is left unanswered. If they do so, why does the method proposed by the authors need to use all the light cones from previous observations, while in special relativity previous observations are Markovian, i.e. the intersection of all the previous light cones should be equal to the last light cone. \n\nAlso, the merit of the proposed methodology is largely overshadowed by excessive technical details. For instance, the discussion about the proper time definition (which is written wrong in the paper due to omission of Minkowski metric tensor) doesn’t seem crucial. I believe that the special relativity formalism doesn’t need to be overly emphasized, while its only use seems to be constraining the distance between two subsequent representations in a sequential model. Additionally, the pseudo code of the proposed intersecting light cones algorithm is not clear, apart from the soundness of using intersection of light cones in a Markovian system. What are the outputs of the specific subroutines in the algorithm, how does $t$ relate to the number of frames, what is the value of $f_t$ when $t$ is bigger than the number of frames? I feel that the paper could use some restructuring and partial rewriting to improve clarity, as the reviewer required several passes to understand the main arguments of the paper.\n\nAs for the experiments section, although the preliminary results in the paper are interesting, they are only toy examples and they are not sufficient to convince someone of the correctness of claims in the paper. Main problem of this section is that the authors neither compare their method with alternative methods nor present any quantitative metric to be able to evaluate the performance of their approach. In a paper claiming the superiority of causally meaningful latent representations, it is crucial to see how the Minkowski spacetime assumption affects the overall performance when compared to the methods having different regularization policies on the latent space or the methods modelling the same problem explicitly as sequential to utilize temporal information, rather than regularizing the subsequent latent representations. Likewise, authors mention strong theoretical guarantees of their framework in the abstract, but they don’t provide neither theoretical nor empirical guarantees about their approach.\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for Paper3294",
            "review": "The paper proposes to consider light cones in the space-time domain for causal inference. The author argues that, at a given time and space point, all possible events should lie inside the cones, and for several events causally related points should be in the intersection of the cones defined by the event.\n\n1. This intuition makes sense for the real world events, where the event is specified as a 4D vector (3D location + time). Although in a latent space which is not directly related to the real space and time, it may not be reasonable to enforce the light cone constraint. This inherent difference between the real and virtual space is not discussed in the paper.\n\n2. Moreover the images in a (real) video are the projections of real world scenes. The points that are far apart in the real world can be mapped next to each other, thus one should be very careful when applying the light cone constraint in the image domain - in my thoughts the proposed constraint seems not valid in the projected images. If the images are treated as 2D arrays of pixel values (i.e., not directly related to the real scenes), it is argued in the previous paragraph (1.)\n\n3. In the real world the speed of light determines the light cone. However in an abstract space there is no such real constraints for causality. This is also noted by the authors (in \"On the Entropy and the Aperture of Cones\") and they chose to enforce an ad-hoc 45-degree cone constraint. There is not any meaningful justification of this decision.\n\n4. There is no experimental comparison with previous works or other algorithms. The experiments (MNIST and KTH) presented in the papers are also not realistic or complex enough to show the benefit of the proposed algorithm. It is even not certain whether the proposed algorithm is generating reasonable results from the presented frames. At least, to show that the proposed cone constraint is really working, the authors could show how the unlikely frames is penalized by the proposed algorithm (not just showing \"No cones\" images - it can be arbitrarily bad) and how it helps to better predict the likely frames.\n\nIn summary I think the technical correctness is in question and experimental evaluation is not sufficient.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but weak link with Pearl/Rubin causality and improvable formalism",
            "review": "This paper suggests the possibility of performing causal prediction by learning latent representations in an embedding space endowed with a Minkowski time-space structure. Prediction relies on sampling from a subspace bounded by the analogue of a light cone (or an intersection of light cones).\n\nI find very interesting the idea of modelling the latent space with a structure that would explicitly encode time information. However, I think the presentation is in some points unconvincing. This may well be due to my misunderstanding, and I'd be happy to be corrected.\n\nFirst of all, I am not certain of the connection between the notion causality in this work and in the Pearl-Rubin meaning, as stated in the theoretical foundation and in the final discussion. It seems to me that the light cone definition of causality defines a sort of bound on the reach of causality, while Pearl-Rubin defines causality in terms of interventions/counterfactuals. I don't see, for instance, how light cones would allow us to understand causes as opposed to confounders. The suggestion in the last paragraph of the conclusion suggesting that light cones may be used to generate counterfactuals seems to me unfounded, as proper counterfactuals would need a structural causal model to be generated, which I can not see being learned in the presented model.\n\nI also had quite hard time following part of the theoretical formulation. This is certainly due in part to my limited knowledge, but I think some of the ideas introduced may not be familiar to the machine learning community either. What is the causality group \\mathcal{RM}? How is emergence of causality in the system implied by the chosen metric? What sort of causality is it (see above)? The metric referenced on page 3 would probably better be expressed in the conventional form of a function. Also, in Equation (3), it is confusing that d is using as an infinitesimal, and as a summation index.\n\nAlgorithm 1 is very high-level, and it could also be made more precise and informative. Some variables are unspecified (t, k, C_{MF}), and functions like choose() are left undefined until the Result section. The choose() function I find particularly bugging because it clearly has a significant effect on the final generated samples and I can not clearly see how much hard-coded or learned it is.\n\nMinor remarks:\nParenthesize references\nReference to a missing Equation 4 on page 6 ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The authors posit that causal rules are difficult to derive from nature and hence training data. Hence they propose to overcome this by emedding spatio-temporal information on a Minkowski space-time.",
            "review": "# Introduction\n\nYour first sentence is not correct. Methods have been developed for this (e.g. causal discovery coupled with prediction) but they are not widely adopted yet. But the field as stands is not new and was formalised early on by e.g. Pearl and Spirtes. \n\nN is a variable, wrap it in $$.\n\nYour last part of the first paragraph is very interesting, unfortunately, you only spend one sentence on it. It is a fairly large corner-stone of this paper, so it would be good if you can expand upon that.\n\n# Related works\n\nIt is not a particularly strong claim you are making if you are basing it on Arvanitidis et al's claim which shows evidence, but does not prove, that general Riemann manifolds characterise learned latent spaces better than Euclidean spaces. That is a very large claim. You will need more than evidence to back that up, for that to be a faithful sentence.\n\nSub-sections within the related work would be helpful. You are talking about NLP a lot, and spaces, but then causality sits in there somewhere too. The structure of it all is a bit confusing. Consider revising. Furthermore, there is no logical red-thread running through the four paragraphs. You are merely listing various papers and ideas, within each paragraph, that you build your ideas on. That is fine, but difficult for the reader to follow. Hence, please connect them so that there is a story to partake in.\n\nYou use InfoGAN without spelling out what the abbreviation stands for.\n\nSame with 'CNN'.\n\n# Theoretical formulation\n\nI daresay that most ML readers are more familiar with Pearlian understanding of causality through the do-operator for example. It would be helpful to the reader if you give a longer primer on Rubian understanding of causality. There are plenty of self-contained references you can use (the paper you reference, Rubin, is a good start).\n\nYou say that you \"extend Nickel & Kiela (2018)\" -- what precisely are you extending? That is not clear from what follows that quote from your paper. \n\nIt is poor form and imprecise to present an equation (1) and not tell the readers what the parameters mean therein. You've used $\\eta$ but what are $\\mu$ and $\\nu$. We should not have to guess as to their meaning. They are clearly there for a reason, what is it?\n\nThe opening sentence of your 'Minkowski Space-Time and Causality' paragraph is excellent. A short and concise notion is all we need and does not leave any further questions. Well done.\n\nIf you want to space in this paragraph you have $t_0$ on its own line at the bottom of page 3.\n\nIn figure 1, use less \\hfill, indeed replace it with \\hspace{10pt} or something. Figure 1 does not look great. No doubt you tried to save on space, but the layout has suffered instead.\n\nFigure 1(a) is very poor. The reader cannot possibly see the axis labels, the axis ticks or what the different colours are supposed to mean. Even zooming in on the PDF does not help this. This figure is not acceptable for publication. Redo it. Much too sloppy. The same goes for figure 1(b). These are meant to be visual aids, as you call them, for your algorithm, but they how are we meant to relate to them at this stage if you have not yet shown us the algorithm. Revise this entire paragraph and put it in context with your contribution.\n\nAt the bottom of page four you say 'Following the causality argument' -- what is this? I do not follow at all.\n\nOkay so your algorithm arrives on page five but you present your visual aids before the algorithm on page four. Do this instead. Put all of that on one page to allow the reader to go back and forth between the algorithm and the visual aids. Having to switch page for this is very inconvenient if you want the reader to understand your contribution.\n\n## Algorithm 1\n\n-What is space of the frame sequence? $F \\in \\mathbb{?}^{? \\times ?}$\n-What is space of the frame sequence? $T \\in \\mathbb{?}^{? \\times ?}$\n-Give your algorithm line-numbers so that your reviewer (me) can criticise it! No, but really, please give your algorithm line-numbers. It makes it much easier to follow and further down the line; replicate. \n- Is $Mft$ meant to be $M_{ft}$?\n- At this point you've used some very strange notation in your algorithm (Samples) and there are no comments explaining what is happening.\n-Redo this algorithm. It is very poorly formatted.\n\nThe entropy section needs more explanation. It is not clear what you mean. This statement: \"Calculating the thermodynamic entropy of a macro-world system as in a real world dataset is not trivial and we are not aware of any appropriate method to compute this at the time of writing\" seems implausible. The heat pump of an refrigerator comes to mind as a real system, with data, where this calculation is imminently possible.\n\nI am sorry but section three is not a theoretical formulation. You have formulated a method, granted, but there are no proofs or related derivations to guide the reader through your method. There is a rather poor algorithm (it's presentation that is) to go with this section and figures where much of the detail is indistinguishable. Alas, this section leaves a lot to be desired.\n\n# Experimentation\n\nAs not all readers will be well-versed in differential topology, you may want to give a more lay explanation for \"orthochronous diffeomorphism\" as this is a machine learning conference and not a non-linear systems conference (though they are of course both equally interesting). As most ML people do not know about such things as embeddings theorems and the like, a lay definition will aid their understanding of your paper.\n\nAre the equations from the top of page 6 your own? Are they derived from Nickel & Kiela (2018)? Where do they come from? They have no equation number nor any reference. Further, you say \"Minkowski space-time by utilizing Eq. 4 and 4.\" -- there is no equation four and the four following the four is a dead hyperlink.\n\nYour training and inference sections should be in section 3, as they are rather crucial for your method to work and more importantly, you have made design choices therein which are part of your method. Not of the experimentation.\n\n# Results\n\n- Why 1+8 dimensions, did you do ablation studies? Where are they?\n- The sheer amount of data you use to train your algorithm directly contradicts the third sentence of your abstract where you claim that causal rules are difficult to derive from 'finite training sets'. \n- Unfortunately it is very hard to follow your discussion on cones when applied to Moving MNIST, and we appear to have stopped talking about causality altogether? The last time you mention 'causality' is on page 4. We are now on page five and it not clear what any of this has to do with causality. \n- You mention a number of papers in this work, which you build upon, yet you have not compared your method to a single one of them? Why is that? How are we supposed to judge the viability of your results when there is nothing to compare against? You have chosen famous datasets, which have thousands upon thousands of papers which use them, but none of those results are in this paper. Why? And why are these datasets chosen if yours is a paper on causality? Where is the motivation?\n\n# Reviewer concluding remarks\n\nThis is interesting work. Very interesting in fact. However it feels like it was submitted in a rush, most sections are lacking and seem to be missing the core steps and equations necessary to make this a good paper. Further, experiments are lacking comparison too, as well as ablation studies, and the link with causality broke down half-way through the paper (at least by way of reading the paper). Alas, this is unfinished and unpolished work, but it does seem very promising and I thoroughly enjoyed reviewing this paper.\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}