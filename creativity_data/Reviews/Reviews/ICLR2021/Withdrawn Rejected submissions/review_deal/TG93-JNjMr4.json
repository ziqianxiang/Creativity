{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting work but limited novelty and results are not promising. However, data released might be valuable and overall the methodology and problem formulation is nicely implemented.",
            "review": "Summary:\n\nThis paper presents a graph-to-text transformer model to generate navigational instructions from graph-based OpenStreetMap inputs, by conditioning on salient landmarks existing in the graph of the map + supervised by human instructions---a new dataset of navigation instructions to be released with this work. They empirically evaluate the quality of the generated instructions based on their score in comparison to human-generated instructions for the same samples, as well as how well they could be used to lead a human to navigate in Street View, given the model-generated utterance. \n\n\nReason for score:\n\nI thought this paper approached an interesting problem (generate instructions in a navigation domain) based on a graph representation of a street map. However, I think the results are not up to par and moreover, there isn’t a novel modelling approach (graph transformers have been used widely so far)---and I think a different way might lead to improved performance that would benefit this paper. The analysis is also somewhat lacking (e.g., it would be good to see failure cases of models to understand why the performance is so low and what could help overcome this) and also a simple baseline based on an object-oriented trajectory would be helpful to have as well. \n\n\nPositive points + questions:\n\n1. The example/qualitative analysis of generated instructions in the figure looks great! However this does not align with the empirical evaluations that show quite poor performance---it would be good to show a larger range of examples (instead of just one) as well as some analysis of failure cases that lead to poor performance, and in what aspects the model generation suffers.\n\n2. The data released with this paper is valuable for a range of instruction-following tasks. It would be good to have a better analysis of the language and instructions to understand the linguistic phenomena that this dataset includes.\n\n3. It would also be good to compare this to existing datasets in OSM that have instructions and existing navigation instructions in general, to get a sense of how these instructions fare in comparison to others in the domain. Example, these dataset papers have nice comparisons---and should be cited given the similarities! (https://arxiv.org/abs/2010.07954 and https://arxiv.org/pdf/1811.12354.pdf)\n\n4. While the modelling/graph-to-text setup is sound and makes sense, it would be interesting to see if having more state information for each node (e.g., even making it more real and image-based) helps performance.\n\n\nNegative points + questions:\n\n1. How well do other instruction-generation baselines work? For e.g., one possibility is a simple supervised seq2seq that doesn’t take in the graph structure and is supervised by the human instructions in the dataset.\n\n2. ^for example, previous work linked below, attempts to have a generation model that takes in the route/graph of the environment as a sequence of nodes, and attempts to generate natural language instructions by performing a greedy prediction over word tokens, supervised by human instructions. Since all of these components exist here (e.g., graph, route, instruction supervision) this is a natural baseline to implement and compare to.\n\n3. Another important baseline is a model that does have access to a graph structure and is based on a simple grammar (e.g., CFG-like) and the object-oriented trajectory from the graph route. (this would likely have instructions that are rigid/inflexible/less novel words therefore potentially lower BLEU scores, but would have higher instruction-following results for the human experiments) so an analysis along those axes of evaluation metrics would be interesting to see.\n\n4. Missing reference for speaker-listener generation for instructions: https://papers.nips.cc/paper/7592-speaker-follower-models-for-vision-and-language-navigation.pdf",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Making the dataset publicly available is expected.",
            "review": "The paper describes an approach to a natural language instruction system for route navigation. \nThe system encodes a route on a map into a location- and rotation-invariant graph representation, and from the representation, natural language instructions are generated.\nThe main contribution of this paper seems to be two-fold. The authors created a dataset for Natural Language Landmark Navigation Instructions (NLLNI) using crowdsourcing. They developed a method for NLLNI. The performance of the system is evaluated through an experiment.\n\n\nNLLNI task is still an important and challenging task. The authors tackle the problem of combining map-to-graph representation and graph-to-text architecture by preparing their original data. The method itself seems interesting.\n\nHowever, both contributions have some limitations.\n\nThe proposed method is only compared to the reference in evaluation, as table 1 shows. As the authors mentioned, at least we have rule-based methods and other naive methods for NLLNI. The performance of the proposed method should be compared to such a pre-existing method for evaluation. Therefore, it is difficult to judge if the method is working appropriately.\n\nThough the paper exaggerates the importance of the novel dataset, they are not providing it as an open dataset to my understanding. If so, that cannot be the contribution to the community. \n\nIf the authors add several baseline methods to Table 1 and provide the novel dataset as an open dataset, I think this paper becomes a more valuable paper. \n\n---------------------\n\nBecause of the improvement and the clarification in the revised manuscript, I have changed the rating after the rebuttal.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "lacking in dataset comparison and model error analysis",
            "review": "The authors of this paper construct a new dataset for generating route instructions in natural language from maps (OpenStreetMaps) and promises to release the dataset upon publication.  The authors consider road networks to be a graph, and proposes the task of generating natural language landmark route instruction from this graph.  As a baseline for the task, they also propose a deep learning based model that takes a graph and produces route instructions.   \nThe paper is a resource paper and at the same time engineering paper that establish a baseline for a new task.  \n\nPros:\n+ Compared with previous datasets of similar kind, the constructed dataset is a bit larger and based on street maps (instead of virtual office environments).\n+ A sophisticated baseline model, what seems to be a state of the art graph to sequence model, is evaluated in the experiments. \n\nCons:\n- As a resource paper, this paper is a bit light on comparing proposed dataset  with older datasets.  \n- A bit light on dataset analysis.  What were the causes of requiring second try in navigation run task?  \n-  It's not clear what authors did with slightly wrong instructions or ones that mixes up left and right and so on.  \n- There is only one baseline in the experiment.   This reader suspect that a simple hand crafted heuristic can have easily 50% or more SR@25 considering the results in Cercas Curry et al.\n- Graph to sequence model is nice to have,  but it is a complex model with a lot of tuning point.   It also requires more training data and it would be tough to train it with few thousands.  Careful analysis is required to see if it reached its full potential. \n\nPlease take a look at NLP engineering experiment paper and resource paper section of the following site: https://coling2018.org/paper-types/  and answer every question to improve the paper.  \n\nOverall, the paper seems to address everything minimally required, but its treatment of the problem is not thorough.  \nDue to its lack of dataset comparison and error analysis, I vote for marginally below acceptance.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs Improvement",
            "review": "[Summary]\n\nThis paper presents the task of generating landmark-based navigation instructions given a graph representing a path on a map. The authors collect ~8K instructions by having AMT workers annotate maps with censored street names — forcing workers to rely on landmark entities (e.g., starbucks). Their experiments employ a (graph) encoder (text) decoder transformer. Both human- and model-generated instructions are evaluated via BLEU score and human navigation success (collected via AMT).\n\n[Pros]\n\nThis paper is very well written and easy to follow. In particular, documenting how the data were collected and prepared. I thought the figures being used were very effective. The notation used in the modeling section was also very clearly structured, and I didn’t find myself needing to scan back for previously defined notation.\n\nIt’s also great to see human evaluations included in the results!\n\n[Cons]\n\nMotivation:\n\nMy key issue with this paper is that the motivation isn’t very clear to me. The introduction makes three cases: (1) humans efficiently navigate via landmarks in the instructions, (2) inexact GPS tracking and imperfect human distance judgements, and (3) landmarks are more visually salient than street signs — and are thus better for bikers/bus riders.\n\n(1) It’s true that people anchor to visually salient landmarks, however the landmarks present on the map (and thus mentioned by the AMT workers) don’t necessarily correlate to visual saliency.\n\n(2) While inexact GPS tracking breaks distance based instructions, streets and intersections can still form a graph from which instructions can be generated.\n\n(3) See my point on (1).\n\nData and modeling:\n\nThe author mentions that one of the failure modes during annotation is when workers mix up left/right — which you’d expect, as they’re generating egocentric instructions given an allocentric perspective. This again ties back to point (1), which is to say that landmarks from an allocentric perspective may not correspond to what’s salient from an egocentric perspective. And while the model itself is clear, simple, and well-suited for the task, the model’s capacity seems overkill for the relatively small dataset (~8K instructions).\n\nExperiments and evaluation:\n\nI’m very happy that the reviewers included human evaluations for their model predictions — as language generation tasks too often rely on automatic metrics, such as BLEU.\n\nHowever, my biggest concern is that there are no simple baselines to compare the model’s performance to. Given that this paper presents a new task, it would be beneficial to see results for a trivial rule-based systems, like the one used to generate the 20K pretraining data. Likewise, some ablation experiments, such as: corrupting the graph structure, reordering landmarks, only including the final landmark etc.\n\nFinally, for the human evaluations, in addition to reporting Success Rate (SR) and Edit Distance (ED), the authors ought to look into more recent evaluation metrics for navigation, such as Success weighted by Path Length (SPL) and Normalized Dynamic Time Warping (nDTW).\n\n[Clarifying Questions]\n\nDoes ang(u, v) encode an angle in a global coordinate system or relative to the angle of the previous edge?\n\n[Additional Feedback]\n\nN/A",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}