{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a very interesting decomposition of the neural tangent kernel, which promises\nto decouple effects of the parameters and data. The authors illustrate the effects of this decomposition\nby considering pruning strategies for initialization.\nWhile the approach looks promising, the current paper is somewhat premature: The only \"hard\" \ntheoretical result, Theorem 1, is a direct consequence of the decomposition.  Its consequences for \ntraining discussed in the subsequent paragraph involve quite a few approximations, yet the effects \nof these approximations remain unclear. This general, high-level tone is kept when discussing the \ninitializations.  \nFinally, the N(0,1)-response to Reviewer 3 worries me."
    },
    "Reviews": [
        {
            "title": "A novel perspective on explaining the pruning at initialization",
            "review": "This paper proposes the path kernel which decomposes NTK in terms of path kernel. It decouples the NTK into two parts, the data-dependent part, and the data-independent part, which is the Path Kernel.\n\nThe new perspective to explain several recent pruning algorithms by Path Kernel is novel and valuable. The experiment of the proposed variation of  SynFlow also coincides with the theory.\n \nPros: a good theoretical foundation on the proposed algorithm which separates the data-independent part to Path Kernel.\n\nCons:\n1. The experiment's setup is not clear to me,  can I ask what is the formal definition of compression ratio. I thought it might be the original size/ compressed size, however, it can be small than 1 or even 0 in Figure 2 which confused me. \n\n2. Recent researches show that pruning at initialization is not better than random pruning when keeping the non-zero ratio among layers. Could the authors provide a comparison between the proposed algorithm and keep-ratio pruning? Here's the reference.\nhttps://arxiv.org/abs/2009.11094\nhttps://arxiv.org/pdf/2009.08576.pdf",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In this paper, the authors propose a new kernel named Path Kernel to understand deep neural network training. ",
            "review": "In this paper, the authors propose a new kernel named Path Kernel to understand deep neural network training. The key idea is to reparameterize the network with respect to the active path in the network. In this way, they can decompose the Tangent Kernel into data-dependent and architecture-dependent pieces. They authors rewrite the formulations of the existing pruning at initialization methods with respect to their Path Kernel and provide some new understandings. \n\nThe positive aspects:\n\n1. The proposed Path Kernel is novel.\n\n2. Based on Path Kernel, the authors provide some new understandings to the existing pruning methods.\n\n3. This paper is well written and easy to read.\n\nMy concerns are:\n\n1. We know that NTK focus on the small neighbourhood near the initialization. However, in deep neural network training, the weights could be quite far from the initialization. So I am not sure whether it is reasonable to view the existing pruning at initialization methods from the perspective of NTK.\n\n2. I think this paper may be a bit over claimed. The authors provide some new understandings to the existing pruning methods instead of unify them into a single framework.  The authors essentially rewrite the formulations in existing pruning methods with respect to active paths and show that they all have Path Kernel. I think this is not surprising since the network is reparameterized with respect  to the active paths. Such rewriting can be performed on lots of other models besides pruning and we can always show there exists Path Kernel in them. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Path-NTK (Neural Tangent Kernel) for Pruning at Initialization",
            "review": "### Summary\nThis paper is twofold: 1) the authors propose a way of computing a NTK by decomposing a piecewise linear network into paths; 2) the authors propose to use this decomposition to detect the least useful weights at initialization.\n\n### Details\nFor convenience, I will call the theoretical part \"Path-NTK\" in the following.\n\nThe main usable result of Path-NTK is a proxy for computing the speed of convergence of the neural network. This result is based on two approximations: 1) the influence of the eigenvalues $\\nu_i$, related to the data, is neglected; 2) the sum of the eigenvalues $\\lambda_i$ is assumed to reflect the value of each $\\lambda_i$ (i.e., if $\\sum_i \\lambda_i$ is large, then each $\\lambda_i$ is large).\nThe justification of 1 has been made in Annex A.2, but, once again, at the cost of more approximations, that is, inputs are assumed to be N(0, 1), which is not true in MNIST.\nThe justification of 2 is experimental, and has been made in Section 5.1. However, the graphs are only qualitative, and not quantitative. We can see that the larger $\\sum_i \\lambda_i$, the faster the convergence is. But there is no quantitative estimation of the convergence rate, which could have been compared to $\\sum_i \\lambda_i$ in order to justify completely the approximation. And, last but not least, the dataset MNIST is missing in these experiments.\n\nAbout the pruning experiments, I can see in the graphs at the bottom-right of Figure 2 that the proposed method outperforms slightly the precedent ones, but this seems very subtle to me.\n\n### Significance\nPath-NTK is original, and the idea is interesting and probably reusable. However, this is limited to piecewise linear networks, and the only practical result is based on many approximations that have been certainly discussed, but they could be improved.\n\nThe Lottery Ticket Hypothesis is a certainly a hot topic, but the specific topic \"pruning at initialization\" seems to be marginal. Moreover, the overall significance of the results is not encouraging.\n\n### Additional comments\nThe justification of all approximations made in the theoretical part seems to be itself a research topic. \n\nEdit:\n### Rebuttal\nRemark 1: \n * Figures are much clearer;\n * about Figure 1, I see that MNIST has been added, which is good;\n\nRemark 2:\n * \"We can always normalize inputs such that they are distributed as N(0,1)\": this is not true. At best, we can put the mean to 0 and the variance to 1. It does not mean anything in terms of distribution *shape*;\n * I maintain that experiments with MNIST were missing in Figure 1 in the first version of the paper. This is more important than pretended in the rebuttal, because, in the MNIST dataset, the pixels are not N(0, 1) (even when centered and normalized), so MNIST does no meet the approximation made in Annex A.2. However, the results are not too different from those obtained with the other datasets.\n\nThe remark about the limitation to piecewise linear NNs has not been correctly addressed by the authors. The setup used in this paper corresponds to \"Piecewise linear NNs\", which is *really* limiting, since the biases are supposed to be zero. This is very different from \"Piecewise *affine* NNs\", which are actually widely used (biases can be non-zero). Adding the influence of the biases would break the entire computation made in this paper (or, at least, would necessitate more effort to take it into account). I assert that this limitation is not discussed in the paper, and my attempt to discuss it here has been eluded in the rebuttal.\n\nDespite the new and more fashionable figures the authors presented in the rebuttal, I am disappointed by its lack of accuracy and vagueness, especially when discussing the \"piecewise linear NNs\". So, I lower my rating.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Neat NTK perspective, but for what?",
            "review": "__summary.__\nThe paper studies the problem of neural network pruning at initialization through the lens of neural tangent kernels (NTK). As a result, the paper delivers a unified perspective on SNIP, GRASP, and SynFlow. Based on the framework, the paper provides a method to approximate the convergence dynamics of pruned models. The paper also motivates a SynFlow-variant from the theoretical framework.\n\n__overall opinion.__\nI could not find a very big empirical benefit from adopting the proposed theoretical perspective. Given the limited novelty and theoretical significance, I do not think this paper makes enough contribution to be shared at ICLR.\n\n__originalty.__\nAs a matter of fact, I did not find the proposed approach strikingly novel. The fact that the considered pruning methods (which are being unified under the given framework) are all based on the first-order approximations is not very surprising; I would rather say that the fact has been quite well known. The use of first-order approximation itself has been prevalent in the network pruning society, at least since the [optimal brain damage paper](https://papers.nips.cc/paper/250-optimal-brain-damage.pdf) at 1989 and has been used until quite recently, in [layerwise obs](https://arxiv.org/abs/1705.07565) or [lookahead pruning](https://openreview.net/forum?id=ryl3ygHYDB). Also, the combination with the NTK theory for the case of pruning at initialization was already outlined with details in the GRASP paper.\n\n__clarity.__\nThe paper is clearly written.\n\n__significance.__\nI do not see a big merit, especially about the pruning variants. The performance gain of using the proposed variant is nonexistent or marginal, compared to the original SynFlow. The materials presented in section 5.1 seems to be potentially useful, but the paper does not elaborate for what purpose such predictability can be used.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}