{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "We thank the authors (and reviewers) for engaging in a detailed and constructive discussion, and providing a revised version of the paper after the initial round of reviews.\n\nRegarding quality, the work is technically correct and the amount of experiments significant. However, as highlighted by reviewers 2 and 3, some important questions remain unanswered, in particular 1) more empirical evidence to support the claim that the UMAP loss is a relevant for neural networks, and 2) more comparison with existing approaches (beyond t-SNE).\n\nRegarding clarity, the paper is overall clear and pleasant to read. However, after the revision round, all details about the proposed methods have been moved to the annex. While the initial version was criticized for the opposite reason (all experiments were in a annex), the balance may not be found yet; e.g., the equation for the UMAP loss, which is at the core of the paper, would certainly find its place in the main part of the manuscript for an ICLR paper.\n\nThe originality is the weakest aspect of the paper (besides the lack of comparison with related work). As mentioned by several reviewers, plugging the UMAP loss to a differentiable model is nowadays an idea that lacks originality. What would be important to justify that such a \"straightforward\" idea makes it to ICLR would be to demonstrate convincingly that it outperforms existing alternative approaches.\n\nFinally, regarding the significance of the work, it is limited by the lack of thorough comparison with existing method. On the other hand, if the method is implemented in a fast and easy-to-use package, it may find its public as illustrated by the positive evaluation of Reviewer 1 from a potential user point of view."
    },
    "Reviews": [
        {
            "title": "An interesting parametric approach to UMAP.",
            "review": "Response to authors:\n\nThe authors have largely responded well to my original concerns. However, after reading through the discussions with other reviewers, I agree with reviewer 2 that more work is required to make this publishable. In particular, this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function. Given this, I have downgraded my score accordingly.\n\nOriginal Review:\nThis paper presents a parametric approach for UMAP, a dimensionality reduction method. This area is of interest to the community as dimensionality reduction can be useful in a lot of different tasks such as visualization, semisupervised learning, etc. If my comments below can be addressed, I would be willing to increase my score.\n\nPros\nThe parametric version presented here appears to work well in the experiments given. The incorporation of the UMAP loss directly in a neural network as a regularization is also interesting.\n\nCons\nSome of the results don't appear to have a corresponding figure or table, e.g. \"Reconstruction accuracy\" in section 3.3. These should be included.\n\nUMAP tends to inherit some of the weaknesses of t-SNE as it tends to overemphasize local structure at the expense of global structure. In particular, it's been shown [R1] that UMAP and t-SNE are basically equivalent when using the same initialization. Could similar results be obtained by using the same initialization as UMAP instead of including the regularization term? \n\nUMAP is traditionally used for visualization. Some of the applications presented (e.g. SSL) require/would benefit from higher dimensions than 2 or 3. t-SNE is known to be considerably slower for higher dimensions. Does UMAP inherit this problem? If so, that should be mentioned as a potential drawback.\n\nAll of the figures are given in the appendix. While this allows for more results, I think it would be a better paper if some of the figures were included in the main paper and some results were moved to the appendix.\n\nThe authors should verify that their references are as up to date as possible. For example, the PHATE paper should be updated to the Nature Biotechnology version (not bioRxiv).\n\n[R1] Kobak and Linderman, https://www.biorxiv.org/content/10.1101/2019.12.19.877522v1\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Rather naïve approach lacking sufficient justification and comparisons",
            "review": "**Update following discussion:**\n\nFollowing the revision by the authors and the discussion with them, I am updating my score from 3 (Clear rejection) to 4 (OK, but not good enough - rejection). This reflects in great part the revision the authors made to have the main paper (limited to 8 pages) be self contain and present their main results, while using the appendices for complementary and technical information.\n\nHowever, I still maintain the paper is not ready for publication in its current form. The extension of UMAP to implement the optimization via a neural network applied to input data rather than directly assigning coordinates is rather straightforward. The advantages it provides over UMAP in terms of natural inference on new data without the need for separate (more computationally intensive) out of sample extension method are a direct result of this neural network implementation, and they would be true not only for UMAP, but in fact for any method implemented in a \"parametric\" way via a neural network compared to nonparametric coordinate assignment. Similarly, allowing the addition of reconstruction or classification objectives in training is clearly a direct byproduct of this neural network implementation as well, and not unique to UMAP.\n\nTherefore, an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks, for example, to enable visualization or improve various tasks. The authors already look into this to some extent by comparing to parametric tSNE as one alternative approach, but there are many others, as I mention in the initial review, relying on constructions from topological data analysis and manifold learning - most, if not all, of which rely on some graph construction on the data and then ensuring the coordinates provided by a hidden layer in the network match the relations encoded in the graph, similar to the proposed UMAP loss term. How are reconstruction and classification affected by using such other regularizations compared to the UMAP one? Is inference speed the same for these other approaches? How does training speed compare between them? One can clearly expect some tradeoff between such properties and the geometric information encoded by different methods (UMAP and tSNE emphasize clusters, while other methods may emphasize other patterns), but this should be discussed and demonstrated clearly rather than just ignoring the vast amount of related work on parametric approaches to capturing intrinsic geometry in data.\n\nNow, beyond the described lack of relevant comparisons for autoencoding and semi-supervised classification, even simply as a parametric implementation of UMAP (which would be a rather narrow scope, which is not very enticing as a motivation on its own),  I am not sure this work is sufficient to establish the presented approach. First, for the inference or embedding speed - this is essentially and out of sample extension task. As such, even if one insists on only comparing to UMAP-based methods, there are multiple OOS methods that can be used, such as Nystrom, geometric harmonics, etc. Some analysis of the tradeoff between extension quality and speed seems warranted here, but as I said previously - I think a comparison should also be provided to other parametric embedding methods beyond just OOS of UMAP (and tSNE for that matter). Second, as the authors clarified in discussion - their approach relies on the suitability of the UMAP loss to be incorporated directly in the network optimization, essentially comparing activations to the UMAP graph. However, an alternative approach presented in related work is to provide a loss term between activations and a UMAP embedding. This second approach is more general, since other embeddings can also be considered there, but also probably has some disadvantages (for example, the a priori fixed dimensionality, as the authors suggest). The differences between these two approaches should be addressed better in the manuscript, and importantly, since previous work exists already on the embedding loss approach, the authors should present a comparison establishing the benefits of the graph-based loss one, in addition to discussion regarding them. \n\nTo conclude, the idea behind this work seems reasonable, albeit rather straightforward since it's a reimplementation of the UMAP optimization. However, as it currently stands, I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones. I would like to encourage the authors to invest the time in adding such comparisons and clarifying not only how they are different from other methods, but also how they are better, and why choose UMAP to begin with as the basis for their proposed loss terms (compared to various other approaches - not just tSNE).\n\n---\n\n**Initial review:**\nBefore getting into the details of this work, I note that in my opinion it should have been desk rejected for violating the page limit base on the way it is written. The main 8 pages of the paper are far from being self contained, and regularly reference materials from the appendix as integral parts that are crucial for the presentation and understanding here. These include not only methodological illustrations, but also all results establishing the method. In fact, the main paper here does not show ANY result - it only describes the setting for getting them. ALL the figures and tables showing results appear solely in the appendix. If we are to ignore the appendices and only judge the paper based on these main eight pages, then there is no support, no results, and very little in the way of presenting the method here. If, on the other hand, we include the result figures as integral parts of the main paper (as they should be), then it clearly has significantly more than eight pages. Considering most papers submitted to this conference do  try to provide a coherent and relatively self-contained presentation of their work within the page limit, according to the guidelines of the conference, while only leaving technical and supporting details to the appendix, I believe it would be inappropriate to consider this work as meeting the conference page limit.\n\nAs for the work itself, this paper presents a rather naïve attempt to combine together the UMAP visualization with deep learning. It essentially proposes to consider the coordinates optimized by UMAP as resulting from a neural network applied to input data. Then, instead of adjusting directly these coordinates via the UMAP optimization, the method here continues to backpropagate the coordinate optimization through the network to provide a parametric model, via a feed forward neural network, that embeds the data in low dimensions while preserving the local neighborhood structure in the same sense that UMAP, tSNE and LargeVis do with their nonparametric approach. This neural network formulation can also naturally be extended to consider other loss terms, such as reconstruction loss of autoencoders or any predictive loss (classification, regression, etc.) enabling supervised visualization. \n\nFrom a methodological perspective, this is a pretty straight-forward extension of the UMAP optimization, and does not indicate a clear advantage over it for the main task of unsupervised visualization or dimensionality reduction, neither in embedding quality or scalability. The authors show some interesting results (albeit only in the appendix and not in the main paper) on supervised visualization and out of sample extension speed, but these are not compared to relevant baselines that directly aim to address these tasks. Moreover, there is significant related work that is either ignored by the authors, or just mentioned in passing in the appendix without providing proper discussion and comparison with the proposed method. For example, in A.4, the authors mention topological autoencoders, connectivity-optimized representation learning, SCVIS, VAE-SNE, geometry regularized autoencoders, IVIS, and Differential Embedding Networks, but they do not compare their work to any of these, even though such comparison seems highly relevant here. More work that is completely ignored here includes, for example, Diffusion Nets (Mishne et al., 2015), Laplacian Autoencoders (Jia et al., 2015), DIMAL (Pai et al., 2019). Finally, briefly looking at Duque et al. (2020) cited here, while the main method there uses PHATE coordinates to regularize autoencoders, it seem they have also proposed the incorporation of UMAP loss terms in autoencoders, albeit only mentioned as somewhat of a sidenote together with tSNE regularization in their appendix. A discussion about the difference between these two approaches should be added to the main paper here, and it seems some comparison between them should also be presented to establish the advantages of the proposed approach here. Hence, even without the page limit argument, it does not seem the work presented here reaches the ICLR acceptance threshold without major revision to its presentation, discussion, and results. I must therefore recommend its rejection at this stage.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "non-convincing results of the experiments",
            "review": "The authors propose a parametric version of UMAP by replacing sampling embeddings in the optimization of UMAP with directly learning weights of a neural network. The paper is very well and clearly written, but I have several significant concerns:\n\n1. I don't see significant methodological novelty. Replacing embeddings with neural networks learning seems to be quite basic and straightforward. It is certainly a cherry on top of original UMAP, but I am not sure it could be counted as a separate method. The simplicity of methodology could be neglected, if the authors demonstrated significant improvement in their experiments, especially on downstream tasks.\n2. A large part of the experiments is devoted to the comparison with tSNE, however it is not very clear why there is a lack of comparison with other parametric methods, such as Topological Autoencoders. Also not very clear why the authors mention these very relevant methods only in Appendix and not in the introduction in the beginning. \n3. The performance of parametric UMAP achieves similar results to non-parametric UMAP, which is certainly nice, but also quite expected. Therefore, I would consider applications of parametric UMAP to other downstream tasks as a more significant and interesting contribution. However, experiments on this part are not convincing at all (especially on CIFAR10 dataset). Would be interesting to see the performance on some other datasets. Also, it would be very interesting to see confidence intervals for Figures 15, 16, 18.\n4. In terms of speed I also don't see an improvement compared to non-parametric UMAP (TF). I see clear improvement compared to UMAP-learn version, but this as far as I understood due to a different implementation of the original UMAP and not in particular novelty of this paper. \n\nAfter authors' response to revisions, I reconsidered my evaluation and updated the score.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A great paper",
            "review": "In the manuscript, the authors introduce a parametric version of UMAP, replacing the original embedding optimization step with a deep learning solution detecting a parametric relationship between data and embedding. The novel approach compares favourably with the standard algorithm and, as a major contribution, defines a loss function that can be employed for other important applications such as constraining the latent distribution of autoencoders, and improving classifier accuracy for semi-supervised learning. \n\nThe paper is well written, complete and thoroughly detailed, both in the theoretical and the experimental section. The introduced material represents a significant advancement in the field, becoming a valuable resource for researchers in several areas. \n\nA couple of notes:\n- An application to one or more large real world dataset (e.g. single-cell sequencing, or weather radar data) would strengthen even more the authors’ claims and the paper’s impact, so I would suggest to include it, at least in the Appendix.\n- Fig.3 in the Appendix is extremely useful to graphically explain the algorithm to a broader audience - I understand the page length limit, but I would strongly recommend to fit it in the main text.\n- I would also suggest to include (maybe in the Appendix) a kind of “how-to” fully worked example to help researchers in optimising the use of novel algorithm in a data exploration pipeline\n- I would point out (within the limitation of the anonimity requirement) the availability of the code for the algorithm\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}