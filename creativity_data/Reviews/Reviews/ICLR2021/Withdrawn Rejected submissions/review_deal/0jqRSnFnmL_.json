{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting and promising approach, but impossible to interpret the results without additional information",
            "review": "The paper presents a method for learning DAGs from data within the search and score setting. The approach is reinforcement learning based and uses deep Q learning to decide the optimal order to add edges including when to stop adding further edges. The results show it outperforming other methods, but the fact that timing information is missing from the results makes it hard to know whether this is a real effect.\n\nThere is a good amount of previous work in the area cited. By necessity, this is rather brief. A previous reinforcement learning approach is explained to an adequate degree, with the issues with this method being stated and hence motivating a novel approach.\n\nThe approach using BIC as a score function. This is an entirely reasonable choice but it is not clear whether the proposed approach is specific to this score or whether it would work for others. Obviously some changes would be needed in the MDP reward function, but would it be feasible to compute other score functions in this way? If so, would it work for arbitrary scoring functions, or is local decomposability necessary? Some clarification in the text would be useful.\n\nThe algorithm seems to control the trade-off between exploration and exploitation in an odd way. It uses both epsilon greedy and a UCB-inspired score in action selection. It isn't clear to me why the epsilon greedy step is needed - the net effect is that sometimes it is exploring wholly at random and the rest of the time it is exploring in the least regretful way. Why not just pick actions using the UCB score and control the degree of randomness through the c parameter in that formula?\n\nSection 5, page 6 - it states that results might include \"nonexistent edges\". What does nonexistent mean here?\n\nThe techniques evaluated against seem sensible and the benchmarks seem reasonable. Contrary to the claim made in the paper, the settings of the algorithms do not appear to be given in the supplementary information.\n\nAn important omission in the results section is that the authors do not seem to provide any timing information, which makes it hard to interpret the accuracy results which are given. Most (or maybe all) of the mentioned algorithms would give increasingly good results if they were set to run for longer. As the most extreme case, an entirely random search would always find the optimal network if allowed to run for long enough. Without this information, it is hard to see whether the new technique is better than the previous ones.\n\nThe tables of results are rather hard to read - I would recommend bold or italics being used to draw attention to key results (e.g. the best in each case). It isn't clear what the numbers in the tables actually represent - e.g. means and standard deviation, medians and interquartile range?\n\nThe paper ends extremely abruptly. There are no conclusions (nor discussion or future work). At the very least, I would expect to see conclusions fairly state what could be determined from the paper and what the current limitations of the approach are.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Improvement over baselines but I am not convinced about the formulation",
            "review": "The paper proposes a RL formulation to learn DAG structure learning method. It iteratively learns to add edges to the DAG til convergence, using Q-learning. On numerical experiments, the proposed methods achieves generally better accuracy than existing RL baseline algorithm, as well as some existing DAG structure learning algorithms. \n\nPros: \n- the writing and presentation is clear\n- the formulation on edge learning is different from baseline RL, which I think is well motivated. \n\nCons: \n- The empirical evaluation is not fully convincing. The testing graph are rather small (30 nodes for example), and existing exact DAG learning algorithms could solve those graphs reasonably well. A comparison with exact learning on results and SHD is missing. \n- I have a general concern of using RL approach for DAG learning. RL is known to be not sample sufficient, and the scalability is an issue. RL agent repeatedly samples a trajectory in edge addition space, which likely contains duplicates and no guarantee optimality. One could just choose the best trajectory in the whole action space, and return it as the best DAG. Hence, there is no learning needed at all and just a better sampler. RL, as a sampler, seems wasteful and I'm not sure of its effectiveness in exploring the whole DAG space. It would be interesting for the authors to compare with some other sampling based DAG learning algorithms (which they are plenty, e.g. \"Annealed Importance Sampling for Structure Learning in BN\").  Without it, I'm not sure why RL would be needed, since the setting is limited to one graph and does not generalize to other graphs. \n\nDetailed Comments:\n- is faithfulness assumption necessary in your formulation? I don't see it is used anywhere. \n- What's the performance of these methods without equal variance assumptions?\n- Action space: why is reverse edge or delete edge action not used? Would they improve the performance?\n- \"a pair of nodes (i, j) to make c_t+1(i, j) = 1\" I'm not clear on this point. If c(i,j) = 1, it means there is a path between i and j. How would this result in cyclicity? \n- the action space of a single step at a time is very greedy in nature. Optimality in generality is a concern. \n- I am slightly concerned that the graph encoder takes all data as input in the beginning. No batches?\n- bold the best performing methods in tables \n- on linear datasets, especially with large nodes, the performance of AL is worse than NOTEARS. This brings the question the scalability of the proposed method. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "This paper proposes to use RL as an alternative in the score based learning of DAGs. \nConcretely, the state space is the graph, the action space is adding an edge or keeping the current graph, and the reward is the change in the BIC score. \nTo optimize for the reward, the DQN algorithm is used, where the Q-function is parameterized by a graph convolutional network. \n\nOverall the paper is well written and easy to follow. My main concern is that it seems unnecessarily complicated (DQN + GCN) for a 12 dimensional problem with 256 data points (Table 1). There should be a stronger justification to use a larger model with many more tuning knobs. One might argue that as the problem becomes harder, the model complexity will pay off, however this is not the case when the dimension increased to 30 in Table 2, where simple model NOTEARS seem to perform decently. \n\nOther comments: \n- Did the authors consider \"remove an edge\" in the action space? (Or a separate backward pass like GES.) What prevented using this choice? \n- The experiment results in Table 3 and Table 4 for CAM is surprising. Especially Table 4, where ground truth is GP, the same as CAM's model assumption. It would be great to double-check the results. \n- As the model is substantially complicated, it would be good to include runtime in the main text. \n- How should one choose the horizon T? Is it set as T = d^2, or some number smaller than that? \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incomplete Paper with Several Weaknesses",
            "review": "# Summary:\nThis paper proposes an approach to inducing directed acyclic graph structures from data.\nThe proposed method is an adoption of the well-known Alpha-Zero framework.\nMoreover, it is accompanied by two additional strategies for ensuring correctness and pruning: (i) A greedy algorithm flipping the direction of edges if it helps to maximize the reward and if it does not introduce cycles. (ii) A pruning algorithm removing edges if their regression coefficient is smaller than a threshold value. According to the provided experiment results, the approach seems to work well.\n\n# Pros:\n+ The basic idea of the methods seems to be appealing and the results are promising.\n+ Broad evaluation comparing to several competitors.\n\n# Concerns:\n\n- First of all, the paper seems to be *incomplete*. On page 8, the paper stops abruptly in the middle of the result discussion. I have the impression that there at least parts of the result discussion and the conclusion are missing.\n\n- A lot of publications are referenced in the introduction, but I am missing a careful discussion relating and distinguishing the proposed method from the already available methods in the literature. Therefore, the paper is not well put into the context of already existing works.\n\n- The overall organization of the paper and the writing could be improved significantly. Sometimes notations are introduced, e.g., on page 3 in the paragraph \"Action\", the origin and destination node are dubbed v_{at}^{(o)} and v_{at}^{(d)} but this is never used again. Sometimes notations are used without prior definition, see for example page 2 introduction of chapter 2 \"problem statement\", the set pa(i) is not defined. Furthermore, the signature of the f_i is not given. Then, the notation is overloading the variable n, which is used for describing the shape of matrix X and as a vector of noise terms n_i. The authors should revise their formalisms and only introduce notations whenever they are needed.\n\n- Equations 4 and 6 are erroneous: One cannot pull the sums into the log. By applying the log law, the sums should become multiplications.\n\n- The results presented in the tables do not match the plots provided in the supplementary material. Somehow RL1 and RL2 appear to be significantly better than listed in the tables. However, maybe I compared the wrong plots and tables.\n\n- Numerical analysis is a somewhat confusing name for the experimental evaluation. I would have expected a mathematical analysis (numerics).\n\n- The authors state that the two additional post-processing strategies ((i) + (ii)) have significant impact on the overall performance. An ablation study for assigning credits to sub-components of the approach would increase the insights into the plain model performance and the impact of the post-processing.\n\n- The AL1 and RL1 models are missing in Example 3. Is there any specific reason for this? Nothing is mentioned in the prose. Why cannot the BIC_1 be used for this example? The only difference is the assumption about the noise terms isn't it?\n\n# Minors:\n- Eq. 3: The B should be bold, as in Eq. 4.\n- What is the definition of R?\n- What are casual relations? => causal relations?\n- searching process/searching algorithm => search process/search algorithm\n- The result tables are pretty hard to read. \n- Scores are usually maximized. This might be confusing as the authors define the problem to minimize the score function.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}