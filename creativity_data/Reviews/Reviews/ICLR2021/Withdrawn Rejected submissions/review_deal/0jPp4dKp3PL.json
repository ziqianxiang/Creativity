{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Most reviewers did not feel that this paper was ready for publication. I thank the authors for answering all the concerns of the reviewers, running new experiments and submitting a revised version, however, this was not not enough to alleviate the reviewers' concerns, notably relating to the handling of the ethical consideration in the writing of the manuscript."
    },
    "Reviews": [
        {
            "title": "Integration vs. Extraction",
            "review": "Summary\n\nThis work proposes the approach of integrating priors into a DNN in the form of Linguistic sub-models that capture characteristics of OG. The authors use the example of the PAN-12 dataset for sexual predators to use information about linguistics behaviour for the grooming phases. The work then goes to highlight the augmentations that are done on baseline DNN models to include these CL characteristics. The authors then go on to show the impact of these augmenations on performance of classification on the PAN-12 dataset.\n\nQuestions\n \n- When reading the descriptions of the linguistic sub-models in the DNN, one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models. and may have learned these representations for normalisation etc. vs the integrated CL knowledge. \n\nCould we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations?\n\nThe above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators, so if we can learn how the DNNs are extracting information, we can better create interpretability models that can be more general for NLP + DNNs.\n\n- The work does well to show the gains we get from including these priors. I think we would be better suited if we also understood in the base models, how much of the priors were learnt.\n\n- Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Combining deep networks and corpus linguistics for online grooming detection",
            "review": "This paper presents an approach to natural language processing which integrates corpus linguistics knowledge within deep neural networks (namely, an LSTM-based architecture with attention). The approach is tailored and evaluated on a specific application, namely online grooming detection.\n\nThe approach is based on (1) the normalization of word embeddings by exploiting word semantics representations and word variants; (2) the decomposition of conversation analysis to identify subgoals, by exploiting online grooming processes (or phases); (3) the use of attention to modulate the input gate of LSTM cells. Another significant contribution of the paper is a novel corpus, which extends a previous one (PAN2012). The proposed neural architecture presents several variants in otder to incorporate linguistic knowledge within the model, and the paper reports about such an ablation study.\n\nIn the experimental evaluation, linguistic knowledge is injected within two base models, namely one based on LSTMs, and the other one on XL-Net. Performance is shown to be improved with respect to the state-of-the-art.\n\nAlthough the considered task is indeed very important, a weak point of the paper is that it considers a single domain, and the method looks tailored to such domain. The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations, but this is not confirmed by the experimental evaluation. For example, what about the process of identification of \"variants\": is such process hand-made? Would it be possible to have more details on such part?\n\n- Pag. 7, \"This may be due to this capturing of language subtleties helping with distinguishing OG conversations...\" -> this sentence should probably be rephrased\n- Pag. 7, \"have same aim\" -> \"have the same aim\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some good ideas, but needs more work",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposes two families of methods for integrating external knowledge\ninto neural networks aimed at classifying instances of online grooming. The\nfirst family focuses on incorporating knowledge of word semantic similarity into\ntheir representations. The second, on different attention mechanisms for\nincorporating knowledge about theoretical stages of online grooming.\n\nThe authors perform a solid amount of experiments to assess the differences\nbetween their suggested methods and baseline models.\n\n\n##########################################################################\n\nReasons for score:\n\nHowever, the way the paper is written and structured make it difficult to\nunderstand. While looking at the results, it is hard to really assess the\ncontribution of each suggested strategy, and how they compare to each other.\n\nFinally, the paper presents some serious conceptual gaps that undermine its\noverall credibility.\n\n##########################################################################\n\nPros:\n\n- Fair amount of experiments for assessing impact of each proposed strategy.\n- The ideas for modeling different word variants could be built upon. I\n  especially like the idea of Elastic Pulling for combining the semantic\n  information of word representations. This idea would be useful to the research\n  community focusing on combining word representations.\n\n\n##########################################################################\n\nCons:\n\n- Some serious conceptual gaps, and wrong claims. .\n- The paper is overall unclear and difficult to understand.\n- The task the paper is addressing is not well specified.\n- The methods are also not clearly explained.\n- Results are difficult to interpret and analyses are lacking.\n- Lack of ethical considerations for a system that could be used in law\n  enforcement. I would have liked to see a more detailed discussion on the\n  societal implications that systems aiding law enforcement could have, and in\n  particular, which measures should be taken for avoiding the prosecution of\n  innocent people by systems like this.\n\n#########################################################################\n\nComments and suggestions for the authors:\n\n- The term \"text normalisation\" is spread throughout the paper, but is never\n  concretely defined, and is not immediately inferable.\n\n- I was not familiar with the \"word semantics representation\" noun phrase. After\n  reading the paper I am pretty sure that you mean \"vector space model\", or word\n  embeddings. Why not use these terms that will probably be more familiar to\n  potential readers?\n\n- In Figure 1 (left), you wrote Embedding; at the right you wrote WSR. Are these\n  equivalent?\n\n- I understand that you are doing classification at the conversation level, but\n  in page 3, in the \"Base models\" paragraph, you mention that \"with the WSR's\n  embedding provided as input to the OG classifier in place of a sentence\n  embedding\". In order to classify a conversation, you need a vector\n  representation of it. How is this obtained? In other words, how are you\n  aggregating the contextualized word representations (i.e., the output of the\n  LSTM or the XLNet encoder), into a single vector representation of the\n  conversation? Are you using the last hidden state of the LSTM or a pooling\n  method? Are you using the [CLS] token of XLNet or something else?\n\n- You mentioned that your dataset contains full conversations with an average of\n  431 messages per conversation. Are all the conversation turns separated by the\n  [SEP] token? What is the average message length? What max input length did you\n  use as a hyperparameter? Did you use the same text input for Model 1 and Model\n  2?\n\n- Saying XLNet is the SoTA for NLP is a false statement (p. 3 second-last\n  paragraph). First of all, NLP encompasses several tasks and there is no single\n  model superior to all the others in every task. Second, XLNet has already been\n  beaten in several tasks. See the Glue benchmark\n  (https://gluebenchmark.com/leaderboard) for a few examples. You mention that\n  XLNet is the SoTA of NLP again in p. 6; sec. 5.1; second paragraph.\n\n- Saying that XLNet iteratively refines word embeddings from a WSR similar to\n  that of LIU et al. (2017) is only tangentially true and is misleading, in my\n  opinion. Recurrent models such as those relying on LSTMs, are profoundly\n  different to those based on transformers such as XLNet.\n\n- I am not sure that there is a clear correspondence between using recurrent\n  models such as LSTMs and better handling of class imbalance. How is a\n  two-layer LSTM going to help handling class imbalance?\n\n- You mentioned several times that \"text normalisation\" was a strong point of\n  your contribution, but in the last paragraph of section 4 (p.4) you say that\n  you do not apply \"text normalisation\". I understand that you might be\n  referring to different kinds of normalisation, but I think this terminology is\n  confusing. I suggest using more precise language to clearly differentiate\n  what you are referring to.\n\n- What do you mean by \"intersection tests\"?\n\n- I suggest using the word \"representations\" rather than \"coordinates\" for\n  referring to the vector representation of a word (p. 4; elastic pulling\n  paragraph).\n\n- The correspondence between rows in Table 1 and the strategies discussed in\n  section 4.2, are not immediately apparent.\n\n- When not using GloVe embeddings, how did you initialize your word\n  representations?\n\n- You mention in Table 1 that bold are improved results, but improved with\n  respect to what?\n\n- You report precision and recall in Table 2, but not in Table 1. I think it\n  would be valuable to include these in Table 1, rather than the distance\n  reduction and average resulting distance metrics which are only valid for a few\n  strategies.\n\n- You could check the following papers for more context on combining\n  representations of different word variants:\n  * [Attention-based Conditioning Methods for External Knowledge Integration](https://www.aclweb.org/anthology/P19-1385/)\n  * [Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics](https://www.aclweb.org/anthology/P13-1149/)\n  * [Composition in Distributional Models of Semantics](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x)\n\n\n#########################################################################\n\nSome typos:\n\n- p. 4; Manifold Learning - \"by building, using manifold learning, a new space\"\n  could be better written as \"by building through manifold learning a new space\"\n  or \"by building a new space through manifold learning\"\n\n- p. 4 - \"it is possible reduce dimensionality\" -> \"it is possible to reduce\n  dimensionality\"\n\n- p.5, sec. 4.2, first paragraph - \"collocates and std the span\". Not sure what\n  std is supposed to mean here.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}