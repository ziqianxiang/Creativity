{
    "Decision": "",
    "Reviews": [
        {
            "title": "Intuition makes sense; writing could be made clearer.",
            "review": "\nThis paper presents a method for transfering a contextual language model from English to e.g. Chinese. First, one aligns the embedding spaces. Then, one trains on the new language (but relying on bitext) and an external word aligner reorders the internal hidden states of the contextual language model to respect the new language's word order. \n\nPros:\n- I think the general intuition makes a lot of sense: we should indeed introduce reordering into contextual language models when doing cross-lingual transfer. \n\nCons:\n- I found the paper somewhat difficult to follow. See suggestions below.\n- The reliance on bitext is quite a large assumption. As such, I think it's necessary to compare with other methods that more directly utilizes the bitext. Also, it is good that Fig 2 examines the effect of bitext amount on BPW, but it would be more convincing to directly show the UAS, F1, or accuracy results as you have in Tables 1-3. \n\nSuggestions:\n\n- The main motivation of the paper is not extremely clear. It took me a while to realize that your goal is to transfer an existing English LM to a new language to avoide pre-training from scratch on a new language (this is the pre-training cost vs. migration training cost in Section 5). At first I thought you simply wanted to build LMs in a new language (without this pre-training cost issue), and were more concerned with multilingual transferability issues like those addressed by multilingual BERT and its variants. \n\n- Contributions should be more clearly delineated. For example, you discuss the adversarial word embedding alignment in the abstract but it is just borrowed from previous work. I would suggest focusing just on the TRIlayer, which is your main contribution. \n\n- There are many results but the exact experimental conditions were not clear. What were the datasets (and their sizes and characteristics) for the downstream task vs the pre-training task, and how they relate to the models in the tables. For example in Table 1, do TRI-BERT-base, BERT-small, BERT-base, m-BERT-base all use the same pre-trained data? Or is BERT-base and m-BERT-base downloaded, while BERT-small and the TRI-BERT-base have new pretrained data? In general these conditions are unclear for all the experiments, so more clarification would help. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "It does not bring anything new to the field of cross-lingual transfer learning with pretrained LMs",
            "review": "=== UPDATE ===\nI would like to thank the authors for their work and effort invested into revising the paper. However, some of my concerns (e.g., comparison to relevant baselines, linking core motivations with the experimental setup) still remain. I'd suggest the authors to start from the revised version and further embed some of the great feedback received from all the reviewers, and produce a stronger paper with a clearer presentation (and a clearer motivation and list of contributions) for some other venue in the future.\n===============\n\nThis papers aims to tap into the very populated area of transfer learning via pretrained language models (such as BERT, RoBERTa, XLM-R). The main idea is to allow for easy adaptation of pretrained language models in the source language to the target language via the usage of parallel data (integrated into model adaptation through a cross-lingual language modeling (CdLM) objective), without the need to retrain the whole model from scratch. In other words, the idea seems to leverage some cross-linguistic similarities to perform better on target language tasks, while keeping the costs of pretraining manageable. Overall, while the paper does an okay work in describing the main idea, profiling the new model in a range of tasks (in two languages only though), and running side experiments and ablation studies, I have several major concerns with the paper, and I am unsure what exact novelty and true contributions this paper brings.\n\n1. Limited novelty A. The main idea largely resembles the idea behind cross-lingual LM pretraining implemented for the XLM model. While the authors do discuss some minor difference to XLM's TLM objective in the appendix, this must be scrutinized more carefully. I am also unsure why the authors never compare to bilingual XLMs for their target languages, and it remains unanswered why this particular model should be preferred over some existing solutions based on XLM, XLM-R, etc.\n\n2. Limited novelty B; missing baselines and related work. The fact that 'bilingual tying' of pretrained LMs (both monolingual and multilingual ones) can be improved by leveraging some bilingual signal: e.g., see the work of Cao et al. (ICLR 2020), K et al. (ICLR 2020), Mulcaire et al. (CoNLL 2019), Liu et al. (CoNLL 2019), among others. However, the paper does not recognise and does not compare to this line of work at all. Therefore, leveraging parallel data as the signal for quick adaptations of pretrained LMs is definitely not new. The paper should discuss previous work adequately and also compare to these approaches leveraging parallel data. \n\n3. Inconclusive results. Besides lacking several baseline models, even the provided comparisons do not really show the core benefits of the proposed approach - the results, if at all, are only marginally above baselines (from BERT and m-BERT). Considering that recent larger models (such as XLM-R) offer even stronger results on a range of well established tasks, I wonder if these marginal improvements would be visible in that setup as well. Even the argument on the 'economic benefits' (from Table 4) does not hold completely - given training times, why should one opt for the new approach instead of training target-language models such as RoBERTa, XLNet or ELECTRA directly? I fail to see a true benefit here. Further, given that some previous work on improving/aligning monolingual and multilingual LMs applies the 'transfer adaptation' post-hoc (i.e., after training), the whole argument is even more fragile. \n\n4. A small number of target languages. Given recent evaluation initiatives in cross-lingual transfer learning (e.g., the work on XTREME and XGLUE benchmarks, not cited at all), I wonder why the authors do not provide a wider-scale evaluation on a larger set of languages. This would also allow the authors to analyse the benefits of their method (which takes into account vocabulary alignment and order information) with respect to different target languages and their properties.\n\nAs said, while the paper works on a very important problem, I currently do not see it delivering anything beyond what already exists in this crowded area, and minor and inconsistent improvements over an incomplete set of baselines fail to fully convince the reader on the efficacy and usefulness of the proposed method.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "No Baseline From The Literature",
            "review": "## Summary\nResearch Problem: Training BERT from scratch for a language is expensive, transferring knowledge from existing BERT could be more efficient.\n\nThis paper proposes a set of techniques to transfer monolingual BERT to other languages. It includes adversarial embeddings alignment, MLM loss with unsupervised word-level cross-lingual signal from bitext, and reordering layer. It considers transferring English BERT to two languages: Indonesian and Chinese. It shows reasonably good monolingual downstream performance while more efficient than training from scratch.\n\n## Pros\nIt presents various components for transferring BERT, and presents ablation study on some of the components and modeling decisions. \n\n## Cons\n1. While it discusses closely related work like Artetxe et al. (2020) and Tran (2020), and certain ablation is presented in the appendix, downstream performance comparison against prior work is not presented. Additionally, while this paper proposes a number of techniques, it’s unclear how much each technique contributes and the necessity of each proposed component. For example, is the proposed adversarial embedding alignment better than the separate vocabulary approach in Tran (2020)? Without grounding this work in terms of prior work in the experiment, it is hard to assess its contribution in pushing the progress on the main research problem.\n2. The model description is a bit hard to follow.\n\n## Questions during rebuttal period\n- Why pick English BERT as the source model as opposed to multilingual BERT?\n\n## Reasons for score\nOverall, I advocate rejecting. While I find the idea interesting, it is unclear whether it is necessary as no apple-to-apple comparison against previous work is presented. Hopefully the authors can clarify and address my concern in the rebuttal period. \n  \n## After revision\nThank you for answering my questions! However, I still find that the current version does not fully address my concern. I would recommend the authors include baselines of Artetxe et al. (2020) and Tran (2020), and fine-tuning mBERT in the main table in future revision.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a method to transfer a language model pre-trained in English to a new language. The approach consists of learning a mapping to the target language via the adversarial of Lample et al. (2018), a new intermediate layer where hidden representations of the input tokens are reordered based on alignment information of an external aligner tool, and a multi-step training process. The model is evaluated on Chinese classification tasks from the CLUE benchmark and Chinese and Indonesian dependency parsing datasets where it mostly performs competitively with a monolingual BERT Base and slightly outperforms multilingual BERT.\n\nPros:\n1. The paper tackles an important setting, the transfer of models to languages other than English.\n2. The proposed method performs competitively with a monolingual BERT model while being more efficient to train.\n3. The proposed intermediate layer that explicitly incorporates alignment information is novel. \n\nCons:\n1. Lack of evaluation on other languages. Despite proposing a general framework for cross-lingual transfer, the paper only evaluates on two fairly high-resource languages, Chinese and Indonesian. Given that the datasets used by the authors for evaluation such as the CoNLL-09 and UD datasets cover additional languages and in light of the introduction of recent massively multilingual benchmarks such as XTREME (Hu et al., 2020) or XGLUE (Liang et al., 2020), evaluation on multiple typologically diverse languages is necessary to demonstrate the generality of the proposed method.\n2. Missing baselines. The paper mentions two papers, Artetxe et al. (2020) and Tran (2020) that propose methods for the same setting—transferring an English model to other languages—in the related work section but does not compare to them during the experiments. The authors should also compare to a state-of-the-art multilingual model, such as XLM-R (Conneau et al., 2020).\n3. It is unclear where the benefit of the proposed approach comes from. The multi-step training process employed by the authors trains only on the monolingual corpus of the target language in the final phase of training. Recent work (Pfeiffer et al., 2020; https://arxiv.org/abs/2005.00052) has shown that such target language adaptation improves performance significantly over a pre-trained multilingual baseline. I was not able to find an ablation regarding the effect of this training phase, so the improved performance of the model could be due to fine-tuning only on the target language. At the very least, the authors should compare to a multilingual baseline that was adapted in the same way. This phase also makes the approach less general as the model is specifically tuned for the target language and less transferable to other languages.\n4. Lack of references of related work. The intro does not provide any references for related work on cross-lingual modeling and the rest of the paper is generally sparse with references of relevant work. For instance, given that the authors employ an algorithm that has been previously used for mapping cross-lingual word embeddings to map contextual embeddings, mentioning relevant work on this topic such as (Schuster et al., 2019; https://www.aclweb.org/anthology/N19-1162/) would be useful.\n5. Some passages in the text are unclear or should be supported with additional evidence. For instance, I found the claim that \"symbol sets, symbol order, and sequence length [...] are the three challenges of machine translation\" hard to justify without additional evidence given the presence of many other big challenges such as understanding semantics and context, resolving ambiguity, dealing with coreferences, etc.\n6. Given the page limit of 8 pages, the organization of the paper overall could be improved. Many important pieces of information such as the entire description of triple-phase training and the entire ablation analysis have been moved to the appendix, with no description of the key takeaways in the main body of the paper. Instead, Section 3.1 for instance, which mostly discusses an existing method could be shortened considerably.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}