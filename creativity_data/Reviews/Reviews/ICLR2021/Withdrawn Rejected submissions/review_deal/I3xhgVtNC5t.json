{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper provides a reformulation of the distributionally robust optimization problem into a (difficult in general) transportation map problem. In the new reformulation, the authors provide strong convergence results albeit requiring strong conditions, such as solvability of the auxiliary problems in reasonable time or complexity per iteration. At this point, it is very difficult to accept the paper   given the issues that reviewers pointed. In particular, authors' method which requires an expensive MCMC implementation is not scalable. Based on the run-times reported in Table 2, the proposed method is 10x slower than their baseline (Volpi et al 2018) achieving performance within their statistical error."
    },
    "Reviews": [
        {
            "title": "Incremental theory and incomplete experiments",
            "review": "This paper proposes a \"three-player framework\" for distributionally robust optimization (DRO), where the authors considered lifting the constraint of the ambiguity set of using the Lagrange multiplier method. The authors propose two algorithms, one for convex loss and one for non-convex, based on exponentiated gradient and gradient descent. Convergence rates are proven provided the steps in the algorithms can be carried out exactly, and some experiments are provided to demonstrate the empirical performance.\n\nIn my opinion, this paper contains a lot of strong and implicit assumptions that the authors did not address:\n\n\n*****************Major issues, theoretical*****************\n\n(i) Theorem 2 assumes that we can \"update\" the distribution in Algorithm 2 (for \\pi_t), which is in general an impossible task. The authors merely address such an issue by saying that one can try to take samples using Metropolis-Hastings algorithm, but two problems remained unsolved. First, taking a sample of \\pi_{t+1} is very different from \"playing\" \\pi_{t+1}, which involves computing an expectation over \\pi_{t+1}, and the latter is required in the theoretical analysis. Second, even if we ignore such discrepancy, the Metropolis-Hastings algorithm is known to converge extremely slow in almost all practical applications. Instead, the authors might want to try out gradient sampling methods such as Langevin dynamics https://arxiv.org/abs/1611.01838.\n\n(ii) The authors assume the Lagrange multiplier to be bounded and did not address how this additional constraint impact the original DRO formulation. It seems to me that the boundedness of the Lagrange multiplier is purely introduced to enable regret-type analysis with bounded domains. If this is the case, the authors should make it explicit.\n\n(iii) Both Theorem 1 and 2 require to compute, in every step of the algorithm, the best response of the Q-player exactly, which is impossible (and in fact quite daunting in practice to even get an approximation). In addition, the authors did not mention how the accuracy of approximation impact the rates.\n\n**********************************************\n\nBy assuming (i)-(iii) above, I feel that the analysis in this work reduces to plain (exponentiated) online gradient descent and does not add much insight to the DRO problem. \n\n\nFinally, there are a lot of missing details for the experiments, where the authors did not mention how their algorithms are run (how many steps to approximate the best Q-response? how many steps for sampling? etc.). The computation aspect is totally lacking for all the benchmarks as well (how many steps are run for these methods? how much time in total did all the methods cost?)? I would suggest the authors to add these information since they are necessary to judge a new algorithm. I would also suggest the authors to use LeNet in their non-convex example as it seems to be the minimally acceptable neural networks to consider in the robust training community.\n\n**********************************************\n\nI thank the authors for the detailed response. After reading the rebuttal and other reviewers' comments, I feel that the consensus is the impracticality of the proposed method. I therefore retain my evaluation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review ",
            "review": "*Summary:\nThe papers deals with the Wasserstein DRO problem.\nTheir basic idea is to use a Lagrangian formulation of the problem in order to get rid of the Wasserstein distance in the constraints. Then they show how it can be solved using standard primal-dual approach while assuming an access to an oracle that can solve the transportation map.\n\n*Detailed Comments:\n-The primal dual formulation and the use of primal dual method to finding an equilibrium is quite standard. \nTranslating the hard computational problem into a transportation problem is nice. Nevertheless, it preserves the hardness of the problem when $\\lambda$ is small, and actually it might be the case that the optimal $\\lambda$ is indeed small.\n\n-The paper is clearly written and clearly covers prior works on this topic.\n\n-Experiments do not establish a considerable improvement over the baselines.\n\n*Summary of review:\nThe paper is clearly written and deals with an interesting problem. \nMost of the presented derivations and ideas are quite standard, there is however one nice idea which tries to handle the hardness of the problem by translating it into a  transportation map.\n\n\nPost-rebuttal: I have read the authors responses and I keep my score\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviewer 2's Report",
            "review": "This work presents a three-player game framework for solving Wasserstein DRO problems, and it can handle general loss functions with new algorithms. I find this work interesting and potentially applicable to many problems. Below, I list my questions.\n\n1. In Lemma 1, the authors state that the strong duality holds. Is there any requirement on the l function?\n\n2. For Theorem 1 and 2, can the authors change alpha and beta/eta to a range that the results still hold?\n\n3. For theorem 3, is there any distributional assumption to get 1/\\sqrt{n} rate? For example, if the problem is a high-dimensional estimation problem, such rate doesn't hold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting approach but lacks rigor",
            "review": "This paper consider the distributionally robust optimization (DRO) problem with Wasserstein ambiguity set. Instead of formulating the problem as a two-player game (min-max form), this paper lifts the problem to a three-player game: the first player (the decision maker) chooses the decision, the second player (the nature) chooses a probability measure, and the third player (Lagrange) chooses the multiplier. The introduction of the third player aims to remove the Wasserstein distance constraint. The paper also explore the case where the loss function is non-convex. The paper proposes iterative algorithms to solve the three-player game, along with providing guarantees (both numerical complexity and statistical).\n\nPositive points:\n- The problem is pertinent to the field (robustness, non convex loss) and any rigorous results along this line can really help pushing the field forward.\n\n\nMajor concerns:\n- Lemma 1 requires further assumptions. The results from Sion (1958) require that R_Q(\\pi) is a lower semicontinuous function in \\pi, and is a upper semicontinuous function in Q. As both \\pi and Q now live in functional space, these semicontinuity is not trivial (a linear function is not necessarily continuous).\n- Algorithm 2 contains some obscure points:\ni) What is the nature of the prior distribution pi_1? Is pi_1 a discrete, continuous or mixed distribution?\nii) What is the nature of the distribution pi_{t+1} in each iteration?\niii) If pi_1 is a uniform distribution on Omega, should pi_{t+1} be continuous as well? In that case, why is the output \\hat{\\pi_t} a discrete measure?\niv) There is some hidden complexity in computing pi_{t+1} for the denominator. First, evaluating the integral is difficult. Second, even subroutine 3 requires storing Q_{1}, ..., Q_{t} which is highly undesirable\nv) The operation BEST_{Q, mu} relies on finding the optimal transport plan (9), and problem (9) is notoriously difficult. The authors did not resolve this difficult but \"leave this problem to future work\".\n\nThe paper thus relies on a number of heuristics (bounding lambda by B, using heuristic ascent to solve (9), etc.), and which renders the framework significantly less attractive.\n\n\nMinor concerns:\n- Please justify the claim that Problem (3) and (2) are equivalent for convex loss (after equation (3))\n- The title of the paper says a three-player game, so I was expecting a min-max-min formulation. The formulation (7) is thus a bit misleading because (7) can be regarded as a two-player game. Is it possible to pose a min-max-min formulation first and arrive at problem (7) as an equivalence?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of Wasserstein Distributionally Robust Optimization: A Three-Player Game Framework ",
            "review": "This paper proposes two algorithms to solve Wasserstein distributionally robust optimization (DRO), posed as a min-max-max problem, with three players taking actions at each round. The authors derive some theoretical bounds and do some experiments to showcase the effectiveness of the proposed algorithms. Unfortunately, the algorithms in this paper are quite standard and not scalable. In terms of performance, the results are within statistical error of the existing baselines. The theoretical results are also standard adaptations of online learning literature. Hence, I conclude that the paper falls short of the threshold for acceptance. Here are some specific comments for the authors to improve the quality of the paper.\n\n**Q1.** In terms of the solution and the theory, all the methods seem to be generic methods for solving generic optimization problems. Can you please explain which part of the algorithms particularize to Wasserstein DRO and would not be applicable to other types of loss functions? Could you also explain what the novelty is in the theoretical results with respect to existing results in the literature?\n\n**Q2.** The proposed MCMC method for the non-convex problem setup is not scalable. Can you please report running times for your algorithm and compare to the other baselines?\n\n**Q3.** The reported numbers seem to be within statistical error with baselines. Can you please add error bars to all tables and figures and explain whether there is any statistically significant advantage associated with the proposed algorithm?\n\n**Q4.** Can you please share your code with reviewers and ACs?\nTypo: In Algorithm 2 $Q_t \\leftarrow \\text{BEST}_{Q,\\mu_1} (w_t, \\lambda_t)$ needs to be updated. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}