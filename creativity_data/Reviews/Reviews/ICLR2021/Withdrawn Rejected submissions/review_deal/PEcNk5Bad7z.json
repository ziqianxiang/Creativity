{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work investigates an algorithm to learn representations of Lie groups. It first learns a representation of the Lie algebra by enforcing the Jacobi identity using known structure coefficients. Then obtains the group representation via matrix exponentiation.\nThe paper also proposes a Poincaré-equivariant neural network, and applies this model to an object-tracking task.\nThe paper is well-motivated, the derivations could be more clearly presented but are otherwise sound. The experimental results are promising but rather limited in scope at the time."
    },
    "Reviews": [
        {
            "title": "Learning Irreducible Representations of Noncommutative Lie Groups review",
            "review": "Paper summary:\n\nThe paper proposes the algorithm LearnRep that uses gradient descent methods to learn Lie algebras from structure constants, before obtaining the corresponding group representation through the exponential map. The algorithm is tested on SO(3), SO(2, 1), and SO(3, 1). In addition to this, the paper proposes SpaceTimeNet, a Poincaré-equivariant neural network architecture, and applies this architecture to an object-tracking task involving MNIST digits moving uniformly through space.\n\n------------------------------------------\nStrengths and weaknesses:\n\nThe paper proposed a well-motivated algorithm for learning irreducible group representations and performed sensible checks against well-studied Lie groups. The proposed Poincaré-equivariant convolutional network was similarly well-motivated, and the experimental results were promising.\n\nAs it stands, I’m assigning a score of 5. I like the paper and think that it would be a good workshop paper but is not ready for the main conference. The reason for this is that the theoretical contributions, while novel, are not large enough on their own, and the experimentation to support the theoretical contributions are not extensive enough to demonstrate that the theoretical contributions demonstrate a major step forward in terms of functionality.\n\nGoing forward, I think the paper could be improved by including more thorough experimentation for both LearnRep and SpaceTimeNet. Space could also be made for this through a more concise presentation of the background material in the first 5 pages.\n\n------------------------------------------\nQuestions and clarification requests:\n\n1)\tWhy did you choose the norm penalty that you did in equation 6? Did you consider other choices?\n2)\tIn section 2.5 you mention Tensor Product Nonlinearities. Where did you end up using them in the paper?\n \n------------------------------------------\nTypos and minor edits:\n\n-\tPage 2, bottom – “det A = 0” -> “det A = 1”\n-\tPage 3, second paragraph, condition (ii) – “R^3” -> “R^{m+n}”\n-\tSection 2.4, paragraph 1, sentence 1 – “represenation” -> “representation”\n-\tSection 2.4, paragraph 2, sentence 2 – “R^{n_{1}}” -> “R^{n_{1} x n_{1}}” for A matrices and similar for B matrices\n-\tSection 3.1.1, paragraph 3, sentence 4 – “section 2.5” -> “Section 2.5”\n-\tAppendix A.1, paragraph 1, sentence 4 – “A formulae to obtain real-valued representation matrices” -> “A formula to obtain real-valued representation matrices”\n-\tAppendix A.1, end of paragraph 1, “it may be easily checked that Kx , Ky , Jz satisfy the applicable commutation relations from Equation equation 3” -> “it may be easily checked that Kx , Ky , Jz satisfy the applicable commutation relations from equation 3”\n-\tAppendix A.2, paragraph 1, sentence 1 – “Lorentz group defining its action upon the spacetime” -> “Lorentz group defining its action upon spacetime”\n-\tAppendix A.2, paragraph 1, sentence 1 – we need u_{i} in R^{m}, not R^{n}\n-\tAppendix A.2, just before equation 10 - \\kappa(\\rho_{1}(\\alpha) …) -> \\kappa(\\rho_{0}(\\alpha) …)\n-\tAppendix A.3, paragraph 1, sentence 2 – “only if there is a nondegenerate nullspace corresponding to a unique set of Clebsch- For SO(3) and SO(2, 1), …” -> “only if there is a nondegenerate nullspace corresponding to a unique set of Clebsch-Gordan coefficients. For SO(3) and SO(2, 1), …”\n-\tAppendix A.3, paragraph 2, sentence 2 – “allowing for operations such as taking the tensor product of mutliple group representations” -> “allowing for operations such as taking the tensor product of multiple group representations”",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting topic, algorithm appears to be sound but more experiment might be needed",
            "review": "==== Summary ====\n\nThis work studies the problem of learning irreducible group representation without prior knowledge, and such algorithm (LearnRep)  is further used to build an object-tracking model (SpacetimeNet), which has guarantee of Poincaré group equivariance.\n\n==== Comments ====\n\n- *Strength*: The topics of learning irreducible representation (irrep) and Poincaré group equivariance look interesting to me. The technique of using optimization for finding irrep is simple and appears to be novel and effective. A complete introduction of the preliminary knowledge about group theory is presented in this paper, which reduces some of the difficultly for reader who is not familiar with this topic.\n\n- *Weakness*: One contribution claimed in this paper is the SpacetimeNet for object-tracking task. So I expect the proposed model can be evaluated through more realistic data sets instead of just MNIST. Also, as a person who is not very familiar with this task, I think it would be helpful to add some illustration on how the dataset is built and how the model is evaluated. Currently the experimental section seems to be difficult to follow due to the lack of explanation. \n\n In addition, I am wondering how LearnRep is properly motivated and analyzed? Specifically, I am not sure why the resulted representation from LearnRep is irreducible. Currently the irreducibility has only been demonstrated through experiments, and there is no motivation about how the loss function is derived. So I think it could help if the author can present some theoretical analysis about the correctness of the learned group representations.\n\n====  Reason for scoring ====\n\nOverall, the topic for finding irrep is important and suitable for presenting in ICLR. My main concern is on the experiment section, which needs more realistic dataset and explanation to demonstrate the importance and effectiveness of the proposed model/method. I think moving most of the contents in Sec.2 to the appendix for space, and adding more experiments and illustrations mentioned above can greatly enhance the soundness of this paper.\n\n \n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Complex and incomplete: Reject",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a new framework based on noncommutative Lie groups to learn irreductible representations. Such representations can manage many kind of operation like rotation, translation, Lorentz boost... The interest of such representations is important since many application must be insensitive to some modification.\n\n##########################################################################\n\nReasons for score:\n\nOverall, I vote for rejecting (see cons for more details). The framework is very interesting but too complex to have a large audience. Furthermore some clues are missing on how the equivariance are declared.\n\n##########################################################################\n\nPros:\n\n1. This paper proposes a very generic framework for learning equivariant representations. This is of broad interest.\n\n2. Lie groups are able to manage many kind of transformations, thus the framework could lead to new application of deep learning.\n\n##########################################################################\n\nCons:\n\n1. The tensor A (the structure constants) of the algebra seems the main element of the whole framework. The construction of such tensor is unclear and seems non-trivial. For a given set of transformations, how can we derive the structure constants? The authors must explain this point.\n\n2. The equilibrium between appendix and main article is not good. Some figures are cited into the main article while they are in the appendix. On the other side, a large part of the paper is used to introduce the Lie groups. Perhaps a good way to reduce the complexity of he paper would be to take one example as introduction and lets the more formal parts in the appendix. As such the paper is too complex to catch the audience it merits.\n\n3. The experiments are not useful as we don't have any description on how there are done. For example on section 5.1 how the structure constants are given. Are they learnt or estimated?\n\n##########################################################################\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above, especially point 1.\n\n#########################################################################\n\nSome typos: \n\npage 4: representation instead of \"represenation\"\n\npage 6: Adam instead of \"adam\"\n\npage 6: please put parenthesis for the Adam paper citation",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}