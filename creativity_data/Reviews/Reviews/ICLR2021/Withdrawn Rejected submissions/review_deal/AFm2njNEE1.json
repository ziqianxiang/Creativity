{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting research direction for NAS. More efforts needed for novelty, benefit, and justification.",
            "review": "This work proposes to search the topology structure of a cell in DARTS-like differentiable NAS. It directly decouples operation selection and topology search, and explicitly models pairwise connection between nodes with topological variables, in the same manner of DARTs modeling the operator selection. Integrated into two existing algorithms DARTS and MiLeNAS, the experimental results show improved performance with less parameters.\n\nIn general, this is an interesting idea and the model architectural topology search problem is clearly an important one. However, more efforts might be needed to further improve the current work.\n\n1.\tNote that DARTS has implicitly addressed the topology search problem by including a zero operation. It is not clear why the decoupled and explicit modelling of the topology as considered in this work is a better idea. Actually, it introduces other issues that need to be carefully addressed, such as invalid topologies. Some deeper justifications are needed.\n2.\tThe experimental improvement seems not that significant. Results on more datasets that can show the clear benefit of topology search are desired.\n3.\tIt would be interesting to show some example searched architectures and compare with those manually designed ones such as ResNet, Transformer, etc. This is to show that topology search can discover new better architectures with notable structures that are superior to existing ones.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper proposes a method to separate modeling operations and topology in searching for neural architecture. To discover optimal topology for CNNs, the paper utilize the idea of differentiable neural architecture search in the work of DARTs (Liu et al., 2019) using gradient-based optimization to edge configurations to construct a neural architecture with connected nodes. This usage is a new approach to improve to find the best topology in CNNs.  \n\nUsing novel variables for bi-level optimization to explore the importance of topology in neural architecture design is a very interesting idea and has some contributions to this topic.  \n\nHowever, there are some concerns.\n\n- Although a new approach to the neural architecture search challenge, the method heavily relies on the same approach of DARTs (Liu et al., 2019). This leads to questions about the novelty of the method.\n- One of the problems of DARTs is the reason why such bi-level optimization works? The paper does not provide a clear explanation for this point.\n- What is the reason why the paper choose DARTs for exploring topology? How's about if using other search methods for finding the topology of CNNs: evolutionary search, reinforcement learning, etc.?  \n- Need to provide more analysis of the influence of different topology of CNNs on the results.\n\nIn summary, with the current contribution, the paper is not sufficient enough for the conference.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An incremental improvement over DARTS",
            "review": "This paper presents an improvement over DARTS that aims to decompose the learning of topology and operations. A set of parameters are added to model topology, and the results look good in some cases.\n\nOverall, this paper provides an incremental contribution to the NAS community. It verifies that the search space can be better explored using a modified version of parameterization (relaxation), but many problems that are more important remain uncovered.\n\nUsing an additional set of parameters to model the topology is a proper idea. It solves two issues: (1) the zero operator in DARTS is eliminated, which is believed to cause an optimization gap because the weight on zero is often large; (2) the network uses explicit parameters to determine the topology, so as to avoid choosing two most important ones at the end of DARTS (sometimes one of them can be a weak edge).\n\nHowever, the benefit is somewhat limited. The critical issue of DARTS lies in that it uses shared weights in the super-network (amortized among different architectures), so that it can fail to provide an accurate estimation on the performance of each architecture. The proposed approach did not deal with this core issue. Even with an additional set of parameters, each operation in the super-network can be used by different topologies, so that each of them has to balance itself among different topologies. To make things worse, in the \"arbitrary\" setting, the topology can vary in a wider range, making each operation even more difficult to fit different topologies (I would like to see some discussion or results on this point if my statement is wrong). So, it is difficult to claim that the method indeed brings stability to DARTS.\n\nBTW, I think the ability to explore more complicated spaces is beyond the ability to provide stable search results. So, I am wondering if the authors can perform an analysis on the search stability - searching on CIFAR10 for 10 times and observing the behavior of the baseline and the proposed approach. Does it achieve success (satisfying search results) more frequently, or report a lower deviation?\n\nAnother issue comes from the search efficiency. With a larger N, the \"arbitrary\" search setting may incur a larger number of topologies. I think this slows down the search procedure, but the authors did not report the search cost in the paper. This somewhat limits the method from generalizing to more complicated search spaces.\n\nAs a relatively minor issue, this paper needs heavy revision. There are many typos and grammatical mistakes in the paper - even the title contains a grammatical error: it should be \"explicitly learning ...\" or \"an explicit learning of ...\". Please try to polish the paper carefully.\n\nProvided the limited technical contribution and that this paper did not try to solve the important problem of NAS, I suggest a score of 4.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A NAS method to explicitly account for topology of CNNs",
            "review": "The paper proposes TopoNAS that decouples the learning of the topology (connections between various operations) and operation types during the search. The experiments are provided on CIFAR-10 and Imagenet datasets.\n\nPros:\n+ The paper is well-written and explains the idea clearly.\n+ The idea of decoupling topology search and operator search is interesting.\n\nCons (see below for details):\n- Parts of the approach do not make complete sense or need better/correct explanation.\n- Results have not been compared against some very standard networks (and would in fact underperform the newer baselines like EfficientNets).\n- FLOPS for the models have not been reported (which is essential (but still not sufficient) determine if the models generated are actually efficient). Moreover, the hardware-aware models such as ProxylessNAS would likely result in more efficient models.\n- Overall, the improvements are not impressive enough. Just 0.1% improvement for CIFAR-10 can be hardly called a significant improvement. For Imagenet, the generated models do not outperform EfficientNets and PC-DARTS.\n- Other issues related to prior work and optimality of the proposed algorithm.\n\nDetailed Comments:\n1. The reviewer’s main concern about the approach is with the description given in Section 3.1.1: (Statement: Note that a DAG can be described…). Accordingly, the authors shift the modeling from input edge perspective to output edge perspective and claim that this resolves some difficulties.\n\nHowever, does this make sense? The network is still exactly the same, so how does changing the modeling help the authors achieve any better behavior?\n\nMoreover, wouldn't there be the same issue during backprop? If the perspective was input edges, the features from dominant nodes will dominate in the forward pass. But similar to forward pass, each node will have gradients associated with it which can be more dominant for some nodes than others. So, it is easy to see that whatever problem the authors resolved by switching to the perspective of outgoing edges is going to become a problem on the backward pass (and hence does not change anything!).    \n\n2. The Imagenet results are not at all convincing. Even according to their own table, the proposed method does not outperform PC-DARTS. Several newer baselines like EfficientNets must also be compared.\n\n3. Number of parameters is not the only measure of actual hardware-efficiency. The authors must provide FLOPS (and if possible real latency numbers on real hardware like Pixel phones). Currently, the efficiency claim is not well-justified.\n\n4. Again, just 0.1% improvement (over MileNAS) on CIFAR-10 cannot be justified as a significant improvement. Significant improvements must be shown on large-scale datasets like Imagenet.\n\n5. Other issues: Didn’t RandWire paper already explore the topological angle in depth? Their conclusion was that even the random topologies achieve competitive accuracy in NAS (still with not too large number of parameters). And this was not even random search but just one instance of a random topology. Especially, the arbitrary topology search proposed in this paper would be expected to leave the problem too unconstrained. How do we know if the SGD is able to really find the optimal solution in such an unconstrained scenario? The authors must do additional experiments to demonstrate that just some random wiring is not giving similar solutions. Alternatively, the authors can also provide some theoretical grounding to current work to show that this indeed works.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}