{
    "Decision": "",
    "Reviews": [
        {
            "title": "Incomplete analysis of the impact of pixel normalization on entanglement.",
            "review": "The paper investigates the entanglement effects observed when sets of units in a Progressive GAN are ablated, for example, when units that cause the removal of one object class also cause the insertion of a different object class.  It proposes that, within Progressive GAN, pixel normalization causes entanglement.  To test this, the paper fixes pixel normalization coefficients while ablating units and compares the effects to output when pixel normalization is allowed to change as normal.  The paper also argues that object classes can be divided into three types that exhibit different entanglement effects.\n\nThe strength of the paper is in its main hypothesis about the effect of pixel normalization and proposed experimental setup.  Since normalization will amplify non-ablated dimensions, it is reasonable to hypothesize that normalization is what causes other objects to appear, and it is good to test the interaction directly by removing the effects of normalization.\n\nHowever, the paper does not sufficiently develop the quantitative results for this idea.  After the metric for estimating causal effects of pixel normalization on entanglement are defined in Eq 9 and illustrated in one example in Figure 4, the paper does not fully describe scaled experimental results computing this metric on a larger set of generated images or classes of objects.  If pixel normalization is causal, how large are the differential effects of holding pixel norm constant when measured across large sets of images?  How many object classes were tested? How many images were tested?  Full experimental details and results should be described.\n\nThe experiment should be set up in more than one way to narrow down the causal effects.  For example, in the proposed setup, normalization effects are tested by holding normalization coefficients constant in all layers from layer 4 to 13.  But it would be intuitive that the entanglement effects are mainly caused by normalization in the layer with the intervention. Are the same effects seen with a narrower causal test, holding just normalization at a single layer (e.g., layer 4) constant?\n\nIt is also interesting to ask whether the presence or absence of pixel normalization during training has an effect on entanglement. Would a progressive GAN trained without pixel normalization in a single layer exhibit less entanglement in the representation at that layer?\n\nIn section 5.2 it is argued that the distribution of causal units in the representation can be used to predict how entangled interventions will be.  However, it is not sufficiently explained how this effect is tied to the main hypothesis about the causal effect of pixel normalization.\n\nThe conclusion closes with the phrase that the experimental “framework theoretically guarantees” the soundness of the conclusions, which is a strong overclaim.  That sentence should be removed.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of DAD approach for PG-GAN",
            "review": "This paper provides an investigation of the effects of pixel normalization on disentanglement of PG-GAN. \nThe main hypothesis of this paper is that the normalization function incorporated in the architecture of PG-GAN causes entanglement of the generator. \n\nOne of the main weaknesses of this paper is horrible writing and overly convoluted narrative structure which makes it impossible to understand what authors meant. Figure 1 is an example of disrespect to the readers, both due to the fact that it is rotated which makes the reader break their neck to look at it, and because the caption uses abbreviations and notation (e.g. Type-1 entanglement or ACE units) that are introduced far later in the paper. \n\nIn the first paragraph of the page 3, authors actively use words ablated objects and in-painting objects without properly explaining what they mean by that and how they achieve it. Some symbols used are never even introduced, e.g. S in equation 4. In page 4, while providing a definition of entanglement (which should have been done much earlier, in my opinion), authors claim that \"T_a is disentangled if and only if T'_a is disentangled). But isn't the definition of disentanglement only used on for transformation on the data manifold, not on representation units?\nLater, in definition 3, authors introduce function phi that \"denotes pixel normalization function's effect\". Effect on what? How do you measure it? And why is it additive with effects of other functions effect? It is not clear from the paper. \n\nOverall, this paper aims to shed light to a very important problem of disentanglement in GANs, but unfortunately, it doesn't provide a coherent and mathematically sound apparatus to analyze this problem thoroughly. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comments",
            "review": "*Summary:\n\nThis paper aims to explain how entanglement is originated by GAN transformation. To this end, the authors propose a difference-in-difference (DID) counterfactual framework for PG-GAN. Experiments clarify the mechanisms of how pixel normalization causes PG-GAN entanglement during an input-unit-ablation transformation. My detailed comments are as follows.\n\n*Positive points:\n\n1. Studying what mechanisms causes the entanglement in GAN is an important problem. Moreover, this paper would help to improve PG-GAN.\n\n2. The experiments clearly demonstrate clarify the mechanisms of how pixel normalization causes PG-GAN entanglement.\n\n*Negative points:\n\n1. The motivation of the research should be emphasized. For example, why use a difference-in-difference (DID) counterfactual framework to analyze the entanglement mechanism in the Progressive-growing GAN (PG-GAN)? In addition, the relationship between DID and pixel normalization is not clear.\n\n2. The contribution of this paper is limited. The authors only analyze the entanglement mechanism in PG-GAN in this paper. Except for PG-GAN, BigGAN is also an approach to generating a high-resolution figure. It would be better to analyze the entanglement mechanism and bridge normalization and disentanglement in a general GAN.\n\n3. Some technical details are not clear. The use of notations in this paper is very confusing since they are used without clear explanations. For example, what is $ S_c $ in Eqn. (4). In addition, it would be better to provide an intuitive understanding of Definition 4.\n\n\n**Minor issues:\n\n1. In the abstract, the sentence “Our experiment clarify … a input-unit-ablation transformation” has some grammatical errors. Please check the grammatical errors throughout this paper.\n\n2. The abstract and the conclusions are almost the same. Please refine these two sections.\n\n3. The placement of Figure 1 is not friendly to readers.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}