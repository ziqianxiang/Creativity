{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper gives a way to learn one-hidden-layer neural networks on when the input comes from Gaussian mixture model. The main algorithm uses [Janzamin et al. 2014] as an initialization and then performs gradient descent. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give a local convergence result when the samples come from a mixture of Gaussian. The paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound."
    },
    "Reviews": [
        {
            "title": "This paper investigates the training convergence of the general one-hidden-layer fully connected neural network  in a more general and practical scenario that the input features follow a Gaussian Mixture Model (GMM) distribution and proves that the GMM induced learning algorithm converges linearly to a critical point  of the empirical risk function. This result makes a progress on the training convergence study of this network.",
            "review": "This paper analyzes the convergence behaviour of the general one-hidden-layer fully connected neural network  in the practical scenario that the input features follow a Gaussian Mixture Model (GMM) distribution. Under certain assumptions, the authors prove that the GMM nduced learning algorithm converges linearly to a critical point  of the empirical risk function, which really  makes a progress on the training convergence study of this network.  However, I  have the following concerns: (1).   What is the ground-truth weights?  For a large traning set, it is possible that there may be no such ground-truth weights. Moroever, the ground-truth weights cannot  form a critical point of the empiirical risk function. So, this assumption is not reasonable. (2).  The assumptin of the Gaussian mixture model is special, not general, and its parameters are  assumed to be known a priori. This  may be too strict and limits the significance of the result. (3). There are some errors  in the mathematical denotations like Eq.(4).\n\n\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviews",
            "review": "This paper considers the problem of learning one-hidden-layer neural networks with Gaussian mixture input in the teacher-student setting. The authors consider the neural network with sigmoid activation functions and the learning algorithm is gradient descent plus tensor initialization.  There is a line of research studying such a problem, and the main contribution of the current paper is to extend the standard Gaussian input distribution to the mixture of Gaussian input distribution. My main concern is the contribution of the current paper. The main techniques used in this paper seem to be based on existing approaches in Fu et al, 2020 and Zhong et al, 2017, and the Gaussian mixture input setting considered in this paper seems not to be very interesting and realistic. Here are some problems I have for the current paper:\n1. Please clarify the main differences of the current analysis compared with Fu et al, 2020 and Zhong et al, 2017.\n2. Please elaborate more about Assumption 1. Intuitively, what can we imply from this assumption and why you think it is a mild assumption?\n3. The description of the tensor initialization at the end of page 4 is not very clear.\n4. Please add some comments on functions in Definition 2,3,4. It is unclear what are the meanings of these functions.\n5. Thereom 1 looks not correct since there is no requirement on the step size in Algorithm1. In addition, why the parameter v can belong to (0,1)?\n6. Since the paper claims to use tensor initialization, the experiments should also include such results.\n7. The presentation of the current paper is good. However, there are some concurrent works [1,2] also study the training and generalization of neural networks, the authors may want to discuss them in the introduction section.\n\nReference:\n[1] Zou, Difan, et al. \"Gradient descent optimizes over-parameterized deep ReLU networks.\" Machine Learning 109.3 (2020): 467-492.\n[2] Cao, Yuan, and Quanquan Gu. \"Generalization bounds of stochastic gradient descent for wide and deep neural networks.\" Advances in Neural Information Processing Systems. 2019.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper is interesting and provides some new theoretical insight into the learning problem of fully connected neural networks when the input data are from mixture of Gaussian distributions.",
            "review": "In the paper, the authors provide theoretical analysis of learning one-hidden-layer neural networks when the input distribution follows a mixture of location-scale Gaussian distributions instead of single location-scale Gaussian distribution as in the previous work. I think the results in the paper are interesting \n\nHere are my comments with the paper:\n\n(1) The literature with Gaussian mixtures is quite poor. Except for the references with the application of Gaussian mixtures in the paper, I think the authors may consider adding a few more relevant references about theoretical aspect of Gaussian mixtures. For instance, the work of [1] provides convergence rate/ sample complexity for estimating unknown location and scale parameters when the data are generated from Gaussian mixtures. Furthermore, the work of [2] provide theoretical analysis of optimization algorithms, such as gradient descent/ EM/ Newton algorithms,  for learning location and scale parameters in Gaussian mixtures (Section 4.2 in this work).\n\n(2) The assumption that the weight, location, scale of Gaussian mixtures as well as the number of components $L$ are known is quite strong in my opinion. In particular, the number of components $L$ in Gaussian mixtures is rarely known in practice; therefore, we usually choose some $\\bar{L}$ as the number of components and $\\bar{L}$ can be much larger than $L$. By doing that, we overspecify the number of components in Gaussian mixtures. This over-specification leads to the slow convergence rates of estimating weight, location, scale of Gaussian mixtures; see the references [1], [2], and [3]. For the settings that being considered by the authors, when $\\bar{L} = L + 1$, if we use EM algorithm, the convergence rates of estimating these parameters from the EM algorithm for location parameter is $(d/n)^{1/4}$ and for scale parameter is $(d/n)^{1/2}$ when $d \\geq 2$ (n stands for the sample size) (see references [2], [3], and [4] for details). Therefore, in light of the results in the paper, the total sample complexity is $\\sqrt{d \\log n/n} + (d/n)^{1/4}$ when the parameters of Gaussian mixtures are unknown. When the covariance matrices are not spherical and $\\bar{L} > L$, the work of [1] show that the sample complexity of estimating location and covariance matrices depends on the solvability of a system of polynomial equations and eventually grows with $\\bar{L} - L$. I think the authors should provide a clarification of these points in the paper.\n\n(3) The paper specifically assumes that the covariance matrices of each component are $\\sigma_{l}^2 I_{d}$, i.e., homogeneous among all dimension in each subpopulation. When the covariance matrices have a bit more realistic structures like $\\text{diag}(\\sigma_{l1}^2, \\ldots, \\sigma_{ld}^2) I_{d}$ for all $1 \\leq l \\leq L$, will the results in Theorem 1 still hold?\n\n(4) In Theorem 1, what is the intuition behind $K^{5/2}$ and $\\Gamma(\\lambda, M, \\sigma, W^{*})$$ on the difference between $\\widehat{W}_{n}$ and $W^{*}$ as well as $D_{12}$ in the condition of sample size $n$?\n\n(5) Can the authors provide some initial theoretical analysis/ discussion for the setting of multi-layer neural networks? I agree that one layer neural network is quite interesting; however, it will be useful for the readers to understand the challenges of extending the \ncurrent results to the multi-layers settings.\n\n(6) A few minor comments:\n\n- In Definition 1, what is $M_{3}(I_{d}, I_{d}, \\alpha)$?\n\n\n\nReferences:\n\n[1] N. Ho and L. Nguyen. Convergence rates of parameter estimation for some weakly identifiable finite mixtures. Annals of Statistics, 44(6), 2726-2755, 2016\n\n[2] N. Ho, R. Dwivedi, K. Khamaru, M. J. Wainwright, M. I . Jordan, B. Yu. Instability, computational efficiency and statistical accuracy. Arxiv preprint Arxiv: 2005.11411.\n\n[3] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, B. Yu. Singularity, misspecification, and the convergence rate of EM. To appear, Annals of Statistics.\n\n[4] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, B. Yu. Sharp analysis of Expectation-Maximization for weakly identifiable models. AISTATS, 2020.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting convergence guarantees, but the results are oversold and the numerical experiments rather weak (updated version)",
            "review": "**Update after response of the authors**\n\nThe authors partly corrected the points I mentioned, and I thank them for extensively addressing my comments. However I still believe that the paper oversells its results and analysis, although in a much less strong manner (e.g. in the last sentence of the abstract) and that many concerns remain. For instance, my concern on the tensor initialization has been partially addressed by the authors which showed that it performed as well as a random initialization close to the solution. The very small dimension (d = 5) for which these simulations were performed is however still a concern for verifying this analysis, which would need to be more rigorous and much more extensive to justify replacing the tensor initialization with the random one.\nAs a consequence of all the changes made by the authors, I have raised my grade from 3 to 4 (and I would grade the current state of the paper between 4 and 5).\n\n**Summary:**\n\nThe paper focuses on the behavior of gradient descent in one-hidden-layer neural networks (with fixed second layer weights), in a “teacher-student” setup. In this setup, the labels are generated by an unknown teacher network with the same architecture, and the student aims at recovering the teacher weights. The main ambition of the paper is to provide guarantees for the convergence of gradient descent on the cross-entropy loss, for an input data which comes from a non-trivial model, here a mixture of Gaussians. The algorithm is moreover initialized with a tensor initialization method, which will be crucial to assess its performance. The paper provides a precise theorem to guarantee convergence of this algorithm, and studies how the learning process can depend on the parameters of the mixture of Gaussians. Importantly, the theoretical analysis also relies on the knowledge of these parameters. In particular, they derive limit regimes in which learning should be very hard, e.g. when the variances of the mixture are either very small or very large. They finally provide numerical evidence to support their claims. \n\nGiven the length and available time, I did not check the calculations given in the supplementary material.\n\n**Overall decision:**\n\nConsidering all my criticisms and comments, I cannot recommend publication of this paper at ICLR 2021 as it is. For me to reconsider this decision, the authors would have to slightly improve the quality of the writing (see the remarks and typos), to improve the numerical analysis, and to provide a more convincing presentation of the impact and novelty of their theoretical results with respect to the previous literature. \n\n**Strengths of the paper:**\n\n- The authors provide an involved tensor initialization method to start the gradient descent algorithm, which provably reaches a basin of attraction of a local minimum very close to the true weights.\n\n- The bounds derived in Theorem 1, both on the convergence of the algorithm and on the distance between the local minimum and the true weights are quite explicit, in particular as a function of the number n of samples, the dimension d of the data, and the number K of hidden neurons. Having these two bounds together is important and interesting, as it allows not only to probe the optimization but also the generalization properties. Moreover, the assumptions needed on the activation function (Assumption 1) are quite generic, and allow for a large class of activation functions. \n\n- They show that the sample complexity needed for precise estimation with this algorithm is in the scale $d \\ (\\log d)^2$. I wonder if such a bound is sharp? Also perhaps the authors could comment on the work of [Mei, Bai & Montanari, ‘18], which showed that in this scaling, the landscape of some estimation problems is already trivialized (i.e. close to the population loss landscape). They showed it in particular for a single-hidden-node model: it could be interesting to compare their result with the present paper in the limit $K = 1$.\n\n- Corollary 1 allows to study the impact of the parameters of the mixture of Gaussians (i.e. the structure of the data) on the learning procedure, with the mentioned algorithm. In particular, they show that either too large or too small variances can be detrimental to optimization. While it is intuitive that large variances would harm optimization, the finding on small variances is particularly interesting, as one would expect that small variances in the Gaussian mixture would imply an easier optimization problem. On this point, Fig. 2 is very qualitative, but shows the dependency of the sample complexity on the parameters of the mixture of Gaussians. In particular, Fig.(2b) manages to show the interesting divergence of the sample complexity when the variance goes either to $0$ or $\\infty$. \n\n- Figures 3 and 4 do a decent job at showing the dependency of the final convergence time on the parameters of the mixture, of the convergence rate on the size of the hidden layer, and of the distance of the true weights to the critical point achieved by gradient descent on the number of samples. The results are quite consistent with the theoretical analysis. \n\n**Concerns and remarks:** \n\n- In the introduction, the authors explain that they consider a setup in which the labels are generated by a ground truth network, and provide some recent literature on this hypothesis. They however do not mention that this assumption is known as the “teacher-student” setup, and it has been studied for a long time in the statistical learning community (in particular from a statistical physics point of view in which such a setup is very natural). The paper should correct this point by providing a much more exhaustive view of the literature on this topic. For instance, they can refer to [Seung, Sompolinsky & Tishby ‘92], [Engel & Van den Broeck ‘01]. See also [Goldt & al, NeurIPS’19] for recent applications to neural networks. \n\n- Fig 1 (in the numerical experiments) is somehow unclear. While the authors pretend that the sample complexity needed to recover is indeed almost linear in d, this is not obvious from the picture. Moreover, the difference in orders of magnitudes between n (from 6000-60 000) and d (from 1 to 30) indicate that, even with a quasi-linear dependency, the prefactor would be huge, and the authors should discuss this point.\n\n My two main concerns are the following:\n\n- First, as emphasized in the beginning of Section 4, the tensor initialization is crucial, as it actually returns an estimate already in the basin of attraction of a critical point very close to the ground truth.  However, in Section 5 (on the numerics), the authors precise “We use random initialization rather than tensor initialization to reduce the computational time”, without any justification of how this could impact the results. This reduces greatly the relevance of these numerical results, and their relation with the theoretical findings. This also raises the question: is the tensor initialization numerically tractable? If the tensor initialization provably returns an estimate in the correct basin of attraction, this algorithm is actually doing the most important part of the estimation, and replacing it with a random initialization close to the ground truth removes a lot of the relevance of the theoretical findings (as obviously, when starting in a convex region, gradient descent will work). \n\n- Secondly, the following bold claims can be found in the abstract and the main text, and are very emphasized:\n\n1) “Instead of following the conventional and restrictive assumption in the literature that the input features follow the standard Gaussian distribution, this paper, for the first time, analyzes a more general and practical scenario that the input features follow a Gaussian mixture model of a finite number of Gaussian distributions of various mean and variance.”\n2) “This paper provides the first theoretical analysis of learning one-hidden-layer neural networks when the input distribution follows a Gaussian mixture model containing an arbitrary number of Gaussian distributions with arbitrary mean and variance.”\n\n3) “This is the first theoretical characterization about how the input distribution affects the learning performance.”\n\n4) “Theorem 1 provides the first theoretical guarantee of learning one-hidden-layer neural networks with the input following the Gaussian mixture model.”\n\n    In my point of view, these are exaggerated statements. While it is true that there is ample room for new studies of non-trivial input distributions, several previous and impacting papers have followed very similar approaches, beyond [Du&al ‘17] which is the only paper cited by the authors in this context. For instance, the following (very incomplete) list of papers all either consider training a one-hidden-layer neural net on a dataset with a non-trivial covariance, or a mixture of Gaussians data model:\n    1. Mei, Montanari & Nguyen [PNAS 2018] study one-hidden-layer networks in the mean-field limit trained on a large class of distributions, including a mixture of Gaussians with the same mean (see Fig 1 of the paper). \n    2. Li & Liang [NeurIPS 2018] studied over-parameterized one-hidden-layer nets \"when the data comes from mixtures of well-separated distributions\".\n    3. Yoshida & Okada [NeurIPS 2019] study one-hidden-layer neural networks with inputs drawn from a single Gaussian with arbitrary covariance.\n    4. Goldt et al. (arXiv:1909.11500 and arXiv:2006.14709) study one-hidden-layer networks with inputs drawn from a wide class of generative models. \n    5. Mignacco et al. (arXiv:2006.06098) give exact equations for the size-one minibatch SGD evolution in a perceptron (i.e. a single-node network) trained on a mixture of Gaussians. \n    6. Ghorbani et al. (arXiv:2006.13409) consider labels which depend on low-dimensional projections of the inputs, which, I believe, is very related to a mixture of Gaussians.\nSome of the papers in this list (e.g. [1,2,4,6]) also provide a rigorous analysis of some of their results.\n\nWhile, to the best of my knowledge, the theoretical results of the present paper (i.e. global convergence guarantees for gradient descent with tensor initialization, for data coming from a mixture of Gaussians) are indeed new, their impact and novelty is, I believe, exaggerated. In particular, similar results already exist in the literature cited above, and the new results of this paper should be discussed in comparison to them. \n\n**Minor points and questions:**\n\n- The notations section is very long. While some precisions are useful, many are very standard (N, Z, R, or the transpose, or the L2 norm) and, I believe, do not have to be reminded. Similarly, the footnote at the end of page 5 is not necessary. \n\n- The paper studies one-hidden-layer neural networks with fixed second layer weights. The authors should mention that this setup is known as the committee machine, and they should refer to some literature on this (besides some references given in the concerns section, one can for instance refer to [Aubin&al NeurIPS 2018, Schwarze&al ‘92,’93, Monasson&Zecchina ‘95] and many others).\n\n- Is it possible to add a noise in the gradient descent algorithm without affecting the theoretical findings? Even an uncorrelated noise (i.e. Langevin dynamics), as I expect that the noise in plain SGD will not be easily tractable.\n\n- Algorithm 1 requires a constant learning rate: I wonder if the authors tried (even just numerically) to see if the bounds could be improved by considering an adaptive learning rate (for instance with a linear decrease)?\n\n**Typos:**\n\n- At the beginning of the introduction: “Neural” → “neural”.\n- Just after, I believe: “theoretical underpin of leaning neural networks” → “theoretical underpin of learning in neural networks”.\n- Again, just after: “lack of the theoretical generalization guarantee” → “lack of theoretical generalization guarantees”.\n- In the “Contributions” paragraph,  the “etc” when listing the applications of the Gaussian mixture model does not read well. \n- In the “Contributions”: “One interesting finding is the”→ “One interesting finding is that”.\n- Just after: “all the variance approaches” → “all the variances approach”.\n- In Section 2: “Let kappa denote the number that kappa =...” → “Let kappa = …”\n- Below eq.(6): sigma_l \\in R → sigma_l \\in R_+.\n- End of page 4: “more details of” → “more details on”\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}