{
    "Decision": "",
    "Reviews": [
        {
            "title": "Contributions are not clear and more comprehensive comparisons are needed",
            "review": "This paper proposes a CNN architecture for addressing the problem of adversarial robustness. This architecture consists of complementary (named push-pull) filters and is claimed to suppress the responses to noise patterns and consequently make the network more robust against common natural image perturbations. Experiments on CIFAR10 and IMAGENET datasets are presented. \n\nMy main concern is regarding the comparison with the state-of-the-art and added value in comparison to Strisciuglio et al. 2020. While the proposed architecture seem to be effective at improving the robustness against natural image perturbations it’s not clear where these improvements stand with respect to prior art. While many of the prior work were concerned with image-augmentations, there are a number of works that focus on architectural proposals like [1] and [2]. It seems necessary to see those comparisons to better assess the value of the current work.  Some recent baselines could be [3] and [4] that have shown the additive effect of many architectural and image augmentation methods. \n\nRegarding the comparison to Strisciuglio et al. 2020, it seems to me that this paper have already studied the effect of the proposed architecture on the robust accuracy of the network on MNIST and CIFAR10. So the authors need to clarify what the contribution of the current paper is in more detail and in comparison to this paper. \n\nOther comments: \n* please clarify in the figure-3 caption which dataset was used \n* please explain the logic behind selecting a larger push kernels and its relation to the neurophysiological observations from Li et al. 2012\n\n[1] Richard Zhang. Making convolutional networks shift- invariant again. In ICML, 2019.\n\n[2] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec- tive kernel networks. 2019.\n\n[3] Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" arXiv preprint arXiv:2006.16241 (2020).\n\n[4] Lee, Jungkyu, Taeryun Won, and Kiho Hong. \"Compounding the performance improvements of assembled techniques in a convolutional neural network.\" arXiv preprint arXiv:2001.06268(2020).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Method works on CIFAR-10 but not ImageNet, questioning the significance of the results",
            "review": "The authors propose to enhance the robustness of convnets to image corruptions by a biologically inspired push-pull mechanism consisting of a convolutional filter coupled with an up-sampled version of the same filter with sign-flipped weights. The authors investigate how replacing the first convolutional layer or the first block in ResNet and DenseNet architectures with this push-pull layer affects generalization to CIFAR-C/P and ImageNet-C/P compared with the original model architecture. Furthermore, the authors investigate the effect of combining this model architecture with existing data augmentation techniques known to improve generalization to corrupted images.\n\n\nStrengths:\n+ Interesting idea taken from biological vision\n+ Simple approach that does not increase number of parameters\n+ Clear improvement on CIFAR-C/P\n\n\nWeaknesses:\n- The proposed push-pull layer does not lead to an improvement on ImageNet-C/P\n- Experiments on ImageNet are not described well\n- Several control models are missing\n\n\nDetails on major issues (all of which would need to be addressed in a convincing manner for my score to change):\n\n(1) The effect due to push-pull on ImageNet-C is negligible. A 1% improvement is tiny compared to typical effects of other methods on ImageNet-C. This failure to generalize to ImageNet undermines the significance of the results substantially. At the very least I would have expected a discussion of this issue. For people to get excited about this method despite the failure to scale to ImageNet, the paper would have to make a very convincing case why push-pull is still the right direction to be pursued and how it might eventually be scaled successfully to large-scale datasets like ImageNet.\n\n(2) Improvements in FR on ImageNet-P (Table 3) could be caused exclusively by anti aliasing, not by push-pull. It is (a) not clear whether the models with anti aliasing that are listed in this table include the push-pull layer (and, if so, which version: -pp or -b1?) or not. If they include it, an ablation is missing where the authors use only anti aliasing but not push-pull. If they don't include it, I'm not sure what's the relevance of these three models in the context of the present paper. These questions would have to be clarified in the text and the ablation models should be added to Table 3.\n\n(3) As the authors state, the push-pull component acts as bandpass filter, which seems to be in line with the generalization improvement being high for corruptions/perturbations with high-frequency components. To what extend are the improvements presented in the paper due to the specific design of the push-pull layers and to what extend they could they have been achieved by just bandpass-filtering the corrupted input images, using the original architectures. This would be an important control.\n\n\n\nMinor comments\n\n- Eq. (1): How was \\alpha=1 chosen? Did you experiment with different values for \\alpha? Also, how does the value of h affect the results?\n\n- Fig. 2 would benefit from a color bar in the cross-correlation plots, as it is unclear if white or black corresponds to a high value\n\n- Fig. 3: The representation of the individual results makes it hard to assess the overall effect. Is there a net improvement due to the additional anti aliasing? It looks like for many, but not all, combinations of architectures and metrics the Rect-2 version looks better, but it's not clear by how much on average and what's the effect for the other two filter versions.\n\n- What does CE in Table 3 refer to? In Appendix B, only mCE and rCE are defined\n\n- Color bar on the kernels in Fig. 2 to better compare the push / pull kernels\n\n- Understandability of Figures 3, 4 and Tables 1, 2, 3 would be improved by introducing all abbreviations used in the caption (eg. -pp, b1, WRN-28-10-b1, Tri-3, etc.)\n\n- Section 3.1: “The decision of having pull kernels larger than the push kernels is motivated by neurophysiological findings (Li et al., 2012).” Since the reader of this paper might be not familiar with neurophysiological literature, you could expand a bit on the motivation of using larger pull kernels\n\n- Eq. 4 should read f(x_j) instead of f(j)",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nInspired by push-pull circuits in early visual cortex and retina, the authors develop a push-pull module that can be inserted into deep nets. They hypothesize that since push-pull circuits may act as band-pass filters, their module might control for domain shifts caused by noise. They test on clean and corrupted versions of CIFAR and ImageNet and find that the module hurts in evaluations of the former but helps in evaluations of the latter.\n\nI'm impressed by the simple implementation of the module, but as I mention in the weaknesses, I have trouble evaluating the impact of this method since it *does worse on clean data*. I do not feel like this paper presents a clear direction for how to address robustness beyond throwing different regularizations at the problem. If that *is* the only way forward, then perhaps there's a need for a systematic evaluation of regularizations wrt robustness.\n\nStrengths:\n\nThe implementation looks to be fairly efficient. There's a lot of evaluations of different corruption datasets. The figures are pretty.\n\nWeaknesses:\n\n\"These can be considered as average cases of adversarial attacks and are very common in vision tasks.\" I take issue with this sentence. Are you saying that domain shifts of p(data) should be considered adversarial attacks? This is by definition of [1] not the case right?\n\nIt looks like the \"push-pull\" that you're implementing is a tuned surround suppression. I'm curious about the distinction (if any) you're drawing between the two? The important difference to me is that surround suppression is recurrent and purely modulatory, whereas the operation you're implementing does not appear to be. It looks like the cited Li et al., 2012 is examining tuning curve broadness rather than spatial extents of \"push-pull\" as it is presented here, although perhaps I'm misreading it. Regardless, I'd like more discussion of this issue, as there is a large literature on extra-classical receptive field mechanisms and effects in brains + their extensions to models that would need to be included as related work.\n\nUnfortunately the proposed model does worse on clean data. This is a real problem with proposed solutions to robustness. More often than not, they exhibit worse performance on clean data and marginal improvements on noisy data. The problem with this is that the parsimonious explanation for the new algorithm is that it is just another regularization for object classification. What if you train with more kinds of augmentations? How does this compare to every off-the-shelf regularization that is out there? There is an enormous search space for constraining model expressivity, and there is no evidence here that the proposed method is doing something above-and-beyond this.\n\nThe authors seemingly exploit this huge regularization search space by combining their method with anti-aliasing. They find marginally worse performance on the clean datasets but marginally better \"robustness\". I'm struggiling to figure out if there's a path forward for robustness in deepnets, or if it's just a matter of combining arbitrary regularizations.\n\n\nDo you think that the sparsity you're seeing in Fig 4 is reflecting the expressivity trade-off that you're observing in your results? Is there a way to get the best of both worlds? Should the observed sparsity and its impact on performance be interpreted as if models were trained with l1 regularization? How is the push-pull different than a weight decay or activity regularization?\n\n\n[1] Szegedy C et al. 2013. Intriguing properties of neural networks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "What about receptive field size?",
            "review": "Summary\n-------\n\nThis paper takes inspiration from neurophysiological observations of the presence of push-pull cells in the brain. These cells act as non-linear band-pass filters where the excitatory response of one cell is combined with the inhibitory response of another with a larger receptive field. In the paper, a model of such push-pull layers is used to replace the convolutions in a number of near state-of-the-art models including ResNet and DenseNet variants. The authors' demonstrate that their networks using inhibition are more robust to various noises in the input than the baseline models. Further experiments demonstrate that the performance can be further enhanced by incorporating other recent advancements in network design and data augmentation. In addition it is observed that the proposed technique has a secondary effect of increasing weight sparsity in the network.\n\nPositives\n---------\n\n- I like the idea behind this paper --- it's good to see an observation being taken from neuroscience and incorporated into a network model.\n- Overall (with a caveat about my main concern below regarding the interpretation of the results), the evaluations are well thought through, utilising a range of near state of the art models with appropriate datasets and metrics.\n- Aside from a slight lack of detail on the model (see questions below), which is easily corrected, the paper itself reads very well.\n- The proposed approach might actually be particularly useful not because of robustness, but because of the sparsity it induces.\n\nConcerns\n--------\n\nI have a major concern around the fairness of the evaluation of the models with inhibition against the baselines; it appears to me that each push-pull module has twice the receptive field size of the convolution it replaces. In a deep model the effective receptive field size (in terms the size of the set of pixels in the input image that influences a single pixel in a feature map) will be greatly compounded by this change. \n\nIt isn't at all clear to me that larger effective receptive field size would not be the greatest contributor to increased robustness --- indeed, one would expect that by taking into account more samples from the input would naturally lead to higher noise robustness. Note also that including the anti-aliasing filters would further increase the effective RF size, which would also explain the results that are presented.\n\nIn order to address this question one would probably have to make baseline that had the same receptive field sizes as the push-pull network, whilst having the same number of parameters. Probably this would mean learning small kernels and performing upsampling in the same way the pull layer does. I could believe that the push-pull mechanism itself might introduce an inductive bias that gives the network an improvement over the 'fair' baseline, although I would want to see experimental evidence for this.\n\n\nRationale for score\n-------------------\n\nI'm leaning towards rejecting this because whilst I like the idea presented, I strongly believe there is an alternative interpretation of where the model's gains in performance arise from. This issue clearly needs addressing with experimental results before it should be accepted.\n\nQuestions during rebuttal period \n--------------------------------\n\nObviously it would be nice if my concern regarding the fairness could be addressed --- perhaps the authors already have evidence at hand to show that increased RF size is not the contributing factor to the results? Or perhaps they have a convincing argument as to why RF size is not an issue? If not, I suspect it is probably unreasonable to ask that the authors do additional experiments during the rebuttal period as I would expect it would take a substantial amount of time.\n\nI do have some additional (small!) questions that I couldn't find covered in the paper:\n\n- Do the convolutions in the push-pull modules have biases? If they do, how is this dealt with?\n- Why is a factor of 2 used for the difference in receptive field between the push and pull? (I get that there is evidence in biology for them being different, but why exactly 2?)\n- Is there biological evidence that the weights of the push and pull units should be tied? Increased weights notwithstanding, would it be better to learn both units independently?\n- If a baseline model has a convolution with K outputs, does this get replaced with a push-pull module with K outputs? Does this also mean that the number of learnable weights is equivalent between the baselines and PP models?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}