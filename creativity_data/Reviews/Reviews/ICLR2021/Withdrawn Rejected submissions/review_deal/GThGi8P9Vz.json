{
    "Decision": "",
    "Reviews": [
        {
            "title": "An incremental algorithmic paper without strong motivation",
            "review": "Summary. This paper proposes a unified framework for stochastic proximal gradient descent called ProxGen, which introduces both momentum and preconditioner to stochastic proximal gradient descent. Furthermore, it can cover a broad range of stochastic gradient descent algorithms. Under this framework, the authors give a convergence analysis of stochastic proximal gradient descent for both nonconvex loss and nonconvex regularizer.  Moreover, they apply ProxGen to training sparse or binary neural networks with promising results. \n\nOverall. I think their proposed framework is interesting with the solid theoretical analysis. However, their motivation is not clear enough, and the application of ProxGen is narrow in their work. Thus I think this work should not be accepted to ICLR in the current version. \n- Although able to give a theoretical analysis of stochastic proximal gradient descent with both preconditioner and momentum, the benefit of introducing preconditioner is not supported in theoretical analysis, as the convergence bound is the same $\\mathcal{O}(\\frac{1}{\\epsilon^4})$ compared with previous works (e.g., Davis et al. 2020). \n- The introduction of nonconvex regularizers needs more motivation. Empirical results of nonconvex regularizer compared with convex ones (e.g., $\\ell_1$ regularizer) is not supportive enough either. \n- Their application is too narrow (only limited to sparse and binary neural network training),  which does not agree with their title ``for deep learning''. \n\nMain concerns. \n- The introduction of preconditioners to stochastic proximal gradient descent is not well motivated. While the author highlights the difficulties of analyzing the convergence of stochastic proximal gradient descent with preconditioners, such modification's benefits are not clear in their theoretical analysis. Although the authors have demonstrated the superiority of ProxGen in experiments, it would be better to analyze the impact of preconditioners in the theoretical analysis (I think it should affect the constant in convergence rate). \n- The proposed nonconvex regularizer also lacks enough motivation, and the empirical results seem not very supportive. Although the authors claim that nonconvex regularizer can be more effective than convex ones, \nthe experiment results in Table 3 indicate the opposite result: $\\ell_1$ regularizer works better than other regularizers except on ResNet-32 (where the performance is very close). The experiment results in Fig 2, 3, 6, and 7 also seems to have the same trend, and perhaps the authors should find some other applications that are especially suitable in deep learning for nonconvex regularizers. \n- The authors only apply ProxGen to training the sparse and binary neural network, which seems too narrow considering ``for deep learning'' in their title. Perhaps the authors can add more deep learning applications that involve (possibly nonconvex) regularizers in the objective, or it would be better to change the paper to focus more on sparse or binary neural network training. \n\nMinor issues.\n- The theoretical analysis seems unable to cover neural networks with batch normalization (BN) or dropout (DO). Moreover, the authors also do not indicate whether they use BN or DO in their experiment details. \nSince the use of BN and DO can significantly affect neural networks' performance, the authors should add those experiment settings in the appendix. \n- The experimental results in Figures 2 and 3 (also in Figures 6 and 7) are not easy to understand. Specifically, when I try to compare different regularizers' performance, I found it very difficult to read across different subfigures (and even with different scales in the y-axis). Perhaps the authors can find a better way to present these results. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary: \nThe authors studied the training of regularized neural networks where the regularizer can be non-smooth and non-convex. They proposed a unified framework for stochastic proximal gradient descent, PROXGEN, that allows for arbitrary positive preconditioners and lower semi-continuous regularizers. The authors presented the first proximal version of ADAM and a revised version of PROXQUANT. They also analyzed the convergence of PROXGEN and show that the whole framework enjoys the same convergence rate as stochastic proximal gradient descent without preconditioners.  \n\n\nComments:\n1 . The authors proposed PROXGEN for stochastic proximal gradient descent with non-convex non-smooth regularizers and preconditioners. Compared with previous works, the major improvement in this paper is the extension to enable positive preconditioners.\n\n2 . The authors show that the proposed PROXGEN algorithm enjoys the same convergence rate as stochastic proximal gradient descent without preconditioners. It would be best if the authors could show the advantage of using preconditioners in PROXGEN compared with previous works in the convergence rate.\n\n3 . Previous studies have shown that there exists a non-convergence issue in the standard Adam algorithm.\n\n\"On the convergence of adam and beyond.\" ICLR (2019).\n\nEssentially without using an extra max step for the preconditioner part, Adam may diverge in certain cases. Here the author directly extend stochastic proximal gradient descent on to Adam and showed its convergence, the authors might want to show the readers why such a non-convergence issue does not exist in the proximal setting.\n\n4 . The following work shows that unifying SGD and AMSGrad(Adam) lead to better result with convergence guarantees in the nonconvex setting\n\n\"Closing the generalization gap of adaptive gradient methods in training deep neural networks.\" IJCAI (2020).\n\nAlso the following work shows a tight convergence rate for the adaptive gradient method in the nonconvex setting.  \n\n\"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n\nThe authors might also want to comment on these works.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors oversell the content a little bit. ",
            "review": "This paper oversells a bit. This paper combines the ideas of momentum and the proximal regularized quasi-newton method. The analysis of this method relies on the Lipschitz gradient assumption and the objective function is even assumed to be Lipschitz continuous (C-3) (ii). These conditions are unlikely to be satisfied in the neural network setting. However, it seems to me that the title and the introduction say that the algorithm is tailored for deep learning. This is of course not the case, though it is possible to apply this algorithm heuristically to the training of deep learning. As for the algorithm, the only difference is to involve the momentum term compared to Stephen Becker et al., ‘On Quasi-Newton Forward-Backward Splitting: Proximal Calculus and Convergence’.  \n\nThe regularizer can not be an arbitrary lsc function, once you choose Frechet subdifferential for analysis. The Frechet subdifferential \\partial f(x) can be empty, say f(x) = -|x| at x = 0.\n\nI do not understand the last paragraph on page 5. The approximate Hessian Ct and the regularization parameter \\delta is set by users, it is doable to set this matrix to be positive definite. Why do you need to verify it for a specific application? Whatever the application is, this matrix should be the same since it is chosen by the users. \n\nI think in Corollary 1, it should be 1/\\sqrt{T}. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This submission is an extension of proximal-sgd for nonconvex nonsmooth composite optimization. Both theory and experiments are a bit weak.",
            "review": "This manuscript considers solving a special class of nonconvex composite optimization via adaptive gradient methods, in which the function is the summation of a nonconvex smooth loss and nonconvex-nonsmooth regularizer which allows easily computable proximal mapping.  Convergence to the stationary point (to be more precise, Frechet subdifferential containing the zero vector) is proved. Empirical studies on training sparse and binary neural network seems to justify its effectiveness. The comparison of the proposed ProxGen and Prox-SGD is also considered. I have following comments. \n\n1. The update in line 8 of Algorithm 1 requires a closed form of proximal mapping w.r.t. a potentially nonconvex nonsmooth regularizer. Although it holds for some examples (\\ell_0 regularization, SCAD, MCP, etc.), It is hard to hold in general. This assumption is a bit strong.\n2. It is not immediately clear to me what the technical challenge of proving the convergence in Corollary 1. From my perspective, the proof is adapted from (Xu et al. 2019) and (Chen et al. 2019), in which (Xu et al. 2019) considered solving the same class of problems using Prox-SGD, while (Chen et al. 2019) considered solving nonconvex smooth problems using adaptive gradient methods.\n3. It is not shown in theory that the proposed Prox-Gen is better than Prox-SGD in (Xu et al. 2019).\n4. The large minibatch requirement in Corollary 1 is not practical for deep learning applications.\n5. Assumption (C-3) (i) is kind of weird. Why is it a reasonable assumption?\n\nExperiments:\n1. In the experiment of training binary neural network, why the baseline of Prox-SGD is not compared?\n2. Only CIFAR10 experiment is reported. It would be desired to see the results on ImageNet, since ImageNet is usually used as a benchmark dataset in quantization literature.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}