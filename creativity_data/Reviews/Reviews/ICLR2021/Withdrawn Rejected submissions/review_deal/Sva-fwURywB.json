{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This submission builds on recent work by Bengio et al. (2020) to learn disentangled representations of causal mechanisms. The main innovation in this work is that the authors propose to use compare the generalization gap between a model A->B and its causal inverse B->A to determine the true causal mechanism. \n\nReviewers  are in consensus that this paper in its current form is not ready for publication. Aside from issues with writing, and comparatively weak experimental results, the reviewers noted significant technical problems in the writing and proofs. As noted by R3, Proposition 1 mentions \"unbiased estimator of correct causality direction\", which is a nonsensical combination of words in absence of a definition of the causality direction as a random variable. Moreover the proof in the appendix only shows that that 0 = KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A)) when P(A|B) = Q(A|B) and P(B) != Q(B). As R3 also notes, the authors also do not clearly distinguish between the true data distributions (which are not known), datasets that are sampled from these distributions, and approximations to these distributions that are learned from finite sample sets.  \n\nThe metareviewer would suggest that the authors either improve the level of precision of the formalization of their approach, and/or more explicitly position it as an empirical study, in which case more substantive experimental evaluation would strengthen the contributions."
    },
    "Reviews": [
        {
            "title": "Interesting approach, structure of paper could be improved",
            "review": "The paper presents a novel approach for determining the causal direction between two random variables $A$ and $B$. The approach is based on the assumption that the conditional distribution $P_{A \\rightarrow B}(B \\mid B)$ does not change between the train and transfer distribution. As a result, a model that predicts the correct causal direction $A \\rightarrow B$ should generalize better from the train to the transfer distribution compared to a model predicting the wrong causal direction $B \\rightarrow A$. While previous work has proposed to use this insight to determine the causal direction by comparing the adaptation speed, this paper proposes to directly measure and compare the generalization performance. The results indicate that the proposed approach leads to the same performance in terms of causal relation prediction, but that it is more sample efficient and faster.\n\n\n\nStrengths:\n* The paper presents an interesting approach for determining the causal direction between two random variables. While the generally accepted wisdom is that a better causal model will lead to better generalization, their approach follows the reverse logic: if a causal model achieves better generalization, it is interpreted as the better (i.e. correct) causal model. The paper builds on this logic proposed by Bengio et al. (2020) and proposes a more efficient measure for the generalization performance of a model.\n* The results show a clear advantage of the proposed approach in terms of sample efficiency and speed.\n* The presented proofs seem correct and are presented in a clear way.\n\n\n\nWeaknesses:\n* The paper builds heavily on the previous approach presented by Bengio et al. (2020). While this is not a problem per se, the paper should be written in such a way that it can stand alone. In the current version, it is not possible to follow the experimental results without consulting the paper by Bengio et al. (2020). It would be very helpful to add a description of the data and models used, as well as the measures compared, e.g. what is $\\sigma(\\gamma)$?\n    * Consulting the paper by Bengio et al. (2020), it seems that the results in Figure 2 can only be interpreted as correct if the model also predicts the correct causal direction. I assume this is the case for the presented models, but it is not stated anywhere.\n* As far as I understand (Algorithm 2, line 3), one model is trained to optimize the loss for both causal directions simultaneously. That approach seems rather counterintuitive to me. I would imagine that this could lead to the model learning a conditional distribution that does not match either causal direction well, effectively rendering the proposed approach unworkable. Wouldn’t it be better to train one model for each causal direction separately and to compare the generalization performance between these models?\n* It would be interesting to see how the proposed approach compares to other methods for causal discovery.\n* The paper could be improved in terms of clarity:\n    * There are a few grammatical errors, especially the articles “the” and “a” are missing quite often.\n    * The structure could be reformatted to be more efficient. At the moment, the introduction spends a lot of time on a very general introduction of causality. I would recommend reducing this and instead, focus more on the proposed approach. Based on the current introduction, I could understand how the previous approach worked, but not the proposed one.\n    * The title should be more concrete. \n\n\n\nOther comments:\n* What happens if $S_G = 0$?\n* There is a mismatch in Figure 1.b): While the caption states that the x-axis shows the “computation time in seconds”, the x-axis in the plot is labeled as “Number of episodes”.\n* The paper could benefit from a clearer statement of all the assumptions that are necessary for the presented approach. For example, I would expect the approach to rely on noiseless dynamics and a fully observed setting without hidden confounders.\n* Regarding noiseless dynamics, the paper presents an experiment in the appendix showing how performance degrades when increasing the standard deviation of additive noise. For this experiment, it would be interesting to know the standard deviation of the underlying data, otherwise, it is unclear how the noise relates to that.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel idea but lack of empirical comparison, particularly for real datasets. ",
            "review": "The paper proposes an efficient approach to learn disentanglement representation causal mechanism, based on a generalization loss between a training data set and transfer data sets. Empirical studies show it achieves better sample and computation efficiency than a previous work (Bengio et al., 2020). \n\nPros: \n- Improvement over the existing baseline is significant\n- Theoretical statements on the results, particularly on biased and unbiased estimators, offer good understanding for the approach. \n\nCons:\n-  Currently, I find the setting and results on real datasets rather weak. It would be great if authors could demonstrate such a causal direction identification problem in some real setting, with the distribution shift occurrence between different sets of data. \n- There are no comparison with other methods. It would be great if authors could show baseline methods used in (Mooij et al., 2016) perform, by considering the train alone and train+transfer data together. In (Mooij et al., 2016) datasets, representation learning are not needed; for other datasets when representation learning is needed, existing criterions of these baseline methods could replace the generalization loss. With these results, it would be better to judge the effectiveness of generalization loss. \n\n\nOther Thoughts:\n- Besides the marginal distribution shift, P(A|B) and P(B|A) may both change significantly, even one of them is the correct causal direction, as the underlying distribution may shift.  Can authors comment how their approach could handle such situations?\n- In representation learning, is the decoder never used?\n- since Section 2.4 is only a small part of paper, it may also be worth to also test the causal direction identification without the representation learning part, for example, in linear cases. \n- The statement \"this work in causal representation learning will be helpful for more advanced artificial intelligence\" is rather vague and pompous. \n\nRating: \nTo me the rating is borderline and I'm not yet convinced it is above the acceptance threshold, hence I left it as 5. Hopefully authors' rebuttal could address my concerns. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, clearly not yet baked",
            "review": "This paper describes an approach for learning a representation U(X,Y), V(X,Y) of data (X,Y) such that U and V are causally meaningful, and U causes V. The approach relies on observing data from two domains, P and Q, where P(V| U) = Q(V |U) (reflecting the causal structure). The approach is a modification of Bengio et al. The main new idea is that the objective function can be tweaked by replacing a KL divergence term with a term involving domain-shift induced generalization errors.\n\nThe paper covers an interesting subject, and the idea to directly use domain generalization error learn causal structure is exciting. However, the paper is clearly not yet baked. The writing is generally poor, and the key ideas have not been formalized. The main ideas of the paper are unclear, as are the validity of the core insights. In particular, there is a fundamental confusion between estimators and estimands.  The paper will require extensive revision and formalization before it's ready for public consumption.\n\nBelow I including some free form thoughts I had below. These give a flavor of my issues with the paper, and will hopefully provide some direction for the authors. However, I stress that these issues are only examples. The paper requires extensive revision.\n\n1. the phonemes/acoustics example just shows that structured learning may be beneficial, it doesn't rely on causality\n2. the explanation of equation 1 is unclear (or, possibly, wrong). Presumably, the actual aim is to compare the likelihoods of two distinct models corresponding to A->B and B<-A. The paper argues that the higher complexity model will have lower likelihood. But this is false in general; a very flexible model will simply memorize the training data.\n3. in general, the paper suffers from a confusion in the notation between population parameters and finite-sample estimators. The notation generally suggests the former, but the prose, appealing to sample-complexity handwaving, suggests the later. \n4. Is proposition 1 meant to be a theorem? The text doesn't reference any estimator, much less an 'unbiased' one.\n(update: I read the appendix, and the intended statement is simply, \"If P(A|B)=Q(A|B), but P(B|A)!=Q(B|A) then 0 = KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A))\"\n5. In section 2.3, \\script{L} has changed from denoting log-likelihood to denoting risk (incorrectly called loss in the prose)\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}