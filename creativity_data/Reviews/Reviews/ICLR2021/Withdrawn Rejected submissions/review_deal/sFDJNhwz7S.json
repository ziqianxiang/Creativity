{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Thanks for your submission to ICLR.\n\nThis paper presents an extension to Deep Hashing Networks that utilizes angular similarity, and show improved results using the proposed method.  The reviewers were somewhat mixed on this paper, with two of three reviewers on the negative side.  Some reviewers appreciated that the paper was easy to follow and well written, though one reviewer felt that the paper's writing and presentation could improve.  A big concern about the paper expressed by multiple reviewers was that the paper was incremental, in that the main architectural difference seemed to be a change in loss function over existing work.  Unfortunately, the reviewers were fairly unresponsive to attempts to get them to respond to the rebuttals offered by the authors.  \n\nUltimately, I took a look at the paper and found it to be borderline.  I do think the contribution is a bit limited, particularly as it is in an area which has seen many papers over the years (and thus has a high bar for new work).  However, with some additional work this paper could definitely be acceptable.  I think it could use an additional round of editing and review, and I'd encourage the authors to submit this paper to another venue."
    },
    "Reviews": [
        {
            "title": "Theory needs to be justified more and some questions on the experiments",
            "review": "The authors consider the problem of learning a hash function such that semantically similar elements have high collision probability.  They modify the approach Deep Hashing Networks (Zhu et al., 2016) with a new loss function. Rather than use a sigmoid based loss function, the authors argue that a loss function based on angular similarity and SimHash would be better. Specifically, they use the probability of SimHash collisions as a loss function. They then experimentally verify their method on synthetic data from a Stochastic Block Model distribution, image data (CIFAR-10 and ImageNet), and text data (OSCAR). They show improvements over related methods.\n\nOverall, I found this paper to be incremental compared to previous work, such as (Zhu et al., 2016). The theoretical contributions are fairly weak. Why is the relation to SimHash useful? The authors need to better justify their choice of loss function. Additionally, how does the method compare to using a sigmoid loss but changing the temperature parameter so that the loss function doesn't saturate as quickly?\n\nThe authors do improve over related methods in the experiments. However it is not clear if this is due to the choice of loss function or the use of negative mining. The authors should also improve the clarity of how their metrics are defined. Is precision, recall simply based on the elements that collide together?\n\nTo summarize, I think this paper needs to better justify their use of loss function in theory and also perform ablation tests in the experiments before it can be accepted.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose an approach for learning embeddings such that approximate nearest neighbor search becomes an exact hashing problem.  The experiments show that their approach is promising. ",
            "review": "########\nPros\n########\n\n- The paper is well written and easy to read\n\n- The authors have explained the background, motivation and prior work quite comprehensively\n\n- The idea of learning embeddings such that nearest neighbors have the same hash is interesting. \nBecause of this property, nearest neighbor search becomes a simple hash lookup. \n\n\n\n########\nCons\n########\n\n- The authors claim, \"We extend semantic hashing methods to problems with **substantial label noise**\", but there doesn't appear to be any specific modeling to handle label noise in Section 3.\n\n- In practice, for IR, can this method support a ranking of the items based on query similarity? For example, if for query q, documents A, B, C are relevant in that order (most relevant to least). It appears that this method would all assign the same hash to A, B, C and therefore, retrieve them, but not in any particular order?\n\n- Experiments don't appear to be necessarily a fair comparison that can show the merits of this approach conclusively.\n\n\n########################\nComments / queries\n########################\n\nQ1 Section 4.1,  Table 1 shows very low numbers for DHN. Is that a typo, given that DHN performs quite well in all the other experiments. \n\nQ2. Section 4.1, Table 1: Can you also provide the numbers for LSE without false negatives for fair comparison with the baselines?\n\nQ3. Section 4.2, Table 2: It appears that LSE is able to perform well and beats the baselines in many cases. Can you provide the statistical significance p values?\n\nQ4. Section 4.3, Table 3: LSE has a substantially lesser number of clusters, making it difficult to isolate the merits of the method vs. outcome that is a result of just lesser clusters. E.g. Recall can be better just by returning a larger cluster, which is probably the case if the number of clusters are lesser. Can you provide a fairer comparison, for example, by lowering the number of clusters of the other methods or some other way?\n\n\n########\nTypos\n########\n\nSection 4.2: \"100 images per category it test query set\"\n\nSection 4.2: \"Results are shown in Table 4.1.\" --> Table 2?\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes an angular-based metric which tries to pull similar objects together and dissimilar ones apart. Three different experiments, i.e., a toy example, an image retrieval task, and a co-occurrence modeling task, demonstrate the benefits of the learned embeddings.",
            "review": "+++Pros.  \n-----The idea learning embeddings via angular similarity is interesting and important for exact hashing retrievals; and the design of experiments on different domains to validate the proposed method is worth encouraging.\n\n+++Cons.  \n-----This paper just devised the angular similarity, whose contribution is limited for ICLR.  \n-----There are some technical minors or typos, such as the binary cross entropy loss in Eq.~(5) (should be “+\\beta(1-y_{ij})log(……)”).  \n-----There are several grammar minors, such as “…utilizes cosine similarity to as the crossing layer between the two halves…” in the beginning of Section 3, “We measure precision, recall and F1-score on with the data generating factions as the target.” in Section 4.1 above Table 2; and some confusing expressions, such as “in Table 4.1.” (there is no Table 4.1.).  \n-----Better replace beta with \\beta, lambda with \\lambda, and keep them identical in texts and figures. And better organize figures with subtitles.  \n-----At the end of each equation marked (1), …, (10), there should be appropriate punctuations, such as commas.  \n-----The authors concluded that “The learned representations show superior performance in the exact hashing retrieval setting”. But the results in Table 2 does not support it.  \n-----Besides, the authors mentioned that … a “word2hashes” model which is novel to the best of the authors’ knowledge …, and I do not agree with such statements.  \n-----Actually, I think Figures 4-8 and Table 5 should be regarded as the main texts; and if so, this paper exceeds the page limit.  \n\n+++Conclusions.  \n-----Based on the above pros and cons, I think this paper is interesting, but the contribution is limited; thus, I would make a REJECT recommendation.  \n\n+++Suggestions.  \n-----Need careful writing and presentation.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}