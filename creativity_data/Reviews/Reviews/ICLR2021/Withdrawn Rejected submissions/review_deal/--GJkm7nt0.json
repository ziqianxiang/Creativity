{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes an online distillation method for efficient object recognition. The main idea is to employ a binary student network to learn frequently occurring classes using the pseudo-labels generated by a teacher network. In order to identify rare vs frequent classes, an attention triplet loss is used. The proposed scheme is empirically evaluated on CIFAR-100 and tiny-imagenet datasets.\n\nThe major and common concern from reviewers about this draft is the quality of presentation, which has made it difficult to read and understand the ideas and their underlying motivations. While specific instances of this were mentioned by the reviewers, and responded by the authors, the readability issues go beyond these instances. In fact, I could independently observe presentation issues different from those mentioned by the reviewers. For example, in Eq (1), what is eta (never defined before), what is the loss function here for which the gradient update leads to Eq (1)? The proof also is not clear and seems to have issues. For example, in Eq (13) what is the meaning of multiplying two vectors? If it means dot product, it should be denoted more clearly, e.g. as w^T w^* or <w,w^*>. In Eq (12), can alpha be negative? If not, should be clarified why, and if it can be negative, then (<w^*,w>)^2 >= (alpha n)^2 does not need to hold, but this inequality is used in Eq (14) anyway.\n\nOverall, I think the submission can benefit from an overhaul of the writing. I encourage authors to resubmit after improving on that."
    },
    "Reviews": [
        {
            "title": "This paper proposes a knowledge distillation framework to achieve efficient object recognition when adapting a large model to a new scenario. It features in an adaption module to learn a binary student network and an attention triplet loss to guide the student network to focuse on semantic information. It mimics the data adaptation scenario by splititing the datasets CIFAR-100 and Tiny-Imagenet into inliner and outliner classes and shows improvement over the baseline on multiple metrics.",
            "review": "- The idea is interesting to focus on the frequent labels by distilling a special binary network. But the technical details are not clearly presented and important experiments are missing. Overall, this paper does not reach the acceptance threshold.\n\nDetailed Comments:\n\n- Why is this method an 'online' distillation considering that the teacher network is pretrained and fixed? How do you define offeline distillation and online distillation?\n- \"The OL detector uses the softmax output of the BSN...\" How is the softmax output is used? Do you have a separate class named outlier apart from all the object categories? It should be made clear.\n- Why does BSN converge faster than RvSN considering that BSN is a binarized version of RvSN? What is the retionale behind this?\n- In the experimental part, only comparison to baseline methods are provided, but not to state-of-the-art efficient object recognition methods in the literature. Various methods have been proposed recently to improve the efficiency of an object recognition network, such as distillation, pruning, tensor decomposition, quatization. There should be comparison to these methods as well as baselines. Besides, the performance on Imagenet (not only tiny imagenet) is a common practice in the literature, but is missing in this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting use of triplet loss in distillation, paper organization and writing should be improved.",
            "review": "The proposed work trains a teacher-student network using an online distillation paradigm. The student is a binarized network (BSN) trained to be accurate on frequent classes. An attention triplet loss is employed to improve the accuracy of the BSN and its ability to detect outlier classes. Faster convergency of BSN vs Real Valued Student network is claimed. A new metric to evaluate the actual gain in network efficiency is proposed. \n\nStrengths\n- The idea of using the attention triplet loss to improve the quality of attention maps of the BSN and thus increasing BSN accuracy is interesting\n- Experimental evaluation is very thorough\n\nWeaknesses\nThe main weakness of this work lies in the presentation and organization of the paper.\n- It is not clear how outlier detection is performed and why it is needed for the approach to correctly function. \n- It is not clear why at inference time false outliers are then processed by the teacher network. Is this because of the online setting? The setting should be better specified.\n- Sentence \"In conventional knowledge distillation, the attention map of the BSN can focus on the background\neven when the attention map of the TN emphasizes the semantically meaningful regions of the image.\", should be followed by one (possibly more) citations supporting this claim.\n- Lemma 3.1 should be followed by a sketch proof, being one of the main contribution, with the full proof in the appendix (as it is already).\n\nRelated work missing:\nExisting literature binarizing CNN in distillation exists:\n[a]Distillation Guided Residual Learning for Binary Convolutional Neural Networks, 2018\nTriplet losses are used in distillation:\nTriplet Loss for Knowledge Distillation, 2017 (arxiv), IJCNN(2020)\n\n\nThe main reason for my score regards the overall presentation of the paper. The main two contributions are the faster convergence and the use of triplet attention loss. The lemma should have been better highlighted with a sketch proof or at least some intuition so that readers not willing to sift through the appendix could get a grasp of it. The triplet attention loss should be better framed in the introduction (see above). Finally some of the mechanisms of the approach are unclear (how to get the OL score, why the TN must evaluate fp of the BSN) and so on. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper but needs some clarification ",
            "review": "The authors tackle the problem of efficient object recognition and outlier detection using online distillation. They propose to complement the standard distillation loss with a triplet loss using attention maps from the teacher to help the student focusing on the relevant part of the input images. They also provide a large set of experiments to validate the proposed approach. The paper is overall well written.\n\nI have two main concerns about the paper:\n- If my understanding is correct, the *hard-positive* samples used to compute the attention triplet loss are selected from the most probable class assigned by the TN while the *hard-negative* are using the second most probable class. Why is it desired for these first and second most probable categories to have different attention maps? Intuitively, if they are semantically similar it would make sense to focus on the same regions of the image to be able to distinguish them from each other.\n- Even though discussed in section A.3, I think that the claim \"BSN is adaptively trained only on IL class images without any knowledge of OL class images\" is incorrect (first paragraph of page 3). Since the TN is used as supervision and as a guide for the SN representation, a lot of knowledge about the OL classes can still be transmitted to the SN.\n\nAnd a few questions:\n- How does the SN perform when directly trained on all classes? It would give a tight lower bound to the performance of ENVISE, since some architectures used for the SN are already very competitive (especially Resnet 18 and 50).\n- It is surprising to see the BSN outperform the RvSN. Is there any reason for which the real-valued network can't converge to the same level of performance as the one achieved by the BSN?\n- Since table 5 shows that similar performances can be reached using a Resnet-18, why using a Densnet-201 for most of the experiments? It seems that it can improve the performance in terms of GiE but would just increase the overall inference latency of ENVISE. \n\n\nMinor remark:\n- Legends of fig 2b, 3 and 4 are too small.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Knowledge distillation for efficient object recognition at scale with attention supervision. The methodology is not well motivated.",
            "review": "A knowledge distillation framework is proposed for efficient object recognition. In this framework, the teacher network (TN) performs high accuracy prediction while two student networks (SN) mimic the prediction from TN. The first SN learns from TN while the second SN is a binarized form (BSN) of the first SN.  The design is made for online inference is that the BSN first recognizes the image, leaving the rare category objects to be recognized by the TN. Furthermore, an attention supervision scheme is proposed to enhance the CNN prediction by focusing on meaningful image content.  The proposed method has been validated on CIFAR-100 and Tiny-Imagenet.\n\nWhile the distillation for fast recognition and attention supervisions sound interesting, there are a few issues to make the current manuscript not convincing on the contribution side. The details are listed in the following:\n\n1. The major issue is on the motivation side of this framework design. In the paper, a BSN and a TN are both adopted to recognize objects. The reason claimed is for the efficient recognition for rare category objects. However, there is no motivation for why distillation can indeed solve this problem.  This reviewer agrees that objects from category distribute unevenly and class imbalance occurs during training. How class imbalance correlates to knowledge distillation is not clear. A common strategy is to adopt a focal-loss-based loss function to reduce contributions from easy samples. Suppose we want to use two CNNs for cascaded recognition, which is the core idea in this paper, the typical choice is to collect ordinary category objects for the first stage training and rare objects for the second stage training. A pure distillation from the TN does not ensure the differentiation of ordinary and rare categories.  \n\n2. The claim of online distillation is weird. Normally the CNN is trained offline and there is no sign of online model training in the manuscript. As online distillation appears everywhere in the manuscript, this reviewer doubts whether this claim is suitable for illustrating the CNN training process.\n\n3. The attention scheme is from Grad-CAM, which back-propagates CNN prediction to formulate attention supervision terms. The number of categories indicates the number of back-propagation during one training iteration. This computational complexity is tremendous compared to the fast convergence claim of BSN. There shall be some analysis on how to compute the attention maps efficiently in practice.\n\n4. In the experiments, the comparison shall be made to the sota object recognition methods (ResNet, EfficientNet based methods) besides outlier detection methods, As the proposed method focuses on efficient object recogntion. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}