{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for \"Searching for Robustness: Loss Learning for Noisy Classification Tasks\"",
            "review": "The authors of the paper propose an interesting framework to automatically search for a loss function robust to noise labels in order to tackle the important problem of learning with noisy labels. The proposed algorithm involves the use of the \"Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) \" algorithm to iteratively search for an optimal set of parameters of a 10-degree Taylor polynomial to serve as the robust loss function. A wide range of NNs and datasets with manually injected noise are used to search for the optimal parameters. The authors of the paper empirically evaluate the performance and demonstrate the effectiveness of the proposed method.\n\nIn general, the paper is clearly written. I also find the proposed idea very interesting and unexplored, at least in the context of deep learning to the best of my knowledge. The proposed method seems technically sound, and empirical performance is encouraging. \n\nThere are two weaknesses to the paper in my opinion. Firstly and most importantly, I find the empirical evaluation section to be rather weak. Despite the apparent improvements shown in Tables 1,2 and 4 of the proposed method on datasets like MNIST and CIFAR, the improvements could have been potentially due to overfitting to these datasets (based on my understanding of the paper). As a justification, the authors of the paper mentioned that the meta-learning was done using MNIST and CIFAR, and a clean validation dataset is needed to search for the robust loss function. As such, I find the comparison in Tables 1,2, and 4 to be quite an unfair one, based on my understanding, even though VGG and ResNet are not used during meta-learning of the loss function. Thus, the only meaningful result, in my opinion, is the experiment on the \"Clothing1M\" dataset. Yet, the authors of the paper only conducted one experiment with a single type of architecture, without any error bars either. Due to this weakness, I am not yet convinced at the moment that the proposed method can be useful in general. \n\nMoreover, the paper is also lacking in terms of ablation studies. While the paper conducted an experiment to study the generalization of the learned loss function across noise-levels, additional ablation studies are also very crucial in my opinion. For instance, does the kind of noise injected into the datasets during meta-learning important? What if the noise model used during meta-learning is significantly different from that of the actual noisy dataset of interest? I think that additional experiments and ablation studies would be helpful in making the paper stronger. \n\nSecondly (and less importantly), despite the interesting application, I find that the paper is somewhat lacking in terms of novelty. Specifically, the proposed method is a direct application of a widely known algorithm in the context of noise-robust learning. \n\nAs such, due to the reasons mentioned above, I recommend rejecting the paper for now. \n\nQuestions:\n1. What if no clean validation dataset is not available? Can the proposed meta-learning algorithm still work? \n2. It was stated in the paper that \"conjecturing that training on a difficult task would be sufficient for generalization to other tasks with diverse noise conditions\". Why do you think this is the case? What is the intuition/rationale? \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting problem but some unclear parts need to be clarified",
            "review": "This paper studies an interesting problem of learning robust loss function in classification under label noise. Instead of previous works that propose certain loss functions from the optimization or information theory point of view, this paper explores a data-driven approach to loss design and search for the noise-robust loss in a family of them using Taylor polynomials. An algorithm and experimental results on some benchmarks and a real-world dataset are also provided. \n\nThe learning to learn approach of automatically constructing robust loss functions to label noise is interesting to me. But I have the following concerns:\n\n-The space of potential loss functions is an important factor to consider. There is a trade-off between capacity and efficiency. In the paper, the authors choose the Taylor series approximations and choose beta equals to 4. How to select the parameter in practice? Is this beta optimal for different tasks? Providing more theoretical justifications or experimental explorations may be helpful.\n\n-It is natural to score the searched loss functions according to their validation performance. But it seems to be a lot of burden in practice if we have to train a new model on noisy data and evaluate its clean validation performance to score each loss function during our search. It seems that the authors assume the loss function be class-wise separable and thus the gradient does not depend on x. My feeling is that this assumption does not have much practical meaning, could the authors explain more on this?\n\nOverall the paper studies an interesting problem but some unclear parts need to be clarified.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Taylor polynomial representation based robust loss searching algorithm ",
            "review": "This paper applies the CMA-ES method to the noisy label learning scenario to find the proper loss in the class of Taylor series function space. To make the learned loss function generalize to different network architectures, the authors apply the domain randomization strategy to evaluate the performance during the meta-learning stage. The authors use the Taylor polynomial representations as the candidate set to find proper loss for noisy label learning. The proposed method is a straightforward combination of the CMA-ES method and domain randomization strategy. Experiments show that the proposed method is robust to various label noise.\n\n\n\n\npros\n1. The authors use the Taylor polynomial representations to construct the candidate loss function set for noisy label learning. Such a design is interesting, and the empirical studies show its effectiveness.\n2. The proposed method could generalize to a variety of deep neural network architectures, which has practical potentials in real-world applications.\n\n\ncons\n1. The proposed method is quite straightforward. Although the experiments show empirical evidence, but there lack theoretical guarantees for the proposed method.\n2. Some symbols in section 2 is not well-defined mathematically. And also, the detailed description of the proposed method is missing, making this paper not self-contain.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea, but need to improve",
            "review": "This paper proposed a method to learn the loss functions against label noise deep learning by meta-learning methodology. To be specific, the authors proposed to parameterize the loss function with multivariate Taylor polynomial, and then learn the parameters in the polynomial using evolutionary algorithm within the meta-learning framework. The experiments across several datasets and network architectures showed improved performance of the proposed method against label noise, which verified its robustness. The paper is well written and easy to follow.\n\nMy major concern is about the way of parameterizing the loss function. In machine learning practice, a loss function generally has only one valley. However, this feature might not be guaranteed by Taylor polynomial parameterization. In fact, a polynomial function could have multiple valleys, or even do not have minimum. I notice that in Fig. 1, the learned loss function has the desired property. Therefore, it might be better further discuss about it.\n\nAnother problem is about the order of Taylor polynomial. The authors used the forth order polynomial and mentioned it is a good trade-off between modeling capacity and meta-learning efficiency. However, in my opinion, this is a hyperparameter that should be further analyzed and discussed, especially with comprehensive empirical evaluations.\n\nIt can also be observed that the performance of the learned loss function is not stable. Specifically, it does not consistently achieve the best performance, and even perform significantly worse than the hand-craft loss functions in some cases. Considering the computational cost for searching the loss function via meta-learning, the practical usability of the proposed methodology is limited.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The presentation needs to be improved",
            "review": "This paper focuses on learning with noisy labels. The authors aim to search the loss functions which are robust to noisy labels. They first exploit Taylor polynomials to parameterize the search space of loss functions, and then use Covariance Matrix Adaptation Evolutionary Strategy to discover the robust loss functions. Experimental results verify the effectiveness of the proposed method. \n\nThe idea is somewhat novel. However, the writing of the paper needs to be improved. The logic and technical details of the paper are difficult to understand and follow. The description of the algorithm and experiment in the paper is insufficient. \n\n- The logic of the paper is not clear enough. The description of the method is mainly placed in Section 2. However, this part is very hard to understand. The authors miss the explanation at a high level. I suggest that the authors can give an overall algorithm flow, and add some necessary analyses. It is very important for readers to understand this paper. \n\n- The Taylor loss may have some side effects? As shown in Figure.1, when the predicted probability is high, the loss is also large. If the noise rate is small, it may cause underfitting to clean labels. How does the proposed method solve this problem? \n\n- The explanation of the experiments confusing. The first is the experimental results. I find the reported results are very different from the results in the original or existing papers, such as [1][2][3]. I understand that the experimental environment may affect the performance of the methods. However, the performance of the baselines is almost very bad. I recommend that the authors elaborate on the implementation details of these methods. The second is the analysis of experimental results. For example, why GCE and FW fails in many cases? Why the proposed method outperforms the baselines on a range of tasks? The authors should add such analyses to make the paper convincing to readers. The third is some errors in the experiments. For example, SCE achieves the best performance (Resnet18 CIFAR10 in Table 1). Its result should be bolded. \n\n- The AutoML approach is also used in [4]. Please analyze the difference between the proposed method and the method in [4]. \n\nTo sum up, the idea of this paper is somewhat interesting. However, this paper needs a lot of improvement. I therefore recommend “reject”. \n\n[1] Yisen Wang et al. Symmetric Cross Entropy for Robust Learning with Noisy Labels. In ICCV, 2019. \n[2] Xingjun Ma et al. Normalized Loss Functions for Deep Learning with Noisy Labels. In ICML, 2020. \n[3] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NeurIPS, 2018. \n[4] Quanming Yao et al. Searching to Exploit Memorization Effect in Learning with Noisy Labels. In ICML, 2020. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}