{
    "Decision": "",
    "Reviews": [
        {
            "title": "Some Interesting Experiments and Results, But Need A Better Theoretical Understanding Of Approach",
            "review": "This paper considers a large scale application of continual learning to the taskonomy dataset which contains 11 relatively diverse tasks with over 300,000 training examples. A large portion of the paper (section 4) is devoted to establishing that standard replay may outperform approaches like A-GEM in this setting. Standard replay based approaches matching or surpassing the performance of GEM based algorithms had already been established in [1] and [2]. I think this section was overblown in the paper as it could have been positioned more as just a particular finding of the empirical results that come later. I also am confused about the set of performance metrics used on the taskonomy dataset. It seems they are all relative metrics about changes in performance over time with none actually speaking to the values of performance experienced, which could have a big impact on the interpretation of these relative metrics. For example, it would be important to provide something like the acc metric used in later experiments. \n\n[1] \"Learning to Learn Without Forgetting By Maximizing Transfer and Minimizing Interference\" Riemer et al., ICLR 2019. \n[2] \"On Tiny Episodic Memories in Continual Learning\" Chaudhry et al., 2019. \n\nThe other finding of this section of the paper is that the representation drifts more over time in replay buffers with a small amount of memory. While on some level it is nice to actually show this, this is again well known in the community. My understanding is that the authors focus on these ideas as a way of empirically setting up motivation for their approach. However, the motivation behind the particular details of the approach were not adequately established in my view. There have definitely been past papers that have started with the idea that replay is a strong baseline for continual learning and proposed that to aid with drift some sort of knowledge distillation or gradual learning based approach should be added. The approach of this paper seems to fit that general paradigm while not actually theoretically comparing to alternative designs for such a setup. Additionally, while the authors are arguing that this approach is particularly suited to large scale continual learning datasets like taskonomy, it is not made clear how the characteristics of this dataset and datasets like it are related to the poor performance of A-GEM, the strong performance of replay or the strong performance of CAR. I think additionally clarity on the reasons behind these phenomena could really improve this paper by convincing readers that this is a trend that is likely to continue on other large scale datasets moving forward. \n\nAnother concern of mine about the presented experiments is whether the comparison of CAR with other replay based approaches is really apples to apples in the submission. While the memory may be of the same size in terms of number of sample, were there any precautions taken to make sure the approaches are comparable in terms of bytes stored? I am just concerned that by storing extra side information (even when compressed) with a limited budget of experiences, this will increase the storage cost per experience making it more comparable to methods that do not store side information but utilize more total experiences. A related question is how you position your approach in comparison to approaches that compress the input space themselves as in [3] and [4]? I guess compressing the input space may be hard for complex images, but on the other hand your storage per example would go down in comparison to generic replay rather than going up as in the approach of this paper. \n\n[3] \"Scalable Recollections for Continual Lifelong Learning\" Riemer et al., AAAI-19. \n[4] \"Online Learned Continual Compression with Adaptive Quantization Modules\" Caccia et al., ICML-20. \n\nOverall, I think this paper has some nice experiments and reports some compelling results. I just learn towards rejection in its current form due to a lack of understanding about what exactly is driving various more surprising aspects of the results and lack of significant novelty in the proposed approach. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper Review",
            "review": "Summary: \n-------------------\n\nThe paper analyses the performance of Experience Replay (ER) in Continual Learning. The authors show that ER is still a competitive baseline, and further propose to not only replay input / output pairs but also intermediate layers to prevent feature drift. The authors show that the memory burden of activation storage can be mitigated by compressing said hidden activations. \nThe authors investigate both the offline CL setting on the Taskonomy dataset,  and the online CL setting on more standard benchmarks. \n\nPros: \n-------------------\n+ The paper is well presented and easy to follow. The proposed method is clearly described\n+ The empirical analysis is strong and oriented towards realistic settings. The authors explore challenging datasets, and give a clear picture of the memory overhead for each method. \n+ The proposed method seems to address the problem of feature drift shown by the authors, and with minimal memory overhead. \n\nCons:\n------------------ \n- The setting explored for the Taskonomy task revolves on requiring the task id at test time. When the alg. has access to this information, parameter expansion methods are designed to have 0 forgetting, a property that ER does not have. The argument made by the authors is that expansion-based methods can have a a) growing training memory footprint and b) growing inference cost.  \nI would very much like to see the following simple baseline, where you store the checkpoints from the SGD baseline at every task. In other words, you train a single model continually, but save the state of the model on every task switch. This way, you can deploy a different model for each task (b) while only training one model at a time (a) (while leveraging a potential forward transfer between tasks). By design this would have no forgetting yet would not suffer from the downsides of expansion based methods. \n- There is some previous work looking into replaying activations and / or memory-efficient ER methods for continual learning. Namely: \n\n[1] replays compressed activations for memory efficient ER\n[2] looks at replaying logits instead of x,y pairs\n[3] does rehearsal on intermediate layers\n[4] and [5] learn a compressor for memory efficient ER \n\nI understand there are differences with the current work, however I think this line of work should at least be discussed in the related work. \n\n+ [1] Hayes, Tyler L., et al. \"REMIND Your Neural Network to Prevent Catastrophic Forgetting.\" arXiv preprint arXiv:1910.02509 (2019).\n+ [2] Buzzega, Pietro, et al. \"Dark Experience for General Continual Learning: a Strong, Simple Baseline.\" arXiv preprint arXiv:2004.07211 (2020).\n+ [3] Pellegrini, Lorenzo, et al. \"Latent replay for real-time continual learning.\" arXiv preprint arXiv:1912.01100 (2019).\n+ [4] Caccia, Lucas, et al. \"Online Learned Continual Compression with Adaptive Quantization Modules.\" arXiv (2019): arXiv-1911.\n+ [5] Riemer, Matthew, et al. \"Scalable recollections for continual lifelong learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n\nComments:\n------------------ \n+ the \"forgetting\" definition usually excludes the last task, as there has not been any forgetting on it yet. \n\n\nOverall I think this paper has potential, and I'm ready to bump my scores if the authors can include the baseline above and position themselves in the related work. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review  Effectiveness of Memory Replay ",
            "review": "The paper proposes a large scale continual learning experiment on the Taskonomy dataset. Several baselines are run. The experiments show that memory replay obtains very good results. A new method based on compressed activation replay is proposed.\n\npros:\n- experimental setup on Taskonomy dataset is interesting. \n\nnegs:\n- the papers main contribution (based on title) is not sufficient for ICLR. Effectiveness of memory replay has been noted before Lampert, iCaRL; Chaudry 2019 'Tiny...', gDumb (ECCV2020), IL2M, Belouadah (ICCV 2019 ) \n- for being an evaluation paper, I think more methods should be compared. \n- the proposed compression method is not discussed in the context of other relevant works on compression for CL (REMIND Your Neural Network to Prevent Catastrophic Forgetting, and Lucas Caccia, ICML 2020).\n\nThe task on Taskonomy is very interesting and I would motivate the authors to perform a more complete study. The proposed compression method for CL is probably best published separately. There is no main contribution that merits publication in ICLR. \n\nminor:\nWould like to see a citation to support the claim that catastrophic forgetting gets worse with deeper/wider models. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough material for a full paper",
            "review": "\n**Summary** \n\nThe paper studies different continual learning methods in a relatively large scale vision benchmark -- Taskonomy. The authors show that a very simple replay buffer based baseline, ER, outperforms other CL methods on this benchmark extending similar results on smaller benchmarks by the other authors. Further, the authors propose to replay task activations alongside input-output pairs to further stabilize the performance of the CL algorithms. The experiments are reported on Taskonomy, Split CIFAR, and Split miniImageNet benchmarks. \n\n**Positives**\n1- The paper is well-written and easy to follow. \n2- The empirical study of CL benchmarks on Taskonomy is quite nice. It suggests that the ER is still the toughest baseline to beat.\n\n**Negatives**\n\n1- While studying different CL methods on a relatively larger benchmark is nice, I am afraid that seems to be the only contribution of the paper and as such, I am not sure, it warrants a full paper submission. For the other contribution about the activation replay see my comments below.\n\n2- **Activation Replay (CAR)**: Replaying activations along with the input-output pairs is a very obvious idea and something that has already been tried (see [activation_replay](https://baicsworkshop.github.io/pdf/BAICS_8.pdf) and similar such).\n\n3- **Comparing CAR with ER**: The memory required for CAR is greater than the ER replay buffer. A fair comparison would have been to fix the replay buffer size (say in MBs) and then see how well each method performs. Currently, the numbers are misleading. Also, it seems that the more you regularize with the activations the less flexibility you give the model to learn on a new task which results in a higher performance drop. The authors have already acknowledged this as the weakness of CAR. All in all, I don’t think CAR is useful on top of ER. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}