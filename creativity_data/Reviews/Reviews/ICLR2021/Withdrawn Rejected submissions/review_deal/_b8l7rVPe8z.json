{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a transferable adversarial attack method for object detection by using the relevance map. Four reviewers provided detailed reviews: 2 of them rated “Ok but not good enough - rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur these major concerns and agree that the paper can not be accepted at its current state."
    },
    "Reviews": [
        {
            "title": "interesting idea with good transferrability",
            "review": "This paper presents a method for adversarial attacks on object detectors by exploiting relevance maps that are originally intended for model interpretation. Unlike most of the existing methods that attack detection scores directly, the proposed approach focuses on suppressing the relevance map associated with target objects by image perturbation. The idea is interesting and demonstrates good transferability on the tasks of object detection and segmentation. \n\nThe paper is mostly well written and easy to follow. The adversarial object dataset can also be helpful to the research community.\n\nThe main downside of the paper is that some of the comparisons are not apple-to-apple in the experiments. For example, the proposed approach applies update techniques (i.e. Translation-Invariant) to improve transferability. However, it is not my impression that this was done on the baseline methods, which leads to unfair comparison.\n\nSome technical details need to be better articulated in the paper. For example, there is no mentioning of the adversarial loss function and how it is optimized. As pointed out in [2], attacking CNN interpretations is not trivial. Without the details of how to update the gradients of the relevance map, It would make reproducibility difficult.\n\nWhile focusing on a different problem,  the proposed approach shares some similarities with methods designed to attack model interpretations such as [1] and [2], which should be discussed as related work.\n\n[1] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681–3688, 2019\n[2] Xinyang Zhang, Ningfei Wang  Hua Shen, Shouling Ji, Xiapu Luo,Ting Wang, Interpretable Deep Learning under Fire, USENIX Security '20\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "large room for improvement",
            "review": "**Summary**:\nThis work proposes to attack object detectors by targeting their relevance maps of the different detected objects. The proposed RAD attack shows better black box transferability across different detectors on MSCOCO dataset. The relevance maps are calculated based on SGLRP act as an attention mechanism to the attack to focus on relevant regions in the more meaningful image and hence produce more transferable attacks. \n\n**Strengths** :\n- Good attack performance and transferability between detectors, which poses a security threat for SDV applications that use object detectors\n- Eight different detectors and three segmentation models are used in the RAD attack, which shows good generalization.\n\n**Weaknesses**:\n- Missing important references [a,b,c]. All of these works attack object detectors and target transferability.\n- The paper is poorly written and ambiguous. Variables are introduced without proper definitions. It is not clear how to obtain the gradients in eq(3) with respect to the relevance maps.\n- No use of the proposed dataset. The authors propose a new dataset of adversarial objects but never mention or showcase the dataset's usefulness. A straightforward way to show the dataset's usefulness is by performing adversarial training and making robust detectors against the proposed attacks. \n- No enough ablation is performed. The only ablation to the proposed method is in table 7 regarding the way to pick the detection target. The relevance maps based on LRP are expensive and worse than recent saliency maps like CAM and grad-CAM. The attack budget $\\epsilon =16$ picked in the experiments is not justified ( it might be big or small for attack success ), and a plot of mAP vs. $\\epsilon$ for different detectors would give more information about the effect of the attack.\n- All the attacks in the paper are performed on YOLOv3 and transferred to other models. It would be more informative to show transferability matrices of attacks performed on all models and transferred to all others.\n- The novelty of the proposed methodology is limited. While the use of relevance maps to improve the transferability of attacks on object detectors is novel, no proper explanation is provided. The attacks are based on PGD, and the relevance map is adapted from SGLRP. The paper offers no theoretical results or exciting insights. \n\n\n\n[a] Huang et al. \"Universal Physical Camouflage Attacks on Object Detectors\", ( CVPR 2020)\n[b] Wu et al. \"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors\", (ECCV 2020 )\n[c] Xu et al. \"Adversarial T-shirt! Evading Person Detectors in A Physical World\" (ECCV 2020).\n\n\nMinor issues :\n- Many grammar mistakes:\" because they possess multiple-output.\", \"Among the classification attacks and detection ones, cross-domain attack (Naseer et al. (2019)) is the most effective, but RAD is more aggressive\" ..etc.\n- No question marks in titles 3.1-3.5.\n- Table 2-5 could have been visualized better by using a bar chart, for example, to observe the relative performance of attacks and defenses. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The Topic is not very valuable",
            "review": "Summary:\nThe paper found a new way, called Relevance Attack on Detectors (RAD),  to generate high transferability adversarial examples against detectors by suppressing the multi-node relevance.\nMoreover, a dataset generated by this attacking method is introduced.\n\nPros:\n1. The designed RAD is new and technically sound.\n2. The experimental results on multiple detectors, like YOLOv3,  RetinaNet, Mask R-CNN, etc. are promising.\n\nCons:\n1. It's not surprising that the object detectors can be attacked together.  One naive way is that using the ensemble attack by average the negative CE loss like the paper \"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors\" proposed. So I think some baselines need to be added in experiments. \n2.  By imposing diversity transformation on input image can also improve transferability (\"Boosting Adversarial Attacks with Momentum\"). Similar rules also can be applied in detectors.\n3. I cannot see a very significant value of the generated dataset. Like in the image classification dataset, one can generate adversarial examples easily, but there is not an individual dataset that only contains generated adversarial examples generated by one specific method. Some famous dataset like imagenet-C (\"Benchmarking Neural Network Robustness to Common Corruptions and Perturbations\") is designed by natural perturbations.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A relevance map proposed for adversarial attack on object detection. The overall contribution seems limited.",
            "review": "A transferable adversarial attack method is proposed for object detection. A relevance map is used to discover the common weakness of existing detectors. An adversarial dataset for object detection is created for experimental validation.\n\nThe relevance map proposed indeed derives from SGLRP as mentioned in Sec. 3.3. The only modification is to change a single node target t to the average of a set of nodes.  As the major contribution claimed is on the relevance map, the minor modification makes the contribution limited.\n\nThe relevance map is computed by back-propagating the relevance R from the final layer to the input following rules as illustrated in Sec. 3.3. However, the proposed method is claimed to perform a black-box transferable attack. Also, the gradients are utilized to update the relevance map. It is thus not clear how this relevance map correlates to the black-box transferable attack.  \n\nIn the experiments, the proposed attack method shall be compared to sota detection or segmentation-based methods (e.g., EfficientDet, Cascade-RCNN, Libra-RCNN). \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}