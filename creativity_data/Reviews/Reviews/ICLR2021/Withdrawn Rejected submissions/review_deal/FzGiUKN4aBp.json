{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting paper but some clarity issues",
            "review": "The authors consider the problem of learning classifiers that generalize to out-of-distribution datasets. For example, this problem will see image data taking by a particular photographer in a particular location and would want to generalize to image data from another photographer in another location. They start from the Invariant Risk Minimization approach of (Arjovsky et al., 2019). Here, the goal is to learn a  set of \"invariant features\" $\\Phi$ and a classifier $w$ such that it minimizes the average risk over all environments while $w$ is an optimal predictor given the invariant features $\\Phi$. They then build on the information theoretic perspective of IRM derived in (Ferenc, 2019). They then develop the Inter-Environmental Gradient Alignment algorithm, which they argue is superior to IRM because it doesn't require to directly learn the invariant features $\\Phi$. The authors then experimentally demonstrate that their approach is superior for out-of-distribution problems based on the MNIST dataset.\n\nI found this paper to be overly dense. The notation is hard to follow and intuition is in short supply. I was not able to understand the problem setting until I looked at the related work (Arjovky et al. 2019) and (Ferenc, 2019). Section 3.2 is in particular unclear, with dense notation that only seems partially defined.\n\nAdditionally, I do not fully understand the derivation of the IGA algorithm. There are many approximations without justifications in its derivation and the function itself is not completely interpretable. The authors should (1) justify all approximations in the derivation of the IGA loss function and (2) develop an intuitive understanding of the loss function.\n\nThe authors then do experiments on variants of MNIST where spurious correlations due to environmental factors are included. They show that  their approach performs the best on unseen environments. While the experiments are nice, I think the clarity of the theoretical sections of the paper need to be improved before I can fully appreciate the experimental results.\n\nAdditionally, I see in (Arjovky et al. 2019) that they achieve 70.8 accuracy on Colored MNIST, while you have them achieving 0.592. What is the cause of this change?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need stronger experimental results than MNIST and more baselines",
            "review": "This paper provides theoretical justification on the MIP principle (proposed by Ferenc) for invariant risk minimization. The authors then develop an algorithm IGA to solve it and demonstrate the effectiveness on Colored-MNIST and its extended version.\n\nThe theory part looks solid. I did not checked details of the proof, but they seem to be correct. My major concern is that in the current version, the experiments are only done on MNIST-based datasets. Can the method be extended to more complicated datasets such as CIFAR10-C, which is a common choice for OOD generalization?\n\nI would also like to see more baseline algorithms if possible. As the authors mentioned, there is another line of research that formulates the out-of-distribution robustness problem as a distributional robust optimization (DRO) problem. The authors did not compare with these methods in the experiments. I was wondering if the authors could provide comparison against some of the DRO methods such as:\nVolpi et al. Generalizing to Unseen Domains via Adversarial Data Augmentation\n\nAs a side note, I saw another ICLR submission https://openreview.net/forum?id=BbNIbVPJ-42, which claims that invariant risk minimization often does not outperform standard ERM. In the author response phase, I am curious to see if the proposed method can resolve some of the issues method by the other submission.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A good idea that is not sufficiently mature for publication.",
            "review": "The paper re-formulates the domain generalization problem as the problem of finding a maximal invariant predictor, i.e. a predictor that minimizes the worst-case error across a set of environments where the phenomenon under consideration behaves slightly differently. This is a very promising idea, which generalizes earlier work of Peters et al. and Arjovsky et al. \n\nUnfortunately, the required theory is presented in a totally incomprehensible way. This already starts right at the beginning (first paragraph of section 3.2): the authors introduce a function sigma that depends on features Phi and the environment E and then claim that this function remains invariant when the arguments are replaced with other variables. I see no reason why this should be the case. Moreover, the new variables arise out of the blue, without any explanation of their meaning. It is also unclear how this relates to Darmois' (1953) result, which states that probabilities can be re-parameterized through deterministic functions and independent noise variables -- the present paragraph mentions no probabilities where this could apply. \n\nThe confusion continuous when variable E is split into a part that is independent of the features Phi and its complement. Why is this useful or even possible? For the sake of argument, consider the situation where one collects data in different countries, so that E is the respective country label. How would one split up this variable?\n\nThe subsequent example of the animal photographer does not make things clearer. The authors say that the photographer's preference cannot influence the the animal's appearance (which is plausible) and conclude that the two variables are independent (which is clearly wrong -- photographers might prefer beautiful animals). Likewise, they claim that animal appearance is independent of the background, which is also wrong -- fish typically have a water background, whereas foxes have a forest background.\n\nProposition 3.1 is potentially an important result, but also unclear as stated. In the assumptions, f* depends on the features Phi, whereas in the conclusion, Phi does not occur at all -- how is it possible for the two versions of f* to be related? The proof in appendix B does not help understanding either and is once again incomprehensible (and is hard to find, because results are numbered differently in the appendix).\n\nAnother mysterious result is the Inter Gradient Alignment algorithm, which forms the basis of actual optimization in practice. In the crucial step of the proof in appendix C.2, it remains unclear how equation (39) follows from equation (38).\n\nWhile it might be possible for the reader to mentally fix these problems, there is no motivation to do so once one sees the experimental results. The C-MNIST dataset is constructed such that the optimal invariant predictor achieves 75% accuracy across all environments. However, the OOD accuracy of the proposed method in figure 7 (b) does not even come close: it is only 60%, barely better than guessing. There is no point in arguing that this is superior to the IRM baseline -- it is simply not good enough to be useful.\n\nIn summary, the paper is not ready for publication in its present form.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some unrealistic assumptions for the theory and unclear experimental settings",
            "review": "This paper proposes a method to cope with out-of-distribution (OOD) generalization with the so-called maximal invariant predictor (MIP). The paper has great practical relevance and is of interest to this community. The paper is positioned well in the context of other papers in OOD. \n\nHowever one of the major deficiencies that this reviewer can tell is in Proposition 3.1, Definition 3.2, Corollary 3.3, and Proposition 3.4 the main results. They are all justified under the unrealistic assumption that for all \\epsilon_\\phi, there exists some \\tilde{\\epsilon}_\\psi such that some conditional independence relation holds. The authors motivated this by an example involving the photography of some animal. While that example is apt, one notes that the statement must hold *for all* \\epsilon_\\phi. This is arguable very restrictive. This is because one has to have the conditional independence relation hold for all physical appearances, tones, lightings, rotations and scales of the photographed animal. In reality, this assumption is difficult to be satisfied exactly. What the authors can do is to consider a more relaxed notion of \\delta-conditional independence for some \\delta>0. This more realistic setting can provide more insights into the optimality of the solution at a cost of a more complicated solution. \n\nDarmois' (1953) result is known as the \"functional representation theorem\" in information theory. But this terminology came later. \n\nThe optimization algorithm seems heuristic. For example, the authors should specify a rule-of-thumb to tune the step size \\alpha and regularization parameter \\lambda. It is not mentioned in the experimental section, but this is important for other practitioners who would be keen to use the IGA algorithm. \n\nThe approximation in (9) should be carefully justified. It seems like \\alpha must be small for the conditional MI to be approximated by the Fisher information. Authors should comment on this and whether \\alpha used in the experiments is so small such that this approximation is applicable. \n\n\"twig\" --> \"tweak\"\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}