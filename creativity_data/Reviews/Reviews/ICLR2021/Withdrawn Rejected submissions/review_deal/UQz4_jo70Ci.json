{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "All three reviewers initially recommended reject.  The main concerns were:\n1) weak technical contribution and insight [R1, R2, R3, R4];\n2) incremental novelty (another variation of SiamFC) [R1, R2, R3];\n3) unconvincing experiment results against missing SOTA [R1, R2, R3];\n\nThe author's response did not assuage these concerns."
    },
    "Reviews": [
        {
            "title": "A siamese short-term tracker with a cross-guided network is introduced.",
            "review": "The architecture of the tracker is standard siamese. The novelty is at a technical level, modules of the \"cross-guided\" type have been proposed. It does bring an improvement, but not to the state-of-the-art level.  There is no significant insight, training, updating novelty or theoretical. Recent short-term trackers output segmentation, the proposed tracker outputs a bounding box.\n\nThe performance of the tracker is evaluated on UAV, VOT 2018 and VOT 2019. UAV is saturated. The performance VOT 2019 is worse than state-of-the-art, which is not reported -- the trackers selected for comparison do not include the best performing ones. It is not clear why  VOT 2020 was not included. \n\nOverall, this is \"yet another siamese tracker\", which is not sufficient for ICLR acceptance.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents a Siamese-based single object tracking method using attention mechanism both in channel-wise and spatial-wise for learning deep correlation between exemplar and candidate images. ",
            "review": "This paper presents a Siamese-based single object tracking method using attention mechanism both in channel-wise and spatial-wise for learning deep correlation between exemplar and candidate images. Extensive experiments on UAV123, VOT2018 and VOT2019 demonstrate the effectiveness of the proposed method, and the 35 FPS running speed indicates its competitive efficiency.\n\nStrengths:\n1. The idea of multiplying the weights of aggregated template feature to the search feature is interesting and rather intuitive, the experimental results on several datasets confirmed the effectiveness.\n2. This paper is well-organized and easy to follow.\n3. Comprehensive experiments including the appendix are carried out, and convincing.\n\nWeaknesses:\n1. Lacking important literature reviews on highly relevant works.\nThe proposed SOT method belongs to 1) anchor-free and 2) attention-based pipelines. There are several latest and popular works the author should mention in Related Works:\nSiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines   https://arxiv.org/pdf/1911.06188.pdf\nSiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking    https://arxiv.org/abs/1911.07241v2\nSATIN: Siamese Attentional Keypoint Network for High Performance Visual Tracking\nhttps://arxiv.org/pdf/1904.10128.pdf\nAll three works above are anchor-free methods, and the third paper SATIN adopts attentional mechanism for SOT.\nThe author compared the proposed method with SiamFC++ and SiamCAR in experimental results, but the review of these methods and explanation of difference in methodology are missing in this paper.\nAs a matter of fact, the most relevant work to this paper is SATIN which also adopted channel and spatial attention in Siamese architecture. The only difference is multiplying the attentive channel weights of exemplar to candidate, which is called cross-attention in this paper. \nThe author should give a more thorough review and explain the difference between these works.\n\n2.  The contribution of this work is rather limit.\nThe paper describes three contributions in section 1, however,  the second point (anchor-free and DIoU) is hardly the contribution of this work  but reimplementation of previous works, and the third point is only a report of experimental results. In fact, the only contribution of this work is introducing cross-channel attention to explicitly learn the correlation of template and search images, which is a small trick in network design. Compared to SATIN, which also adopts channel attention to template feature and spatial attention later,  this paper does not bring a substantial contribution to visual tracking community. \n\n3. The comparison with SOTA methods are not comprehensive.\nThe experimental results on UAV123 do not include SOTA method SiamFC++, similarly, results on VOT2018 do not include SiamCAR, and results on VOT2019 do not include SiamFC++ and SiamCAR. Moreover, the author should also compare the results with SATIN on both OTB and VOT datasets.\n\n4. The presentations of Figure 1 and Figure 4 are terrible.\nThe resolution of Figure 1 is rather low, and the author did not even explain which one corresponds to the challenge situation of three aspects in figure caption. It is hard to tell which one is better in case 1 (first row) and case 2 (second row) due to the low resolution.\nFigure 4 is rather confusing. The author did not explain the visualized confidence map belongs to which layer, and this attentive map is whether from template feature or search feature. I assume the second row on left side is from the subsequent frames, then what’s the meaning of both rows on right side, and why there is a blank gap between left and right?\nThe author should pay more attention to improve the figures in this paper, and the captions should be more thorough without ambiguity.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Reasonable method but contributions not enough",
            "review": "## Summary\n\nThis paper proposes a siamese network for visual tracking, namely SiamCAN, which utilizes cross channel attention, spatial attention, and anchor-free regression head. The method achives state-of-the-art performance on four visual tracking benchmarks. \n\n## Strengths\n\n +. The design of the network is reasonable, including using template's channel information to help the search branch to learn more specific feature, using spatial attention to aggregate location information from the depth-wise correlation, and utilizing anchor-free regression head to locate the object. \n\n+. The final results are good, and ablation study shows the effectiveness of these modules.\n\n## Weaknesses\n\n-. The technical contribution is weak. The main components, including channel attention, spacial attention and the anchor free network, are not new. This proposed method is quite similar to SiamBAN, especially the multi-head fusion and the anchor free network.  Furthermore,  the cross channel attention is more like a cross-correlation between the search branch feature and the pooling template vector. In this case, are the 1D conv layer and the sigmoid function necessary?  \n\n-. The description of the method is not clear. In Figure 2, there is only one correlation map but in Sec. 3.3 the author sad it has N correlation maps from different layers of the backbone. Furthermore, the index \"i\" of these N layers is conflict with the position index \"point(i, j)\", which would lead to misunderstanding. Also in Figure 2, the classification map's size is 25x25x1, but it is 25x25x2 in Sec. 3.3. The description of spatial attention is not clear. We can find Max/AvgPool in the figure but find no description. So how is the spatial attention used? The only word I can find is in related works \"CBAM (Woo et al. (2018)) utilizes both max-pooling and average-pooling to generate the merged attention, includes channel and spatial attention\". Is that the same? In Sec 3.2, \"Inspired by (Wang et al. (2020b)), we add channel attention and spatial attention into our network. \". However, Wang et al. (2020b) did not use spatial attention module. \n\n## Overall Rating\n\nDue to the weakness described above, I rate a \"Marginally below acceptance threshold\" score for the paper. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed method is effective and technically sound, but lacks novelty and originality",
            "review": "#### 1.summary\nIn this paper, the authors introduce an cross-channel attrention mechanism and the anchor-free box regression branch with Diou-loss to deal with the clutters during the tracking procedure.\n#### 2.strengths\nThe experimental results show that this method has excellent performance on several public benchmark datasets.\n#### 3.weaknesses\n- The novelty of the paper is deficient , the proposed method such as cross-attention mechanism [1], anchor-free regression [2, 3]  have been previously exploited in existing models.\n- The proposed method is obviously based on the SiamBAN[2], however, there exist much reduplicate content which has already mentioned in [2].\n- In experiments part, there are missing some important algorithms to compare, such as SiamBAN which is the baseline method for this paper.\n- the analysis of the proposed method is deficient.\n\n  *[1] Y. Yu, Y. Xiong, W. Huang, M. R. Scott, Deformable siamese attention networks for visual object tracking, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6728– 6737.*\n\n  *[2] Zedu Chen, Bineng Zhong, Guorong Li, Zhang Shengping, and Ji Rongrong. Siamese box adaptive network for visual tracking. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6668–6677,  2020.*\n\n  *[3] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines. In AAAI, pages 12549-12556, 2020.*\n\n\n#### 4. Correctness\nThe claims, method, and empirical methodology are correct.\n#### 5. clarity\nThe paper writing is clear and easy to follow. But there exist some grammar faults. Moreover ,in sec 3.3, there are two same subtitles and similar content, which may be confusing. Besides, in figure 4, the arrangement of the pictures does not seem to match the description text.\n#### 6. Relation to prior work\nIt is an incremental work based on the prior work.\n#### 7. Reproducibility\nYes.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}