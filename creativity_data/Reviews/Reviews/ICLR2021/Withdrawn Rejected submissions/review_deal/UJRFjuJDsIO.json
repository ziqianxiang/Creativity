{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers raised a number of concerns, but \nthe authors provided no rebuttal to the reviewers' comments.\n\nOne reviewer felt the experimental fitting was not thorough enough.\nSuppose one used layers of oriented bandpass filters, separated by\nnon-linearities, would that perform well on the task convnets are\ntrained on?\n\nThe AC doesn't agree with the arguments of R3.  I hope the comments of\nthe reviewers, particularly the many specific comments of reviewers R1 and\nR2, will be helpful to you as you revise the manuscript.\n\nThe AC feels a more thorough experimental evaluation, and following-up\non many of the suggestions of the reviewers will lead to a strong\npaper.  As it stands, however, with 3 recommendations for rejection (1\nweak), and only 1 weak recommendation for acceptance, we need to reject.\n"
    },
    "Reviews": [
        {
            "title": "Already known, not convincing",
            "review": "## Review\n\n### Summary\n\nThe authors propose an alternative explanation to the fact that Convolutional Neural Networks (CNNs) learn oriented bandpass filters. They suggest that it is due to the architecture of neural network and not the structure of natural images.\n\nThe two main contributions are:\n1. the alternative explanation\n2. empirical support that oriented bandpass filters are learned at each layer\n\n### Strengths\n\n* The paper is well written and easy to follow.\n* This is a good summary of Linear Operators and Fourier Analysis for non-expert\n\n### Weaknesses\n\n* To me, the contributions are not real. The paper intends at formulating a problem where there is none: the theory is known already and empirical evidences already exist (the repeated observations: yes CNN learns oriented bandpass filters and it is not a surprise). \n* It mixes contributions such as Olshausen and Field (1996), Karklin and Lewicki (2009) with the litterature of CNNs and the theoretical work of Bruna and Mallat (2013). The first two papers show how localized oriented bandpass filters emerge from unsupervised learning of a non-convolutional dictionary under sparsity constraints. This is a strong indication that these localized bandpass filters are appropriate to represent natural images and a strong confirmation for the use of convolution (a translation invariant representation). It appear that this is also what we observe when evaluating neuron receptive fields. Finally, the work of Bruna and Mallat pushes further the idea of invariance, trying to find a theoretical explanation of why stacking Linear and Non-linear operation provide an efficient representation. They build on wavelet transforms (oriented bandpass filters) because it is a translation invariant representation known to be sparse.\n* The theory is not rigorously presented. This is a good enough overview of a graduate course on Fourier analysis and linear operators.\n* Empirical evidences are not convincing. How would you get if you were fitting random 3x3 filters with oriented bandpass filters ?\n\n### Minor comments\n\n* The text in the figures is too small...\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A signal processing view of why CNNs learn oriented bandpass filters (for visual recognition tasks)",
            "review": "This paper gives a signal processing analysis of why CNNs learn oriented bandpass filters and provide empirical evidence by analyzing AlexNet, VGGNet, and ResNet trained on ImageNet.\n\nStrengths:\n1. The problem is indeed worth investigating. This type of analysis may have a profound impact on understanding (at least) many low-level (e.g. denoising, super-resolution) and middle level (e.g., boundary detection, texture analysis) algorithms based on CNNs.\n2. The authors give very nice references and credits to previous related work, which is not commonly seen when the whole community enters the era of deep learning.\n3. The signal processing analysis is a nice try.\n\nWeaknesses:\n1. The reviewer is not convinced by the analysis provided in the paper. The reviewer argues that the types of filters learned by CNNs should be task-dependent. For example, for image denoising, the CNNs should be oriented to (effective) low-pass filters; while for edge detection, the CNNs may be encouraged to learn high-pass filters. As for ImageNet (or generic) classification, the CNNs tend to learn oriented bandpass filters.\n2. The reviewer agrees that $w(x)$ should be localized, but it is not necessarily lowpass (as  Gaussian in figure 2).\n3. The distribution of $u_c$ learned by CNNs is not discussed.\n4. The reviewer suggests the authors empirically analyze some more CNNs optimized for different tasks to see whether the results hold across a wide range of tasks.\n\nMinor comments:\nIt is a very well written paper, and the reviewer enjoys it during the review.  \n  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Bandlimited concern is confusing for standard convnets (purely discrete-time formalism) : discrete sub-selection and distorsions are not necessarily associated with bandlimited processes",
            "review": "The continuous domain formalism is not adapted to what the authors would like to prove because convnet discrete kernels make inadequate and questionable, any inference through a continuous formalism: one can say anything we want, giving that one can choose the intrinsic underlying continuous kernel as we want. \nLet us develop:\nIn convnets, any convolution kernel h is associated with a discrete (countable) sequence of coefficients and this, necessarily (one cannot store an arbitrary infinite sequence on a computer). Coefficients of h can be seen as samples of an underlying continuous domain function g.\nThe filter corresponding to the discrete sequence h is H (Fourier transform of the impulse response h) and the filter corresponding to g is G. H is periodic even if G is not periodic. So, we can forget the other periods and focus only on a single period. Thanks to that, H is necessarily bandlimited (no need of explanation). For a pure “bandlimited property”, one needs to consider an infinite length sequence h. I hope the authors can prove the contrary because otherwise, there is no need for more explanation.\nI also found a paper [1] that shows that filters learned are restricted to more or less effective “band-limitation” mainly depending on the length of h (3x3 for VGG or up to 11x11 for alexnet). The conclusion is that if you use convolution kernels with short size (2x2 or 3x3 for instance), so only very few filters can have nearly zero magnitude in their Fourier transform and the bandlimited assumption is not really a sharp forcing to zero, but a weighted frequency penalization. This tends to prove my remark above. Maybe it is more convenient to use different terminologies than those used in standard signal processing based on Shannon sampling theory (from continuous to discrete, in contrast with the intrinsic discrete definition of standard convnets): remember that there is no unique way for making inference from discrete samples towards a continuous-time / continuous-spatial process. Bandlimited seems to make no sense when using a purely discrete-time formalism.\nMaybe the most important contribution of the paper could have been explaining the main orientation properties of the selective parts of the convolution filters. In that case, a proof of explanation is more straightforward when generating oriented training datasets and proving that the filters learned are oriented in the same directions.\n[1] A. M. Atto, R. R. Bisset and E. Trouvé, \"Frames Learned by Prime Convolution Layers in a Deep Learning Framework,\" in IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2020.3009059.\nI insist (update): band-limited is a very very inappropriate terminology for convnets. I am waiting for author's' answers (need for clarifications): the paper needs to be revised.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting paper with somewhat limited analysis experimental validation.",
            "review": "This paper presents an explanation of why convolutional neural networks learn oriented bandpass filters - as has been commonly shown for early layers in various ConvNet architectures. The main argument is that oriented bandpass filters are the eigen-functions of localized convolution operators and in order to span the input signal space (regardless of its a structure) the network will always learn these functions as filters. Additionally, because this result is independent of input signal it should happen at all layers. These are demonstrated by examining filter of several trained neural network architectures and fitting them to Gabors - showing the fit is good with low residual error.\n\nAll in all I think this is a very interesting paper which addresses a fundamental question and empirical result observed many times in the past. There are, however, many open questions here, many of them I think the paper should have addressed:\n\n* rotation of the basis functions - the main argument of the paper is that these filters are learned because the are the eigen-functions of the convolutional operator and any task that requires spanning the input space (==all tasks) will learn these. However, there are many other possible basis function combinations that would still span the input space but will not be eigen-functions i.e diagonlized. For example, any rotation of these learned filters would be such a basis (but the filters would not be oriented band-pass filters in that case). I don't think there is a good explanation here as to why this specific basis is learned out of all possible bases.\n\n* loss, learning etc - although the authors mention these results are independent of learning algorithm or loss, I would have wanted to see this demonstrated. Either by choosing a different task, a different optimisation method or some combination of the two. Specifically, demonstrating these results hold on non image data would have been more convincing I think (see below).\n\n* discretization - all the analysis in the paper is for the continuous signals, operators and convolutions. Do results change for the discrete case (which is the case here)? If so how? this is not discussed in the paper.\n\n* filter support and size - related to the previous point - how do results change with different filter sizes? the filters used in some of these networks are only 3x3 - this may result in a very coarse estimate for the bandpass-oriented filter parameters. I would like to see some discussion of this.\n\n* no convolutional model also learn such filters - older, non convolutional models also learn oriented band pass filters (sparse coding, ICA etc.) when trained on natural images. These results, as the authors suggest, are related to the structure of natural images - is this related in any way to the proposed explanation? I think this requires more discussion - especially since the statistics of natural images is what makes convolutions such a natural choice for processing them. \n\n* why do later layers get even better?  - it seems that deeper layers, across all architectures, get lower residual errors. Why is this? does it have to do with filter size? any other explanation?\n\n* statistics of orientations and scales - is there any different in orientation and scale statistics for different layers and architectures? this should be easy to obtain and visualize and I think would add to the breadth of results.\n\nBottom line - I think this is an interesting paper which can be even better with more in-depth discussion.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}