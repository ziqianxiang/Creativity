{
    "Decision": "",
    "Reviews": [
        {
            "title": "The theory is very confusing and not convincing",
            "review": "This paper proposes a way to optimize the behavior regularization hyperparameter $\\alpha$ in offline RL. With the proposed objective authors proposed to optimize $\\alpha$, the paper shows the algorithm improves from all the model-free offline RL algorithms.\n\nPros:\n* It is true that we need to somehow accurately obtain the hyperparameter $\\alpha$.\n* Very strong experimental results\n* The experiment result section is well structured, and it is easy to see the effect of varying newly introduced hyperparameters.\n\nCons:\n* The underlying theory is vague and hard to understand, and I believe that it is mostly wrong.\n - First, what does Proposition 1 imply? Although the policy set $\\Pi$ is introduced, the policy set $\\Pi$ is not used to design the algorithm used in the paper. During such kl-regularized offline RL, the policy being optimized can easily deviate outside of the proposed policy set since the policy is flexibly parameterized, and the Q we estimate is far from exact. Consequently, it is hard to say that the kl-regularized algorithm satisfies $\\pi_\\phi\\in\\Pi$, and the first two arguments below Proposition 1 is meaningless. I also cannot understand why $\\pi\\in\\Pi$ is written as a requirement for equation (6), where it is not.\n - The condition $\\pi\\in\\Pi_k$ in $\\arg\\min$ of equation (8) does not seem to be necessary. The policy can be improved by directly minimizing KL without the condition.\n - I do not understand the statement \"second term in equation (9) is the minimum upper bound to ensure policy improvement\" in Theorem 1. $\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_k}[\\log \\frac{\\pi_k(a|s)}{\\pi_b(a|s)}]=\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_k}[\\frac{Q^{\\pi_k}(s,a)-V^{\\pi_k}(s)}{\\alpha}]$ simply holds by the definition of $Q^{\\pi_k}$ and $V^{\\pi_k}$. If we update the policy to be $\\pi_{k+1}$ according to equation (8) but ignoring the condition $\\pi\\in\\Pi_k$, we get $\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi_k+1 } [ \\log \\frac {\\pi_{k+1} (a|s) }{ \\pi_b(a|s) } ] \\le \\mathbb{E}_{ s \\sim \\mathcal{D} , a \\sim \\pi_k+1 } [ \\frac{ Q^{\\pi_k} (s,a) - V^{\\pi_k} (s) } {\\alpha } ] $. This is independent from $\\pi_{k+1}\\in \\Pi_k$, and $\\pi_{k+1}$ may or may not be in $\\Pi_k$ while ensuring the policy improvement, i.e. $Q^{\\pi_{k+1}}(s_t,a_t)\\ge Q^{\\pi_k}(s_t,a_t)$ for all $(s_t,a_t)\\in \\mathcal{D}$.\n - Starting from Proposition 1, the overall quantity the paper uses to optimize $\\alpha$ is (assuming that $Q^\\pi$ and $V^\\pi$ are exact value functions of $\\pi$), $\\frac{Q^\\pi(s,\\arg\\max_{a^*} Q^\\pi(s,a^*))-V^\\pi(s)}{\\alpha}-KL(\\pi(\\cdot|s)||\\pi_b(\\cdot|s))=\\frac{Q^\\pi(s,\\arg\\max_{a^*} Q^\\pi(s,a^*))-\\mathbb{E}_{a\\sim\\pi}[Q^\\pi(s,a)]}{\\alpha}$. This objective is strange since the numerator is always positive, and $\\alpha$ will always try to converge to zero. \n* As I pointed above, the equation (14) without $\\beta$ and $B$ will always try to minimize $\\alpha$. I assume that this is why the authors introduce $\\beta$ and $B$ to make the algorithm work, by penalizing the first term to stop $\\alpha$ from converging to zero. If $\\beta$ is introduced to mitigate the bias by $Max-Q$ sampling policy, $\\beta$ should be larger than 1 since the $Max-Q$ approximation will always be smaller than the exact maximization of Q. If $\\beta$ smaller than 1 is working the best as shown in the experiments, I do not think that it is there to mitigate the bias.\n* While the experiment results are strong, I believe that the algorithm is equivalent to setting the fixed target KL-divergence with KL-regularized actor-critic, similar to the SAC that sets the target entropy. KL-regularized RL is not very new, e.g. [1], and the adaptive hyperparameter issue is first dealt by [2] in offline RL context. I believe that the underlying theory of this paper is not correct, and I recommend rejection despite the strong experimental results.\n - [1] Fox, Roy, Ari Pakman, and Naftali Tishby. \"Taming the noise in reinforcement learning via soft updates.\" UAI 2016.\n - [2] Lee, Byung-Jun, et al. \"Batch reinforcement learning with hyperparameter gradients.\" ICML 2020.\n\nMinor comments:\n* In equation (8), the latter term in $KL$ is not a distribution. The LHS of the equation should also be a distribution to match $\\arg\\min_\\pi$.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "Summary: The paper intends to learn a robust policy from low-quality offline data. It builds on behavior regularized offline RL which maximizes the sum of discounted reward while ensuring that the KL divergence between the learned policy and the behavior policy is bounded by some constant. The paper proposes to make the KL constraint adaptive so that the learned policy is able to move away from the behavior policy in case it’s not as good (i.e. the offline data is of low-quality). The method is evaluated on mujoco gym tasks with offline datasets of different quality (mixed, medium, low, very low). Adapt-AC achieves good performance even when the dataset has lower quality. Additionally, the paper contains ablation studies for choices of correction coefficient and maximum target divergence.\n\nPros: The method is well motivated and the evaluations seem comprehensive as the paper compares the method to other offline RL methods on a variety of mujoco tasks with the offline datasets of different quality. Additional ablation studies are helpful.\n\nCons/Suggestions: The low-quality datasets seem a bit artificial and the evaluations are done on simpler mujoco tasks. It would be nice to see how the method works on adroit envs with expert and cloned dataset. This would help as adroit envs are harder and more realistic. Moreover, the expert dataset includes only expert data (high quality data) and cloned dataset include both expert data and data from cloned policy which isn’t that successful (low quality data). How will the proposed method fare against other offline RL methods on cloned dataset for adroit envs?\n\nReasons for score: Weighing the above pros and cons, I vote for (weak) accepting.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "KL-constrained policy iteration with actor-critic ",
            "review": "This paper studies infinite-horizon offline reinforcement learning when the behavior policy data is contaminated with large noises. The authors propose a policy iteration algorithm that penalizes distributional deviation from behavior policy in KL divergence and bases on which, searches within a local neighborhood of the behavior policy. For problems with large states, the off-line policy iteration is approximated by an actor-critic algorithm. \n\nAlthough the numerical performance of the proposed algorithm is uniformly good for various tasks, the presented idea seems quite straightforward and there are not many theoretical results backup the methodology, which makes me reluctant to believe it fits the standard of ICLR. Let me elaborate below.\n\n1. Using KL divergence to constrain the policy exploration space is definitely not new, and has been used in various existing algorithms including TRPO as mentioned by the authors. And the actor-critic algorithm for the continuous state space seems also quite standard. Therefore, I did not find much methodological innovation in the proposed algorithm.\n\n2. For continuous state space, computing the argmax in (6) (or a' in (9)) is not practical. The paper proposes Max-Q approximation, but I doubt such approximation can have a large gap statistically, because this looks like approximate the global maximum of a continuous function by the maximum on a few sampled points.\n\n3. One of the critical issues in offline policy evaluation and improvement is the so-called curse of horizon (e.g., http://papers.nips.cc/paper/7781-breaking-the-curse-of-horizon-infinite-horizon-off-policy-estimation.pdf). Can you explain how the proposed framework circumvent this issue?\n\nMinor issue: I am not sure if the proof of Theorem 1 is entirely correct. In particular, can you expand on the argument on how to use equations (19-23) to show the convergence to $Q^{\\pi_{k+1}}$? It appears that some form of contraction mapping is used but I am not sure if this holds for the off-policy problem.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "offline RL algorithm with good intuition and nice empirical results; Some question about the algorithm derivation and experiment set up",
            "review": "Summary of the paper:\nThis paper proposed a new offline (batch) RL algorithm. The algorithm uses a relative entropy constrained to prevent offline learning from large extrapolation, similar to most previous algorithms.  The core idea in this paper is to adaptively update the coefficient of the relative entropy penalty term. It shows a convergent tabular prototype (policy iteration) algorithm with the relative entropy penalty term and the updating rule. Then it proposes a deep RL approximation to the tabular algorithm, showing that it outperforms several SOTA model-free match RL baselines in 6 MuJoCo domains, under several data-collection conditions. \n\nClarity: The algorithm derivation in section 4 is not clear enough. See cons 1. \\\nOriginality: Constraining the distance between behavior and current policy and using the relative entropy as constrained in offline RL both seem not new. (See BCQ, BEAR, BRAC) But how to adaptively tune the coefficient of the penalty term/constraints is not yet discussed enough and this work proposed a novel idea to that.\\\nSignificance: The empirical study of the proposed algorithm shows a significant result. Compared with other empirical work (BCQ, BEAR, CQL) the theoretical justification of this algorithm is not solid enough. (This might due to the clarity issue of section 4 and that I did not fully get the justification in section 4.) See cons 1 and 2.\n\nDetailed comments:\nPros: \n1. The question of adaptively tuning the coefficient of the (behavior) penalty term is natural. The proposed solution is also based on a natural intuition to tackle this question: try to constrain the policy within the smallest neighborhood (in terms of KL divergence) around behavior policy but still containing the optimal policy.\n2. The empirical performance of the proposed algorithm shows a significant advantage over other SOTA algorithms. The experiment is run on a large enough set of empirical benchmarks with a relatively standard data collection regime.\n\nCons:\n1. I think the overall presentation of the algorithm can be improved a lot in terms of clarity and formalism, especially section 4. \nSection 4 introduced an entropy regularized policy iteration algorithm as the tabular version of the proposed algorithm and studied its convergence. In general, I think it needs to be *very clear* to people that in each step in the algorithm which parameters are for updating and which are fixed in each equation. Then which part is about explaining the behavior of the algorithm. (In case it will help, maybe split out the description of the algorithm and the study of its theoretical guarantees. In the algorithm description maybe it's better to use \\leftarrow to explicitly indicating the update rules.) Then the description of the algorithm can be clear by being formalized as an optimization problem with a clear definition of the constraints and the optimization variables. \\\nWith the current representation, it's unclear to me what's the updating rule to $\\alpha$ in the tabular algorithm (or whether $\\alpha$ is fixed there). As the later proposed algorithm is a practical approximation of it, the tabular part is important to understand the intuition.\\\nSome minor confusions: \\\n(1) My understanding of Prop 1 is that it gives a relaxation (Eq 6) of the constraints in Eq 5. Now the presentation of results may look like eq 5 is an assumption.\\\n(2) The RHS in eq 9 seems to be a further relaxation of the upper bound in Eq 6. Then what's the relationship between that with the RHS of Eq 6. Further, Thm 1 seems to provide a justification for Eq 9 and from section 4.2 the derivation of the algorithm seems totally detached from Prop 1. Then what's the connection between Prop 1 and Eq 9 that is actually used in the algorithm?\\\n(3) Is Eq 9 a constrained about $\\pi$ or $\\alpha$? \\\n(4) Eq 14 gives the derivative of $J(\\alpha)$ but where is it defined? What's the connection between $J(\\alpha)$ and section 4.2? (For $J(\\theta)$ and $J(\\phi)$ I can found them but for $J(\\alpha)$ it seems unclear.)\\\n\n2. The modification of the Bellman operator also changes the optimality, especially in this paper the parameter alpha in the new Bellman operator is also changing. I think it needs to be discussed how much this changes the optimality. The original entropy-regularized Bellman operator have analysis but I'm not sure if it works for the variable $\\alpha$ settings\n\n3. The word robust in the title seems not very related to the main part of the paper. This paper is not really talking about robustness, at least from the algorithmic side (not even give a definition of it). The only link to the robustness might be the robustness of the very low quality of data in section 5.\n\n4. Though the experiment section shows results in 4 data generation conditions, 3 of the 4 are mixtures of random policy and mid-level policy with different ratios. So the general pattern of them is quite similar and I wonder how much this mixture can represent the data distribution from a really low-quality policy. To be consistent with the intuition of the algorithm, I think a more careful ablation study here is needed. For example, if the data is collected from a partially trained policy with earlier stopping or lower threshold.\n\n5. It worth mentioning some recent offline RL algorithms such as MOPO and MoRel as related work, though they are model-based. It will be better if they can be included in the empirical comparison.\n\nSummary and questions for rebuttal\nBased on the comments listed above, I think the major issue of this paper is the clarity and lack of formalism in the derivation of the algorithm. The empirical results presented seem very impressive, but I still have some minor concerns about it (cons 4 and 5) which weaken my confidence in accepting this paper purely based on the empirical contribution. If the author can clarify my questions related to section 4, I would consider updating my overall score to accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good algorithm but poor clarity & reproducibility",
            "review": "The authors derive an update for the constraint hyperparameter in behavior regularized RL to adaptively adjust regularization to allow policy improvement in offline RL. A practical variant of the algorithm is examined on a variety of offline datasets in the MuJoCo domain, where it is shown to outperform several recent algorithms. \n\nStrengths:\n- The performance of the proposed agent is very strong over a comprehensive number of datasets, environments and baseline agents. Ablation studies covers some of the design choices. \n- The problem (an adaptive constraint) is important and missing in prior work. This is a relevant contribution to the literature.\n- The approach builds on prior work but extends it in a meaningful way.\n- Comprehensive introduction to prior/related work.\n\nWeaknesses:\n\nAlthough the paper has a lot of upside, in general the clarity & reproducibility of the paper is very poor. \n\n- A lot of the theoretical results and proofs make large jumps in logic. Implicit assumptions are unmentioned. The paper is difficult to follow without being familiar with prior work. Specific examples:\n   - It is unclear how the authors jump from Eqn (2) to Eqn (3) given Eqn (2) is over policies and Eqn (3) is over the optimal policies.\n   - It is unclear how Eqn (17) holds from the Bellman equation.\n   - Eqn (10) is introduced stating that it comes from Eqn (7). However, the jump from the Bellman operator to the Bellman residual is both not exact nor is it correct with stochastic transitions. In the end, the authors use a more commonly used FQI approach with target networks, but this type of presentation is misleading. \n- The structure of the paper and arguments of each section were hard to follow. The key objective of the paper appears to be optimizing $\\alpha$, however the theoretically grounded section (4.2) uses a fixed $\\alpha$, the learning rule only appears in Eqn (14).\n- Important implementation details are scattered throughout the paper, seemingly an afterthought and in some cases missing entirely. Complete algorithmic description in the appendix omits these details & code is not provided.\n    - An important consideration for prior methods is the estimation of $\\pi_b$. The authors fail to mention if this value is estimated or if they use the ground truth value.\n    - The authors mention they use the reparameterization gradient below Eqn (13) but omit an equation.\n    - The algorithms are tested with only 3 seeds. \n    - Environment-specific hyper-parameters are especially concerning in an offline setting where it is difficult to evaluate their importance in the actual environment.  \n\nAdditional Comments:\n- I would like to see a discussion on the fact that Section 4.2 relies on a distribution over all states vs. Section 4.3 is a practical algorithm over a finite data set. \n- Personally, I’m not a fan of the title since it emphasizes the result of the algorithm rather than providing some detail about the algorithm itself. \n\nScore:\n\nI believe there's a non-zero chance I've misinterpreted parts of the paper, however at a certain point the responsibility of understanding falls upon the author rather than the reviewer. As of now I would recommend rejection but I'm more than happy to increase my score after the rebuttal phase with enough improvements to the clarity & reproducibility of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}