{
    "Decision": "",
    "Reviews": [
        {
            "title": "Clarity issue for this submission",
            "review": "If my understanding is correct, this work proposes a preprocessing method to generate noisy pseudo data in particular regarding the pronoun references and the gender biases and employs the method for GAN-based training so that the discriminator is trained to correct the noises in the data. Experiments are carried out on the low-resource settings of Mongolian-Chinese, Korean-Chinese and Arabic-Chinese translation tasks and presents (potentially) statistically significant gains over non-preprocessed GAN settings.\n\n# pros\n* This work shows gains on small data setting.\n\n# cons\n* It is totally unclear how preprocessing is done.\n* It is not clear why it is using GAN based approaches given that it could be used for other approaches, e.g., simply training on vanilla Transformer.\n\n# Detail comments\n\nThe core proposal is described in section 2.1, but it is unclear to me how preprocessing is done in this work. In addition, the motivation of using GAN based approach is totally unclear. The terminologies, e.g., VIN, are not clearly defined. Given the clarity issues, I've found no merit of accepting this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not a simple method",
            "review": "The paper proposes a method to resolve referential problems in machine translation, especially mistranslating pronouns, by distinguishing errors in translation process using GAN techniques, enhanced by the focus module which prioritizes what to detect by the discriminator.\n\nThe main contribution of the paper seems its simplicity of overall methods compared with other ones requiring \"mathematical knowledge\" and \"complex training rules\". However, the method written in the paper sounds too complex already. The preprocessing requires linguistic knowledge for each language to replace focused properties, which looks almost the same as an artificial feature engineering. The method also introduces a training strategy involving GAN and RL; both methods are harder to control than simple feed-forward models, e.g., usual translation models.\n\nI also don't understand clearly the motivation of the method. To be clear: which sentences are preprocessed: only source, or source and targets? If the former, what is the merit to distinguish generator results when perturbed source is given?\n\nThe paper also does not contain any comparison with other \"complex\" methods than GAN approaches although the paper mentioned some, so we could not figure out any advantages of the proposed method against other work.\n\nSome presentation errors also make the paper hard to understand. Especially, the Eq. (1) looks not a usual math description, although it may describe a GAN objective.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Pronoun resolution improvement for NMT through RL: problems with evaluation and lack of novelty",
            "review": "### Summary\nThis paper addresses the problem of reference resolution in low-resource machine translation (MT) from Korean, Mongolian and Arabic to Chinese.\nIn order to teach the MT model correct references, GAN-style training with a designated reward function is proposed. This yields empirical quality gains compared to standard MT training while not slowing down training.\n\n### Strengths\n- Mostly large empirical improvements.\n- The augmented data, if released, could serve as a benchmark or challenge set for future studies.\n- The paper addresses an important problem, as pronoun solution is a frequent failure case for NMT and has large implications for the successful use in real life, even more so for low-resource languages.\n\n### Weaknesses\n- The hypothesis is not tested sufficiently. It is not clear if this particularly benefits low-resource translations or rather any weak MT system, or any MT system translating into Chinese. To better locate the effect, a comparison to a higher-resource pair, e.g. English-Chinese would be insightful, as well as low-resource translations into languages other than Chinese, e.g. by reversing the translation direction of any of the studied language pairs.\n- Lack of baselines or evaluation metrics from the literature (the referred rules and features), such as data augmentation by e.g. backtranslation to estimate the effects of the increase in training corpus.\n- Related work is missing, see below.\n- Clarity is poor, see detailed questions and comments below.\n- Reproducibility: Neither code nor data are released.\n- The proposed training seems very similar to Yatu et al. 2019a and 2019b. It is not clear if there is any novelty except for the reward function. \n\n### Recommendation\nThe main contribution is the development of a pronoun-focused training algorithm. The leverage of contrastive examples for pronouns is effective, but the comparison to previous methods and evaluation metrics is missing. The clarity of the paper is rather poor, which additionally prevents the reader from completely understanding its contribution and estimating its impact. Therefore, I recommend to reject the paper in its current stage, but I'd encourage the authors to improve clarity and comparisons, because progress in accurate low-resource MT would be very valuable.\n\n\n### Questions & Comments\n- The paper would greatly benefit from more precision when it comes to describing the problem, the proposed algorithm, and the research hypotheses. Here's a list of examples:\n\t- The abstract mentions that the \"accuracy of pronouns\" is improved, but accuracy is not the evaluation metric (a BLEU variant instead).\n\t- The added sentences are described as \"obvious and contrary to the facts\" - what does obvious mean here?\n\t- BLEU does not describe a \"distribution gap\" (Sec 2.2), but rather precision-based overlap of a hypothesis with one ore more reference translations.\n\t- \"We mask out the rest except pronouns\" (Sec 4.1) - how is this masking realized and how does it compare to an accuracy evaluation of generated pronouns (closer to previous works)? \n\t- The reasoning against \"complex mathematical rules\" is not very clear: How come GANs should be considered simpler?\n\t- The paper repeats the mention of \"greedy search\". Does the NMT inference not include beam search?\n\t- The introduction promises to solve \"a series of problems\", of which reference resolution is only one. However, the evalutation only focuses on reference resolution, so this promise would need more support to be fulfilled.\n\t- Does BLEU_Seq describe sequence-level or corpus-level BLEU?\n\t- What kind of scores are shown in the heatmap in Fig. 3? The text mentions accuracy, but on a token-level it should be binary and the scores are continuous. Are these attention scores or alignments?\n- How are the alternate options generated? A POS tagger is mentioned, but it would be good to show pseudo-code for the exact preprocessing of the training sentences, so that the methods becomes reproducible.\n- How does the method compare to previous approaches to improve pronoun choice?\n- How well does this method generalize to other low-resource tasks, when the existence of a POS tagger is assumed? (e.g. reversing the translation direction) The solution might be limited to translating into a high-resource language.\n- How dependent is the method on POS tagger quality?\n- What is the rationale for masking pronouns? Why are masks expected to generalize better? In the paper it is presented as an alternative to dropout - so why not use dropout if that doesn't require additional data processing? The direct comparison to dropout would help to understand the contribution of this masked noise.\n- To support the search space argument it would be good to show how a correct choice of pronoun early in the sentence facilitates a correct prediction later on - or how far apart the different options are in the beam.\n- How is alpha in Eq. 4 set for the experiments? It would be good to compare against the standard BLEU reward for adversarial training (alpha=0?) to deduce the effects of data augmentation through sampling.\n- How are lambdas set/tuned in Eq. 5?\n- The first paragraph on page 5 is completely unclear. Which optimal value is meant and what is meant by \"control the measurement of the value of F\"? Is it just about how to set alpha? If so, how is it adjusted accordingly?\n- How do Transformer baselines compare to the shared task submissions in CWMT 2017 or the current state of the art?\n- The labels of the heatmaps in Fig. 3 are not readable. Please increase the font size. \n- The connection with biased pronouns is interesting, but I wonder why the noising should increase the model's tendency to produce correct choices (\"her\" in the example) rather than invalid choices (\"its\" in the example) if they're equally likely to appear in the training data. Is the reward distinct enough to cause the model to prefer the correct choices? It would be great to compare examples for rewards for that purpose.\n- Critical typo: Sec 2.2. police -> policy\n\n### Missing Related Work\n- Improved pronoun resolution by larger context, including standardized evaluation metrics:\n\t- Wong et al. 2020 (https://www.aclweb.org/anthology/2020.acl-main.530)\n\t- Voita et al. 2018 (https://www.aclweb.org/anthology/P18-1117/)\n\t- Stojanovski & Fraser 2018 (https://www.aclweb.org/anthology/W18-6306/)\n\t- Stojanovski & Fraser 2019 (https://www.aclweb.org/anthology/W19-6614/)\n- Challenge sets and shared tasks for pronoun translation, including standardized evaluation metrics: \n\t- Hardmeier et al. 2015 (https://www.aclweb.org/anthology/W15-2501)\n\t- Müller et al. 2018 (https://www.statmt.org/wmt18/pdf/WMT007.pdf)\n\t- Shimazu et al. 2020 (http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.447.pdf)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A difficult to read paper that could be interesting to read if the motivation and objectives were more clear",
            "review": "The paper presents a study aiming to solve the referential problem in low-resource NMT and propose a method to integrate a pre-processing step to integrate noise in the data and an adversarial training module to improve the quality of translations generated by the NMT system. \nAlthough the abstract and introduction states the aim of the paper if to improve generalization, it is not clear in which scope the reference errors are addressed (which type of errors) and what is exactly improved in the output of the system. \nThe paper also lacks discussion to related work especially other methods using adversarial training in NMT and motivation for the proposed approach. Throughout the paper related work is mentioned but in an ambiguous way, so without a clear and sound statement on how the method solves any issue that previous studies cannot and a clear description of the aim of the study it is not meaningful to publish this study.\nThe language is mostly ungrammatical which makes it difficult to understand most of the content, an entire revision is necessary for the paper to be readable and then may be reviewed again.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}