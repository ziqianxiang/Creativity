{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors introduce vPERL, a model that generates an intrinsic reward for imitation learning. vPERL is trained on demonstrations to minimise a variational objective that matches a posterior formed by \"action backtracking\" and a forward model, with the intrinsic reward coming from the reward map. The authors might be interested in related work on few shot imitation learning: e.g., \"One shot imitation learning\", Duan et al, 2017, \"Watch, try learn: meta-learning from demonstrations and rewards\", Zhou et al 2019. As all reviewers pointed out, and I can confirm, the paper is quite tricky to understand in its present form, and would very much benefit the writing being re-visited to more clearly express the ideas within (in particular, section 3, which is the core of the contributions). \n"
    },
    "Reviews": [
        {
            "title": "Learning a policy from one demonstration without rewards",
            "review": "This paper assumes no access to the reward values and attempts to learn a policy by starting just with one demonstration to define the reward. For obtaining the reward, the authors rely on the ideas from Value Iteration Networks (VIN) method and they add the modules that help to deal with cases with complex transition dynamics. The resulting method is tested on atari domain and on continuous control tasks.\n\nStrong points:\n\n- The ideas from Value Iteration Networks are applied to the environments with complex dynamics.\n- The problem setting is interesting. The restrictive assumption about the amount of demonstrations makes the method practical. \n- The results are promising and seem to significantly outperform the competing baselines from inverse reinforcement learning.\n\nWeak points:\n\n- The paper relies a lot on VIN method ideas without introducing it with sufficient detail. This makes it difficult to follow as many  concepts (and notation) in Section 3 are not properly introduced. Besides, the contributions of this paper are not very clearly stated.\n- The analysis part in 4.1 is not very informative and does not provide *experimental* evidence on *why* the proposed method works well and what issues with previous methods it addresses. For example, it is still not clear to me why the small model performs better than the big model in Table 2. Another example is the lack of ablations to understand the contribution of each of the components of the method. Additionally, the reader would benefit from understanding how the gap between the proposed method and the baselines changes with the growing amount of demonstrations: In particular, does the proposed model address the issues of low data regime or does it help in general?\n- The atari environment actually includes the scores in the input images (clearly in Breakout, Bean Rider, Space Invader, Seaquest, Kung Fu, and maybe in Qbert, BattleZone). Is it fair if no reward is assumed to be available?\n- The paper is quite hard to understand, it immediately jumps into details of the method in Section 3 without providing sufficient overview and intuition into what the method tries to achieve and what the issues with previous works are. \n\n\nI am leaning towards the rejection of this paper. While the experimental results are very encouraging as they are provided for the environments with complex dynamics and with very little expert data, my main concerns are that 1) the method description is currently very hard to understand, the contributions are not stated clearly, and 2) the experimental results do not include any informative analysis to understand the underlying reasons for why the method performs well. \n\nQuestions:\n- In the conclusion, the authors say \"policies that generalise well to new tasks\", what is meant by this and what are the supporting experiments?\n- Why does the small method model work better than the big model?\n- What is the contribution of different components of the method? How sensitive the method is to the choice of hyperparameters, such as K?\n- In Figure 4 it seems that many methods didn't converge to the final score yet. Why was such a number of training steps chosen? What happens if the experiments run for longer? \n- What is the computational complexity of the proposed algorithm?\n- Why is it possible to take average or max reward values of the reward map? How is the reward map idea justified when it is provided not for the map of states but where each map corresponds to one state?\n\nAdditional comments\n- The clarity of writing could be further improved. For example, see the Conclusion section (but it applies in other parts of the paper as well), avoid -> avoids, outperform -> outperforms, limiting and limited are repeated etc. Figure 3 has its labels shifted and Figure 1 seems to have the rows swapped. In Figure 2: is q_\\phi meant? \n- Maybe the analysis part in 4.1 is more suitable for the related work.\n\n=== Post rebuttal ===\n\nI would like to thank the authors for the very detailed response and the improvements in the manuscript. I found the ablation experiments and experiment with \"No scores\" particularly useful. However, I am still a bit confused about better performance of the smaller model. Maybe, more understanding could be gained if there was an additional \"tiny\" model that would show that when going beyond a certain size, the performance degrade. Additionally, if the overfitting is an important issue, some regularisation methods could be explored. \n\nFinally, in the light of many changes in the paper and the original request of all the reviewers to have the writing in the manuscript improved, I think the paper would benefit from another round of reviews. However, I find this method promising and if the paper is not accepted this time, I encourage the authors to re-submit a revised version.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach, needs more empirical analysis",
            "review": "This paper proposes a method for inverse reinforcement learning that incorporates a differential planning module. Explicit transition dynamics modeling with inverse value iteration is added to promote meaningful reward learning. Empirical evaluations on several high-dimensional Atari environments and 2 continuous control environments are provided which show improvements over existing inverse reinforcement learning baselines when given only one-life demonstrations. Some visuals are also presented to show that the proposed method is able to learn more meaningful reward maps than previous methods.\n\nPositives:\n1. The proposed method is natural and makes use of a few existing works. The combination of Value Iteration Networks (VIN) and the Generative Intrinsic Reward driven Imitation Learning (GIRIL) is novel. \n\n2. The visual reward maps show that the proposed method learns a more semantically meaningful reward map than VIN. When using PPO on the learned reward function, it learns a better performing policy faster in most cases.\n\n3. The results in continuous control environments demonstrate generalizability.\n\nNegatives:\n1. There is little analysis of the learned reward maps beyond a few plots for visual comparisons. As the proposed method bears close connections to VIN (used as a submodule) and GIRIL (same joint learning pipeline), more detailed comparisons are definitely needed. A few concrete suggestions are: \na) add reward maps for GIRIL and provide qualitative analysis on the difference, e.g., in semantic meanings.\nb) for the proposed method, there are two variants on using the predicted reward maps: mean and max. It is unclear which version of the maps are shown in the paper and the empirical results are not conclusive on which one is better. It would be interesting to analyze the differences between them and, as a bonus, provides a rule-of-thumb on which one to use.\n\n2. There are several places where the writings could be improved. Some missing derivation details make understanding the paper difficult.\n\nQuestions:\n1. Please clarify a few terms in the paper: \na) Embedded MDP in section 3.1 near the bottom of page 3\nb) What is the conditional VAE? And the derivation details of equation (1) in the paragraph about Variational solution to vPERL.\n\n2. It is somewhat counterintuitive to me that vPERL-small generally outperforms vPERL-large since the small version has less state information. Could the authors provide some explanation for this?\n\n3. The baseline method VIN/BC is confusing. Could the authors clarify whether it means Behavioral cloning (as in supervised learning with state-action pairs from demonstrations) or VIN?\n\n======================\nPost-rebuttal comments:\n\nI want to thank the authors for providing clear answers to my questions and comments. I found the answers satisfactory so I raised my score to 6. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper presents a inverse RL method that utilizes a differential planning module and dynamics modeling to learn reward signals. The learned reward function can be used to train policies that can match or even exceed demonstration, as demonstrated in several Atari games and two simple continuous control tasks.\n\nThe paper demonstrates a novel usage of VIN like planning module for inverse RL and the result is quite compelling. Although the writing can and should be improved to help reader to understand what is going on, including multiple grammatical errors like \" we introduce a variational solution to optimize the both submodules jointly\". \n\nSome technical questions:\n(1)$p_\\theta$ takes $z$ and $s_t$ as input, but in equation (1), it also only takes $s_t$ as input. I am a bit confused here.\n(2) As far as I understand, the learned transition dynamics is only meaningful for the expert action, for example, given $s_t$, if the $a_t$ is completely different from the expert action, it is likely to predict completely wrong $s_{t+1}$, and thus completely wrong reward estimate with the setup in the paper. Since the policy is able to learn much better with the learned reward function, I will imagine it learns to take action that is completely different from the demonstration. Any intuitive explanation why this will work so well? It will be also interesting to see how the learned reward compare to the true reward in those region not covered by the demonstration.\n(3) The continuous control tasks is like a toy example, but I guess the impressive part is that it can learn so well with only 1000 demonstration data. But are there any challenges to scale this to more complicated tasks given enough demonstration? If not, potential limitation should be discussed.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting approach backed up by good empirical results",
            "review": "This paper addresses the problem of imitation learning. The authors\npropose a system where you first learn a surrogate reward function\nfrom the demonstrations, and then you do RL to optimize that reward.\nThe surrogate reward function is the result of learned forward and\ninverse dynamics models that are \"planning-embedded\" in the same\nsense as value iteration networks.\n\nI think this paper is quite interesting overall, and the idea seems\nworth pursuing. I know that the general approach to imitation learning\nof first learning a cost function from the demonstrations and then\napplying RL to optimize it has a very rich history, but I have not\nseen prior work that learns the cost function in the manner proposed\nin this paper. Additionally, the experimental results seem good to me,\nespecially on the chosen Atari games. I think the result that vPERL\nachieves performance far greater than that of the demonstrations, in\nsome instances, will be of interest to the community. Therefore, I am\nleaning toward recommending acceptance, but I am not familiar enough\nwith the scope of related literature to be certain.\n\nI do have some questions which I hope the authors can address in the rebuttal.\n\n1. What is special about Kung-Fu Master and Breakout that causes vPERL\nto not perform well? Have the authors tried Atari games other than the\n8 shown? Why were these 8 picked?\n\n2. Regarding Figure 1, I'm a bit unclear about the interpretation. A\nvalue function is a function mapping states to values; what does it\nmean to have a \"value map\" for a single state? Is the idea that these\nare spatial domains where an agent is moving around, and so the map\nyou're showing us implicitly represents the value of a state where the\nagent is at that location?\n\n3. After Equation 1, you say that \"\\pi_E(a_t | s_t) is the expert\npolicy distribution.\" But where do we get this distribution from? We\nonly have a dataset of expert behavior, and states are continuous, so\nwe will never see the same state twice in the data. So how can we\nbuild a distribution over what actions the expert would take from any\ngiven state? Will this distribution just be a Dirac in practice?\n\n4. I am curious how important the use of both forward and inverse\nmodels is in the reward learning module. The authors provide some\njustification why both are needed, but it would be good to support\nthis with an ablation experiment.\n\n5. How sensitive is the approach to the optimality of the\ndemonstrations? That is, in your experiments, you generate the\ndemonstrations from a policy trained on 10 million PPO steps; what if\nthis number of steps were lower?\n\n6. To me, an obvious baseline would be to run PPO with the exact\nconfiguration that you use in your experiments (with 50 million\nsimulation steps), but using the ground truth reward instead of the\nlearned reward. I am curious if the authors have considered this\nbaseline, and how it performs.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}