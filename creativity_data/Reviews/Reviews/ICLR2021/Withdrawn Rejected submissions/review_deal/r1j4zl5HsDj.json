{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors present an adaptive model that learns a good policy by adversarial training, focusing on the setting where the query budget is very small. Some experiments are carried out to validate the proposed method. The reviewers' opinions turned out to be split on this paper. On one hand, all reviewers appreciated the idea of the problem and recognized its importance. On the other hand, there are have been multiple concerns regarding readability (but that has improved during the discussion) and about the empirical validation/evaluation.\nBased on the above, as well as my own reading, I believe this paper contains interesting ideas but, as it currently stands, is not ready for publication."
    },
    "Reviews": [
        {
            "title": "Differentiable instance dependent learning",
            "review": "The paper \"Learning to Actively Learn\" proposes a differentiable procedure to design algorithms for adaptive data collection tasks. The framework is based on the idea of making use of a measure of problem convexity (each problem is parametrized by a parameter theta) to solve solving a min-max objective over policies. The rationale behind this objective is that the resulting policy of solving this min-max objective should be robust to the problem complexity. The algorithm then proceeds to sample problem instances and making use of a differentiable objective to find a policy parametrized by a parameter psi, which could be parametrizing a neural network.  \n\nOne potential drawback is that the authors assume the dynamics of the problem instance rewards are known by the learner (for example they are a gaussian), which is necessary for computing policy gradients through the policy parametrization. A second drawback lies in the problem tessellation over the theta space. As it is written the method does not seem to scale beyond very small dimensional problem instances, since otherwise the value N would have to be exponential in the dimension, and therefore intractably large. The paper falls within the differentiable \"meta-learning\" for bandits literature, and it does a good job of placing itself within that literature. It also has a convincing experimental section. Other works in the area have not tackled the problem that the authors set themselves to solve: designing algorithms that can adaptively perform well depending on the instance they are fed.\n\nI also find particularly interesting the use of the model complexity balls that can be defined using other existing results in the literature such as in the case of transductive linear bandits. I would suggest to add more explanation to what r is earlier in the paper as it is hard on the reader after a first read. Overall I think this is nice work. The paper itself is more applied than theoretical but I think it is appropriate for ICLR. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper. But I have some concern.",
            "review": "Summary:\nIn this paper, the authors present an adaptive model that can learn a good policy by adversarial training. The proposed model is based on theoretic lower bounds. They focus on the setting when the query budget is small and conduct some experiments to verify the proposed method.\n\nPros:\n- The authors derive some theoretical results.\n- Using adversarial training is interesting.\n\nCons:\n- The notations are too complicated and it's hard to follow the author's thought.\n- I feel like the query budget in the experiment is very small (T=20). This setting might be unrealistic. I would like to see if the proposed method can have good performance when the budget is reasonably large.\n- The current experimental results are not very convincing for me. Uncertainty or SGBS seems to be a good choice in some cases as well. Maybe the author can do experiments on more datasets and see if the proposed method performs better in most datasets.\n- I notice that the authors subsample the training examples. It seems that the training the proposed method is slow so the number of training data and the budget are limited.\n\nTypos:\n- I thought in Figure 2, 3, 4, 6, 7, \\hat{pi} should be pi_*",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not fit for publication in its current form",
            "review": "The paper sets out to address a class of pure exploration problems. Of particular interest to the authors seem to be settings where the sample complexity requirements are very stringent. A good example of such a task would be that of tuning the hyperparameters of an expensive training algorithm like BERT pre-training or ResNet.\n\nEven though the problem domain is very interesting, the paper is just not fit for publication in this form for the following reasons:\n\n1- The write-up is just horrific:\n\n  a- The problem setting doesn't mention anything about intermediate rewards, which was very confusing because the reader is given the impression that learning happens based on just the total reward at the end of the trajectory.\n\n  b- There are some definitions that just don't make sense: for instance, Equation (1) is subtracting a number from a policy. My guess is that the equation is missing some parentheses, but what's the point of formulas if they're not precise. Also, a more minor point is that the is no such thing as arginf: the whole point of inf is to deal with situations where the function doesn't have a minimum, e.g. inf_{x>0} 1/x = 0. What you mean is argmin, in which case you need some sort of compactness assumption in the policy space.\n\n  c- It's completely unclear what the point of Prop 1 is.\n\n  d- More generally, the paper could really benefit from a table of notations so the reader doesn't have to keep jumping back and forth.\n\n2- It's really unclear what the contributions of the paper are: there are tons of papers on linear bandits in the pure exploration setting, so what does this paper add? There is no sample complexity analysis in there, which would be fine if the paper was a solid experimental paper, which it is not as discussed in the next point.\n\n3- I find the experiments very unconvincing: as mentioned above, a really good motivating use-case is hyperparameter tuning for a very complex models or at least a diverse set of examples. A good example of a paper that does a satisfactory job of providing convincing experimental results is the Hyperband paper [Li et al, 2017], which render the paper valuable despite its weak theoretical results.\n\nPlease get one of your colleagues to read the paper before resubmitting it.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors introduce a novel, robust approach to Learning to Actively Learn. Even though this is an interesting solution to an important problem, the paper is difficult to follow and the evaluation results needs significant improvements. ",
            "review": "Even though the paper introduces an interesting solution to an important problem, it suffers from two main weaknesses:\n- it lacks an lacks an intuitive explanation of the proposed approach, thus making it hard to read\n- the Empirical Evaluation seems to lack both a unifying theme and certain critical details\n\nIn order to make the paper easier to read, it would be great to add, right after the introduction, a section that explains in an intuitive manner the proposed approach. The authors should choose a motivating example  (ideally, it should be a real world domain, but it could also a be a simplified, synthetic one)  on which to explain all the basics of the proposed approach: which are the equivalence classes, how is the adversarial training leveraged, and how/when will one suffer the catastrophic loss referred to at the very bottom of page 1.\n\nThe empirical validation will greatly benefit by tightening the various arguments.\n\nFor 20 Questions:\n- the scalability issue should not be confined to APPENDIX I: what would have taken to train on the entire dataset? What would take to use a dataset with 10x celebrities, 10x questions, or both? \n- the statement \"Uncertainty sampling even outperforms our r-dependent baseline by a bit which in theory should not occur â€“ we conjecture this is due to insufficient convergence of our policies or local minima\" certainly deserves a paragraph of its own, and a lot deeper an explanation than currently provided\n- the accuracy results in Table 1 deserve a discussion:  why is your approach better than SGBS, when in 5 out of 6 settings SGBS has farr better results than your method?\n- last but not least, you define should explain how \"Accuracy\" is computed (Table 1 is the only place in the paper where this term is used), and you should also use the \"Win Rate\" metric so that we can have an apples-to-apples comparison with the results in the original paper  \n\nFor Joke Jester:\n- please explain why your are restricting the dataset to include only jokes that are rated by all users. Is it a scalability issue? Or something else?\n- you should also provide results in terms of Normalized Mean Absolute Error (NMAE), so that we can compare with results in the original paper\n- In Table 2, the Uncertainty Sampling approach has better performance than your proposed method, even though in Figure 7 it seems to be doing consistently worse. Could you please discuss this phenomenon?  \n\nOther comments:\n- it is very difficult to make sense of Figures 2 & 3 even when zooming-in of a large display. \n- In Figure 2, for r > 20, there seems to always be 2-3 policies better than the proposed one; however, the image is so \"crowded\" in the r <= 20 region that it is impossible to see what is going on even at max-magnification. The authors should explain in prose what is going on & why they consider their policy best-overall. They should also provide in an APPENDIX several 1:1 graphs that compare the proposed policy against each of the other main contenders.\n- In the paragraph that covers Figure 3, the authors write \"Ï€âˆ— performs almost as well as the r-dependent baseline policies over the range of r.\" However, Figure 3 does not include the r-dependent baseline.\n- the authors should be consistent in notation: Figure 4 uses the LAL acronym, while Figure 5 uses the full name for it.           ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}