{
    "Decision": "",
    "Reviews": [
        {
            "title": "Convincing premise, unconvincing experiments",
            "review": "This paper studies the question of whether attention can be treated as interpretations or not. It argues that attention masks may encode label information in a way that subsequent layers can decode the label, and as a result, the masks don't serve any purpose as explanations for what input components models look at. To address this, the paper proposes a method that essentially tries to decorrelate the label from the attention masks. It reports experiments on two text classification and two image classification tasks.\n\nThe premise of the paper is very interesting, and well supported, especially via the toy experiments. The example that illustrates the idea of combinatorial shortcuts is well constructed, and it makes a convincing case. However, the paper falls short in the experiment section (see below).\n\n*On \"interpretability\"*: The paper focuses on only one aspect of \"interpretability\", namely predictive accuracy. Indeed, while the introduction says \"Model interpretation explains how models make decisions\", later in the paper, in section 3.1, it defines a mask $m_1$ to be superior to $m_2$ if it lead to a lower loss for an example. But is this the only criterion for an interpretable mask? Sure, a lower loss would imply that the mask $m_1$ picks more informative aspects. But shouldn't interpretability and explanation also involve human judgments of usefulness and trust? (For example, the LIME paper includes human judgment experiments.)\n\nAs an example, consider two masks $m_1$ and $m_2$ for an object recognition task. Suppose $m_1$ selects a bounding box of the object in question, but because of the shape of the object, it also includes distractors. In contrast, say $m_2$ picks every other pixel in the image, effectively downsampling it. It is quite possible that the accuracy of the classifier with $m_1$ is lower. But clearly $m_1$ is a more interpretable attention.\n\nThe only place where we actually see the interpretations generated are the small number of examples in the appendix. The experiments only evaluate the accuracy of the various models.\n\n*Assumptions*: In section 4.2, the paper makes several assumptions. It is not clear how realistic these assumptions are. The paper does mention that the effectiveness of the approach relies on the validity of the assumptions, but it would be helpful for a subsequent user of this approach to get a sense of when these assumptions would hold, and how crucial they are. They are clearly important to make the proof work, but what's their practical import?\n\n*Experiments*:  The experiments in the paper feel very underspecified and don't provide too much details. For example, for each of the tasks, what is $P(Y \\mid M)$? How stable is the weighting approach? And how easy is it to tune hyperparameters for?\n\nFinally, the discussion of the results in table 3 are unconvincing. Both the explanations for why the image tasks prefer weighting and the text tasks prefer weighting are unclear. Why is \"a single word is more informative than a single pixel\" an explanation for this difference? Also, what does \"continuous, \"discrete\" and \"co-adapting\" mean? Does the difference have to do with the size of the inputs or the masks perhaps?\n\n*Minor comments*: Some minor comments follow\n* Why is the transformer attention introduced in the introduction when it is not really used later?\n* The paper says at the start of section 4 that it introduces two practical methods, but in the second paragraph of 4.1 contradicts the earlier statement by saying that random attention pretraining is not practical.\n* Table 3 caption: IMBD -> IMDB\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but with vital flaws",
            "review": "# Summarise\n\nThis paper proposes an explanation to the lack of interpretability of attention mechanism. The authors propose to name the \"effects of nonrandomised combination for X and M as combinatorial shortcuts\", and went on to describe the some efforts in experimenting with RCNN on a couple of sentiment analysis tasks. Finally, the paper proposes two methods to avoid combinatorial shortcuts, which gives good improvement over baseline.\n\n# Pros\n\nThe paper is well written and easy to follow. Experiments well explained and I do not feel that there are any more information to be added.\n\nIt is also interesting to see Weighting's more stable than random, but not always better.\n\n# Cons\n\n1. My major concern over the Combinatorial Shortcut concept is the paper's stance on it. There is no valid reason to believe that the existence of such is empirically harmful for performance (maybe not optimal, but the training data should carry the blame as well), and we have known for quite some time that attention is not alignment, nor does it necessarily highlight important parts of the input. The latter is especially the case with transformer/BERT based models, where token position and attention weights' relationship are quite flexible. Although experimental results do show some unstable improvement on some datasets over previous work and a weak baseline, it is hard to make an argument to suggest that Combinatorial Shortcuts is necessarily something we don't want. Neural network feature extraction may not work the way we assumed it would, that doesn't mean it's bad. And when one bases the majority of experiment on this assumption, I feel the paper itself maybe a bit biased. \n\n2. The RCNN model in this paper is !!NOT!! attention. Similar experimental design may be used on real attention models, but for this work, this is a critical failure.\n\n3. The mathematical reasoning for Combinatorial Shortcuts is too obvious and lacks novelty. People have known this for quite a while, giving it a name hardly counts as a contribution.\n\n4. The experimental section 5.2.2 shows really unstable performance for the Pretraining method, sometimes much better than the weak baseline and sometimes much worse (MNIST). Without more thorough error analysis I find it quite concerning. Unless the performance of the proposed two mechanisms consistently improves model performance, and not just with one model but more, especially on more complicated tasks than sentiment analysis and old image classification datasets, I find it really hard to justify the bias against Combinatory Shortcuts, if it is indeed appropriate to be named so.\n\n5. The title is kinda misleading. Just because attention doesn't necessarily highlight meaningful part of the input sequence, doesn't necessarily mean it is not interpretable, neither does this paper answer the question fully. If we intend for the models to be interpretable, attention is only one aspect, far from the full picture.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for \"Why is Attention Not So Interpretable?\"",
            "review": "The paper introduced the concept of \"combinatorial shortcuts\" when training a neural network with attention layers.\nSpecifically, the paper argued that the attention weights may not be pure importance indicators due to the biased estimates caused by sample bias during training.\nThe paper demonstrated the phenomenon with an intuitive experiment, proposed two methods to address the issues.\nLastly, the paper tested the two methods under a commonly adopted framework and demonstrated that the proposed methods could improve the interpretability of the attention neural networks.\n\nOverall the paper is well written and fairly interesting to read. Some terms needed to be defined more clearly while being used.\nMy major comment is that, it'd be nice to see more rigorous definition of \"combinatorial shortcuts\" in mathematical terms.\n\nBelow are some more detailed comments:\n(1) Page 3, the first sentence \"...adversarial methods that...\" assumed particular set of methods, for example, methods that select features sequentially instead of selecting a subset. It'd be good to either add a reference or clarify the specific methods with the targeted property. \n\n(2) Page 3, Section 3, it mentioned \"definitive explanations\"; what is the definition of \"definitive explanation\"?\n\n(3) Results reported in Table 1 and Table 2 lacked variance; it's unclear how variable those average percentage values are and thus it's hard to conclude the statistical significance of the difference.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "marginally below acceptance",
            "review": "##########################################################################\n\n\nSummary:\nIn this paper, the authors show attention's interpretability issues through causal effect mechanisms, and show the existence of combinatorial shortcuts. They propose ways to mitigate the effect of combinatorial shortcuts to give better explanations. They evaluate their method on simulation and real world text and image data. \n##########################################################################\n\n\nReason for Score:\nWhile the paper presents interesting theoretically motivated reasons for analyzing interpretation issues with attention, the paper only evaluates on 2 architectures. It would be interesting if the authors compared their results on BERT, LSTM or other state of the art NLP attention based architectures. Qualitative comparisons for their ``more interpretable version of attention is missing.\n\n##########################################################################\n\n\nPros:\n1. The causal effect perspective on interpretation from attention and the existence of combinatorial shortcuts is novel and interesting. \n2. The simulation experiment including uninformative tokens is interesting. \n\n\n##########################################################################\n\n\nCons:\n1. The reason why the sampled mask should be independent of the label Y isn't convincing. Ideally one would expect the mask to highlight positive words for a positive sentiment review. \n2. What is the performance of the model on BERT that has multiple layers of the attention mechanism?\n3. The authors should show qualitative explanations for the real world data to make sure the training methods they propose indeed highlight useful words. \n4. The exact mask used and the meaning of K(V) in Figure 1 isn't clear. The authors should provide more details at the equation level how they calculate attention in this experiment. \n5. The authors report the post-hoc accuracy, what is the prediction accuracy of the model trained using their strategies? \n\n\n##########################################################################\n\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}