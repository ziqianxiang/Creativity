{
    "Decision": "",
    "Reviews": [
        {
            "title": "results look good, but details unclear",
            "review": "The paper describes a method for self-supervised pretraining of speech classification/spoken language understanding model which incorporates both speech audio and text corpora to learn a representation which can be easily fine tuned on downstream classification or question-answering tasks with speech input. The proposed method is similar to some recently published work in SLU, but is able to achieve better performance using less labeled training data.\n\n### Positives\n- Seemingly simple yet effective unsupervised pre-training approach can lead to good performance on several spoken language understanding tasks.\n- Beats state-of-the-art on some benchmark tasks.\n\n### Negatives\n- Key training and experiment details are not clearly explained which 1) makes it difficult to interpret the results and draw conclusions, and 2) limits reproducibility.\n- Underexplores token-level alignment, experimenting with only a very simple alignment scheme, which significantly underperforms the simpler sequence-level approach.\n- Limited novelty.  The proposed model is heavily based on Denisov and Vu, 2020 and Zhang et al. 2020a.  Given the significantly improved performance this isn't a problem.  But a more careful discussion/experiments exploring why this approach is able to outperform previous work which uses on larger datasets would strengthen the paper.\n\nOverall, the results seem good, but the too many details are unclear in the text.  The paper would be significantly improved by expanding the description of the full training procedure (Sec 3.4) and different model variants used in the experiments (Sec. 4.1).\n\n### Detailed comments:\n\n- Sec 1, para 3: typo \"languaget\"\n- Sec 2, Might be good to add a reference to this recent work https://arxiv.org/abs/2006.11477, which gets extremely good ASR performance with self-supervised pretraining.\n- Sec 3, sentence 2: typo: \"architecture learning algorithm\" should be \"architecture and learning algorithm\"\n- Sec 3.3., Token-level alignment, Figure 2.: The alignment mechanism here is a bit unusual.  Is there any reason not to use more conventional soft-attention (as in transformer encoder-decoder attention) across all audio frames, enabling multiple frames to contribute to the loss for each text token?  This would be as simple as replacing the max in equation (3) with a softmax.  Given that the token-level alignment didn't work as well as the sequence -level alignment in the experiments (at least when paired with MLM pretraining), it seems worth at least a brief discussion of alternate approaches which might improve performance.\n- Sec 3.4, \"we then randomly sample 10 hours of transcripts\": Are these sampled from the same train-clean-360 subset as the audio used to pretrain the speech module?  Please clarify.\n -  How many utterances does 10 hours of audio correspond to (roughly 3000?)?  This seems like an extremely small dataset to use for pretraining a BERT-style MLM. Why not train on a larger corpus given how easy it is to collect text for language modeling?  This deserves some more discussion.\n  - It would be helpful to explicitly enumerate all of the training stages.  There appear to be 4: 1) speech MLM pretraining 2) text MLM pretraining 3) fine-tuning (both of?) the above as part of speech and text alignment task 4) fine-tuning on a downstream SLU task.\n- Sec 4.1.  The description of different variations of AlignNet is very unclear.  In particular, the difference between AlignNet-Seq and AlignNet-Seq-MLM (and similarly for the Tok variants) is  confusing.  What does it mean for the language model to be \"update with MLM\"?  According to Sec 3.4, only the speech module is used for fine-tuning.  Is this about the alignment training stage (stage 3 in the list above?).  The text does not specify whether the MLM losses are used during that stage.\n- Table 2.  The numbers shown fpor Denusov & Vu appear to be incorrect.  They correspond to performance using ground truth speech transcripts, not the learned SLU model.  I believe the correct numbers would be 95.5 for FSC and 60.2 for SwBD.\n- Sec 5.1.  \"version of AlignNet which uses only 1 hour...\"  What is this 1 hour subset used for? Only learning the speech-text alignment?  Is the text MLM still pretrained on a 10 hour subset?  Please clarify\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Pre-training jointly from speech and text data help; some questions about the experimental comparisons and lack of analyses",
            "review": "In this work, the authors propose a technique to pre-train models on unlabeled speech and text data in order to use them in downstream spoken language understanding applications. The main novelty of the proposed work, is to train the speech and text modules using a masked LM task on the unlabeled data, along with a component of the loss which attempts to ensure that the embeddings learned in the speech and text spaces are aligned to create a shared semantic latent space. In experimental comparisons, the proposed methods are shown to achieve strong results on four tasks.\n\nMy main concerns about this work are related to the experimental comparisons and the lack of more detailed analyses of the results\n\nMain Comments:\n1. The idea of creating a semantic latent space for the speech input has been explored in the past. Many of these works have focussed on using light supervision (example: images and spoken captions describing the images, but without any explicit transcription). I think that the authors should cite the following works in the related work section, and discuss how the proposed methods relate to these prior works:\n- The numerous works of David Harwath et al. on this topic, for example\nDavid Harwath, Antonio Torralba, and James R. Glass, “Unsupervised Learning of Spoken Language with Visual Context” Proc. Neural Information Processing Systems (NeurIPS), 2016.\nDavid Harwath, Galen Chuang, and James Glass, “Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech,” Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n- Also the works of Settle and Kamper: \nKamper, Herman, et al. \"Visually grounded learning of keyword prediction from untranscribed speech.\" Proc. of Interspeech, 2017.\nSettle, Shane, et al. \"Acoustically grounded word embeddings for improved acoustics-to-word speech recognition.\" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.\n\n2. Questions about the datasets used in the experimental comparisons: One aspect of the paper that I would like to clarify is the data that was used in the authors’ experiments and the baseline systems that the authors compare performance against. \n- (Lugosch et al., 19) appear to use all of the Librispeech960 data, whereas as far as I understand, the results in the paper are reported when training on a subset of the train-librispeech-360 data. This seems like it would demonstrate a benefit of the proposed approach, and I think this should be highlighted.\n- The dataset description in (Duran and Battle, 18) mentions very different statistics for train/validation/dev sets (See Table 1 of their paper) relative to Table 1 of the authors’ paper. Does this mean that that numbers reported in Table 2 are not directly comparable?\n- The authors report 75.9% accuracy for the system of (Ghosal et al., 18). I wasn’t sure what number was being quoted here. Is this the performance of the MU-SA unimodal system? I would suggest that the authors clarify the settings in which the models are evaluated and how the authors’ configurations compare against the previous work.\n- The authors report results on the Spoken SQuAD dataset from (Chuang et al., 20), but I think it’s worth pointing out from that paper that the cascade (non-end-to-end configuration) actually does significantly better (and comparable to the authors’ results). I think the authors should add that result to Table 2 as well.\n\n3. I feel that the paper could be strengthened by conducting more analyses of the results. For example, MLM pre-training of the language module seems to be important to get good results, although this is done with a very small amount of data. However, this is the case with all model configurations except for one -- AlignNet-Tok-MLM appears to perform worse than AlignNet-Seq-MLM on the Spoken SQuAD task, which is counter-intuitive (65.9 vs 58.0). I think the paper could be strengthened by some analysis of the results to understand why this is the case.\n\n4. A high-level question about the way the authors define the speech module. If I understand correctly, the authors 80-dim log-mel filterbank features. What frame rate do the authors use (as far as I can tell, this isn’t mentioned in the paper)? Assuming that the authors use the standard 10ms frame rate, I am somewhat surprised at the results because consecutive log-mel features are typically highly correlated, so I would have imagined that the specific masked LM task would be relatively simple unless entire contiguous chunks of frames were removed. The authors seem to suggest that they “blank out” individual frames with probability 0.15. Could the authors comment on this?\n\n5. I’m curious about Equation 3. Was the weighting by IDF of the term important to get good results? \n\n6. If I understand correctly, (Denisov and Vu, 20) mention that they don’t use it during fine-tuning because it hurts performance. In Section 3.1, the authors mention that they use SpecAugment for channel masking. Could the authors clarify if they use SpecAugment on the channel dimension for both pre-training as well as fine-tuning, or just for pre-training.\n\nMinor Comments: \n1. Why is the special token referred to as [CLS]? (for example, as opposed to SOS (start of sentence)). I’m curious what CLS stands for.\n2. Page 4, Token-Level Alignment: The authors mention that it is “very tedious” to get frame-level alignments through forced-alignment. This is a very commonly used operation in speech recognition tasks, and excellent open-source toolkits such as Kaldi are available for this (especially on open-source sets like Librispeech). I would suggest that the authors re-phrase this, for example to say that forced-alignment requires a pre-trained ASR system, etc. instead. \n3. A clarification question about Section 3.4: For the language module pre-training step, the authors use 10 hours of text-data. Is the following alignment step conducted on the same 10 hours of data, but with the corresponding paired acoustic data?\n4. Page 6, Spoken question answering: “The model is evaluated by …; the more overlap between the predicted span …”  --> “The model is evaluated by ...:; the greater the overlap between the predicted span …”\n5. It would be easier to read the results in Table 2 if AlignNet-Seq-MLM and AlignNet-Seq-MLM 1-hour were closer together to clearly indicate that they are related. For example, by re-ordering the Tok and Seq results.\n  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A straightforward speech-language pre-training approach with \"it-depends\" results",
            "review": "Summary\n\nThe paper proposes several variants of a relatively straightforward speech-language pre-training approach, and applies it to four downstream tasks. The model consists of a BERT text path and a BERT-like audio path that are combined with an \"alignment\" loss during parts of the pre-training (hence the \"semi-supervised\" property) to encourage that the audio path learns representations that are similar to the text path. This allows the audio path to be fine-tuned more easily towards four downstream tasks. The results are interesting, but after reading the paper I am not sure I understand if and when the approach is better than other approaches, and when it would really want to use it - the results say \"it depends\" to me and seem to require more analysis.\n\nReasons to accept:\n\n- The paper treats an important problem, representation learning on audio, and applies generally state of the art methods. The overall story - finding a model that can go directly from audio to semantic representations - is solid, and worth pursuing\n\n- The approach seems quite straightforward and should be easy to implement and reproduce, the datasets are available, and the authors provide some ablations \n\n- The paper achieves overall good results\n\n- The paper is generally well written and uses sound practices, i.e. averaging results across three runs, etc\n\nReasons to reject:\n\n- The core approach is reasonably well motivated, interesting and simple, but I need a more detailed description of the training process, and I would like to see more analyses to better understand what is really going on:\n\n- If I understand correctly, for the best-performing \"AlignNet-Seq-MLM\" model, the speech part (L_sp) is first pre-trained (using MLM and self supervision) on 100s of hours of audio, and then L_sp, L_text, and L_seq are trained together, on 10 hours of data, correct? Or is L_sp held fixed while L_text and L_seq are being trained?\n\n- Would it not make sense to interleave these stages, e.g. train a couple of mini-batches with L_sp only, then one mini-batch with all losses together, then a few more mini-batches with L_sp only, so that the model can learn a generic representation on large amounts of data and a specialized representation on a small amount of data at the same time? \n\n- How big are the differences between the individual training runs given that they seem to comprise three stages (audio-only pre-training, multi-modal pre-training, fine-tuning)? It would be helpful to understand that this is a stable and reliable/ reproducible training scheme\n\n- It is interesting to see that the \"simple\" sequence-based alignment outperforms token-level alignment. Did you try out other more advanced methods to align the two sequences? I understand that BERTScore would inspire me to try this kind of approach on this task/ data\n\n- How fair are the comparisons? E.g. \"Ghosal et al. (2018)\" was trained on audio only, if I understand correctly, while the proposed approach requires transcriptions on at least part of the (pre-)training data. The authors assumption is that both models have been fine-tuned on the same amount of data and under the same (audio-only) conditions? \n\n- Table 3 shows an ablation study and compares several variants of the two main approaches on different amounts of fine-tuning data. The results show that \"AlignNet-Seq-MLM\" performs best, but its advantage over the simpler methods varies greatly: for FSC, \"AlignNet-Speech\" may be fully sufficient, while for MOSEI even more data may be needed to see the same effect, but there are no clear conclusions beyond that\n\n- There is also another \"AlignNet\" (https://arxiv.org/abs/2007.08973) which may give rise to confusion\n\nInitial review:\n\nThis is interesting work and I'd like to see it pursued further, but for the reasons outline above I feel like the paper is not yet quite ready for publication. The approach seems overall interesting, but there is more than one or two experiments missing before I could be convinced that this is solid.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}