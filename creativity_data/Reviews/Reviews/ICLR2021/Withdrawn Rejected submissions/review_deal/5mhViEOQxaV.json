{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper first aims to propose a new controllable Pareto multi-task learning framework to find pareto-optimal solutions. But after the revision according the comments, the paper claims to find finite Pareto stationary solutions. But the paper still can not prove their proposed method can find the Pareto stationary solutions. Even if they can find the  Pareto stationary solutions, they can not guarantee find the pareto front which is conflict with the experiments and claims. There are major flaws in the paper."
    },
    "Reviews": [
        {
            "title": "a paper with overstatements",
            "review": "This paper proposes a controllable Pareto multi-task learning model by generating the Pareto-stationary solutions.\n\nEven though the idea to generate a Pareto-stationary solution seems interesting, the proposed method is overstated. It is well known that the MGDA can find Pareto-stationary solutions, which however are NOT Pareto-optimal solutions. The steepest gradient descent method to solve problem (10) is just the primal form of the MGDA method and hence it cannot find the Pareto-optimal solution. To the best of my knowledge, there is no method which can guarantee to find a Pareto-optimal solution for general multi-objective optimization problems. Authors claim that their method can generate Pareto-optimal solutions but actually they can generate only Pareto-stationary solutions at most. Authors confuse the Pareto-optimum with Pareto-stationary solutions and this is misleading. In this sense, the proposed method is not so appealing as Pareto-stationary solutions are easy to obtain in many methods.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Multi-Objective Optimization for Multi-Task Learning",
            "review": "The paper proposes a method to controllably generate models on the trade-off front of multi-task learning problems. The key idea is to use a hypernetwork that will generate the MTL parameters on demand conditioned on the desired trade-off. The hypernetwork can be trained along with the MTL in an end-to-end manner.\n\nStrengths:\n+ Paper addresses an important practical problem. Instead of training many models independently to span the trade-off front, it can generate an MTL model on demand for any desired trade-off point.\n+ The idea of using a hypernetwork in the context of MTL is new. \n\nWeaknesses:\n- Conceptually and technically the improvements over [1] are minimal. The main contribution of the paper is to use a hypernetwork that will generate the weights. Although this is new in the context of MTL, it is in fact a common idea in the context of Neural Architecture Search. In NAS we have a supernet which is equivalent to the hypernet and then perform reference based multi-objective optimization over it. See [2] for an example.\n- Method does not seem to be very scalable in terms of MTL model size. The experimental evaluation is on a small scale. For larger models the hypernet needs to generate a large output for the desired trade-off. In that case training the hypernet gets more challenging.\n- In terms of the exposition, firstly the main paper has very few details apart from the high level idea, there is a lot of repetition of the same points. Secondly, a lot of the actual paper and the appendix itself is background material that is already well known in the multi-objective optimization literature. \n- The experiments are a bit disappointing. The advantage of the proposed method in comparison to linear scalarization would be on concave pareto fronts as shown in Fig. 6. Unless I am missing something, looks like in none of the actual experiments is the trade-off a concave curve.  \n\nOverall, in the current form the paper looks more like a proof of concept. I would encourage the authors to demonstrate or discuss the scalability of the solution.\n\nClarifications:\n1) I guess one of the benefits of this approach over training multiple models is in terms of the total number of parameters and computational complexity. But there is no discussion of these aspects. How does the HyperNet compare in size wrt to the MTL model part? I would imagine the HyperNet is much bigger since it has to learn to predict the MTL parameters. \n2) How about inference? How long does it take for the HyperNet to generate the parameters since you still have to solve an optimization problem? \n3) The paper claims real-time but there is no discussion of this claim.\n4) In Fig.7 there seem to be some solutions that are better than the trade-off front obtained by the proposed method. Any comments on what is limiting the current approach from reaching/surpassing those solutions?\n\n[1] Pareto Multi-Task Learning\n[2] Neural Architecture Transfer",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed technique does not match with the announced contribution.",
            "review": "This paper proposes a novel controllable Pareto multi-task learning framework, which aims to learn the whole Pareto optimal front for all tasks with a single model. The motivation is straight forward and the proposed method is inspiring. However, the proposed technique does not match with the announced contribution.\n1. This paper announces that it proposes a novel Pareto solution generator that can learn the whole Pareto front for MTL. However, actually, this paper adopts fixed shared parameters (pretrained feature extractor) and only optimizes a part of parameters of a MTL, which degrade the solution space and the solutions may not be the Pareto solutions. The Pareto front generated by such method may not be the real Pareto front for MTL.\n2. Using fixed shared parameters conflicts with the essence Pareto MTL. In MTL, the tasks mutually regularized by the feature extractor, which improve the generalization ability of each task. \n\nThis paper is globally well organized and clearly written. However, some important details are missing.\n1. The details about the hypernetwork are unclear.\n2. The paper lacks of analysis on the experimental result.\n3. Some notations are not clear, e.g., does the loss used in the paper denotes empirical loss?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}