{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper is about a reinforcement learning algorithm that operates in a Constrained MDP and is provided with a baseline policy.\nAlthough the reviewers acknowledge that the paper has some merits (well-written, clearly organized, significant empirical evaluation, reproducible experimental results), some concerns have been raised about the novelty of the proposed solution and of its theoretical analysis. The reviewers feel that the authors' responses have not properly addressed all their doubts.\nThe paper is borderline and I think that it is not ready for publication in the current form.\nI encourage the authors to update their paper following the reviewers' suggestions and try to submit it in one of the forthcoming machine learning conferences. "
    },
    "Reviews": [
        {
            "title": "Good paper, some aspects to discuss and improve.",
            "review": "update after rebuttal: the authors answered all my questions to my satisfaction.\n\n* Summary and Contribution\n\nThis work presents a novel approach to safe reinforcement learning. In particular, in line with several other works, the others propose to start from a baseline policy, which is then improved. Three key steps are identified: (1) updating the baseline to maximize expected reward, (2) measuring and controlling the ``distance'' between the new policy and the baseline, and (3) projecting the policy to satisfy given safety constraints. The authors provide both the an theoretical analysis and a number of experiments. \n\n\nReasons for Score\nI believe this is a nice paper with good contribution and valid evaluation, but some parts can be improved.\n\nStrengths\n\n- A novel method for safe RL\n- The baseline policy is not required to already satisfy the safety constraints\n- A thorough theoretical analysis\n- Convincing, though small-scale, experiments\n\nWeaknesses\n\n- Some related work should be discussed. The statement that all other approaches using a baseline do require a safe baseline is not entirely true and needs further detailed discussion. \n- Better motivation why this is Safe RL -> How to 'safely' obtain a baseline?\n- The approach seems like a (nice) collection of already established building blocks. \n\n\nQuestions for Authors\n\n- I would like a better discussion on why it is realistic to have a baseline policy. In a (physical) safety-critical scenario, any baseline may be either so bad that the approach is not effective anymore, or it may be obtained using actual unsafe interaction with the environment.\n\n- A key step of the approach seems to be 'Trust region policy optimization' work, which is well known. How would you argue that your work is really novel, and not just reusing that approach? \n\n- The paper states that other approaches using a baseline require it to be safe. I do not think that is true, see the detailed comment.\n\n- Please compare your work to the literature on model repair, see the detailed comments.\n\n\nDetailed comments\n\n- safe baselines, please discuss: \n\n[1] makes two assumptions: a set of safe states is given, and there is regularity for the safety function. My understanding is that these assumptions allow the agent to explore within the safe region until it learns enough to explore new areas. Also, [2] and [3] seem to make the same assumptions. [3] states that \"Our approach [...] does not require a backup policy but only a set of initially safe states from which the agent starts to explore.\"\n\n\n\n[1] Akifumi Wachi, Yanan Sui 2020\n   Safe Reinforcement Learning in Constrained Markov Decision Processes.\n   In International Conference on Machine Learning (ICML) 2020\n\n[2] Sui, Y., Gotovos, A., Burdick, J. W., and Krause, A.\n  Safe exploration for optimization with Gaussian processes. \n  In International Conference on Machine Learning (ICML), 2015\n\n[3] Turchetta, M., Berkenkamp, F., and Krause, A. \n  Safe exploration in finite Markov decision processes with Gaussian processes. \n  In Neural Information Processing Systems (NeurIPS), 2016.\n\n\n- model repair, please discuss\n\nIn the essence, and admittedly, the work [4,5]listed below is from the formal methods community, so it may just not be known for ICLR. Yet, what happens: A model (for instance, a policy), is 'repaired' to satisfy (temporal logic) safety constraints, with minimal distance to the original policy. At least a discussion is in order.  \n\n[4] Shashank Pathak, Erika Ábrahám, Nils Jansen, Armando Tacchella, Joost-Pieter Katoen:\nA Greedy Approach for the Efficient Repair of Stochastic Models. NFM 2015: 295-309\n\n[5] Ezio Bartocci, Radu Grosu, Panagiotis Katsaros, C. R. Ramakrishnan, Scott A. Smolka:\nModel Repair for Probabilistic Systems. TACAS 2011: 326-340\n\n\n- general related work:\n\nThere is some other related work both on Safe RL that could be discussed.\n\n\nPrashanth L, A., and Michael Fu. \"Risk-sensitive reinforcement learning: A constrained optimization viewpoint.\" arXiv preprint arXiv:1810.09126 (2018)\n\nZheng, Liyuan, and Lillian J. Ratliff. \"Constrained upper confidence reinforcement learning.\" arXiv preprint arXiv:2001.09377 (2020).\n\nSebastian Junges, Nils Jansen, Christian Dehnert, Ufuk Topcu, Joost-Pieter Katoen:\nSafety-Constrained Reinforcement Learning for MDPs. TACAS 2016: 130-146\n\nNils Jansen, Bettina Könighofer, Sebastian Junges, Alex Serban, Roderick Bloem:\nSafe Reinforcement Learning Using Probabilistic Shields. CONCUR 2020: 3:1-3:16\n\nGuy Avni, Roderick Bloem, Krishnendu Chatterjee, Thomas A. Henzinger, Bettina Könighofer, Stefan Pranger: Run-Time Optimization for Learned Controllers Through Quantitative Games. CAV (1) 2019: 630-649\n\nMohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening:\nCautious Reinforcement Learning with Logical Constraints. AAMAS 2020: 483-491\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solving CMDP faster with the help of a baseline policy",
            "review": "The paper considers the constrained Markov Decision Process (CMDP) problem where the goal is to maximize cumulative reward while satisfying the safety constraint on cumulative cost. Solving CMDP is challenging, and this paper proposes a shortcut by utilizing a given baseline policy. The idea of the proposed SPACE algorithm is to have three steps for each iteration of policy optimization. The first step is a trust region optimization step to optimize reward, the second step projects the policy to a region close to the given baseline policy, and the third step projects the policy to the constraint set. \n\nCompared with other CMDP methods, the step of projecting close to the baseline policy allows the policy to quickly improve if the baseline policy is decent. Compared with some behavior cloning methods, the steps of reward optimization and constraint satisfaction allows the policy to achieve good final performance. I feel the ideas of the proposed SPACE algorithm is good, but I have some concerns for the current version.\n\n- The constraints (5) and (6) in Step 2 and Step 3 need to be further discussed. From the description of Step 2, the constraint (5) should be $J_D(\\pi) \\leq h^k_D$ for the region around $\\pi_B$. One can replace $J_D(\\pi)$ by $J_D(\\pi^k) + \\frac{1}{\\gamma} \\mathbb{E}_{s\\sim d^\\pi, a \\sim \\pi}[A^{\\pi^k}_D(s, a)]$ by using the performance difference formula, but the state distribution is different from the distribution in (5). It seems like (5) could be some approximation, but how it relates to the desired region and how the guarantee may be affected should be discussed. Similar for (6).\n\n- In Lemma 4.1, only a big-O value for the update of $h^{k+1}_D$ is provided and no description on what the big-O hides. Since this value is required in SPACE, it's not clear how to implement the algorithm without the specific value. According to the proof of Lemma 4.1 in the appendix, this value seems to depend on some expectation whose computation may not be trivial.\n\n- The practical version of SPACE described in Algorithm 1 does not solve the optimization problems (4)-(6). Since it uses (7) to update, the actual three steps of SPACE are the approximations (16)-(18) in the appendix. I think the actual steps should be stated in the main text with discussion on the effect of the approximations.\n\n- For experiments, it is very confusing by introducing f-(P)CPO and d-(P)CPO without any explanation on the weight and the weighted objective. It is also unclear what is the value of $h_C$ and not possible to evaluate constraint violations of each methods.\n\n- The comparisons with f-(P)CPO and d-(P)CPO is interesting, but the comparison with these baselines provides little idea for the main messages of the proposed method. SPACE is supposed to learn quicker compared to other CMDP methods by using the baseline policy, but all the compared methods actually use the baseline policy in some non-efficient way. Instead, the comparison with original CPO and PCPO will be much more meaningful as they might show the accelerated learning of SPACE, but all methods might have similar final reward/cost performance given enough training samples. Another possible comparison would be with other behavior cloning methods given a good baseline policy.\n\n\nAfter reading the authors' response, I would like to thank the authors to clarify some of the my questions. But my main concerns still remain so I plan to keep my marginally below score. The value of the key bound in Lemma 4.1 is unknown and it's not clear about the effect of the described empirical trick. For the experiments, the proposed SPACE algorithm indeed outperforms all considered baselines. But performing better than all baselines in the final performance seem to indicate that the choice of baselines and experiment design do not present a fair comparison. One would expect the final performance of a reasonable baseline algorithm to be similar to SPACE because the idea of the proposed method is to \"accelerating learning\" not achieving better final performance. The poor final performance of the baselines seems to be due to the inappropriate adaption of existing methods.\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Accelerating safe reinforcement learning with constraint-mismatched policies",
            "review": "Summary of review:\nA clear problem statement & technical approach for an important problem, supported by positive empirical results, and some theoretical analysis.\n\nDescription:\nThe paper proposes a 3-part approach to constrained RL:   (1) perform policy optimization, (2) minize distance to the baseline policy, (3) project the policy into the policy space that satisfies the constraints.  The analysis shows better empirical performance, and a finite-sample guarantee.\n\nStrengths:\n-\tThe paper’s overall approach is easy to understand, and decomposes in a few conceptually simple steps.\n-\tI appreciated that the experiments considered tasks with both safety constraints and fairness constraints.  The latter are seldom considered in RL papers, yet very important in practice, so it’s nice to see this considered here.\n-\tThe paper includes a good range of baseline comparisons.\n\nWeaknesses:\n-\tThe theoretical analysis uses several simplifying assumptions (e.g. approximation of the reward fn and constraints, and those in Assumption 1.)  What is the impact of these? Are they typically met in benchmarks?  What is the usefulness of Theorem 5.1?  I wasn’t sure what to make of this result, what it was significant or useful.  Can Eqn 12 give one intuition of when to use more / less data?  \n-\tKey experimental details are missing.  How many seeds did you use for the experiments?  Why show standard deviation rather than standard error?  How were the hyper-parameters selected?\n\nQuestions:\n-\tTable 1: There are many more works on constrained MDPs. What was your inclusion/exclusion criteria for this table?\n-\tWhy is the Disc. reward for SPACE and f-PCPO going down in Grid (Fig.3, top right plot)?  This should be explained.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work, but it might lack novelty",
            "review": "Summary\n\nThe authors propose SPACE, an RL algorithm for learning a policy that maximizes reward while satisfying given constraints in a setting where a baseline policy is provided. They design a three-step update rule for learning such a policy and provide a finite-sample analysis of the resulting method in a simplified setting. They report numerical results in several domains that show how the proposed approach outperforms competitive baselines.\n\nPros\n\n- The paper is very well-written and easy to follow.\n- The experimental evaluation is performed in complex domains. The results are significant and well-described. Full details are provided in the appendix to enable reproducibility.\n\nCons\n\n- The paper might lack some novelty. The approach seems quite incremental with respect to Yang et al. 2020 (though I am not very familiar with that work to precisely judge this). In particular, handling the additional constraint due to the baseline policy seems not that different from how the \"cost constraint\" is handled, and indeed the two approaches have a very similar update rule. The experimental domains seem also the same (except for the car racing one which was not present in Yang et al. 2020).\n- The setting considered for the finite-sample analysis might be overly-simplified. In particular, the analysis seems not really \"finite-sample\" but rather \"finite-time\", in the sense that it is carried out for a finite number of learning steps in a standard optimization setting where all functions (e.g., the gradients) can be exactly evaluated and are not estimated from finite samples.\n\nDetailed comments\n\n1. While it is intuitive why we need to project the policy onto the set of policies satisfying the constraint in Eq. 2, it was not very clear to me why we need to perform a projection to stay close to the baseline policy (especially since the actual \"closeness\" h_D is changed by the algorithm). It seems that, after all, what we care about is to stay close to this policy during the initial phases of learning (if possible) and slowly stop caring about this as learning proceeds. So could we simply regularize the standard RL objective with the distance wrt the baseline policy and make the regularization parameter fade over time? Would that perform worse than projecting?\n\n2. Lemma 4.1 provides a formal justification for increasing h_D over time. How is the actual increase performed in practice? Is there any fixed \"increase value\" or is the theoretical value from the lemma used? \n\n3. Could you provide some intuition on why the finite-sample bound for SPACE with KL divergence scales with the maximum eigenvalue of F^k while the one for l2-norm scales with the minimum eigenvalue of the same matrix?\n\n4. It seems that the work of Yang et al. 2020 also reports a similar convergence guarantee as the one derived here. Could the authors clarify what are the technical difficulties in extending such a result to this setting? It seems to me that after the simplifications the two considered settings are quite similar.\n\n5. In the experiments, the authors mention that \"PCPO agent cannot robustly satisfy the cost constraints\". Why does this happen? Doesn't PCPO handle the constraints in the same way as SPACE?\n\n6. Why is there no baseline for comparison in Figure 4?\n\nSome minor comments:\n\n- In the caption of Table 1, I would say \"w/ expert/optimal demonstrations\" to be more precise and \"constraint-cost satisfaction\" (cost alone might be interpreted as the negative reward)\n- Third line of page 8: \"gird\" -> \"grid\"\n\nOverall comment\n\nOverall I believe that the paper is interesting and it reports good results. However, at the present time, I am not sure about the novelty wrt Yang et al. 2020, so I am quite borderline. I will be happy to increase my score after the authors have clarified my doubts.\n\nReferences\n\nTsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based\nconstrained policy optimization. In Proceedings of the International Conference on Learning\nRepresentations, 2020.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}