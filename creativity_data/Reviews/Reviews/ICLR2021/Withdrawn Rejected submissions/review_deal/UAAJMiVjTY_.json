{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper addresses the difficult problem of combining ILP in a meta-interpretive framework with noisy inputs from a neural system.   The essential idea is to use MIL to \"efficiently\" search for constraints on the neural outputs (eg z1 + z2 + z3 = 7, or z2< z3) as well as logic programs, with a score related to program complexity as well as probability of the best constraint-satisfying neural outputs.  It is interesting work for the right audience but it's clear from the reviews that the presentation was difficult for ICLR readers, even ones with appropriate background. \n\nSome potential weaknesses of the approach include:\n\n1 - it's unclear how scalable the MIL framework is - presumably the intrinsic difficultly of the search means that programs and constraint sets must be small\n\n2 - it's unclear how general the approach is beyond the digits-as-separate-inputs setting of the two experimental studies, and its unclear how accurate the perceptual layer needs to be - MNIST obviously being an example of a case where there is little noise with a modern classifier.\n\n3 - it's unclear how constraints can in general be used to backprop any information to the underlying neural system, and without this the joint training seems to be quite limited.\n\nOverall the paper is judged as inappropriate for ICLR."
    },
    "Reviews": [
        {
            "title": "Potentially exciting, but not explained well enough",
            "review": "I like the idea of this paper, however the paper seems to be more written to impress rather than inform.\n\nI cannot see how it \"learns ... simultaneously from raw data.\"  I think the interface is in the nn(image=value,prob) but nn() is never explained in the text. I can't work out: what are the inputs? What are the outputs? What. is the relationship between them? Similarly,  prove() used in Figure 2 is never explained; what are the argument? What is the intended interpretation?\n\nIt seems funny to have 0/1 probabilities (equation (4)). Surely there is so much noise that you can't perfectly predict the outputs.\n\nI didn't see how you overcome the exponential complexity promised in the abstract (and why do you think it is caused by the interface?).\n\nIn the experiments, why is MAE (or log MAE) a reasonable measure? What is the accuracy of the correct program? (I think there is supposed to be a correct program you are learning).  Is the correct program in the search space with a non-zero prior?\n\nIs \"Acc\" in table 2 correspond to just predicting the digit? Why is there so much variability? What is the accuracy of nn()?\n\nThe paper needs to be self contained. For example, you need to tell us that #= means equality and what Prolog's permutation  predicate is  (is it related to permute() in Figure 3?)\n\nI understand you are trying to learn the simplest logic program that can produce the output from raw images. What you did at the top-level seems right, but it is not described well enough. You need to provide enough details so that it is reproducible.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary.\n\nIn this paper, the authors have presented a framework that combines meta-interpretive learning and the abductive learning of neural networks. The high-level idea is to formulate a unified probabilistic interpretation of the entire algorithm so that both the inductive logic programming module and the neural network modules can be trained jointly from data. The authors have demonstrated the application of the proposed algorithm to learning arithmetic operations and sorting operations by looking at input-output mnist digits.\n\nComments.\nThe key idea of the paper has been presented clearly. The authors demonstrated two tasks: cumulative sum/product, and sorting. Both tasks require learning recursive rules, and the bogosort task requires predicate invention. These are challenging tasks for both neural networks and ILP algorithms.\n\nHowever, my major comments about the paper is that the experiment sections are relatively weak and they have definitely missed some important baseline comparisons. Concretely, taking the cumulative summation task as an example, the MetaAbd model has very strong inductive biases, because of the builtin \"add\" operation and the metarules built into the system, which strongly favors recursive rules of specific forms. However, at least the \"add\" operation was not built into other baselines.\n\nSecond, there have also been many other works trying to solve this task:\n- partial ILP (Evans & Grefenstette, 2018) and machine apperception (Evans et al.,\n2019) that can learn mnist digits with much weaker assumptions: they can even learn the \"succ\" relationship between digits.\n- Neural GPU (Kaiser and Sutskever 2015) that can learn to add multi-digit numbers without any builtin \"add\" operations.\n- Differentiable Neural Computer (https://deepmind.com/blog/article/differentiable-neural-computers)\n- Neural Programmer-Interpreters (Reed et al 2015) and its follow-ups: they support integrating human-written primitive functions (such as the \"add\" operation) with neural networks.\nThe authors are encouraged to make comparisons with these methods as well.\n\nThird, the learned logic rules are relatively simple. This makes me less convinced about the applicability of the paper. The authors have made very strong claims in the abstract/intro about \"To the best of our knowledge, MetaAbd is the first system that can jointly learn neural networks and recursive first-order logic theories with predicate invention.\" For example, partial ILP and machine apperception can do that, too. Recently, there have also been other trials on using relational neural networks for bridging perception and rule learning, such as,\n- Graph Neural Networks (https://arxiv.org/abs/1806.01261)\n- Neural Logic Machines (https://arxiv.org/abs/1904.11694)\n\nOverall, I think this paper is not matching the publication standard of ICLR.\n\nMinor:\nPlease change the latex formatting of the model name. There is currently an extra space between M and e.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "In this paper, the author proposes Meta_abd which is a hybrid model that learns a deep recognition model and FOL rules the same time. The goal of this work is to learn FOL rules from raw data such as digits presented in image patches in an end-to-end fashion. The model is evaluated with 3 induction benchmarks associated to the MINST digit dataset.\n\nPersonally, I find this paper to be difficult to read and many details in methods and experiments are missing, making it hard to understand the authors' contribution. Given its current state, I would recommend rejection. My concerns are as follows:\n\nMotivation:\n\n- Neural-symbolic integration usually refers to combining logic reasoning into deep model's decision process for better interpretability or sample-efficiency, or to use deep models to help the logic reasoning tasks such as ILP or deduction.\n\n- With that being said, I find the claim for this hybrid model to be unjustified. In Meta_abd, NN is only used for data pre-processing which is completely agnostic to the later logic component.\n\n- In fact, diff-ILP (Evans & Grefenstette, 2018) also uses NN for pre-processing MNIST digits for ILP task which I personally find to be similar to the proposed method, though I'm happy to be proven wrong.\n\nClaims: I find many of the claims in the paper to be ambiguous and lack justifications\n\n- In section 2, the author claims that differentiable ILP methods rely on fully trained NN for pre-processing -  this is untrue, for example NeuralLP is an end2end model that can be extended to the MINST benchmark with a perception module that's jointly trainable\n\n- The author also claims that most existing NeSy systems only utilize a pre-defined knowledge base. I find this claim to be confusing and the author does not discuss how the proposed method has addressed this limitation\n\t\n\nMethod: I find the method to miss many details\n\n- The author defines the learning problem with Eq1 and Eq2 in Section 3.1 and claims to it would be learned through EM. However, Eq1 and 2 do not reveal details about the method, and the exact procedure of EM is unclear to me\n\n- In section 3.3 the author claims the proposed method is an extension of MIL, but this concept is not formally introduced in the paper.\n\n- I find Figure 2 to be difficult to understand - why is Prolog used here? What's the connection of Prolog to the proposed method? It seems to suggest the proposed method is using Prolog for solving the constraints?\n\nExperiment: \n\n- The proposed method is only compared to LSTM and RNN.  The author should include some ILP baselines such as diff-ILP or NeuralLP and substitute the digit image into its ground-truth digit symbol for reference, though I personally think adding a NN pre-processing module is straightforward  as well.\n\n- 3 benchmarks are fairly small consisting of only a few predicates and rules. How does the proposed method scale with the number of predicates and the size of the grounding space?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting EM framework for bridging perception and reasoning, but with exposition gaps.",
            "review": "The paper proposes an EM framework (Meta_abd) for iteratively learning the parameters of a neural network (\"perception\" component), and inducing the logical rules underlying a domain (\"reasoning\" component). The rule-learning component is the existing MIL system (with some modifications that are bolded in Figure 2). The paper's ability to learn recursive theories and invent predicates is directly inherited from MIL. Hence, I find its contributions with respect to these two aspects to be incremental at best.\n\nThough interesting, I find that significant details about the Meta_abd's EM model are omitted, and this prevents one from ascertaining Meta_abd's true effectiveness (please see questions below).\n\nFurther, EM as a framework for bridging noisy inputs and symbolic reasoning has been explored by prior work (the pLogic, ExpressGNN, and pGAT systems listed below). Could the authors position Meta_abd vis-a-vis these systems, and mention them as related work? Meta_abd's \"neural probabilistic facts\" map to uncertain knowledge graph triples in these systems (the \"ground logical expressions\" mentioned in Section 3.1 of the paper), and Meta_abd's neural network's parameters map to entities' embeddings. Like Meta-abd's logical component, the logical part of these systems are used to infer the probabilities of the triples (neural probabilistic facts), which in turn are used to train an embedding model. The difference between these prior systems and Meta_abd is that Meta_abd learns its logical rules whereas the previous systems require the rules to be pre-specified. In the context of these existing systems, the contribution of Meta_abd appears to lie only in learning rules with the existing MIL system, and thus seems moderately incremental.\n\n\n* Probabilistic Logic Neural Networks for Reasoning. Qu and Tang, 2019\n* Efficient Probabilistic Logic Reasoning with Graph Neural Networks, Zhang et al., 2020\n* Probabilistic Logic Graph Attention Networks for Reasoning, Vardhan et al., 2020\n\n\n\nQUESTIONS\n\n* Page 4, Figure 1: When optimizing the \\theta parameters of the neural network in the M-step, are the labels for each digit image probabilistic? If they are, how are the probabilities derived (e.g., from conflicting pseudo-labels)? If the label per image is deterministic, how is the label chosen?\n\n* Page 5, Eqn 5, step 1: How is H *sampled*? From the paper's description, it seems that H is learned deterministically from themodified MIL (Figure 2). Do you perturb the H returned by MIL probabilistically in some way? If not, how does one justify the claim that H is sampled? (Is H the mode?)\n\n* Page 5, Eqn 5, step 2: How much does the the pruning of z with the abduced constraints help? There is no ablation study to verify the contribution of the abduced constraints.\n\n* Page 5, Eqn 5: How are the parameters and/or rules initialized? EM is particularly sensitive to initialization conditions. This is something that the paper alludes to too on page 7 paragraph 2 (\"converges to saddle points or local optima\"). How does Meta_abd ameliorate this sensitivity? Does it make multiple runs of EM, each from a different initialization? If so, how does it choose the best model? What is the \"success rate\" of these runs, or how many runs are needed to learn a good model? Are the comparison systems treated fairly in being allowed multiple runs too? The answers to these questions would help to elucidate the workings of Meta_abd.\n\n* Page 6, para 2, \"provides a good initialization ... improves the learning performance\". \nHow much does the pre-trained convent help exactly? And from what base?\n\n* Could the authors comment on Meta_abd's scalability? What're its space and time complexity? This information is particularly helpful in view that the empirical evaluation only uses fairly small datasets from a narrow domain.\n\n* Supplementary material, Page 4, Figure 10:\nHow sensitive is Meta_abd to the meta-rules? Would it be fair to say that the meta-rules are cleverly specified to provide a strong enough structural bias for Meta_abd to be able to learn the correct rules? What happens empirically if the meta-rules are removed (one at a time)? These meta-rules seem to imbue Meta_abd with an unfair advantage over the comparison systems.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}