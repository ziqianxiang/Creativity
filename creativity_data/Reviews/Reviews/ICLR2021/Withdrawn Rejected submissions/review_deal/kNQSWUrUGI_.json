{
    "Decision": "",
    "Reviews": [
        {
            "title": "The topic is of interest, but the paper needs significant improvement. ",
            "review": "This paper tries to address the issues of non-iid data and differential privacy in federated learning. It proposes an algorithm that incorporates a stochastic gradient Langevin dynamics in local update. Theoretical analysis is provided to show convergence of the algorithm. \n\nStrength \n+ The problem in consideration is timely and of interest. \n\nWeakness\n- The presentation and writing of the paper need to be improved. The current version appears to be written in a rush, and many statements and derivation details are presented without much discussion and explanation. This hinders understanding of the paper. \n\nDetailed comments\n- It should be specified which F_i function is used in Equation (7). (4) was originally defined with (2) but (7) seems to use (5). \n- Many notations should be in math form. \n- There is no formal definition of F(x, lambda_1, â€¦, lambda_n). \n- Section 3.3 appears to be incomplete. \n- The experiments are poorly done. First of all, only one benchmark is compared? Second, the experimental results are not presented and discussed in details, making it very hard to see the conclusions. \n- The paper should try to better highlight the novelty and significance of the results. In particular, what is new in the algorithm and analysis? How does the new algorithm perform compared to state-of-the-art benchmarks. This could be due to the presentation of the paper. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "incomplete manuscript",
            "review": "This paper proposed an SGLD-type algorithm for the Federated Learning task with (1) non-iid distribution; (2) differential privacy guarantee (claimed in abstract). I am not an expert on Federated Learning. But, this manuscript seems largely unfinished, with very limited (and incomplete) experimental results in Section 6 and without claimed privacy analysis (incomplete Section 3.3). Overall, I cannot recommend acceptance for such unfinished work.\n\nComments:\n\n* section 3.3 seems uncompleted. No differential privacy guarantee for the proposed algorithm.\n* It is not clear to me what is the A/B/C/D in the figure of Section 6.\n* page 3, section 3.1: max-min -> min-max ?\n* too many typos",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A poorly written paper with little innovation",
            "review": "This paper considers the problem of federated stochastic gradient MCMC using stochastic gradient Langevine Dynamics. Each local node takes SGLD with respect to its local augmented Lagrange. \n\nHere are my main comments:\n\n1) In the abstract, the authors claim that non-i.i.d. data distribution and differential privacy are two main problems in federate learning. However, the DP part is only merely mentioned in section 3.3 and it seems that section 3.3 is still unfinished. Even for the first problem, section 2.1 states that \"The data are distributed i.i.d cross N distributed nodes\". So why do the authors mention these two problems while not even address one of them?\n\n2) The convergence analysis seems to be a corollary of Eberle et al. (2019). A detailed discussion of the theoretical contribution and technical innovation should be presented.\n\n3) The proofs are poorly written. Theorem 4.1 and Theorem 4.2 are not even proved rigorously. \n\nThere are many typos throughout this paper:\n\n1) Abstract: \"stochastic differential equations(SDE) We would\"\n\n2) Section 3.3: \"PoissonS ample\"\n\n3) There should be spacing before citations.\n\n4) Equation 4 and 12 use different notations of the inner product.\n\n5) etc.\n\nI suggest the authors examine their paper carefully before submission. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Recommendation to Reject",
            "review": "This paper studies two important topics in federated learning (FL), non-iid data distribution and differential privacy. It formulates the local objective as a min-max augmented Lagrangian, and proposes a stochastic gradient Langevin dynamics   (SGLD) oracle to optimize it.\n***\nStrengths: This paper can be viewed as a mix of multiple papers: Wang et al. 2019, Zhang et al. 2020, Eberle et al.2019.  \n***\nConcerns: The paper is in bad writing. Many grammatical errors make reading difficult. Notations are very unclear. For instance, x_0^r and x_i^r should be distinguished. More importantly, the paper seems incomplete. See Sec 3.3 and experiments. There are tremendous works to do to complete it. Further, the novelty is arguable. In particular, the analysis mostly come from Eberle et al.2019, and the technical contribution is unclear. Every equation is referred, which is another sign on bad writing. \n***\nFor future improvement, I recommend to emphasize the contribution of more clearly. What's the additional gain from using SGLD to replace the regime in Zhang et al. 2020. What are the technical difficulty and contribution in analysis comparing with Eberle et al.2019. The writing has to be substantially improved to make paper readable.\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}