{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of Paper1411",
            "review": "This paper considers the learnable D-Minv (LD-Minv) procedure as an iterative formula to allow for a differentiable means of approximating matrix inversion. The results also extend to problems with orthogonality constraints by taking a manifold optimization approach.\n\nThe paper presents some nice ideas in terms of establishing a problem with learnable parameters for matrix inversion based on the higher-order iterative method. However, it is generally hard to follow, as the authors are unable to clearly state what exactly the contribution of the paper is. For example, they propose LD-Minv as a differentiable method \"to perform matrix inverse efficiently.\" But, in eq.(1) they recall the well-known fixed-point problem (based on the Neumann series) that does exactly that, namely it (approximately) performs matrix inversion. Instead, they consider a parameterized version of the iterative formula (called LD-Minv), which they view as being implemented by a DNN, and for solving which they rely on a properly initialized gradient descent (Algorithm 1). However, while it is claimed that GD on LD-Minv will lead to a set of learned parameters that will somehow help with matrix inversion, there is generally confusion about approximately inverting a single fixed matrix, versus achieving small error over the learnable parameters w.r.t. a set of training data matrices A_i, i = 1,...,n. Understanding this key distinction is critical to seeing why we might even consider LD-Minv (as opposed to D-Minv) in the first place, as currently the paper is not so clear on this point. Note that the reader is told the guiding question of the work is \"Does there exist an efficient and differentiable way to perform Minv and SVD?\", and so it should then explain how the differentiable problem learned on training data helps us achieve this goal. As for the experiments, which are presented only in the appendix and are done predominantly for synthetic data, we again come to the question of whether the goal is the quickly invert a single matrix (as is seemingly claimed in the abstract), or to find a set of learned parameters that achieve small error on the training set.\n\nOverall, this work could use much work in terms of both clarifying the objectives of the work, as well as explaining why such an approach is even considered in the first place, given the fact that, as already noted by the authors, \"fixed D-Minv already converges extremely fast\". ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting algorithm with provable guarantees",
            "review": "The paper presents  a fast differentiable algorithm for matrix inverse and SVD, with provable guarantees. This makes it possible to \"train\" the algorithm in order to tune it to a particular data distribution. Finally, the paper analyzes the sample complexity of the proposed training method.\n\nPros:\n- The problems studied (matrix inverse and SVD) are some of the most widely used subroutines in data analysis. Differentiable algorithms for those problems could be very useful (although I am not sure what is the state of the art in this area).\n- The sample complexity analysis is rather rare in this line of research - most of the comparisons in the literature are empirical.\n- The paper demonstrates empirical improvements (on real data) of the proposed approach, in the context of  non-blind deconvolution (D-NbD).\n\nCons:\n- If I understand correctly,  the higher-order  iterative  method for matrix inverse computation is a known technique in the numerical analysis community. So the contribution here is in \"neuralizing\" this approach and evaluating it theoretically and empirically.\n- For SVD, I could not find any empirical comparison between the proposed technique and the power method. Is the dependence on the latter on the eigenvalue gap an issue in applications ?\n\n\n\nMinor comments:\n\n- The writing style could be better. For example, these two sentences alternate between the third and the second person:\n\n\"Specifically, manifold GD first updates the variable in the manifold tangents pace along the objective function’s projected gradient.  Then, map the updated variable in the tan-gent space to a feasible point on the geodesic...\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiments are not convincing enough to affirm that the method is fast.",
            "review": "Summary\n\nThe authors propose an iterative method for matrix inversion. Given a large dataset of matrices that should be inverted, the authors then propose to learn some parameters of the iterative algorithm, in order to have faster matrix inversion. Since this iterative method is polynomial, it allows to differentiate the matrix inversion. The authors then use their method to differentiate Riemannian gradient descent on the Stiefel manifold with the Cayley transform, which is finally used to derive a differentiable singular value decomposition.\n\nMajor comments\n- The article is cluttered, and is full of small but annoying inconsistencies (see minor comments). I found this article much harder to understand than it should be.\n- The main selling point of the authors is that matrix inversion is supposedly not differentiable, but this is not true: the differential of $X^{-1}$ is $X^{-1} dX X^{-1}$, which is not especially more costly to compute than the inverse itself (it only requires two more matrix multiplications). This operation is implemented efficiently e.g. in Pytorch and Tensorflow.\n- The authors claim that their algorithms are efficient, but they do not provide numerical experiments comparing their method to the very wide an well established literature about numerical matrix inversion.\n- The authors do not provide code for their novel algorithm, so I reimplemented their method for matrix inversion. On my computer, using pytorch, with matrices of size 200 x 200, with L = 4, one iteration of D-Minv (k=1) takes about as much time as doing torch.inverse(A), so even with the best training possible I do not see how this can be competitive with usual linear algebra algorithms.\n- The addition of the training parameters in D-Minv to make it learnable seems ad-hoc. In figure 1, it seems that there is barely any gain in learning these coefficients. (In contrast e.g. with the method of Gregor & Lecun for unfolded sparse coding, where we usually see a ~ tenfold acceleration)\n\nMinor comments\n\n- It would be worthwhile to provide pseudo code implementation of the D-Minv algorithm\n- Is there a particular reason to assume L >=4?\n- The article starts by assuming that the matrix $A$ is square, and then in lemma 1 the matrices suddenly become non-square, and the notion of matrix inverse becomes matrix pseudo-inverse..\n- In lemma 1, the authors show that $\\|AA^{\\dagger} - AX\\|$ goes to $0$. This result, by itself, does not show that $X$ converges to $A^{\\dagger}$, as the subsequent result in prop.1 is needed to show it. Further, what we are really interested in is $\\|A^{\\dagger} - X\\|$, for which the authors do not provide a bound.\n- In the beginning of Sec. 2.2, the authors mention that $\\| \\cdot\\|_F$ is the Frobenius norm, but it is not used anywhere near this sentence.\n- The statement about the exponentially fast convergence of $\\|X^K - A^{\\dagger}\\|$ to $0$ is not precise.\n- The authors do not provide a practical example where it would be useful to learn how to invert some matrices.\n- After discussion 1., the authors mention the non-convexity of the problem, but this is before the authors even introduce the corresponding optimization problem.\n- The cost function (4) sums over $k$, why not only optimize w.r.t. The output of the algorithm?\n- The appendix is a collection of diverse results, figures, and notation, and is very hard to read.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Need more experimental comparisons",
            "review": "This paper proposes a deep neural network-based approach that computes matrix inverse and SVD for massive data. Let A be a given matrix and X be a matrix corresponding to A^-1, the proposed approach is based on the observation that (AX)^-1 can be represented as the Neumann series. By introducing a learnable coefficient, this paper develops an approach that approximately computes matrix inverse of the given matrix. Besides, it introduces an approach to compute SVD based on the proposed approach. \n\nIn my understanding, the proposed approach is based on the assumption that lim_l (I-AX)^l =0 since it uses the Neumann series to represent (AX)^-1, although this is not explicitly described in the paper. This assumption could limit the usefulness of the proposed approach.\n\nAs described in Section 1, the motivation of the paper is to improve the efficiency of matrix inversion and SVD, computation and memory costs of the proposed approach are not shown in the paper. Therefore, it is difficult to evaluate the superiority of the proposed approach against previous approaches. As shown in Equation (3), D-Minv can be represented in the iterative form. Since the size of matrix E_k is d \\times d, D-Minv seems to need O(d^3L) time and O(d^2) space. When handling large-scale data, these costs are impractical. \nBesides, since no experimental results are shown in the body of the paper, it is difficult to evaluate the effectiveness of the proposed approach; the paper is not self-contained. Even though experimental results are shown in the appendix, matrix sizes are quite small although the motivation of the paper is to handle massive data. To improve efficiency of SVD, several approaches have been proposed. So, it is good to compare the proposed approach to the previous approaches. \n\n- Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions, Halko et al.,SIAM Rev., 53(2), 217–288.\n- Simple and Deterministic Matrix Sketching, Liberty et al., KDD 2013: 581-588\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}