{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposed a new in-processing approach to train fair predictors under several notions of statistical fairness. Tho this end, the author rely  on  the Exponential Renyi Mutual Information (ERMI) between sensitive attributes and the target variable as notion f fairness, and show that it is a strong notion of fairness that provides guarantees on several previously discussed fairness metrics. \n\nThe paper is overall well written and interesting, but as with many other papers on this area, I wonder even after rebuttal whether the paper indeed constitute a step forward in the field. I find the concern raised by the reviewers about the tightness of the bound important and, while the authors properly addressed this point in the rebuttal period, I still believe this is an open question which probably does not have a better answer. On the positive side, the experimental evaluation support the theoretical results. However, comparisons to previous methods are only performed on the Adult and the German dataset, which makes me wonder if the advantages of the proposed approach generalize beyond these two well-studied datasets. As a consequence, the paper remains borderline, as it is an interesting paper but its impact and significance remain limited.  \n\nMoreover, I believe that there are some missing recent related works, that I believe the authors should also compare to. For example, see the recent Neurips 2020 paper, \"A Fair Classifier Using Kernel Density Estimation\" by Cho et al.  Also, as a side note, previous approached have already considered non-binary (although most of the times categorical) sensitive features, see e.g., [42]. Finally, the author may want to consider complementing their italic comment on the second paragraph of the Intro with existing works that already discussed biased in the labels, due to e.g., the selective labeling problem (see [1-3] below).   \n\n[1] Lakkaraju, Himabindu, et al. \"The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables.\" Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017.\n[2] Kilbertus, Niki, et al. \"Fair decisions despite imperfect predictions.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n[3] Bechavod, Yahav, et al. \"Equal opportunity in online classification with partial feedback.\" Advances in Neural Information Processing Systems. 2019.\n\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "A review",
            "review": "Summary: \n\nThis paper introduces a new fairness notion that takes an exponential form of Renyi mutual information. The strongness of the notion is claimed via the proof (Theorems 1/2/3) that the notion is an upper bound of other well-known notions like mutual information, Renyi mutual information, Renyi correlation, L1 fairness violation. Two methods for estimating the notion via samples are developed under some assumptions. The performances of the methods are then demonstrated on three datasets and compared with some prior algorithms.\n\nStrengths: \n\n1. An interesting observation is made via Theorems 1/2/3: ERMI is an upper bound of other popular notions.  \n2. Explicit algorithms are developed for estimating the notion as differential functions w.r.t. model parameters.\n\nWeaknesses:\n\n1. While the observation in Theorems 1/2/3 is interesting, the proof relies upon the techniques in well-known literature and/or some standard tricks. Perhaps more importantly, it is not investigated in depth how tight the proposed notion is relative to other notions, except for the binary classification case. I believe the tightness analysis is more important, as a loose upper bound plays a less role. \n\n2. Algorithm 1: No justification is provided behind the assumption, in particular (13). Also no analysis is provided regarding the accuracy of the density estimates (16). In particular, this reviewer wonders whether or not the condition density estimate (the 2nd in (16)) is accurate – usually, a precise accuracy requires an exponential number of samples w.r.t. the cardinality of yhat. \n\n3. Experiments: Some baselines based on mutual information are missing, e.g., [A], [B]. Also in the 1st experiment (Fig. 1), (Baharlouei et al 2020) is missing – I think it is needed to compare, since the notion estimation method may be different although ERMI is equivalent to Renyi correlation. Moreover, it is not clear which algorithm to use in all experiments between Algorithms 1 and 2.  \n\n[A] B. Zhang, B. Lemoine, and M. Mitchell. Mitigating unwanted biases with adversarial learning. AIES, 2018.\n[B] J. Cho, G. Hwang and C. Suh, “A fair classifier using mutual information,” ISIT 2020. \n\nClarity: Overall it is well-written and easy to follow. \n\nOther comments: \ni. Why italic in the last sentence in the 2nd paragraph on page 1? \nii. Details on hyperparameter tuning are entirely missing in all experiments.\niii.  A benchmark dataset, COMPAS, is missing. \niv. Not clear why the proposed algorithms offer greater performances. No insight/analysis is provided.\nv. Figure 2: Only RFI is compared, although many other baselines can readily be extentisible to the non-binary classification.\niv. Figure 4: Not clear as to what points the authors wish to make here.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Confusing notations",
            "review": "The paper studies fair classification by using the notion of Exponential Renyi Mutual Information. As certain notions of fairness can be encoded using (conditional) independence, this paper propose to use some information theory notions of mutual information to quantify this degree of (conditional) independence, which indirectly translates to the degree of fairness. A classification algorithm with low value of mutual information between the prediction and the sensitive attributes can be considered as fair.\n\nThe paper establishes that the Exponential Renyi Mutual Information is a strong notion to ensure fairness: the authors show that this notion is stronger than many existing notions such as Lq fairness, etc. The authors propose an algorithm to train a fair classifier, with the mutual information being penalized in the objective function.\n\nStrength:\n- The idea of combining information theory notions to measure independency and applying it to fair machine learning is natural.\n\nWeakness:\n- I have some concerns with the mathematical notations in this paper.\ni) The definition 2 is very misleading. Consider for equalized odds with \\mathcal Z = \\{0, 1\\}: it is not clear to me why this definition correctly capture the conditional independence. To my best understanding, the conditioning here should be taken as 2 separate conditional expectation: one conditional expectation with Z = 0, and another conditional expectation with Z = 1. The mathematical definition in equation (2) does not seem to segregate the values of Z. Moreover, why is the expectation taken over Z when we already condition on Z \\in \\mathcal Z?\nii) What is D_R(\\hat Y_\\theta, S) in equation (11)? I guess the authors mean the D_R function with condition as in equation (2)?\n\n- The authors show that ERMI is stronger than existing notions, which is nice. However, it is not clear why a stronger notion is preferable for the penalized optimization of the form (11). One can think of penalizing the Shannon mutual information with higher penalty parameter lambda, and one may expect to see similar end results as problem (11) -- especially if we plot the accuracy-fairness tradeoff similar to Figure 1.\n\nMinor comments:\n- Lemma 1 is quite trivial. I think this lemma should be put as discussion in the paper, and not a separate lemma.\n- Why are the expectation in equation (15) taken with X?\n- The proof of Theorem 5 does not seem to prove what is stated in Theorem 5.\n- The notations in equation (11) can be made more explicit. For example, I think there is a dependence of \\hat Y_\\theta on X which is not made explicit.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good work, provides a unified view of fairness violation notions and propose a measure to upperbound quite a few, good algorithmic contribution and relatively solid experiments.",
            "review": "The authors provide a unified view of the existing fairness violation notions and propose a new notion: Exponential Rényi Mutual Information (ERMI) between sensitive attributes and the label. ERMI is easy to compute and provides an upper bound on existing notions. Based on ERMI, the authors propose a framework, FERMI, which can be optimized with SGD with convergence guarantee. In experiments, results show that FERMI leads to a better tradeoff between performance and fairness even if fairness is not measured by ERMI.\n\nMy major concern about the methodology is that it depends on the quality of the empirical estimate of the probability distributions P(s), P(y) and P(y,s). In the experiments, these distributions are relatively simple as S takes either binary values or are samples from Gaussian distribution. And Y is discrete and takes few values. I wonder how the performance of the FERMI would be influenced by the quality of estimates of the three probability distributions. It would be interesting if the authors can show results on the estimation error of the three vs. the performance of FERMI.\n\nIn section 3, the authors show that ERMI is an upperbound of several popular notions: Shannon MI, Renyi correlation, and Lq fairness violation. I have several suggestions and questions: (1) It would be better to specify whether some of them are known before this work. (2) In addition, is there any other transformation of Renyi Mutual Information that can achieve the same goal? (3) Is it possible to specify how tight those bounds are?\n\nIn 5.1, the authors mentioned that one reason FERMI outperforms baselines is that ERMI upperbounds the other notions. I would appreciate it if the authors can clarify this point.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}