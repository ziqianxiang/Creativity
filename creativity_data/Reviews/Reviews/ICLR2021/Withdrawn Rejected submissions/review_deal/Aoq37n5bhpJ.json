{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents a personalized federated learning approach using a mixture of global and local models. Four reviewers evaluated this paper; one of the reviewers is luke-warm (6) while the rest of the reviewers pretty negative to this work (3, 3, 3). The reviewers pointed out many weaknesses, especially about novelty, motivation, contribution, presentation, etc. Most importantly, although the idea of a \"mixture of experts\" makes sense, it is not clear what the real technical contribution of this paper is in terms of federated learning.\n\nConsidering all the comments by the reviewers, I believe that this paper is not ready yet for publication. The authors need to improve the novelty and technical soundness of the proposed direction to convince the readers including reviewers. "
    },
    "Reviews": [
        {
            "title": "This work proposes a model personalization method with a gating network that ask fuse the output of the local model and the global model, showing advantages of this approach over fine-tuning the global model with experimental results. ",
            "review": "1. Strengths\n\nThe authors target an important problem in Federated Learning: how to personalize the model to mitigate the nonIIDness.\n\n2. Weakness\n\nThe proposed method is not novel. The third step which fine-tunes in the local and global models using a gate network is essentially fusing the global and local models. It is surprising to me that this method works better than fine-tuned after FedAvg. Most importantly, such an empirical method lacks analysis or convincing experimental results.\n\nHyper-parameters are not well-discussed. The author mention that all experiments use the same learning rate 0.0001. This is definitely misleading. We have to adequately tune the hyper-parameters for each baseline and then make a fair comparison. Using the same learning rate for all baselines are wrong experimental settings. I believe fine-tuning after FedAvg can even get comparable performance if fully tune the hyper-parameters (learning rate, decay, batch size, epochs, rounds, etc).\n\nThe authors claim that “client and global models are not constrained to be the same model and could be implemented any two differentiable models.” However, the authors do not provide the experimental result for this argument. I guess when the model architecture is different, the difficulty of hyper-parameter optimization will increase, which weaken the application of the proposed method.\n\nThe proposed method has a severe efficiency problem. It requires holding three DNNs at the edge. This is impractical in federated learning where the edge devices are mainly resource-constrained (low memory, low computational ability)\n\nThe training time is not mentioned.\n\nThe proposed method does not use a client sampling strategy, a common practice in cross-device FL, to mitigate the scalability issue. What the performance if we want to learn 10 thousand sensors? Please check the original FedAvg for details.\n\nThe dataset CIFAR10 and CIFAR100 are not difficult enough to demonstrate the concept of the proposed algorithms. Does it still work in a high-resolution setting like ImageNet (224*224). I believe training three DNNs will lead to serious efficiency issues.\n\nThe opt-in and opt-out strategy is totally empirical without any intuition about why it works. That the author connects this strategy with a privacy guarantee is somewhat misleading to readers. Please provide an analysis in revision and properly describe the benefit.\n\nIn the Introduction section, the following argument is a lack of evidence. Please cite related works to make the argument more convincing. \n“Extended phases of local training between communication rounds can similarly break training, indicating that the individual client models will over time diverge towards different local minima in the loss landscape. Similarly, different distributions between client datasets will also lead to divergence of client models”.\n\nThe overall writing does not affect my understanding but can be improved. \n\nRelated works are not fully discussed. In some knowledge distillation-based method, the personalization is also their benefit. For example, FedGKT [1] also has a personal client model and a global server model, which is just a single step training method without the need of multiple steps training.\n\n[1] Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. https://arxiv.org/abs/2007.14513\n\n3. Overall Score\n\nGiven the above concerns, I recommend reject this paper in the current stage. \n\n4. Questions\n\nMay I have the comparison results between \"the naive fine-tuning after FedAvg\" and the proposed method? Please fully tune hyper-parameters for each baseline.\n\n5. Suggestions\n\nI encourage the authors to do a deeper analysis and a better experimental design. If all the above concerns are addressed, I am happy to increase the score.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting combination between Personalized FL and Mixture-of-experts, but need a major revision to improve it.",
            "review": "The paper proposed a novel personalized federated learning method using a mixture of global and local models. In particular, a gating function is proposed to leverage the trade-off of two models on the device, and this solution is inspired by a classic work – Mixture of experts (Jacobs, 1991).\n\nPros:\n\n1. The paper proposes a new federated setting by considering opt-in and opt-out. \n\n2. In step 3, the training of the local mixture-of-experts method doesn’t need to upload data and gradients to the server-side. \n\n3. The proposed personalized federated learning framework is a practical solution. \n\nCons:\n\n1. The proposed framework requires each client to choose which part of the data is sensitive. This is a very strong assumption in real-world applications.\n\n2. The proposed framework includes three steps. Step 1 & 2 are FedAvg and local supervised learning that are existing methods. Step 3 is to train a personalized local model by mixing local and global models. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. The only novelty part of the method is to apply the mixture-of-experts (Jacobs et al., 1991) method to combine the local and global models. \n\n3. The paper’s writing could be improved. The paper is an integration of the mixture-of-experts method with existing personalized federated learning. The paper's contribution is incremental.\n\n4. The authors should add a discussion to clarify how the old method (mixture-of-experts) can fit into the new environment: federated learning setting and deep learning. \n\n5. To be a paper with self-contained contents, the paper should give a clear definition of the gating model/function h^k, and how to solve the optimization problem described in equation 8.\n\n6. An overall workflow or architecture figure is recommended. \n\n7. An algorithm description is also required. \n\n8. The experiment part is too weak to validate the effectiveness of the proposed method. For example, more datasets and baselines are required.\n\n9. A typo: “We denote the number of clients by k…” It should be “denote the index of clients by k … ”\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper's experiment cannot support its claims.",
            "review": "\nThe paper proposes a federated learning framework using a mixture of experts to trade-off the local model and the global model in a federated learning setting. A three-step pipeline is designed to train personalized FL with a mixture of global model and local model. \n\nPros:\n\n1. The proposed setting is a new scenario that considers both opt-in and opt-out devices.\n\n2. The mixture of the global and local model is a reasonable solution to solve the personalization problem in FL. \n\n\nCons:\n\n1. The paper lacks an overall loss function for the whole procedure. In particular, the three steps have three different loss functions, and the updating of the shared global model will cause inconsistent in minimizing multiple loss functions defined by equations 4, 5, and 6. \n\n2. In Section 4, there is a big blank space before the section head and Table 2. The content layout and arrangement could be improved. \n\n3. In Table 1 dataset CIFAR-10, the number of clients is 5 that is very small number for federated setting.  Moreover, the CIFAR-100 data only divided into 50 clients that are also a small number for FL.\n\n4. In Table 2, p = 1 indicates that each client has two classes only. However, the results of FedAvg are 17.13% that is much less than the reported results (85%) of the FedAvg paper by Google. \n\n5. Authors claim the proposed method can protect user privacy since a client can select which data need to be excluded from the federation. However, no corresponding experiments support this claim.\n\n6. Convergence of the gating function is not discussed.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "what is the motivation of the paper?",
            "review": "The proposed method is a federated method allowing to have a certain amount of data shared between all the learners and some data specific to each learner. The targeted field of application is classification for problems where strong privacy is crucial. The method consists in learning a global classifier (with the shared data) as well as local classifiers (one per learner, using the local data). The inference, for each learner, is done with a local expert (another neural network) trained to combine inferences from the local and global models.\n\nThe first objection I will make to this paper is the confusion it causes when talking about privacy. Federated learning methods, in a context of privacy protection, aim at building a global model, using private data, without revealing anything about the private data. Here, private data are not used to build the global model, and the local model obtained is not intended to be given, which makes talking about privacy seem irrelevant to me. \n\nFor the reason explained above, I wonder about the motivation of the paper. Isn't the best solution, in this case, simply to learn a standard classifier for each local problem, using both local and global data and using a loss function that weights the examples according to the class distributions? It could be argued that if there are many local problems it is more expensive than learning a global classifier and combining it with a local classifier.  In this case efficiency is not privacy is the motivation and the gain should be evaluated in terms of efficiency. This is true, but in this case it becomes a domain adaptation problem: a model in which the distributions of p(y_i) are different from those of the local problem is given and should be used to improve the local problem. In this case, the domain adaptation methods apply and should have been investigated by the authors.\n\nMy last comment is on equation (8). I do not understand the relationship between equation (7) and (8). In (7) only the weights of the expert are learned. In (8) all weights are learned. If equation (8) is applied, this means that local data is used to learn the global model through w_g, so there is a leak of local information which contradicts the strict respect of privacy.  \n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}