{
    "Decision": "",
    "Reviews": [
        {
            "title": "Amazing experimental results on link prediction (misunderstanding or breakthrough?) ",
            "review": "The paper proposes a new knowledge graph representation learning model based on Quantum Logic and translational techniques. It achieves amazing performance on link prediction.\n\nThe model is a combination of two existing models E2R (Garg et al., 2019) and TransE (Bordes et al., 2013). The technical contribution is not significant.  And the combination strategy is only to combine their scoring functions and loss functions. In fact, the techniques of the two models are quite different. Such a combination, in my view, doesn't make sense. More technical details should be given to clarify this. Besides, the loss function of TransE in Eq. (12) is not accurate. It is only the loss for one pair of positive and negative relation triples rather than the total loss. And many symbols are left unexplained.\n\nI have my doubts about the performance of the proposed model on link prediction. The model with only **4-d** embeddings achieves the **90+** Hits@1 scores on the difficult benchmark datasets such as FB15K-237,  almost three times greater than the SOTA results.  It is amazing and unbelievable! I carefully check the results including those of baseline models and find some results of baselines also strange. For example, in the original paper of the baseline model ADRL (Wang et al., 2020), the reported Hits@1 result of ConvE (Dettmers et al., 2018) on FB15K-237 is **46.2**, while the score in the ConvE paper is only **23.7**. I do not know why. Comparing with these unreliable results reduces this paper's credibility. I want to take a look into the code because the authors provide a GitHub link in the paper. But the GitHub repository has no code. I was unable to reproduce the amazing experimental results reported in the paper. I would suggest the authors to provide the source code to support the experiments. \n\nThe writing of this paper is poor. Many baseline models in Tables 2&3 come with no citations, and some of these models even never get citations anywhere they appear in the paper, making me dazed. Besides, there are many typos and strange sentences. Some examples are listed as follows:\n- word2vec(Mikolov et al., 2013) ->  word2vec (Mikolov et al., 2013)\n- any other KGE model are candidates -> any other KGE models are candidates\n- To better adapting to the table...\n-  the performance of link prediction and its complexity demonstrates in two tables and a figure, respectively.\n-  according the key technology they based on.\n-  our model is also outstanding, and they outperform the existing model 33.29% in MRR...\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The main contribution of this work is to combiine E2RB(Garg et al., 2019) with TranSE (Bordes et al., 2013).",
            "review": "In this work, the authors present a novel model for Knowledge Graph Completion. The model mainly combines  E2RB(Garg et al., 2019) with TranSE (Bordes et al., 2013) for logic and structural feature capturing in the same vector subspace, respectively. The two components have synergy with each\nother and achieve impressive performance at low cost which is close to the efficient model TransE. \n\nHere are the comments:\n\nPositive:\n1) The research topic is very interesting and related to ICLR community.\n2) The performance of the proposed methods are pretty good compare with baselines with 94% improvement.\n\nNegative:\n1) The novelty of this paper is not very high since it mainly combines  E2RB(Garg et al., 2019) with TranSE (Bordes et al., 2013).\n2) The presentation can be improved. For example, The Section 3.1 is not very clear. like the U_{r_i} and some other notations are not explained well.  Readers have to go back to E2RB(Garg et al., 2019) to figure out the notations.\n3) The complexity of the proposed model is not discussed. It will be more convincing to tell the readers about the time/space/parameter complexity.\n4) The author say the code is on https://github.com/PandaCoding2020/QLogicE. However, I found nothing there.\n\nIn a nutshell, the research topic in this work is important and the proposed model have very good performance. However, the technical contributions are not very impressive, mixture of two exiting work.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially useful model if correct, but paper very difficult to read and results seem highly unusual",
            "review": "Summary: The paper aims to improve representation of knowledge graph entities and relations, for the purposes of link prediction (as used for downstream tasks such as question answering etc), by combining recent work on \"Quantum Embedding\" with an established  simple KG model, TransE, that represents relations by vector addition. The goal of combining logical reasoning and statistical learning is important in a broad sense and is expected to improve the specific link prediction task, thus the aim of the work is sensible and potentially useful.\n\nQuality: The paper lacks sufficient quality due to: (i) being very difficult to read (grammatical errors throughout), (ii) being poorly motivated - complex numbers appear and disappear, quantum mechanics bears no obvious relationship to the problem so why would its associated maths, (iii) largely repeating the QE paper (1.5 pages in sec 3.1) and (iv) reporting highly unusual results (See below)\n\nClarity: insufficient - the paper requires a thorough proof read for grammar. The paper also requires improved motivation and clarity in terms of what it is doing, why maths steps are taken, etc.\n\nOriginality: Relies heavily on QE paper, but seems a novel addition of TransE\n\nSignificance: Not possible to assess given results seem so unusual\n\nPros: If reliable, the results would be impressive.\n\nCons: The results are extremely questionable. For all data sets, MRR, Hits@! and Hits@10 are *the same number*. This is not the case for any other model and makes no sense given that MRR is computed very differently to Hits@k. Further, this would require the model to predict each fact in 1st place or else not in the top 10 - in every instance for every data set. That seems implausible and strongly suggests a potential error. If not an error, these anomalies require very clear explanation in the main body of the paper, and since the results are so much better than other models, there needs to be a very clear explanation as to where the performance gains come from and why. Overall, the paper is very difficult to read and it is not clear what it adds beyond the two papers it combines (E2R & TransE), which are combined in a poorly explained way. It also contains much unmotivated maths, ie not clear how maths for quantum mechanics can just be used for word semantics.  \n\nIn case relevant: evaluation errors have occurred elsewhere where models assign the *same score* to many facts/triples but consider a particular test triple to be predicted correctly if it receives the highest score, without considering that many others may have that same score (e.g. if all triples were assigned a score of X, then any test triple - assigned X - technically receives the max score, ie X = max{X, X, X, X}  and so might naively appear to be the best candidate).",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThis paper proposes to use Quantum and Translation embeddings in synergy to further improve the SOTA on the link prediction task in Knowledge Graphs. It uses the QL inspired embeddings proposed by [Garg et al.](http://papers.nips.cc/paper/8797-quantum-embedding-of-knowledge-for-reasoning.pdf) and Translation embeddings method [TransE](https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf) and combines them using a weighted loss function. The proposed method is able to outperform existing SOTA methods. \n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nPros:\n\n+ Proposed method is able to integrate existing KGC methods to achieve SOTA performance on all datasets except WN18.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nCons:\n\n- The proposed approach is new, however it is not novel. It combines already existing approaches in a weighted fashion.\n\n- Hits@1 and Hits@10 are same that means the method either ranks a triplet at position 1 or  gives it a really low rank and that's why we see relation between Hits@1 and MRR too. The same problem is seen in E2R ( Quantum embedding method ) and yet this paper does not attempt to solve or even explain the problem.\n\n- This paper provides no comparison with E2R in the main section. In appendix they compare their method with E2R but only on WN18 and FB15K.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nI vote for rejecting the paper. I believe the paper has no new contribution to the field except beating existing SOTA methods, which is not enough in my opinion. \n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nQuestions and Suggestions:\n\n- Adding a distribution of ranks of positive test triplets will provide some insight related to point 2 in weak points section.\n    \n- Compare with E2R on all the datasets that have been used in the paper. Stating that \"The paper Garg et al. (2019) didn’t provide results on the datasets FB15k237 and WN18RR.\" is not sufficient.\n    \n- What is the evaluation protocol used in this method? How does this method deals with same score triplets while ranking?\n    \n- Experimentation with the hyper parameter lambda_l will be interesting because the results follow the same pattern as E2R which indicates that the proposed model is highly influenced by it.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nMinor Comments/Typos:\n\n- Section 3.2: In equation 12, please provide an explanation for the terms in the loss function. \n- Section 4.1: the author believed that FB15k was test leakage -> the author believed that FB15k had test leakage\n- Section 5.2: As preciously mentioned -> As previously mentioned ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}