{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a method to generate attention masks to interpret the performance of RL agents. Results are presented on a few ATARI games. Reviewers unanimously vote for rejecting the papers. R1, R3 give a score of 5, whereas R4, R5 give a score of 4. Their concerns are best explained in their own words: \n\nR1 says, \"The use of attention maps to analyze and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either.\"\n\nR3 says, \"the analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze-like, and ball-and-paddle) may shed light on the learned attention's general property.\"\n\nR5 says, \"I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive\"\n\nIn their rebuttal, to address R1's concern authors suggested that the use of attention on both value and policy networks is novel. This is not sufficient, because it does not show why such attention maps are more useful than ones proposed by prior work. As suggested by reviewers, a systematic study or a human study clearly showing that the proposed method adds more interpretability is critical. However, this is missing.  In response to R3, the authors provided experiments on more games. But this is not the point -- because it's not about the number of environments in which experiments are provided, but rather the nature of the analysis that is performed. Finally, R5 comments that it's unclear whether attention actually provided interpretability or not. \n\nDue to the lack of convincing analysis that demonstrates the utility of the proposed method in advancing the understanding of decisions made by RL agents, I recommend that the paper be rejected.\n\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #5",
            "review": "- Summary\n    - This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function \n    - The authors demonstrate their method on 3 Atari games and use A3C as the training algorithm\n- Strengths\n    - To the best of the reviewers knowledge, this is the first work to apply this type of visual explanation to RL\n    - The interpretable agent performs on par with the black box one.\n- Weaknesses\n    - How were the points/frames in figure 2 chosen?\n    - To my untrained eye, the attention masks in figure 2 aren't very interpretable.  Human studies to verify that the explains actually help the humans understand (or predict) the agent's decision would be very helpful in this regard.\n    - I am uncertain that the contribution is enough to warrant publication at ICLR.  While this is the first work I am aware of to apply this type of visual explanation to RL, using attention masks is well known in the literature (Mascharka et al, 2018; Fukui et al, 2019) and it doesn't appear like any considerable modification necessary to apply it to this domain.\n    - Using the attention masks to to interpret the decision of the agent based on just the current frame is misleading.  This is because the attention is conditioned on s_t, not o_t, where s_t the output of ConvLSTM(o_t, s_{t-1}).  The consequence is that we do not know whether attention is high for a given location because of the visual information in o_t or the visual information in any other observation. While it is entirely plausible that the most influential location in the ConvLSTM output is most correlated with the current frame, this hasn't been shown.\n- Suggestions\n    - Show both the frame and the frame with attention in Figure 2.  Currently it can be hard see the content of the frame.\n- Overall\n    - Overall, I am not convinced the contribution is enough for publication at ICRL.  More importantly, without additional verification the attention masks cannot be used to explain the decision based on the current frame as they are conditioned on the current frame __and__ all previous frames.  \n- References\n    - Mascharka et al, 2018: Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning\n    - Fukui et al, 2019: Attention Branch Network: Learning of Attention Mechanism for Visual Explanation\n\n\n## Post Rebuttal\n\nI have increased my rating slightly but still don't think the paper is ready for publication.  I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice case studies but need more work",
            "review": "Summary:\n\nThe paper introduces an attention mechanism into A3C-based reinforcement learning agents to identify the attended visual regions for vision-based reinforcement learning tasks. Specifically, they applied a mask-attention mechanism for the policy and value prediction neural network columns of the A3C model based on the convolutional LSTM (Xingjian et al., 2015). They evaluated the proposed attention mechanism on three selected ATARI games (namely, Breakout, SpaceInvader, and Ms.Pacman) and show that the attention mechanism generates intuitive visual attention in decision-making. They also compared the attention's impact on game scores against some ablative attention mechanism variants.  \n\n\nPros and Cons:\n\n++ The paper contributes to transparent decision making of reinforcement learning methods by figuring out attended regions in the observational space (pixels). The identified attention regions are intuitive and action-conditional. The cases discussed in the three selected ATARI games are informative.\n\n-- The analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze-like, and ball-and-paddle) may shed light on the learned attention's general property. Current analyses on very sparse time indexes of three selected ATARI games may not provide sufficient evidence or insights to support claims.  Some additional experimental studies on other games or similar domains with high-dimension perceptions would strengthen the paper's contributions. \n\n-- The paper briefly mentions other attention mechanisms for reinforcement learning methods. It seems that some in-depth discussions on the relationship between the proposed approach and the prior art are needed. How is the proposed attention mechanism different from previous ones? Does it address some limitations of previous methods, such as capturing more action-conditional information or more robust to initialization conditions? An additional empirical comparison would also be informative. \n\n-- The motivation and goal on the inverting gaze area are less clear. It would be of interest to see if the attention mechanism would make the learned policy robust to interventions in un-attended regions.   \n\n-- The paper has some confusing details. The comparison method named Mask-Attention A3C Double seems identical to the proposed Mask-Attention A3C method. Some clarification on this would be helpful. The paper also has some typos. ``is indicates'', ``our method also learn'', ``is calculates'', etc.  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "lacks a clear justification of novelty and contribution",
            "review": "This article proposes to include an attention mechanism to Deep RL (focusing on actor-critic architectures), to provide a visual \"explanation\" of the learned policy. There is solid evidence that attention is an important aspect in perception and learning, and the use of soft attention to improving deep neural network performance has been successful for a variety of tasks such as visual recognition (Wang et al, CVPR'2017) and image captioning - the proposed approach is similar to those. \n\nThe proposed approach is to learn attention maps to inhibit part of the visual feature, separately for the value and policy networks. The attention maps being differentiable can be learnt jointly with the rest of the network. The approach is straightforward and similar to attention maps used in, eg, image captioning. \n\nThe issue of including attention in RL is not completely unresearched. In particular, it would be valuable to discuss the 2019 DeepMind paper at NeurIPS paper by Mott et al (\"Towards Interpretable Reinforcement Learning Using\nAttention Augmented Agents\") as the claims and purported aims are similar. \n\nThe discussion of the type of attention that could be applied (eg, map vs spotlight, soft vs hard) is missing in the article as only one model of attention is evaluated. \n\nThe article is generally clear, although some design choices could have been discussed in more details and some arguments are unclear. For example, I did not understand the author's argument of why using bottom-up saliency requires backpropagation. \nMore importantly, the novelty of the article is not clearly argued: The use of attention maps to analyse and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either. \n\nAnother issue is that I think the use of the term \"explanation\" is a bit misleading in this article: the proposed approach provides activation maps, which offer some hints at the system's process, but still require a large amount of human interpretation for an actual explanation. \n\nIn sum, the article is fairly well written, but the contribution should be outlined more clearly, and more experimental work could be provided to justify what type of attention is most effective - the relation to some previous works (in particular Mott et al) would also be desirable. \n\nRefs. \nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML'2015.\n\nFei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, XiaogangWang, and Xiaoou Tang.  Residual attention network for image classification.  CVPR'2017. \n\nMott, Alexander and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Jimenez Rezende, Danilo. Towards Interpretable Reinforcement Learning Using Attention Augmented Agents. NeurIPS'2019\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "This paper applies the mask attention mechanism on the DRL model (actor and critic), to make the learned policy explainable. The empirical results in Atari 2600 show that the performance of A3C is further improved by implementing mask attention on the actor and critic network separately.\n\nStrength: \n- The paper is well-written and easy-to-follow.\n- The experiments demonstrate the effectiveness of the attention mechanism in three Atari environments.\n- The comparison of the score by the inverting gaze area is interesting.\n\nWeakness:\n- The contribution is marginal. The idea is straightforward. It is not a new concept that implementing an attention mechanism in a deep neural network to make it explainable. Applying attention mechanisms to augment the RL agent is also studied in previous work[1, 2, 3]. This key idea of this paper is similar to them, inserting the attention mechanism into the network to explain the RL policy in a top-down fashion. The difference is in the implementation details of the mask-A3C, e.g. using ConvLSTM in the state encoder to keep the spatial information and applying different attention mechanisms for the actor and critic. It is necessary to further discuss and compare these works in the paper. \n- The method is only evaluated in three environments. It is necessary to validate it in various environments, especially showing the results in 3D environments (visual navigation, robot arm manipulation). It would be nice to report and discuss the failure cases in the experiments, instead of only the success cases.\n\nRef:\n\n[1] Mott, Alexander, et al. \"Towards interpretable reinforcement learning using attention augmented agents.\" Advances in Neural Information Processing Systems. 2019.\n\n[2] Manchin, Anthony, Ehsan Abbasnejad, and Anton van den Hengel. \"Reinforcement learning with attention that works: A self-supervised approach.\" International Conference on Neural Information Processing. Springer, Cham, 2019.\n\n[3]Tang, Cheng-Yen, et al. \"Implementing action mask in proximal policy optimization (PPO) algorithm.\" ICT Express (2020).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}