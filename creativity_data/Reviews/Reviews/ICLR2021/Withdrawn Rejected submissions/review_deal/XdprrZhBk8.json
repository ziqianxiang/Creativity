{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper provides a functional approximation of the error of ResNets and VGGs pruned with IMP and SynFlow on CIFAR-10 and ImageNet, showing that it is predictable in terms of an invariant tying width, depth, and pruning level.  In particular, it formulates the test error as a function of the density of the network after pruning and identifies a low-density high error plateau, a high-density low error plateau, and a power-law behavior for intermediate density. It further demonstrates that networks of different sparsities are freely interchangeable. The paper provides an interesting insight on the power law structure of the error as networks are pruned, however the results are very limited to specific types of networks (ResNets and VGGs), pruning methods (IMP and SynFlow) and datasets (CIFAR-10, ImageNet). Hence, it's not clear if the proposed functional approximation generalizes to other network families, pruning methods, and datasets. I understand that adding a new architecture or dataset is expensive, but fitting the proposed scaling law (the five parameters) requires pruning only a small number of networks, as mentioned by the authors. Comparing the calculated error and the actual error of the pruned network for different architectures and datasets can help verify the findings in the paper, and significantly widens its scope."
    },
    "Reviews": [
        {
            "title": "limited applicability but good theoretical contribution",
            "review": "The paper investigates the behavior of the test error as a function of the density of the\nnetwork after pruning and identifies 3  regimes:\n1) a low-density high error plateau; 2) a high-density low error plateau; 3) a power-law behavior for intermediate density. \nThe authors propose a functional form that captures the test error behavior in all these regimes, \nalong the lines of Rosenfeld et al. ICLR 2020. The approximating function contains the unpruned network's error and other 3 parameters that have to be fitted on each architecture.\nThe functional form had to be slightly modified with respect to Rosenfeld et al. to better describe \nthe density dependence near the low error transition. \n\nMoreover, they generalize the approximation to take into account also the width and depth scaling factor,\nand the dataset size as well. The generalized functional form is still relatively simple,\ncontaining only 2 extra parameters. In fact, the authors were able to identify a roughly invariant\nquantity characterizing constant error manifolds. This greatly simplifies the modeling.  \n\nNumerical experiments are performed on CIFAR10  and ImageNet datasets, using VGG and\nResNet architectures. \nTwo algorithms, iterative magnitude pruning and SynFlow, are used for pruning.\nThe proposed functional form is in good agreement with the experiments on each combination of\narchitecture, dataset and pruning algorithm presented.\n\n\nThere are a few drawbacks to the paper. \nNumerical experiments involve only image classification tasks, on a small set of datasets and architectures. \nAlso, one of the applications motivating this kind of analysis, neural architecture search,\nis in my opinion unpractical under the framework proposed, as I argue below. \n\nOn the other hand, the experiments available are quite convincing,  \nand the scaling form proposed is helpful in the qualitative and quantitative understanding and reasoning about the test error surface as a function of pruning and architecture.\nTherefore, I suggest acceptance of the manuscript.\n\n\nMajor Comments\n- End of Section 2:\n\"\"\"\nWe also eliminate configurations where increasing the width or depth of the\nunpruned network lowers test accuracy (e.g., 144 of the 294 CIFAR-10 ResNet configurations of l, w,\n1 and n); these are typically unusual, imbalanced configurations (e.g., l = 98, w = 1/16\n). Of the 12,054 possible CIFAR-10 ResNet configurations, about 8,000 are eliminated based on these sanity checks.\n\"\"\"\nThis seems to be a very delicate point. It implies that the region of parameters for which the prediction\nis a good approximation to the error is not well characterized. Minimization of the approximating function may well give configurations that lie outside those boundaries and that in practice give a much worse error. \n\n- Appendix D. The section explains how to obtain the fit coefficient by varying only a few dimensions at a time. \nUnfortunately, experimental details (e.g. number of points used) are missing. \nNo indication is given of the minimum number of points that one has to acquire in order to have robust fit. \nThe main concern here is that the fitted parameters, and the solution of the minimization problem in Section 6 as well,  may be very unstable until a very high number of points is acquired.\n\n- Since the construction of the fitting function relies on a high number of point acquisitions to perform the fit,\nit seems that the motivating question \"Given a family of neural networks, which should we prune (and by how much) to obtain the network\nwith the smallest parameter-count such that its error does not exceed some threshold \u000f k ?\"\ncould be better answered by direct black-box optimization or pruning. Therefore the applicability of this\ntheoretical framework seems very limited. \n\nMinor Comments:\n\n- Fig.3: would you consider adding depth vs width? Does the invariant w*l^\\alpha = v hold? I'm aware that discretization could be a problem.\n- pag. 2: \"To achieve sparsity levels beyond 20%\" -> \"To achieve density levels below 80%\"\n- pag. 3: \"has been fit\" -> \"has been fitted\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting Power Law Analysis of Test Error with varying Depth, Width, Dataset Size and Pruning",
            "review": "# Summary\n\nThis paper explores the possibility to fit power laws to the behaviour of  a neural network's test error under weight pruning and modifying the architecture (depth and width) and subsampling the dataset. This is interesting for practitioners when determining how to change an architecture to achieve desired performance by fitting the proposed power law to a few trained networks.\n\n# Strong and weak points\n\n## Pros\n- The fit of the free parameters yields a very high accuracy for the architectures considered.\n- The resulting parameters of the functional forms yield a direct interpretation.\n- The work is self-contained and well-written\n\n## Cons\n- This is a very high bar, but let me phrase this distant goal anyway: This work does not provide an ab initio derivation of how network depth, width, dataset size and pruning are related to the test error\n- There is room for considering more datasets, architectures. From my side, you don't need to do this to be accepted. But the weight of an empirical paper with structural claims grows with the number of experiments performed.\n\n# Questions\n- Do you have any failure cases for when you are not able to successfully fit the free parameters? I would imagine that e.g. different choices of hyper parameters might yield different laws.\n- If the primary goal in the pruning literature is badly chosen in terms of resulting number of parameters, does it still serve as a benchmark for comparing pruning methods?\n\n# Recommendation\nThis work reduces finding architecture hyperparameters by fitting parameters of a surrogate problem using a small amount of training instances. This can prove useful in practice and the surrogate problem seems to capture the main dynamics well.\nThis makes the work a useful tool and I recommend accepting it to ICLR.\n\n# Minor remarks\nI have some suggestions for the figures in the paper: Please make the lines in legends thicker, like the lines in Figure 6. I had to zoom in significantly to distinguish the colours in most other plots, especially for the dotted lines in Figure 4.\nInstead of taking log10(X) in your figures, please use logarithmic ticks so that one can immediately grasp the values. For example, I took out my calculator to know what $10^{-1.2}$ is in Figure 1.\n\nMaybe you can reduce the number of footnotes in the paper. Personally, I am a huge fan of in-or-out – state it in the main text if important and leave out completely if not.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "On the Predictability of Pruning Across Scales",
            "review": "Paper summary\n\nThe authors propose a functional approximation to the error of pruned convolutional neural networks as a function of network hyperparameters. This functional approximation depends on a number of hyperparameters that are fit on the error of already trained and pruned networks on a certain task (in this case, image classification on CIFAR-10 and ImageNet are the tasks under consideration). The authors demonstrate that this fit is very accurate over many orders of magnitude, which demonstrates their hypothesis on the power law nature of the error distribution as a function of the hyperparameters under consideration.\n\n---------------------------------------------------------------------------------------------------------------------------\nPositives and negatives\n\n+The paper is very well written. It is very easy to follow the author’s argument and the figures illustrate the point in a concise manner.\n+The experimental section is very strong. The authors do a good job of motivating their hypothesis with empirical data, and explaining why they propose a power law.\n+After proposing the power law hypothesis, the authors fit the expression on a large amount of data, and convincingly show that the power law expression holds over multiple orders of magnitude.\n-The contribution of this paper is not as significant as I was lead to believe after reading the title and the abstract. While the authors have convinced me that the behavior of a network after pruning under different densities does follow a power law, the expression they derive depends on fitting 5 hyperparameters (equation 2), and after the fit can only be used only for that specific architecture and dataset.\n-The generalization of this expression is not well explored. The authors only consider one type of CNN and two datasets. Trying different CNN architectures and datasets (small datasets such as SVHN, mnist or fashion-mnist would have been fine, as long as quite different architecture types would have been probed).\n\n---------------------------------------------------------------------------------------------------------------------------\nRecommendation\n\nI recommend a weak accept for this paper, in light of the following considerations: the paper is well written and it provides an interesting insight (the power law structure of the error as networks are pruned), the paper’s formula could be useful to applied practitioners. However, I believe the way the results are presented are strongly overstated. I would like to see the abstract changed to reflect that the functional approximation derived is an empirical fit dependent on architecture and task; and I would like to see a little more variation in network architectures investigated. If those issues are addressed I would be willing to upgrade my recommendation.\n\n---------------------------------------------------------------------------------------------------------------------------\nQuestions\n\n* I am a bit confused as to why n is a hyperparameter in eq 2 at all as it only seems to feature indirectly via the error. Could you add a bit more explanation as to why that is considered?\n* Given that the formula is fitted with empirical data I find it surprising that the study of fit quality as a function of data points is relegated to the appendix. I think this point is important enough to feature in the main text, and would like to see results as the number of points trends towards 5 (the minimum number of points for which we’d expect a consistent fit). Maybe a plot with fit residual as a function of fit points?\n\n---------------------------------------------------------------------------------------------------------------------------\nFeedback (not related to the score)\n\n* Figure 3 is not very informative. I guess the point is to compare the quality of the fit contours with the empirical data. In that case the information about the fit is not enough ( a single line). I would rework the figure to contain full iso-lines for both fit and empirical data. And the caption could be more informative too (why do we need to see the density/depth plane and density/width plane if they are almost identical)?\n* In Eq. 1, it would have been useful if p had been introduced in the notation section.\n* In figure 4, please add legends to the different colors in each plot.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studies how to estimate the performance of pruned networks using regression models. The authors first empirically observe that there exist three distinct regions of sparsity: (1) In the low-sparsity regime, pruning does not decrease the accuracy (2) In the mid-sparsity regime, a linear relationship between the sparsity and the accuracy is observed (3) In the high-sparsity regime, pruning does not decrease the accuracy again. Under this observation, the authors proposed a regression model called the rational family and empirically verified its performance. The authors further extended this model to incorporate the network width and depth under some empirical observation called the error-preserving invariant. The authors performed experiments to verify different perspectives of the proposed functional form.\n\nOverall, I like the main idea and experiments on the interpolation/extrapolation using the proposed functional form. However, I believe that the following concerns are critical.\n- The authors removed a huge amount of test configurations: about 8000 configurations among 12000 configurations are eliminated for CIFAR-10 ResNet. It significantly reduces the reliability of estimations from the proposed functional form as the removed cases are occasionally observed in practice. \n- The authors only consider limited experimental setups. For example, the effects of network architectures, rewinding weights, iterative pruning, and training epoch to their observation (e.g., having three different phases, error-preserving invariant) are not verified. I am curious whether the proposed functional form is still valid for other architectures (e.g., DenseNet, recurrent networks, language models) and without rewinding.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}