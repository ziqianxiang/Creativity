{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper is rejected.\n\nThe authors focus on offline RL for the sequential recommender system problem and propose an approach that:\n* builds multiple models based on splits of the offline data using domain knowledge\n* splits the policy into a context extraction system and context conditioned policy (similar to Rakelley et al.)\n\nWhile R1 and R4 appreciate the changes, they both feel that the paper is not ready for publication at this time. R1's main concerns is the generalizability of the proposed solution because it relies heavily on manually defined rules and domain expert knowledge. R4 was concerned with the definition and precision of robustness. How is robustness quantified? Finally, many of the baselines were not built for partially observed environments, so it is unsurprising that they perform poorly. Baselines with recurrent policies would strengthen the paper.\n"
    },
    "Reviews": [
        {
            "title": "Paper 652 Comments",
            "review": "This paper discusses the problem of RL based sequential recommendation system and proposes a model-based learning method to solve the offline RL in real-world applications. It addresses the two challenging problems:  (i) real-world recommendation\nenvironments are usually non-stationary, and (ii) real-world environments recommendation are often with stochasticity. The proposed model tackles these two problems by learning to adapt to different simulators generated by the offline dataset. Experiments demonstrate the effectiveness of the proposed model against state-of-the-art baselines on one dataset. This paper could be improved in the following aspects:\n\n1. Implementation code is not released.\n2. More details of the deployment of the model should be provided.\n3. Only a few baseline methods are introduced with the proposed model. More RL based recommendation models and simulator models should be incorporated to demonstrate the effectiveness of the model.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel idea to tackle with stochasticity and non-stationarity in RL for Recommender systems when learning using offline data.",
            "review": "The paper presents an adaptive model-based off-policy learning algorithm with applications to Sequential Recommender Systems (SRS) in mind. The novelty in the work is that it extends on other model-based and adaptive algorithms to suit the needs of SRS where stochasticity and non-stationarity are a part of the problem--this any learned model has to be robust to distortion because of off-policy setting. The key idea is to construct an 'environment-parameter extractor' to identify the dynamics system to adapt at runtime. \n\nThe background and notations in Section 3 and 4 are well written and rigorous, however, a little more grounding to a sequential recommender system setting would help in two ways: make the paper more self-sufficient for a Recommender systems audience, and also motivate the distinction between other model-based and other adaptive policy learning methods. \n\nIn the experiments section, it is worth discussing what is the trade-off between choosing an application-specific dynamics set has with models that do not have access to this knowledge in a different application. Moreover, one could add experiments where a misspecified model class is used to define the dynamics and show how the performance degrades wrt other algorithms that do not require this assumption.\n\nIn Section 5.1, the last experiment with distortion parameter,  the choice of distortion per simulator makes more sense than having an unlimited number of simulators to test robustness to distortion and hence makes me wonder (again) about how this model would perform against other model-based/adaptive algorithms.\n\nOn a minor note, the authors should make the setup in section 3 a little more self-sufficient and motivate the problem from a recommender system perspective i.e. how stochasticity and non-stationarity come into play.\n\nOverall, I find the paper convincing and novel and would recommend acceptance. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A divide-and-conquer method for off-policy reinforcement learning based on model identification",
            "review": "This paper studies off-policy reinforcement learning for sequential recommendation. The basic idea is to summarize each possible environment dynamic (i.e., the state transition) into an environment context vector, and optimize policy with respect to this context vector accordingly. The proposed solution is evaluated on a simulated sequential recommendation environment, and a real-world ride-hailing platform, which considerably increases the creditability of the proposed solution.  \n\nThe problem setup is a mix, which affects the generalization of the proposed solution. On the one hand, the solution is general for off-policy reinforcement learning, as I do not find any strong dependency of the proposed problem setup (i.e., a MDP) and the target sequential recommendation problem. However, on the other hand, the details in the proposed solution are so specific, which makes me question the generalizability of the proposed solution. For example, the reward function is assumed to be given by human experts. But in the experiments, it seems it is just the reward is defined by human expert, e.g., in the ride-hailing platform the reward is defined as the number of completed orders, rather than the reward function, e.g., how the state and action pair maps to a numerical reward. A clarification is needed here. And do we assume any non-stationarity in the reward function at all? There are other manually defined heuristics to “reduce model learning complexity”, e.g., “a hand-coded function $map(s’|\\bar{s},s,a)$ to construct the left of the states”. And actually I did not find the details about this hand-coded function in Appendix F. If even the details were provided, I am not sure how to construct this mapping for new problems, e.g., beyond the ride-hailing problem, in general.\n\nThe key idea behind the proposed solution is to exhaust possible environment models from offline data, and estimate the policy for each estimated environment model. The environment models are created by splitting the offline data into non-overlapping partitions, and use a particular environment model learning algorithm (e.g., with different hyper-parameters) to generate different environment models. Hence, how to partition the offline data and how to estimate the environment model become vital; but there were not enough discussion or experiments on these two important aspects. The mentioned solution for dataset partitioning is “by domain knowledge” and environment model learning is by existing algorithms. And I also did not find any experiment studying how these two factors affect the proposed solution. \n\nAn autoencoder-based method is used for environment model identification, but I cannot exactly follow how the autoencoder informs the construction of environment-context extractor $\\phi$? For example, how should I understand this sentence: “the extractor $\\phi$ can infer environment-context both with $v$ and $\\tau$”? \n\nBy the way, in Algorithm 2, what is the purpose behind line 19-22? Why do we need to generate those new episodes from the learnt environment model? \n\n**Response after author rebuttal period**\nI highly appreciate the detailed explanations and discussions the authors have provided during this period. It indeed clarifies my concerns and helps me better understand the setting and proposed solution. However, my major concern about the generalizability of the proposed solution still remines, as there are too many design choices depending on domain knowledge. I would keep my original recommendation; and if the paper could be accepted, I would like to encourage the authors to discuss the limitations of the proposed solution.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The  paper extends domain randomization techniques from robotics to SRS, which can take care of non-stationarity (for free) on top of the usual modeling errors.",
            "review": "Paper proposes curtaining the impact of simulator not being faithful to reality in a few ways: (a) define a policy that takes environmental context as input, (b) define an environment context predictor, (c) training scheme that learns the adaptable policy as well as the context predictor. The idea is that the learned policy is adaptable to diverse simulators defined using offline data. So when it is deployed online, it can adapt to the dynamics of real world changing across time. They use sequential recommendation systems (SRS) as the application to demonstrate this.\n\nThe key idea of the paper seems to be to extend domain randomization techniques from robotics to SRS, and justify that it can additionally take care of non-stationarity (for free), on top of the usual modeling errors.\n\n\n1. The inability to sample in the real-world maybe restrictive for robotics, but is a moot point for SRS. There is almost no cost for exploration (you will have to show recommendations anyway and the cost of showing bad recommendations is almost nil at medium-large scale) and augmenting your existing data should be much much easier. Is there compelling evidence otherwise?\n\n>> solve an offline problem that policies can applied to real-world applications without any additional online sample.\n\n\n2. Is there a quantitative reference for non-stationarity of consumer behavior and its impact on revenue loss or something similar? Kruger et al. has no real world examples itself. Even if it is non-stationary, in real world systems, policies are retrained at a high enough frequency that this should not be an issue.\n\n3. The description before equation (1) seems wrong. If there is a \\forall \\rho, then equation (1) should have such a constraint. Taking expectation with respect to \\rho is not considered robust. This misunderstanding decreases confidence in the validity of the results presented.\n\n\n4. The paper is generally dense and hard to understand at several places. Some of the statements are imprecise (some words seem to be misused) and detract from the key contributions of this work. \n\n\n - For instance, the following should be rephrased and jargon should be minimized:\n\n>> Hidden confounder factors (Shang et al., 2019) obstruct the deterministic predictions. As a result, the high-confidence regions are drastically reduced and thus the exploration of policy learning are obscured.\n\n - Unclear what this is aiming to say (without explicitly discussing how policy 'exploration' is constrained):\n\n>> instead of constraining policy exploration in high-confidence regions\n\n - Unclear (what is inconsistent?):\n\n>> in regions with inconsistency\n\n - Unclear. What is the two level structure below. Is the example with domains/countries supposed to be a proxy for environmental context?\n\n>> the environments include a two-level structure: ... \n\n - Unclear (what is the special structure? how is the environment made context agnostic?):\n\n>> and a special environment structure makes the environment context agnostic\n\n - Unclear how stochastic environments have an influence in policy optimization without fixing a learning algorithm:\n\n>> However, in stochastic and non-stationary environments, the learned dynamics have high uncertainty and thus constrain the policy to exploration in a small region.\n\n - Unclear what the performance metric of each simulator is:\n\n>> maximize the performance metric of each simulator\n\n - Repeated use of suboptimality and optimality before formally defining it: Also unclear what does \"all possible transitions out of consistency region\" mean:\n\n>> to reach suboptimal policies (in Figure 1(c)), we learn to make optimal decisions for all possible transitions in\nthe states out of the consistency region\n\n\n - What is \"exploitation\" here, isn't this confounding/overloading an existing technical term:\n\n>> learned policies tend to exploit regions where the model is distorted\n\n\n - What is a penalty and how is it consistent, and how does it obstruct? If it is relevant to differentiate the proposed solution with respect to this weakness of prior works, more information and clarity is needed.\n\n>> Since the environment has stochasticity, learning by consistency penalties will obstruct the policy into a small region for exploration.\n\n>> without relying on the consistency penalty\n\n - Unclear:\n\n>> instead of constraining policy exploration in high-confidence regions\n\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}