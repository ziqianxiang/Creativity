{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary:\nThis paper proposes ClaRe-GAN, a GAN architecture which the author claims a suitable approach for generating synthetic time-series data that both captures inter-class and intra-class characteristics. The authors measure the quality of the time-series in three aspects (i.e. diversity, fidelity, usefulness), where ClaRe-GAN seems to outperform other baseline models.\n\nPros:\n- Seemingly good performance\n  > Experimentally, ClaRe-GAN questionably outperforms other methods on some of the datasets.\n\nCons:\n- Poor presentation\n  > This paper is not ready to be reviewed. There is an exceptional amount of typos and bad formatting throughout the paper. Furthermore, it is not just that the writing is poor, but the entire paper lacks so many details, including the model architecture, optimization process, hyperparameters values, dataset statistics, experiment setup, evaluation setup. There is no way a reviewer can make an informed decision based on this state of paper.\n  > Some of the most troubling parts:\n    * \"we prove in this work that the performance of these models is limited on datasets with high-variability\": The authors do not \"prove\" anything. They simply test three previous models on five datasets, the details of which are severely lacking to begin with.\n    * \"The extracted latent codes are later concatenated with an input noise vector in a sophisticated manner\": What exactly is the \"sophisticated manner\"? There is no mention in the paper how the authors \"sophisticatedly\" concatenate latent codes with noise vectors.\n    * Equation 3: What is the point of using the same D_X^{Cl}(E_1 (x_1)) two times in the first term of the RHS? (Same goes for D_X^{Cl}(E_2 (x_2)))\n    * \"The Discriminative score denotes classification error of a test set consisting of a mix between real and generated samples\": Then why is it the case that the lower discriminative score denotes a high-fidelity? If the generated samples are similar to the real samples, then the discriminator should be confused, and hence the higher classification error denotes high-fidelity.\n- There are more than just GAN-based models for time-series generation. If the authors want to claim state-of-the-art performance, they should compare the proposed model with other baselines such as autoregressive models, or VAE baseline models.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review",
            "review": "*Summary:*\nThe paper introduces a generative model for time-series by adapting standard techniques from the GAN literature. Namely, the paper proposes a recurrent conditional GAN generator, class encoder, and two discriminators, one for learning class encoding and another recurrent one for sample discrimination. The approach is empirically compared to prior work on GAN-based time-series generative models on 4 public datasets.\n\n*Quality*\n- The paper doesn't address some important questions when it comes to the bigger picture of time-series generative models. E.g., what are the main challenges in the time-series generation that makes GANs a suitable choice? Why are GANs better than other generative models, e.g. autoregressive models?\n- Experiments of the paper are not convincing. What makes the chosen datasets a strong test-bed? Why isn't there a direct comparison to TimeGAN on the same datasets they report on? Is there any ablation study to show the importance of model choices?\n\n*Clarity*\n- The paper in general is clear and easy to follow, but it lacks some important details, such as how does the generative mode work in the proposed model (i.e. testing vs. training modes)?\n- Experiments: How big are the datasets used in the experiments? How do we ensure comparisons are fair with baseline? How do we ensure the metrics are good and correlate with sample quality? E.g. it's not clear where do we get generated samples from in order to train the LSTM used for discriminative and predictive scores.\n\n*Originality*\nThe paper applies standard techniques from GANs to the time-series domain. I don't find any deep discussion about the modeling choices, which casts the paper more as an applied contribution.\n\n*Significance*\nI am not an expert in the time-series domain so I cannot assess the significance of the work.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review from R4",
            "review": "This paper proposes a new class-specific GAN to generate time-series data, aiming at capturing both inter- and intra-class variations. The proposed model is demonstrated to be superior to three baselines in terms of both quantitative and visual results.\n\nThe novelty is limited as most components are from different existing works (e.g., class discriminator, mapping function, multi-scale discriminator, mode seeking regularization, etc.). The authors have not provided insightful findings and a thorough analysis of utilizing and combining them in this task. At least, ablations studies to validate whether each proposed component achieves its designed goal (e.g., class discriminator for extracting class-specific attributes, losses from image-to-image for diversity, etc.) could be included.\n\nThe quality of writing is another critical issue of this paper. Substantial rework needs to be done to fix grammar and formatting issues and improve clarity.\n\nIt is highly encouraged to plot the visualizations with class information.\n\nC-RNN-GANs and TimeGANs for each class should be compared.\n\nHow does the proposed method perform for imbalanced classes with fewer samples? \n\nMinor issues: \nIn Fig. 1, the terms and flows need further clarification.\nPlease put the parentheses properly around the references.\nPlease make CLaRe-GAN and ClaRe-GAN consistent.\n\nWhy in Tables 2/3, only results on 3 of the 5 datasets are shown?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper but have critical concerns",
            "review": "This paper proposed a novel generative model for time-series data.\nThe results look promising; however, I have several critical concerns.\n(1) The proposed model is a function from X to X. Usually, the generative model is a function from Z to X. If we would like to make a function from X to X for a generative model, identity function is the optimal.\n(2) The number of networks is too large. Training n different GAN jointly with other components seems very difficult and computationally expensive.\n\nThe detailed comments can be found below.\n\n1. Additional baseline\n- As mentioned in the introduction, utilizing specific GAN for each class can be another solution. \n- I agreed that it would be less useful because it cannot utilize the entire dataset. However, I think it is still meaningful to compare this to verify this claim.\n- Maybe in some cases (like differences among the classes are very large), a separate GAN would be better.\n\n2. Figure 1(a) \n- Clare-GAN- It seems like the generator directly utilizes the original time-series (X_1 and X_2) information to generate new time series. \n- But the other models utilizes the \"noise\" to generate the synthetic time-series. In that case, the problem is different.\n- In this paper, the authors make X to X function and the other methods try to make Z to X. \n- I think it is a big advantage of the proposed model and it is not sure whether this is the fair comparison.\n- This is because when we just have an identity mapping function, our generation would be the best in terms of the metrics in this paper because the generated samples = original samples.\n\n3. Training n different GAN\n- As can be seen in Equation (1), the proposed model needs to train n different GAN.\n- As we know, training GAN is unstable and difficult. Therefore, I am not sure whether training multiple GAN simultaneously would result in unstable training.\n- It would be good to discuss how the authors deal with this training stability problem quantitatively and qualitatively.\n\n4. Computational complexity\n- Note that the number of networks in the proposed method are much larger than previous methods. (at least N times larger). \n- Therefore, not only the computational time, but also the large memory is needed.\n- It would be good to compare the computational complexity as well.\n\n5. Equation (3)\n- It seems like the optimal solution for D is always outputting 0 or 1?\n- In that case, regardless of input, equation (3) would be minimized.\n- I am not sure what is the motivation of Equation (3).\n\n6. Datasets\n- The authors claimed that the proposed method would work well with large diversity across the different labels.\n- In that point of view, it would be good to show if the proposed method is scalable to a larger number of classes (such as 100). \n\n7. Table 2 and 3\n- In Table 1, the authors provided information for 5 datasets.\n- However, in Table 2 and 3, the authors reported the results for 3 datasets.\n- It would be great if the authors provide all the results (the space is enough I think)\n\n8. Figure 3\n- These qualitative results are not easy to understand.\n- Figure 2 is straightforward that we can see the overlapping between orange and green.\n- However, it is not clear what I should see in Figure 3.\n- It would be good to highlight the key takeaway of Figure 3 in the caption.\n\n9. Ablation study\n- In this paper, the authors proposed various components (can be seen in Equation (5)).\n- It would be great if the authors provide the detailed ablation study to highlight the source of gain for each component.\n\n10. Etc\n- There are some typos in the paper (e.g., TimeGAN (Fig. 1 (d)) -> Figure 1. (b))\n- Equation (3), \",\" should be replaced with \".\"",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}