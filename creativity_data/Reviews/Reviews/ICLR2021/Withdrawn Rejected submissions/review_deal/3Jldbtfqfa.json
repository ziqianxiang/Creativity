{
    "Decision": "",
    "Reviews": [
        {
            "title": "Well-written but unclear contribution and small practical impact",
            "review": "## Summary of contribution\nA method for knowledge distillation in the structured prediction setting is introduced. The idea is to train the student model with a cross-entropy loss with respect to the marginal distributions over substructures derived from the teacher model by dynamic programming. The approach is similar to that of Wang et al. (2020), who the authors cite, but seems somewhat more general (although this is not completely clear from the paper; see below). It is shown empirically in experiments on named entity recognition and dependency parsing that the proposed approach performs better than relevant baselines (\"unstructured\" knowledge distillation), albeit with quite narrow margins.\n\n## Strengths\nThe problem formulation is clearly stated with clear derivations of the inference problem and dynamic programming solution and the paper is highly readable overall. The four different cases considered are well motivated, explained and the experiments are adequate. Related work is reasonably well covered.\n\n## Weaknesses\nThe difference between this approach and the \"posterior\" approach of Wang et al. (2020) is not clear from the paper. Wang et al. also computes marginal probabilities and uses these in a cross-entropy loss function. The results of the latter are also stronger than the results suggested in this paper. For example, table 2 of Wang et al. (2020) shows that the average results on CoNLL for the \"posterior\" method is 87.7, compared to 85.3 in this paper. Similarly, the results on WikiAnn for English (the only language included in both papers) is 83.0 (table 3 of Wang et al.), compared to 81.4 in this paper (table 5). To conclude, the difference between the methods needs further clarification and the claims about the superiority of this method seems somewhat questionable.\n\nThe effect size in most cases is small (this is partially obscured by the very narrow range of the y-axes in figure 1) and even so the results are not statistically significant for many of the cases according to tables 5-6. This suggests that the practical impact of the proposed method is small.\n\nThe paper would be made stronger if some other task, such as image segmentation, was considered apart from the closely related tasks of named entity recognition and dependency parsing.\n\n## Recommendation\nI recommend Reject, primarily due to the unclear contribution beyond Wang et al. (2020) and based on the unclear practical significance of the approach.\n\n## Questions\nIn what ways does this method differ from that of Wang et al. (2020)?\n\n## Suggestions\nReferences to seminal papers on graph-based dependency parsing (McDonald et al.) and CRFs (Lafferty et al.) are missing. The approach is also quite related to the posterior regularization framework of Ganchev et al. (2010).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments are not sufficient to demonstate the efficacy of the proposed structural KD-loss. ",
            "review": "This paper proposes a novel factorized KD-loss for the teacher-student model.  The derived KD-loss is interesting.  However, the reviewer still has several concerns about the current submission:\n\n1.  Does the proposed structured KD-loss is model/task agnostic compared with (Hinton et al., 2015)? \n\n2.  The current experiments are not sufficient to demonstrate the effectiveness of the proposed structure KD-loss.  In fact, the performance of KD methods (Hinton et al., 2015) can largely be boosted by utilizing hint loss/intermediate representation.  Does the proposed structured KD-loss can also achieve better results?  More strong baselines should be included to demonstrate the efficacy of the proposed methods.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work, however, seem not enough",
            "review": "Summary:  In this work, a different factorized form of the knowledge distillation objective is proposed. Four different scenarios are considered in the two tasks: sequence labeling and dependency parsing models. \n\n+ves: \n1. A different factorized form of the knowledge distillation objective is derived. This provides a different view of this objective.\n\n2. In this work. Four scenarios are explored. These scenarios consider the different complexity of teacher and student models. This could provide a different way to knowledge distillation.  And the experimental results on the four scenarios are shown on different tasks. And zero-shot cross-lingual transfer setting and the seeing with unlabeled data are also considered in this paper.\n\n\n\nConcerns: \n\n1. In this paper, the authors think the student partition function is ignored because the student model is locally normalized. However, I do not agree with this. It can still provide gradient during the optimization of the student model. Could you say more about this?\n\n2. It is interesting to see the different computations of the structural KD objective. In this work, the authors present the KD baseline result with previous computation and the new results with the proposed computation. The KD baseline in this paper is the token-level KD objective?  It should be clarified clearly for this point. This leads to one question, how about the comparisons with other structural KD methods [1] and [2] that are still computable.   \n\n 3. In equation (3),  During the derivation, y is the whole label sequence,  u is the substructure of y.  It is can be hard to compute  P_t( u | x) in the general case. For linear-chain CRF, you can use the forward-backward algorithm. How about in the general case with more complex structures, e.g., high-order models?\n \n4. Comparisons with other structured distillation papers [1][2]  are missing. \n\n\n\n\n\n\nQuestions during the rebuttal period: \nPlease address and clarify the cons above:\n1. In equation (1), I think Score(y, x) and Score(u,x) are two different functions: one is to score the whole sequence, the other is to score the substructure of the sequence. It is better to use different notation, otherwise, it is misleading. P_t(y|x) and P_t(u | x) are also different functions. It is better to clarify the difference.\n \n2. In this work, the author only considers the first-order model. The substructure is two consecutive labels. It could be constrained. \n \n3. In Tabel 4, the caption seems inconsistent with the table. \n \n4. Some related work on structured distillation [2][3] is missing. \n \n[1] Sequence-Level Knowledge Distillation. EMNLP 2016\n\n[2] Benchmarking Approximate Inference Methods for Neural Structured Prediction. NAACL 2019\n\n[3] ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation. ACL 2020\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a structural knowledge distillation work, but need to be further polished and generalised",
            "review": "### Summary:\n\nThis paper introduces a structural knowledge distillation approach, named Struct. KD to effectively transfer learned knowledge from a teacher model to a student model. It uses the factorization method to alleviate the output space issue, as well as improve the prediction accuracy under four different scenarios when the teacher model has the same/larger/smaller/incompatible output structure with the student model. \n\n ### Strengths of the paper:\n\n1. This paper summarised 4 different scenarios between the teacher model and the student model in terms of structure output forms.\n\n2. The proposed approach greatly boosts the running speed and decreases the model size. \n\n3. Experimental results show that the proposed models can boost the performance of the student models.\n\n### Weaknesses of the paper:\n\n1. This paper summarised 4 different scenarios between the teacher model and the student model in terms of structure output forms. However, cases 2 and 3, in which the output structure of the teacher model is smaller or larger than the student model’s output structure, should also be the scenarios of case 4, in which the output forms of the two models are different (incompatible).\n\n2. The assumption that “the scoring function of the output structure can be factorized into scores of a polynomial number of substructures\" need to be justified. Because in section 3, the authors state that \"the structured output space is exponential in size\" (e.g. $N$ input and output $k^N$), so given an output structure (e.g. $N-1$) the set of its substructures should also possible be exponential (e.g. possibly $k^{N-1}$).\n\n3. There should be some descriptions about the connections between the tested applications (e.g. NER ) and the structure output (e.g. a sequence, a tree, or a graph), as I cannot clearly see that the experimental applications should be regarded as the structure learning tasks.\n\n4. There should be some connection descriptions between the structure formulation (Eq (3)) and the four cases, as I can only see Case 1a is related to Eq (3), others seem to be independent to Eq (3).\n\n5. The presentation of this paper should be further polished, especially for the result analysis part. Some key notations are unclear, for example:\n   - The ‘gold target’ appears only once in Section 2.2 without a clear definition or explanation. \n   - \"Linear-chain CRF\" should be given a reference.\n   - Term \"Struct. KD\" in result tables and figures should be explained at the beginning of the experiment section.\n   - The y-axis is missing for Figure 1, where the y-axis should represent the accuracy, but the metric is not specified. The same issues also can be seen in Table 2.\n\n6. In Section 4.1, the quantity of the unlabelled data is not clearly specified for each dataset. Therefore, it’s unclear if the unlabelled data used in Section 5.1 is a subset of all the unlabelled data or not.\n\n7. The authors only provide some specific models and their experimental results for the four cases (e.g. both the teacher and the student are linear-chain CRF models for Case 1a), so it is hard to judge whether this idea can be generalized to other more sophisticated NLP models. Or at least, should be compared with other baselines with the similar model size.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}