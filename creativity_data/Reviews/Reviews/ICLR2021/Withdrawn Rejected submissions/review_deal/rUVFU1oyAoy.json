{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work proposes to analyse convergence of episodic memory-based continual learning methods by looking at this problem through the lense of nonconvex optimisation. Based on the analysis a method is proposed to scale learning rates such that the bounds on the convergence rate are improved.\n\nPros:\n- I agree with the reviewers that this is an interesting and novel perspective on continual learning\n\nCons:\n- Reviewers point out concerns/issues with the clarity of the manuscript with respect to several parts: \n- reviewers raise concerns with respect to the significance of the evaluation \n- reviewers point out that the theoretical analysis itself is somewhat standard and not novel in itself, and 2 reviewers raise concerns with respect to the analysis made\n\nUnfortunately the authors seem to have missed the upload of the revised version. The reviewers have nevertheless considered the rebuttal by the authors and the consensus is that this manuscript is not ready yet in it's current form. "
    },
    "Reviews": [
        {
            "title": "Nice Theoretical Analysis, But Empirical Results Leave Many Questions Open",
            "review": "This paper takes an interesting nonconvex optimization perspective on the continual learning problem. More specifically, the authors pose continual learning with episodic memory as a smooth nonconvex finite sum problem. They then consider the requirements for a theoretical proof of convergence to a stationary point for previously learned tasks. This results in the proposed NCCL method that leverages these ideas to modulate learning rates for the current and previous tasks to prevent escape from the feasible region. Overall, the strength of this paper is its theoretical analysis and I find the idea of connecting continual learning with the associated nonconvex optimization problem compelling. I am not an expert in nonconvex optimization, but my understanding is that the analysis itself is not that unique for the field. Rather, what is novel is the interesting application of the ideas to the continual learning problem. I find the theoretical aspect of this paper strong, but still lean towards rejection in its current form as I am very skeptical that the idea is at all validated by the experiments. This potentially suggests that the theory may lack relevance in these domains. \n\nThere are some comparisons to baselines and prior work that I found a bit questionable. On the bottom of page 6, the authors state that existing GEM based algorithms only focus on canceling the negative direction, but actually maximizing transfer even when gradient dot products align was explored in [1]. The authors also suggest in section 4.2 that, despite worse empirical results, the NCCL approach is superior to GEM because of its inefficient quadratic program computation. However, this was already addressed in A-GEM [2], so it is not so clear that there is a significant computational advantage to NCCL. I would think that the authors should actually compare compute times inline with prior work. I also am almost 100% sure that the comparison to reservoir sampling is incorrect. If you look at results in [1] and [3] you see that reservoir sampling consistently performs right around GEM and sometimes better than GEM on exactly these same benchmarks. The 10% number seems unfathomable to me and at the very least needs an explanation about how this could be true. \n\n[1] \"Learning to Learn Without Forgetting By Maximizing Transfer and Minimizing Interference\" Riemer et al., ICLR 2019. \n[2] \"Efficient Lifelong Learning With A-GEM\" Chaudhry et al., ICLR 2019.\n[3] \"On Tiny Episodic Memories in Continual Learning\" Chaudhry et al., 2019. \n\nThis last point is related to my biggest overall concern, which is that it is not clear that the learning rate weighting scheme proposed in this work actually helps in comparison to generic replay. For example, it would be a really important ablation to try the very same buffer setup but with no learning rate modulation. My experience leads me to believe that the gap between the GEM based approaches and NCCL is likely larger than the gap between these approaches and vanilla replay. As a result, I am very skeptical that the learning rate modulation component adds value based on the current results. Additionally, it would be very interesting to look deeper into how the model is working to understand its effect on learning. For example, the authors should detail patterns with the chosen modulated learning rates over time. \n\nWhile I appreciate the theoretical analysis of this paper, I think the experiment section is too short and leaves many important questions unexplored. Unfortunately, I feel that I must support rejection of this paper in its current form as my doubts about the experiments leave me unsure that the approach works at all in practice. \n\nAfter The Rebuttal: I really appreciate the author response and it is a shame that the revisions do not seem to be correctly uploaded. Unfortunately, the responses to my comments rely heavily on references to the revision that I cannot see, making it impossible for me to validate if my concerns were actually adequately addressed. The other reviewers have mentioned some very valid concerns about the submitted draft as well. As such, I continue to lean towards rejection of the submitted paper as significant revisions are certainly needed. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "AnonReviewer2 Review",
            "review": "**Summary of paper**\n\nThis paper analyses the convergence of episodic memory-based continual learning methods by looking at it as a nonconvex optimisation problem. They analyse the convergence rates for the case where all memory from past tasks is stored, and then consider the case where there is only a subset of past data, leading to overfitting on the episodic memory. They then introduce a method that scales the learning rates of the their update method, with the goal of tightening the bound obtained in the convergence analysis. Finally, experiments are shown on different benchmarks, and the proposed method is compared to some competing baselines.\n\n**Summary of review**\n\nI am recommending rejecting this paper. Although the goal of the paper is commendable (convergence analysis for nonconvex episodic memory-based continual learning), I feel like there are many parts of the paper that can be improved (see later in the review).\n\n**Pros of paper**\n\n1. The paper attempts to analyse the convergence of continual learning methods theoretically (especially Section 3.1). This is very important to do, so that we can understand the problem of nonconvex continual learning better. This has not been attempted enough in the literature, partly because this is a very difficult problem. \n2. The work appears to be well-positioned with related work on convergence rates (as far as I am aware).\n3. The paper builds nicely, from Introduction to Preliminary Work to Theoretical Results to Experiments.\n\n**Cons of paper (/questions for the authors)**\n\n4. Although the aim of the paper is great, it appears to me as if the methods the paper mentions are not instances of the update that the paper analyses (Equation 6). Specifically, GEM and EWC (mentioned in the first paragraph of Section 3.1): GEM has a different optimisation technique (quadratic programming algorithm), and EWC does not store any episodic memory (only stores previous model parameters).\n5. I am struggling to see the significance of Section 3.2 (\"Overfitting to Episodic Memory\"). It appears like the authors are just pointing out that there is a bias introduced by storing only a subset of past data, without sufficiently commenting on the effects or significance of this bias.\n6. Appendix A (proof of Theorem 1) is incomplete.\n7. Something seems wrong to me with the BWT metric in Section 4.1:\na) My own experience with Fine-tune and EWC strongly suggests that both methods should have BWT<0. This is because the methods first learn the task well and then forget it slowly over time, and is fully expected from such algorithms. However, the authors report BWT>0.\nb) Fine-tune on Permuted-MNIST (Table 1) has an ACC of 2.43% but a BWT of 12.10%. Surely, be definition, BWT<=ACC always (Equation 18)?\nc) A final point on BWT: A BWT<0 does not \"imply that catastrophic forgetting happens\" (final paragraph page 7). Although it does imply *forgetting*, this is not necessarily *catastrophic forgetting*, which is only when BWT is extremely negative. For example, the concept of *graceful forgetting* will still have BWT<0 (but is usually distinguished from catastrophic forgetting).\n8. Can the authors comment on why the proposed method performs better with 1 epoch per task than with 5 epochs per task (Tables 1 vs 2, Permuted-MNIST)? This result appears to indicate that, despite the correction terms of the method, the method is forgetting tasks as it trains for longer.\n\n**Additional (minor) feedback**\n\n9. I would strongly recommend proof-reading the paper (or else asking a native English speaker to do so).\n10. Figure 1 is a nice sketch visually, but I did not see how it shows the benefit/key idea of NCCL specifically (which is about finding optimal learning rates). There is no visual/diagramatic element of how those learning rates might be chosen. (Alternatively put, a similar figure could be used to describe eg GEM).\n\n**Update to review**\n\nThanks to the authors for responding. They did clear up point 5 (above) for me. However, I shall keep my score of 4. Unfortunately I cannot see the new revision of the paper that the authors refer to, meaning I cannot change my score. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The main claim of this paper is doubtful",
            "review": "This paper attempts to provide a convergence analysis for nonconvex continual learning with episodic memories, and try to theoretically show the degradation of backward transfer caused by  overfitting to memorized samples.  It further proposes an algorithm for learning rate scheduling in  nonconvex continual learning based on these results.\n\nThe reason of the score of the paper is that the theoretical proof is wrong in my understanding and cannot support the main contribution claimed in this paper,  the main problems are as below.  \n\nThe proof of the main theorems is questionable regarding the nonconvex assumption, which is the most important contribution claimed in this paper.  Regarding the inequality in  Eq.(5),  in my understanding it is hold to be true only when f is a convex function [1].  And the theorems are based on this inequality which cannot be hold  for nonconvex case if this inequality is not true for nonconvex functions.  If I'm wrong, authors please provide proof of how to get Eq.(5) by L-smooth nonconvex functions.  \n\nMoreover, in the proof of Theorem 1, Eq.19  (Appendix A), the inequality of the last step cannot be hold unless the inner product of gradients < \\Delta f,  \\Delat g > is always positive, which cannot be guaranteed. Otherwise, there is no reason to develop gradient-based approaches in continual learning, such as  GEM [2] or AGEM [3].  So even if Eq.(5) can hold for nonconvex case, the theorem is still questionable. Therefore, the main claim of this paper is highly suspicious to me.  If authors cannot clarify these issues, this paper would be considered as with significant flaws. \n\nDespite the questions on the main theorem, the assumption of the initial state of the model is quite strong as it assumes the initial values of parameters are close to the optimal values, which is not very practical unless a pre-trained model is applied.  So the  significance of this paper is further limited.  \n\nAs the theoretical part is incorrect, I haven't reviewed the experiments part of this paper. If the authors can clarify all above main concerns, I'm willing to make another round of review. \n\n[1] Nesterov, Yurii. \"Introductory lectures on convex programming volume i: Basic course.\" Lecture notes 3.4 (1998): 5.\n[2] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" Advances in neural information processing systems. 2017.\n[3] Chaudhry, Arslan, et al. \"Efficient lifelong learning with a-gem.\" arXiv preprint arXiv:1812.00420 (2018).\n\n############################feedback to authors' response#############################\n\nI'm aware of the non-convex setting is valid, but since the corrected proof of the theorem is not uploaded, I will raise my score to 3. \n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "revision and improvement are required",
            "review": "In this paper, the authors provide theoretical justifications for memory-based continual learning (CL) methods and provide a scaling learning rate method NCCL to improve the practical performance. The results look quite exciting (there is quite scant theoretical paper for CL), however, after looking into the details of the paper, I was confused by many places and would say the authors need to further improve their manuscript in order to qualify for the ICLR standard.\n\n1. The theoretical analysis is not very impressive. The theory just split out the catastrophic forgetting term C and demonstrated that performance degradation depends on C. However, where C comes from (I know it is an additional term directly from mathematical derivation, but what's the meaning and intuition) is not clearly discussed. Also, the theorem based on the unrealistic assumption e_t is unbiased (Assumption 2), which can never happen in memory-based CL methods. The authors do mention approaches such as NCCL without Assumption 2, but no theory is provided. Probability section 3.2 is on theory without Assumption 2, then please provide a complete theorem instead of just waving hands.\n\n2. Moreover, there are many flaws in the proof, I just list a few of them here (or correct me if I misunderstand). \n- In proof of Theorem 1, second inequality of eq (19), why does the cross product term disappear? i.e., why $||\\nabla f + \\nabla g||^2 <= ||\\nabla f||^2 + ||\\nabla g||^2$?\n- why $C_t = E_I (\\tilde{C}_t)$, when taking an expectation over $I_t$? $C_t$ is defined in eq (7), and there is no randomness over $J_t$ (already with $E_J$). But $E_I (\\tilde{C}_t)$ still has randomness over $J_t$.\n- why $E||e_t||^2$ is written as $||e_t||^2$, we also have randomness in $e_t$ over $I_t$, see definition of $e_t$.\n- In proof of Lemma 1, why $E(||\\nabla f||) = O(E(||\\nabla g||))$ or how do we get the second equality?\n- How do we get the relation of $E||\\nabla g||^2 = O(\\beta^2\\delta / \\sqrt{T})$? I see it is directly assumed in Corollary 1 (expected stationary of $g$ be $\\delta/\\sqrt{T}$). But I think we should derive this instead of simply making an assumption. Actually, $f$ and $g$ are equivalent and interchangeable, if we assume $g$ already converge, does that mean $f$ also assumed converge? But if we directly apply results derived from $f$ this will be circular reasoning. So I am not sure, the authors better make more discussions on this.\n\n3. For practical performance, if we compare NCCL (68.52 accuracies in Table I) with GEM (89.50), A-GEM (89.10), GSS (77.30), or even EWC (68.30), there is no performance improvement at all. The authors further claim their methods are faster in computation, then please also include a time comparison, instead of just mentioning it. Otherwise, it is hard to quantify the contribution of the new method.\n\nOverall speaking, I am afraid that such work does not have sufficient theoretical or algorithmic contributions. And I doubt the true value of designing a new method without any performance improvement. However, I still appreciate the motivation of the paper and will be more tolerant since there are quite scant theory papers for CL. So I would be happy to adjust my rating if all my concerns were properly addressed. If there is any misunderstanding, please also let me know.\n\nupdate: Thanks for the response. However, there is no updated revision in the revision history of this paper. Based on the flaws that I have previously pointed out, it is impossible for me to validate if my concerns were actually adequately addressed without seeing the updated version. I will keep my score unchanged. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}