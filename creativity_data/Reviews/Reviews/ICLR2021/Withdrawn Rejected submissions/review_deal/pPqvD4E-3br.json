{
    "Decision": "",
    "Reviews": [
        {
            "title": "Limited novelty, needs more baselines",
            "review": "In this paper, the authors propose a Partial Label Masking technique to solve \nlabel imbalance issue on multi-label classification tasks. The PLM method\nactively sub-samples the positive or negative labels based on an optimal\nration which gets updated once each epoch.\n\nThe proposed method is a variation of under-sampling of frequent labels\nand has limited novelty. There should be more discuss among the differences\nbetween the proposed method and existing over/under-sampling methods.\n\nThere is not enough theoretical analysis why the propose method would\nwork. For example, why choose sampling method like Eq(4)? \n\nThe experimental study show that PLM can improve model performance on various\ndatasets over vanilla BCE and re-weighting. However, more baselines from\nrecent literature should be used in comparison.\n\nGenerally I think this paper is not ready for publish and recommend for\nreject.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "PLM: Partial Label Masking for Imbalanced Multi-label Classification",
            "review": "This article tries to address the problem of class imbalance which gives a suboptimal performance during classification using Partial Label Masking. The authors proposed a generalised model for both single and multi-label classification problem. \n\nThey propose balancing the ratio between positive and negative samples using an adaptive strategy to determine the ideal ratio which minimizes the difference between predicted probability and ground-truth distributions. This proposed method was empirically evaluated on both multi-label datasets (imbalanced MultiMNIST and MSCOCO) and single-label datasets (CIFAR10 and CIFAR100) and claimed that classifiersâ€™ performance was improved.\n\nThe approach well-motivated. And well spaced in the literature review\n\nThis paper supports the claims based on the proposed method and metrics used. The empirical result presented shows an improvement in classification performance. \n\n\nOverall, I vote for accepting. Class imbalance is a classification problem, especially when allied with class overlap and disjuncts. The problem could be worsened with a multi-label classification problem. I love the idea of alleviating the problem, especially with a multi-label dataset. But my major concern is about the clarity of the paper on using the proposed model for both single and multi-label problem. Hopefully, the authors can address my concern in the rebuttal period. \n\n The paper takes one of the most important issues of classification. For me, the problem itself is real, practical and gives a suboptimal solution.\n \nThis paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. The results obtained shows that the performance more convincing.\n\nAlthough the proposed method provides several related works, I still suggest the authors conduct the following ablation studies to enhance the quality of the paper: \n\nWhat is the performance of the proposed model a dataset with more labels? \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "More work needs to be done",
            "review": "This paper proposes a method to deal with the class imbalance issue for multi-label classification. Specifically, it adopts a partial label masking strategy to reweight the loss by adaptively estimating the optimal ratio for each class. Empirical results on two multi-label image datasets (i.e. MultiMNIST and MSCOCO) and two multi-class (i.e. imbalanced CIFAR-10 and CIFAR-100) image datasets illustrate its effectiveness.\n\n###########################################################################################\npros:\n1. Overall, this paper is well organized.\n2. Authors note that for MLC, not only an imbalance in the number of positive samples between different classes, but also the ratio of positive and negative samples for each class matters. \n\n###########################################################################################\ncons:\n1. For multi-label classification (MLC), there are many evaluation measures, which authors also mentioned in this experimental parts. Please clarify which measure(s) the methods aim to (or can be able to) optimize since different measures for MLC have different effects on the class imbalance issue, which is the key point why to solve the class imbalance issue.\n2. For the \"ratio adaptation\" part in the method discussion, I do not get the point that how to form the discrete distribution (i.e. $P_c^+$, $\\hat{P}_c^+$). Furthermore, in the following I do not get the motivation why to normalize the divergence. Please give more description to clarify it. \n3. In my opinion, the proposed method is a bit heuristic, which may lack theoretical support.\n4. For the experimental results in Table 3, LDAM-DRW usually achieves superior performance than the proposed method. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}