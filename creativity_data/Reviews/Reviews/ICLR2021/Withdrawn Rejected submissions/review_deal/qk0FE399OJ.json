{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper analyzes the behavior of random search-based NAS and provided new insights (e.g., a low ranking correlation among top-20% candidate architectures in the search phase). An extensive set of experiments were also conducted. However, most reviewers found the incremental nature and similarity with previous works to be a concern. I would encourage the authors to better position their work and better explain the novel methodological aspects.\n"
    },
    "Reviews": [
        {
            "title": "The paper aims to bring the help of search strategies to construct the approximate search space.",
            "review": "This paper proposes Evolving the Proxy Search Space (EPS) as a new RandomNAS-based approach. The goal is to find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS’s search efficiency while at the same time keeping a good correlation for the top-performing architectures. EPS runs in three stages iteratively: Training the supernet by randomly sampling from a PS; Validating the architectures among the PS on a subset of the validation dataset in the training interval; Evolving the PS by a tournament selection evolutionary algorithm with the aging mechanism.\n\n\nThe paper is well written and easy to follow. The idea of efficiently sampling from the search space sounds interesting and the paper aims to bring the help of search strategies to construct the search space by itself. However, it is kind of incremental work since the EPS (Algorithm 1) is exactly similar to the aging evolution in Real et al. (2018, 2019) and this paper is using this search strategy to gradually build the search space on the fly. \n\nI think any of the existing search strategies 1) random search 2) Evolutionary algorithms (Real et al. (2018)) 4) progresive decision process (PNAS; Liu, et al 2018), etc can be used to find a proxy search space. While a random sample from GS is simply random search, proposed EPS is exactly the evolutionary strategy to build search space. One may even use progressive NAS algorithm as a proxy search space!  Comparing how these different proxy search spaces improve efficiency will improve the novelty and make the paper more strong.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Improving Random-Sampling Neural Architecture Search",
            "review": "This paper claims that random search-based NAS methods show a low ranking correlation among top-20% candidate architectures in the search phase. To address this issue, this paper proposes to introduce a proxy search space consisting of good architectures and evolve it using evolutionary algorithms. This paper also proposes a simple size regularization to help the NAS algorithm escape from the small architecture traps. The experimental results show that the proposed approach achieves competitive performance with baseline methods.\n\nPros\n- This paper analyzes the behavior in the random search-based NAS in detail and proposes a new strategy based on the observation to tackle the issue.\n\nCons\n- The proposed method should be compared with recent NAS methods to clarify the contribution of this paper. Many related studies are missing.\n- The details of the algorithm of EPS are unclear. For example, it's not clear what exactly is being done in the RandomInitArch and Mutate operations. Also, what is the reason for using sample_set instead of population queue?\n- What do you mean by the population is a proxy search space on page 4? Does the proxy search space mean a specific architecture without over parameterization or an over parameterized architecture? Please elaborate on the definition of the proxy search space and each population.\n- In section 2, the authors give an analysis on random search-based NAS, but does this hold for other conditions such as for the search space used in DARTS and Robust DARTS?\n\nOverall, the analysis in this paper is interesting, but there are some unclear points to be clarified for publication as mentioned above. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Please explain the novelty and motivation clearly",
            "review": "Motivated by exploring the ranking correlations of the existing RandomNAS in NASBench-201, this paper proposes EPS to improve the search efficiency and keep good ranking correlations by evolving the proxy search space (PS) in RandomNAS. Specially, EPS contains three stages: 1) training the supernet in PS, 2) validating the architectures among the PS and 3) evolving the PS by tournament selection with the aging mechanism. Furthermore, a model-size-based regularization is introduced in the selection stage. Experiments on some popular benchmarks demonstrate the effectiveness of the method.\n\nStrengths\n1) The paper is well written and easy to follow. The algorithm procedure is clearly provided and the code is released.\n2) Some empirical evidences are provided to explain the limitations of the existing RandomNAS.\n\nWeaknesses\n1) Lack of novelty. One the one hand, EPS seems a combination of [1] with a weight-sharing supernet, and the differences 2,3,4 with CARS do not convince me well. Except for the motivation and the fitness in EA, EPS and CARS are highly similar. One the other hand, the intuition of the solutions for the limitations which the authors presented in Sec. 2 is not clearly provided. From my view, Q_pop in Algorithm 1 is same with the population in EA methods. The proxy search space is not clearly explained.\n2) How to choose the 4000 architectures from RandomNAS in Fig. 2(a)?\n3) EPS validates each architecture only on a single batch. Dose one batch validation bring biases to the performance and the ranking evaluation of the architectures? Does the phenomenons in Sec. 2 are caused by one batch validation due to the biases? Comparison with Full batches validation (the whole validation set) should be considered.\n4) Results on PTB are not promising. RandomNAS (RSPS) achieves better perplexity with less search cost.\n5) “32 x 5 runs” and “32 settings” should be “36 x 5 runs” and “36 settings”, respectively.\n\nThe novelty and the similarity with previous works are my main concerns. I am currently leaning towards a negative score but would like to see the authors' responses and other reviewer's comments.\n\n[1] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In AAAI, 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "Summary:\\\nThis paper finds an assumption in neural architecture search (NAS) where \"performance estimated in a weight-sharing setting translates to actualy achievable performance\" only holds for uniformly sampled architecture but breaks down when sampling from top-performing architectures. As a result a new sampling method based on tournament selection for NAS is proposed.\n-----\n+Strengths\\\n+Novel insights are revealed for NAS work which may be useful for future research.\n+Clear experiments are devised to demonstrate the insights.\n+Strong experiment and ablation results compared to prior NAS approaches.\n-----\n-Concerns\\\n-My biggest question here would be how the best architecture from EPS compare to other NAS-based models that are successfully adopted in a wide range of vision application, such as EFficientNet. It would be interesting to show performance on larger dataset.\n-I am also interested to know the range of performance between best and worst for top 100/60/20% of models to better understand how poorly correlated top architectures in GS actually translate to loss in performance beyond performance of the final architecture.\n-----\nRecommendation\\\nTo my best knowledge, this paper uncovers some interesting insight and show performance improvements. My main concern is the viability of the proposed structure in more practical vision problems such as recognition on larger dataset. My recommendation is leaning towards weak accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}