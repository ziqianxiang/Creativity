{
    "Decision": "",
    "Reviews": [
        {
            "title": "Recommendation for Acceptance",
            "review": "**Summary**\nThe paper introduces a paradigm for few-shot learning (FSL) referred to as flexible few-shot learning (FFSL). The main difference to classical FSL is that in FFSL a datapoint does not have to belong to a unique class, but instead the same data point can have different labels across episodes: episodes are thus not constructed based on pre-defined classes, but instead each episode constitutes a (binary) classification task based on some hidden context represented as a mapping from a set of hidden (binary) attributes to a class label. The word flexible thus refers to the observation that two instances may belong to the same class in one episode, but to different classes in the next, depending on the context, i.e., the hidden properties on the basis of which examples are compared and categorised. This view motivates a representation learning perspective which aims at inferring the hidden attributes to be able to quickly adapt to new tasks. \nIn order to study the newly introduced FFSL task, the paper introduces two benchmark datasets extracted from the existing attribute-annotated Celeb-A and Zappos-50k datasets. The paper then compares a number of different representation-learning/feature extraction methods for classifier and prototype-based FSL methods and proposed a new method, Mask-ProtoNet, which combines ideas from both of the latter (feature weighting + prototyping) and outperforms the other baselines on the introduced benchmarks, especially in combination with the best-performing representation learning method which is shown to be unsupervised feature extraction via contrastive learning (SIMCLR) with subsequent fine-tuning.\n\n**Pros**\n- the paper is very well written, clear, and easy to follow; it is also self-contained with all the necessary details\n- related work is discussed extensively and in an accessible way\n- figures are used effectively to communicate key ideas\n- the need for a new FSL paradigm is well-motivated\n- the paper contributes 2 datasets which is helpful for supporting research on the new FFSL task\n- the paper introduces a new method which is derived following a logical argument and which is shown to perform well\n- experimental evaluation is rigorous, extensive, reproducible and well-described\n\n**Cons**\n(I have very little to criticise about this submission, but the following could be improved)\n- the newly proposed Mask-ProtoNet could be described in more detail (Algorithm 1 and Section 5.3)\n- (as someone not working in this area) I found the distinction between meta-learning and FSL not very clear; in parts the two seem to be used interchangeably, though I imagine there are some differences that did not come across\n\n**Evaluation**\nBoth the presentation and the content of this work are of very high quality: the paper was a pleasure to read. I therefore clearly recommend acceptance.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not convincing that an attribute-based task (a harder task than object classification) is more useful for FSL.",
            "review": "This paper defined a new few-shot learning scenario where each example may belong to different classes flexibly depending on the chosen context of each episode. It made two new benchmark datasets for this scenario. In experiments, it conducted an empirical evaluation of existing FSL methods on their new benchmarks and found that a combination of unsupervised contrastive learning and supervised attribute classification can achieve improved performance. Furthermore, it proposed a method called Mask-ProtoNet that performs feature selection of relevant features for the context of each episode.  \n\nPros:\n\n1- It is interesting to think about using hard tasks for evaluating few-shot learning.\n\n2- Two new benchmarks are proposed.\n\n3- The writing is good and so easy to follow. \n\nCons:\n\n1- About the baseline. This paper proposed the flexible few-shot tasks which have different contexts across different tasks. To handle the “task-specific context”, we can leverage the task-specific few-shot learning methods such as TADAM[1] or TAFE-Net[2] that are expected to adapt the image representation (features) conditional on the task images (including latent attributes). However, this submission did not use the above methods but a simple metric-based ProtoNet as a baseline and made the arbitrary conclusion that current few-shot learning methods fail in the new benchmark. This is not convincing.\n\n2- About the proposed method. For the proposed Mask-ProtoNet, I am not sure why and how it works better than general classifiers, after reading through the paper. Can authors explain what are the benefits of separating classification into two steps --- “feature selection (by masking)” and “classification”? (Given the authors stated in the paper “A linear classiﬁer learns a weight coefﬁcient for each feature dimension, thus performing some level of feature selection. The learning of a classiﬁer is essentially done at the same time as the selection of feature dimensions.”) While my understanding is general classifiers are doing the feature selection in a soft way, i.e., selected dimensions get higher weights in the classifiers. Making such soft weighting to be a hard marking (in this submission) is not intuitively helpful. \n\n3- About the overall idea. The proposed flexible FSL is absolutely a harder task than normal FSL, as even the same attribute show different appearances on different objects (e.g. “having a handle” on “pot” and “chair”). No matter it is for classifying attributes or objects, a good image representation is always the key. So far, most of the few-shot learning methods are aiming at such representations (more specifically, aiming at good generalizable image representations that can be fast adapted to new tasks). My question is why do the proposed new tasks (attribute-based) encourage further FSL research? Is that because the problem of existing benchmarks (e.g. miniImageNet/tieredImageNet) is “easy to be overfitted by powerful deep models”?\n\nIn summary, I am not convinced that an attribute-based task (a harder task than object classification) is more useful to evaluate few-shot learning or meta-learning method. And, the proposed method is based on the prior knowledge of \"task is based on attributes\" so it made use of attribute labels in training. Its \"new\" masking operation on feature dimensions is not convincing for me, too. \n\n[1] Oreshkin B, López P R, Lacoste A. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in Neural Information Processing Systems. 2018: 721-731.\n[2] Wang X, Yu F, Wang R, et al. Tafe-net: Task-aware feature embeddings for low shot learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 1831-1840.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea with a few significant omissions ",
            "review": "The authors propose a new view on few shot classification. Instead of having a fixed set of classes split into base and novel subsets, they propose to use image attributes to construct classes on the fly during training and testing. That is, in every episode, a class is constructed by random sampling a pair of attributes (such as living and has_legs) and taking the images which have these attributes (i.e person and horse) ad positives, and the ones that don't have at least one of them (such as chair and fish) as negatives. This ensures that the learned representation can't overfit to a particular category definition and has to be truly generalizable. They argue that this setting corresponds better to the real world, where a category of an object can strongly depend on the context.\n\nIn an experimental evaluation on CelebA and Zappos they demonstrate that pertaining a representation on the attribute classification task and finetuning on the proposed attribute-based few shot benchmark provides a strong baseline, compared to directly training for few-shot classification. They also demonstrate that training with a contrastive loss objective first leads to further improvements, presumably, because contrastive loss helps to learn  generalizable features. Finally, they propose an extension of PrototypicalNetworks with a learnable feature selection module which outperforms a simple linear classifier baseline and vanilla PrototypicalNetworks in most settings.\n\nThe paper is very well written and is easy to follow. The idea of a using attributes to define a more challenging setting for evaluating few shot learning methods is interesting and novel to the best of my knowledge. Using attributes to learn more generalizable features has been explored before, however (see Tokmakov et al., ICCV'19). The authors seem to be not aware of that work, which also proposed a very similar approach of using an auxiliary attribute classification loss to learn a more generalizable representation for few shot learning. Moreover, that paper provided attribute annotations for a subset of the ImageNet dataset. The authors should discuss their relationship to Tokmakov et al., and report an evaluation on ImageNet using their attributes, which would be a lot more convincing compared to the 2 toy datasets currently used in the paper.\n\nI also have a few other concerns regarding the evaluation:\n\n1. Why are the episodes sampled differently for the 2 datasets? Either a strong argument has to be provided, or the settings should be unified. \n\n2. Why are you using a cosine classifier for the prototypical networks, but now for the logistic regression baseline? Chen et al., report significantly stronger performance of the Cosine classifier compared to the vanilla one. It has to be added to all the experiments.\n\n3. Another observation in Chen et al., is that the depth of the network has a major effect on the performance of few-shot learning methods. The current ResNet-12 backbone used in all the experiments is not deep enough to make any strong conclusions about the relative performance of the methods. To the very least, results for ResNet-18 and -34 need to be added, and, ideally, also for ResNet-50.\n\n4. In Section 6.1 you are claiming that U-SA closes the generalization gap between SA and SA* on both datasets which is not true. Please correct this statement.\n\n5. Some details of the evaluation protocol seem to be missing. For instance, how many episodes are sampled during evaluation?\n\nOverall, this paper proposes an interesting idea for a new few-shot learning setting but falls short both in acknowledging prior work and in providing a convincing experimental evaluation. If the authors address the concerns about the evaluation protocol listed above and additionally report results on ImageNet using the attributes from Tokmakov et al., showing that their conclusions still hold, I will consider increasing my score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Better to have more technical contribution",
            "review": "The authors propose a more flexible setting for few-shot learning, where the task is not defined strictly as a classification over several classes but a description of several attributes. Although i think this setting is interesting, i am concerned about under what circumstances will be few-shot if the task is defined in this way? As for my understanding, the definition of attributes can be more general (and flexible as mentioned by the authors) \"class\", why this will potentially have few-shot problems? It would be better if the authors can elaborate more in the paper.\n\nAlso, I am also curious why the authors choose Celeb-A and Zappos-50K as the datasets? why not ImageNet (or subsets) as traditional few-shot learning datasets? The ImageNet also provides some attributes which can be found here: http://image-net.org/download-attributes.\n\nIt is interesting that in Fig 3, it shows protonet has significant overfitting in the FFSL setting, which is not consistent as the traditional few-shot learning setting according to my experience. In Fig 5, seems a simple LR + L1 can outperform protonet. What is the possible reason for the overfitting of protonet in this setting? and is it possible to improve the performance with simple techniques like L1/L2 norm, weight decay? (while the mask is trying to overcome the overfitting too)\n\nThe maskprotonet is incremental over the original protonet and the training strategy, i.e., contrastive learning and pertaining + supervised learning, has been explored in previous literature as well. I think the technical contribution is not enough for publication at this stage.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}