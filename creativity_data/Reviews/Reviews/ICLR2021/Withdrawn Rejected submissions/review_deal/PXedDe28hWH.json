{
    "Decision": "",
    "Reviews": [
        {
            "title": "Perturbation Regularization for Smoothnes",
            "review": "Summary:\nThe authors propose a regularization on Learning To Learn (L2L) optimizer-training schemes, specifically, one that minimizes the difference between the current input and a perturbed one.\n\nComments:\nIt seems sketchy that Adam does worse than SGD on mnist.\n\nOriginality: \nI have not seen this perturbation style regularization which I find very interesting. \n\nClarity:\nI found the paper to be clear.\n\nSignificance:\nI am unable to comment on significance or in a deep way in originality due to lack of experience in L2L.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "This paper proposed a L2L optimization algorithm using smoothness regularization.",
            "review": "This paper proposed a learning to learn (L2L) network optimization algorithm. The basic idea is to generate the parameter update by a network based on the accumulated knowledge as states in the optimization trajectories. The major contribution of this work is to constrain the smoothness of the L2L network, by constraining that the similar state variables deduce the similar parameter updates.  Experiments on MINIST, CIFAR and few-shot learning show that the additional smoothness constraint improves the training performance of the baseline optimizer. \n\nI appreciate the introduction of a simple smoothness constraint on L2L optimizer training. This is interesting, however, straightforward. \n\n(1) The proposed smoothness constraint is mainly applied to the simple-Optimizer. Whether the smoothness constraint can consistently improve the optimization performance of different L2L optimizers?\n\n(2) The smoothness constraint can be imposed by different ways besides the proposed smoothness regularizer, e.g., constraining the lipschitz constant of the network for generating the parameter updates. Why is the proposed smoothness constraint selected?\n\n(3) On the hyper-parameters of \\epsilon and \\lambda, this paper selects them by grid search within certain ranges. Do we need to reset these hyper-parameters for different training tasks (including different datasets and network architectures)? \n\n(4) The compared optimizers are insufficient. As far as I know, there are several learning-based optimizers and improved human-designed optimizers in recent years. More  human-designed optimizers and learning-based optimizers should be compared. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concern on the comparison with other learn-to-learn baselines",
            "review": "Summary:\nThis paper tries to improve the learning-to-learn framework by adding smoothness constraints. To achieve this, the authors use projected gradient descend to find the perturbed direction and minimize the distance between the original objective and the one after perturbed. \n\nPros:\n- The smoothness of the weight in optimization is indeed an important problem and hasn't been fully explored in the learning-to-learn area. \n\nI have a few questions and hope to hear from the authors.\n- What is the relationship between your methods and meta-learning? You compare with meta-LSTM in the paper. Can you compare it with other meta-optimization (like Meta-CNN)? Can your constraints be adapt to the method like MAML without RNN structure? \n- How do you model the function m shown in Figure1? The corresponding question is what \\phi represents in experiments 5.1 and 5.2? How many extra parameters do you use compared with the original model? What is the increased computation cost using the proposed optimizer comparing with SGD or Adam? \n- In the experiment part 5.2, the figure shows the plot of loss on the optimization of unseen images. What's the accuracy of those different methods on the testing set and tiny-imagenet?\n\nAnother concern is about the experiment in the FSL part. Currently, the baseline using the learn-to-learn method can achieve pretty good performance. If your methods can be applied to those better baselines, the conclusion will be more convincing. \n\n\n\nThanks!",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Summary\n\nThe paper presents a regularization method for learned optimizers that enforce smoothness. The introduced regularizer is the maximum difference among the updates in l2-sense, among the vicinity of each sample. The method is empirically demonstrated to enhance learned optimizers for both classification and few-shot learning.\n\nWhile the idea presented in the paper is interesting, there are some issues in the presentation of the paper and experimental results that make it hard to suggest the paper for publication in its current form.\n\n# Strengths\n\nThe method improves the convergence speed of learned optimizers, and is applicable to any one of them, as it is a simple regularizer to add on top. This generic nature of the method increases the potential impact of the proposed method.\n\nWhat the paper refer to as smoothness is in fact the stability of the optimizer. As with everything in numerical, we want our algorithms to be stable. This paper touches on this important characteristic.\n\n# Weaknesses\n\n## Incorrect claims\n\nA major concern that I have with the paper is that there are a couple of places where the paper misinterprets the experimental results. The first occurrence is just above algorithm 1, where the paper claim\n\n> Time cost by computing this regularization term has little impact on (the) efficiency of the algorithm and more discussions are included in Appendix B\n\nHowever, it is stated in the appendix that the time difference with and without the regularization term is nearly three-fold during training. It is indeed correct that during inference this does not matter, but it can not be claimed that this has little impact. It has a significant impact.\n\nThe second occurrence is related to Figure 3. The paper claims\n\n>  our proposed smoothed optimizer outperforms all other baselines including hand-designed methods and the original SimpleOptimizer by a large margin\n\nHowever, the difference between the smoothed and the non-smoothed one is marginal.  What is also concerning here is that the smoothed version starts overfitting when comparing the smoothed DMOptimizer in (a) and (d). This, however, has not been mentioned in the text. \n\n## Convergence\n\nAs the method introduces a regularizer, it mustn't interfere with the final result. However, the empirical analysis in this paper focuses on the transient properties, and experiments are run only up to a modest amount of optimization steps, not until the methods converge. This is concerning, especially given that, for example in Figure 4 (d), the smoothed version and the non-smoothed version converge to similar results at the 10k-the step. What happens if this runs for much longer? Would the smoothed version start being overtaken by the non-smooth one? \n\nIn short, all experiments should be run until all optimizers have completely converged, to validate there is no cost being paid for the faster convergence rate in the beginning.\n\n## ADAM and SGD performance\n\nADAM and SGD graphs seem to suggest that their step sizes are too small. The paper does mention that a grid search of hyperparameters was performed. What is the objective that was used to perform this search? What are the optimal values found for each handcrafted method?\n\n## Tested architectures are simple\n\nThe method seems to improve convergence when simple architectures are used, but not with complicated state-of-the-art (SOTA) architectures. For example, GoogLeNet is not very recent but is already enough to make SGD the best optimizer. How does the method perform when training more complicated networks? For example ResNets? \n\n## Experiment details\n\nEven with the appendix, the details on how the training is performed is not entirely clear. For example, how were the training splits divided? How many iterations were performed to train the learned optimizer? How was the validation set used? Was early stopping implemented? Without these details, it is hard to determine the correctness of the experiments.\n\n## English\n\nThere are quite a few grammatical errors and unnatural sentences. For example, in Section 3.1\n\n> neural optimizer m parameterized by phi can accordingly outputs the ...\n\n> the optimizer just updates\n\nSection 5.1 is written in both present and past tense.\n\nThe paper use phrases such as \"it should be clarified\" or \"Note that\" when it actually is just more detail being added to prior description. This makes it a bit confusing.\n\nIn page 6, the paper reads\n\n> smoothed optimizers are almost based on original settings, except for two extra hyperparemeters\n\nShould it simply be that they are identical except for the two extra hyperparameters?\n\n## Math notations and descriptions\n\nThe next sentence describing that all operations are coordinate-wise is also unclear until just above Eq. 3, when the paper mention that the output is scalar. I also suggest a revamp in equations ditinguishing vectors and scalars. Perhaps it might be worth introducing indices for the parameter?\n\nThe Pi symbol is often used as a product symbol. I would suggest a different symbol to avoid confusion. This projection operation is also not well defined.\n\nIn 4.3, typo when first introducing $\\nabla_{s'}$ (it's written $\\nabla'_{s}$). and also \"backprobagate\".\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}