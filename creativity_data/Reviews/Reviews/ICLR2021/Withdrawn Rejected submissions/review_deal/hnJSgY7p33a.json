{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper proposes a general framework to improve graph-based neural network models by introducing self-supervised auxiliary learning tasks. Experiments can be further improved.",
            "review": "This paper proposes a general framework to improve graph-based neural network models by introducing self-supervised auxiliary learning tasks including auto encoding, corrupted feature reconstruction and corrupted embedding construction. The overall framework is composed of a shared encoder and four output heads, one for each task. Experiments are conducted on text classification which show improvements over the baseline.\n\nOverall the idea of augmenting graph-based neural network models with self-supervised auxiliary learning tasks is interesting and well-motivated. The auxiliary tasks are carefully designed. The paper is easy to follow overall. Experiments are conducted with dropout tests.\n\nConcerns:\n1)\tThe idea of incorporating auxiliary tasks to improve the performance of the target model is not new, which limits the significance of novelty of the paper. Although the authors claim that the proposed work is the first attempt to introduce self-supervised auxiliary tasks to GCNs using a multi-task framework, the proposed framework is quite straightforward.\n\n2)\tIn the section of related work, the authors include a quite thorough survey of graph representation learning, self-supervised learning and multi-task learning, which could be briefer. More importantly, the authors fail to discuss the related works of improving the performance of a model with auxiliary tasks, which is more relevant to the paper. \n\n3)\tAlthough the experimental results of the proposed framework are superior to baselines, the behaviors of the variants in the proposed framework are not stable, more insights or investigations on more tasks would be helpful here for a practical guide of the proposed framework. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "GNN + SSL in an interesting and trending topic, but the paper misses related works and the experiments are insufficient",
            "review": "Summary:  \nThis paper aims to study how to enhance GNNs using multi-task learning and self-supervised learning. Specifically, the paper proposes a framework to improve graph-based semi-supervised node classification by jointly training on three self-supervised tasks, i.e., auto-encoding, corrupted embeddings reconstruction, and corrupted features reconstruction, while adopting hard parameter sharing and task-specific head for each task. Experiments show that the proposed method achieves better performance than GCN for both one-layer and two-layer cases.\n\nPros:   \n1. How to apply self-supervised learning in graphs is an interesting and hot research topic.  \n2. Jointly training the semi-supervised task with three auxiliary tasks is intuitive to understand.  \n\nCons:  \n1. Many recent related works regarding self-supervision + GNN are missing, e.g., [1-4] (many more informal publications can be found in arXiv). Though I acknowledge that they are pretty recent (marginally outside of the concurrent date of ICLR 2021), the authors need to compare with these methods to truly demonstrate the novelty of the proposed method.  \n2. Following (1), the three self-supervised tasks proposed in this paper are more or less similar to those studied in [1-4], which greatly limits the technical novelty of the proposed method.  \n3. The experiments are unconvincing and insufficient. Specifically: (1) the authors only adopt GCN as the base GNN architecture, neglecting many competitive baselines such as GAT, GraphSAGE, GIN, etc. Thus it is unclear whether the proposed strategy is generally to work with different GNN architectures; (2) I acknowledge that the adopted three citations graphs were commonly used as benchmarks for GNNs. However, recent studies show that they are insufficient to compare different GNNs (see [5-7]). Thus, more large-scale datasets are needed; (3) there lacks experiments regarding parameter sensitivity, thus some arguments like \"no need to tune hyper-parameters” are not well supported.   \n4. The authors claim that their proposed method can help to alleviate the over-smoothing problem, but only demonstrate this point using a two-layer GCN, which is no way near to be considered a “deep architecture”. I suggest the authors follow [8-11] for appropriate evaluation settings.  \n5. The writing can also be greatly improved. Currently, the organization of related works, preliminaries, and introduction is a bit disordered. There is also redundancy in formulations of the three auxiliary tasks which can be briefed so that there are more spaces for analyses and experimental results.  \n\nMinor points:  \n1. There lacks theoretical or/and empirical analysis on why the proposed auxiliary tasks can help training GNNs or why they are the most suitable choices as GNN self-supervisions. Appropriate analyses can better support and enhance the proposed methods.\n2. There are duplicate references.  \n3. In Section 4.1, it is more common to adopt comma as digit group separators, e.g., 1,000.  \n\nBased on the above concerns, I am voting for rejection.  \n\n[1] Strategies for Pre-training Graph Neural Networks, ICLR 2020.  \n[2] When Does Self-Supervision Help Graph Convolutional Networks, ICML 2020.  \n[3] GPT-GNN: Generative Pre-Training of Graph Neural Networks, KDD 2020.  \n[4] GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training, KDD 2020.  \n[5] Link Prediction Based on Graph Neural Networks, NeurIPS 2018  \n[6] Graph Meta Learning via Local Subgraphs, arXiv:2006.07889  \n[7] Pitfalls of Graph Neural Network Evaluation, arXiv:1811.05868  \n[8] DeepGCNs: Can GCNs Go as Deep as CNNs?, ICCV 2019  \n[9] PairNorm: Tackling Oversmoothing in GNNs, ICLR 2020  \n[10] DropEdge: Towards Deep Graph Convolutional Networks on Node Classification, ICLR 2020  \n[11] Simple and Deep Graph Convolutional Networks, ICML 2020  \n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposed three different auxiliary tasks for self-supervised learning to improve the performance of GCN on node classification.",
            "review": "Pros:\n- Self-supervised learning has been proven effective in many tasks. Recently, there has been a trend of migrating SSL to graph representation learning.\n\n- The three auxiliary tasks (AE, FR and ER) are set reasonably. The performance of the proposed method looks good when compared with GCN.\n\nCons:\n- The novelty and contributions of the paper are not significant. The auxiliary tasks set in the paper (AE, FR, and ER) are widely used in Computer Vision. Transferring the method to graph representation learning is not difficult and thus of limited technical contribution.\n\n- In the title and the abstract of the paper, the authors claimed that the proposed method is a general framework that can be used for many graph-based neural network models. However, it is just improved over the basis of GCN but didn’t show how it could work with other GNN models. The authors should provide more details in order to make this claim valid.\n\n- The paper misses many existing SSL approaches proposed for graph representation learning. It is also suggested to perform empirical comparison with them. Such comparisons are necessary in making the results convincing and significant. Current study only compares with GCN, which is weak.\n\n[1] Self-Supervised Graph Representation Learning via Global Context Prediction\n[2] Self-supervised Learning on Graphs: Deep Insights and New Directions\n[3] SGR: Self-Supervised Spectral Graph Representation Learning\n[4] When Does Self-Supervision Help Graph Convolutional Networks?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review#3",
            "review": "This paper studies training GCNs with multiple self-supervised auxiliary tasks. The auxiliary tasks are autoencoding, corrupted feature reconstruction, and corrupted embedding reconstruction tasks.\n\nI give a score of 4 to this paper for the following reasons:\n\nThe results in this paper are only on Cora, Citeseer, and Pubmed datasets, and considering the complexity that the multi-task learning framework has introduced, the performance improvement is marginal. Also, a set of challenging and more realistic graph datasets have been proposed in “Open graph Benchmark: Datasets for Machine Learning on Graphs”. I expect this paper to show results on the more realistic OGB datasets.\n\nRegarding the results on over-smoothing, over-smoothing usually occurs on much deeper networks. As shown in “Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View”, over-smoothing starts to harm performance when using more than four layers in Cora. If the paper claims to relieve the over-smoothing issue, it has to show results on deeper networks. Also, recent work has shown that we are able to train around 10 layers of deep GNNs by adding normalization and if the work claims to relieve over-smoothing, it should compare with those strategies proposed (e.g., \"Toward Deep Graph Neural Networks\").\n\nThis work claims to be the first to introduce auxiliary tasks for the training of GCNs. However, some works already proposed similar architectures (e.g., in “Strategies for Pre-training Graph Neural Networks”) which are not discussed in the Related Work Section.\n\nQuestions:\nIn the auxiliary tasks loss functions (equations 2 and 4) the set of nodes is mentioned as V_{FR} and V_{ER}. However, it’s not explained what these subsets of nodes are. Can the authors elaborate on them?\nIn equations 2 and 3, it’s assumed that one at a time, a subset of features will be selected (I assume it’s a random selection of features) and then the model will reconstruct those features. My question here is that if we mask out a column feature completely, how the model will train to output that specific feature given all of the column is masked out?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}