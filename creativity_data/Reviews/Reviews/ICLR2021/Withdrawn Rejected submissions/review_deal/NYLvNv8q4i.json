{
    "Decision": "",
    "Reviews": [
        {
            "title": "Well written but lack of novelty",
            "review": "This paper reviews intragroup sparsity (section 3) and introduces some formulae to analyze the sparsity structure (section 4). In section 5 a variation of targeted dropout training is introduced (5.1) and the choices of group size (5.2) and accuracy as a function of sparsity level (5.3) are considered. Secton 6 describes the experimental setup for the models trained for maximum accuracy and minimum compute with detailed results presented in appendix A.\n\nPros\n\n* Well written and presented\n* Thorough literature review and related work section\n* Encouraging experimental results\n\nCons\n\n* The theoretical analysis is relatively straightforward and doesn't provide any surprising insight\n* The new training algorithm seems like a relatively minor extension\n* The experimental results are encouraging but not significant enough to draw any strong conclusions\n* A large part of the paper consists of reviewing existing work (authors' contributions only start on page 5)\n\nOverall, I believe this paper falls short on novelty. The theoretical analysis and proposed algorithm don't significantly advance our understanding of intragroup sparsity and experimental results are not significant enough to stand on their own.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review of Intragroup sparsity for efficient inference",
            "review": "Summary: This paper proposes to use intragroup sparsity, a known sparsity regularization, for producing lightweight networks. With small weight group sizes, the proposed sparsity method outperforms current lightweight architectures and other structured sparsity models. The experiments demonstrate that this intragroup sparsity is useful for hardware acceleration. Some theoretical analysis of the proposed model is also provided.\n\nStrengths of this paper are: (1) The idea of using intragroup sparsity is interesting. Compared with unstructured sparsity, a key advantage of the proposed is that groups of nonzeros can be properly organized so that no irregular data access. Hence, it improves the efficiency of neural network inference; (2) The experimental results are promising with respect to model efficiency v.s. accuracy compared with baseline methods. \n\nWeaknesses of this paper are: (1) The theoretical analysis is too simply (not insightful). Equation (2) defines a relationship between sparsity level and group size. However, it does not tell us how the accuracy is affected by the intragroup sparsity. Will a certain level of group size help or improve the accuracy? Or, the intragroup sparsity always makes a trade-off between model efficiency (reduce computing time and storage size) and accuracy; (2) The idea of applying the intragroup sparsity to neural networks is relatively new. However, in general, novelties and technical contributions of this paper are limited. Hence, I tend to think that contributions of this paper are not strong enough for ICLR.\n\n\nMore comments & questions:\n(1) Names and figures titles are not well presented. Is CCI-sparsity Cross-channel Intragroup sparsity? To improve the readability of this paper, it is better to make the algorithm names consistent. Fig 2(d) is CCI-sparse --> CCI-sparsity ( consistent with titles in Fig 3(b)). Table A.4 CI-sparsity --> CCI-sparsity.\n(2) Compared with no-group results, as shown in Figure 2(d), the CCI-sparsity method has a large gap when target rate =15. Why is this the case?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Currently recommending rejection",
            "review": "**Summary**: The paper studies a hardware-friendly version of the weight pruning, which imposes a structural (blockwise) constraint on the weights: given a block, only a certain fraction of the weights are allowed to be non-zero. The interest in such constraint is well motivated from the hardware perspective: with fixed (and known a priori) blockwise weight sparsity s, the FLOPs in matrix-vector multiplications can be reduced by a factor of 1/s. The contributions of the paper as follows: 1) authors give a theoretical FLOPs reduction for two particular variations of intragroup sparsity (BB and CC) 2) authors assess the implications of group sparsity constraints under the assumption of pruning the weights uniformly at random and 3) authors modify the targeted dropout sparsification method to suit the intragroup sparsity constraints and give experimental results.\n\n**Decision**: While the studied problem is well-motivated, the paper's contributions are not enough to recommend acceptance. \n\n**Detailed comments**:\n*Pros*: Authors make a good job of explaining the concepts of intragroup sparsity, and fig 2 makes a nice graphical supplement. The paper is well-timed: with the recent announcement of intragroup sparsity support in NVIDIA's Ampere GPUs, this research topic is of great interest.\n\n*Cons*: The claims in the paper are not substantiated or not discussed in a rigorous manner. *Major concerns*:\n- In the introduction, the authors write that they provide a theoretical analysis and the resulting generalization performance of the weight sparsification.  The analysis is given in section 4, where authors study the pruning problem in probabilistic treatment, assuming every weight has the same probability of being pruned, and analytically compute the $P_d$ â€” \"the probability of any weight failing to be assigned to an encoding slot.\" Unfortunately, this probability has nothing to do with generalization error nor with the actual pruning. Firstly, the actual  (optimal) pruning is a model-dependent procedure; thus, it cannot be uniform. On the contrary, most empirical results suggest that weights are pruned non-uniformly (some layers and some neurons tend to contain many more weights than others). Secondly, even if the uniform pruning assumption is satisfied, the analysis only gives an insight into the hardness of assigning weights into intragroup sparsity pattern. The actual implications of this, i.e., generalization loss, are not studied and most probably will require much complex machinery.\n- The suggested improvement (modification) to the targeted dropout is not clearly communicated. While certain parts and ideas are given, the actual implementation of the method would be a challenging task to the readers of the paper (certainly to me). At the current stage, it is unclear how does the final algorithm looks like and what are the differences when compared to targeted dropout? Having pseudocode and/or rewriting section 5 would improve the paper. \n- The evaluation/comparison of the results is not convincing. Authors only compare to some structured pruning results (some are relatively outdated, e.g., results from 2016 for VGG16 pruning), and some intragroup sparsity results. The literature of model compression has many more approaches and results. Some comparison points for a) structured pruning results:\n    1. AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters (Neurips2019)\n    2. DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures (ICLR2020)\n    3. Minimizing FLOPs to Learn Efficient Sparse Representations (ICLR2020)\n    4. HRank: Filter Pruning using High-Rank Feature Map (CVPR2020)\n\nb) other approaches on minimizing the FLOPs:\n   1. Automated Multi-Stage Compression of Neural Networks (ICCV Workshops 2019)\n   2. Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer (CVPR 2020)\n   3. TRP: Trained Rank Pruning for Efficient Deep Neural Networks (IJCAI 2020)\n\nWithout the relevant comparison, the claim of \"our methods produces models that outperform state-of-the-art lightweight architectures and pruned models\" cannot be considered to be valid. That being said, I believe there is a very interesting avenue for this paper: there is no need to beat the state-of-the-art results; perhaps obtaining a smaller (already compressed model) and further speeding-it up with intragroup sparsity would be a stronger positioning? \n\n*Minor concerns*:  In my opinion, the literature review can be significantly improved to better position the paper in the field of model compression and to underline the authors' contributions.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Practical overview of sparsification",
            "review": "## Summary\nThe authors investigate intragroup sparsity as a practical alternative to fine-grained/unstructured sparsity, to show that the latter is more amenable to hardware implementations. They also show that I/O costs at inference time can be reduced by inducing intragroup sparsity.\n\n## Pros\n* The results are comprehensive, and showed on a larger scale dataset such as ImageNet\n* Perhaps one of the first models with < 2M parameters that achieves nearly 70% top-1 accuracy on ImageNet\n* The algorithm is well-designed and motivated, with a clear goal of improving inference performance\n\n## Some concerns\n*  The theoretical justification almost entirely rests on Eq (1), a count of the number of subsets of cardinality i given a set of cardinality G. This is intuitively clear, however, the justification for why the particular approach (L1/2 regularization) is effective and further strengthened by this reasoning.\n* Algorithm 2 seems quite standard for matrix multiplication with structured sparsity, perhaps I am missing the novelty here.\n\n## Recommendation\nI would suggest marginal accept since the other reviewers may see something I may have overlooked; however, overall, I believe the results are valuable and convincing, and the problem space is well-motivated - but I do not find the algorithms and analysis particularly novel.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}