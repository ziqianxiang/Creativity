{
    "Decision": "",
    "Reviews": [
        {
            "title": "An inspiring paper...",
            "review": "The authors analyzed the gradient penalty in GANs from a maximum margin perspective and proposed a unifying framework of expected margin maximization and showed that such a framework can be degraded to a wide range of gradient-penalized GANs. Specifically, the authors showed that gradient penalties induce a large-margin classifier and the expected margin maximization can help reduce vanishing gradient at fake samples. Finally, the authors have done various experiments to prove the efficacy of proposed $L_\\infty$ gradient norm penalty with Hinge loss. \n\nGenerally, this paper was well written. Moreover, it is interesting to connect the gradient penalty and maximum margin. The authors described how expected margin maximization helps reduce vanishing gradients at fake examples, which is very inspiring.\n\nI personally like the paper and would vote a weakly accept. Some of my major comments are listed as follows. \n\n(1)\tThe contribution is a little limited, since the authors just considered the gradient penalty based GANs whose merits have been analyzed by several previous papers and the authors merely explained why the maximum margin can help reduce the vanishing gradients. It could be even more interesting if the authors can consider more general cases (e.g. some non-gradient penalized GANs such as SNGAN which might be also related to maximum margin).\n  \n(2)\tThe toy example of Figure 1 appears a bit simple. It may be better to conduct more complex experiments. For example, a two-moon experiment (a non-linear classification task) may better reflect the nature the algorithm.\n\n(3)\tIn section 5.2.3, it may be better to design a simple experiment to analyze the gradients at fake samples. For an example, the authors can plot the distributions or histograms for gradients at fake samples. \n\n(4) Some analysis is missing. It may enhance the paper if more analysis should be conducted. For examples, the gradient visualizations may be shown and the sensitivity of hyper-parameters can be analyzed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Gradient penalty review",
            "review": "##########################################################################\n\nSummary:\n \nThe paper shows that gradient penalty can arise from a maximum margin perspective of a binary classification problem. This novel and interesting connection is applied to GANS and various discriminator objectives arising from this framework are compared.  \n\n##########################################################################\n\nReasons for score: \n \nCurrently I tend to a weak reject. The aforementioned connection is definitely interesting and original. Additionally it seems to increase the performance over the Wasserstein GAN. However, the main concern is the quality of the presentation, formally as well as in parts content wise (more details in Cons section). However, depending on the rebuttal I would be willing to raise the score.\n\n \n##########################################################################\n\nPros: \n \n\n1. The paper draws a very interesting connection between gradient penalty and the maximum margin perspective of classification. \n \n\n2. Based on the above it explores various possible objectives (i.e. combinations of various loss functions and gradient penalties) and provides evidence that a larger margin indeed leads to a better FID.   \n \n\n3. Additionally evidence is provided that a larger margin also stabilizes the training process. \n\n\n4. I belive that this connection might in general be interesting for classification problems, not only for GANs.     \n \n\n##########################################################################\n\nCons: \n \n1. In general the paper is written somewhat confusingly and is not well structured. Obvious or well known results (e.g. Bounded gradients <==> Lipschitz continuity, extensive introduction into GANs) are described in detail whereas some parts/claims lack appropriate arguments (e.g. see point 2). \n\n2. Often it is not clear if the authors provide empirical evidence or a mathematical proof of statements. E.g. in section 5.2. it is stated that they prove that \"better gradient at fake samples ==> stable GAN training\". Such claims often are merely supported by experiments or are even only argued loosely. \n\n3. While the experimental results look promising, they lack significance tests. And while it is of course reasonable to compare to WGAN it would also be nice to test against other architectures.  \n\n\n##########################################################################\n\nQuestions during rebuttal period: \n \n\nPlease address and clarify the cons above \n \n\n#########################################################################\n\nMinor comment: \n\nThe reference to HÃ¶lder inequality and the dual norm at the beginning of page 4 is not necessary since these are well known results.\n  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Gradient penalty from a maximum margin perspective ",
            "review": "This research states that a wide range of GANs and shows that gradient penalty can be explained from the margin maximization perspective. Some experiments are conducted to show that penalizing gradients could induce a large-margin classifier/discriminator in GANs. They further prove that expected margin maximization is equivalent to training a classifier with a fixed Lipschitz constant and subsequently helps reduce vanishing gradients at fake samples. On top of this framework and insights, the research hypothesizes and experimentally shows that an $L_\\infty$ gradient norm penalty improves the robustness of the GAN family models.  \n\nStrength:\n1.\tThe idea that reformulating GANs from the perspective of expectation margin maximization is interesting as a new conception, which might inspire future work.\n2.\tThe paper proved that expected margin maximization helps reduce vanishing gradients at fake samples and thus improves the robustness of classifiers in GANs.\n\nWeakness:\n1. The margin reformulation is not surprisingly new from a technical perspective,  which could be regarded as a simple extension of  Elsayed et al. (2018)'s work.\n1.\tThe organization should be improved. Especially, the theoretical arguments are not organized with a clear logic.  For example, it might be better to highlight the three statements in Sec.5 in the form of Propositions.  The statements such as \"classifier maximizes an expected margin\" , \"better gradients at fake samples\", and \"stable GAN training\" should be represented with rigorous mathematical expressions. And the proofs in the appendix could also be reorganized in such a manner.  \n2.\tIt seems that a systematic review of related work is missing. \n3.\tThe authors claim in the abstract that they propose a new  $L_\\infty$  gradient norm in this paper. However,  Table 1 shows that $L_1$ gradient penalty and $L_2$ gradient penalty have larger expected $L^p$ margins than $L_\\infty$ gradient penalty.  So I'm wondering what is the advantage of such a new gradient norm. It is better to involve some theoretical analysis/evidence to prove the advantages of $L_\\infty$  gradient norm penalty. \n5.\tIn section 3.1, I cannot quite understand the difference between i) the minimum distance between a sample and the boundary or ii) the minimum distance between the closest sample to the boundary and the boundary? Perhaps rephrasing the words might help to eliminate ambiguity. \n\nAbove all, I do believe that the idea of reformulating the GAN family models as a margin maximization framework would inspire future studies. Nonetheless, the current version of the paper is not mature for publication both from a theoretical perspective and from a practical perspective. I have to suggest a rejection currently. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very interesting, but a bit handwavy",
            "review": "The paper explores margin maximization through the use of gradient norm regularization. The motivation of the paper's contribution is that this idea was motivated through the use of wasserstein GANs, which have a 1-smooth requirement of the dual problem's gradient, which turns into an inf-norm regularization of the primal problem's gradient. The paper then goes on to claim that in fact, in a much wider array of problems, margin maximization can be achieved via gradient norm regularization.\n\nI am actually pretty intrigued by this premise and do not believe it to be false. However, I think that, given the simplicity and elegance of the idea, the proofs could be much more rigorous. \n\n - First, it is not clear why it is necessary to consider this framework over GANs, as really, any margin-based classifier could be studied in conjunction to this, and would simplify the notation significantly. \n\n - I am mainly not convinced that $\\gamma(x)$ is the right definition of the Lp norm margin. Is that common? It seems that, for an interpolating classifier, gamma(x) --> 0 for any x close enough to the optimum. Why would this quantity be maximized? \n\n - I do agree that the penalty gamma-tilde makes more sense than alpha-tilde, since, as we know in logistic regression, not penalizing by the gradient can lead to arbitrarily large loss values with basically no change in the effective margin--hence I do believe the premise of the tool is well-founded.\n\n - One curiosity I had, if we interpret the negative gradient as the dual variable, then an inf-norm penalty on the gradient is equivalent to a 1-norm constraint on the primal variable. In a sense, it is forcing the primal variable to be sparse--this does not really correspond to the margin being maximized\n\n - The term \"better gradients\" threw me for a loop a bit, but if the authors are saying that the loss is basically worse for fake data, assuming that classification worked really well, then I agree. I do not know if that really corresponds to \"better gradients\" because in fact, if you don't gradient regularize, you could get arbitrarily large gradients as well. \n\nOverall, I think that the paper is a bit simple, and the premise that gradient regularization causes margin maximization is reasonable, but not really proven. After looking through the proofs, I do not find these concerns really addressed. \n\nI would appreciate any discussion the authors have on this.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}