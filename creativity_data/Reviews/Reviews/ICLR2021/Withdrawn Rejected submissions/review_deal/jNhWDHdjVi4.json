{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a semi-supervised model (named CPC-VAE) that trains a variational autoencoder (VAE) and a NN classifier simultaneously. The method maximizes an ELBO subject to a task-specific prediction constraint and a consistency constraint. The constraints are defined as some expectations of the variational posteriors. Such constraints are known as posterior regularization. Though the consistency constraint seems to be new, the prediction constraint has been well-examined under deep generative models (see e.g., max-margin deep generative models for (semi-)supervised learning, IEEE TPAMI, 2018). The paper needs more a thorough analysis and comparison. "
    },
    "Reviews": [
        {
            "title": "A New VAE model for semi-supervised problems",
            "review": "Summary:\n\nThis paper proposes a new VAE framework for semi-supervised problems, which uses the latent representation \\\\(z\\\\) to reconstruct input image \\\\(x\\\\) and to serve as the features for the classification of the label of \\\\(x\\\\). Based on this framework, the paper also proposes additional \"cycle\" losses, where the label prediction based on \\\\(\\overline{z}\\\\) of \\\\(\\overline{x}\\\\) is close to the true label (for data with supervisions) or the label of \\\\(x\\\\) (for data without supervisions). In addition, the paper also introduces another loss term of \"aggregate label consistency\" and applies other techniques including noise likelihood and STN. The proposed approach outperforms M1 and M2 of Kingma et al. 2014 on the synthetic dataset and shows more stability than M1 and M2. It seems that the performance advantage of the proposed method over others is not very significant on real datasets.\n\nPros:\n- The idea of using  \\\\(z\\\\) to reconstruct input image \\\\(x\\\\) and to serve as the features for the classification of the label of \\\\(x\\\\) is simple and straightforward (in a good way). The consistent PC loss is also intuitive and adds more credits to the technical depth of the paper.\n\n- The released code is a plus of the reproducibility of the paper.\n\n- The writing of the paper is clear in general.\n\nCons:\n\n- The demonstration of the motivations of the paper is a bit confusing to me. The half-moon data in Fig 1 is simple, therefore C=2 is good enough for the models to fit. The proposed model is more stable when C increases. But it's less motivated to increase C in this case given the data simplicity. So C=14 might not be a good setting in this case. Also in Fig 2, MNIST is clearly more complex than the half-moon data. Therefore, C=2 might not be a good setting in this case. The settings of Fig 1 and 2 are less intuitive to demonstrate the motivations.\n\n- One of the main claims is the high running cost of M2. The basic framework (PC) of the paper might have an advantage in running cost. But I'm wondering if it still has when added with the consistent PC loss, aggregate label consistency, and also STN. With those components, the complexity of the proposed model clearly increases and it seems that without those components the proposed model has no clear advantage over others. The paper didn't provide any empirical comparison of the running cost.\n\n- There seems to be no clear advantage of the proposed method over others in terms of performance. There are wins and loses across different datasets. I also have a concern about the fairness of the comparison. For example, it is reasonable to expect that with STN, the performance of M1/M2/others might be also improved. In the comparison, CPC is with STN but the numbers of M1/M2 are from their original papers. Therefore, it is unclear where the performance gain comes from.\n\n-  As the proposed method involves several components. Some of them are not detailed enough. For example, it's a bit unclear to me how the noise likelihood is employed given a short paragraph of description.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe author response addresses some of my concerns. So I have updated my rating from 4 to 5. I recognise the novelty of the proposed framework but I feel that the main framework has not shown a clear performance advantage given several other components are added. Therefore, I am unable to give a clear recommendation for acceptance. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok submission, experiments and results not very encouraging",
            "review": "The paper proposes a framework for semi-supervised settings to leverage both unlabeled data and (limited) labeled data where VAEs are trained subject to regularization terms from label information. More specifically, the proposed method trains a VAE and a NN classifier simultaneously by optimizing an objective that consists of the usual (unsupervised) variational lower bound, classification error for the labeled data based on the latent space, and consistency term for all data encouraging the same prediction for latent representations corresponding to the original and reconstructed version of a data point. The proposed method is compared with a few other deep generative semi-supervised learning methods on three image datasets.\n\n+:\nThe paper is well written. It mentions some of the shortcomings of the previous approaches and motivates the proposed method and puts it into context. It is nice to see the details of the hyperparameter values used in the experiments. The results show reasonable performance in terms of classification accuracy, and demonstrate the effectiveness of using unlabeled data, predictive loss regularization, consistency loss regularization, and aggregate label consistency regularization.\n\n-:\nThe proposed method involves several pieces that are put together. The regularization term from prediction loss, the regularization term from consistency loss, aggregate label consistency term, entropy regularization term for predictions, “noise-normal” likelihood, considering dimensions accounting for transformations in the latent space, KL scaling, …. The importance and contribution of some pieces to the performance is not clear, like the likelihood and entropy regularization. Especially that some like the likelihood, KL scaling, and transforms seem to be applicable to other VAE-based semi-supervised methods. Additionally, as all these terms have an associated multiplier, investigating sensitivity to these hyperparameters is important and will be helpful. The results in Table 1 and 2 are not very promising in terms of outperforming other existing methods.\n\nOverall, the paper adds another VAE-based semi-supervised learning method that tries to address some of the limitations of previous approaches, but due to the above points, in my opinion in its current format is at a marginal level.\n\nPost-rebuttal: Thanks for the authors’ response. After reading the responses and other comments and checking the updates to the paper, I retain the score and my recommendation at weak accept.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, with some concerns",
            "review": "The authors developed two constraints (and other techniques like using Spatial transformer, aggregate label consistency) that can better balance the generative and discriminative goals when learning VAEs and other generative models. The paper is easy to follow, the proposed methods look interesting and sound to me. According to Figure 1 and 2 in the paper, the proposed methods do have some benefits over other prior approaches. However, I have the below concerns which make me hesitate to accept this paper.\n\nRegarding the overall performance:\nOne contribution, as highlighted in the abstract, is that the proposed method PC VAE and CPC VAE can boost semi-supervised classification performance. However, the major results shown in Table one and Table two seem do not strongly support this claim. In the Table one, the proposed approach only clearly outperforms other methods in NORB. Actually, many counterpart methods this paper compared with do not have results for NORB.\nIn the Table 2, clear benefits are from cycle-consistency rather than the predictive constraint, and the CPC also does not show clear benefits over M1+M2.\n\nRegarding predictive constraints:\nThe PC VAE is very well motivated. However, it finally falls into a multitask objective (equation 9) for handling both labeled and unlabeled data. It is not clear that how PC itself seriously improve over prior VAE-based semi-supervised learning approaches. \nIn the Table two, PC does not outperform M2 and M1+M2. Also, in the Table two, can we compare PC directly with its deterministic counterpart?\n\nRegarding consistent PC:\nThe proposed approach is a little bit similar to the ``\"Unsupervised data augmentation for consistency training”. According the Table two, this technique improves the performance a lot. What if this consistency loss (and or other tricks like aggregate label consistency) was added to loss terms like (7)?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel methodology for SSL-VAE, but too many loosely motivated heuristics",
            "review": "##########################################################################\n### Summary\nThe paper presents a novel methodology for semi-supervised learning with variational autoencoders.\nThe discriminative part is added via prediction constraints to the standard VAE, resulting in PC-VAE. \nThe model is further extended with \"consistency constraints\" (an extra loss component that improves the prediction quality). The final model, CPC-VAE, is experimentally shown to perform better of similar to the competitors. \n\n##########################################################################\n### Reasons for score\n Overall, I give this paper a weak accept. The methodology is novel (to my knowledge), and is shown experimentally to outperform the competitors. However, there is no sufficient motivation behind the consistency constraints. \n The discussion of its effect is also lacking.\n \n##########################################################################\n### Pros \n1) The paper is well written and is easy to understand.\n2) A good introduction into the problem is given, with comprehensive background on unsupervised\nand semi-supervised VAE.\n3) The first proposed model, PC-VAE, is well motivated.\n4) The experiments are thorough. Enough details are given (in the supplement) to ensure reproducibility.\n\n\n##########################################################################\n### Cons\n\n1) Related work is scattered throughout the paper (intro, background, section 3.2), which makes\nit hard to quickly place the work in perspective.\n2) The hyperparemeter \\lambda seems to play crucial role in balancing the generative and predictive parts. However,\nthe analysis of the relationship between \\epsilon and \\lambda is not thorough. There are no specific recommendations provided for the choice of \\lambda based on the desired \\epsilon.\n3) The second model, CPC-VAE, is different from the PC-VAE in 2 ways: the consistency constraints  (eq 10-11)\nand the aggregate label consistency constraint. Neither of these additions is motivated by the model or inference scheme and are heuristic regularisers. At the same time, the unmotivated heuristic regularizer used by [Kingma 2014] (and further works) was one of the big motivation points for this work.\n4) The paper does not discuss reasons for the bad performance of the first proposed model (PC-VAE) in Table 2,\nand only focuses on its modification (CPC-VAE). My intuition is that the discriminator part in PC-VAE overfits.\nThe issue seems to be fixed by the consistency constraints. This should be discussed and more intuition \nshould be provided as to how these consistency constraints fix the PC-VAE model.\n\n\n##########################################################################\n### Some typos/ minor comments\n\nFig. 2: The plot for the M2 model is unnecessary and can be misleading because the latent space\nin M2 only provides auxiliary information to the class,\nis not supposed to encode the class, and is not used to infer the class.\n\nP.6 \"[...], which forces the distribution of label predictions for unlabeled data is align with a known target distribution \\pi.\"\nBad sentence/incorrect grammar. Rephrase, please, e.g. \"[...], which forces the distribution of label predictions for \nunlabeled data to be aligned with a known target distribution \\pi.\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}