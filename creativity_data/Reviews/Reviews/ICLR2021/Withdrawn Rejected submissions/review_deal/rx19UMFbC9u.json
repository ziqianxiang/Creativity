{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces All-Alive Pruning, an approach which checks for and removes the connections to and from units with zero gradient (\"dead\" units). The method is shown to improve performance of IMP at extreme (>128x) compression ratios on MNIST, CIFAR-10, and Tiny ImageNet. All reviewers felt that the problem the authors study -- how to identify and remove dead units -- is an interesting one. \n\nHowever, there were concerns about the practical utility of the method, given that AAP only improves performance for extreme compression ratios in which performance is already substantially degraded relative to unpruned models. I share these concerns, which mute the practical impact of this work. There were also concerns about a lack of proper baseline comparisons to more simple approaches to removing dead units. As mentioned by R1, given that the problem the study is an interesting one, the paper could make up for the lack of practical utility by providing detailed analyses of the settings in which dead units emerge, differences among pruning approaches, etc., but analyses provided here are limited. \n\nI would encourage the authors to explore these areas in a future revision of the paper, but recommend that the paper be rejected in its current form. "
    },
    "Reviews": [
        {
            "title": "Novelty is limited",
            "review": "This work proposes a novel pruning method, called all-alive pruning (AAP), which is a general technique to remove dead connections from pruned neural networks. AAP is broadly applicable to various saliency-based pruning methods and model architectures. AAP equipped with existing pruning methods consistently improves the accuracy of original methods on three benchmark datasets.\n\nStrengths\n1.\tThe motivation is very clear, and AAP is expected to improve existing pruning methods by removing dead connections especially with high compression ratios.\n2.\tThe authors perform various experiments on three benchmark datasets changing experimental settings such as classifiers, base pruning methods, and compression ratios.\n3.\tThe paper is written well. It is easy to understand, and the ideas and experiments are presented well.\n\nWeaknesses\n1.\tThere is no theoretical study of AAP. Algorithm 2 removes dead connections in a greedy manner by making the scores of dead weights as zero, even though they can be alive by revived connections at future iterations. Repeating Steps 1 & 2 of Section 3.2 until the unguaranteed convergence can make it remove important weights having high saliency scores at the worst scenario. Theoretical guarantees on the convergence of the algorithm and the scores of removed weights are needed.\n2.\tThe experiments are not practical. The proposed approach works well only with high compression ratios which degrade the performance of original models. Considering that the objective of model compression is to maintain the original accuracy requiring less resources, such high compression ratios that decrease the accuracy more than 5 points do not seem practical in real-world scenarios.\n3. The novelty is limited. Reviving dead connection is not a new idea.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple yet effective pruning approach focusing on removing dead connections in a network",
            "review": "Summary:\nThis paper talks about a novel network pruning method, called all-alive pruning (AAP), which seeks to effectively remove dead connections in a network. The proposed approach aims at enhancing the saliency-based pruning, and technically the approach searches for the dead neurons by inspecting their gradient flows.\n\nReason for score:\nOverall, I recommend accepting this manuscript. Although the proposed solution (AAP) is simple, the experimental results on several different saliency-based pruning scenarios consistently demonstrate its effectiveness and versatility. \n\nPros:\n- The motivation was effectively described and the problem was well-defined which made it easy to read. \n- versatile and applicable, state-of-the-art\n\nCons/Questions:\n- In Section 2, the authors seem to be differentiating between the terms \"useless connections\" and \"dead connections\". However, In 3.1, they make use of those two terms in a similar manner.\n- Regarding the pruning steps introduced in Sec. 3.2 (specifically the \"Repeat\" step), I would be interested in getting more information about the convergence during iteration. What would be the most significant factor for the convergence?\n- Authors repeatedly mention that one of the positive byproducts of AAP is that it is more compatible with modern memory architecture. Additional descriptions would be appreciated.\n- In Section 4.1, the authors \"conjecture that having the skip connections is advantageous when training after deleting the ResBlock\". A bit difficult to grasp the purpose of this sentence, along with the previous sentences. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A necessary improvement to pruning, but method needs justification",
            "review": "== Summary ==\n\nThe submission deals with eliminating neurons in a network where either a) all the input connections xor b) all the output connections have been pruned. When this is the case, the unpruned a) output or b) input connections are unused and can also be pruned: and the freed parameter budget used for other more useful connections. This is shown to improve the accuracy of pruned networks at a given sparsity ratio, especially for very high levels of sparsity.\n\n== Strengths ==\n\n**Significance:** Nearly all neural network pruning approaches should consider this kind of improvement. Zombie (dead but un-pruned) connections in a pruned model will incur a cost with no benefit. As observed in the related work, some previous literature does explicitly ensure there are no dead neurons, but this is not universal. The authors propose a fairly general kind of approach, so it has potential for adoption fairly broadly across sparsity methods.\n\n**Experimental results** Overall, the experiments are well-executed and show a good effect. The effect of the proposed method, varying with the model size, on the accuracy is clearly shown. The results should be very reproducible, with explicit random seeding and multiple trials.\n\n**Clarity** The core method is described very straightforwardly and completely. Provided code is also very clear, and runnable out-of-the-box, and helped provide some answers.\n\n== Weaknesses ==\n\nWhile this is a well-executed investigation of the idea, I'm not convinced that using the gradient to detect dead neurons is the right approach.\n\n**Complexity** It's not a great deal of additional complexity, but it seems unjustified. Fig 2 itself illustrates that the \"dead\" connections could be found by directly inspecting whether the weights are non-zero. The most thorough explanation, that I can find, as to why the authors did not use the \"basic\" approach is that \"it is not applicable for complex networks with shortcut connections.\" But (Liu et. al. 2020), *do* consider a ResNet network. That paper also refers also to:\n\nHe et. al. \"Channel Pruning for Accelerating Very Deep Neural Networks\" ICCV 2017\n\nthat goes into how shortcuts affect pruning filters in more details.\n\n**Possible drawbacks** It seems possible that an activation or gradient may be zero in one iteration simply due to a filter not being active on examples in a *that minibatch*. So measured, a dead connection on this minibatch does not imply that it will be dead for all possible examples. Especially given that a filter may be preferentially responding to a particular kind of image structure, see for example:\n\nZieler & Fergus \"Visualizing and Understanding Convolutional Networks\" ECCV 2014\n\nIt has also already been observed that some pruning can essentially discard accuracy on example with rarer properties, to preserve performance on data more similar to the mode, see:\n\nHooker et. al. \"What Do Compressed Deep Neural Networks Forget?\"\n\nand it seems possible that the submission's method will tend to work this way to an even greater extent.\n\nThis also possibly isn't measured well by accuracy on small-scale experiments such as MNIST and Tiny-ImageNet: with a larger model and larger datasets I'd guess that probability is greater that a dead connection will be spuriously identified somewhere in the model.\n\nI'd especially appreciate any rebuttal that elaborates on why this method was chosen, rather than directly analyzing the elements of the weights $\\mathbf{w}$ or mask $\\mathbf{c}$ to determine if any node has all-zero input or output connections.\n\n== Misc Comments and Questions ===\n(not relevant to score)\n\n  * Grammar in \"we experiment with dynamic pruning instead of LAP is not compared since our method considers all the parameters in the network\" on page 8?\n  * Typo \"respecitvely\" on p8\n  * Why use np.abs in pruning/magnitude.py instead of torch.abs?\n  * What is the meaning of plus/minus notation in tables in Appendix C? Standard deviation? Difference between highest and lowest?\n  * Is it certain that the all-alive pruning will converge? I.e. is there an obvious reason that the outer loop in Algorithm 2 is known to always terminate in finite time?\n\n== Reason for Rating ==\n\nEliminating \"dead\" neurons, and not wasting the unused connections on them, is obviously a good choice when pruning a network. The central novel contribution of the paper, though, is in specifically how this is achieved. The method proposed in not completely justified by the submission.\n\n== After Rebuttal ==\n\nQ1 provides a helpful clarification. Some of the more concrete possible drawbacks listed above, especially those about pruning \"false positives,\" no longer seem as much of a concern.\n\nSome of the earlier investigation that the authors report, on more direct methods for finding dead connections, may need to be given a more complete treatment in the paper itself. I am not convinced that the direct methods cannot be done on arbitrary architectures, given that previous literature has managed to in a wide variety of examples. Without considering the simpler techniques, the leap to a more complex method for what could be a relatively simple task doesn't have the necessary support.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An effective but trivial idea",
            "review": "The paper proposes to remove dead neurons and their connected parameters through a very simple check while reviving pruned (salient) parameters up to the prespecified sparsity level, such that the sparse network obtained could perform better. The main (and perhaps the single major) contribution of this work is in its demonstration that such a simple method is indeed effective for different pruning methods on various network architectures and datasets. The proposed method (AAP) can perhaps be considered as a generic post-processing step that could be equipped to any pruning method leaving dead neurons.\n\nWhile the result is not unimportant, a few concerns remain. AAP is literally just checking for zero gradients (Line 6 in Algorithm 1) which by itself is very trivial and thus it remains questionable if this is really enough contribution as a sole idea for a conference paper (rather than for a workshop or so to speak); in other words, the idea is too generic that it could be done without having a read  of this paper. Moreover, except for visualization of remaining parameters, the idea is mainly demonstrated only for the performance, and it lacks in-depth analysis, ablations, and insights including for instance, why existing methods produce dead neurons, what’s the cost of AAP (or N value) in practice and theory, etc. So I’m leaning slightly towards rejecting it for limited novelty.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}