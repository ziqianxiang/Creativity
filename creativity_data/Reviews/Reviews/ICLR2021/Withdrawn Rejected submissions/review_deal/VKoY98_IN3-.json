{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of 'Image Animation with Refined Masking'",
            "review": "Reanimating an image with a different 'driving' video is an interesting problem.  Results have come a long way in the last few years.  Techniques like that in this paper allow one to animate an image of one person's face, for example, with the video of another person, with realistic results.\n\nOriginal approaches to the problem were generally model-based, relying on a model of object being animated.  More recent approaches have taken advantage of successes in unsupervised, category-specific, keypoint learning.  Given keypoint correspondences between a source frame and a 'driving' frame, one can essentially infer a flow field that is used to warp the source image toward thee driving frame.\n\nIn this paper, rather than use keypoints, they instead introduce consider the extraction of masks of the objects in the source and driving frames.  Then, given those two masks they obtain the warped source image, first at coarse resolution and then refined to high resolution. With masks it is argued that they incorporate less unwanted information about the specific object in the driving frame when they animate the source frame.\n\nThe use of masks and other minor modifications to previous work are intuitively reasonable, and they clearly yield improvements.  This is evident in the experiments where they compare to several baselines.  They also provide perceptual measures of image quality (SSIM) which are useful for applications like this.  SSIM is not the best measure of image quality, but it is much better than simply relying on readers to look at videos and judge for themselves, and it is differentiable (if in fact the authors wanted to include that in their objective).\n\nNevertheless, in my view, the work in this paper is somewhat incremental on top of work in 2019 papers by Siarohin et al (in CVPR and NeurIPS).  The work is mainly applied, and provides relatively weak contributions to machine learning and representation learning per se.  I think the work would be more interesting to the computer vision community, but I am not sure the contributions will be of broad interest or significant to the ICLR community.\n\n\nI also think the clarity of the writing could be improved greatly. Below I briefly outline some aspects of the exposition that the authors might want to consider revising.\n\n1) I find many of the design choices in the proposed system, beyond the use of masks, to be poorly motivated. This includes the overall architecture as well as other design decisions:\n* For example, an 'identity perturbation operator' is introduced which, as described, simply sets all pixels in the mask lower than a threshold (the median value of the mask) to zero.  Why call this an identity perturbation operator? In what sense does this obscure or perturb identity?  The name and the purpose of setting pixels to zero are both unmotivated.\n* The paper describes augmentations, but does not explicitly discuss why the augmentations are needed, or how critical the specific augmentations used in the paper are.  The reader can infer that the augmentations are used to ensure that correspondence between objects in different frames can be matched when taken from different videos, even though during training different frames are taken from the same video.  This would be straightforward to explain and might aid the reader's understanding.\n* The paper also states that it uses four autencoders (eg for the mask generation, the mask refinement network, etc.).  But I don't see an explicit need for an autoencoder in this context.  I do understand the use of an encoder-decoder network, but I don't see the need for autoencoders per se.  It would be good to clarify.\n\n\n2) The general quality of the exposition can also be improved, beyond clarifying and motivating the architecture.\n* It was very surprising, for example, to see no citations throughout the introduction and the vast majority of the paper except for those in the related work section and the experiments where different techniques are compared. This is both odd stylistically and does not make reading the paper more straightforward.  As one example, when 'post-extraction methods' are mentioned in the second paragraph of the introduction, a naive reader would have no idea what they are without references.\n* The term 'identiy' is used in the first pargraph of the introduction, but it is not clear what it refers to at that point.  In the third paragraph this is resolved in talking about the identity elements of the person in the video, but later in the introduction the paper states that the approach is not specifc to people.\n* On page 1 the paper states that all  training is done with the driving videos while on page 3 the paper states that all training is done with source videos.\n\nNone of these issues are critical flaws, but they do make undrstanding the paper\n unnecessarily difficult for the reader.\n\n3) I think it would help to make the formulation more explicit.  For example, in the loss in (10), what are the parameters that one needs to optimize?  The variable fs in (10) is presumably a function of the masks and hence the parameters of the mask generators, as well as refinement networks etc.  I think it might be helpful to be more explicit in the formulation.\n\n4) The limitations of the method in the paper deserve attention.  I would assume that not only do the objects in the source and driving videos have to be of the same type, should they share similar viewpoints? Is scale an issue?  Are camera motion, cluttered (moving) backgrounds, or multiple objects of interest in the training videos troublesome?  What are the failure modes?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A method for unsupervised object animation is discussed in which the motion params are modeled by masks not keypoints",
            "review": "The paper presents a model free method to animate arbitrary objects in images. It uses masking to extract and retarget animation. They perform perturbations in the mask to delete any identity or appearance information and leave only the pose and shape information. Generation is performed two-stage, low resolution followed by high resolution. \n\n**1 Contribution**\n\nThe paper offers 5 contributions in the introduction, out of which, several require further clarifications, such as (i) requires rephrasing, since FOMM also applies masking to its inputs such as occlusion mask. (ii) which is a contribution of MonkeyNet, Lorenz et al. 2019, or even X2FACE. (iii) foreground and background are somewhat separated in Lorenz et al. 2019. \n\nI'd summarize a contribution of this work as a different motion representation compared to MonkeyNet and FOMM. The representation consists of a mask, which serves as a means for retargeting motion, while in FOMM keypoints and local affine transformations serve as the motion representation. I believe, the contributions paragraph should be edited to better reflect what the present paper proposes. \n\n**2 Method and novelty**\n\n*Masking*. In my understanding, the method  works since the framework is able to remove the identity from the driving mask. Typically when a high-throughput channel such as a mask is used to retarget animation, identity can leak into the final rendering through the mask and worsen the results. To combat the issue here, they use a set of perturbations, such as erosion-like operation during inference and changing color, scaling etc during training. A similar approach to solve the same issue was taken in (Lorenz et al. 2019), they used change of color and shape in training.\n\n*2-stage generation*. The generation is performed in 2 stage low res generation followed by high res generation. I wonder, would it be possible to build a 1-stage system? It makes me even more curious, since it seems that both of them use the same inputs according to fig.1? Fig 2 is different, in it the high res generator uses only low res image? Could the authors comment. Can a bigger generator be used to produce high res images in 1 step?\n\n**3 Results**\n\nAccording to the results there is improvement over the FOMM. I wonder, however, whether improvement comes from considering background not the foreground. As far as I understand, FOMM does not handle background and if there is any camera motion the background will be animated poorly. Could the authors comment here?\n\nI believe the identity is not preserved well during the generation. In Fig. 4 the last row of the vox-celeb example has a different identity. Some of the examples in the supplement have a slightly changed identities. \n\n**4 Rating**\n\nOverall, I think, the paper is below the acceptance threshold. The results look promising, but the proposed idea of using a mask to retarget animation is not particularly exciting.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but no quantitative evaluation",
            "review": "The paper presents an interesting idea for image animation: use object masks as driving embedding. However there are some problems with evaluation and explanation:\n1) The main problem of this paper is the lack of proper quantitative evaluation in both the sota comparison and the ablations:\n\n    1.1) Baselines can not be compared under image reconstruction task with the proposed method. The main problem is that methods with larger bottlenecks has an unfair advantage over the methods with smaller bottlenecks, because in general the larger the bottleneck the better the reconstruction. The bottleneck size of X2Face is 128, for MonkeyNet it is 50 and for FOMM it is 60 float32 numbers. For X2Face the bottleneck is the  driving embedding vector; for MonkeyNet it is 10 keypoints and covariances; for FOMM it is 10 keypoints and 10 affine matrixes. Similarly P2PHD  and FSAL has the bottleneck of $48 \\times 2=96$ numbers, for 48 keypoints.  In the proposed method the bottleneck is the mask.  The size of the mask $64 \\times 64 = 4096$, and given the $P_{test}$ operator  (which zero out half of the values) it will be at least[^1] 2048 number, which is still significantly higher that in the baselines methods. Overall in order to fairly compare under image reconstruction task, the threshold should be at least 97 percentile, which will probably break the entire method. So other methods of evaluation should be devised for the proposed method.\n\n    1.2) For the ablation studies the is no quantitative evaluations at all. \n2) Another problem is the explanation:\n\n    2.1) Figures 1 and 2 is almost impossible to understand, the arrows go back and forth which complicate understanding significantly. I suggest do not use 2 figures for the training and testing stage, instead figures may be separated into` \"mask part\" and \"generation part\". \n\n    2.2) There are many not explained and not ablated decisions:\n    - In Eq.(2) what is the reason behind using D(s) as one of the inputs, isn't $m_s$ enough?\n    - What is the motivation behind augmentation A? Would the method work without it?\n    - In the $P_{train}$ what is the motivation behind horizontal and vertical scaling? Why the image should be split in 6 parts? Why $\\lambda=20$ in Poison distribution, is this value established trough cross validation?\n    - In Fig 3.  there is no visible difference between c and f, which raises a question if the generator h is usefull. Another question if  s in Eq.(4) is useful.\n\n3) Other minor points:\n\n    3.1) In case of FOMM, so called 'absolute' animation is used for comparison. Which is fair, since 'relative' animation is unable to animate images in arbitrary poses. However this should be specified explicitly.\n\n    3.2) It is not clear from where the name 'RobotNet dataset' and the corresponding reference comes from. in FOMM  it is called 'Bair robot pushing dataset', and the reference should be \"Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In CoRL, 2017\"\n\nFrom the bright side it seems that user study confirms the claims made by the authors, however only the user study is not enough and quantitative evaluations should be provided both for the ablations and for the sota comparisons. Moreover non-intuitive decision should be explained or ablated (see 2.2).\n\n[^1] In fact the amount of passed information will be higher since numbers that was zero outed, also provide the information, e.g the number in particular position is less than a threshold. This is one bit of information, overall if we consider standard float32 numbers, there will be $2048 \\times 32 + 2048 \\times 1 = 67584$ bits of information. Which is equivalent to 2112 numbers.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}