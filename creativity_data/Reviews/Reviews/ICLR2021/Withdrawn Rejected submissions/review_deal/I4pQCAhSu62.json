{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes Feature Contractive Learning (FCL), a training framework that takes a more nuanced view of robustness, refining it to the sensitivity of the feature.  There are some differing opinions among the reviewers, with some applauding the simplicity of this new take on robustness while others are unsure of its underlying definitions and relationship to adversarial robustness.  The authors claimed to have clarified some of these points in their rebuttal / revision, but unfortunately, there was not much follow-up discussion by the reviewers.  Ultimately, there are still enough lingering issues that rejection is warranted."
    },
    "Reviews": [
        {
            "title": "Official Blind Review # 3",
            "review": "Summary & Pros:\n- This paper introduces contextual feature utility and contextual feature sensitivity to measure and identify high utility features and their associated model sensitivity, and proposes Feature Contrastive Learning to  balance robustness\nand sensitivity in deep neural network training. \n- For the evaluation, the analysis experiments are extensive.\n\nHowever, I have still some concerns below:\n- Topic Concerns. The goal of this work is to balance robustness and sensitivity. In fact, I am confused the definition of robustness and sensitity as adversarial robustness contains the concept of sensitity. \n- There is not much related work in the paper , and I don't know how important the direction is.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting and simple idea",
            "review": "### Summary\nIn this work, the authors focus on the robustness against only common corruptions and perturbations by defining a contextual feature utility metric. It measures the magnitude of the change in the loss of a perfect model that an input feature can incur. They leverage this metric to design a utility-aware perturbation that they use to control the trade-off between model robustness and its sensitivity to high utility features. They formulate the problem as contrastive loss which can be added as a regularizer for advanced training stages. They dubbed this method as Feature Contrastive Learning (FCL). Finally, they defined another metric dubbed contextual feature sensitivity that is loosely defined as the magnitude of the change in the activations of a model that an input feature can incur.\n\n### Strengths\n1. FCL is a simple method that can be added as a fine-tuning step\n1. Clear submission with sound experimental setup and sufficient results on small synthetic and real datasets\n\n### Weaknesses\n1. The impact of this work would greatly benefit from ImageNet experiments as it is the main benchmark for this limited type of robustness\n1. Two metrics were introduced (utility and sensitivity) but only one of them was used in the rest of the work with only a hand-wavy explanation of their relationship (e.g., it would be interesting to see how they interplay when training with and without FCL)\n1. The motivation behind the contextual feature utility metric should make it model-independent but defined as the model's own loss gradient w.r.t. the input (e.g. in practice, it could be defined by a pre-trained model instead and used for training)\n\nCan the authors comment on the negative points mentioned above?\n\n### Rating\nI like the simplicity of the idea and its applicability. However, I gave it this score mainly because of the previous reasons and its potential impact. Since the authors are interested in these simple corruptions, the value of the work is hindered by the absence of ImageNet experiments.\n\n### Verdict\nThank you for addressing all my concerns. This gives me the confidence to slightly increase my score. \n\nOne more small thing, for future work, you might also consider incorporating clipping the metrics to the input range.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a new contrastive learning based methods to obtain stable features",
            "review": "Summary of work \nThe authors introduce the concept of contextual sensitivity to describe the importance of the feature, which is defined as the absolute value of the Jacobian of loss with respect to the input. High-utility and low-utility perturbations are created by perturbing most important and least important input variables respectively. The embedding of the original input forms a positive pair with the embedding of high-utility perturbation, and forms a negative pair with the embedding of low-utility perturbation, based on which two contrastive loss functions are proposed.\n\nstrength : \nThe authors propose a novel feature perturbation approach by performing perturbation to features with low or high sensitivity distinctively, based on which a contrastive learning loss is developed. Experiments are conducted on CIFAR10, CIFAR100 dataset, and a new synthetic MNIST dataset. The performance of the proposed approach surpasses baseline methods.\n\nweakness : \nThe authors define two loss functions to calculate the contrastive loss and choose the latter one for all the experiments. It would be nice if the authors could provide an explanation for such preference. Does the latter loss function perform better than the previous one? How the data attribute affect the choice of different contrastive loss?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review #2",
            "review": "Summary:\nThis paper introduces the concept of contextual feature utility and sensitivity to illustrate the trade-off between robustness and sensitivity. The authors propose Feature Contrastive Learning (FCL) to regularize models to be more sensitive to features that have higher utility, i.e. change the classification loss of the model to a larger extent. FCL first ranks features according to how much they change the loss values. Gaussian perturbations are then added to the top- and bottom-ranked pixels to form negative and positive pairs with the original input respectively. Contrastive learning is conducted by training the feature extractor of the classifier to minimize distance between the two hidden states in a particular positive pair, in order to align feature sensitivity with utility. The authors create a synthetic MNIST classification task where a subset of digit classes are affected by the presence of other digits at the corner of the image, to test a modelâ€™s selective sensitivity to input features. Through experiments on the synthetic MNIST and two variants of CIFAR datasets, FCL is shown to improve classification accuracy under noisy conditions while maintaining good clean accuracy.\n\nPros:\n\n+Good direction to train models that strive to have a better balance between robustness against label-preserving perturbations and sensitivity towards label-changing perturbations.\n\n+The idea of using contrastive learning to improve the robustness of models is interesting and could potentially be used for other kinds of perturbations.\n \n \nCons:\n\n-Lack of studies on robustness against adversarial examples (L-p and invariance types), experiments currently only show results on robustness on gaussian/uniform noise. \n\n-Little theoretical support or justification on why aligning the feature sensitivity and utility proposed here would help balance between sensitivity and robustness (such as against L-p and invariance adversarial examples).\n\nRecommendation:\nWhile the idea of training a model to be both robust and sensitive at the same time is well-motivated and promising, the experiments here fall short of evaluating robustness beyond gaussian/uniform noise. The claims of the paper would be stronger with robustness evaluation against the widely-studied L-p norm and recent invariance adversarial examples that are also mentioned in this paper.\n \nConsidering the lack of the aforementioned experiments and of more rigorous theoretical support for FCL for robustness that generalizes beyond the gaussian/uniform noise evaluated here, this paper is still not ready for publication.\n\nOther questions and comments:\nWhat is the performance of the model if Contrastic Learning is done by creating positive pairs by just adding gaussian noise to the original image? \n\nSince the proposed method, FCL, relies on contrastive learning, it would help to discuss prior work on contrastive learning, especially highly similar ones such as: https://arxiv.org/pdf/2006.07589.pdf\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}