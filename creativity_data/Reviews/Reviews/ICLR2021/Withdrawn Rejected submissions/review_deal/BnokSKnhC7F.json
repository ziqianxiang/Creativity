{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper considers an alternative to the standard MDP formulation, motived by the novo drug design problem.  The formulation is meant to optimize a notion of expected maximum reward along the trajectory rather than the expected sum of rewards.  The formulation is presented through a variation of the Bellman equation.  Thus mode of presentation does not make it entirely clear what the fundamental problem is and whether it is the right formulation for the application.  The reviewers point out some problems with the analysis.   Experiments compare the proposed max-Q algorithm to Q-learning and demonstrate that it achieves higher maximum reward.  Experiments involving novo drug design show promise.\n\nThis looks like an interesting idea and direction, but the consensus view is that the project deserves further work and polish."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a modified bellman equation for reinforcement learning that optimizes the maximum expected single step reward along a trajectory, instead of the maximum cumulative reward. This formulation is applied to the generation of molecules with optimized properties of interest. A recently published molecule generation algorithm, that constructs molecules step wise via the (predicted) chemical reactions of building blocks, is modified with this new bellman formulation, and shows modest improvements in optimizing for some HIV activity targets.\n\n##########################################################################\n\nReasons for score: \n\nOverall, I vote for acceptance. The proposed max-bellman formulation seems to make sense in the context of molecule generation. However, the performance of this formulation in the molecule optimization task is not particularly impressive.  \n\n##########################################################################\n\nStrengths:\n\n*Paper is generally written and structured clearly\n\n*(From a non-RL expert), the proposed max-bellman formulation seems to make sense for molecule generation\n\nWeaknesses:\n\n*Performance improvement of the max-bellman formulation in the molecule optimization task is pretty modest. Eg from table 2, the PGFS model + max bellman only shows significant improvements over the vanilla PGFS baseline in 1 out of the 3 HIV tasks (with applicability domain restrictions on the property predictor)\n\n##########################################################################\n\nOther comments:\n\n*“For all the three HIV reward functions, we notice that PGFS+MB performed better than existing reaction-based RL approaches (i.e, PGFS and RS) in terms of reward achieved at every time step of the episode” – why is it important that the hiv_x scores are compared at each step. Shouldn’t we only care about the top hiv_x scores independent of the step?\n\n*“However, in the proposed formulation, we noticed that the performance is sensitive to the discount factor and the optimal is different for each reward.” – how is the discount factor tuned for the different models in the molecule generation task\n\n*What are the summary statistics of the top 100 produced molecules for QED and clogP?\n\n*In table 1 and figure 3, do the reported HIV-RT, HIV-INT, HIV-CCR5 results have the applicability domain (AD) restrictions?\n\n*What is the termination criteria for the PGFS model? \n\n*One of the public molecule optimization benchmarks, Guacamol, was mentioned in the paper. What is the reasoning for not using that standard evaluation suite in the evaluation of this work?\n\n##########################################################################\n\nThanks for the response to my feedback. Unfortunately, after reading the reviews/responses from the other reviewers, I have decreased my rating to 5, due to some concerns related the technical aspects of the paper\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Intriguing idea, but the \"Bellman equation\" is wrong.",
            "review": "I like the idea of finding the max-reward policy, but the entire framework seems to stand on a shaky ground.\n\nThe main problem is with Equation (3), where the second equality is wrong. It is easy to come up with a counter-example. Consider an MDP with 4 states plus a zero-reward absorbing state. Assume a fixed deterministic policy and $\\gamma$ close to 1. Let $s_1$ be the start state, with deterministic reward $r(s_1)=1$. With probability 1 $s_1$ goes to $s_2$. Let $r(s_2)=0$, and with equal probability it transitions to either $s_3$ or $s_4$. The immediate reward $r(s_3)=2$ and $r(s_4)=0$, after that it always goes to the absorbing state with 0 rewards thereafter. Since there can be only 2 trajectories, one with max-reward 2 and the other with max-reward 1, we have that $Q(s_1)=1.5$, but it is easy to see that $Q(s_2)=1$, and $\\max(r_1,Q(s_2))=1\\neq 1.5$.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Motivated by the de novo drug design, this submission proposed a new objective in reinforcement learning, i.e., to maximize the expected maximum rather than the accumulated reward along trajectories. The authors defined the corresponding Bellman operator, and then proved its theoretical properties, including monotonicity and contraction. In the experiments, the authors first showed on a simulated grid that when compared with Q-learning, the proposed Max-Q algorithm can achieve higher maximum rewards along trajectories. Finally, the authors tested on de novo drug design task, by modifying the TD target in the previous PGFS algorithm. The new variant achieved better performance across different metrics.\n\nThis work has a good motivation, from a practical perspective. The authors also investigated theoretical properties of the proposed operator, and the results are well presented, which I also appreciate. Furthermore, the simulation results are consistent with theoretical properties to show different characteristics of the Max-Q algorithm. The results on de novo drug discovery are encouraging, as the proposed method consistently outperforms its competitors, thus highlighting its practical significance. On the other hand, some descriptions are a bit confusing and more clarification is needed.\n\nMy detailed comments and questions are as follow:\n1. In Section 2.2., could you provide more explanation on how the maximum reward along a trajectory is related to the the issue of synthesizability there?\n2. When proving contraction in Page 4, f_i looks not to be a function of (s, a). Also, can you add the input, i.e. (s, a), for $r$ in the proof to make it more explicit?\n3. What exactly is the reward $r$ in drug discovery in Section 4.2? How is $r$ related to the metrics in Tables 1 and 2, e.g., RT & INT?\n4. In Page 7, the definition for L_{auxil} is unclear. How does the two terms interact each other?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review ",
            "review": "**Summary:**\n\nThis paper proposes a max reward instead of cumulative reward objective for reinforcement learning. This objective is primarily motivated by applications like chemical synthesis where the goal is for the RL agent to generate the most desirable state possible. The paper then defines the corresponding varaint of the Bellman operator (the max-Bellman operator) and proves tabular convergence guarantees by a contraction argument. Some experiments in a gridworld and simulated chemical synthesis indicate that this objective modification can improve prior algorithms. \n\n--------------------------------------------------------------------\n\n**Strengths:**\n\n1. Formulation of the problem. It is a useful insight that many application may want to use reinforcement learning as a generative model over the state space. In these cases, cumulative reward may not be the best objective.\n2. Theory showing algorithm is sound. The derivation of the modified Bellman operator and subsequent algorithm is done cleanly. The proof of contraction and convergence \n3. Proof of concept that the new objective is useful. The application to chemical synthesis shows that the new problem formulation is able to improve performance on a relevant task.\n\n--------------------------------------------------------------------\n\n**Weaknesses:**\n\n1. Novelty and comparison with prior work. This is the primary weakness for this paper and there are a few ways it manifest itself. The main issue is that a quick search for related work turned up the 2006 paper \"Maximum reward reinforcement learning: A non-cumulative reward criterion\" by Quah and Quek [1]. From what I can tell, they propose the exact same problem formulation and algorithm based on a modified Bellman equation as this paper. Granted, the motivation and subsequent theory are different and I assume the authors had no knowledge of the Quah and Quek paper (I did not either before looking for related work). This does not invalidate the usefulness of presenting this max reward formulation again to a new community for new reasons, but this paper should be cited and claims of novelty should be reduced. A broader discussion of prior work that looks at non-cumulative objectives like reward at the final state would also be helpful to contextualize the paper.\n2. Clarity of the motivation for new formulation. Intuitively it is true that a generative model may only care about the maximum reward on a trajectory, but there is not a clear and formal decription of the separation between problems that can be represented as max reward versus cumulative reward problems. For example, it is not clear that doing something like using a time dependent reward that only has nonzero reward at the last state in a trajectory cannot capture most of the relevant generative problems. A more formal description of the sorts of problems that are not representable as cumulative reward problems would be useful.\n3. The toy example problem leaves some unanswered questions. Specifically, the use of a non-markovian problem makes the example somewhat suspect. By using a non-markovian problem, the example is now outside of the setting being discussed in the rest of the paper. It is not clear that the non-markovian nature of the problem is necessary to make the max-Bellman algorithm look better, so I would suggest coming up with a modified example that respects the MDP structure while still showing the utility of max-Bellman.\n4. Description of experimental setting is lacking. It is entirely unclear from reading the paper what exactly the experiments are doing, just that they use the ENAMINE dataset of reactants and several reward functions are cited. The state and action spaces are not defined. The transition dynamics are not defined. And the reward functions are not described or defined in a self-contained way. All of this information should be provided (in the appendix, no need to put it in the main text). Without this information it is difficult to judge how useful the experiments are, especially for someone not intimately familiar with related work from the RL for drug design community (as would be the case for most readers at ICLR).\n5. Presentation of experimental results is unclear. There is not much analysis of each of the figures which leaves some things unclear. For example, in figure 3(a) it appears that MB outperforms cumulative reward at every timestep. This would seem to mean that the MB algorithm in fact gets higher cumulative reward than an algorithm trained to maximize cumulative reward. This would immediately call into question the story about why MB is a useful modification if it actually leads to better performance on the cumulativ reward objective in this specific task. I may be misunderstaning this plot, but this lack of clarity is a problem. As another example, the gaps in table 1 seem relatively small between PGFS and PGFS+MB. A discussion about the scale of the improvement we can expect from MB would be useful to contextualize these results.\n\n--------------------------------------------------------------------\n\n**Recommendation:**\n\nI gave this paper a score of 5 (weak reject). This reflects the fact that I think the paper introduces an interesting problem and clean solution, but does not do a good job connecting to prior work and has a few issues with clarity especially in the experiments. I gave a confidence of 3 primarily because I am not very familiar with the literature on ML/RL for drug design so I cannot precisely guage the potential impact of the paper on that subfield. \n\nI am willing to increase my score if the authors provide a more comprehensive connection to prior work and improve the clarity and experiments section based on the weaknesses listed above.\n\n--------------------------------------------------------------------\n\n**Questions for the authors:**\n\n1. Is there potentially a connection between the proposed maximum reward formulation (especially for chemical synthesis) and the learning to search approach to structured prediction problems (see e.g. [2])?\n2. Is there a connection between the proposed maximum reward formulation and optimal stopping problems?\n\n--------------------------------------------------------------------\n\n**Additional feedback:**\n\nTypos:\n\n- The last paragraph on page 1 is not grammatically correct. Each phrase should be formulated like \"symbolic regression which is interested in\" instead of \"symbolic regession is interested in\".\n- In section 2.1 it should be \"RL algorithms easily generalize across states\" instead of \"the RL algorithms are easily generalized across states\"\n- In the leftmost column on table 2, the PGFS score of 7.81 ought to be bolded as well since it is equal to the score of PGFS+MB.\n\n\n\n[1] Quah, Kian Hong, and Chai Quek. \"Maximum reward reinforcement learning: A non-cumulative reward criterion.\" *Expert Systems with Applications* 31, no. 2 (2006): 351-359.\n\n[2] Chang, Kai-Wei, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. \"Learning to search better than your teacher.\" In *International Conference on Machine Learning*, pp. 2058-2066. PMLR, 2015.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but clarity and experiments can be improved",
            "review": "Summary: \n\nThis paper proposes a new reinforcement algorithm based on max-Bellman operator which trains the policy to optimize the maximum reward achieved in a trajectory, i.e., $R(\\tau) = \\max_{t\\geq 0} \\gamma^{t}r_{t}$. The authors analyze how the newly proposed max-Bellman operator leads to an optimal policy. Experiments on a toy task and de novo drug design tasks show better performance compared to the considered baselines. I think the proposed idea is promising, timely, and impactful. However, I think (a) the problem setting should be clarified and (b) the empirical evaluations are below the standard of ICLR conference. Especially, regarding (b), the experiments only compare with the Bellman operator under MDP with cumulative rewards that do not align with the true objective. However, researchers already know how to design MDPs with cumulative rewards aligning with the true objective and this paper should consider this as their most important baseline.\n\nPros:\n- This paper tackles an important class of problems, where the agent aims to identify states (or solutions) that achieve the highest score. Examples include drug discovery, (mathematical) combinatorial optimization problems, and program synthesis. \n- The proposed algorithm is fundamental and can be extended to many problems. \n\nCons:\n- This paper confusingly use the terminology \"reward function\" to indicate two meaning. First, it is used as a reward function associated with a Markov decision process (MDP) (taking $s_{t}$ and $a_{t}$ as inputs), where the agent is trained to optimize the cumulative summation or maximization of rewards over the trajectory. Second, it is used as a score function (taking molecule as input) that is associated with the given problem and independent of MDP (used for solving the problem). The distinction between two functions is very important and existing methods often shape rewards different from the scoring criteria. \n- In the experiments, the authors only compare the Bellman and max-Bellman operators defined on an identical MDP. Especially, the MDP is designed so that only the max-Bellman operator aligns with optimization of the true objective of the problem. This dismisses how one can also design MDP so that the Bellman operator can optimize the true objective of the problem. \\\nIn drug design, [You et al. 2018, Shi et al., 2020] consider assigning the desired score of the drug as the reward only at the terminal state of the MDP. This allows the Bellman operator to properly solve the drug design problem without any modification. I note how this approach is briefly mentioned in the introduction. Authors claim that they fail to optimize for the very high reward molecules that may be encountered in the middle of the episode. While I partly agree with such a claim, this should be empirically verified to prove the empirical superiority of the max-Bellman operator over the Bellman operator. \\\nIn combinatorial optimization and program synthesis, e.g., [Chen et al. 2019], it is common to assign the difference of scores between intermediate states, i.e., $r_{t} = c_{t}- c_{t-1}$ where $c_{t}$ is the true objective of the problem evaluated at state $s_{t}$. This also alleviates the mismatch between the training of RL and the true objective of the problem.\n- For the drug design experiment, I also suggest the authors provide a comparison on the maximum reward per episode during training, e.g., Figure 4 (b), to ablate the effect of the best max-Bellman operator. \n- In molecule generation tasks, it is common to provide an illustration of the generated molecules to check whether if they are indeed helpful and synthesizable in real-life. This aspect is especially important since PGFS was initially proposed to constrain the drug design over molecules with a synthesizable structure. \n- Given the fundamental nature of the proposed algorithms, I encourage the authors to provide more demonstrations on the superiority of the max-Bellman operator. Especially, I think it is important to compare with settings where the agents receive rewards at end of the episode, e.g., [You et al. 2018]. This paper only compares with the case where the cumulative summation of score function is maximized.\n- In (No Ad, RT) column of Table 2, PGFS+MB is marked bold even though it achieves the same score as PGFS.\n\n[You et al. 2018] Graph convolutional policy network for goal-directed molecular graph generation\n\n[Chen et al. 2019] Learning to Perform Local Rewriting for Combinatorial Optimization\n\n[Shi et al. 2020] GraphAF: a flow-based autoregressive model for molecular graph generation\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}