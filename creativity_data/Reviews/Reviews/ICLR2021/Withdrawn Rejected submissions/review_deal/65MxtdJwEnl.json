{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a method for improving the learning of neural controlled differential equation (CDE) models. Neural CDE models provide a number of advantages over neural ODE models in terms of their ability to incorporate continuous-time observations. The primary strength of this paper is that it proposes a mathematically rigorous approach to enable neural CDE models to be learned more efficiently from long time series by converting the CDE to an ODE via the log-ODE method. The results are promising in that the method is able to simultaneously improve accuracy, reduce running time and reduce memory required during learning. \n\nThe paper has two main weaknesses. First, the authors claim that due to the problems they are solving (time series with up to 17,000 steps), there are no viable baselines outside of the family of methods that they are proposing. As was noted in the reviews, it would be advisable to consider even very basic baselines for these experiments in addition to current benchmark results. For example, the EigenWorms data set was used in the time series classification benchmark described in Bagnall et al. and there are benchmark results available that appear to outperform those shown in Table 2 (see mean test accuracy results reported here: http://www.timeseriesclassification.com/results/AllAccuracies.zip). The authors are also encouraged to consider even coarse RNN approximations such as partitioning the time series into tractable blocks for learning. It is not clear that the data sets actually have long-range dependencies despite being long. \n\nThe second weakness is that the representation that underlies the log-ODE method (the log-signature transform) has been used in previous work in conjunction with discrete-time RNNs. It can be viewed as a preprocessing method in a sense, as was noted by a reviewer. However, it is much more fundamentally integrated with methods for solving CDE's than its prior application to RNNs indicates. \n\nOverall,  support for the paper did not rise to the bar required for acceptance, but we encourage the authors to revise and re-submit the work to a future venue. \n"
    },
    "Reviews": [
        {
            "title": "Review of the \"CDEs & the Log-ODE Method\" for ICLR 2021",
            "review": "**Summary.** The authors describe how to apply a log signature to temporal datasets. This operation reduces dimensionality along the time axis at the price of adding some dimensionality to the spatial dimension. Then they train a neural controlled differential equation (Neural CDE) on the transformed dataset and show that their model learns more quickly and achieves better test generalization. They report results on two real-world datasets (EigenWorms and the TSR vitals dataset).\n\n**Strong points.** This paper is technically sound and the method shows clear improvement over “no preprocessing.”\n\n**Weak points.** The authors are proposing a simple method of reparameterizing time series data so that information is transferred from the time dimension to the space dimension. Strangely, the title “Neural CDEs for Long Time Series via the Log-ODE Method” implies that they are proposing a new model. They are not. Rather, they are proposing a data preprocessing technique that they apply to a dataset _before_ they train an ML model on it. It’s also worth noting that this feature engineering technique is already being used on time series data (eg Liao, 2019, “Learning Stochastic…”). The only difference is that here the authors are using a continuous-time analogue of an RNN. But since this feature engineering trick is applied to the dataset independently of what model is used, I’m not sure what new scientific insights are to be gained. This work seems very similar to Liao et al (2019); compare Figure 1 in the two papers, for example. To the authors: am I missing a critical new contribution? In what ways is the “log signature” feature engineering trick significantly different in the context of CDEs compared to RNNs?\n\nAnother issue with this paper is that it presents the theory in an unnecessarily mathematical manner. Some math is useful. But whenever the authors introduce terms that an ML audience is not familiar with, they should offer a sentence of intuition regarding what that term means. From my notes on this paper, here are phrases that I found confusing and were not accompanied by any explanation or intuition: “Riemann–Stieltjes integral,” “n-fold iterated integral,” “tree-like equivalence,” “Holder continuous paths,” “Mobius function,” “Lie brackets whose foliage is a Lindon word,” and “Magnus expansion.” Again, it’s ok to introduce these concepts, but 1) you should give some reasonable intuition for what they are and why they are relevant and 2) they should be directly relevant to the main contributions of the paper.\n\nFor other reviewers/readers who want an intuitive introduction to log signatures, I thought that [this README](https://github.com/kormilitzin/the-signature-method-in-machine-learning) did a very nice job. To the authors: are there ways that we can simplify the theory section so that it remains technically correct while also being readable and accessible? Are there specific reasons that you chose to introduce the \"log signature” operation in the way you did?\n\nThe experiments are technically sound and the results are presented well. However, it is surprising that the authors did not compare to the RNN/RNN model that they mentioned in the second paragraph of related work. That would seem to be the natural baseline; using the non-preprocessed dataset is a rather trivial baseline, as the performance of recurrent models degrades quite seriously across such long sequences.\n\n**Recommendation.** 4: Ok but not good enough - rejection\n\n**Reasoning.** The Log Signature method has already been shown to be a viable preprocessing technique for time series data. The main contribution of this paper is to show that it also works with CDEs. Since the log signature method is model-independent, it does not seem surprising that this is the case. The experimental results are technically sound but would be improved by adding a stronger baseline. It would make more sense to compare to the RNN/RNN model described in the Related Work section, or to another method of data preprocessing that reduces dimensionality along the time axis.\n\n**To improve the paper.** When using a mathematical concept that will be new to an ML audience, give a one-sentence intuition for what it does and how it is relevant. Try and give a simple, intuitive example of how one might apply the log signature to a short, example time series. Make sure the reader is able to quickly grasp how a log signature of, say, order 2, works in practice. You might consider transitioning to a deeper theoretical treatment after doing so, or possibly refer the reader to a relevant tutorial such as (Chevyrev and Kormilitzina, 2016).\n\nOne way to improve the experimental results would be to compare to a stronger baseline such as RNN/RNN. If you don't want to compare to RNN/RNN, you could even just compare to a simple tensor reshape operation that folds the time dimension into the space dimension (eg., given a dataset with axes [\"num_examples\",\"time\",\"space] = [a,b * c,d], reshape to [a,b,c * d]). There are probably more clever and effective approaches than that. But the point is that, instead of comparing your dataset preprocessing method to \"no dataset preprocessing at all\", try comparing it to \"another dataset preprocessing method\" that does something similar. In this case, that method would involve transferring some of the dataset dimensionality from time to space).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The training time of neural CDEs is reduced by replacing the original input time series with corresponding log-signature.",
            "review": "Summarizing the paper claims\n------------------------------------------\nThe paper introduces an approach that allows training Neural Controlled Differential Equations (CDEs) for long time series. In contrast to the Neural ODE that is determined to its initial condition, Neural CDE produces a trajectory dependent on time-varying data. The authors propose to use log-signature as input instead of the original time series. Log-signature can be understood as a lossy representation for time series, which has a much smaller length and varies slower over the same time interval. Hence, larger steps may be used in the numerical solver, and that yields to the training speed-up.\n\nIn a little more detail, the log-signature is constructed as a sequence of statistics computed using the original time series. It is characterized by the depth N, where N states for the maximal length of paths used to calculate statistics. By applying log-signature transform, the solution of the CDE may be approximated by the solution of the ODE.\n\nStrong points\n-------------------\n- The paper is clearly written.\n- The experiments are conducted for four real-wold problems (worms classification, predicting a person's heart rate/respiratory rate/oxygen saturation). \n- The authors provide an ablation study for the (log-signature length) / (number of channels in time series) trade-off,  investigate an influence of log-signature depth and solver's number of steps to the performance. \n- The paper states the limitations of the proposed method.\n\nWeak points\n-----------------\nThe paper provides an overview of many RNN based models used for long time series; however, it doesn't compare with any of them. That would be interesting to see how good is a proposed method comparing to the RNN based one in terms of test accuracy/training time/memory usage.\n\nRecommendation (accept or reject)\n------------------------------------------------\nI recommend accepting the paper. The paper provides a detailed theoretical formulation and demonstrates a significant training time speed-up for various real-world problems.\n\nUpdate: The authors thoroughly addressed all the questions, the experiments demonstrate an improvement, the theory coincides with experiments. From my perspective, that would be useful for the neural ODE community to know more about the proposed log-signature-based technique. I increase the score. \n\nQuestions\n--------------\n- Which type of ODE solvers has been used for the experiments? Does the solver's order influence the neural CDE performance?\n- What order is the value of  $\\beta(v, N)$ in the experiments?\n- In section 4.1, it is written that the naive subsampling achieves speed-up without performance improvements. Could you provide time and accuracy for these experiments?\n-  What architecture is used for $\\hat{f}$ neural network? How the choice of architecture affects performance?\n- The standard deviation in Table 1 has quite large values. Haven't you tried to tune training hyperparameters to reduce it?\n- What is the stopping criterion for the training? I'm curious why in Table 1, for the same number of solver's steps, the training time for $NCDE_2$ is longer than for  $NCDE_3$? (It seems that for the same number of training epochs, the time should be shorter because we do less preprocessing computations for log-transformation) \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Seems novel and motivated, but should be more approachable.",
            "review": "### **Summary and Contributions of Paper**\nThis paper proposes a new method for computing Neural CDEs via the signature transform, which transforms a path integral into log signatures, i.e. a collection of iterated integrals. Then standard ODE tools are applied to each piecewise log signature.\n\n### **Strengths**\n- The writing quality is rigorous.\n- The approach seems motivated and based on a clever mathematical trick via the signature transform.\n- Experiments are convincing and sound.\n- Appendix provides proof of the approximation properties of the clipped-term signature transform (which originally requires infinite basis for exact approximation)\n\n### **Weaknesses**\n- The signature transform seems somewhat esoteric and nonstandard for readers without specific knowledge in this field. It would be very good if the authors could give more intuitive/pictorial views of this transform (I needed to read online surveys multiple times to understand the intuition behind this). For instance, I read this in detail: https://arxiv.org/pdf/1905.08494.pdf (NeurIPS 2019), which provides a much cleaner explanation of the signature transform, but also demonstrates that an entire paper is needed to simply explain the method. \n- While the authors claim that there is an ease of implementation via pre-existing tools, the larger bottleneck seems to be actually understanding the method itself (which seems to also be a function of how the paper treats this material). While I have no doubt that this work would be great for a very mathematically minded community, I am unsure of its merits for the ICLR conference community. I think the authors should provide more high level overview of the signature transform, and keep the strict math in the appendix.\n\nI am not an expert on these types of methods, so my confidence will not be as high, but I believe that this paper contributes via its insight with the signature transform, and thus my rating is marginally above the acceptance threshold.\n\nIf the authors could perhaps make the work more approachable, I would be happy to raise my score.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}