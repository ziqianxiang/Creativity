{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "As several reviewers pointed out, the contribution is  too incremental from previous work."
    },
    "Reviews": [
        {
            "title": "Interesting study of learned neural features, with premature conclusion due to experiments on limited data",
            "review": "Motivated by the need to assess a classifier's confidence on its decision,\nthe paper proposes to use predicted likelihood of the learned features\nof an unknown sample, and shows that samples with low feature\nlikelihood tend to receive a wrong decision.\nFurthermore, the paper argues that using a GMM do model the feature\ndistributions is better than other methods like VAE or AR flow.\n\nThis study is interesting in the sence that it attempts to analyze the\ndistributions of the learned features, which is much needed for better\nunderstanding of what a deep neural net attempts to do.\nThe findings appear to make good sense as the network training process\nis designed to push samples of like-class towards tight clusters when\nmapped to the learned feature space.  Those lying in the outskirts signal\ndifficulties in this process,  therefore they are likely to get\nerroneous decisions.   The feature distributions are modeled\nclass-blind, so that the prediction can be applied to unseen cases\nwithout class labels.\n\nThe work can be improved by performing this analysis at different stages\nof a deep network,  as one expects that the levels closer to the\noutput layer demonstrate more of such clustering effect.  It\nwill be useful to confirm with this analysis.\n\nOther than showing numerical results, it will be more convincing if\nexample images are shown that are identified by this method to\nhave unreliable decisions by the classifier, and what error the classifier\nmakes on them.\n\nA more significant weakness is that the experiments are done only with\nan image classification problem, and a single dataset.  This makes the\nconclusion somewhat premature,  as images of physical objects tend to\nform good patterns.  Is the GMM estimation good just because the\ndata happen to be well clustered?  Will this conclusion be confirmed\nby classification tasks on other types of data, e.g. text?\nWhat if the class labels are scrambled?\nWhat makes some network settings better than others in learning\nsuch confidence-suggestive features?\n\nMisc.:\n\np.1, line 4 from bottom, \n\"... capture the distribution of the learned feature space\" ->\n\"capture the class-conditional distributions in the learned feature space\"\n\np.4, line 5,\n\"... state-of-the-art models assigning higher likelihoods to samples\n... \" ->\nWhat kind of models does this refer to?  What is the model supposed to\ndo?  Why do they assign higher likelihoods to out-of-distribution samples?\nThere is a lot to be filled in here; citing an external reference is\nnot enough.\n\np.6, line 7 from bottom,\n\"... assigned higher BPDs ...\", please expand the acronym BPD at its\nfirst mention.\n\np.8, conclusion,\n\"... verified that features extracted from inputs consistently lie\noutside of the training distribution and can be detected by their low\npredicted log-probability.\"   This sentence is garbled.  What have\nyou verified?  Do you mean to say this for a special type of input?\nWhat consistently lie outside of what?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak recommendation to accept",
            "review": "The authors provide a new method for detecting when deep networks are likely to fail and demonstrate through extensive experimentation its accuracy against generalization errors, out of distribution samples and adversarial attacks. The method builds on prior Mahalanobis metric of (Kimin Lee, et al., A unified framework for detecting out-of-distribution and adversarial samples. 2018) in two respects. First the authors use a single GMM fit to the model parameters that is class agnostic rather than a set of GMMs for each class, thus making it suitable for application in semi-supervised datasets. Second, the authors show ability to detect instances from a test set that are likely to cause a misclassification due to a failure to generalize. Surprisingly, the proposed approach performs better in most cases than the prior Mahalanobis approach even though it requires less information (no labels). \n\nPros:\n\n1. Well written and organized paper.\n2. More general and simpler approach than prior art\n3. Extensive empirical results that are competitive with or improved over prior art.\n\nCons:\n\n1. This paper is very similar to and provides only a relatively small incremental improvement over prior art (Lee, et al.)\n2. Like the Mahalanobis method, the proposed hypothesis testing method requires fitting a GMM to an \"incorrect\" distribution. Please make it clear what this distribution is for the experiments. It seems this would require knowledge of the type of attack, or the out-distribution making it non-blind and unrealistic for real-world out-of-distribution or adversarial attack scenarios.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review",
            "review": "**Update after rebuttal:** The author rebuttal clarified some minor issues for me, but it did nothing to address my main concern, which is that very similar methods have been proposed before. I'm therefore keeping my score the same.  \n\n---------------------------------------------\nThis paper proposes a simple method for out-of-distribution detection. The basic idea is to fit a GMM to training examples in the feature space (as opposed to the pixel space). The experiments are generally rigorous and well executed, however my main problem with the paper is that it seems a bit too incremental to justify another paper. As the authors correctly point out very similar methods have already been proposed before (Zheng & Hong, 2018; Lee et al., 2018). \n\nOn page 2, the authors claim “Zheng & Hong (2018) and Lee et al. (2018) train a conditional generative model on the feature space learned by the classifier and derive a confidence score based on the Mahalanobis distance between a test sample and its predicted class representation”. However, this is not correct, Zheng and Hong (2018) don’t use the Mahalanobis metric. They use the exact same likelihood-based criterion in this paper, except they do this on a class-conditional basis as the authors correctly point out, but that seems like a small difference (as labels will obviously be available for the training data in a supervised setting, and the method can be easily adapted to the semi-supervised setting with some tweaks). I just don't see how this small difference could have a huge effect on performance.\n\nThe authors do have some comparisons with these earlier methods, but I’m wondering if these comparisons are done fairly: for example Zheng and Hong (2018) have a similar K parameter in their GMM model, what is the value used for that parameter in this paper? Is it the same as the one used in the class-agnostic GMMs? Have you tried tuning that parameter for these earlier models? How was the threshold parameter chosen? As far as I can see, these important experimental details are not discussed at all anywhere in the paper. Similarly, the semi-supervised setting in Figure 5c is not explained at all. How exactly does it work for the different models shown in that figure? Also, why does the Mahalanobis metric seem to work much better than the GMM in Figure 5b?\n\nAnother question I have is whether the authors have tried using features other than the final embedding layer features (and possibly a combination of features from multiple layers: something like this was done before in the deep k-nn paper by Papernot & McDaniel: https://arxiv.org/abs/1803.04765).\n\nMore minor comments:\n\nPage 6: please make sure you mention what BPD means (binned probability distribution), I don’t think this is as commonly known as CDF or PDF.\n\nTypos: “Aims at modeling confidence score that are” (p. 2), “task of detection out of distribution samples” (p. 7).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting observation but not enough",
            "review": "The main novel contribution is that the authors use generative models for several tasks but on the feature space and not the input pixel space. They show interesting behavior that mistakes tend to have low density in feature space, which isn't obvious as the NN that computes the features can map them wrongly to a high likelihood location.\nHowever, the authors claims of usefulness is not properly demonstrated in the experiences.\n\n- The authors claim several times that they perform \"rigorous hypothesis testing\" (including the abstract and conclusions so not a minor point in the paper) but it doesn't really mean that the mistakes can be identified as one can understand from the text. It just means that the distributions are distinct (but can have large overlap as can be seen in fig.2)\n- While GMM on feature space are better then other models on feature space and pixel space, it isn't convincing that the error detection can be of any use. \n- Adversarial detection was only tested on adversaries that do not try to fool the detector. As was shown in \"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods\" by Carlini&Wagner this can lead to wrong detection claims.\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}