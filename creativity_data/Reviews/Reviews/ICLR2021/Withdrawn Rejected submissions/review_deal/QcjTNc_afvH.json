{
    "Decision": "",
    "Reviews": [
        {
            "title": "Methodological contributions are unclear",
            "review": "The paper deals with the problem of image editing. The editing is done by blending two images in the latent space by using a segmentation masks outlining which parts to take from each image. The main contribution of the papers are StyleMaps - topological feature maps which are used to perform image blending. Results are reported on CelebAHQ and AFHQ datasets, showing the benefits of the StyleMaps for image generation, image editing and image reconstruction.\n\nPros:\nThe paper addresses an interesting problem of image editing.\nThe validation of the approach is performed on multiple datasets.\n\nCons:\nThe presentation of the paper could be improved.\nThe methodological contributions are unclear.\n\nDetailed review and clarification questions.\n\n- The connection between image generation and image editing is a bit unclear to me from the current paper presentation. For example, why do we need to validate the generative approach if the focus is on image editing and the image editing approach does not seem to have any stochasticity in the model - it seems to be a deterministic model that outputs a blended image given two images and a blending mask. It would be fine to just present the description of the image editing pipeline (fig 2.). Could the authors clarify the connections between the three tasks discussed in the paper: generation, image editing and reconstruction?\n\n- Is topological style map really a new thing? In deterministic models, the latent variables in the model oftentimes have topological structure e.g. UNet architecture. In the context of generative models, the topological latents have been explored already in http://papers.nips.cc/paper/6141-an-architecture-for-deep-hierarchical-generative-models.pdf and https://arxiv.org/abs/1606.04934. Could the authors comment on the novelty aspect? \n\n- Could the authors add the numbers of parameters next to each model under comparison? Does the proposed approach contain more parameters than the previous ones? Without such information, it is difficult to fully interpret the reported results.\n\n- Could the authors comment on the importance of each loss component? Adding an ablation study would be beneficial.\n\n- The paper lacks discussion on the limitations of the methods. Could the authors comment on the potential limitations of the suggested approach?\n\n- Results: There is a large quantity of results and visualization in the paper. However, I find the discussion of the results not detailed enough, especially, for the image editing experiments. I would recommend moving some visualizations of the results to the appendix and expanding the discussion of the results in the main body of the paper. \n\n- Could the authors comment on why AP (for real vs fake images) is the right metric for image editing task?  Could we use alternative metrics here, e.g. FID or classification accuracy score (https://arxiv.org/abs/1905.10887v1)?\n\n\"...since the GAN itself lacks an inverse mapping from an image back to its corresponding latent code.\" There is some previous work aiming to learn inference models in GANs, see for example https://arxiv.org/pdf/1606.00704.pdf and https://arxiv.org/abs/1605.09782. \n\nAdditional comments:\n\n\"In addition, we remove per-pixel noise which is an extra source of variation because it makes the projection complicated. Instead, \\beta plays the similar role.\" This is a bit unclear to me. Could the authors clarify this part?\n\nFig 1.: style map and style vectors have different dimensionalities. Same applies to latent code z. What is the effect of these values on FID scores?\n\nIt would be nice to have appendix A in the main body of the paper. Some visualizations could be moved to appendix.\n\n\"we choose the 8 \\times 8 resolution as our best model and use it consistently for all subsequent experiments\", by looking at the Table 1 8 \\times 8 maps are not among the top performing ones. Could the authors comment on this?\n\n\n\nOverall, I acknowledge the importance of the problem tackled in this paper; however, the contributions are unclear, the presentation could be improved and there are some important references missing (see above).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for: A STYLEMAP-BASED GENERATOR FOR REAL-TIME IMAGE PROJECTION AND LOCAL EDITING",
            "review": "Summary: In order to use GANs for image manipulation, one either needs a GAN-with-an-encoder variant or one must train an encoder afterwards, e.g. using an optimization process with SGD on the generator. This paper proposes to include spatial information in the latent representation of a StyleGAN, which helps to train an encoder simultaneously with the generator, in order to manipulate each region with more precision and less side effects.\n##########################################################################\n\nReasons for score: \n\nThe approach has merits and the method might be valuable, but I had multiple questions about whether the results are as strong as the authors suggest. If my concerns about the results are addressed appropriately, I am willing to improve my score.\n\n##########################################################################\nPros:\n- Real-time encoding of input images while retaining high output quality has not been conclusively solved yet, and the method seems to improve in this direction.\n- The qualitative results seem promising.\n- Some of the quantitative results indicate state-of-the-art results.\n- The approach appears novel and interesting, though not a major departure from previous works. The closest work seems to be SEAN but it requires semantic masks during training, while the proposed approach does not.\n- The presentation is mostly clear and there are multiple relevant experiments.\n\n##########################################################################\nCons:\n- Overall, the evaluation of generative models has to take into account a lot of factors, since there are various trade-offs that can take place. One is whether the actual style-mixing (i.e. combination of 2+ latent inputs) capability is conserved in your proposed architecture. I see no comparison of this aspect?\n- It is not clear to me whether the FID and FID_lerp results are reliable when one uses 500 samples only. Previous works usually use 10 000 or 50 000 samples. Dispersion is not given at all by the authors. I appreciate that projecting 3000 or 10000 samples takes a lot of time, but if it is not feasible, then you just cannot use this metric to begin with, right? A simple experiment you could do quickly to refute my point: now that you have your 500-sample for each baseline, then take a subsample of 400 and 450 separately for each. Do the FID results remain the same, relative to each other? If not, then your sample size of 500 is not informative. If they do, then I stand (probably) corrected.\n- Results seem rather dependent on the size of the stylemap. In Fig 3, 8x8 is said to be the only good choice. Yet, the numeric results in table 1 are much worse for the 8x8 than for the boldened 32x32 case. I grant that many of the results at 8x8 are still better than vanilla StyleGAN, but to me it seems like the authors are using different variants of the model when measuring different things, while not explicitly justifying this in the text.\n- In the tables 2-3 that follow, it is not clear which stylemap size is being used. I find no mention of this.\n- Prior work on this field is wider than covered here; there are recent encoder-based approaches for style modulation based decoders e.g. [1] which also have very fast encoding (no iterations), some of which could be natural baselines here. At the very least, it would seem relevant to mention them unless the authors can justify otherwise.\n\n##########################################################################\nQuestions during rebuttal period: \n\n- The architecture appears somewhat bigger than regular StyleGAN. What is the number of parameters in comparison to StyleGAN? If the difference is large, then this raises a question, whether the comparison to vanilla StyleGAN is fair in this regard. (I realize the major difference in the speed of encoding remains.)\n- You downgraded the resolution of StyleGAN's maximum throughput (1024p) to 256x256. Was the StyleGAN CelebA-HQ network trained on 1024p or 256x256? If it is trained at higher resolution, it is not clear if the comparison is fully fair in Table 2.\n- Do you compare LPIPS on the full image instead of center region only, and if you do it on the full image, can you comment on the suitability of that approach as opposed to comparing to the center region only? The focus on center region is used e.g. in Perceptual Path Length (at least in Karras et al. 2018) calculations and elsewhere to remove the effects of the backround.\n- The early StyleGAN projection scripts require specifying learning rates and the number of iterations per picture. What are the values of those parameters here? (Low LR + a lot of iteration improve considerably the StyleGAN projections)\n- The authors do not really compare to any of the established GAN-with-encoder setups. Given that we now allow an encoder, decoder and a discriminator, one would presume the first baseline to be one of the known GAN-with-encoder variants such as VAE-GAN [2], simply extended to use the StyleGAN decoder instead of the traditional decoder. Can you justify not doing this kind of comparisons? The rationale is to determine whether the quantitative gains (e.g. in LPIPS) arise from stylemaps or simply from the setup  of training the encoder in unison with the other networks.\n\n[1] Stanislav Pidhorskyi, Donald Adjeroh, Gianfranco Doretto. Adversarial Latent Autoencoders. arXiv preprint arXiv:2004.04467, 2020.\n[2] A. Larsen, S. Kaae Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. In International Conference on Machine Learning (ICML), pages 1558–1566, 2016.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The results are good, but more comparative experiments are needed.",
            "review": "Summary: This paper presents StyleMapGAN, which adopts stylemap as the representation of latent space.  Benefits from spatial dimensions of this representation, the jointly trained encoder can provide the high-fidelity real-time projection. In addition, their method shows high-quality local editing results superior to current state-of-the-art models.\n\nThe paper is well written with clear diagrams for the overall architecture. The idea is novel and seems to be relatively effective in practice. Other than that, I have the following questions and remarks:\n\n1. In Figure 3 (8x8), there is still a visible border near the nose. Does the method proposed in the paper only apply to the perfect mask? Can it perfectly edit only a small part of the face?\n2. From my point of view, only the blending application is really effective. The result of embedding and interpolation does not show sufficient superiority over Image2StyleGAN. On the blending task, the author should add some comparisons with Poisson Blending and Image2StyleGAN++.\n3. Does the proposed method always need a reference image? Since two images may not be aligned, how to edit the facial components like eyes and mouth? There should be some examples of unaligned transplantation on human faces. \n4. Why not compare with StyleGAN2 on the FFHQ dataset. \n5. Typo : and and in page 7\n\nOverall, my rating lies between borderline and weak accept. My rating might be higher if the above questions are resolved properly.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Misleading terminology and unclear experiments",
            "review": "Summary: This paper proposes a new method for image generation and editing. A smaller network generates multi-scale feature maps, which are used to modulate (affine transform) the features of a larger network in corresponding scales. Besides, an encoder is trained to project real images into the lowest-resolution feature map of the smaller generative network. At test time, the model can perform efficient and faithful reconstruction and spatial editing similar to image composition.\n\nStrengths:\n+ The paper is overall well-written.\n+ The idea of encoding real images into spatial feature maps, instead of non-spatial feature vectors like many prior works do, is interesting and well-motivated. \n+ Experiments show that the method performs better than several baselines on image editing in both quality and speed.\n\nMajor weaknesses:\n- I do not think the proposed intermediate feature map should be called “StyleMap”. The image “style”, sometimes used interchangeably with the term ”texture”, is usually represented by the patch distribution/statistics within an image [2, 3, 4]. It is by definition spatially-invariant. It is therefore very inappropriate to call the intermediate feature map “StyleMap” because they are spatially-variant. In fact, I do not see any major difference between the “StyleMap” and the commonly used intermediate feature map. Is there any evidence showing the “StyleMap” encodes style but not the content? What is the difference between the proposed generator with a normal generator with two branches and skip connections? Is there anything more “style” in the proposed generator? The only difference I see is that the normal skip connection only has addition/concatenation but the skip connection here has multiplicative terms in the modulation layer. This is related to the gating mechanism but has nothing to do with style. In summary, I strongly discourage the use of the word “StyleMap”, which is a misleading and unnecessarily fancy term in my opinion, and advocate to change it to something plain and faithful such as \"feature map\". \n- In StyleGAN, they show that different levels of styles can be mixed to create an output image from the high-level style of one image and the low-level style of another. Does the proposed method support such manipulation? Note that StyleGAN2 reports such mixing is no longer feasible if instance normalization is removed because the style injection becomes cumulative rather than scale-specific. This paper replaces instance normalization with layer normalization, but my intuition is that layer normalization will make the style injection cumulative as well. Also note that this is different from the spatial mixing experiments conducted in the paper.\n- It is a natural and well-motivated idea to encode a real image into spatial feature maps rather than style vectors, in order to better reconstruct it. However, I doubt if it is necessary to change the StyleGAN architecture to achieve this. StyleGAN already has inputs that represent spatial details, which is the noise space. Image2StyleGAN++ [8], which is not cited here, already explores this idea by optimizing the noise space. The proposed method is arguably faster because it's feedforward rather than optimization-based. But the basic idea remains the same. I suspect that StyleGAN2 with feedforward encoder into both style space and noise space could perform similarly to the proposed approach.\n- The paper only compares StyleMapGAN with StyleGAN2 on CelebA-HQ and AFHQ. Neither of them was used in the original StyleGAN2 paper. Is there a reason why the paper doesn’t perform experiments on the datasets StyleGAN2 was originally trained on (e.g., FFHQ, LSUN)? Plus, AFHQ is a limited benchmark for the unconditional generation due to the small sample size. Data augmentation [1] should be able to improve the results of both the baseline and the proposed method considerably.\n\nMinor questions/problems that do not affect my score:\n- The localized editing task is very similar to Shocher et al. [5] but there is no comparison with them. Some related work [6, 7] is not discussed or compared with as well.\n- StyleGAN2 suggests that, in modulation, only scaling is important (shifting is not needed). Do the authors find shifting useful for their architecture?\n- state-of-the art -> state-of-the-art\n\nOverall, I vote for rejection because the terminology is misleading to me, and the experiments leave a lot of things unclear. \n\n\n[1] Karras et al. \"Training generative adversarial networks with limited data.\" NeurIPS 2020\n\n[2] Portill and Eero. \"A parametric texture model based on joint statistics of complex wavelet coefficients.\" IJCV 2000\n\n[3] Gatys et al. \"Image style transfer using convolutional neural networks.\" CVPR 2016\n\n[4] Huang and Belongie. \"Arbitrary style transfer in real-time with adaptive instance normalization.\" ICCV 2017\n\n[5] Shocher et al. \"Semantic Pyramid for Image Generation.\" CVPR 2020.\n\n[6] Pidhorsky et al. \"Adversarial Latent Autoencoders.\" CVPR 2020.\n\n[7] Park et al. \"Swapping Autoencoder for Deep Image Manipulation.\" NeurIPS 2020\n\n[8] Abdal et al. \"Image2StyleGAN++: How to Edit the Embedded Images?.\" CVPR 2020",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}