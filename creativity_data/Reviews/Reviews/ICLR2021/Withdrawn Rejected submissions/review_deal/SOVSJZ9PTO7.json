{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all reviewers (including positive reviewer) have concerns on unconvincing experimental results (due to missing baselines for instance). I basically agree on negative reviews that this submission fails to have enough quality considering the high standard of ICLR."
    },
    "Reviews": [
        {
            "title": "An interesting paper",
            "review": "This paper presents an approach to jointly pre-train language models and representations for knowledge graphs. In particular, natural language texts (English Wikipedia) are used to train context representations, while knowledge graphs (Wikidata) train entity representations (and both depend on each other). Experiments show that the approach outperforms baseline methods on several natural language understanding tasks: few-shot relation classification, knowledge graph question answering, and entity classification. \n\nThe presented approach looks very promising, however, it also leaves several doubts. One natural ablation question is whether we could simply include the knowledge graph as plain text in training only a language model (via a similar pre-training and fine-tuning)? So, whether the additional hybrid structure involving graph convolution networks is actually necessary? (Maybe the gain in accuracy is only due to the additional information that is available in the knowledge graph?) Another natural question is why the authors have not made further experiments on other datasets and knowledge graphs? Does this approach only work for English Wikipedia and Wikidata (maybe because these two are matching extraordinarily well)? Finally, I would have expected further experimental comparisons to related approaches.      \n\nAfter rebuttal: I'm also still not convinced by the experimental evaluation. For this reason, I slightly downgraded my overall rating.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for JAKET",
            "review": "# Summary\n\nThis paper proposed a new language modeling pretraining method that leverages the knowledge graph information. Specifically, the paper replaces the entity embedding in one hidden layer of BERT context embedding, with the corresponding graph attention embedding that is obtained from the knowledge graph. The pretraining tasks contain not only the language related tasks (like predicting masked tokens), but also the knowledge graph tasks like entity classification or relation type prediction. Experiments on few-shot learning tasks, question answering and entity classification show better performance over other pretraining counterparts. \n\n\n# Pros\n\n- The motivation of combining knowledge graphs and unstructured text data for pretraining is valid. \n- The joint training architecture is interesting. \n- The improvement over other kg+lm or purely lm baselines is consistent. \n\n\n# Cons\n\n- Some of the designs need justification.\n- Some of the baselines/ablations might be missing. \n- The experiments have some flaws.\n\n\n# Details\n\nOverall I lean towards accepting the paper, if my concerns can be properly addressed. \n\nI like the idea of having a joint training method for both the kg embedding and text embedding. Although it encounters additional implementation/computation difficulties, it entangles the two sets of information in a coherent way. Also the pretraining has been done on a large scale, with a reasonable size of knowledge graph as the backbone. I think the paper has demonstrated a nontrivial contribution to the field. \n\nThere are several potential issues with the current paper:\n\n1. It seems the paper sacrifices too much on the efficiency, while having many heuristic approximations/designs that are un-justified. For example, a) how much would the entity embedding update affect the fine-tuning; b) how does the neighborhood sampling affect the knowledge graph embedding in the context of BERT training; \n\n2. It is not sure how each individual pretraining method affects the quality of the embedding. Also for knowledge graphs, a commonly used embedding learning method is the link prediction, where one samples positive/negative <e1, r, e2> triplets for learning. The entity category prediction or relation type prediction is less common. Would it be helpful to do the link prediction instead? \n\n3. An important baseline is missing, where one first pretrain the KG embedding and BERT embedding separately, and concat/merge them with downstream fine-tuning tasks. I’d like to see the results on the three tasks in the experiments. \n\n4. For table 2 it would be nice to include the knowledge graph based approaches that don’t leverage the pretraining, including the pullnet results, or the results that come with original metaQA paper. I don’t get the comment at the bottom of page 8 why approaches like VRN in MetaQA paper are not ‘fair’, as these models actually use less information. \n\n5. For table 1 and 2, the improvement over RoBERTa seems marginal. However for table 3 the gap is huge. Could the authors provide explanations for \n1) why use entity classification tasks? It is somewhat unfair, as the pretraining of the proposed method already involved with the entity classification tasks (although in a transductive setting); Also as I mentioned in 2, link prediction would be a more preferable task for evaluating the quality of the knowledge graph embedding; \nand 2) why is the gap so large, is it due to the issue in 1)? \n\n# Questions \n\nI’d like to hear the answers to my questions above. \n\n# Improvement\n\nI highly encourage the authors to include the necessary baselines in experiments, as suggested above. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "JAKET Review",
            "review": "The paper proposed a pretraining framework for language models and knowledge graphs. As authors mentioned in the paper, many approaches recently focused on this topic, with different variations with respect to entity embedding, initialization, over-parametrization, masked language model, fine-tuning task and etc.\n\nNovelty: The authors mentioned entity embeddings from previous works in this area mainly are computed by external resources and are injected  to the model or they are learned as parameters of the model. What about K-BERT? Doesn’t seem to have these problems. The paper doesn’t provide a comprehensive comparison with K-Bert. I think the novelty of the work may need to be further justified.\nAlso, what is the relation text or description? What kind of description do the relations have? Is it useful to use Roberta for relation embedding initialization?\n\nExperiments: \nThe paper proposes a few baselines and compares the architecture with those baselines. However, there are several other works in this area. Why not comparing with those related approaches?  As an example comparison with K-BERT?\n\nIt would be also interesting to have more baselines without language models (like the GNN in Entity Classification task)? For example, just initialize the knowledge graph by a pretrained language model and evaluate the knowledge graph on questions answering? Or even have a GNN on top of that without language model? Comparison with other knowledge graph embedding approaches on Wikipedia? Also, what if we separately pretrain knowledge graph and language model. How does affect the experiments?\n\nThere is a lack of experiments to show the Effect of number of steps in graph embedding update and also training time?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A joint KG and language pre-training model",
            "review": "This work proposes a method for joint pre-training of knowledge graph and text data which embeds KG entities and relations into shared latent semantic space as entity embeddings from text. The proposed model JAKET consists of two main parts: a language module and a knowledge module. The model is pre-trained on a collection of tasks: entity category prediction, relation type prediction, masked token prediction and masked entity prediction. The proposed framework enables fine-tuning on knowledge graphs which are unseen during pre-training.\n\n\nOverall, I believe that the work on knowledge-enhanced language models to be an interesting and important area of research. However, I believe the paper is not ready for publication in its current form as (i) the demonstrated improvements obtained by pre-training with the added KG module seem minor compared to the computational overhead of having to compute entity and relation embeddings using GNNs; and (ii) experimental comparison to some relevant prior work is missing.\n\nQuestions/comments for the authors:\n\n1. One of the drawbacks of the proposed method is that it assumes entity descriptions to always be available, which might be the case for Wikidata, but it is not usually the case with e.g. standard knowledge graph completion datasets WN18 and FB15k. How would fine-tuning work on knowledge graphs that do not have entity descriptions?\n2. What does M in RoBERTa+GNN+M stand for? Is it memory?\n3. The improvements over a pure language model on few-shot relation classification and KGQA are minor, especially given the computational overhead that adding a KG module entails. The authors should include a discussion on computational overhead of having a KG module vs a pure language model.\n4. The experimental comparison to existing knowledge-enhanced language models from Section 2 is missing.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}