{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a simple modification to Laplace approximation  to improve the quality of uncertainty estimates in neural networks.\n\nThe key idea is to add “uncertainty units” which do not affect the predictions but change the Hessian of the loss landscape, thereby improving the quality of uncertainty estimates. The “uncertainty units” are themselves trained by minimizing a non-Bayesian objective that minimizes variance on in-distribution data and maximizes variance on known out-of-distribution data. Unlike previous work on outlier exposure and prior networks, the known out-of-distribution data is used only post-hoc.\n\nWhile the idea is interesting and intriguing, the reviewers felt that the current version of the paper falls a bit short of the acceptance threshold (see detailed comments by R3 and R2’s concerns about Bayesian justification for this idea). I encourage the authors to revise and resubmit to a different venue.\n"
    },
    "Reviews": [
        {
            "title": "Simple, yet powerful idea to tune uncertainty.",
            "review": "POST DISCUSSION UPDATE\n------------------------------------\nI like the proposed method and I will keep my score.\n\nEND OF UPDATE\n------------------------------------\nThe paper proposes a new method to learn uncertainty under Laplace approximations. The method relies on uncertainty units that do not change the predictions but increase the dimensionality of the parameters and help learn better uncertainty by the proposed loss function.\n\nStrong points:\n\n- The paper proposes an ad-hoc method that can be applied to any MAP trained network.\n- The proposed method is simple, powerful, and not expensive relative to the training time.\n- Good experimental analysis.\n\n\nClearly state your recommendation (accept or reject) with one or two key reasons for this choice.\n- I recommend to accept this paper.\n- This type of work is definitely needed to enable more powerful models that have accurate predictions and better uncertainty estimates with small additional cost.\n\n\nQuestions:\n- What are the hyperparameters that the method is sensitive for? For example, the size of OOD, number of epochs, learning rate,..,etc\n- How was the number of LULA units chosen? Why did you choose 64? Why not try 32 or less?\n- Were the hyperparametrs of LULA tuned on a separate validation set than the one used to tune the uncertainty?\n\nMinor comments:\n- Table 1 is very hard to read. It would be great if the best numbers are in bold as in Table 2.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea with some unclear points",
            "review": "***\nPOST DISCUSSION UPDATE\n***\nI am satisfied with the authors' response and will increase my score to an accept.\n***\nEND OF UPDATE\n***\n\nThe paper proposes an uncertainty estimation method for deep learning. The idea is, building on prior work on Laplace approximations for neural networks, to augment a pre-trained network with additional units such that predictions with point estimates remain unaffected, while the variance may change. The weights of those units are then trained on the validation set/out-of-distribution (OOD) data such as to minimise the variance on the in-distribution data and maximise it on OOD data.\n\nOverall, I think this is a novel and interesting method, which could be useful in practice considering that it builds on pre-trained networks. There are a couple of points which are unclear to me (see below for details) as well as some details regarding the experiments which could be improved, so that I would **not recommend acceptance** at this point, however I believe that these can be rectified over the discussion period.\n\nUnclear points:\n* How exactly are you training the LULA weights considering that the Hessian (and hence the variance of the corresponding weights and therefore the samples in the MC integral in eq 8) depends on them? I assume you are treating the Hessian as constant? Surely differentiating through its computation would be prohibitively expensive considering that it requires a pass over the entire training dataset? Would it be possible to construct an example (e.g. a reduced version of MNIST) where you can differentiate through the Hessian to see if this has much of an impact?\n* I'm somewhat confused by the discussion regarding the closed-form approximations for the predictive distribution around eq. 4. Isn't the point of those to avoid an MC approximation of the integral over the posterior? I understand that computing eq. 5 efficiently for a Laplace approximation over all weights is not generally supported in Pytorch and tensorflow, but wouldn't it be feasible for the last-layer approximation that you use? Or is the whole discussion mainly to motivate the LULA objective in eq. 7?\n* Are you using the multi-class equivalents of eq 4 and 5 for test-time prediction or do you do a MC integral over the approximate posterior? \n\nExperiments:\n* The setup seems slightly unfair to me in that proposed method is the only one (except KFL+OOD) to be tuned on out-of-distribution data. It would be good to see the DPNs included in Tab. 1 & 2.\n* In Tab. 1 it looks like the test predictions from the proposed method are somewhat underconfident. Could you also report test log likelihoods?\n* That being said, I find the evaluation a bit narrow in that it only looks at out-of-distribution detection. Could you add e.g. an evaluation of the robustness to data shift? For example, making the predictions using your trained networks for the corrupted CIFAR10 and CIFAR100 variants from (Hendrycks & Dietterich, 2019) shouldn't be too much trouble. See e.g. (Ovadia et al., 2019) for a recent use of those datasets in the literature. I understand that if you're making predictions using eq. 4 accuracies would be unaffected, but it would still be interesting to see log likelihoods and calibration errors.\n* Would it be possible to use a more recent architecture for CIFAR100? 50% test accuracy seem somewhat subpar e.g. compared to even small Resnets (which get over 70%).\n* Finally, as more of an optional request/suggestion, it would be nice to have some experiment in a domain other than image classification. Considering that you're only implementing inference for the last layer, I would imagine that your method would be compatible with any architecture (e.g. RNN, Transformer) that feeds into a fully-connected output layer? Having more variety in the empirical tasks would strengthen the paper.\n\nReferences:\nHendrycks & Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In ICLR 2019.\nOvadia et al. Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift. In Neurips 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An Interesting Predictive Variance Module in form of Laplacian approximation",
            "review": "#################\nPost-Rebuttal Reviews: Thank the authors for the detailed responses. The proposed approach is an interesting add-up for the Laplacian approximations. However, I think the paper still deserves more works. As far as I am concerned, applying the approach to multi-layers instead of only the output-layer is important. I will keep my score for now.\n\n##################\n\nBayesian deep learning aims to bring uncertainty quantification to modern neural network models. Laplacian approximation, which transforms a trained deterministic neural network into stochastic by a second-order Taylor approximation, is especially appealing due to its pos-hoc nature. However, since the network parameters are trained and fixed, few parameters can be optimized in Laplacian approximation. To enhance the capacity of Laplacian approximation, this paper proposes the *learnable uncertainty units (LULA)*. The LULA units do not affect the network prediction, but the Hessians of the LULA units are nonzero. In consequence, training LULA units increases the capacity of Laplacian approximation by adapting the Hessians. \n\nIn general I think the paper proposes an interesting module for parameterizing the predictive variances. The proposed $v(x)$ in Eq(5) is similar to the expression of the neural tangent kernel (Jacot et. al., ‎2018), thus might be suitable for parameterizing the predictive variance. However, the current version of the paper leaves me several questions regarding the methods. \n\n1. It seems to me that the weight matrix does not affect the prediction as long as it is in the form of $\\tilde{W}^{(l)} = [ W_{map}^{(l)} \\; 0 \\;  \\backslash \\backslash  \\; \\hat{W}_1^{(l)} \\; \\hat{W}_2^{(l)} ]$. However, the proposed method sets $\\hat{W}_2^{(l)}$ as zero. Setting $\\hat{W}_2^{(l)}=0$ blocks the backpropagation through the augmented hidden units, which does not seem reasonable to me. \n2. The approximate Hessian $\\hat{\\Sigma}$ depends on the augmented weights, do you backprop though it when computing the gradients ? \n3. The predictive variance is $J^\\top \\Sigma J$. For multi-output networks like in classification, how do you compute it efficiently ?\n\n**Experiments**\nThis paper conducts experiments for out-of-distribution detection in order to test the performance. However, in my perspective, the experimental results are not strong enough to prove its superiority.\n1. More experiments on uncertainty quantification can be conducted to support the proposed methods. For example, the adversarial example experiment (Ritter et al, 2018) and the calibration experiment (Guo et. al., 2017). These experiments are only my suggestions. I think any experiment regarding to uncertainty quantification would be helpful.\n2. The paper uses Laplacian approximation only for the last layer, and all previous layers act as feature extractor. In this scenario, the proposed method is not that different with a parameterized variance neural network $f_{var}(x)$. Then the Laplacian approximation formulation in the paper is not very useful. Moreover, using only the last layer is not a fair comparison with KFL (Ritter et. al. 2018) either.\n3. Compared with KFL+LL and KFL+OOD, the vanilla KFL seems to be better based on Table 1 and Table 2. However, KFL+LL and KFL+OOD are fine-tuned versions of KFL, it is weird that tuning the prior variance hurts the performance. \n\n**A few typos**\nEq(2) misses the const $\\log p(D)$; \ntwo paragraphs before prop1, \"where d it the resulting number\"\n\n**References**\nJacot et. al., ‎2018, Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nRitter et. al. 2018, A scalable laplace approximation for neural networks\nGuo et. al., 2017, On Calibration of Modern Neural Networks",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not a correct way of doing Bayesian inference",
            "review": "The paper proposes a post-hoc uncertainty tuning pipeline for Bayesian neural networks. After getting the point estimate, it adds extra dimensions to the weight matrices and hidden layers, which has no effect on the network output, with the hope that it would influence the variance of the original network weights under the Laplacian approximation. More specifically, it tunes the extra weights by optimizing another objective borrowed from the non-Bayesian robust learning literature, which encourages low uncertainty over real (extra, validation) data, and high uncertainty over manually constructed, out-of-distribution data.\n\nUsing Eq (7) and (8) to quantify posterior uncertainty doesn't seem correct to me. Instead of manually building some fake OOD data and forcing their posterior variance to be large, plus forcing all real data points' posterior variance to be small, one should focus more on the different posterior uncertainty *within* the real observed data. For example, data in highly uncertain areas should naturally have higher posterior variance. But in the proposed method, they are all forced to have small variances as long as they are real data.\n\nAnother confusion I have is more related to the math behind. The extra network weights and hidden dimensions are designed not to have any connection to generate the output. If so, does it mean that no matter what values the extra network weights take, their curvature to the output prediction should be entirely flat? How would these values affect the uncertainty if their Hessian values are zero? It would be beneficial if the authors could explain more about why that's not the case.\n\n\n--------------------------\nPOST DISCUSSION UPDATE\n\nThe central part of this work about how the extra network weights only affect the curvature still confuses me. But I'm more motivated by the proposed objective function that borrows idea from non-Bayesian robust learning literature.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}