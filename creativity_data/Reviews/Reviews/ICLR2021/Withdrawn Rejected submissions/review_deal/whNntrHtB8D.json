{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The range of the initial reviews was fairly high with overall scores ranging from 4 to 7.\n\nThe authors provided a good response that answered most of the reviewers' comments and questions. One of the reviewers even increased their score following the authors' response. \n\nThe focus of some of our discussions and what ultimately led to my suggestion was the related work of MIR [1]. The methodological differences between (the three versions of) MIR in [1] and GMED [this paper] appear less significant than what the current submission suggests. While there is some disagreement between the authors and Reviewer1 about the exact differences, I find that the current manuscript does not acknowledge the close relationship between these two contributions. Further, from the experimental standpoint and without further justifications the gains from GMED+MIR could be attributed to using more replay (from combining GMED and MIR).\n\n\nIn their response, the authors disputed the view of Reviewer1. I believe the source of the confusion between the author and the reviewer might be captured in this sentence from the author response to Reviewer1: The approach does not learn a generator that can “generate examples that are more forgettable for the classifier”; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.\n\nLooking at Equations 2, 3 and Algorithm 1 from [1], in GEN-MIR while two different procedures are used to obtain forgettable examples for the generator (B_G in Alg. 1) and the classifier (B_C in Alg. 1), the generator is used in both cases. In other words, the generator is used to generate examples for both itself and for the classifier. So, I think it's fair to say that the generator does indeed generate examples that are more forgettable for the classifier (Eq. 2). \n\n\nI strongly encourage the authors to prepare another version of their work where the differences between MIR [1] and their contribution are clearly highlighted and the results show the advantages of GMED (including memory-editing in data space). \n\n\n[1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS 2019. https://arxiv.org/abs/1908.04742"
    },
    "Reviews": [
        {
            "title": "This paper proposes a gradient based method to improve the samples stored in the replay buffer of continual learning algorithms by taking derivatives with respect to the forgetting measure. The paper's idea is nice and intuitive but the presentation and empirical validation can be improved.",
            "review": "Differentiation with respect to forgetting looks a promising idea that can complement differentiation with respect to current task accuracy and leads the CL algorithm towards a more stable solution by a good tradeoff between stability and plasticity.\n\nOne major concern about this paper is the empirical validations. Some baselines like AGEM are underperforming compared to reported numbers on other papers. The idea of memory editing can also be added as an additional loss to other methods too (not only to ER and MIR). Is that true?\n\nRandom edits look quite promising. It may suggest that most of improvement is coming from replay (it can be anything) and the regularization effect that it provides to not to overfit to the data.\n\nIsn't figure 4 telling something trivial? Because each class has a unique pattern of active inputs and that leads similar activation patterns on data and subsequently to a similar gradient profile within the class.  \n\nIn equation 6 when we replace the forgetting measure (d_t) with equation 5 loss function \\ell is going to have a different coefficient (-\\alpha-\\beta) which basically is a special way of weighing the two loss terms on \\theta and \\theta'. It's good to elaborate on this more.\n\nThe connection of eq. 1 and eq. 2 to te final editing formula (stated in eq. 3) is a little loose. Especially starting from eq. 2 approximation using eq. 3 is not the most natural or organic step. \n\nThe paper is well written and easy to follow. \npage 2 --->... one online model...\npage 3 ---> an categorization ...\npage 4 ---> also show that ...\nAlso note that citations are not properly using parentheses. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting method to improve memory based continual learning ",
            "review": "This paper deals with continual learning. Specifically, given a stream of tasks we want to maximise performance across all tasks. Typically neural networks suffer from catastrophic forgetting which results in worse performance on tasks seen earlier in training. There are many proposed solutions to this problem. One specific set of approaches are \"memory based\" algorithms. Here we store some training examples in memory from the tasks seen thus far. These are then mixed in with new training data so as to encourage the model to not forget past tasks. \n\nThere are two central question for approaches of these kinds: what examples to store and how to use them. This paper details a method that is complementary to most approaches. Specifically, given a system of storing examples (here reservoir sampling) and using them, this methods deals with how best to extract performance from an example. Specifically, it \"edits\" the example before storing and replaying it, to make it more useful to the model. \n\nThe main motivation for the editing process is as follows: past works have shown that the most useful examples to replay are those that are most likely to be forgotten. Thus, if we take our examples in memory and update them (via gradient ascent) to be \"more forgettable\", then when replayed these will result in the most benefit to the model. \n\n* One comment I have here is that the way this motivation is presented in the paper can be improved; perhaps with a more clearly written paragraph in the preliminaries. On my first reading of the introduction for example, this sentence for example, \"_edits examples stored in the memory with gradient-based updates so that they are more likely to be forgotten_\" sounds like you're trying to get the model to forget examples more which is the opposite of what we want (as opposed to making the example inherently more forgettable so it is better when replayed). \n\n* The reasoning here is that because naturally forgettable examples help during replay, _making_ an example \"forgettable\" will naturally improve its usefulness. I can't fault this reasoning based on the results but to me it is not naturally clear that this follows. Would the authors be able to provide some intuition for what that means from an optimisation perspective? \n\nFurther, the authors propose an online metric for how much the network forgets an example (by comparing the loss at consecutive timesteps). Then when taking an example from memory, we perform a gradient update on the image to optimise this metric. \n\nLastly, we replay the edited example to the network and perform a gradient update based on both this edited example and the current training example. Figure 2 is very useful and well done to illustrate this. \n\nThe experiments are performed across standard tasks and show modest to good improvement over the range of datasets. \n\n * Two baselines that would be useful to include in the table are EWC (https://arxiv.org/pdf/1612.00796.pdf) and MbPA ((https://arxiv.org/pdf/1802.10542.pdf), the latter of which is a memory based method that also does not require tasks IDs. \n\nLastly there is some good visualization and discussion of what editing does. \n\nOverall this is a good paper with that deals with an interesting problem and proposes an interesting method of increasing performance.\n\n*  I think some ways to strengthen this paper would include addressing some of the comments above, specifically around motivating the \"editing\" process better and clarifying some of the language around it. \n* Finally, the field of continual learning in general could do with moving to more large scale tasks. This would allow testing these methods more thoroughly. While the results here are promising, they do not increase performance uniformly across all tasks -- for example on mini Imagenet which is harder than MNIST -- this method does not help. In general considering larger scale tasks (based on Imagenet, Omniglot or reinforcement learning like Atari, for example) would provide more evidence and allow further improvement of the method. \n\nIn the current form however, this is still a very good submission and I would recommend acceptance. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ok paper, but gradient-based memory editing for CL was already proposed",
            "review": "**Summary**\n\nThe authors propose a gradient-based memory editing scheme for replaying samples that are undergoing the most forgetting, coined GMED. They also propose a hybrid method with MIR [1]. An extensive evaluation on standard benchmarks shows some improvements throughout. \n\n**Concerns**\n\nMy main concern is that the authors seem to have completely missed that gradient-based memory editing has been proposed in [1]. Specifically, in their GEN-MIR and AE-ER methods, gradient-based optimization is performed on some latent codes of the data to maximize the \"forgetting\" measurement. \n\nGEN-MIR seems superior to GMED as it can perform multiple gradient-based edits on the data to achieve a significantly different replay sample. GMED can't because it re-uses the initial label as the label of the editable sample. More GMED edits will thus increase the probability that the initial label is wrong. I think this is why the authors only do one edit. You can see, however, that one edit doesn't seem to change the replay data, as exemplified in Figure 5. [1] also proposes AE-MIR when $p(x)$ is difficult to model online. Furthermore, I would argue that gradient-based editing is more sensible in latent space (GEN-MIR and AE-MIR) than in input space (GMED) because it will be much easier to stay on the data manifold. It is also much more computationally efficient. \n\nAll of section 4.1 and most of section 4.2, including \"estimating forgetting online\" can be found in [1]. Algorithm 1 in section 4.3 is also astonishingly similar to the ones in MIR. \n\nI think this work is too incremental to merit a publication at a top conference like ICLR. Thus, I encourage the authors to either submit to a lower-tier conference or further develop the methodology.\n\n**minor concerns**\n\n- The related work seems to be missing a branch of task-free CL, namely Continual-Meta Learning. \n  \n- no need to re-explain Reservoir Sampling\n\n- typos: a lot of hyphens are missing. e.g. *gradient-based* memory editing\n\n\n__________\n\n**POST-REBUTTAL**\n\nSadly, I'm decreasing my score a notch because the authors lack a deep understanding of MIR [1] that would make it evident that GMED is *much* closer to the three methods proposed in MIR. Specifically, the authors have responded to my concern about the lack of novelty:\n\n    In GEN-MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The generator is indeed optimized for “maximizing the forgetting”, but the “forgetting” here is measured for the generator - i.e., the generator retrieves most forgettable examples for the generator itself. The approach does not learn a generator that can “generate examples that are more forgettable for the classifier”; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.\n\nThis is simply not true. If you take a look at Equation 2 in MIR, you will see that the generator is generating forgettable examples **for the classifier**. It also uses it for itself, see Equation 3. GEN-MIR is thus a gradient-based memory editing for CL.\n    \n    AE-MIR (...) It is a hybrid approach of example compression and ER-MIR. There is no online optimization towards more forgettable examples for the classifier like GMED\n\nAgain, just like GEN-MIR, AE-MIR uses gradient-based memory editing for CL for the classifier.\n\nIn GMED lies somewhere between ER-MIR and [GEN-MIR, AE-MIR]. My guess is that the MIR's author didn't propose the GMED method because it doesn't make a lot of sense to update edit a sample **and not edit its label**. \n\nHere is an intuition on the behavior of each method: let's say your models sequentially learning to visually classify objects. The model is now learning about zebras and it's causing some interference on the horse's representation.\n\n- ER-MIR will search in its buffer and retrieve a horse for the classifier to do replay on.\n- GEN-MIR will search inside the latent space of a generative model to find generated horses for the classifier to do replay on.\n- AE-MIR will search the latent space of an autoencoder to retrieve past horses that appeared in the data stream for the classifier to do replay on.\n- GMED randomly samples some data in the buffer, e.g. a car, and takes one gradient update on the car such that it resembles more a horse. Then the classifier is fed that modified image (i.e. x) as well as the unchanged horse label (i.e. y)\n\nThe empirical section shows us that one needs to add MIR to GMED to obtain the best results. This comes as no surprise. Combining methods with each other and increasing computing needs and/or replay will give you a better performance on forgetting.\n\n\n\n\n_________\n\n\n[1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, andTinne Tuytelaars.  Online continual learning with maximally interfered retrieval.  In NeurIPS 2019.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Intriguing Empirical Results, But Lacking Theoretical Justification",
            "review": "## Summary\n\nThis paper proposes a task-free continual learning method called GMED that extends experience replay.\nThe key idea is to modify the individual data points in the replay memory to maximize the one-step forgetting at the current time step.\n\n---\n\n## Pros\n\n- The idea of editing the data in a replay buffer with gradient descent is novel.\n- The experiments show a significant performance gain compared to the baselines.\n\n---\n\n## Cons\n\n### Lack of justification for editing memory\n\nGiven a somewhat arbitrary objective function, updating the data in memory by gradient descent will make the data points move away from their original distribution. Since continual learning aims to learn the data's distribution, I think modifying each data point does not align with this goal.\n\nOne possible explanation for the performance improvement is that the editing acts as a kind of data augmentation. As shown in Figure 5, the difference between the edited data and the original data is minuscule. Since the performance would drop if the editing is significant, the magnitude of updates should be small such that the data do not deviate too far from the original distribution.\n\nFrom this point of view, I think it is critical to show that GMED is better than other data augmentation tricks.\nThe authors already performed an ablation study showing that GMED is better than adding random noise to the data.\nHowever, there is no much detail about the experiment to verify its validity. Since it is an important experiment, I strongly recommend the authors to provide enough details for reproducing.\nAlso, I would like to see a comparison with other standard data augmentation methods.\n\n### Confusing motivation and writing\n\nThe fundamental hypothesis of this paper is:\n> replaying examples that are likely to be forgotten by the current model helps retain its test performance.\n\nHowever, the final algorithm is far from this hypothesis. As I mentioned previously, I think it is closer to a data augmentation algorithm.\n\n### Unreasonable experimental setting\n\nFor the Split CIFAR-100 and Split mini-ImageNet experiments, the authors use a memory size of 10,000. However, the size of the whole training set is 50,000, which means the memory can store up to 20% of the whole data. In my opinion, the current trend is to keep the memory size under 1-2% of the whole dataset, which corresponds to 500-1000 in the case of CIFAR100. Note that the authors also follow this trend for MNIST and CIFAR-10. With 20% of the dataset, I suspect that even i.i.d. training on the memory would not differ much from i.i.d. training on the whole dataset.\n\nAlso, I think it is more constructive to compare models under a fixed number of examples in memory, not the number of bytes as in Table 3. While trying to equalize the memory usage of ER(+GMED) and CN-DPM, the number of examples in memory became unreasonably large for ER(+GMED) (over 40% of the whole data).\n\nThe brutal fact is that the current level of continual learning is far from practical. At present, most CL papers work on toy problems like CIFAR-100. At this level, I think comparing the actual memory usage is not meaningful. In my opinion, what the community should look for is an efficient algorithm that can scale to real-world problems. For this reason, I think it is more meaningful to constrain the number of examples in memory. Also, note that if we scale up the image size, the size of image data can be more dominant than CNN's parameter size.\n\n---\n\n## Overall evaluation\n\nThe proposed method does not have any theoretical justification, and several settings in the experiments are unrealistic. Nonetheless, since the experiments show significant improvements over baselines, I think it can be an effective technique. Unfortunately, at the current state of the paper, I cannot exclude the possibility that simpler data augmentation tricks on the replay memory may outperform the proposed method. Although I currently vote for rejection, I am willing to raise the score if the authors can offer enough evidence that GMED is better than trivial data augmentation tricks.\n\n---\n\n## Post-rebuttal\n\nDuring the rebuttal period, the experiments largely improved, resolving the majority of my concerns. I raise my rating accordingly. I expect the authors to reflect other suggestions as well in the final version. Especially, R1 raised a serious concern about the similarity between GMED and GEN/AE-MIR. A proper comparison should be included in the final version.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}