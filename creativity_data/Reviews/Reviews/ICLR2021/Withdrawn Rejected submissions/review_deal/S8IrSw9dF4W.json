{
    "Decision": "",
    "Reviews": [
        {
            "title": "Ok paper but limited contributions",
            "review": "This paper demonstrates that the trojan trigger rate in a trojaned neural network can be reduced by applying simple and small spatial transformations (i.e., translations, rotation, etc.) to the trojaned inputs. The authors further demonstrate that an enhanced attack (i.e., a torjaned network that is robust to such spatial transformations) by training over a small number of random transformation to the training data while training the trojaned model.\n\nStrengths \n-------------\n- The paper demonstrates that spatial transformations destabilize the triggers of trojaned networks. \n\nWeaknesses\n----------------\n- The fact that spatial transformations will destabilize the triggers is not very surprising. Especially as the authors did not provide any evidence that the underlying networks are robust to spatial transformations even in the absence of the triggers.  \n\n- The method of adding randomly sampled transformations applied to the training data and retraining the model is very similar to adversarial training. Therefore, the contribution seems to be very limited. \n \n- I am not sure how strong the physical realizability claims are. Given that these transformed images do not tend to be realistic (i.e., black edges), I think the authors should either tone down their claims or demonstrate with direct experiments that the retrained model is robust to real physical translations. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "simple and intuitive solution for backdoor attacks",
            "review": "This paper explores the propertires of backdoor adversarial attack and further proposes an effective method to improve the robustness of backdoor attacks. Especially, it is shown that the attacks easily become vulnerable (meaningless) as the location and appearance of the backdoor triggers (local patch) change from the original ones. Based on this discovery, a simple defense method, against backdoor attacks, is proposed to use simple image transformations such as flip or scaling. Moreover, this can also be applied to improve the robustness of backdoor attacks. The same simple transformations are applied on the poisoned images during training phase. \n\nThe overall story is quite straightforward that it starts with the fundamental deficiency of backdoor attacks and comes up with a solution, which is simple and efficient than other methods.  The solution doesn't require any model to be learned and can easily plugged into any backdoor attacking methods. However, it isn't an optimal solution for all methods that the experimental results show that 'Consistent Attack' are not especially and relatively working well with it. It needs some explanation about this result to backup this drawback. \n\n1) Could you please explain how this vulnerable of backdoor attacks relates to the network size?\n2) Are flipping and shrinking the only meaningful transformations? How about shifting and rotating? Is there any reason to not mentioned these transformations? Could these transformations especially help physical backdoor attacks?\n3) What happens if the scale (size) of triggers changes from the original one? \n4) There is a typo in Eq3, C (S(x; w))\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "### Summary\n\nThrough investigating previous backdoor attack methods, this paper identifies that previous attacks are sensitive to the location and appearance of the injected trigger. Based on this observation, the author presents a defense method against previous attacks based on static trigger injection. The author also presents a method for enhancing the robustness of previous attacks. \n\n### Pros\n\n1. This paper demonstrates that previous attacks is vulnerable to the change of the trigger location and appearance.\n2. This paper is well-written.\n\n### Cons\n\n1. Although the author presents a useful observation, this observation seems to be a natural drawback of previous static attack methods and the following defense method is similar to add data augmentation in the training process, which is naive and has no technical novelty.\n2. Some experimental details for reimplementation are missed, such as the training strategy of the target model.\n3. In the experiment, there seems to be no data augmentation in the original training process. Intuitively, adding data augmentation can increase the robustness of the original attack. Thus the author should further conduct experiments on the target model trained with data augmentation. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}