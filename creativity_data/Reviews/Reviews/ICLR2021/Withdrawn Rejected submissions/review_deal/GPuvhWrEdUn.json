{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper, the authors change the loss function of NNs to reduce the separability of the different classes in one of the hidden layers. The rationale for this assumption that the trained network will be more robust against white-box model inversion attack. The reviewers all concur that the paper had some merit, but that the paper is not well presented and believe the paper is not ready to be presented at ICLR.\n\nAlso, the separability issue is not totally explained, because a reduced L2 norm might not be the whole story that explains why a white-box model inversion would rely on for leaking information. This might need to be proof further and a couple of experiments in which there is still leakage of information shows the additional robustness from the new penalty.\n"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "Post response update\n\nI would like to thank the authors for their detailed response. The response addresses my confusion around the use of the terminology of model inversion (I would further suggest that the author use the term data reconstruction rather than than model inversion to avoid readers misunderstanding model inversion as referring to [1]). I still have concerns around the fact that differential privacy is not used as a baseline here, which would strengthen the argument made in the response that it provides orthogonal guarantees. \n\n-----------\n\nThe paper looks at the tradeoff between data separability in an embedding space and vulnerability to model inversion. The paper hypothesizes that increasing data separability improves accuracy but exposes the model to model inversion attacks. \n\n- At a high level it is not clear why the model inversion is a well-motivated attack vector given that model inversion extracts an average representation of the points from a class, not specific training points. The introduction talks about data recovery but it is not clear what it means to “recover” a data point in the context of this work.\n- Grammar in page 1: “ The central question here is how to better protect the data from being reconstruct while keeping useful information to the classification task.”\n- Grammar in page 1: “ by proposing and a self-supervised learning- based feature extractor”\n- Unclear what the following means: “  At a high level, we focus on the pipelines combining local data representation learning with global model learning manner.”\n- Section 3.1 proposes data separability as a measure of privacy, but it is not clear why it is necessary to introduce a new definition for privacy. Why not consider well-established definitions such as differential privacy? \n- Section 3.1 proposes informal statements tying data separability but the statements are not demonstrated, and the analysis does not outline how the results would be proved (There is also not proof in supplementary materials). \n- Section 3.2 introduces concepts such as “confusion” in embedding space without defining them. Moving forward, making the claims more precise would help make the paper more readable.\n- How does the approach from Section 3.2 relate to other losses like the triplet loss which compare distances between different points in the embedding space?\n- Section 4.1 does not specify whether the input x is part of the training and test set. Is the goal of the model inversion to invert a training point or instead a test point which the model is inferring on? \n- It is not clear why the experimental setup from Section 4.2 is well-motivated to study the model inversion problem given that the model architecture being fully connected, and the dataset being synthesized, but not used in prior work. \n- In Section 4.2, it is unclear how the quality of a reconstruction is evaluated. Is a successful model inversion evaluated based on human perception and a similarity metric? Does the lack of similarity mean that there is no privacy leakage? Here it seems that this comes back to the definition of privacy used, which looks at average case rather than worst case.\n- The introduction discussed a distributed setting scenario but this does not seem to be considered in the experimental setup itself. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting problem of inversion attack is studied with not a convincing approach ",
            "review": "The paper aims at strengthening DNN against inversion attack. It proposes to utilize an extra term in NN training objective function, called L_mixcon, to play with separability of  hidden representation of data in different classes. Following is my concern:\n-\tReducing hidden representation separability of data points in different classes equates to more confusion for classification. All the results shown in the paper, e.g. Table 3 or Figure 4, confirm that and show that there is a trade off between accuracy and robustness against the inversion attack. Thus, this proposed method could be helpful if the user is willing to give up on some accuracy in the hope of getting a more robust model. However, the paper does not highlight this fact and presents the method as if it provides the same accuracy with higher robustness. For example in page 8 it is stated that : “We select (lambda, beta) to match the accuracy results of MixCon to be as good as Vanilla training (see Accuracy in Table 3),” whereas in Table 3 the accuracy is not as good as Vanilla, and it is misleading.\n-\tIt is also obvious from formula (1) that minimizing L_mixcon with lower beta or higher lambda degrades the accuracy, and the provided results show the same thing. But it is not shown that the sweet spot exists as there is always a trade-off.\n-\tRegarding formula 1:\no\tWhy do we only look at p data points per class and not all?\no\tThe formula says i-th data point form class c_1 would be compared against i_th data point from class c_2. How do you do this one-to-one mapping between points of different classes? Or did you mean all pairs and the formula is not written properly?\no\tAlso, as a suggestion, I think designing beta as a function of classe (c) would be more appropriate.\n-\tRegarding the layer h, I am wondering if there is any recommendation how to choose that and how many layers to choose. In this paper only one layer is considered. I can also imagine the best choice of beta and lambda depends on the choice of layer.\n-\tIt is mentioned that the local feature extractor is a shallow NN in the setting, is it one layer CNN with nonlinearity or a linear model?\n-\tThe paper is understandable but there are multiple typos and the English could be improved.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "ICLR 2020 Conference Paper995 AnonReviewer",
            "review": "Post response update\n\nThanks for the response. However, my major concern is still that the technical contribution of this paper is limited.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThis paper studies an important problem, i.e., the vulnerability of deep neural networks to model inversion attacks. To address this problem, the authors design an objective function to adjust the separability of the hidden data representations as a way to control the trade-off between data utility and vulnerability to inversion attacks. Specifically, the authors propose a consistency loss (MixCon) to train data feature extractor to help protect original data to be inverted by attacker during inference. The authors also conduct experiments on synthetic and benchmark datasets to evaluate the performance of the proposed method. However, I still have the following concerns.\n\n[-] The proposed consistency penalties (i.e., Eq. (1)) are quite straightforward and intuitional. Additionally, it lacks theoretical analysis of the proposed loss terms. For example, it would be better if the authors could analyze how much the attackers’ attacking ability are reduced by using the proposed loss terms and the approximation error before and after incorporating the consistency penalties.\n\n[-] For the threat model, the authors assume that the attacker not only has access to extracted features but also all network parameters of the trained model. However, these assumptions are strong and unrealistic in the real-world applications.\n\n[-] More details are needed. Firstly, in Theorem 3.2, the authors claim that under certain cases, (stochastic) gradient descent algorithm can find the global minimum of neural network function $f$. It would be better if the authors provide the proof for this theorem. Additionally, the authors mention that lower bound and upper bound imply better accuracy and hardness of inversion, respectively. However, the definitions of both lower bound and upper bound are not clearly. It would be better if the authors provide the formal (mathematical) definitions.\n\n[-] The authors fail to cite existing state-of-the-art works on defending model inversion attacks, e.g., [1,2,3,4]. Additionally, in experiments, state-of-the-art baselines are not adopted. Currently, there are some existing defenses against model inversion attacks, e.g., [1,2,3,4]. It would be better if the authors give further discussion and compare the performance of the proposed method with that of these existing works.\n\n[-] Some typos should be corrected. Just list some of them.\n* “by proposing and a self-supervised learning based feature extractor” in Page 1.\n* “force for the data representation” in Page 4.\n\n\n[1] “Improving Robustness to Model Inversion Attacks via Mutual Information Regularization”, 2020.\n\n[2] “Defending Model Inversion and Membership Inference Attacks via Prediction Purification”, 2020.\n\n[3] “Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures”, 2015.\n\n[4] “Adversarial neural network inversion via auxiliary knowledge alignment”, 2019.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}