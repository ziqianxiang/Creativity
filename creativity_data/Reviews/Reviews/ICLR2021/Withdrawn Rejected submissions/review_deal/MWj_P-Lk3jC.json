{
    "Decision": "",
    "Reviews": [
        {
            "title": "Provides important theoretical understanding of efficacy of parameter sharing in MARL; need more clarifications on the experiments ",
            "review": "The paper focuses on the problem of learning policies in multi-agent systems that allow decentralized execution. One of the biggest problem that such a setup faces is that of non-stationarity during learning. Not only does the paper give a good intuitive description of what the problem is in the form of information \"ringing\" but comes up with a mathematical model of this phenomenon based on a recently introduced MAILP framework. This results into some theoretical proofs for why information \"centralization\" can improve learning convergence in multi-agent settings. Since parameter sharing is a form of such information \"centralization\", these results partially explain their current success. Then authors combine the idea of parameter sharing introduced by Gupta et al. 2017 with more recent deep RL algorithms to demonstrate the efficacy of the idea of parameter sharing.\nHowever, the idea of parameter sharing seems restricted to homogenous agents with same observation and action spaces. \nInterestingly, this paper describes some ideas for how it could be done for heterogenous agents especially if their observations were disjoint, but also with non-disjoint spaces with agent indication. Unfortuntely, they only give theoretical justification for the possibility but no actual experimental demonstrations are performed to confirm their efficacy.\n\nAlthough the paper doesn't point it out, but some form of parameter sharing is quite common in all current MARL algorithms including QMIX, COMA, DCG, etc. The usual explanation about having fewer parameters to train doesn't seem like the full explanation. So finally having a theoretical\nunderstanding of why it's so useful especially in the multi-agent scenario in the form of reduced non-stationarity is quite useful.\nThe community might find the cited MAILP model of general interest.\n\nWhere the paper misses out a bit is on the experimental side. It's somewhat unclear in terms of how many seeds were used to obtain the results in Fig 3 and Fig 4.\nSimilarly it's understandable that the paper is focused on the problem of decentralized execution (should be more explicit though), otherwise it's an interesting question of centralization in the form of concataning all observations and using a centralized policy with maybe factorized heads as used for baselines in Gupta et al. 2017. Information is being centralized in that scenario as well, so would the same ideas hold? Similarly all the experiments were performed on environments from Gupta et al. 2017. Often not recognized in the literature but because environments and algorithms are usually developed and tweaked together they can overfit to each other in their designs. Although parameter sharing has been shown to improve upon MADDPG even on the environments developed for MADDPG in [1], it would be great if the paper performed more such experiments showing the utility of parameter sharing on other environments. Similarly it would be useful if there some experiments that compared parameter sharing approaches on heterogenous scenarios along with the proposed theoretical proofs.\n\n\n\nThere is a missing citation in Sec 2.2.1\n\n[1] https://arxiv.org/abs/1908.01022",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Sheds some light theoretically and experimentally",
            "review": "Summary: \nThe authors study parameter sharing from the point of view of information transfer when cooperative agents learn in a multiagent scenario, showing that increased learning centralisation leads to better performance. \nThey validate this result by applying parameter sharing to a number of state of the art algorithms and experimenting with homogeneous agents in a number of environments, and show that the performance is much higher than when the agents learn independently. \nThe authors also discuss how to extend some of these results to heterogenous agents.\n\nStrong points:\nThe results are promising and will likely bring renewed attention to parameter sharing for cooperative multiagent systems with homogeneous agents.\nThe authors apply the MAILP model to prove the benefits of centralised learning w.r.t. independent agents.\nExperimental results are promising, showing significant performance improvement when using parameter sharing in a high number of deep-RL algorithms for homogeneous agents in 3 cooperative environments.\nCode has been released for easier reproducibility.\n\nWeak points:\nUnder the MAILP model, the authors assume that parameter sharing is equivalent to having the most centralisation. Even though this could make sense intuitively, I find this relationship loose, or not formally stated at least. \n\nI miss two lines of work in the related work section that would be interested to discuss: mean field games, where all agents share the same policy; multitask learning where the idea of augmenting the input with a tag has been used in supervised learning for long time, e.g., (Caruana, 1998)); and where finding a policy that can handle different tasks has proved effective (Parisotto et al., 2016).\n\nQuestions:\nRegarding the connection between centralisation and parameter sharing, suppose all agents have access to the observations of every other agent, but they have one different value and policy networks each. If the weights of each network are initialised differently, then all agents could optimise with all information, but converge to different network weights. Similar reasoning holds if they have different parameter network architecture. Would the theoretical results still hold in these settings?\nIs there a way of formally relating parameter sharing with centralisation? What about other ways of information sharing?\n\nAre the lower bounds for single agent (Theorem 1) and multiagent (Theorem 2) algorithmic agnostic? Can they be used to bound the performance of specific algorithms? \n\nAre the authors assuming finite state-action sets? If so, please say it explicitly in Definition 1 and comment why this finite assumption is needed. Otherwise, fix the probability distributions so the range of the densities is the nonnegative real cone.\n\nIt is known that cooperative games are potential games, so in the case of parametric policies, they can be expressed as single-objective problems, for which existence of solution is guaranteed under mild conditions. What is the contribution of Theorem 4 in this case? Why cannot these results be used? Is it perhaps the partial observability?\n\nFurther comments (not influencing the rating decision):\nThe last paragraph of Sec. 2.1 could be improved. The reference to the first PG method should be to the first deep RL method with PG. Also TRPO appeared earlier than DDPG and it has been also cited and used extensively, so it would be more accurate to cite them together.\nWhen introducing POSG, (Hansen, Bernstein, and Zilberstein, 2004) might be a good additional reference.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Blind review",
            "review": "This paper analyzes parameter sharing in multi-agent deep reinforcement learning (deep MARL), where a single network is shared between agents for the the policy or value function. There is theoretical analysis for both homogeneous and heterogeneous case as well as experimental analysis for the homogeneous case. \n\nParameter sharing is a common idea in MARL, but as the paper points out, it hasn't been widely analyzed. Theoretical and experimental analysis on parameter sharing would be very helpful for understanding its properties and could lead to better MARL algorithms. \n\nThere isn't an algorithmic contribution in the paper (as the paper points out, parameter sharing even with padding and agent ids has been done before), so the contributions in this paper are the theory and experiments. Unfortunately, the theory is insufficiently clear and formal and the experiments are not sufficiently comprehensive. \n \nThe proofs depend on the multi-agent informational learning process (MAILP) in 2.3, but this section is missing many details. For example, it isn't clear what 'information' is or how many of the terms (e.g., information, coordination coefficient) are determined. Maybe these details are in the other paper that is cited (Terry and Grammel 2020), but currently it is very hard to understand what was proposed, whether it is correct and how it is useful. All the proofs in 3 (the main proofs about convergence speed and parameter sharing) depend on the MAILP so they cannot be understood and verified. \n\nIn general, I am very skeptical that the MAILP can be easily used to describe MARL problems or that it would be helpful in analyzing them. That is, how would it even be possible to define numeric values for concepts such as information, coordination coefficients and centralization coefficients? Even if such values are possible to define they would likely require a model of the problem to generate. It may be possible to define problem metrics such as these and prove that parameter sharing improves learning speed, but the current proofs are not convincing. If these parameters only depend on the policy and value function representations, the paper should clearly describe how to generate the parameters from these representations. In general, it is worth rethinking what exactly the paper is trying to prove as well as a formal representation of this problem. \n\nThere are additional proofs for the heterogeneous case in 5, but they don't prove anything related to convergence. Instead, they show that parameter sharing can be used in the heterogeneous case. This idea is widely known (and used as is pointed out in the paper) and the proofs are (4 based on the definition of a policy and 5 with just renaming observations). This section should also be clarified, but it could mostly be moved to the appendix. \n\nThe experiments show that parameter sharing works well in a number of domains from the Gupta 2017 paper, but more comparisons are needed to properly evaluate performance. For instance, the results only show a set of single-agent RL methods applied to the domains, but they only compare the parameter sharing and non-parameter sharing versions of these algorithms. Do these methods outperform other MARL methods (MADDPG is mentioned, but the results aren't clear)? And these domains are quite old. Results should be shown in other, more common domains. \n\nAs a result, it is hard to see what to take from the results. It is clear that parameter sharing should sometimes help. Are these domains examples in which parameter sharing is particularly useful? Can you say something about what domains parameter sharing works best in, possibly using your metrics? There should also be domains in which parameter sharing hurts. For instance, maybe the agents are homogeneous, but they are located in very different parts of the environment and have to do very different tasks (e.g., as an extreme case imagine a two-agent problem where each agent is operating in a different independent MDP). In this case, it seems like parameter sharing would be hurtful (e.g., slow learning). It does seem like parameter sharing has promise, but both the theoretical and experimental analysis should be more comprehensive to try to answer these questions.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Major issues with novelty and misleading / wrong claims. Haven't read the theory in detail. ",
            "review": "-The issues with this paper begin in the abstract:\n\n\"The most centralized case of learning is parameter sharing, an uncommonly used MARL\nmethod, specific to environments with homogeneous agents.\"\n\nThis statement in wrong in two important ways: \n1) There is no sense in which parameter-sharing is 'the most centralized case' of learning. A large number of methods have been developed in the Centralized Training with Decentralized Execution regime (CTDE) that can be used (and are typically used) on top of parameter sharing. See eg. the QMIX and  COMA paper referenced in the intro.\n2) *uncommonly* is simply wrong. Parameter sharing has been the default for a huge number of papers in CTDE. In fact, even some of the very first papers on Multi-agent Q-learning (\"Multi\u0001Agent Reinforcement Learning\nIndependent vs Cooperative Agents\", Ming Tan 1993), investigates this. \n\n-\"A nonstationary environment...  so difficult Papoudakis et al. (2019).\" This entire paragraph is misleading. \"The Non-Stationarity Problem\" referenced in Papoudakis et al is specifically about non-transitive (rock-paper-scissor) dynamics that exist in general sum games. In fully cooperative games convergence is not in general a problem (see e.g. the convergence proof in COMA). \n\n-\"Parameter sharing, ... was concurrently introduced by Gupta et al. (2017) for DDPG, DQN\nand TRPO and by Chu and Ye (2017) for a special case of DDPG\". As mentioned above, parameter sharing was introduced in 1993 (and potentially earlier) and has since been used in a vast number of multi-agent papers, in combination with policy gradient and value based methods. \n\n-\" This “agent indication” technique was first used by Gupta et al. (2017).\". This unfortunately is also wrong. Agent indexes have been used abundantly in multi-agent RL, see for example \"Learning to Communicate with Deep Multi-Agent Reinforcement Learning\", Foerster et al, 2016. \n\n-\"Partially-Observable Stochastic Games (“POSG”) (Lowe et al., 2017), \". POSG have been around for decades. Please do your homework.\n\n-\"Since in parameter sharing, all agents share the same neural network, the agents always have the\nmost up-to-date information about the behavior of all other agents. Thus, it achieves the theoretically\nbest value K? = 1.\". This statement must be wrong. I can very easily construct an environment where the different observations of the different agents are none overlapping. As such, in these settings there will be no advantage from sharing parameters. Furthermore, I can also construct settings where parameter sharing is going to hurt performance (e.g. when different agents need to avoid each other on opposite sites of the hallway).\n\n-\"Parameter sharing has previously only been used experimentally with relatively simple DRL methods\n(plain DQN, plain DDPG, and TRPO)\". As pointed out before, this is false. In fact, many of the MARL methods mentioned in your paper use parameter sharing and it is so common practice that it is typically barely mentioned. It has also been combined with many of the state of the art methods, e.g. IMPALA (\"ACHA also used parameter-sharing across the\ndifferent players in combination with an agent-specific ID that is part of the input.\", from \"The Hanabi Challenge: A New Frontier for AI Research\", Bard et al 2020), R2D2 (\"We have also assumed that agents are sharing\nparameters, θ, as is common in cooperative MARL\", in  \"SIMPLIFIED ACTION DECODER FOR DEEP MULTI-AGENT REINFORCEMENT LEARNING\", by Hu et al 2020). \n\n-The experiments would be more convincing if (a) they were carried out on a challenging multi-agent benchmark that has been properly benchmarked, and (b) they included uncertainty measures / significance. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}