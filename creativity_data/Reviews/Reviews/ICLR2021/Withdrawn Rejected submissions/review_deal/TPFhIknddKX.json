{
    "Decision": "",
    "Reviews": [
        {
            "title": "An experimental paper with limited contribution",
            "review": "This paper studies the routing network and finds that a random routing approach performs better than the original routing network when the number of tasks is large and tasks are similar to each other.\n\nIt is well known that the reinforcement learning is hard to optimize, let alone the multi-agent reinforcement learning algorithms. Hence, such finding does not surprise me to much. However, experiments do not show whether a random routing algorithm outperforms other multi-task learning algorithm which are out of the routing network. In this sense, I do not know whether such a random routing method achieve state-of-the-art performance and hence cannot judge the usefulness of this random strategy.\n\nWhen different tasks are not so similar, how is the performance of the random routing method?\n\nMoreover, is it possible to adaptively do routing and random selection? This is more interesting to study.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper investigates the issues about learned parameter sharing, which is known as routing networks by examining their performance and identifying a few challenges.",
            "review": "It seems that the paper is written well and provides an insightful result. In particular, this paper identifies the problems including the cold start problem and combinatorial explosion, and proposes a random selection policy. The reviewer still wonders whether the conclusion can be generalized because such results may depend on the dataset and problem.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A confusing empirical paper on routing networks - requires a careful rewrite to better explain novelties and main findings",
            "review": "[Summary]\n\nThis paper is an empirical investigation of the method of Rosenbaum et al. called Routing Networks. The authors investigated the difficulties of training both a routing policy learner and model weights. In particular, the authors investigated the scenario where task interference is low and a model with learned routing may struggle to beat baselines. Based on their observations, the authors proposed a new method, building on the Routing Networks algorithm, where the policy learner is replaced by a model that fixes the routing route randomly at initialization. The authors validated their model on two datasets, showing significant improvements when compared to the original Routing Networks algorithm.\n\n[Decision reason]\n\nMy decision to award a score of \"3 - clear rejection\" is based on the [Cons] (see below). I am concerned about the novelty of the method and what can be learned from the empirical observations. I believe the manuscript would benefit from a serious rewrite to better explain the results, the method and their relation to existing theory.\n\n[Pros]\n\n1. The results compared to the original paper are good - the presented method yields significant improvements when compared to the original Routing Networks paper (corrected and non-corrected code)\n\n2. The empirical section in 4.3 (Similar Tasks Grouped Together) is interesting and provides interesting thoughts on task similarity.\n\n3. The related work section is well written and helps the reader understand the literature surround the work.\n\n[Cons]\n\n1. The description of the main presented method (Fixed Random Routing) is severely lacking. Firstly, it is difficult to fully understand what is being presented and how it works exactly. Secondly, the method is poorly justified, both in terms of i) how to share weights/modules amongst tasks and ii) how to jointly optimise a routing agent and network weights. If the development of the method (Fixed Random Routing) is based on the empirical observations of the paper, then the link needs to be clear. How exactly does the method better find which parameters should be shared amongst tasks?\n\n2. Rosenbaum et al. published a follow up paper [1] (in April 2019) on the challenges of routing networks, namely enumerating the following main issues: i) training stability, ii) module collapse, iii) overfitting. Given that this paper explores limitations of the routing algorithm, and considering previously published extensive empirical paper by Rosenbeaum [1], what new observations can be learned?\n\n3. In Tables 1 and 2, why does Fixed Random Selection perform fractionally better than No Sharing? If No Sharing is a simple Single Task network, what exactly is the advantage of using Routing Networks with Fixed Random Selection?\n\n4. As a mainly empirical paper, I would like to see if the observations in 4.1, 4.2 and 4.3 generalise across mainy different datasets. As of now, it is difficult to ascertain whether much can be learned about the behaviour of the models.\n\n[Other]\n\n1.  In the original paper, the authors used MNIST-MTL, MIN-MTL and CIFAR-MTL. The authors only looked at CIFAR-MTL and Fashion-MNIST. Is it possible to evaluate their new method on the same baselines? The authors used a different version of CIFAR-100 than Rosenbaum. Would it be possible to test their method on the original formulation?\n\n2. In 3.2 - how exactly are modules defined? Is 1 module one specific 3x3 conv filter in a layer?\n\n3. In the related work, I would suggest also adding various other modular papers which are specifically related namely, Modular Networks by Kirsch et al. [2], which proposes a different way of solving modularity in neural networks and Stochastic Filter Groups by Bragman et al. [3], which proposes to learn routing of information by determining task-specificity of kernels.\n\n[1] https://arxiv.org/pdf/1904.12774.pdf\n[2] https://arxiv.org/abs/1811.05249\n[3] https://arxiv.org/abs/1908.09597",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "the paper is difficult to read and the novelty is not clear",
            "review": "This paper studies the effectiveness of routing networks for multi-task learning. The authors argue that if the key assumptions are not satisfied, a fixed random selection routing strategy is the best. In general, the paper is not easy to follow. All the details are presented in a rough description by text. The novelty of the method is also not very clear or convinced.\n\n1. The authors hypothesized three key assumptions for the routing networks to work effectively. When do these assumptions hold or not hold in experiments and applications? There is no further investigation.\n\n2. When is the fixed random routing taking the advantage? This is also not fully investigated. If the tasks are not too dissimilar, or the learned models are not the different, then random routing actually works as a single task. For this case, complicated routing strategy may actually fit the noise and deteriorate the performance. However, if the learned models are complementary, they should be able to benefit from each other. So, it seems it would be more interesting to investigate how to get complementary task learning.\n\n3. In Table 1 or Table 2, the accuracy by \"the Fixed random selection in Conv 3\" is very close to the one by \"No sharing\", 73.8 vs 73.5.  Is the difference between them significant?\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}