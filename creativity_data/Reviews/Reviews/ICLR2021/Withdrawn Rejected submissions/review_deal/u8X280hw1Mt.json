{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The equivalence rules for the margin are quite interesting, but I have two main concerns with the current paper (1) the theory does not seem to justify why increasing the number of negative examples helps in contrastive learning -- in fact Table 9 shows that the bound gets smaller as K increases. (2) The experiments use a large batch size (N) but use a smaller number of negative examples (K), which does not reduce the computation cost by much. The theoretical issue (1) can be a matter of improving the writing to put less emphasis on the theory. I invite the authors to address these issues and resubmit to other ML venues. \n\nDetailed feedback: I believe you are missing a negative sign in your definition of optimal \"loss\" $\\mathcal{L}_{opt}$ in Eq. (3), which resulted in AnonReviewer4's final comment."
    },
    "Reviews": [
        {
            "title": "Good observation and experiments, but a bit over-claim",
            "review": "This paper introduces a margin term for positive pair in contrastive loss, and discovered an equivalent rule between margin m, temperature \\tau, and number of negatives K. The authors demonstrate that once they set the three values following the equivalent rule, then relatively smaller number of negatives can also work well for contrastive learning.\n\nPros:\n- It is interesting to introduce a margin m, and properly set a relation between it and temperature and number of negatives.\n- Theoretically, the authors show that once you set the term contains K in the lower bound of MI to be constant, then the lower bound will be irrelevant to K. The equivalent rule is very interesting, but the theorem and Figure 1 is a bit trivial to me(see cons).\n- The experimental results with MoCo, MoCo v2, and SimCLR shows that the equivalent rule improves the stability over different number of negatives for contrastive learning.\n\nCons:\n- The reason why the lower bound now doesn't depends on K, is because you set the term that contains K as constant now. Now K is not an variable for the lower bound anymore, and the only variable in the lower bound is the density ratio, which is almost stable once the training converges. Also, theoretically the converged density ratio should be invariant to K. Therefore, you don't even need to plot Figure 1(b), but you will guess it's constant. To extreme, you can set I >= 0 = f_bound, but it tells you nothing. Thus,\n(1) The real question is that, how this bound can sensitively and faithfully reflect the change of ground truth mutual information? To this end, I would recommend authors to follow the experiments in [A1] to generate variables that share different amount of mutual information, and check how your bounds track that. E.g., Figure 2 in A1.\n(2) you can do such bound estimation on held-out data to check.\n- I found myself not convinced by some statements in this paper:\n(1) In abstract, \"for the first time, we can perform self-supervised contrastive training using only a few negative pairs\", some prior work [A2] has already reduce the number of negatives. So the tone here is a bit arrogant to me. Besides, BYOL paper has already shown that you can completely remove the denominator term in contrastive loss, and only do alignment.\n(2) The first bullet of contribution section that says \"large size of negative samples is critical\" is a wrong interpretation. I kind of agree that small negative also works well, at the ImageNet scale. But what will happen for uncurated dataset whose size is far beyond ImageNet, is still unknown. So the claim here should be more restricted.\n- The SIMO design choices you proposed here is exactly the same as SimCLR v2 [A3].\n(1) No memory bank, key and negative from the same batch of momentum encoder. Yes, check SimCLR v2.\n(2) SyncBN. Yes, it's in SimCLR and SimCLR v2.\n- Missing related work:\n(1) Big Self-Supervised Models are Strong Semi-Supervised Learners. NeurIPS 2020.\n(2) Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. NeurIPS 2020.\n(3) What makes for good views for contrastive learning. NeurIPS. NeurIPS 2020.\n(4) Whitening for Self-Supervised Representation Learning. arXiv\n\n[A1] On Variational Bounds of Mutual Information. ICML 2019\n[A2] Whitening for Self-Supervised Representation Learning.\n[A3] Big Self-Supervised Models are Strong Semi-Supervised Learners. NeurIPS 2020.\n\nOverall, I like the equivalent rule for stabilizing the training across different number of negatives. Meanwhile, I hope the authors could reconsider some statements. Also, it would be of great interest to see the experiments on estimating mutual information. I would consider changing my rating if my concerns get solved properly.\n\n\n==== update ====\n\nI found the bound the author provides is problematic (see my response for details). In short, the bound is problematic in that the bound will be more accurate when only one sample from the product of marginal (or negative) is used, i.e., $K=1$, while more negatives leads to less accurate bound. This definitely counters the intuition in theory where more samples should let you estimate the KL divergence between $p(x,y)$ and $p(x)p(y)$ better.\n\nI believe there is a mathematical issue of directly setting up $m=\\tau \\log \\frac{\\alpha}{K}$ as in Eq(5). Though the empirical results look good, I recommend **NOT** accept papers with theory issues.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper with some concerns",
            "review": "The authors propose Eqco which can get rid of the effect of the large negative sample size in contrastive learning. \n\nPros:\n1. The authors tackle an important question whether the large negative sample size is important in contrastive learning\n2. The authors propose Eqco which introduces an margin term to the InfoNCE loss. \n3. The authors empirically shows that the result algorithm Simo is less sensitive to negative sample size.\n4. The authors prove that large negative sample size does not affect the lower bound and the bound of the gradient.\n5. The authors improve Moco-v2's result using Eqco.\n\nCons:\n1. In figure 2, even though the accuracy is less sensitive to K. But increasing K still helps the performance. This result is against the theorem 1 and 2. Is there any other explanation?\n2. Gradient and lower bound may not be the only thing that affect the performance. What about generalization? Will large negative sample size help generalization? This needs a discussion\n3. Does representation learning through minimizing the mutual information help improve the performance of the downstream classifier (linear or MLP)? Will large negative sample size help because of the downstream classifier is linear/MLP? This needs a discussion.\n\nSignificance:\nThe problem is significant and the authors made some progress on it.\n\nClarity:\nThe paper is clearly written in general.\n\nOriginality:\nThe paper is novel.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting topic and observation, but there are some issues with the theory and result",
            "review": "This paper studies the performance of contrastive learning under a varying number of negative examples. The authors propose a small modification to the InfoNCE loss and show tuning some hyper-parameter could significantly improve the results with a small number of negatives. I found this topic very interesting, and the proposed solution to be quite simple. It challenges the conventional belief that a large number of negatives are required for contrastive learning (though this has been challenged before, recently by BYOL). The results show that when the number of negatives (K) is small, by using the proposed method, it could greatly improve the performance.\n\nHowever, there are some questions/issues that I came across:\n\nThe theoretical analysis shows by setting m using eq 5, the mutual information lower bound could be made completely irrelevant to K; instead it’s controlled by a constant alpha. This is clearly problematic: 1) it suggests one could simply adjust alpha to improve the lower bound to an arbitrarily large number, which is impossible given that mutual information is a fixed number given a dataset. 2) experimental results show there is still a significant gap when using small K (Figure 2). This suggests alpha is not some number one could arbitrary set to improve the bound, and it may actually be related to K, which makes the current theoretical analysis potentially flawed or incomplete.\n\nThe experimental results are pretty nice when K is small (compared to baselines), however, these are somewhat inconsequent since a small K is obtained by dropping out some negative examples in the current or past mini-batch, not actual small batch size (one still cannot use a small batch size due to batch norm and actual training time). The improvement on the best of MoCov2 result looks somewhat marginal and may not be related to the number of negatives (which contradicts the claim on why SiMo helps), as the performance of MoCov2 has saturated with increased K (Table 6). If the author’s claim were true, it should be possible to improve SimCLR's performance on small batch size (to reach the same performance of large batch size training), but this was not shown in the paper (Figure 2b). What's more, Figure 2b looks worse than expected. According to SimCLR paper (Table B.1), the gap between small and big batch sizes can be significantly reduced by using a proper learning rate, so it would be more convincing to use a better learning rate (or compare performances with different learning rates).\n\nOther minor issues: 1) for Table 7, the number of training epochs is not specified. 3) the methods in Table 2 looks a bit arbitrary, several existing similarly/better performing methods (e.g. BYOL, SWAV) are not included.\n\nGiven these issues, I will give an initial rating of 5, which is contingent on the responses from the authors.\n\n--after rebuttal--\n\nI appreciate the response, and it addressed my initial concern on the mutual information bound. However, the major concern remains which is the toy-ish setup and its practical value. The main result authors showed is that dropping some negatives doesn't hurt the performance if one adjusts m using EqCo rule. While this is interesting, the current setting of few negatives (by throwing out available negatives) is toy-ish, and may not reflect the real situation when a mini-batch is small thus only few negatives are available. I'd really like to see how this could be used to deal with the situation where you indeed only has few negatives. In MoCo, one could always easily buffer negatives with EMA the network, so there's no need to use a smaller set of negatives. In SimCLR, there seems to be a bigger potential of improvement from EqCo with actual smaller batch size, but the authors should also tune the learning rate (for smaller batch sizes) to make the baseline convincing, and compare to the results where large batch size is used. While the authors added a new point of alpha=4096, the current results of the paper are still incomplete, so I would keep my score. I'd encourage the authors to update/complete these results regardless the paper is immediately accepted by ICLR.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Excellent and Insightful Discussion on Negative Samples in Self-supervised Contrastive Learning",
            "review": "*****************************\nSummary: \n\nThis paper focuses on self-supervised contrastive learning. Previous contrastive learning methods heavily rely on a large number of negative samples. This paper proposed a novel method with an additional margin term, and mathematically investigate the relationship among the margin term, the temperature, and the number of negative samples. The number of negative samples can be significantly reduced by tuning the margin term, while the performance remains more stable compared to previous contrastive learning methods. Furthermore, this paper proposed a MoCo-based strong baseline that can achieve comparable results with an extremely small number of negative samples.\n\n*****************************\nReasons for Score: \n\nOverall, I vote for acceptance. This paper proposed equivalent rules which successfully reduce the number of negative samples for contrastive representation learning while maintaining the performance. This motivation is very novel and interesting, and I believe the findings in this paper significantly contributes to the community of contrastive representation learning. My main concern is the assumption that the prior probabilities $P^+$ and $P^-$ satisfy $P^+/P^-=e^{-m/\\tau}$ (see Sec. 2.1). \n\n*****************************\n \nStrengths: \n\n\\+ This paper focused on the problem of using a large number of negative samples in self-supervised contrastive learning, which is very important but not fully investigated before.\n\n\\+ This paper proposed a simple but strong baseline that can achieve comparable results with an extremely small number of negative samples. To the best of my knowledge, this is the first time to succeed with a small number of negative samples in contrastive learning. It may also inspire the applications in other self-supervised learning problems where a large number of negative samples are hard to be maintained.\n\n\\+ This paper provides detailed mathematical analysis and theorems, which makes the paper theoretically strong.\n\n\\+ The proposed strategy achieves significant improvement compared to the most popular self-supervised contrastive learning methods, MoCo and SimCLR, with a small number of negative samples. \n\n*****************************\n \nConcerns & Questions: \n\n\\- The Eq. (2) includes $P^+$ and $P^-$ as prior probabilities. It is not clear to me how the priors are taken into account and whether it is theoretically correct. In addition, it is not clear the assumption $P^+/P^−=e^{−m/\\tau}$ is correct. Could more mathematical details be introduced? Why is including $P^+$ and $P^-$ called a generalized form?\n\n\\* In this paper, the temperature $\\tau$ is fixed when tuning $K$ and selecting $m$. It would be an interesting issue if Eq. (5) is rewritten as $\\tau=\\frac{m}{\\log\\alpha-\\log K}$. I wonder whether it is possible to select $\\tau$ by fixing $m$ and tuning $K$. If not, is there any insight behind this?\n\n\\* In the proposed SiMo, all the negative examples are obtained from the current batch via the momentum encoder rather than the dictionary. Table 3 in the appendix shows that momentum update is important. Note that the momentum encoder in MoCo is used for the consistent dictionary of negative samples. Since the dictionary is not used in SiMo, the momentum update should have a different role in contrastive learning, perhaps like Mean Teacher. It would be great if more analysis or explanation on this can be provided.\n\n\\* SiMo uses Sync BN rather than Shuffling BN. I wonder whether Sync BN causes extra computational cost or slowers the training speed.\n\n\n\n\n\n============================================\n\n\nAfter rebuttal:\n\nAccording to the reviewers' feedback, I would keep my score to 8 and still vote for acceptance. However, there are still two details that are expected to be fixed in the future version. First, the reason why momentum update in SiMo is important is not convincing to me. It is not clear why letting $\\theta_k=\\theta_q$ cannot ensure the loss becoming smaller. More theoretical and experimental analyses are expected to address this issue. I still encourage the authors to rethink this detail. Second, the computational cost is provided in the feedback. I encourage the authors to include the numbers in the paper. 35\\% additional cost cannot be ignored.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}