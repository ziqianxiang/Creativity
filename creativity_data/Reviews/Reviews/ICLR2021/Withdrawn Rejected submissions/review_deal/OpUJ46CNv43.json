{
    "Decision": "",
    "Reviews": [
        {
            "title": "A new algorithm for hyperpparameter optimization",
            "review": "The method for hyperparaemeter optimization proposed by the authors is based on latin hypercube search. The algorithm then is enhanced by reduced search space that is obtained by Factorial Performance Analysis and Factorial Importance. \n\nAs to originality, I think it is ok, but not outstanding. Most ideas of the method have already exist. The paper is not hard to follow and I think it is well-written. \n\nThe pros:\n1. It is an important topic that how we can speed up and make hyper-armament optimization more effective. MOFA is pretty light weight algorithm and can be parallelized naturally. \n2. Incorporating orthogonality into Latin Hypercube search is an interesting idea. \n3. Using Factorial analysis to decide important hyperparameters and proposes search ranges for next iteration seems working for the experiments shown in the paper. \n\nCons:\n1. The method may cause the search stuck into a local region. It looks like the search space can shrink each iteration. If the local region doesn't have good configuration, then the method doesn't work.\n2. The experiments is not extensive. The UCI datasets are too small for useful comparison.\n3. It will be nice to have a comparison with Orthogonal Array Tuning Method proposed by Zhang, etc. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Reject",
            "review": "The paper proposes a model-free and highly parallelisable HPO method. The approach samples initial candidates with a sampler with good space-filling properties (based on Orthogonal Latin Hypercubes (OLH), evaluate all candidates in parallel, then select the most promising range, resample candidate until the budget is exhausted. Theoretical (preexisting) results are given justifying the choice of OLH compared to mere Latin Hypercube as it reduces the variance estimator. Finally, experiments are performed when tuning the hyperparameters of three models on two different datasets against standard HPO baselines.\n\nThe main strength of the paper lies in the simplicity of the method: it seems that the method has relatively low complexity and would be easy to reimplement (say compared to BO or BOHB). It is also parallelisable which can certainly be an advantage in some situations. I found also the paper to be generally well-written and easy to follow.\n\nThe main weaknesses of the paper are: 1) the technical contribution is limited (I believe the main aspect is proposing the use of OLH in HPO combined with some simple candidate selections), 2) while being simple the method has strong limitations (not being able to handle categorical variables for instance) and 3) the experiment section lacks in depth.\nIn particular:\n1) The \"theoretical analysis\" section is limited in the sense that it only presents known results that are orthogonal to this paper contribution. The results bounds the estimator performance when random-search/LH and OLH and justify why OLH and LH overperform random-search but those are known methods and results. There would be a theoretical contribution if the author performed a theoretical analysis on their algorithm rather than citing (relevant and interesting) theoretical analysis from blocks they are using without studying the combination they propose. \n2) The fact that the model is not being able to handle categorical is a rather strong limitation, not discussed also there is the fact that your range restriction will only work on some strong assumption (if you have two promising disconnected areas, your approach will restrict its range to one of them and forget the other forever)\n3) The experiments are concerning as results are reported over a single seed (as far as I understood for Fig 2-3 no seed or repetition is mentioned in the text), Tab 1 does repeat over several seeds but only report mean and not std. In addition, the breadth of the experiment is very limited (4 datasets) and 3 blackboxes. Given the simplicity of the method, one would expect much more thorough empirical investigation to justify its performance. \n\nFor those reasons, I recommend rejecting the paper.\n\n# Additional questions for the author:\n\n* 3.3 your decision on whether to exclude regions does not seem robust: for one thing if you have a bad outlier, you will explore the region forever (high-variance) even though it is not promising. Also, I would have expected this to be part of your ablation analysis.\n* 3.5 should also be part of your ablation to show that there is a benefit in doing more subtle that just picking the best.\n* Fig 2-3: these figures really need to report confidence interval over several seeds, consider using pre-computed tabular surrogates if this is too expensive\n* Table 1: confidence interval would be important to show that statistical strength of performance gain\n\n# Additional feedback (not part of the decision assessment):\n\n* 4.2 detail on the baselines are lacking: how is the grid computed? how is BO used?\n* Property 3 \"c/n\" -> \"c/N\"\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Hyperparameter optimization via modular factorial designs.",
            "review": "MOFA: Modular Factorial Design for Hyperparameter Optimization\n\n**(Summary)**  \nThis paper investigates the use of modular factorial designs (MOFA) for hyperparameter optimization (HPO) and more generally black-box optimization. Their method does not use a surrogate to model the output of the unknown black-box function but rather combines ideas from experimental designs and factorial analysis to select promising configurations.  The authors describe the hyperparameters of their method and motivates their choices.  Finally, they study the results on a few HPO problems and show that their method is competitive to methods from surrogate based optimization including Bayesian optimization (BO).\n\n**(Strong points)**\n- The method is sound and very simple and likely runs with much lower overhead than the competing BO methods.\n- I appreciate that the authors used log-scaling for all of the hyperparameters for the BNN tuning experiment as this is a common mistake. Can you confirm that you did the same for the DNN experiment as these parameters should also be in log-scale.\n- MOFA looks competitive compared to the other methods in the experimental evaluation.\n\n**(Weak points)**\n- The paper has a large number of typos, word repetitions, and arbitrarily capitalized letters.  I suggest the authors spend some more time working on the text to improve the flow of the text.\n- The introduction is a bit too light and is missing a rather large number of papers from other methods on hyperparameter optimization from the past few years. \n- Being model-free is argued to be an advantage, but it’s counterintuitive and unclear why that would be the case, especially on many of the low-dimensional experiments considered where GP-EI is expected to be very hard to beat for the evaluation budgets considered.  \n- While the authors consider a few real-world experiments, they choose a weak field of competing methods based on the budget they are considering.  Comparing to Hyperband/BOHB are more suitable for a larger number of evaluations and I would rather see the authors compare to BO with TS[1] / EI[2] / KG[3] as their experiments are all considering low-dimensional problems.\n\n**(Recommendation)**\nI find the work interesting and like that it provides a different perspective on HPO, but I don’t believe this paper is ready for publication.  I recommend that the authors work on the text, improve the experimental evaluations and add more competitive baselines, as well as provide more insight via ablation studies into why their method is preferable over BO methods.\n\n**(Questions for the authors)**\n- As your recommended hyperparameters are a function of N, what values would you recommend in a scenario where it’s unclear what budget to use? An advantage of many BO methods is that they generally have no dependency on N.\n- How does your method perform on high-dimensional HPO problems, say d > 20, and how does it compare to state-of-the-art methods from high-dimensional BO such as TuRBO[4]?\n- Can you comment on how MOFA can be extended to categorical variables?\n- Do you think that other methods can borrow strength from some of the steps in your method? For example, do you think that the factorial analysis and freezing of unimportant parameters can be used by other methods and make them scale to problems with more tunable parameters?\n\n**(Additional feedback)**\n- Note that most models used in Bayesian optimization such as Gaussian processes are non-parametric models even though the paper seems to argue that parametric models are often used. In the general setting we prefer working with non-parametric models over parametric models due to their modeling flexibility.\n\n[1]  J. M. Hernández-Lobato, J. Requeima, E. O. Pyzer-Knapp, and A. Aspuru-Guzik (2017). Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In Proceedings of the International Conference on Machine Learning.\n\n[2] Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efficient global optimization of expensive black-box functions. Journal of Global Optimization.\n\n[3] Frazier, P. I., Powell, W. B., and Dayanik, S. (2008). A knowledge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization.\n\n[4] Eriksson, D., Pearce, M., Gardner, J., Turner, R.D. and Poloczek, M., (2019). Scalable global optimization via local Bayesian optimization. In Advances in Neural Information Processing Systems.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple mehtod questioned by a weak experimental section",
            "review": "Summary of the Paper:\n  \n        This paper describes a strategy for hyper-parameter optimization. It is not based on any particular model to help and guide the search process. By contrast it uses a clear strategy to explore the space more uniformly by sampling candidate configurations using an orthogonal Latin hyper-cube. These configurations are then evaluated in parallel.  After this a range collapsing and factorial analysis is carried out to guide the selection of hyper-parameters and to focus on specific regions of the input space. The method is compared in several experiments with other strategies for hyper-parameter optimization.\n\nDetailed Comments:\n\n        The paper is good written in general. However, it is not clear at all how the factorial analysis step works. The authors should spend more time describing it. It seems one simply computes average values for each range value. It is not clear how the importance is computed. The authors should give some examples and show how those quantities are actually computed. The one shown in the figure is not enough.\n\n        A disadvantage is that the proposed method has two hyper-parameters that need to be adjusted in practice.\n\n        A nice thing of the paper is that it has some theoretical analysis in the form of several theorems. However, the theory seems to be valid when using a Latin Hyper-cube as the search method or random search. This questions the actual utility and novelty of the theoretical results.\n\n        The weakest part of the paper is the experimental evaluation. Even though several experiments are carried out and the proposed method is compared with other techniques from the literature, no error bars are given in the results shown in the figures. This questions the significance of the results. It is also unclear what the authors mean by Bayesian optimization in Figure 3. There are many methods of BO.\n\n        Summing up, in my opinion this paper describes a simple method for hyper-parameter optimization whose actual relevance is questioned by the weak experimental section.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}