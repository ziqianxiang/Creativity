{
    "Decision": "",
    "Reviews": [
        {
            "title": "Over-claimed experimental performance and limited novelty ",
            "review": "This submission presents a deep complex network based on the SurReal network. It proposes a division layer to incorporate the Tangent ReLU into the SurReal network. \n\nThe main contribution of this submission is the division layer. I'm convinced why using the tangent ReLU is necessary for a complex network. If not, what is the purpose of proposing the division layer? \n\nThe authors overclaim the performance of their proposed model. Among the five training data splits, the proposed method shows better performance only on the 60% training split, compared to SurReal. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Complex-valued scaling done properly",
            "review": "#### Summary:\nThe paper improves upon previous works such as Deep Complex Networks (introduced complex valued neural networks) and SurReal (introduced complex-value scaling). DCN introduced and argued for the use of complex-valued neural networks, but did so by increasing the number of parameters quite significantly. SurReal builds upon DCN by introducing complex-valued scaling, but the result of the complex-valued scaling layer is real, therefore disallows the use of any complex-valued layers after the c-scaling layer.  This paper addresses the shortcomings of these two papers and presents solutions to both. The result is a usable multi-layer complex-valued neural network architecture that includes complex-valued scaling invariance, that preserves phase information (which is the whole point of using complex-valued neural networks). \n\n#### Key strengths:\n\nThe development of the theoretical arguments (for the division layer, which is the main contribution) in this paper is gentle, considered and well-supported by experiments and well-grounded in complex theory.  I have checked the maths and it is correct. The experiments are convincing and due to the correct treatment of complex-parameters within the network, the work achieves a drastic reduction in the total number of parameters compared to DCN.\n\n#### Suggestions for improvements:\n1. Figure 3 is confusing. Please provide full explanation like that in Figure 5. You could make Figure 3 and 4 smaller. \n2. Figure 4 seems unnecessary as it makes the point already made well in SurReal paper. \n3. How big is the test set for experiments in Table 2? The details are in the original paper, but it would be worthwhile adding them here. \n4. What is the reason for the variance in 60% train column being higher than rest of the columns on average?  \n5. As the main contributions of the paper are building upon previous work significantly, an area that this paper could contribute well to, is the evaluation. Thus far, all complex-NN papers lack comprehensive evaluation to justify the use of phase (and these use-cases exist, for eg. in physics, MRI or speech). Clear examples of layer by layer retention of phase information and how this helps with the overall classification/regression accuracy would be valuable. The paper uses the same evaluation setup and dataset as SurReal, but perhaps expanding to other scenarios will be helpful in clearly supporting the contributions of this paper.  For example, the arguments under the Results subheading within Section 4 Experiments are quite insightful. In similar vein, if analysis of experiments on a different dataset or a different ML task was provided to compare DCN/SurReal and HyperReal, it would strengthen the paper (and further the understanding of this field). \n\n#### Overall comments: \nWhile the division layer presented in this paper has good theoretical justification, there could be an argument that there isn't enough novelty in the paper. This work addresses shortcomings in existing literature and creates a usable method is worthy of attention, perhaps as a short paper/poster. Will the code be made available? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Justification and experiments can be more strengthen",
            "review": "In this paper, Authors aim to have complex scale invariance $ f (z) = f (sz) $ in the network structure in the classification task using complex neural networks. Surreal (Chakraborty +, 2019) has been mentioned as a previous study, and Authors point out that (1) the relative position information is discarded and (2) the incompatibility to TReLU function as drawbacks. The main contribution of this research is the introduction of the division layer to solve these problems. As a result, the network became able to capture the relative position, and the compatibility with the TReLU function was improved.\n\nThe problem to be solved in the paper is important and interesting, but the justification of the proposed method is slightly weak. The reasons are described below.\n\n1. The purpose of this study is to build a complex scale invariant classification model. However, the proposed method does not show in a theoretically convincing way why it is possible. Why does it happen just by adding a Division layer?\n\n2. This is a question. I think one of the easy way to build a complex scale invariant model is data augmentation. This simply inflates the training data with arbitrary complex scaling. I think another easy way is normalization. For example, the total power of all pixel values ​​is set to 1, and the total angle is set to 0. Is it no good in that way?\n\n3. The experiment is not very complete. Only the experimental results on one dataset are shown. Since the subject of this study is a complex-scale invariant model, it is advisable to prepare an application example in which such a model is significantly useful. It would be even better if we could prepare experimental results that clearly show the effect of the Division layer. It would be nice to be able to go a little further and experiment with the complex scale invariance of the construction model.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}