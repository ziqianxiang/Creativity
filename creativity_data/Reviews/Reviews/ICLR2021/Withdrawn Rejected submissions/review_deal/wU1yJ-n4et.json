{
    "Decision": "",
    "Reviews": [
        {
            "title": "I tend to reject to this paper due to its limited technique novelty and inadequate ablation studies, though the authors proposed a different pair-based self-distillation method for SSDA.",
            "review": "In the work, the authors propose a pair-based self-distillation method for semi-supervised domain adaptation. The teacher set consists of the labeled source domain D_s and the labeled target domain D_{TL} with a limited number of labeled samples per class, while the student set is the unlabeled target domain D_{TU}. They randomly produce teacher-student pairs with the same class label and perform self-distillation by minimizing the distance between their predictions.\nRealizing that the class label in the student set is not accessible, they pseudo-label the unlabeled target dataset and use reliability evaluation to somewhat generate more accurate pseudo labels. Compared with the existing proxy-based approaches, this paper introduces a different solution. Empirical results show that the proposed method works well in some datasets.\n\nPros: \n1.\tCompared with the existing proxy-based approaches, this paper introduces a different solution of pair-based self distillation for semi-supervised domain adaptation.\n2.\tThe proposed method is simple, making it easy to understand and implement. This paper is well-written and easy to follow, though some important explanations are missing.\n3.\tEmpirical results show that the proposed method works well in some tasks on the standard benchmark of DomainNet and Office-Home.\n\nConcerns: \n1.\tMy major concern is about the pseudo label. Similar to other methods with the pseudo label, the problem of the wrong label will be harmful to the learning. The proposed reliability evaluation will still fail since the deep neural networks have been previously shown to face up the problem of miscalibration, i.e., they are over-confident [1]. To this end, we can not guarantee that the pseudo labels are correct even with the reliability evaluation. \n2.\tAnother concern is that Equation 6 is actually the standard entropy minimization loss which has already been successfully applied into unsupervised domain adaptation in RTN [2], but the authors rebrand it and name it as a weighted cross-entropy loss (WCE). \n3.\tMeanwhile, even if we bypass the novelty of WCE loss, more analysis and explanation about this loss is necessary. Why the loss is weighted in this way and how much benefit will be gained by applying the weighting term.\n4.\tAs for the experiment section, the ablation studies are not comprehensive.\nFor example, the authors regard reliability evaluation as one of their major contributions but the ablation study to verify the superiority of reliability evaluation over the simple confidence threshold strategy is missing. Further, it will be better to develop ablation studies on weighted cross-entropy loss (WCE) as mentioned before.\n5.\tIn summary, if we think about the concerns above, the technique novelty of the proposed method becomes weak and limited. Some elegant and novel changes are needed to make this submission meet the bar of this top-tier conference.\n\n[1] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In ICML, 2017.\n\n[2] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In NeurIPS, 2016.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lacking Experiments",
            "review": "\nSummary: This paper proposed pair-based self-distillation with a student network for semi-supervised domain adaptation. The method first generates pseudo-labels for the unlabeled target data and purposes pairing loss based on ground-truth labels and pseudo-labels. The paper also uses weighted cross-entropy to give more weights on reliable samples.\n\nStrengths:\n+ The paper is well written and easy to understand.\n+ The method is simple and easy to implement.\n+ The paper uses two standard DA benchmarks and outperforms SOTA methods.\n\nConcerns:\n- The major concern is that the novelty of the paper is limited\n- The major contribution of this paper is RSS. However, the improvements seem to be marginal in Table 2. The improvement is expected to be smaller in ResNet.\n- It is not clear how the proposed pseudo-labeling works differently than existing pseudo-labeling methods. It is recommended to have a pseudo-labeling method in UDA as a baseline to show the difference.\n- Low-confidence samples will not be aligned with the source domain.\n- How many times did the experiments run? (what are the standard deviations?)\n- While most of the results use AlexNet, how are the results for ResNet 34 for Tables 2, 3, and 4?\n- No analysis with more known target samples (e.g., 5,10, and 20-shots)\n- No theoretical analysis\n\nAs mentioned above, the major concern is the limited novelty as some components such as pseudo-labeling are used in prior domain adaptation works. And, more thorough experiments should be performed to see the clear gain of the proposed components (e.g., multiple runs). From the current results, it is not clear the benefit of the proposed method. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "an incremental method for semi-supervised domain adaptation",
            "review": "This paper proposes a method for semi-supervised domain adaptation (SSDA). \nThe main idea is to discover high-confident target samples via thresholding, and then leverage them in the pairwise similarity term (the so-called self-distillation) and the weighted pseudo-labeling term.\n\nStrength:\n1. new method for SSDA with better results than MME\n2. this paper is well written and easy to follow\n\nWeakness:\n1. the novelty is limited since both pairwise similarity term and weighted pseudo-labeling is widely used in the pattern recognition field. The selection of high-confident target samples is also common, if the threshold $\\alpha$ is really set to 0.95, is the second condition on margin threshold$\\delta$ still effective? \n2. the results on SSDA are not fair, since most experiments are performed with the AlexNet backbone and 1-shot setting except Table 1. It seems that the proposed method is not much effective in ResNet34 with a 3-shot setting.\n3. besides, the ablation study in Table 4 shows that the improvements by introducing weighted pseudo labeling and student-set generation (RSS) are somewhat small. the authors should list their baseline method S+T in this table for fair comparisons.\n4. some related work are missing e.g. [a,b]\n\n[a]. Xu, Xiang, et al. \"d-sne: Domain adaptation using stochastic neighborhood embedding.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.\n[b]. Motiian, Saeid, et al. \"Few-shot adversarial domain adaptation.\" Advances in Neural Information Processing Systems. 2017.\n\nOverall, the proposed method is reasonable and expected to work well for SSDA, but the contribution to this field is somewhat limited and hard to meet the standard of ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}