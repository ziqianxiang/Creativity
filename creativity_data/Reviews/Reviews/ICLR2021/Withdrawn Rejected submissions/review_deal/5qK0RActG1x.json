{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers all found that the Consensus method introduced seemed sensible and applauded the authors on their extensive experiments.  However, clearly they struggled to understand the paper well and asked for a clearer and more formal definition of the methods introduced.  Unfortunately, the highest scoring review was also the shortest and also indicated issues with clarity.  It seems like the authors have gone a long way to improve the notation, organization and clarity of the paper, but ultimately the reviewers didn't think it was ready for acceptance.  Hopefully the feedback from the reviewers will help to improve the paper for a future submission."
    },
    "Reviews": [
        {
            "title": "Official Blind Review",
            "review": "The paper seems to introduce a very important model to evaluate intepretatbility of neural networks.\nHowever, the paper is not extremely clear and intepretable.\nThe main idea is to pool together interpretations coming from different systems and then selecting the best interpretation by voting. The procedure and the model is described in a single section, that is, section 2. It is extremely obscure the way Consensus operates. A running example can be very relevant in this section. Without this running example, it is difficult to attract readers into your paper.\nFor example, one between Figure 4 and Figure 5 could be this running example? Can you use it to explain your model?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents an interesting approach for unlabelled evaluation of ML explainers, but (i) since the contribution is largely based on the extensive evaluation, the paper needs several improvements in terms of design choices and (ii) the usability of the proposed method (e.g. in terms of robustness evaluation of the model or quality of the explainer) should be better clarified. ",
            "review": "The paper deals with explainable machine learning in the supervised setting and especially tackles the case where no ground truth data for evaluating the generate explanations, such as bounding boxes for objects, is available. The proposed \"Concensus\" approach retrains established architectures on the target dataset and averages their generated explanations from out-of-the-box explainers, such as LIME or SmoothGrad. The approach is evaluated for image classification on ImageNet and CUB-200-2011 by comparing the averaged explanations of the committee models when using LIME and SmoothGrad with the ground truth bounding box and segmentation, respectively. Minor evaluations are included for datasets Stanford Cars 196, Oxford Flowers 102  and Foods 101. The results show that the averaged explanations strongly correlate with the mean average precision with respect to the label distances.\n\nThe approach tackles the interesting setting of evaluating explainable ML approaches - here without labelled objects for computer vison tasks. In general, I find it interesting to evaluate such kind of ensemble-based approach for generated explanations, which is a common technique for adding model robustness or approximating uncertainty, and is closely related to bootstrap aggregation. To this end, the authors provide a quite large variety of experiments for different established vision benchmark datasets, including ablation studies on varying the committee size or randomly generated committees. However, I am left with several open questions for the approach, which I summarize below. In general, however, I am missing the added value of the approach with respect to the evaluation of explainable ML, as it remains unclear to me (based on the approach description and conducted evaluation) if the proposed tool can actually replace empirical evaluations in research or, even more important (and possibly intended by the authors) could support end-users in evaluating their models via out-of-the-box explainers.  \n\nI am wondering why the networks always need to be trained from scratch for a new task/dataset? Wouldn't the approach work for standard transfer learning? Here, it would be interesting to explore smaller, challenging datasets as well. \n\nAlso, why is the use of \"known models\" for buliding the committee emphasized? I am aware of prior works which argue that activation-based explainers are bias from the underlying architecture, but the training procedure should have significant impact on the \"explanability\" (at least as measured by approaches such as SmoothGrad) as well. Wouldn't it make sense to vary the training (or additional fine-tuning) parameters/used augmentation strategies as well?\n\nAs the paper highlights the unlabelled evaluation setting, I am missing related work in the field of explainable / interpretable ML, especially on counterfactuals. Approaches such as [1,2] generate such counterfactual explanations by altering the image towards other, potentially risky classes. Could you comment on the relation of the proposed approach to these kinds of works? \n\nFor the evaluation, the authors use labelled datasets for empirically validating their claims. I am wondering - and this might be a general point for such an evaluation - why/how the explanation component can be evaluated independently of the model, as imperfect explanations might uncover model weaknesses. Is your evaluation affected by this? In addition, as mentioned before with respect to the applicability of Consensus to transfer learning, I am wondering if the evaluation should cover cases where the network might overfit or misperform. Would your approach potentially rule out possibly interesting misbehaviors of a network in such a case? Lastly, could you provide design choices for the evaluation parameters, such as committee size or number of sampled networks?\n\nReferences:\n[1] Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D. and Lee, S., 2019, May. Counterfactual Visual Explanations. In International Conference on Machine Learning (pp. 2376-2384). \n[2] Liu, S., Kailkhura, B., Loveland, D. and Han, Y., 2019, November. Generative Counterfactual Introspection for Explainable Deep Learning. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP) (pp. 1-5). IEEE.\n\n+++ Updates after author response +++\nI want to thank the authors for their answers as well as their attempts to improve the manuscript. I have read the other reviewers' comments and the updated version of the paper. The latter improves on the clarity of the approach in terms of the formal presentation of the approach, but a concise problem definition is still missing in my opinion. \n\nI feel that the paper still needs to additionally quantify who many labels are saved compared to classic cross-validation, as the models still need to be trained from scratch or fine-tuned. Here, it is important to establish when the method is actually (guaranteed to be) sufficiently concise, such that it can be used in practice. I therefore keep my tendency to reject the current version of the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some aspects need to be addressed",
            "review": " The manuscript proposes \"Consensus\" an evaluation method to measure the interpretability of a given deep model without the need of a dataset with annotated ground-truth concepts.\n \n The proposed method works by leveraging a set of pre-trained deep models (committee), and a reference model explanation method. The explanation method is use to produce explanations for every image/sample on a target dataset, and the aggregated explanation from every model in the committee is considered as \"approximated/quasi ground-truth\". The interpretability of a given model is defined as the mean similarity between all the explanations produced by the model in question and the quasi ground-truth (aggregated explanation) produced by the committee.\n \nStrong points\n- Experiments cover a good amount of deep models.\n- An ablation study is in place.\n\nWeak points\n- Terminology is ambiguous.\n- limited number of model explanation methods.\n\nOn the stronger side, the evaluation of the proposed method is relatively good, it includes an ablation study considering several relevant factors of the proposed method, e.g. committee size, and the effect of the members that are part of the committee.\nMoreover, it validates the proposed method considering a large amount of deep models. \n  \nOn the weaker size, there are several aspects that I feel need to be addressed:  \n\nI had some difficulties while reading the manuscript. The use of specific terminology is quite ambiguous at times. In several parts of the manuscript there are terms like \"variety of interpretability of models\", \"interpretation result\", \"interpretability evaluation\" that are not standard and hinder the content to a good extent. I would suggest very early on the manuscript defining these terms so that their meaning is clear much later in the body of the paper. \n\nVery related to the previous point, the tasks of model interpretation (identifying the information/features encoded internally in a given model) and model explanation (justifying the predictions made by a given model on a given sample) seem to be used interchangeably. In my opinion, this is another strong source of confusion.\nThis should be addressed when discussing methods like LIME and SmoothGrad which are model explanation methods. \nMoreover, this seems to actually point to the definition of one of the ambiguous terms mentioned in the previous point. It seems that \"interpretation result\"  actually refers to the explanation produced by a given sample.\n\nThe main characteristic of the proposed method is its independence on annotated ground-truth concepts. However, as admitted by the manuscript, there is still the requirement of having the data needed to train the models in the committee. This still sounds like a expensive requirement. \nIn addition, at this point it is not clear how the proposed consensus method would apply to a task/application (e.g. medical imaging) where data is scarce and deep models are not available in abundance. Discussing this direction would strengthen the manuscript.\n\nIn the comparison against the Network Dissection method (Sec. 3.3), it is stated that the rankings produced by the proposed method are quite similar to those of Network Dissection, but with differences between the ResNet152 and DenseNet161 models. Is it possible to get an insight on the source of this difference? Is it possible to know why the proposed method prefers one over the other? \n\nWhen assessing the effect of the size of the committee (Sec. 6), what if just few specific models are responsible for the observed performance? then the number of randomly selected models might not be so relevant as the selection of these few models. Perhaps an incremental approach when models are gradually integrated in the committee should be analysed. \nThis setting, of few models determining the performance, seems to be suggested by the change in mean and standard deviation on performance in Fig.7. For committees with few models (4-9) the large standard deviation pushes performance (of some random committees) closer to the upper bound (0.70). The consideration of committees with larger sizes, and thus the likely potential inclusion of these relevant models, just makes the mean performance converge faster. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited technical contribution, not very clearly explained.",
            "review": "The authors propose a novel method, called Consensus, using an ensemble of deep learning architectures, to study the interpretability of models, when ground truth of interpretations is not available.\n\nThe high-level idea of the paper is promising, i.e. evaluating interpretability without the need for human-labelled groundtruth, which is tedious and expensive to collect, and for this task, can be subjective. \nThe proposed method consists of three stages: forming a committee of deep models, aggregating the results in “quasi ground-truth”, and ranking models of the committee based on the similarity with the quasi ground-truth.\nHowever, it is not clear from the paper how the “committee voting” is performed (is it averaging, majority voting), and what does the method bring in addition to just an ensemble of multiple CNN architectures.\n\nGreat points of the paper:\n- using pretrained models to overcome the lack of ground-truth (which would be subjective and costly to collect).\n- comparison of the consensus results with several architectures, and consensus vs ground truth (via mAP).\n\nPoints on which the paper can be improved:\n1. Clarity of description of the method \n- there is no clear formal definition of how the committee voting works; adding formulas would improve readability.\n- no explanation on how RBF is used to address the dimensionality differences when comparing scores; \n- it is difficult to understand from the paper how the numbers (interpretability scores) are obtained (Sec. 3.3.); please add equations and reference them in the text - especially for correlation score and aggregating scores.\n- please clarify how the significance tests were performed\n- Since the figures are cluttered, it might be worth considering reporting the per CNN scores in a table (few models in the main paper) and the rest in the appendix. \n\n2. Experimental comparisons with other methods \n- the proposed method compares on object domain only, ImageNet and CUB200-2001 (birds). The method compares with Network Dissection (Bau et al, 2017), but no results on Broden dataset are reported. This dataset is particularly interesting, as it is unifying several datasets, spread across multiple domains (objects, scenes, objet parts, textures and materials).\n- Why is it necessary to have the models trained from scratch? (Sec. 2, committee). What would change if the models would be just finetuned?\n- What does the current method bring in addition to LIME and SmoothGrad aggregated over a large number of architectures?\nDatasets as MS-COCO / LVIS provide ground truth labels for object segmentation, which are more diverse than CUB / ImageNet.\n\n3. Style / visuals:\n- axis labels and text on the plots are too small;  pick a smaller number of models, the figures are too cluttered. The full comparison could be in tables, in appendix / supplementary material.\n- “significance tests” instead of “significant tests”.\n- are the 4 decimals in the Pearson coefficient necessary? (Fig. 1 caption) \n- please report the exact p-value.\n- Fig. 5 - please overlay the contour of ground truth over the other visualizations.\n\nThe overall idea is simple - ensemble of CNN architectures, to generate quasi-ground-truth for interpretability. Not clear what \"interpretability\" is -- and how this would be different from object segmentation masks.\nThe paper would greatly benefit from a more formal explanation of how the models are scored, and how the consensus is computed.\nIt would add value to emphasize more how the proposed method could leverage existing trained models for datasets on which such labels are non-existent;\nIt would also prove the effectiveness of the method, if at least one more datapoint would be reported, on datasets with object labels (e.g. MS-COCO / LVIS).",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}