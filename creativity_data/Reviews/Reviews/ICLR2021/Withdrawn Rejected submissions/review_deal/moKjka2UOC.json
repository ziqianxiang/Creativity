{
    "Decision": "",
    "Reviews": [
        {
            "title": "Limited Novelty",
            "review": "The paper tackles the semi-supervised domain adaptation where few target labels are available. The paper calculates prototypes used in few-shot learning for domain alignment. In addition, the paper combines consistency alignment using strong and weak augmentations on the target domain. While the method outperforms state-of-the-art methods by a large margin, the method combines existing techniques in prior works. Therefore, the paper lacks novelty for acceptance.\n\nStrengths: \n- The paper is well-written and clear.\n- The method obtains SOTA results on the three standard benchmarks.\n\nWeaknesses:\n- The major weakness is a limited novelty. The proposed components look similar to previous works and the novel contribution should be further clarified.\n- Prototype-based domain alignment was proposed in prior work. E.g., [1, 2].\n- Consistency alignment/regularization with strong and weak augmentations is also proposed in [3]. And augmentation techniques are also not new.\n- While DomainNet contains 345 categories, MME and FAN use 126 categories only for evaluation. Is it evaluated in the same setting?\n- There is no analysis of hyper-parameters in augmentation techniques. The paper will be more interesting how/what kinds of data augmentation are useful for domain adaptation, not simply utilizing the existing augmentation techniques.\n- How does consistency alignment work on UDA?\n\n[1] Xu, Minghao, et al. \"Cross-domain Detection via Graph-induced Prototype Alignment.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n[2] Pan, Yingwei, et al. \"Transferrable prototypical networks for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[3] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" arXiv preprint arXiv:2001.07685 (2020).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A novel and effective way to exploit landmarks in semi-supervised domain adaptation problem",
            "review": "Summary of the work:\nIn this paper, the authors argued that the previous works on semi-supervised domain adaptation problem did not fully utilize the precious landmarks, and proposes a novel and effective way to exploit the landmarks by using prototypical alignment with data augmentation and consistency learning. The proposed Prototypical Alignment and Consistency Learning (PACL) framework could integrate with most existing unsupervised domain adaptation models and enhance adaptation performance. The authors built the PACL framework upon MME and outperformed state-of-the-art methods on three commonly used datasets.\n\nWeakness:\n- This paper is not innovative enough. The ideas of prototypical alignment, data augmentation and consistency learning are all proposed in other papers, and the author just combined them together then applied them to the semi-supervised domain adaptation problem.\n- There is little theoretical discussion about why prototypical alignment is beneficial to domain adaptation. The authors just claimed that class-level domain alignment could be achieved in this way. However, I am interested in why prototypical alignment is a better choice than other class-level domain alignment methods. I think there should be more theoretical discussion about it.\n- The authors did not elaborate how do the balancing hyper-parameters of different losses affect the model performance. Instead, they just gave a set of balancing hyper-parameters they used.\n\nStrengths:\n- The combination of prototypical alignment, data augmentation and consistency learning is effective for semi-supervised domain adaptation problem, and achieves significant performance gains over state-of-the-art methods on three commonly used datasets.\n- The plug-and-play evaluation part proved that the proposed framework could incorporate most existing UDA methods and improve adaptation performance.\n- This paper is well written and very clear. The problem-setting and concepts are described clearly and easy to understand. The experiments are complete and convincing.\n\nAbove all, this paper is short of innovation and theoretical discussion, therefore I think it is marginally below the acceptance threshold.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "SSDA - PACL Paper review",
            "review": "The authors proposed a new semi supervised domain adaptation (SSDA) using prototypical alignment (PA), augmentation techniques and consistency alignment modules. The basic idea is 1-shot or 3-shot labelled images of one class in target domain is pushed into the  source domain and aligned within the same class (PA). Then several data augmentations are applied to address overfitting as well as enforcing a consistency regularization modules. \n\nPros:\n1- The model has novelty over existing SSDA frameworks by introducing a PA as well as applying strong data augmentation techniques to improve the predictive performance. \n2- The method has comparative results against to existing SSDA and UDA models and the proposed method (PACL) has improved the SOTA results. \n3- Various benchmark analysis was experimented using three different datasets.\n\nCons:\n1- Office home experiment is missing. Authors stated that there are 4 domains were used for the experiments, however which domain adaptation was not mentioned. E.g. Real to clipart? or Art to product?\n2- The number of images used in DomainNet and Office-Home are missing?\n3- An ablation work can be studied based on which data augmentation method gives the best 1-class classification results.\n4- Figure 1 can be re-illustrated as it is not clear to give the systematic approach.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea for semi-supervised domain adaptation; several open questions;",
            "review": "**Summary:**\nThis paper deals with semi-supervised domain adaptation where a labeled source domain data set is available as well as a target domain data set that is split into a small amount of labeled and large amounts of unlabeled data. The authors propose a prototype alignment-based prediction model where the distance between features and the per-class target domain prototypes dictate the model prediction. Motivated by prior work on semi-supervised learning, the authors also propose to leverage data augmentations to (a) better align the two domains and (b) regularize the unlabeled target domain via pseudo labeling.\n\n\n**Pros:**\n- I think the authors propose a neat idea how to use very limited labeled target domain data for domain adaptation.\n- The overall paper is well structured and written, except for some inaccuracies in the notation (see below).\n- The experimental results are really good. (Although it seems the biggest gains come from the augmentation techniques on both labeled and unlabeled data, that have already been proposed in prior work.)\n- The ablation study in Table 4 is great and informative.\n- The plug-and-play evaluation of combining PACL with existing UDA methods is good. It would be interesting to see how it combines with more diverse methods for UDA.\n\n\n**Cons:**\n- It seems Eq. 4 is missing a norm! I would assume $f(s)$ and $c_y$ are vectors, not scalars. Plus it should then probably be $\\exp(-norm(f(s) - c_y))$?\n- I think the notation in Eq. 5 is wrong, specifically regarding the use of $y_s$. $y_s$ is the sampled value from $\\mathcal{S}$, so I guess it should be $y_s \\log [-p(y=y_s|s)]$ inside the sum. (Same holds for Eq. 6.)\n- I think there could have been more baselines evaluated for combining labeled source and target data with different forms of loss weightings or sampling of the data.\n- The technical contribution is somewhat limited to only the prototypical alignment (PA) loss. The data augmentation techniques are essentially taken over from FixMatch (Sohn et al.)\n\n\n**Open questions:**\n- I am somewhat confused about the existence of a cross-entropy (CE) loss on labeled (source and target domain) data in the proposed method. From Sec. 3 it seems there is none. But the statement from the implementation details on page 6 (\"Note that although MME is proposed for SSDA, it can be viewed as an UDA method that naively merges labeled target samples into labeled source samples for the cross-entropy loss optimization.\") suggest that there is a CE loss as an artifact from the method build on (MME). If there is no CE loss, is there no other term that tries to increases the distance between the landmark prototypes? Or is this automatically handled with Eq. 5? And what would happen if the target labels are not part of any cross entropy loss?\n- Related to the previous question, how is the model actually used at test time? Are the prototypes required and stored from training time?\n- Are gradients back-propagated to the features of landmark inputs, i.e., from Eq. 5 to Eq. 3?\n- I am wondering how sensitive the performance is with respect to the choice of the landmark(s). I understand the landmarks are essentially given with the experimental setup, but this sensitivity analysis would still be valuable I think.\n- Is this transductive setting (page 3: \"evaluate the model on $\\mathcal{T}_u$\") the default for prior art as well? In other unsupervised domain adaptation techniques there is a separate set of unlabeled data for training the model and testing it.\n\n\n**Minor:**\n- Page 4: \"by explicitly pushing source samples towards target samples that from the same class.\" is missing a word? \"... towards target samples that ARE from the same class.\"?\n- If the method also uses CE loss on labeled source domain data, it should be mentioned in Fig 1.\n- The paper FixMatch (Sohn et al.) is a NIPS'20 paper now.\n- It would be very helpful to have references for all methods in all tables.\n- The label $y_i$ is not utilized in Eq. 3 except for the sampling itself, right? The label can be dropped I guess since data is sampled already from $\\mathcal{T}_k$.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting application but with limited novelty",
            "review": "This paper studies the task of semi-supervised domain adaption and proposes the prototypical alignment and consistency learning method. My detailed comments are listed as follows.\n\nPros:\n(1) The writing is good, which makes the paper easy to follow.\n(2) The new task of domain adaption under few labeled samples is interesting and meaningful.\n(3) Experiments look sufficient and the results are good.\n\nCons:\n(1) My main concern lies in the novelty. Both the prototypical alignment [1] and consistency learning [2] have been thoroughly investigated by previous methods. This paper combines these two strategies and applies them to the SSDA task. So the novelty and contribution are limited.\n(2) Please explain the difference between the consistency learning strategy with Fix-match [2].\n\n[1] Prototypical Networks for Few-shot Learning. NeurIPS 2017\n[2] Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NeurIPS 2020",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}