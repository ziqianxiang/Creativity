{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes benchmark tasks for offline reinforcement learning.  The paper has major strength and weakness, and it has resulted in very active discussion among reviewers, authors, and other participants.\n\nThe major strength includes the following:\n- The proposed benchmark is already heavily used in the community\n- Offline reinforcement learning is very important to solve reinforcement learning tasks in the real world\n- The paper covers a range of tasks and provides through evaluation of existing methods to be used as baselines\n\nThe major weakness is that it is not sufficiently convincing that the methods that perform well in the proposed benchmark tasks will perform well in the offline reinforcement learning tasks in the real world.  \n\nThis is partly due to the nature of the benchmark tasks of offline reinforcement learning, which require simulators to evaluate the policies learned with offline reinforcement learning.  This means that one cannot simply collect datasets from real world tasks and provide them as benchmark datasets.  \n\nAlthough one cannot do much about simulators, benchmark tasks for offline reinforcement learning still have many design choices.  In particular, how should the datasets in the benchmark be collected (i.e., behavior policies)?\n\nWhile the datasets in the proposed benchmark are collected with various behavior policies including humans, it is not necessarily convincing that the resulting benchmark tasks are good for the purpose of evaluating offline reinforcement learning to be used in the real world.\n\nIn addition to the suggestions given by the reviewers, a possible direction to improve the paper is to focus on the choice of behavior policies used to generate the datasets in the proposed benchmark.  One might then be able to provide some convincing arguments as to why performing well in the benchmark might imply good performance in the real world by relating it to the choice of behavior policies."
    },
    "Reviews": [
        {
            "title": "Premature or high time?",
            "review": "Summary:\nIn this paper a test suite of data sets and corresponding benchmarks for offline reinforcement learning is introduced.\nSeveral existing RL benchmarks are used, the results of several algorithms are presented.\nThe authors claim that the benchmarks were specifically designed for the offline setting and are guided by the key properties of datasets in real-world applications of offline RL.\n\nStrong points:\nThe present paper has already been cited and the benchmarks suite has already been used by other publications. Obviously there is a need for offline RL test suites.\n\nWeak points:\nThe authors' claim that such a benchmark for offline RL should \"be composed of tasks that reflect challenges in real-world applications of data-driven RL\" is only partially met by the paper in its present form. The area of robotics, with deterministic dynamics, is comparatively well represented, but there is no real, industrial application. In particular it seems that so far no benchmark has been included that has the ambition to have the characteristics and complexity of a real application. \n\nRecommendation:\nOn the one hand, the really realistic benchmarks are missing, so that a publication seems premature. On the other hand, the current status is already used by the research community, since there seems to be no test suite for offline RL apart from \"RL unplugged: Benchmarks for offline reinforcement learning\". I therefore recommend to accept the paper.\n\nQuestions:\nTo what extent are the current benchmarks stochastic?\nAre there bi- or multimodal transition probabilities?\n\nAdditional feedback with the aim to improve the paper:\nIn Table 2 and Table 3 average results are reported over only 3 random seeds. This seems to me to be clearly too little, especially since the policy performance of Q-function based algorithms often fluctuates strongly. Since no uncertainties, e.g. in the form of standard error, are given, the reliability of the results cannot be assessed.\nThe way policies are selected before they are tested should be described more clearly. My impression was that in each case the policy is used that results for a considered algorithm and random seed after 500K training iterations or gradient steps. Since different algorithms require different computational efforts this approach does not seem to be in the sense of a real-world application. In most cases there should be the willingness to use much more computational effort for especially good policies. According to the motto: computing power is cheap, data is expensive. \n\nI like the formulation “Effective offline RL algorithms must handle […] data collected via processes that may not be representable by the chosen policy class.“ This expresses the, in my opinion, correct view of the real situation well, while the assumption that there is a \"behavior policy\" that generated the data is not true in general. It may have been different people at different times who performed the actions while the data set was recorded.\n\nPlease check the bibliography for accidental lower case, like „markov“, „adobeindoornav“\n\n\n----------------------------------\n(Dec 3) Taking into account the other reviews, the authors' responses and the changes made by the authors, as well as the extensive and controversial discussion, I rate the paper still with a score of 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review from reviewer 3",
            "review": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning\nreview:\n\nsummarization:\n\nIn this paper, the authors consider the problems of offline reinforcement learning problems,\n and has a focus of dataset and shareable code base.\nWhile no novel algorithms are proposed in this project,\na systematic evaluation of existing algorithms (offline RL algorithms) is proposed.\n\nPros:\n1. Offline RL is a hot topic this year, \nwith a lot of papers exploring efficient and robust ways to utilize offline collected data.\nThis paper, while using simulated data, \nprovides a general platform to benchmark these algorithms.\n\n2. The project provides a comprehensive evaluation and discussion on existing considerations in offline RL.\nIt provides several interesting directions in offline RL.\n\n3. The paper is well-written.\nIt is very clear what the purpose of the project is,\nand it is very clear why the authors make the dataset in the way they did.\n\nCons:\n\n1. The dataset is mostly simulated.\nWhile I understand the difficulty of collection real-data, \nit does raise some concerns that a simulated dataset can be generated by researchers themselves.\n\nSummary:\nWhile the dataset is simulated,\nI do think there’s value in the dataset and the shared code-base to facilitate recent progress in offline RL research.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Concerns about accessibility for underrepresented groups ",
            "review": "The paper proposes a standardized benchmark for offline RL research. The data collection is well motivated from real world scenarios covering many important design factors. I really appreciate that human demonstration and hand-crafted controllers are also included. The evaluation protocol and the API looks clear and easy to use. The benchmark of existing methods is thorough and provides many useful insights. I believe this work will have a high impact on the offline RL community. I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research.\n\nHowever, I notice that this dataset may not be accessible for underrepresented groups. I therefore vote to reject. As the authors note in the paper, each task consists of a dataset for training and a simulator for evaluation. In my understanding, half of the six tasks (Maze2D, AntMaze, Gym-mujoco) depend heavily on the MuJoCo simulator, which is a commercial software and is not free even for academic use. A personal MuJoCo license costs 500 USD per year. I am concerned that MuJoCo is not accessible for most underrepresented researchers. It is not clear when MuJoCo becomes a dominating benchmark for online RL research, though there are indeed free, open-sourced alternatives, e.g. PyBullet (https://github.com/bulletphysics/bullet3). In online RL, we need the simulator for training. One reason MuJoCo becomes popular may be because it's more stable and faster than PyBullet. However, in offline RL, a simulator is used only for evaluation not for training. So the high reliability of MuJoCo may no longer be so necessary. I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares, making RL research more accessible for underrepresented groups. If accepted, this paper will indeed greatly promote the use of MuJoCo given its potential high impact, making RL more privileged.  \n\nOverall, I really enjoy reading the paper and am glad to see a standardized benchmark for offline RL. I am happy to raise my score if the accessibility issue is addressed, e.g., by using PyBullet as the physical engine.\n\n====================\n\n(Nov 24) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A collection of offline datasets",
            "review": "This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks, including autonomous robotics and driving domains. All domains also come with simulation environments.\n\nWhile the dataset provided by the authors may be a useful resource for researchers in offline-RL, this paper does not introduce any new or novel ideas. Overall the main contribution of this work is the gathering of offline data in one place, reducing the time needed for other researchers to do so. It does not seem as if the authors did any non-trivial work other than annotating the data. Furthermore, there have been many previous work which have already collected offline datasets as part of their work.\n\nI do not underestimate the importance the authors' hard work, nor the importance of the provided datasets to the RL community. Nevertheless, I do not believe this work should be published in a high-end conference without presenting any ideas that are not trivial or known to other researches. \n\nThe authors propose to use the following design factors in their offline datasets: narrow distributions, multi-task data, sparse rewards, suboptimal data, and partially observable policies. While these factors may indeed be good for testing offline-RL algorithms, they do not provide a complete picture of real world datasets. Real datasets present many more real-world problems, including:\n1. Non-stationary data. Many real datasets are non-stationary. The non-stationary behavior could be mimicked or simulated from real behavior.\n2. Real policies. Real datasets don't involve policies that were trained by RL agents. The authors could create datasets that are constructed by real human beings (for example, humans playing atari games, with mixed or different expertise). In a controlled setup, the datasets could be constructed so that policies are categorized (e.g., \"level of non Markovianess`\"). \n3. Causal structures. Real policies may act according to some causal structure in the background that is not necessarily known.\n4. Reduction from real datasets. One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality (e.g., removing certain trajectories). If the initial data is of high quality, the corrupted data could be controlled well.\n5. Changing and/or very large action sets. Real world datasets have changing and large datasets. As an example, consider ad placement, or text based tasks.\n6. Non-robotic environments, including games but also real world problems.\n\nIf the authors bring together datasets that generate original ideas that have never been previously explored, then I believe it's more likely that this paper could be accepted in future venues.\n\n\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}