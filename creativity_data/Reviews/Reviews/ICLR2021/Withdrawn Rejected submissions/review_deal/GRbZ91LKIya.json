{
    "Decision": "",
    "Reviews": [
        {
            "title": "Token-level information is certainly relevant, but current presentation is not very clear on how/why that helps",
            "review": "*Summary:*\nThe paper presents a transformer based model that learns alignment between videos and their descriptions. Emphasis is placed on grounding individual concrete words (tokens) such as nouns and verbs, rather than the entire sentence representation. A token-level contrastive loss is proposed that works in conjunction with the global loss (operating at video/sentence level). Thorough experiments are performed on video-text retrieval datasets (YouCook2, MSR-VTT) and action localization (CrossTask), with different feature combinations, pre-training and fine-tuning.\n\n*Strengths:*\n1. Token-level information seems important to make the model focus on individual words. Although, additional analysis and comparison to related works would be useful.\n2. Experiments are quite thorough and use of same features/backbones facilitates proper comparisons. Results are encouraging, especially, there is almost always an improvement in performance from lambda=0 (only global loss) to lambda=0.5 (token + global loss).\n\n*Weaknesses:*\n1. While focusing on token-level information is useful, it is not clear that a BERT like model is not already doing that (to some extent)? The primary indication for this in the paper is difference in performance for lambda=0 and lambda=0.5. I would really like to see better qualitative analysis of how the token-level information is helping. For example, in Fig. 3 (a woman is stirring food), how does the model decide to focus on the \"stirring\" token rather than \"woman\"? Would a query like \"chef stirring food\" also focus on \"stirring\" or the \"chef\" takes priority? Understanding which words drive the retrieval results when using global loss, and how does that shift when using ToCo would be important.\n\n2. A second concern is with respect to the specific dataset used for pre-training. As shown by [Miech2019] and [Miech2020], the HowTo100M video-text pairs are not always highly correlated (only about 50% of them align well). Doesn't this make enforcing the similarity between specific tokens that are not even displayed in the video challenging? Could this be a reason for the large gap in performance with MIL-NCE-t (apart from model size) in Table 2? I feel this point is in direct conflict with the introduction last paragraph: \"Fundamental to our approach is token-level contrastive losses which force the grounding of individual words.\" - perhaps this forcing is too much?\n\n3. Token-level information has been used before for learning improved video-text embeddings. [A] uses part-of-speech specific embedding spaces, while [B] places an emphasis on adverbs. While they are not in the context of a transformer model, these should nevertheless be discussed. The pre-training paragraph of the related work should also emphasize on how this work compares with the others listed there.\n\n4. Questions about token weighing:\n(i) I didn't understand why using a uniform weight on all tokens would be equivalent to no-loss.\n(ii) Wouldn't TF-IDF act as a stop-word filter all by itself, why is a separate one necessary?\n(iii) The \"Worcestershire sauce\" example in Fig. 1 seems way too specific. How are we (or the model) to know that the sauce is Worcestershire, at least based on the frames shown in the example? In fact, I imagine that such rare words might create weird artifacts, especially in conjunction with the fact that not all descriptions actually match visual content (see point 2).\n(iv) Are POS tags actually used in the implementation?\n\n5. Experiments:\n(i) I did not find an explanation of \"Our baseline\". Could you please point to it?\n(ii) Weighting ToCo loss. lambda=1 seems to indicate global loss is 0. However, Eq. 3 suggests this is not the case?\n(iii) Results on CrossTask are presented very hurriedly. Details about what lambda is used here, etc. are omitted. Please include them in the supplementary material.\n\n6. Several minor typos:\n(i) First line of section 3.2, is a bit misleading as to what is a positive pair $p(x_i, y_i)$, or $p(x_i, y_j)$. This is fairly important as it can derail further understanding if the reader is not familiar with the general vision-language embedding task.\n(ii) The math just after Eq. 1, would be nice to use the same sum over $j \\neq i$ instead of $k \\neq i$.\n(iii) (minor) In general, vectors are normally column vectors. Writing $xy^T$ would indicate an outer product in that case.\n(iv) Fig. 2b, caption \"Node\" --> \"Note\"\n(v) In hard negatives, \"At the meaning time\" should probably be \"At the mining time\" or training time?\n(vi) Sec. 5.3, \"whcih\" --> \"which\"\n\n*Overall rating:*\nWhile the idea of using tokens seems promising, there are several issues to clarify in this work. The results are encouraging, however, a deeper analysis and understanding of why / when the performance improves would be highly desirable. I would be happy to increase my rating upon reading the response.\n\n[A] M. Wray, et al. Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings. ICCV, 2019.\n[B] H. Doughty, et al. Action Modifiers: Learning from Adverbs in Instructional Videos. CVPR, 2020.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising idea for video&text representation learning but deeper insights lacking",
            "review": "=Summary\nThe paper presents a new training objective for learning video&language representations. While prior work often considers “summary” sentence representations when designing contrastive objectives, the authors propose a token-level contrastive loss. The idea is to force token-level prediction of whether a sentence and a video match; for that each relevant token is represented with a combination of its initial (not contextualized) representation and its final (contextualized) representation. The authors compare to recent methods and masked language modeling (MLM) and show promising results on 3 benchmarks, both in task-specific training and pre-trining scenarios.\n\n=Strengths\n\nThe motivation and core idea of the proposed method makes sense; intuitively, it is clear that additional token-level losses should make the model “work harder” and thus lead to better overall representations. \n\nThis method is novel to the best of my knowledge.\n\nThe fact that it seems compatible with a variety of visual representations supports the generality of the proposed method.\n\nEvaluation includes both task-specific training and pre-training scenarios and covers 3 evaluation benchmarks, achieving promising or state-of-the-art results across the board.\n\n\n=Weaknesses/High-level comments\n\nI would like to see more discussion on comparison to MLM, both conceptually and empirically. \n\nI would also like to hear why the authors believe their method leads to better concept grounding. \n\nMoreover, I am wondering how the proposed token-level losses actually impact the learned token representations and grounding associations. It seems that these losses may force each token representation to capture the semantics of the entire sentence, in order to solve the task. Do the authors have any insights into this?\n\nA conceptual “apples-to-apples” comparison to recent methods is missing; it would be helpful to highlight the core distinctions between the proposed method and existing methods.\n\nThere is a number of issues with presentation/clarity and overall writing (see below).\n\nSeveral model ablations are missing (see below).\n\nNothing was stated about making the code available.\n\n=Detailed comments\n\nPresentation/Clarity\n- Fig 1 seems to suggests that an explicit token-level grounding is being learned, this seems misleading and is not  supported empirically. \n- P4 “A uniform weighting would be equivalent to no-loss”: why? this is not obvious.\n- Are the TF-IDF weights obtained at clip level? Are they used to weigh each token’s contribution within the loss? An updated equation would be appropriate. \n- P6: “sample 8 negative samples either using random sampling or our hard sample mining techniques” : does it mean that both types of negatives are used jointly?\n- What is “Our baseline” is Table 1? The text suggests that it refers to the version with lambda=0. But that is a separate entry in the table. \n- Figures 4, 5: what is ToCo-base?\n\nExperiments \n- No empirical comparison to VideoBERT (Sun et al. ICCV 2019).\n- How significant is the effect of fine-tuning the linguistic representation h (BERT-base)? Do all the compared methods fine-tune their linguistic representations?\n- No ablation was provided for the proposed TF-IDF token weighting.\n- No ablation was provided for the proposed hard-negative mining scheme.\n- Are the results in Table 1 on the corresponding validation or test sets? Have the authors used the validation sets to select the hyper-parameters?\n- Figure 5: rather than showing noun/verb/adj vs. det/adp/aux it would have been more informative to show the result with (1) all of these token types used together, and e.g. (2) only verbs, (3) only nouns.\n\nWriting (P# - page number)\n- There is quite a few cases of typos/poor grammar in the paper, a few examples: P1 training object(ive?); P2 is(are?) token-level contrastive losses; P2 to fight(?) to rank; P3 inputs(input?) is passed; P3 is far(not?) enough; P4 cirled, P4 upper(?) multi-modal encoder; P4 At the meaning(?) time; P5 a features (feature) map; P8 two layer(s) of, P8 “performs competitively or outperforms comparable models.” repeat twice, P8 between text to(and) video, P8 Fro, whcih etc.\n- P2 “NCE examples” - the term was not yet introduced\n- The usage of words “clean/cleaner” is questionable: P2 “clean grounded alignments”; “A cleaner, simpler, and more effective approach”.\n- Eq (2): missing comma between x_i and y_i\n- P4 “global lexical type”: this seems to just refer to a word token? Why call it “global lexical type”?\n- P5 “we verify its effectiveness”: “it” was not defined.\n- What is #L in Table 2?\n- Figure 3: what is the “baseline” here?\n- Figures 4 and 5 are too close to each other.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea but the paper requires further work",
            "review": "## Summary of the paper\n \nThe paper proposes a new objective for training joint representations of video and text. In details, it proposes to train multi-modal transformers that takes as input a concatenation of a sequence of video features and a sequence of token representation by combining a global matching loss between the input video and text as well as a token that tries to predict the matching between the video and a specific token in the input sentence.  Both losses are based on a contrastive formulation that imposes that the scores for positive pairs are higher than the ones for negative pairs. The main novelty of the work lies in the proposed token level loss (ToCo) that differs from the token level loss that is usually used with these type of models: the Masked Language Modeling loss.  The authors evaluate their method on the YouCook2 and MSRVTT benchmarks.\n\n## Strengths\n\n* The ToCo loss is well motivated and the idea is interesting. In particular the results seem to indicate that care should be taken when selecting which words to sample for the loss. The loss seems to also lead to better results than the MLM loss (Fig. 4.).\n\n## Weaknesses\n\nThe overall writing quality of the paper is not great (as detailed below) and the clarity should be improved.\n\n**Hard negative mining is not ablated**: hard negative mining is introduced but it's not clear if it helps or not in the final results? What k is used in practice? \n\n**Comparison against MLM**: This is an important comparison for the paper (as ToCo is a direct replacement of the MLM loss) and I feel the result should be integrated in a Table rather than just a plot. This would help better feel what is the improvement brought by ToCo.\n\n**Clarity of the model description:** the description of the model and notations are not always clear:\n\n- How is working with the conditional more tractable than the joint probability? (both require to normalize over a very large number of points + subsampling as done in NCE can be applied to both the joint and the conditional no?). Is there another justification why the joint probability is preferred here?\n\n- How the two losses are exactly combined? (3) seems to contradict the description in 5.1. Weighting ToCo loss (where $\\lambda=1$ corresponds to no global component).\n\n- It is not clear how the inference score is computed (when doing retrieval). In particular how do you use the token level alignement exactly (the description below equation (3) is not clear enough. What is the impact of using the token level score for retrieval? Related to inference what is the cost of running the inference on YouCoo2 and MSRVTT (as it requires to compute all pairs). \n\n- Minor: You should use $\\exp(f())$ or $e^{f()}$ but not $\\exp^{f()}$\n\n- Since there is no Masked Language Modeling loss, what forces the contextualized output $c_i^k$ to be aligned with the token $k$? \n\n**Experiment section**\n\n- In Table 1, what is the difference between ToCo ($\\lambda=1$) and Our Baseline? \n- S-100 is not defined in the caption of Table 1. Similarly what does #L stands for? \n- What does I3D with ResNet-152 backbone means? I3D used in (Miech et al. 2020) comes from the original I3D paper [1] and corresponds to a specific video model architecture (in particular its not based on ResNet-152).\n[1] Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, Carreira and Zisserman.\n- More details need to be given on the architecture for f: What is the text encoder exactly? What is the video encoder (is it a transformer on top of the video frozen features?) How many layers are used for the different models?\n\n**Comparison to the state-of-the-art**: Arguably the proposed method already leverage the S3D model that was trained using lots of compute in (Miech et al. 2020) while here using a more powerful language model, still the zero shot performance are far from (Miech et al. 2020). Therefore is the compute the only reason for the difference in performance here?\n\n**Qualitative results** The qualitative results are not really informative. Are those typical or cherry picked? It would be more interesting to be able to demonstrate that ToCo enables to perform better temporal grounding than prior methods such as MLM. Is there a way to visualize something like in Figure 1.?\n\n**Many typos are present:**\n\n- a new simple training object -> objective?\n- far enough -> not enough/far from enough?\n- Node that the token -> Note that the token\n- At the meaning time -> In the meantime?\n- We summarize these the main settings -> drop these?\n- as wen can to\n- ...\n\n## Overall assessment\n\nAs said above the paper proposes an interesting idea (ToCo) and results seem to indicate that ToCo is beneficial to learning better video and text representation. However the paper needs to improve its clarity and quality (see in the Weakness section) to pass the acceptance bar. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing related work, marginal quantitative improvements, little evidence it's linked to the motivation",
            "review": "The paper focuses on text-based video retrieval, where a contrastive loss (between a video representation and a text representation) is frequently used in prior state of the art. In this work, the authors emphasise the addition of token-level contrastive learning. This is based on parsed input captions, into various parts of speech (specifically verbs and nouns) allowing for both a global and a local (token-level) contrastive learning.\nThe method is tested on three standard retrieval datasets (MSR-VTT, YouCook2 and CrossTask), using a variety of video encodings. In each case, the paper demonstrates a marginal improvement when token-based contrastive learning is incorporated. \n\nMajor Concern 1: Missing Related work - overhyped novelty claim.\nMy main concern with the paper is missing very relevant related works that have previously argued for the same token-based (referred to in these papers as word-based or part-of-speech-based) contrastive learning showing improved results. Of these, four main works come to mind that need to be discussed demonstrating what is novel in this paper's proposal over these published works:\n1- Wray et al (2019). Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings. ICCV -- In this work individual PoS loss functions are included in cross-modal retrieval on MSR-VTT showcasing clear benefits. A triplet loss was used here, but in terms of novelty, I see the proposal here as the same as this paper.\n2- Chen et al (2020). Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning. Again in this work word-level contrastive learning is used showing improved results on MSR-VTT.\n3- Suris et al (2020). Learning to Learn Words from Visual Scenes. ECCV. In this work, token-level contrastive (called a word cloze) is used to achieve zero-shot retrieval of previously unseen words.\n4- Sariyildiz et al (2020). Learning Visual Representations with Caption Annotations. ECCV. In this work negatives are particularly learnt focusing on one token at a time.\n-- With all the previous works discussing tokens, the current paper's related work does not discuss a single prior work that focuses on PoS encoding (or Token-based learning). I thus find the related work to be misleading regarding this paper's novelty.\nAdditionally a very recent work [that I understand the authors might not have seen or referenced - so this is not affecting the review but in the interest of sharing knowledge] has attempted a captioning loss from videos in a batch, reporting on the same datasets as this paper.\nPatrick et al (2020). Support-set bottlenecks for video-text representation learning. ArXiv.\n\nMajor Concern 2: Little evidence to link the motivation to the results.\nFigure 1 shows a clear motivation but it is not linked clearly to the results. Fig 3 gives little details of what baseline exactly was used here and how this corresponds to the various experiments run in the paper. As a reviewer, I have little confidence that the motivation is infact being carried forward. Previous related works (pointed above) make a clear attempt to analyse the contribution of various tokens, which this paper can benefit from, for a more convincing argument.\n\nMinor Concern: In 3.2, the authors do not make it clear that this is exactly what all prior works utilise. Adding new terminology like \"global contrast\" and global alignment, for things commonly used, does not make it different from prior works.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}