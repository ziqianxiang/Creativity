{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper is a study in optimizing the Donsker-Varadhan lower bound on mutual information focusing on a \"drift\" problem.  The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together.  They propose a fix for this problem. The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance. The paper does not address the variance (convergence) issues with the DV bound.\n\nWe have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection.  Other reviews are not very enthusiastic.  I will side with rejection."
    },
    "Reviews": [
        {
            "title": "Major revisions needed",
            "review": "Response to authors: After reading the authors' response, I have decided to maintain my original rating. The authors have not adequately addressed my main concerns.\n\nNovelty: The work here, as indicated by the authors, is largely an incremental improvement over an existing work MINE. The authors' response did not alleviate this concern and in fact reinforced it.\n\nCitations and comparisons to other work: The authors did not agree to even include citations to important literature in this area. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions.\n\nTheoretical work: The authors simply pointed to the theoretical work for MINE. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). More theoretical work is needed in this area to justify the use of these estimators over others.\n\nSome responses to other comments that may help the authors with further revisions:\n\n(1) The presentation of MINE should occur in the main paper as this is crucial for understanding the paper.\n\n(2) This was not clear. Perhaps the authors could include similar pointers in the paper with each of these issues.\n\n(3) The bias I'm referring to here is the actual statistical bias of the estimator. From Theorem 6, it seems that the drift problem does seem to create some bias but it would be useful to quantify that, which could then lead to a bias correction approach.\n\n(7) The way this is currently worded, it sounds like you are saying that training with a larger batch size is bad. This part should be clarified to avoid this. \n\nOriginal Review:\n\nThis paper presents a modified version of a neural network-based MI estimator. They investigate a few of the issues of this specific estimator and propose a regularization to help with one of them. MI estimation is an important and difficult topic. Improvements in this area are of definite interest.\n\nPros:\nThe paper appears to be technically correct. The experiments are somewhat supportive of including the regularization, especially when the MI is higher which is a known issue with some MI estimators.\n\nCons:\nThere are some interesting ideas here but the paper feels unpolished. The presentation of the ideas is somewhat unconventional. Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them. The paper could benefit from a bit more focus in this regard. In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement. But it's not clear how much of a problem this drift really is. The authors show that it causes a bias but they do not present how much bias it adds.   \n\nIn addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons (see the references below for some examples, which all have strong theoretical results). \n\nThe theoretical work is also weak with regards to the convergence rates of the proposed estimator as well. While empirical results can confirm that an estimator can be useful in practice, they are easy to cherry-pick and ultimately theoretical guarantees are needed to know an estimator's general performance. Thus the results would be a lot stronger if convergence rate guarantees were given. \n\nOther comments/questions:\nThe authors should define the MINE estimator in this paper.\n\nThe second bullet point on page 2 says that \"training with larger batch size reduces the variance of the MI estimate\". Isn't this a good thing? That would lead to better convergence.\n\nIn Section 3.1 the notation is technically incorrect. Instead of stating $I(X;X)$ it should be written as $I(X_1;X_2)$ where $X_1$ and $X_2$ are i.i.d. The former suggests that you're comparing the same random variables. \n\nOn page 4, it's suggested that joint samples are sparse with reduced sample size. Why aren't joint samples simply included together during training?\n\nDoes regular L2 regularization help with the drift problem? \n\n[R1] Moon et al.\"Ensemble estimation of mutual information,\" ISIT, 2017.\n[R2] Moon et al., \"Information theoretic structure learning with confidence,\" ICASSP, 2017.\n[R3] Moon et al., \"Ensemble Estimation of Information Divergence,\" Entropy, 2018.\n[R4] Singh and Poczos, \"Exponential concentration of a density functional estimator,\" NeurIPS, 2014.\n[R5] Kandasamy et al., \"Nonparametric von Mises estimators for entropies, divergences, and mutual informations,\" NeurIPS. 2015.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Regularized MINE review",
            "review": "The work studies a neural-network based estimator, referred to as MINE, for approximating the mutual information between two variables.  By designing a synthetic dataset, the work studies properties of MINE and, based on these findings, proposes a new method incorporating regularization, called ReMINE, that they empirically demonstrate has nice performance. \n\nStrengths:\n-- The proposed ReMINE algorithm combats drifting of the two terms in MINE's approximation of the DV representation through regularization that forces the network to find a single solution (as opposed to a family of solutions).\n-- The proposed method is novel and performs well compared to state-of-the-art.\n\nWeaknesses:\n-- I found the writing to be difficult to understand and follow. For example, the paper is not self-contained: the authors use terms like \"statistical network\" without providing definitions, and begin simulations without introducing MINE or giving a high-level discussion of how it estimates the DV representation, so it would be very difficult to read this paper without having first read the original MINE paper. \n-- The paper lacks intuition. For example, there is little to no discussion of why the synthetic dataset proposed is a good choice for studying the underlying properties of MINE.\n\nSome additional typos/comments:\n\nPg 1: \"...the value of each term in MINE loss IS shifting even...\"\nPg 2: I believe equation (3) should be a lower bound not an equality.\nPg 2: \"...where estimates of  __ and __ DRIFT in parallel...\"\nPg 4: mu and sigma in Figure 2 were never defined.\nWhen printed, the black-and-white figures are too small to read and interpret.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper with new insights for the community",
            "review": "This paper attempts to answer the four questions raised from the mutual information estimator. To this end, this paper investigates why the MINE succeeds or fails during the optimization on a synthetic dataset. Based on the observations and discussions, the paper then proposes a novel lower bound to regularize the neural networks and alleviate the problems of MINE.\n\nOverall, the paper is easy to follow and new insights have been brought for the MI estimator and the downstream tasks.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Novel mutual information lower bound to regularized the drifting problem",
            "review": "### EDIT:\n\n I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft. Unfortunately, I'm still not very happy with the motivation of attacking the drifting phenomenon on MINE. The main reason for removing the drifting effect is for moving average of history outputs. However, there are various ways for tackling this ( as pointed out in my original reviews, like using a non-drifted mutual information estimator with moving average or plugin some robust density estimators). Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. But actually, the stability of other MI estimators also allows them to avoids having exploding network outputs.\n\n Also, in practice, people don't usually run moving average on MI for representation learning. I encourage the author to explore the importance of moving average of MI estimators further.\n\nR3 suggests the author take some non-parametric estimators as baselines. But I think it's fine to only compare to some parametric(variational) methods on high dimension setting, where most non-parametric estimators fail. Nonetheless, it's always good to have additional experiments compared to some non-parametric methods in low dimension settings.\n\nOverall, I lean toward rejection given current concerns.\n\nSummary\n\nThe paper introduces a generalized version of the mutual information neural estimation (MINE), termed regularized MINE (ReMINE). Some interesting experimental results are firstly provided on a synthetic dataset: the constant term in the statistical network is drifting after MI estimate converges. Also, the optimization of MINE will result in the bimodal distribution of the outputs, in which the statistical network has very distinct values for joint and non-joint samples. The paper presents a theoretical explanation for the drifting phenomenon. In light of this, the author's approach is to add a regularized term to prevent the drifting phenomenon. They impose a $L_2$ regularization on the logsumexp term to enforce the network to find a single solution. Further, the authors make use of the historical estimation for better performance. Empirically, the proposed regularized term works well along with the original MINE estimator and ReMINE  has better performance in the continuous domain\n\nContributions\n\ni) Proposal of a novel regularized MINE objective for solving the drifting phenomenon. The new objective successfully finds a single solution and exhibits a lower variance.\n\nii) Provide interesting insights, such as drifting phenomenon and the instability due to small batch size, out of experiments. \n\niii) Experimental validation of the proposed method for solving the drifting problem. Achieve better performance for mutual information estimation in the continuous domain. \n\nIssues:\n\ni) The drifting phenomenon of MINE is a feature but not a bug, since the drifting term has no effect on the final MINE estimated value. The motivation and benefits of solving drifting problems are unclear to me.\n\nii) Is the drifting phenomenon of the statistical network ubiquitous among the density ratio estimators? For example, does it exist in the density ratio estimator in logistic regression or in JS dual lower bound? If not, we can directly plug these non-drifting density estimators in MINE, instead of regularized MINE. Another apparent remedy is to make the output of statistical network $T$ zero-meaned by subtracting the online sample mean of $T$.\n\niii) The proposed ReMINE is motivated by the drifting phenomenon. But it can also alleviate the exploded outputs / bimodal distribution of the outputs since ReMINE explicitly imposes $L_2$ constraint on its output. The connection between the exploded outputs / bimodal distribution of the outputs and ReMINE is weak in the paper.\n\niv) The paper states that MINE must have a batch size proportional to the exponential of true MI to control the variance. The statement is wrong. Yes, the variance of some mutual information estimator, like NWJ,  is proportional to the exponential of true MI, as proved in [1]. However, the variance of MINE is not proportional to the exponential of true MI in finite sample case (in asymptotic maybe), due to the log function. \n\nMinors:\na) I wonder whether the SMILE estimator (cited in the paper) implicitly solves the drifting problem. Since the optimal statistical network cannot drift freely in SMILE.\n\n[1]A Theory of Usable Information Under Computational Constraints, Xu et al, ICLR20. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}