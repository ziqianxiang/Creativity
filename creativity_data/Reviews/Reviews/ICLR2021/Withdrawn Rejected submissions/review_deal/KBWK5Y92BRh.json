{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new NAS methods that when doing architecture search, returns flat minima using based on a notion of distance defined for two cells (Eq. (2)). Authors then evaluation the effectiveness of the proposed methods against prior work on several benchmarks.\n\nAs authors have discussed in the paper, the idea of using flatness notion in architecture search is not new and has been first proposed by Zela et al 2020. This paper is building on Zela et al 2020 but the proposed algorithm is novel and different than Zela et al 2020. Even though the introduced algorithm is interesting, there are several concerns/areas of improvements:\n\n1- The proposed method's performance is highly dependent to the notion of distance defined in eq. (2). However, the current choice is not well-motived and does not seem like a well-thought-out choice. See for example the issue raised by R1. I think authors need to spend more time on this choice. One other option is to meta-learn the vector representation of each operation.\n\n2- All reviewers agree that the improvements marginal and in some cases not statistically significant. Authors have responded by arguing that this is typical for this area of research. I don't find this answer satisfying. For example, consider P-DARTS (Chen et al., 2019). P-DARTS improves over NA-DARTS (the proposed method) on CIFAR-10 and ImageNet and on CIFAR-100 they are on par given the standard deviation of NA-DARTS (see Tables 4 and 5). Moreover, the search cost of P-DART is 0.27% of NA-DARTS (Table 4). So P-DARTS has clear advantage over NA-DARTS.\n\nGiven the above issues, I recommend rejecting the paper. I hope authors would take feedbacks from the reviewing process into account to improve the paper and resubmit.\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #3",
            "review": "This paper proposes a neighbor-aware method in the neural architecture search (NAS). The paper states that by optimizing a neighbor of the neural network, it can search the result in a flat-minima, which is more stable than the sharp minima. The experiment results in further support that the proposed NA-RS and NA-DARTS outperform the current SOTA in various tasks.\n\nOverall, I think the paper raises an interesting opinion, which stresses that during the NAS process, a stable flat-minima has a better generalization ability. The definition of the neighbor of the neural architecture is also interesting. Below are some detailed points of my opinion about this paper.\n\nPros:\n1. The paper compares with a lot of NAS methods and outperforms most of them.\n2. Stabilize the search result can reach a better performance seems reasonable for me.\n3. The ablation study in the appendix further shows the performance under different aggregate functions and distances, which makes the experiment more convincing.\n\nCons:\n1. From the paper's statement, sample-based NAS is easier to define the neighbor than the gradient-based one. However, from the empirical result, the sample-based NAS result is relatively weak compared with the differential based. DARTS also provides the result of the random sampling on their search space. It is interesting to see whether NA-RS can outperform the naive random sampling and even catch up with some other NAS method in DARTS's setting and search space. Currently, I think only test the sample-based method on NAS-Bench-201 is a little bit toy.\n\n2. PC-DARTS has reported its ImageNet performance, top1 error, 24.2%. I am a little bit confused about why the author here uses their implement result with a lower performance. In addition,  it is interesting that the author states that they can combine PC-DARTS with their neighbor-aware. However, they only show an S3 search space result in the appendix, it would be more convincing to show the neighbor-aware method's generalization ability if you can apply it on the standard search space and directly compare with PC-DARTS's public performance. Otherwise, the 'SOTA' statement looks not very strong for me.\n\n================================After Response==============================\n1. I agree that different DARTS paper usually uses some different settings, the lower PC-DARTS performance in the paper could be reasonable.\n2. Search Space design sometimes can greatly impact the result of the final result, but it makes sense to modify the search space for a better result. I am glad that you state the situation of the performance on the original DARTS space.\n3. I also read other reviewers' opinions. Based on the author's response and my previous rating, I decided to keep this score. This is an acceptable paper, but still has something to do in the future, like a more comprehensive experiment part.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written but incremental improvements.",
            "review": "This paper introduces a searching framework of neural architectures search by modifying the objective function to optimize the aggregated performance over the neighborhood of an architecture. From the observation that flat-minima architecture $\\alpha$ generalizes better than sharp-minima architecture (Zela et al. (ICLR2020), the author proposes the objective function considering the neighborhood to enforce the flat minima. The author supports their method by providing ablation studies and architecture performances from CIFAR-10/100, ImageNet, and NAS-BENCH-201.\n\nStrength\n1. Their objective function can easily be extended to existing NAS methods including Random Search and Gradient-Based methods. \n2. The authors support their methods with various datasets such as CIFAR-10/100 and ImageNet\n3. The paper supports their assumption with various empirical experiments and ablation studies. \n\nWeakness\n1.  The experiment results are incremental improvements (or par) to the existing NAS algorithms. By listing the recently published NAS literature on CIFAR-10: P-DARTs: 2.50%, DATA: 2.59%, SGAS: 2.66%, PC-DARTs: 2.57%, RandomNAS-NSAS: 2.64%. \n2. Few experiments on NAS Benchmark. The experiments don't include NAS-Bench-201 (a popular benchmark for NAS) and small experiments with NAS-Bench-1SHOT1 only comparing with DARTs, PC-DARTs. And PC-DARTs result is better than NA-PC-DARTs according to Table 6. \n\n\nQuestion\n1. How do you select the number of neighbors? In the DARTs setup, the normal/reduce cell would have $(2+3+4+5)*7*2=196$. The possible neighboring architectures from a given subnetwork even with hamming distance 1 may be much larger than 10. Can you visualize the loss landscape based on various $n_{nbs}$?\n2. Have you tried to verify your algorithm for NA-DARTs on NAS-Bench-201 which is the popular benchmark in recent NAS literature?\n3. The reproduced ImageNet test error results on PC-DARTs and P-DARTs are higher than the reported test results. The reported test results on PC-DARTs outperforms the NA-DARTs result. Is the resulting discrepancy related to hyperparameter tuning? Can you clarify this? \n\nOverall, this paper proposes a neighborhood aware objective function that can be adopted in the existing NAS search. Despite the paper well written, the performance of their methodology is incremental improvements (or par) among various existing NAS algorithms. I would recommend a marginally below acceptance threshold for now but may change the decision based on the rebuttal. \n\nReference:\n1. Xu, Y et al. (2019, September). PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. ICLR2020\n2. Chen, X et al. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. ICCV2019\n3. Chang, J et al. DATA: Differentiable ArchiTecture Approximation. Neurips2019\n4. Li, G et al. SGAS: Sequential Greedy Architecture Search. CVPR2020\n5. Zhang, M et al. Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization. CVPR2020\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea for NAS, where the neighborhood of the model is considered. ",
            "review": "** Summary \nThe authors proposed neighborhood-aware neural architecture search, where during the evaluation phase during search, the neighborhood of an architecture is considered. Specifically, when an architecture $\\alpha$ is picked, its neighbors $\\mathcal{N}(\\alpha)$ all contribute to the performance validation. This is built upon the assumption that `` flat minima generalize better than sharp minima’’ and the authors verify it in Appendix C. \nThe authors conducted experiments on CIFAR-10/100 and ImageNet, and obtained promising improvements over the standard baselines. \n\n** Clarify\n1.\tTowards \"Due to the property of the total variation distance, when $d$ is an integer, the neighborhood contains all the cells that have at most $d$ edges associated with different operations from $\\alpha$\". As you reported, $\\alpha$ is a collection of $\\alpha^{(i,j)}$, where each $\\alpha^{(i,j)}$ is a one hot vector “ But in DARTS, $\\alpha^{i,j}$ is a distribution of all candidate operations. In this case, how to select the $\\mathcal{N}(\\alpha)$?\n\n** Significance \nOverall, I think the results are solid.\n1.\tThere is a possible “ensemble’” baseline for your method. Let us take DARTS as an example. First, we independently train $n_{nbr}$ one-shot models. Each one-shot model has an individual $\\alpha$. Then, we sample an architecture from the average of all $\\alpha$’s. This could be another way to leverage neighborhood information and should be compared. \n2.\tImprovement not significant: In Table 4, compared to DARTS+, the improvement is 0.1/0.4 on CIFAR10/100. Compared to PDARTS, on the three datasets, the results between your method and PDARTS are almost the same. Therefore, you should apply your method to more recent advantaged methods to show that it is orthogonal or others.\n3.    As you pointed in Section 2, (Zela et al 2020) also observed a strong correlation between the generalization error of the architecture found by DARTS. It is good to see the exploration in Table 6. I think the \"NA-DARTS-ES\" should be implemented to see whether your method and  (Zela et al 2020) are complementary to each other. \n4.\tThe code should be released to reproduce your work.\n\n== Post Rebuttal ==\n\nI am satisfied with the response \"ensemble baseline\" and \"NA-DARTS-ES\". But my concern about \"Improvement not significant\" is not addressed, which is also mentioned by R1 and R4. I will remain my score as 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The results are good but novelty is not enough",
            "review": "Inspired by the concept that ``flat minima generalize better than sharp minima\", this paper proposes to search the flat-minima architecture by considering the performance over the neighborhood architectures. A random search and differentiable search method based on the neighborhood principle are proposed, which achieve comparable results with SOTA NAS methods. \n\nConcrete comments\n1. The biggest concern lies in the main idea of the neighborhood-aware search formulation. It is known in many previous methods that the flat minima of the loss function of neural network training generalize better. This conclusion is also extended to the eigenspectrum of the Hessian of the validation loss with respect to the architectural parameters in [1]. However, this paper hypothesizes that similar architectures with similar performance point to a flat solution. How it can be proven that nearby architectures are located in neighborhood in the loss space should be clarified. This is of critical importance to support the main concept of the paper.\n2. The proposed method is not so reasonable. The distance between architectures is defined as Eq. (2). However, many cases may fail with Eq. (2). For example, a 3x3 separable convolution is nearer to a 5x5 separable convolution, but further from the identity operation, which cannot be handled with Eq. (2). Moreover, the final operation is determined with the highest probability. [0.05, 0.6, 0.35] and [0.35, 0.6, 0.05] represent the same architecture, but different in Eq. (2).\n3. Experimental results are not so convincing. \n  - In Tab. 1 and 2, only test errors are reported but not Params or FLOPs. It is not clear whether advantages of the method actually exist. Normally, models should be compared with similar sizes.\n  - The proposed method does not show evident advantages over others, e.g., worse or similar compared with PC-DARTS but taking much more cost, 10x than PC-DARTS.\n  - The experiments are based on the DARTS-like search space. However this kind of search spaces hold high latency due to the complicated topology. What about the widely used MobileNet block-based search space?\n4. The novelty is somewhat limited. As the flat solution has been explored in [1], the main contribution seems to be the newly defined loss function ensemble with similar architectures. \n\n[1] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search. In ICLR, 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}