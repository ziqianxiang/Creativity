{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting paper",
            "review": "The paper vertically integrates propositional logic with a deep neural network: the ability of deep neural networks as feature extractors is used to extract intermediate representation from data, and then a disjunctive normal form module performs logical rule-based classification. The approach allows for interpretability/explainability via explicit symbolic representations.\n\nThe idea is very interesting. However, the experimental results on several classification tasks show that the presented approach (in some cases slightly, in others more substantially) loses on accuracy compared to the standard deep learning baselines. Shouldn't the additional logical information help to increase the accuracy? It is argued that this loss of accuracy is traded for a gain of explainability. However, this aspect requires a much deeper experimental evaluation than currently given in the paper.  \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interpretable XOR integrated",
            "review": "This paper proposes an integrated method for training DNFs over learned neural features, ostensibly because DNFs are interpretable rules. The main technical idea is to use a noise parameter to soften the indicators representing the terms, so that the softening is stochastic rather than via interpolation. As a consequence, the method is able to train features through the DNF, and is able to obtain features with decent but not quite SOTA accuracy on some standard benchmarks. \n\nUnfortunately, the resulting features are not really interpretable; the DNF layer fares well w.r.t. some interpretability metrics of course -- the discrete, Boolean attributes have easily identifiable influence on the labels -- but the features themselves are essentially meaningless. So a set of experiments is included in which the features are obtained as the result of supervised learning, and the DNF is separately trained on top. The resulting representations don't obtain accuracy comparable with the prior work by Koh et al., though they do obtain DNFs. But at the same time, given this setup, one could have used existing DNF (rule) learning methods since the joint training is no longer essential. This is an obvious set of experiments to perform at this stage, but they are not included.\n\nTo summarize: the problem with this work is that it seems that the joint, end-to-end training that is one half of the motivation undercuts the interpretability of the DNF that is the other half. The result is that the overall motivation is quite weak, and I recommend rejection.\n\nA comparison to direct application of existing rule-learning methods on top of independently learned concepts might give demonstrate some case for this approach if there were some improvement. Alternatively, if the method could be extended to learn simply a deeper Boolean model, the resulting representations might be at least somewhat interpretable even in the absence of the concept annotations.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review: Neural Disjunctive Normal Form",
            "review": "In the setting of interpretable deep neural networks, this paper aims at coupling the benefits of extracting relevant features with a neural net, and explaining classifications with symbolic rules. The resulting “Neural-DNF” architecture combines a neural-based feature extractor with a DNF classifier, using various techniques including, among others, an Adam optimizer, a Binary Optimizer (Bop), an (improved) Semantic Hashing discretizer, and an adaptive perturbation method for avoiding bad minima. This architecture is evaluated on both symbolic and real-world experiments.\n\nOverall, the idea of coupling a neural net feature extractor with a DNF classifier is well-founded, and can indeed provide additional interpretability to deep learning methods. As indicated in the Appendix, DNF formulas are more suited than linear/polynomial functions for providing different forms of explanations. Yet, I am not entirely convinced that the paper can be accepted in its current state due to several reasons, detailed below.\n\nFirst and topmost, the Neural-DNF architecture looks very complicated. I could not help but think that the framework is essentially the juxtaposition of existing techniques, which have been carefully upgraded and tuned for the present study. The architecture involves many hyper-parameters. Notably, there are two key constants $K$, the number of output features, and $N$, the number of candidate terms in the final DNF. Clearly enough, both hyper-parameters must be fixed in advance, as they capture the dimensions of the matrix used to learn the final DNF. Thus, an obvious question arises: given a binary classification problem, how do we choose $K$ and $N$? In the main paper, both constants are left unspecified, so we have to look into the SM for more information. In Algorithm 1, $N$ is set to 64 as a default value, but for some datasets, it is set to 128, and for other datasets, it is left unspecified. The case of $K$ is even more problematic: in Algorithm 1, it does not appear as a hyper-parameter, which is in contraction with the fact that it was specified as a predefined constant in Section 2. It seems that this value was tuned for each experiment. For example, $K = 5$ for MNIST and KMNIST and $K = 32$ for SVHN and CIFAR10. Finally, it is not clear that we can output exactly $K$ features using the improved hashing technique presented in the paper and the Appendix.\n\nConcerning the experimental part of the paper, we cannot statistically conclude that Neural-DNF outperforms - in terms of accuracy - existing architectures such as SENN or the concept bottleneck model. So, the real merit of Neural-DNF should lie in the interpretability of decisions. For the first scenario (Sec. 4.2), this interpretability is evaluated with respect to the “faithfulness” criterion (more specifically, the “faithfulness metric”) which, according to Melis & Jaakkola, assesses how relevance scores are indicative of true importance. Here it would be nice to report (for example in Appendix) how these relevance scores are calculated for neural-DNFs, using the technique of Robnik-Sikonja and Kononenko. I am also wondering whether this metric changes by selecting different values of $K$ and $N$. By the way, Figure 3 is unreadable and should be enlarged. In addition to the faithfulness metric, it would be interesting to compare neural-DNFs with SENN according to the “stability” criterion described in Melis & Jaakkola, which is a guarantee for robust performance. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea with interesting and diverse case study, though I am concerning on the novelty of the idea.",
            "review": "### Summary\nThe paper proposed a new neural architecture that represents a concatenation of two functions g and p. The function p is a binary feature extractor, which takes raw features as input and outputs a set of binary (0/1) embeddings. The function g is a DNF formula which takes a binary embedding as the input and outputs a binary class. Despite the function contains many steps and not immediately differentiable, authors managed to optimize parameters of both function in an end-to-end manner. Empirically, the new model can be trained with comparable accuracy with other baselines methods, while the new model exposes good interpretability on the embedding. In the case study, authors demonstrated that the interpretable embedding is faithfully relevant the the prediction result, and they expose additional interface for tuning the prediction of a model if the encoding is trained with supervision.\n\n### Strength\nThe idea to incorporate logical representation in neural networks is very interesting.\n\nThe method to learn DNF formula using gradient descent is a good addition to the existing methods that mostly rely on combinatorial search. Based on the experiments, the quality of the learned DNF is good.\n\nThe experiments are quite diverse. Not only they demonstrate that the model learns \"interpretable\" embeddings, but also did they show the benefits of having such embeddings. Author includes both synthetic data to demonstrates the problem it is tackling, and some real word case studies to show the new model's application.  Further, the two scenarios in the real-world case further demonstrates two different uses cases of an interpretable model; one can either obtain faithfully relevant embeddings or allows human interventions to correct prediction errors.\n\n### Weakness\nOn the novelty side, the work is less entertaining. \n* The training method seems to be composed of many existing training tricks. \n* The formulation from a DNF formula to a trainable function has been established in previous works.\n* Further, the DNF learning method does not seem to show any theoretical guarantees.\n\nAuthor did not report the training accuracy of DNF-Real, which is a very important baseline. On the interpretability side, DNF-Real model can serve on the exact same purpose, and it should be compared.\n\n\n### Questions\n* Does the faithfulness metric in general simply favor model that generates predicts from embeddings using a small DNF formula? \n\n\n### Minor Typo\nOn page 6, despite tje first-state -> despite the first stage\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}