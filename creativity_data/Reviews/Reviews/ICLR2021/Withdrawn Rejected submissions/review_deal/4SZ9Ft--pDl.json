{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper was quite contentious.  While there is clearly promise in the method and the idea, and reviewers appreciate the importance of encoding non-trivial prior knowledge into BO, three reviewers express major concerns regarding the presentation (including worries about over-claiming contributions), the specification of the probabilistic model as well as to some extent about the experimental evaluation.  Lastly, the title appears to be a kind of pleonasm - arguably, the key point of using a Bayesian prior is to be able to encode prior knowledge.  The authors consider a different form of prior than perhaps usually meant in BO (in a generative rather than discriminative sense), but the terminology is still confusing.  "
    },
    "Reviews": [
        {
            "title": "Overclaiming and unsound probabilistic model",
            "review": "This paper presents a new method, PrBO, and it incorporates user knowledge on the location of potential optimum to the prior used for Bayesian Optimization. The authors claim that BO can be solved more efficiently using their method and showed promising empirical results on some low-dimensional benchmarks.\n\nDisclaimer: I reviewed the same paper at Neurips. While I was happy to see that the authors have resolved some of term misusage issues and wordings, I felt somewhat disappointed that many good points raised by the reviewers there before were not addressed in the new version of the paper. IMHO, it still remains to be true that this paper needs a bigger surgery to be accepted, not only for Neurips or ICLR, but also other ML venues. And I hope the authors will consider the reviewers' opinions before submitting somewhere else.\n\nPros:\n- Writing is mostly clear. Good clarity and good illustrations.\n- I still think this is a very interesting idea and a quite promising direction to incorporate human knowledge in our used-to-be black-box machineries. This paper could set a good direction if it's done properly.\n\nCons:\n- This wasn't the case of the previous version, but I found it unsettling that the 1st contribution point was \"For the first time, user prior knowledge can be combined with standard BO probabilistic models, such as Gaussian Processes (GPs), Random Forests (RFs), and Bayesian Neural Networks.\" That is just bold and wrong. User prior knowledge is always considered in those models when selecting which kernel structure to use, how many trees would be sufficient, or if we need a convolution layer or not. Even in the narrowest definition of prior, this paper is a perfect demonstration of how to incorporate human knowledge in GPs: https://papers.nips.cc/paper/2015/file/4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf. I strongly suggest not to ever claim one's the first, and when it's really needed, be very very specific. Otherwise there will be bad consequences. (See an extreme case related to Christopher Columbus.)\n- It is still not clear what different probabilities mean to the authors. The authors described the prior P_g(x) as a prior on good points and P_b(x) as prior on bad points. What do those mean exactly? Could you define them correctly in the paper? And how does the GP-induced good/bad point probability interfere with the proposed ones?\n- Footnote on Page 3: \"P b(x) is not a probability distribution\". What do you mean? In the text it says \"prior distributionPg(x)\". \n- P_g is not independent for each x. Will that cause any issues to the method?\n- If both the \"prior\" and \"posterior\" are both pseudo, what do they mean exactly? Or what are even approximating?\n- Can the authors describe a coherent generative model of what they proposed?\n\nI strongly suggest the authors carefully read the reviews from Neurips again. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Strong idea with extensive evaluation and important applications",
            "review": "The paper presents a novel method to incorporate experts' knowledge into BO. This is done through introducing  Prior-guided Bayesian Optimization (PrBO). Different experiments where conducted to compare PrBO vs different baselines and to show the effect of the user provided priors in the cases where it is well-specified or mis-specified. The design of PrBO enables it to guide the search in the early iterations and as optimization progresses, more emphasis is given to the model and the effect of the prior is washed out.\n\nStrong points:\n- The paper presents a method for incorporating priors over good solutions in an elegant way.\n- The method can benefit from the prior information and incase of misspecified priors, it can still recover.\n- Extensive experiments were conducted.\n- The paper is well written.\n- This type of work is definitely needed to increase the adoption of BO methods.\n\nWeak points:\n- Although the experimental design is extensive and covers several aspects. I wish the experiments included one realistic experiment for tuning the hyperparameters of a famous architecture. This would have been a great example to demonstrate the benefits of this method versus the manual search used by most researchers. However, this is not a huge concern.\n\nClearly state your recommendation (accept or reject) with one or two key reasons for this choice.\n- I recommend to accept this paper.\n- Choice of priors affect the performance of BO. This paper provide the intuitive way to add priors by modelling probability of good configuration. This is what the user thinks. I am sure that this kind of work will encourage more researchers to use BO in their problems\n\nQuestions:\n- It is clear how PrBO is incorporated in TPE, but what about GPs and RF? How is PrBO implemented in this case?\n- Are the experiments in the main paper using GPs or TPE?\n- Could you please explain more what 10,000Ã—random search is?\n- How are the weak and strong priors for PrBO generated? ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "\n\n# summary\n\nThis paper provides an interesting setting and tries to incorporate prior\nknowledge of inputs in BO. For example, in a specific application, an expert\nknows the potential good region for some parameters in this problem. More\nconcretely, to incorporate expert knowledge in BO, the authors first specify a\n\"good\" prior, which gives a large mass on more interesting regions. A \"bad\"\nprior can also be specified, and a common choice is 1 - goodprior. These two\npriors are built from expert knowledge.\n\n\n# pros\n\n1.  This paper provides an interesting direction in BO, i.e. incorporating\n    prior knowledge. Moreover, this direction is very realistic, as in many\n    applications, we do know some parameters are likely to take some specific\n    values or fall into some regions.\n2.  This paper also provides comprehensive experiments and verifies the\n    advantages of incorporating prior knowledge in BO.\n\n\n# cons\n\n1.  My major concern is the construction of the \"good\" prior and the \"bad\"\n    prior. If my understanding is correct, the \"good\" prior in all experiments\n    are fitted using KDE. Based on the independence assumption, different\n    parameters are fitted independently. The input data to KDE is {x_i}. To obtain a good fit, a large number of\n    function evaluations is needed (many objective values are needed to get \"good\" x_i) and this violates the principles of BO in\n    the first place, i.e. the objective function in BO is expensive and we\n    cannot afford a large number of evaluations. In summary, to construct a\n    reasonable \"good\" prior, we need a large number of function evaluations but\n    we cannot afford it. If the \"good\" prior is not good, we then need a large\n    number of functional evaluations to wash out such a prior in the following\n    optimizing stage, violating the main principle of BO again.\n2.  In Figure 2, the authors use a misspecified prior to show their proposed\n    method is able to recover from a \"wrong\" prior. Indeed, this property is\n    desirable. My question is if the prior might be wrong, why not use the\n    conventional BO?\n3.  Why not directly set the parameter bound to be the potentially interesting\n    region in optimizing the acquisition? I understand directly setting the\n    parameter bound in optimizing the acquisition function could rule out the\n    optimal region if our prior knowledge is misspecified. If our prior\n    knowledge is \"correct\", I guess this approach could have a good\n    performance. Can the authors comment on this?\n4.  Comparison with SMAC is lacking and I think this is a must. SMAC performs\n    better than TPE in many problems.\n\nMinor issues:\n1. The claim: 10000x faster than random search, sounds like this is an ad instead of an academic paper.\n\nOverall speaking, incorporating prior in BO is an interesting direction,\nhowever, I am afraid this work do not have sufficient theoretical analysis and\nthere are some issues stated above on the prior in this work.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Algorithm but Questionable Usefulness and Novelty",
            "review": "Summary:\nThis paper incorporates a prior distribution given by experts into Bayesian optimization (BO), to leverage useful human knowledge to accelerate BO. The algorithm uses an intuitive approach to combine the prior with the probabilistic surrogate model of BO to derive a pseudo-posterior, which naturally leads the EI acquisition function. As the BO progresses, the prior information is gradually overwhelmed by the observed data, which ensures the asymptotically correct behaviour.\n\nStrong points:\n- The design of the algorithm is intuitive and natural.\n- Experiments are comprehensive. Figure 2 is a particularly nice illustration.\n- The paper is well written.\n\nWeak points/questions:\n- I'm not fully convinced whether it's really useful to ask an expert to specify a prior distribution and then run the proposed algorithm, compared with the baseline of simply asking the expert to specify the search space and then running standard BO. Firstly, asking the expert to specify a search space is intuitively easier for the expert, compared with asking them to give a distribution across the entire domain. Moreover, I think it's hard to justify why the proposed approach is better in practice than simply asking the expert for a reduced search space. I would like to see either a comparison with this baseline (preferably for the experiment in Section 4.4 if possible), or a justification regarding why this baseline isn't competitive.\n- The proposed algorithm is a straightforward combination of PI, TPE, and EI. Therefore, the technical contribution may not be novel enough.\n- Figure 4: It seems that Spearmint consistently outperforms PrBO with weak prior. Does this mean that if you don't have a good prior, you should simply run standard BO?\n- There is only one real-world experiment (Section 4.4) where an expert is asked to specify a prior, while for the other experiments, the priors are artificially constructed. I feel more real-world experiments are needed, given that I'm doubtful as to whether the proposed algorithm is truly useful in practice as mentioned in the first point above.\n- Why haven't you compared with the related work of Li et al. (2020)? It seems very similar to the proposed approach and is thus applicable in similar settings.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Prior-guided Bayesian Optimization",
            "review": "The goal of this paper is to enable the introduction of prior expert knowledge in Bayesian optimization. This is performed by defining a prior distribution for the optimal value, which is included in the pseudo-posterior used to select new points by Expected Improvement. The number of iterations it takes to overcome a potentially wrong prior information is controlled by a parameter, whose sensitivity is studied. Extensive experiments are conducted on toy examples as well are more realistic hyper-parameter test cases.\n\nThis paper could interest practitioners, but some aspects need to be better detailed and more comparison to existing methods should be added.\n\nDetailed comments:\n\nThe proposed model in (3) amount to scale the probability of improvement with an arbitrary threshold by a probability of optimality given by the prior. This is very similar to the use of the probability of feasibility in constrained BO, see, e.g., Schonlau, M.; Welch, W. J. & Jones, D. R. Global versus local search in constrained optimization of computer models. Lecture Notes-Monograph Series, JSTOR, 1998, 11-25 and references therein. Hence couldnâ€™t this so called pseudo-posterior be used directly?\n\nThere are additional references tackling the introduction of a prior (or lack of) to compare with, both in the state of the art and experiments, e.g.,: \n- Anil Ramachandran, Sunil Gupta, Santu Rana, Cheng Li, Svetha Venkatesh, Incorporating expert prior in Bayesian optimisation via space warping, Knowledge-Based Systems, Volume 195, 2020.\n- Bobak Shahriari, Alexandre Bouchard-Cote, Nando Freitas ; Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, PMLR 51:1168-1176, 2016.\n\nAnother naive competitor is the use of the prior on the optimum only at the design of experiments stage, keeping the usual BO loop. This baseline should be added too.\n\nMinor details:\n\nFigure 1: consider showing the density model given by (4) here too, not just in appendix\n\nExperiments: what is exactly the weak prior setting?\n\nP8: on Oh et al., this is a structural prior to cope with high-dimensional optimization, so not clearly related.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}