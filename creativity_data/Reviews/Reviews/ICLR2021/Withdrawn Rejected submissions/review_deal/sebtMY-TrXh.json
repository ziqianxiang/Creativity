{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "the authors propose to use volume coding to enable uniform sampling from an implicit latent space to be used together with a autoregressive language model. all the reviewers find this approach interesting, but all found that the submission would be much stronger with more thorough evaluation. in particular, i noticed that the reviewers wanted to see how the proposed ariel works in comparison to e.g. VAE on a more diverse set of benchmarks, since the choice of two datasets, one synthetic and one small, narrow-domain, is somewhat limited largely due to their relative simplicity. furthermore, the reviewers were unsure whether various evaluation metrics the authors have used are exhaustive nor appropriate to demonstrate the efficacy of Ariel or to put the proposed approach correctly in the context of other approaches. i agree with the reviewers on both of these points.\n\ni'm thus recommending this manuscript be rejected, and strongly recommend the authors give a bit more thoughts on how to demonstrate the effectiveness of the proposed approach in the context of other approaches and the problem of sentence generation (which is the main problem the authors claim to tackle, as the title directly suggests.) with a better planned experiment and analysis, i believe the authors' efforts will have significant impact."
    },
    "Reviews": [
        {
            "title": "Well written paper, but the motivation and evaluation are not clear",
            "review": "This paper describes a novel methodology of volume coding for encoding and decoding sentences. The algorithm is based on arithmetic coding.\n\nIn general, I believe this is a well written paper. However, I couldn't really understand the content until I read the encoding and decoding algorithms, which are in Section 7 of supplemental material. I strongly think the structure of this paper needs revision, so that people without background of arithmetic coding can understand the core method.\n\nOther than the writing, I still have concerns about the motivation and evaluation. In the first point of contributions, the author states that the proposed method improves the retrieval of learned pattern with random sampling. Does this mean when the coding is randomly sampled, we can see more valid sentences comparing to other methods? If so, the validity in evaluation is the key to claim this contribution.\n\nIn Table 3,  the validity percentage of a Transformer with 512 latent dimensionality is only 17.2%. This low score strongly contrasts with our knowledge that a well-trained Transformer language model is very strong at producing valid sentences. One hypothesis is that the amount of training data is not sufficient for Transformer. If this is the case, the proposed method may lose its edge when the training data is abundantly available. Upon reading the experiment section, I couldn't find an explanation.\n\nWhen comparing with VAE, it's important to compare the interpretability of the latent variables, which is the main purpose we train a generative model. However, if the interpretability is not a major motivation of volume coding, then I'm concerning whether it's meaningful to compare with VAE.\n\nConsidering all these factors, I decide to give a weak acceptance to this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "This paper proposes \"volume encoding\" for sequence modeling. Unlike traditional autoencoder or variation autoencoder model family, the proposed model AriEL applies KDTree to map the input sequence to a quantized multi-dimensional space as the code, and supports tasks such as reconstruction and generation.\n\n**high level comment**\nIndeed this paper proposes a novel and interesting way to do sequence modeling. From its root, data like language are naturally discrete. The traditional methods ubiquitously adopt a continuous code as the representation that poses a gap between discrete and continuous mode. AriEL, on the other hand, represents this type of data into the discrete mode.\n\nFrom a different perspective, the continuous code does have its own merits. The smoothness of the manifold may allow us to perform vector arithmetic and have better performance at practical tasks like grammar correction or semi-supervised learning. The paper does not include tasks like these. Could the authors clarify?\n\n**table 1**\nCan the authors clarify on how to quantitatively or qualitatively measure the correlation between these generations and the training dataset bias?\n\n**table 2**\nIt's pretty well-known that VAE type of reconstruction may have grammar errors like repeated words. The AriEL model seems to be able to avoid such error but at a cost of wrong n-grams like \"small large\". Could the authors give some intuition here?\n\n**open discussions**\nThe current version of AriEL seems to rely on a sequential modeling workhouse like the RNNs. Is it amenable to other types of models?\n\nWould the usage of KDTrees and dataset stats amplifies the training set bias so it might generalize poorer than the rivalling models?\n\nWhat kind of practical applications can we dream up the AriEL for, e.g. leveraging the volume code?\n\n**conclusion**\nWith all the questions above, I still vote an accept for the proposed model for its novelty. In my honest opinion, this paper is valuable for the community. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting paper, would benefit from more rigorous empirical evaluations",
            "review": "Summary: This paper proposes Arithmetic coding and k-d trEes for Language (AriEL), a representation learning method for more efficiently mapping discrete data (text) into a continuous space. Specifically, AriEL uses a language model to split the latent space into volumes.\n\nClarity: The paper is overall easy to follow.\n\nOriginality: The idea to incorporate more structure into the learned encodings via arithmetic coding and K-d trees is an interesting one.\n\nPros: AriEL seems to perform quite well on the toy dataset and the GuessWhat?! dataset.\n\nCons: \n- I think the paper would be more significant if the authors would have demonstrated the efficacy of their method on more benchmark datasets; for example, WMT (Kaiser et al. 2018) or WikiText (Merity et. al 2016).\n- I am curious as to how the performance of some of these models are *so* bad: for example, the Transformer obtains a validity score of 4.7% on the toy dataset, and the AE/VAE get roughly 0% accuracy on the prediction task. I would appreciate it if the authors could elaborate more on why exactly AriEL seems to be doing so well, as well as some of its potential limitations. (at least on the experiments side, there doesn't seem to be any, which seems too good to be true?)\n\nQuestions:\n- Can I understand AriEL to have an encoding/decoding complexity of O(nD^2) rather than the O(n) as written later in the paragraph on page 3, since in practice AriEL will most likely always use a language model? Or is this specific to an RNN? I am wondering if this would hinder the use of AriEL on more complex datasets that require larger hidden sizes (D).\n\nMinor typos:\n- \"followed by a 140-unit LSTM\" (Section 3.2)\n- \"Then we turn to\" (Section 3.1)\n\n---\nUPDATE: Thanks to the authors for responding to my questions and updating the submission. I will keep my score as is, since I think that the paper would greatly benefit from more practical/rigorous empirical evaluations to demonstrate the usefulness of the approach. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but not practical for other applications",
            "review": "This paper proposes AriEL, a sentence encoding method onto the compact space [0, 1]^d. It leverages essences of arithmetic coding and kd-tree to encode/decode sentences with a fixed region of the space. With the property of arithmetic coding, in theory, it can map sentences with any lengths into individual values, and any points on [0, 1]^d can map back into corresponding sentence. Although the method relies on neural network based LMs to assign sentences into corresponding regions, the generality of mapping between any sentences/points is kept while changing the LM's behavior. The idea is interesting.\n\nHowever, there is some disadvantages of the proposed encoding which are not always mentioned in the paper. First, due to the topological difference between the proposed encoding and other spaces (e.g., Euclidian space), the proposed encoding could not be treated as embeddings in some usual meanings, e.g., it is hard to calculate the \"similarity\" between two encodings by arithmetics on real numbers as many deep learning methods implicitly does. Actually, there seems no evidence of advantages of the proposed encodings on other tasks which are not designed for this encoding unlike experiments on the paper.\n\nSecond, the resulting encodings will be affected directly by the capability of the actual representation of real numbers. E.g., if we used float32 for each dimension, the [0, 1] space can contain only information up to 30 bits long in the most efficient case, which may be insufficient to encode \"all\" sentences into the compact space. It will be problematic when we encode very long sentences (|sentence| >> d). Experiments in the paper did not figure out this point enough because the mean number of words in the corpus is too small (9.9 and 5.9 words, whereas d = 16).\n\nThere are also several presentation errors in the paper:\n\n- Using different format of all citations.\n- Table 3 clearly exceeds the width limit.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Idea is interesting but evaluation is insufficient",
            "review": "This paper proposes a sentence embedding called AriEL. Specifically, based on arithmetic coding and k-d trees, AriEL maps sequences of discrete data into volumes in the latent space, and can then retrieve sequences by random sampling. AriEL is compared to other standard techniques such as Transformer and Variational Autoencoders. Results show that it can generate more diverse and valid sentences.\n\n**Pros:**\n\nThe idea of constructing sentence encoders by arithmetic coding is interesting and novel. AriEL takes the frequency of sentences into account, and the frequency implicitly models the statistics of human language. I have not seen prior work in this direction, and it is worth exploring.\n\n**Cons:**\n1. My major concern is the evaluation. The evaluation metrics in the paper are very limited and not convincing. They fail to assess the strengths of the proposed sentence embedding. Specific downstream applications are missing here. A solid evaluation should at least include either classification tasks, such as sentiment analysis, textual entailment; or generation tasks, such as simplification, machine translation.\n\n\n2. The paper is hard to follow and the writing has greatly impeded understanding. I will mention a few that confuse me most:\\\n(1) In the second paragraph of Section 3.2, what does “but the probabilities defined by the latter are used as a deterministic Russian roulette” mean? Does that mean the argmax is one-hot?\\\n(2) What is the purpose of defining biased and unbiased sentences? And in Table 1, what does “comply with the bias” mean?\\\n(3) In Section 3.4.2, I can hardly believe the definition of “grammar coverage”. Why is the number of adjective able to represent grammar rules?\n\n\n3. I doubt only modeling the frequency of sentences is enough for a sentence encoder. There are many aspects of language (e.g. taxonomy, lexicon) that should be considered. But again, these are to be proved by downstream tasks.\n\nOverall, I suggest rejecting the paper in its current state.\n***\nUPDATE: The authors have addressed some of my comments. I appreciate their efforts for making the paper clearer. That being said, I would still keep my original score, because my major concern (evaluation) has not been addressed. Also, the paper would be much better if its writing and organization can be improved.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}