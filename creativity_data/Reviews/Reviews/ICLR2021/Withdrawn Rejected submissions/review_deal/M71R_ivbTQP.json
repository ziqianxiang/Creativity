{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Overall, this seems like a neat idea and well-done work. Main principle is to extract a very sparse net that does a good job at locally \"explaining\" a given example. The NeuroChains idea does this with a diffentiable sparse objective. I think this work is well-positioned and has nice properties: (1) retains a very small percentage of \"filters\", (2) it appears that all the selected filters are actually needed/useful (3) there are some generalization properties wrt to unseen samples that are close to the sample of interest.\n\nI appreciate that the authors responded with very detailed rebuttals to the concerns of the reviewers. I'm still worried, like AnonReviewer4, about the generalization around local regions though the follow-up experiments satisfy me for the most part. There is a genuine concern that while this method has the *potential* to produce useful outputs that could be useful for downstream experts to analyze the underlying network, the paper itself doesn't really show this. In other words, while I agree that on the technical side of things, the work passes the bar, it's not clear that the work passes the bar from the impact side of things.\n\nThis did make for a genuinely a borderline case in terms of decisions and unfortunately this work landed on the reject side this time around."
    },
    "Reviews": [
        {
            "title": "Interesting idea and missing validations for the regions around a sample and for non-high confidence samples",
            "review": "content:\nIt is about pruning for explanation. The goal of the methods presented is, given a sample x, to extract a network, which is\nan unmodified subset of the original network,\nand has similar predictions to the original network in a region around x.\n\nThe authors derive a gradient-based optimization procedure and do a heuristic threshold-cutoff postprocessing to remove layers and filter channels after the optimization.\n\nThe authors evaluate faithfulness in the sample itself. \nFaithfulness for the region is evaluated using the nearest neighbors from the dataset and in a high level feature map metric.\n\nstrength: \npaper concept is well explained. Clarity of the idea.\nThe outcome are sample-dependent very small sub-networks.\n\n\n\nweaknesses:\n\n--The validation of the method.\n\nFirst of all, they do not validate the impact on the region of a sample x sufficiently, and that must be done because it is a central claim of the paper. The not satisfactorily attendance to that claim is the main reason to reject this paper at the moment. page3: \"(3) it is for data from a local region instead of the whole data distribution.\"\n\nThat is relevant even more so as the input space has usually some adversarial samples nearby with respect to a metric in the input space.\n\n\nFigure 4 gives a partial result - but for a very high level feature space notion of neighbors rather than with respect to a metric in the input space. \n\nUsing the last layer for defining the metric to obtain the nearest neighbors may result in rather \"semantic\" neighbors with similar high level structure, but very different low level structure, which does not conform to the idea of a \\eps-lp-metric Ball around x in input space. It is not local with respect to the input space.\n\nIn that sense the evaluation of local faithfulness is not complete.\n \nFurthermore using nearest neighbors from the dataset also does not guarantee that they are locally close to x (for regions with low data density this may not be guaranteed)..\n\n\n--one needs to perform evaluation with some kind of sampling of points within a ball around a sample, close to the local ReLU linearity zone and evaluate faithfulness for those sampled points -- adressing the notion of points being close wrt.~a metric in the input space.\n\nThe reviewer would be satisfied, if that would be done for a few hundred test points if 1500 x2 networks costs too much time.\n\n--one needs to perform evaluation on what happens with predicted labels of adversarial samples close to x. At least to consider how likely do they switch back to the original, non-adversarial label when looking at the extracted subnet. \n\nThe reviewer would be satisfied, if that would be done for one adversarial per test point but over hundreds of different test points.\nOptionally to consider how likely do they switch to another wrong label (this relevant for targeted attacks only, thus optional). \n\n-- any test to be done also for both nets\n\n\nSecondly,\n\nthey do evaluate faithfulness in the sample point. The reviewer is not fully satisfied with the metric.\n\nFig 3 decrease in probability for predicted class does not tell if it changes the predicted label. They circumvent this problem by using only high confidence samples, however this creates a biased or limited evaluation (namely how this method works on the most confident samples).\n\nit would be better to measure two things:\n\n-- do these evaluations for all samples on their predicted label (a few hundred ...), not only the very confident ones.\n\n--the change in difference to the highest scoring other class ( this gets negative if the predicted label switches ) when comparing original and extracted net. \n\nFor example by A quotient of  \"diff to highest scoring other class (extracted)\" / \"diff to highest scoring other class (original)\" - this is signed and gets negative if on the extracted net the highest scoring other class is the flipped predicted label\n \n--the probability that the predicted label switches when looking at the subnet \n\n\n\ncentral suggestions for improvement:\n\nrun experiments: \n\n--one needs to perform evaluation with random sampling of points within a ball around a sample, close to the ReLU linearity zone and evaluate faithfulness for those sampled points -- adressing the notion of points being close wrt.~a metric in the input space.\n\n--one needs to perform evaluation on what happens with predicted labels of adversarial samples close to x. At least to consider how likely do they switch to the original, non-adversarial label. \n\n\nFig3 with:\n--the change in difference to the highest scoring other class ( this gets negative if the predicted label switches ) when comparing original and extracted net. \n\nFor example by A quotient of  \"diff to highest scoring other class (extracted)\" / \"diff to highest scoring other class (original)\" - this is signed and gets negative if on the extracted net the highest scoring other class is the flipped predicted label (as it works on the predicted class on the original net, the denominator is always positive)\n \n--the probability that the predicted label switches when switching at the subnet \n\n-- do not perform the experiments only on high confidence samples but on all samples. This may constitute a bias towards \"easy\" samples otherwise.\n\nThis further also allows to answer the question: are difficult samples with lower confidence less sparsely represented than high confidence samples ?\n\n\ntechnical problem:\n\neq (7) seems to have a typo. The idea is understood, to consider to drop a layer and use the output of the layer before, if input and output size matches, which may make sense for NNs with residual connections.\n\nHowever if one looks at the case otherwise in eq.7, then it is F^l(x,W^{1:l}) without any filter masks S. In accordance to eq5 this is the mask-less original NN, which is likely a mistake.\n\nIt seems that for both cases (incl. the first case) F^l(x,W^{1:l}) needs to be changed to include the masks S \nF^l(x,(W^{l'}  \\odot S^{l'}, \\alpha)^{l'})_{l'=1:l}\n\nThat is a fixable problem that does not affect the paper rating.\n\nTechnical Questions to the authors:\n\nSection 3.2 algorithms:\n-- if the loss is above a certain threshold, it is understood that then you perform the finetuning on the S values. In that case, do you roll back then the filter removal using tau ? In that case, do you roll back then the filter removal by G ?\n-- how do you estimate \\tau,  \\lambda and \\lambda_g  ?\n\nGeneral questions to the authors: \n\nThe authors contribution in Fig5 and Fig6 seems to be the sparse network extraction, as the SMOE visualization and the filter activation maximization are known. It seems that these visualizations, while nice to show, are not really central to the questions raised by the authors. However, this should not be misunderstood as a wish from the reviewer to remove them. Rather to remark that the paper novelty is the left side in these figures.\n\nThe reviewer takes the value of the method so far by: \"When applied to samples from a local region in data space, it is plausible that its inference process mainly relies on a small subset of layers/neurons/filters.\"\n\n--Do the authors have a suggestion how the sparse network extraction can be used for any kind of analysis or insight beyond showing the sparse network itself ? \n\n-- SMOE can also be applied on the original, large network. What is the difference (or value) between applying SMOE visualization on the original network to at first pruning and then applying SMOE on the pruned net?\n\ntypos:\n--three quantitative analysis over --> three quantitative analyses over (Plural)\n\n--Fig2 is a table\n\nminor suggestions:\n\n--\"we remove layer-$\\ell$ if G^l < 0.5\" is simpler\n--remove the top subfig in figure 1, as it is anyway shown in Fig 5 and Fig6 in better resolution. I think you need the space for more interesting content.\n\nPost review: The reviewer thinks that the authors did a thorough job of addressing the reviews. The results are interesting in several aspects, for example Fig 5 and 8. That said, regarding the question \"Do the authors have a suggestion how the sparse network extraction can be used for any kind of analysis or insight beyond showing the sparse network itself?\" The reviewer has doubts that non-ML expert end users (e.g. M.D.s) could make use of a sparse network as explanation mode (as this would assume that they can make sense of what a neuron has learnt). The reviewer updated his rating upwards. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice analysis but with a few gaps",
            "review": "This submission extracts a very sparse subnetwork from an RNN. The extraction tries to preserve the computation of the original network in the subnetwork so the behavior of the original network can be analyzed on this subnetwork. \n\nThe submission proposes to multiply every filter with a weight/gate and optimize these weights to extract the subnetwork. Up on that, the method also multiple weights to entire network layers so a network layer can also be pruned. \n\nI have a few questions/comments about the proposed methods\n1. I understand that it is not easy to optimize binary variables, but can you limit the value of gates in (0, 1) by applying sigmoid transformations of free variables? Is it possible to use some simple rules to prune filters, e.g. remove non-activating entries in max-pooling before running the proposed algorithm? \n\n2. When you remove one layer, do you change behaviors later filters because they actually sees different feature maps in the subnetwork. If this is the case, is the analysis of these filters still applicable to the original network? \n\n3. It seems that gate values of the subnetwork are part of the subnetwork in predictions. \"we further fine-tune the nonzero scores\" Since feature maps are multiplied to scores, which is not in the original neural network. The multiplication will change feature maps and thus be likely to change behaviors of later filters. Actually, some important filters may not even be selected. \n\nTo answer question 2&3, is it possible to compute the correlation between the firing behaviors of filters in the subnetwork and those in the original network? If they are highly correlated, then the claim of that the subnetwork preserves the computation is more consolidated. \n\n4. In figure 3 right, \"It shows that the sub-networks suffer from more degeneration if removing a filter with higher score \" -- I don't see a strong correlation. The line shows the direction but not the strength of the correlation. The data points are far from this line. \n\n5. I don't see many unexpected findings from this analysis. The visualization of filters and feature maps are somewhat similar to previous analysis. What are new conclusions from the analysis if I omit the points you want to make?\n\nIn general, I feel that the research in the direction might be able to provide interesting tools for analyzing CNNs, but there seems to be a few gaps to claim that fidelity of the analysis. \n \n\n\n \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting set-up and well-written",
            "review": "Summary: \nThe submission considers the task of extracting a small sub-network of a pre-trained neural network that can explain a local region of the data space (a small number of the same classes). The resulting sub-network can be interpreted as the inference path moving from the input to the prediction and the filters and associated weights along this path can be visualised. The key of the proposed approach is to apply a multiplicative weight to each filter and layer in the network and enforce these weights to be sparse. The subnetwork is then selected by thresholding the weights. Some quantitative and qualitative analyses were provided.\n\nAssessment:\n\nThe paper considers an interesting interpretation and pruning setting: we wish to prune a pre-trained network to explain how the network performs prediction for a class by training on examples from this class only whilst keeping the pre-trained weights fixed. The resulting final sub-network is very small and thus can be easily visualised. The proposed objective is also very fast to train since the local region used is very small. Despite the similarity to existing post-hoc network pruning/slimming work, I think the goal of finding this small network is very different from that of pruning so thus this work could be of interest to the ICLR community.\n\nThe quantitative analysis of the fidelity and the faithfulness of the objective in preserving the output is useful to understand the proposed approach.\n\nThe paper is also generally well-written. \n\nConcerns:\n\n+ since the target of the final subnetwork is explainability/visualisation, could this be compared to existing methods in some quantitative form? Could this be used to identify problems and provide more insights on how the network works on out-of-distribution examples or examples with wrong predictions that other methods do not provide? How do we know if the final subnetworks are in general more explainable/interpretable, and what can we do with these sub-networks? (I’m trying to be pedantic here to generate discussion and for me to understand the goal this paper is trying to address)\n\n+ the proposed algorithm has several hyperparameters that seem to have been manually selected (?). How were these values selected and how sensitive is the visualisation/final sub-networks to different hyperparameters?\n\nOverall I think this is a good submission. Adding a comparison to alternatives and discussing more about how this could be used/useful in practice will greatly strengthen the submission.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but a rather ad-hoc method, experiments not sufficiently convincing",
            "review": "The paper proposes a method called NeuroChains for extracting a sub-network from a deep neural network (DNN) that can accurately match the outputs of the full network for inputs in a small region of the input space. The goal is to be able to explain the important steps that a DNN takes to get from inputs in a small region of the input space, to its predictions for those inputs. NeuroChains initialises the sub-network as the original DNN except each weight/filter is multiplied with a real-valued score. The L1 norm of these scores is minimised while penalising any deviation of the output of the sub-network from the output of the original network. After the minimisation, weights/filters with a score below some threshold value are removed, leaving a sub-network. In the paper there are experiments that aim to verify three claims: (1) NeuroChains can find sub-networks containing less than 5% of the filters in a deep convolutional neural network while preserving its outputs in some small region of the input space; (2) every filter selected by NeuroChains is important for preserving the outputs and removing one of them leads to considerable drop in performance; (3) the sub-networks extracted for small regions of the input space can be generalized to unseen samples in nearby regions.\n\nStrengths:\n1. The experimental results suggest that NeuroChains can, as claimed, extract sparse sub-networks that match the outputs of the original network for some small region of the input space. These sub-networks are easier to analyse than the full network, which should help with interpretability.\n\n2. The descriptions of the method and the experiments were very clear. As a result, the experimental results could be reproduced based only on the information in the paper.\n\n3. The distinction between NeuroChains and related methods was made clear. In particular, comparisons were made to pruning methods and work on interpretable machine learning. \n\nSuggestions and questions:\n1. In the paper it says that scores are not limited to [0,1] due to possible redundancy among filters in the original DNN. If there are redundant filters, their scores could be set to 0 so that these redundant filters are ignored. Why does the possibility of redundant filters motivate unconstrained scores?\n\n2. The KL divergence between the full network output distribution and the sub-network output distribution was used to penalise the scores. Why was KL divergence used over cross-entropy? The KL divergence is equal to the cross-entropy of the sub-network output distribution relative to the full network output distribution minus the entropy of the full network output distribution. The entropy of the full network output distribution is independent of the scores. Therefore, cross-entropy and KL divergence have the same minima and gradient with respect to the scores. Since the cross-entropy is a little cheaper to compute than the KL divergence, wouldn’t it be better to minimise the cross-entropy instead of the KL divergence?\n\n3. In section 4, paragraph 1, the paper states \"(2) every filter selected by NeuroChains is important to preserving the outputs since removing one will leads to considerable drop in performance\". However, in Figure 3 (right) there are many filters with scores greater than the threshold tau = 0.1 that appear to cause almost 0 change in the predicted probabilities when removed. Therefore, the claim that all filters found by NeuroChains are important is not well supported by the experimental results.\n\n4. In Figure 3 (right), the magenta line appears to be a line of best fit, which does not by itself imply correlation. However, in the paper it says \"magenta line implies strong correlation between the two”. If the goal is to demonstrate strong correlation, wouldn’t it be better to report the Pearson correlation coefficient or Spearman correlation coefficient instead of the line of best fit?\n\n5. On page 8, paragraph 1, the paper references Figure 4 and states that sub-networks extracted for local regions of the input space can generalise to nearby regions because their test fidelity (accuracy with which the output of the full network can be reproduced by the sub-network) remains high when the number of nearest images in the validation set is below 100. However, in my opinion, Figure 4 seems to suggest that as the number of nearest images increases, the test fidelity starts to decrease immediately and at a roughly constant rate until 180 nearest images is reached. Therefore, I don't think that Figure 4 provides strong evidence that the sub-networks extracted for local regions of the input space generalise well to nearby regions.\n\nOverall, the NeuroChains method appears to do a good job at extracting sub-networks from large DNNs and these sub-networks make predictions of the original DNN significantly easier to interpret. However, I don’t fully understand some of the decisions made in the design of the algorithm (questions 1. and 2.) and I am not totally convinced by some of the conclusions drawn from the experimental results (questions 3., 4. and 5.).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}