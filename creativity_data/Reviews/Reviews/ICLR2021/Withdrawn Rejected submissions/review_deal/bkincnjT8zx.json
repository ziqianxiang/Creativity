{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents a framework for modeling dynamical systems by combining prior knowledge available as ODE and implemented via a differentiable solver, with statistical modules. This is a key problem consisting in complementing available partial knowledge on a physical system with information extracted from available data with agnostic statistical methods. In their framework, both the ODE parameters and the residual model parameters are learned. Experiments are performed on synthetic and on a simplified but realistic problem. \n\nAll the reviewers do agree that the topic is important and that the paper has merits and brings an interesting contribution. They highlight some weaknesses in the presentation and more importantly in the experimental assessment. Overall, this is a good paper that should still be somewhat improved for publication. The authors are encouraged to investigate further the analysis of their framework in different settings and to bring more experimental evidence."
    },
    "Reviews": [
        {
            "title": "interesting application, novelty of the contribution under question",
            "review": "This work proposes an hybrid framework where the dynamical system inferred in Neural ODE (NODE) is parameterised by two separated components: a component implementing known dynamics provided by a given ODE, and a free component parameterised by a neural network. The rationale behind this approach is to exploit known properties of the data as opposed to a fully data-driven approach.\nPractically, the framework is simply obtained by modifying the neural dynamics of NODE to account for transformations parameterised by the given ODE, and by letting the network take care of the dimensions for which the dynamics are unknown.\n\nThe experiments are carried out on synthetic data generated from the Lorenz system, and on data representing a simplified fusion system and control experiment. The results show improved predictions whenever the dynamics are introduced in the NODE integrator, either in full or partial form, as compared to the full non-parametric implementation through neural networks.\n\nThe work is interesting and provides a convincing argument for the usability of NODE in settings different from the one proposed in the original paper. The idea of hybrid parameterisation of the dynamics is also interesting and proven to be useful in the experimental setup. However, to my opinion the contribution of this work is quite incremental. The proposed method is still a special case of NODE, obtained by using parameterised dynamics. In this sense, standard applications of NODE (with its standard implementation of ODESolve),  can be used for this purpose, by simply parameterising the latent dynamics with the functional implementing the ODE of interest.\nTo my understanding, the contribution of the work is beyond the methodological framework, and resides in the application on fusion systems. However, I feel that the work should provide a more thorough evaluation of the dynamics, and attempt the implementation of more complex systems.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising method for flexible physical predictions",
            "review": "This paper is well written and introduces a novel method to learn dynamical models, incorporating prior knowledge in the form of systems of ODE.\n\nThe Neural Dynamic Systems method is described in sufficient details and multiple variations are given for the handling of systems where only partial or approximate knowledge is attainable.\nThe experiments explore three different applications of the NDS method introduced in the paper, to a simple synthetic and noiseless physical system, a simplified fusion system where the system dynamics are approximate, and to a modified Cartpole control problem.\n\nThe experiments show promising results, and it seems likely that the machinery developed in this paper will find impactful applications in the natural sciences and in model-based RL.\nI would suggest exploring alternative RL models than the Cartpole problem, such as a robotics application, where the impact of an NDS approach might lead to more interesting results.\n\nIn the related work section, I recommend adding citations to arxiv:1909.05862 and arxiv:1909.12790, which explore very different graph-based methods to tackle a similar issue of predicting the dynamics of physical systems.\n\nAll in all, it is an interesting contribution to the literature of physical predictions, and I recommend it for acceptance. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising contributions, but their presentation needs improvement ",
            "review": "The paper proposes a neural-network architecture for modeling dynamical systems that incorporates prior domain knowledge of the system's dynamics. More specifically, the main contributions are the mechanisms for incorporating such knowledge, in terms of fully or partially known structure (differential equations) of the system, which in turn positively affects the modeling performance. The results from the experimental evaluation (on 3 synthetic and one real-world experiments), in general, show that the proposed Neural Dynamical Systems (NDS), and in particular the ones trained with partial prior knowledge, have better performance than several standard benchmarks (such as NeuralODEs, LSTMs, Sparse Regression etc.).\n\nThe paper has a very strong introduction and the authors provide a good (and quite lengthy) overview of the related work. This, however, given the page-limit, has a severe consequence on the latter parts of the paper, that are poorly structured, inconsistent and hard to follow. In particular, the experimental design as well as the presentation and the discussion of the findings need to be improved. \n\nComments:\n- Some (rather important) details of the method are missing, such as a deeper discussion and motivation related to the architecture design. In particular, what is the role of the context encoder and how does it affects the performance of the model? How does the time-span in the history encoder affects the overall performance (also why x and u in the history encoder are of different lengths)?\n- What is the difference between the Full-NDS and Approx.-NDS, with respect to g(.)?  If there is none, why are these considered different? Can you clarify why x1 and u1 in the historical encoder within Approx.-NDS are of length T',  but in the other u is of length T?\n- It is unclear why the reported MSE losses, in general, are so high? Are these summed (and not averaged) for each task? Even the best performing models, when using all the training examples (Fig2), have errors with magnitudes ~10^3 (Ballistic task) and ~10-10^2 (Lorenz task), which are unusually high. What is the interpretation of this? Maybe a plot of the predicted trajectories vs. the ground truth may help. \n- In Fig2, the performances of the Sparse Regression and GBO are constant w.r.t the number of training samples. After checking the appendices, their performance on the smaller sample size is reported as n/a. Does this mean that they were only trained on the complete data, or something else is happening. Moreover, Sparse Regression in particular, in principle is able to model a Lorenz system quite accurately (this is also reported in the original paper Brunton et. al 2015/2016). However here it seems is orders-of-magnitude worse than the rest- how was it parameterized and did you investigated what is the output?\n- The results show that Partial-NDS seem to perform well overall, therefore the details for their parameterization need to be placed in the main part, not the appendix. Nevertheless, while the authors state that they are a partial ablation of Full-NDS, given their performance there is a trade-of between the amount of partial knowledge and the overall performance. Therefore it would be useful to study this amount of prior knowledge given at input. For instance, how will other parts of the equation structure affect their performance (eg. in the Lorenz tasks if you provide x and z but not y) or what is the least amount that is needed which will still lead to good performance with small amounts of data. Also, can parts of these knowledge be approximations etc. \n- The noise/irregular spacing experiments are mentioned in the beginning of Sec 5.1. Besides the figures given in A9, these findings are never properly discussed nor summarized in the main part (except \"NDS does well\"). \n- The \"Sample Complexity and Overall Accuracy\" segment seems out of place and a bit confusing. The paragraph discusses the Fusion experiment but Figure4 (Lorenz) is referenced. Also can you clarify the meaning of \"three points on the spectrum of added structure\" and what are \"our system identification models\"?\n- Adding a summary of the main contributions w.r.t. the results would be beneficial\n\nOther comments:\n- SINDy (Brunton et. al 2015/2016) doesn't perform Sparse Symbolic Regression - but Sparse Regression.\n- Consider moving the PartialNDS set-up in the main part.\n- How is the Partial-NDS parameterized in the Cartpole experiment?\n- In Eq(9) and (10) why is  x_dot = x_dot (and same for the other 3)?  \n- Last paragraph references Table 5.3. and Table A.12  which don't exist.\n\n-------update-----\nThank you for your response. The discussion and the revised manuscript clarified some of my concerns regarding your work. I appreciate that the authors will focus on the parameterization of PartialNDS and the effect of the amount of employed prior-knowledge. However, my concerns regarding the overall performance, reported as very high errors overall, still remain. This might be due to how the experiments are designed, how the results reported or something else - but nevertheless it needs more attention and further investigation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommend to reject",
            "review": "### Summary\nThe paper combines gray box optimization with Neural ODE, for improved predictions of time series data from low-dimensional physics systems. \n\n### Recommendation\nThe jury is still out on how to best combine prior knowledge about a physical system with learnable components, so this is an important research direction; and combining system identification/gray box optimization with Neural ODE is a reasonable approach. However, the results remain ambiguous, and most importantly, I feel the most relevant questions (see below) are not investigated.\n\nThe big questions in system identification methods, and what often limits their usefulness, are a) how to deal with systematic effects not properly accounted for in the model prior and b) how to avoid falling into local optima due to a highly nonlinear optimization landscape, and I feel these are not sufficiently addressed in this paper. First of all, most experiments reported in the main paper (5.1) are synthetic, generated with the exact model prior, on a simple low-dimensional system, without any systematic deviations or even noise. Yet even in this ideal setting, pure NeuralODE is just barely out of the confidence interval of NDS (e.g. fig. 4). The appendix contains some experiments with added uniform noise; the authors state that NDS is affected more strongly affected by noise, but the results are hard to parse, since only relative change is given. It's unclear if NDS is still performing best in this noise setting. These results cast doubts if this approach will be useful in more complicated settings.\n\nSecond, baselines. There should be a baseline for pure GBO (same model and setup as NDS, but turning off the residual, let's call it NDS0). I'd expect this baseline to be comparable or better than NDS at least on the synthetic data in 5.1, since the model is exact in those cases. Instead, there is a GBO baseline using a proprietary MATLAB GBO function, which performs surpringly bad at estimating parameters (fig. 5) for the synthetic data. The cited Ljung et al. doesn't actually describe the MATLAB GBO method, only the interface and GUI. So it's hard to know what the differences in optimization setup are, and whether the Neural-ODE residual somehow helps parameter estimation, or it MATLAB just performs pooly due to a different optimization setup. If it's the former, and NDS actually outperforms NDS0 on parameter estimation, a proper investigation and discussion of this would actually be an interesting finding.\nAlso, the baseline models seem to all have a different NN architecture, with different capacity and different activation. This makes it even harder to extract meaning from the results.\nThe only dataset which is not a toy example is the fusion dataset in 5.2. I'm not a domain expert on this, so I don't know how hard of a task this is, or what's a relevant benchmark method is for this dataset-- however, since this is real data, there must have be an attempt to fit this data to a model by the group operating the tokamak, and such a model fit should be compared to as a baseline. Second, I assume as a real-world dataset this must include measurement noise, and effects not explained by the model prior. Surprisingly however, this is also the experiment where NDS most strongly outperforms pure Neural ODE, and even more surprisingly smaller error bounds than the purely learned methods-- the complete opposite of 5.1., which found optimization through the model prior to cause a lot of instability *even without* noise. This should definitely be investigated more closely, as this could be due to anything from different smoothness, sequence length, luck, type of model ODE, or maybe even regularization effect from the real-world data; and the conclusions to draw for the value of this method would be drastically different depending on the finding.\n\nFinally, another big surprise in this paper to me was that partial NDS strongly outperformed NDS. I'm not sure I follow the authors' explanation--if the model parameter estimation is problematically bad, why would NDS be better than NeuralODE? It is possible that this particular form of omission makes the energy landscape easier to optimize; maybe that's what the paper implies. Or if it's just about training stability due to the coupling of jointly learning residual and model parameters, this should be eliminated by pretraining parameter estimation. But this should be properly investigated, as local minima are a typical problem in system identification, and this result could even hint at a way to alleviate this issue.\n\nIn conclusion, I feel while this is an interesting topic, the actual learnings we can draw on the value of using model priors and GBO are limited. This is not a bad paper, but in its current form I don't think it meets the standards for a top ML conference.\n\n### Further questions\n- How long is the history T'? Do the baselines (NeuralODE, FC, LSTM) have access to the history data? If not, this alone might explain the performance difference\n- Wouldn't you want to feed the output of g_phi into d_tau, so it's easier to compute corrections?\n- what's the performance of NDS w/o the residual, or without g_phi (this should be identical to the NeuralODE baseline; is it?). How much history does it need to estimate parameters well? Does pre-training the parameter estimation module improve performance?\n\n----\n\nUpdate:\nThank you for answering my questions and running the additional NDS0 baseline, this clarifies a vital aspect of the paper. \nI'm still concerned that in its current form, the learnings we can extract from the paper are limited.\nThe new baseline confirms the expected, that for low-dimensional synthetic tasks where the ground truth model is known, pure sysid is a perfectly fine strategy. This is not surprising, and a good sanity check to perform. Unfortunately, this is also the largest part of the experimental results, and I'm not sure we can learn that much more beyond this.\nResults on 5.2 do look promising-- neither NODE nor NDS0 perform as well as NDS, and the new baseline has strengthened this result. But as the only data point demonstrating an advantage of the method, with little analysis to why and when it does well this still feels a bit limited.\nI still think this line of research is very valuable, and I encourage the authors to study the properties of this approach further (e.g. noise, missing systematic terms, investigate why partial NDS performs so well in some tasks), and investigate other non-trivial domains where NDS or variants can shine.\n\nI've raised my score to 5.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}