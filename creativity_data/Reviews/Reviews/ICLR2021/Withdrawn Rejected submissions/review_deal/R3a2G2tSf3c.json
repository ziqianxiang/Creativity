{
    "Decision": "",
    "Reviews": [
        {
            "title": "A new architecture for graph classification ",
            "review": "This paper proposes a new architecture for graph classification based on stacking several known architectures: first a GCN, then an adversarial autoencoder, then a graph-graph similarity network (implementing a sort of dot-product similar to what is done in collaborative filtering) finally a similarity graph is produced (where each node is a graph from the dataset) and k-nearest-neighbors algorithm is applied on this similarity graph.\n\nI am not sure to understand the claim of the authors: 'The existing GNN-based methods aggregates the node-levelembeddings to present a graph, which cannot guarantee that all graph representations are in the samefeature space due to the differences in sizes and structures of graphs.' But GNN can learn inductive representations of graphs, hence I do not understand why the authors need to use an autoencoder here.\n\nIn Table 2, the scores of GIN for example are much lower than in the original paper. Where is this difference coming from?\n\nPlease provide your code with the submission.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice work but comparing to graphlets lacking",
            "review": "\nIn this work the authors focus on classifying graphs. This is a problem that naturally arises in numerous applications, and there is a long history on the problem since the early 2000s. In this work the authors bring together skillfully different components, including GCNs, adversarial auto-encoders, and some heuristics that allow for comparisons of graphs with different number of nodes and sets of nodes to be compared and aligned in a space. This naturally results in a graph2graph similarly graph from which a k-nearest neighbor algorithm can be used to classify the \"nodes\", i.e., the unknown\nGraphs. It is clear that in general the problem the authors aim to solve captures numerous NP-complete problems. For instance, one could set the label of a set of Hamiltonian graphs to 1, and the label of non-hamiltonian graphs to 0. Assuming the number of samples goes to infinity, in theory one should be able to tell whether a graph is hamiltonian or not. It is reasonable to assume contrary to the claim of the authors in some way that again some (perhaps sophisticated, yet) local structures are being exploited. In that sense I find it a major drawback of the paper that it does not compare to  graphlet based methods in the experiments extensively. Also a detailed study of the running times of all baselines should be shown. Also can you be more specific about the shared GCN and give a more precise description across all graphs rather than the generic equation (1)? Please also clarify what \"E the generated embeddings\" looks like at this stage, i.e., dimensions. How was the value 0.5 for the margin loss decided?  The ablation study is well conducted and illustrates the benefits that are obtained over the components the authors combined together. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important work but novelty is limited",
            "review": "Summary:\n\nThis paper focuses on graph classification problems by proposing a method to learn graph-level representations. In order to take into account graph similarities, this paper builds a SuperGraph, where nodes are input graphs and edges are the similarities between two input graphs. This paper transfers a graph classification problem to a node classification problem using the proposed SuperGraph. The proposed model is evaluated on five datasets compared with several graph neural networks.\n\nPros:\n\n1) This paper transfers graph classification problems to node classification problems using the proposed supergraphs.\n\n2) The motivation of this paper is clear and the overall design of the model is well described.\n\n3) The experimental setup and results are well structured. It is nice to see some visualizations on the similarity matrices and the results of some ablation studies.\n\nCons: \n\n1) The motivation of designing each part of the proposed G2G model is not well-explained, especially the Adversarial Autoencoder. The methodology lacks coherency from GCN to Heterogeneous Space Alignment. It is unclear why the adversarial autoencoder is needed to align hidden representations for graphs. The notations in this part are confusing.\n\n2) This paper claims that the G2G model is more efficient than kernel-based and GNN-based methods. However, no kernel-based methods listed in the related work are compared in the experiment. In addition, some important GNN-based baselines for graph classifications are also not included.\n[1] Ranjan, Ekagra, Soumya Sanyal, and Partha P. Talukdar. \"ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations.\" AAAI. 2020.\n[2] Yuan, Hao, and Shuiwang Ji. \"StructPool: Structured graph pooling via conditional random fields.\" International Conference on Learning Representations. 2019.\n[3] Ma, Tengfei, and Jie Chen. \"Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport.\" arXiv preprint arXiv:1912.11176 (2019).\n[4] Meng, Lin, and Jiawei Zhang. \"Isonn: Isomorphic neural network for graph representation learning and classification.\" arXiv preprint arXiv:1907.09495 (2019).\n\n3) Overall, the novelty of this work is limited. The idea of SuperGraph is not novel [5]. The idea of using max-pooling to down-sample node vectors into graph representations seems trivial. In addition, the motivation of Adversarial Autoencoder is not clear. Finally, some important baselines are not compared in this work.\n[5] Chauhan, Jatin, Deepak Nathani, and Manohar Kaul. \"Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures\". International Conference on Learning Representations. 2019.\n\n\nDetailed comments:\n\n1) Page 5,”Heterogeneous Space Alignment”: The heterogeneity in this section is not clear.\n\n2) Page 5, ”Heterogeneous Space Alignment”: The motivation and function to use Adversarial Autoencoder is not well-explained.\n  a) Why/how does adding a discriminator help to improve the performance?\n  b) The ablation study does not compare a baseline that removes the discriminator and only keeps the autoencoder.\n\n3) Section 4.1. This paper uses triplet loss. Therefore, the number of triplets should be provided in this section, beside the split settings.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting graph classification architecture, but lacks a more thorough evaluation",
            "review": "The paper presents a graph neural network architecture for the task of graph classification which specifically takes the relationship among graphs into account. For this, the authors propose a four-stage architecture: (1) A GCN learns node representations, (2) which are then aligned using an adversarial autoencoder. Next, (3) similarities between graphs are established via a zero-one loss and triplet loss formulation, (4) and used to form a supergraph in order to make the final predictions. Experiments are conducted on five small graph classification benchmark datasets, and ablation studies show the importance of each individual stage.\n\nThe paper is mostly well-written and easily comprehensible. I personally found it quite interesting to read since it introduces a novel perspective on the task of graph classification by casting it to a node classification problem on the similarity graph (\"SuperGraph\"). Intuitively, the proposed approach has obvious strengths, since the model learns to reason about the relationship between different graphs.\n\nHowever, there are a few weak points which should explain my overall score and could potentially strengthen the paper:\n\n* The evaluation of the proposed approach focuses on small graph benchmark datasets, and I personally feel that this dataset selection is not sufficient to assess the quality of the presented methodology. Does this architecture bring any gains when operating on larger graph benchmark datasets, e.g., OGB?\n\n* Since the main contribution of the paper is the proposal of a novel framework to tackle the task of graph classification, most of the used building blocks (GCN, adversial autoencoder, triplet loss, kNN) are exchangeable. Therefore, I would have loved to see a more rigorous evaluation of the overall framework. For example, every other GNN (GIN, DiffPool, ...) is also applicable. For the final task of node classification on the SuperGraph, one could also apply a GNN. It is not clear why authors restrict themselves to this specific kind of setup.\n\nMinor comments:\n\n* Since the proposed architecture is more complex than a typical GNN, how expensive is the usage of the additional building blocks in practice?\n\n* Are both zero-one loss and triplet loss equally important for obtaining good similarities or is one of them already sufficient to obtain good performance?\n\n* The heterogeneous space alignment procedure is not absolutely clear to me. As far as I understand, its main goal is to align graph embeddings to the same distribution. However, it is not clear to me why you are encoding/decoding node embeddings rather than graph embeddings. Furthermore, shouldn't the hidden codes of the autoencoder be used for further processing since those represent the aligned representations?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}