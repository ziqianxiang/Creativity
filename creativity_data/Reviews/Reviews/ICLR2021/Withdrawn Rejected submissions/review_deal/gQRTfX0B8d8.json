{
    "Decision": "",
    "Reviews": [
        {
            "title": "Unsurprising results with unclear motivation an applications",
            "review": "This paper aims to extend the landscape of backdoor attacks to unsupervised and generative models, such as autoencoders and GANs. For autoencoders, the authors assume that an adversary can attach backdoor triggers on the training images and modify the loss function such that it minimizes the distance between the targeted images (decided by the adversary) and the decoded image. For GANs, the authors assume that the adversary can modify the input (noise vector) of the generator and use an additional discriminator to differentiate the poisoned images from target distributions (decided by the adversary).\n\nS1: The experimental results show that backdoors can be successfully injected into autoencoders and GANs under certain conditions.\n\nW1: The assumptions are too strong to make the attacks practical. \nW2: The experimental results are not surprising compared to existing results from supervised learning tasks.\nW3: The experiment settings are questionable.\nW4: The author did not explain why the backdoors are a security concern in an unsupervised or generative task.\n\nDetailed comments and questions:\n\nFor autoencoders, you assume that attackers can modify the loss function. Why? Isn't it controlled by a defender? Similarly, for GANs, you assume that the noise vector can be modified by an adversary and there is another discriminator used by the training process. The noise vector and discriminator, however, are also controlled by the defender. This severely limits the contributions of this paper. Please give clear example applications where the above assumptions are valid.\n\nThe experiments are not convincing. For example, there is no evaluation based on the attack success rates for generative models. This also raises a fundamental question about the motivation: given the lack of ground-truth labels, is the proposed \"attack\" really an attack that causes security risk to anyone? In addition, why do you use the mean square error as the loss function for the CIFAR-10 and CelabA, but the binary cross-entropy for the MNIST dataset?\n\nOverall, this paper tries to show that backdoor attacks can be injected into generative models. However, the attacks require changing not only the data distribution but also the training algorithm, which deviates from the common definition of \"backdoor attacks\" and severely limits the practicability. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper performs backdoor attacks to autoencoder and GANs.",
            "review": "This paper proposes to backdoor attack the autoencoder and GANs. \nThe empirical performance looks promising. For attacking GANs, it would be helpful to measure the distribution distance between the targeted backdoor distribution and the original one. Currently, only qualitative results are shown in terms of the difference of the two distributions which is hard to compare.\n\n[1] is very related though it focuses on adversarial attack against the generative models and the authors can also show whether the backdoor attacks can be generalized or transferred to VAE-GAN models.\n\nThe paper shows an interesting phenomenon while lack of certain analysis and understandings. For instance, whether such backdoors transfer to different models. What if slightly different backdoor triggers are used. How many types of backdoor triggers are tolerable. Can we use each trigger to represent each target? I imagine this will be hard for the distribution as a target attacks. The main methodology part of the paper is very limited which makes the technical contribution of the paper concerning. If would be good to draw more insightful conclusions and analysis based on the attack results and comparisons. For instance, whether the existing defenses against the backdoors are effective here or not.\n\n\n[1] Kos, Jernej, Ian Fischer, and Dawn Song. \"Adversarial examples for generative models.\" 2018 ieee security and privacy workshops (spw). IEEE, 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "May be a good blogpost",
            "review": "## Overview \n\nThe paper applies classic backdoor attacks to auto-encoders (AEs) and GANs and shows such attacks' feasibility. \n\n## Contributions & Strengths\n\nThe paper might be the first to show the application of backdoor attacks in AEs and GANs. The following are the specific contributions:\n\n1. showing the possibility of backdoor attacks in AEs to 1) decode triggered images into a fixed image, and 2) decode the triggered images into an arbitrary version of the input image, e.g., the inverse of the input image. \n2. showing the feasibility of backdoor attacks on GANs by using the change in a single element of the noise vector as a trigger.\n\n## Weaknesses\n\n1. The paper doesn't have any novelty, and there are no new insights in this paper.\n\n2. There are inconsistencies between the text and the figures, and the captions (see comments).\n\n## Comments for the authors\n\n* In the results section, you have: \"we set the trigger as a pink square at the top-right corner for both CelebA and CIFAR-10 and a white square for the MNIST dataset,\" while Figure 2 shows the triggers on the top-left. \n\n* Figure 2's caption says: \"Input and output of backdoored CelebA autoencoder,\" while the figure shows AE results on all three datasets.\n\n## Evaluation logic\n\nThe paper is clearly below the standards of ICLR, and hence I vote for a clear rejection. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Motivation not clear",
            "review": "This paper investigates the backdoor attack against autoencoder and GAN-based models. They showed that the adversary can build a backdoored autoencoder (and GAN) that returns a target output for all backdoored inputs (a different distribution) while behaving perfectly normal on clean inputs. \n\nPrior works mainly focused on membership inference attacks against generative models. In this paper, they extend the backdoor attack to autoencoder and GANs. \n\nThe quality of the paper writing could be improved. The two parts (AEs and GANs) are not closely related, this paper seems like simply combining these two parts to fill the space. Also, the motivation for attacking autoencoder is not clear, it makes more sense to me to attack a classifier with autoencoder learned features. Autoencoder is designed to learn a low-dimensional feature representation so a perfect autoencoder can return the input itself. As for me, it is more like a feature extraction stage of other downstream tasks, so it is meaningless and even unrealistic to attack this part since it is in the pre-trained model. Except from this point, the extension is trivial, I don't see enough novelty of this idea.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}