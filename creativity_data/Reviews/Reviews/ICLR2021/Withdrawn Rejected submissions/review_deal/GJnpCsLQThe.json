{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper, the authors provide a Riemannian version of gradient descent/ascent for min-max problems on manifolds. Assuming a tractable retraction mapping for the descent/ascent step, the authors provide a complexity analysis for finding a (local) saddle point in the spirit of Lin et al. (2020).\n\nThe paper received three negative recommendations and one positive, with all reviewers indicating high to expert confidence. After my own reading of the paper, I concur with the majority view that the paper does not clear the (admittedly high) bar for ICLR. After discussing the paper with the reviewers, the concerns that led to this recommendation are as follows:\n1. On the proposed examples: it is not clear what exactly is the motivation for the DNN training example with orthonormality constraints on the weights. In the paper, $x$ is the vector of DNN weights, so it lives in some real space $R^d$. The authors subsequently assume it is constrained to live on some Stiefel manifold, but which one? The Stiefel manifold is the set of all orthonormal $r$-frames on $R^d$, and the formulation in Section 6 doesn't clarify things. Moreover, the papers cited by the authors either concern the initialization of the DNN or a regularization by an orthonormality/orthogonality penalty. This is quite different since DNN training typically involves at least some neurons with very large weights (which is of course disallowed if the weights are constrained to live on some \"norm 1\" subspace).\n2. The presentation is often lacking in mathematical rigor: in a Riemannian setting, it is crucial to distinguish between the Riemannian distance function and the Riemannian norm. However, the two were used interchangeably at several points, and the authors' revision wasn't satisfactory in this regard - even the basepoint for the norm is missing in cases where it is not made clear from the context on which point the norm is considered.\n3. The cost of applying retraction-based methods in DNNs is also unclear. On a Stiefel manifold, the only known retractions involve SVD, so they have a superlinear computational cost relative to the size of the input matrix. If the dimensionality of the input matrix is that of the parameter space of the DNN, the efficiency of the method seems somewhat limited.\n\nThe above concerns regarding DNN training are perhaps less important if we view this as a primarily theoretical paper. In that regard however, the novelty of this paper over that of Lin et al. (and other Riemannian minimization papers) is not clear, so I am forced to recommend rejection at this stage. That being said, I believe that a thoroughly revised version of this paper could ultimately be publishable at one of the top venues of the field, and I would strongly encourage the authors to pursue this."
    },
    "Reviews": [
        {
            "title": "The paper focuses on an important class of problems",
            "review": "The paper proposes Riemannian algorithms for min-max problems where the min problem is over a manifold and the max problem is a strongly convex problem. It presents a rigorous convergence analysis of the algorithms proposed. Finally, experiments show the good performance of the algorithms for robust training of DNNs over manifolds. \n\nThe paper is nicely written and overall easy to follow. The work is relevant for practitioners who use similar min-max problems as it provides necessary theoretical backing to the use of such constraints in DNNs. \n\nComments\nAssumptions on the manifold are missing, compactness is required? What are other restrictions?\n\nSince the max problem is strongly convex, one can in principle directly work the formulation min_{x in M} Phi(x). The Euclidean gradient of Phi(x) can be easily computed (Danskin’s theorem). (One work in a similar spirit is [1]). Of course, I assume that the max problem is solvable with an iterative solver which is easy to implement in many cases. This may be easier to analyze, no? If that is so, then why go for explicit updates of the max variable y? A discussion on this is missing. \n\nThe main contributions are theoretical, and to that end, the experiments suffice. However, it can be broadened by including other baselines. This will strengthen the paper.\n\nThere are some issues with how to use \\citet and \\citep while citing papers, especially in Section 2.2. For example, in “More recently, Sato et al. (2019) has proposed…,” it should either be \\citep or has ---> have. Please look into other such issues in the paper.\n\n[1] Jawanpuria, P. and Mishra, B., 2018, July. A unified framework for structured low-rank matrix learning. In International Conference on Machine Learning (pp. 2254-2263).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Significant mismatch between theory and experiments; incremental theory.",
            "review": "This paper studies the convergence of min-max optimization algorithms when the minimizer takes values in a manifold and the maximizer in a subset of R^d. The authors proposed three algorithms and analyzed the convergence rates to stationary points. Some experiments are provided to evaluate the proposed algorithms.\n\nThere are two major issues with the paper:\n\n1) There is essentially no novelty in the theory. Specifically, under the very strong assumptions 1-5 and that the maxmization is strongly concave, one can simply pretend that the retraction is a gradient step and then the analysis for gradient descent-ascent would hold almost verbatim for the proof provided by the authors. In other words, as the difficulty of analyzing Riemannian algorithms is already \"assumed away\" by the assumptions, there is virtually zero element of Riemannian geometry in the proofs. As a result, I found the theory to be incremental at best.\n\n\n2) The empirical evaluation is rather incomplete and disconnected from the theory:\n\n2a) First, the only algorithms presented are the ones proposed in this paper. Where are the baselines? What is the retraction used and what about its computational efficiency?\n\n2b) The main motivation (and the only considered applications) of the paper is robust training. However, the robust training problem is more commonly formulated as min-\\sum-max instead of min-max-\\sum, and there is a significant difference in optimizing these two objectives. The authors did not provide any justification as why they could switch the order of \\sum and max at will.\n\n\nBased on the above, I recommend rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "In this paper, the authors present and analyze a class of gradient-descent algorithms for solving min-max problems when the first (minimization) variable is constrained to live on a Riemaniann manifold. In the case when i) a retraction and an isometric transport are available on the manifold; and ii) the objective is strongly convex and smooth in the second variable, the authors show convergence rates. Experiments are performed with the setting of minimizing losses of neural nets whose weights are constrained to live in the Stiefeld manifold while an attacker of small norm perturbs the input.\n\n\nThis work is one of the first I see tackling explicitly min-max optimization over a Riemannian manifold. The presented techniques are interseting and seem to have some applications. However, the presentation and writing of the make it quite hard to follow and, in my opinion, not ready for publication at that stage.\n\n\n* The introduction is rather well written and nice to read.\n\n* Then, there is an imprecision that has been bothering me throughout the paper: the authors seem to actually define the nabla_x f(x,y) directly as the Riemannian gradient, not the usual gradient. This is done directly without mentioning it in the bottom of page 3 (otherwise, all the gradient transportations do not make sense). Considering how the functions are defined in eg. (3,4), nabla_x f could stand for the full gradient naturally and then, grad_x f = proj T_x (nabla_x  f ) could be the Riemannian gradient. I think this can be easily fixed but presently it can cause a lot of confusion. \n- In addition, in Assuùmption 1, I do not get the inequality with L21. nabla_y lives in R^n so the transport is identity?...\n\n\n* The authors state that \"this is the first study of minmax optimization over the Riemannian manifold\" however, there seem to be some works on finding stable points of variational inequalities (by extragradient and the like) on Manifolds, eg. \n- (see Sec. 5.2 ) Li, Chong, Genaro López, and Victoria Martín-Márquez. \"Monotone vector fields and the proximal point algorithm on Hadamard manifolds.\" Journal of the London Mathematical Society 79.3 (2009): 663-683.\n- Wang, J. H., et al. \"Monotone and accretive vector fields on Riemannian manifolds.\" Journal of optimization theory and applications 146.3 (2010): 691-708.\n- Ferreira, Orizon Pereira, LR Lucambio Pérez, and Sandor Z. Németh. \"Singularities of monotone vector fields and an extragradient-type algorithm.\" Journal of Global Optimization 31.1 (2005): 133-151.\n\n* In assumption 1, the terms L11 and L12 seem to implicitly contain curvature information on the manifold. Could the authors comment on that? I guess that Assumptions 2 may also imply some \"hidden\" conditions on Y.\n\n* Theorem 1: In the case of deterministic gradients ( ie (8) ), I do not get over what the expectation is. Either it is related to the gradient and hence not need. Or, it is related to the random output (line 10) in which  case the sum seems not needed (as in E[H_\\zeta] below). \n\n* In Table 2, How are the algorithms stopped, is it after 1000 iterations?\n\n* I do not get how the authors obtain the first line after \"By plugging the inequalities (22) into (25), we have\" in Appendix A\n\n\nMinor Comments/Typos:\n* Abstract: \"on the Riemannian manifold\" -> \"on Riemannian manifolds\"\n* Preliminairies: several typos \"We define a retraction R maps tangent space\" \"Exp mapping is a special case of retraction that approximate the Exp mapping up the first order\" (!)\n* Theorem 1: \"Suppose the sequence () be generated\"\n* In Remark 2, the setting B=T seems quite unrealistic.\n* Figure 3: \"Iteratons\"->\"Iterations\" in the 3 subfigures.\n* between (22) and (23) \"By the function f is strongly concave\" \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper should be rejected due to the novelty issue",
            "review": "1.Summarize what the paper claims to do/contribute. Be positive and generous.\n\nThis paper considers solving the minimax saddle point of the form \\min_{x\\in X}\\max_{y\\in Y} f(x,y), where X is a Riemannian manifold and Y is a closed convex set. The objective function f is nonconvex in x and is strongly-concave in y. To the best of the reviewer’s knowledge, this min-max problem with Riemannian manifold constraint has not been considered in the literature. To solve such a problem, a two-time-scale manifold gradient descent ascent (MGDA) approach is then proposed to solve the problem in the deterministic case. An iteration complexity of O(\\kappa^2\\epsilon^{-2}) has been derived for the MGDA approach, where $\\kappa$ is the condition number of the problem. When $f$ is in the form of an expectation or a finite-sum of a large number of component functions, the author proposes a manifold stochastic gradient descent ascent (MSGDA) approach and a variant of MSGDA with the gradient estimator constructed with a momentum-style SARAH/SPIDER technique. Sample complexities of O(\\kappa^4\\epsilon^{-4}) and \\tilde O(\\kappa^3\\epsilon^{-3}) have been derived for the MSGDA and MVR-MSGDA methods respectively. The adversarial training of DNN and a specific instance of the DRO problem are proposed as the motivating example of this work. \n\n2. Clearly state your decision (accept or reject) with one or two key reasons for this choice.\n\nThis paper should be rejected.\n\n3. Provide supporting arguments for the reasons for the decision. \n\n(i). The results for RGDA are largely from section 4 of (Tianyi Lin et al., 2020). Though the authors consider the Riemannian constraint on x, from the author’s experience in Riemannian optimization, it does not make much difference as long as the retraction operator is available. \n\n(ii). The RSGDA analysis also shares the same novelty concern as stated in (i). Moreover, the sample complexity of (Tianyi Lin et al., 2020) is only O(\\kappa^3\\epsilon^{-4}), which is much better than the O(\\kappa^4\\epsilon^{-4}). The reviewer believes that following the proof of (Tianyi Lin et al., 2020) step by step while replacing the gradient step by gradient-retraction step, O(\\kappa^3\\epsilon^{-4}) will be achievable for RSGDA. \n\n(iii). Technically, there are many non-rigorous arguments. E.g., the authors frequently use \n\\|x_{t+1} – x_t\\|^2 \\leq \\gamma^2\\eta_t^2\\|v_t\\|^2. However, this is wrong on manifold. The authors seems to believe \\|R_x(u) - x\\| = \\|R_x(u) – R_x(0)\\| \\leq \\|u\\|, but this is not true. The rigorous statement should be: Exist d,c>0 s.t. for any x\\in X, for any u\\inT_xX with \\|u\\|\\leq d, we have \\|R_x(u) - x\\| \\leq c\\|u\\|. These parameters should present in the final complexity result. They represent the property of the manifold itself. \n\n(iv). Although the result for momentum SARAH/SPIDER variance reduction seems new in this setting, merely this part is not enough contribution.\n\n4. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\n(I) The authors should carefully describe the relation of their result and that of (Tianyi Lin et al., 2020). Specifically, clearly state the differences and additional difficulties when generalizing the result of (Tianyi Lin et al., 2020).\n\n(II) The reviewer suspect that the result of RSGDA can be improved to O(\\kappa^3\\epsilon^{-4}) by following the procedure of (Tianyi Lin et al., 2020) more carefully. If this can be done, it will be better fill this gap. If this cannot be done, the authors should state the difficulty. \n\n(III) Since this work generalizes (Tianyi Lin et al., 2020), and the Riemannian manifold does not add much difficulty. So it might be better to include the nonconvex-concave case in (Tianyi Lin et al., 2020), this will make the result more complete. \n\n(IV) The authors should carefully review their statements w.r.t. Riemannian manifold so that the mistakes pointed out before does not happen. Though, generalizing from Euclidean optimization to Riemannian optimization is conceptually simple. But there are also some subtly in the arguments. \n\n\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}