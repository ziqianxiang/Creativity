{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper adapts the ideas around universal successor features for decentralised multi-agent environments, with a particular emphasis on deriving better exploration from them. Like most of the reviewers, I think this is indeed a promising research direction. Given the complexity of the endeavour however, it may take a few more steps until the empirical evidence can back up the authors' ambition: the reviewers' consensus on the current version of the paper is that it is not ready for publication yet."
    },
    "Reviews": [
        {
            "title": "There are still some issues",
            "review": "Some popular methods like VDN and QMIX focus on the monotonic factorization of joint-action value function, which is not realistic in non-monotonic cases when the agent’s best action depends on other agents’ actions. This phenomenon is common. For example, in the prisoner’s dilemma the value function can be monotonically decreasing in each of the agent’s local value function. One of the effect this paper focuses on is that the monotonically factorization lacks the representational capacity to distinguish the values of coordinated and uncoordinated joint actions during exploration. This effect is well explained in the predator-prey game example, where both VDN and QMIX have undesired performance. \n\nRecent work like QTRAN and WQMIX tried to address the problem of inefficient exploration caused by monotonic factorization. However, these approaches still rely on inefficient-greedy exploration which may fail on harder tasks e.g., again, the predator-prey task above with higher value of p.\n\nThis paper applies universal successor features to the multi-agent setting (Multi-Agent Universal Successor Features MAUSFs). Decentralized agent-specific SFs with VDN enables agents to compute decentralized greedy policies and to perform decentralized local GP. The two components have some good synergy and improves VDN to some extent. The propose Universal Value Exploration (UneVEn) can solve tasks with nonmonotonic values. Both the combination of SF and VDN and the UneVEn algorithm are very intuitive and are easy to understand and implement.\n\nHowever, the strength of the results are moderate. As it is very natural to combine the two it suffices to figure out how strong the synergy is between. This is not observed either through theoretical insights or experimental studies. To be specific, the experiments study only the game of predator-prey and do not yet demonstrate the algorithm’s generalization power beyond this motivating task. It should outperform existing approaches on a wider range of tasks where the monotonicity does not hold, and be at least competitive on tasks where such an assumption hold, to be valuable. The experiment on Zero-shot generalization is interesting, but SF along should be able to present such an effect of transfer learning (generalization). The paper needs to show how the synergy between SF and VDN is really like in the task of transfer.\n\nPro\n \t1. This paper keeps up with the frontier of MARL. It discusses a lot of algorithms proposed recently and provides a detailed background knowledge.\n\n2. It clearly stated the current research gap in value function factorization. \n \t\nWeak points \n \t1. This paper might not fill the research gap it mentioned very well. For example, in the introduction part, it says the shortages of VDN and QMIX is that they are restricted by monotonic property. However, it states in the UneVEn section that, \"the basic idea is some of the related tasks can be efficiently learned using a monotonic joint-action value function. \" Does it mean that the implementation of UneVEn still relies on monotonic value function? \n \t\n2. The author can elaborate more on the experiments from an intuitive view, rather than just stating the experiment data.\n \tFor example, as the experiment shows, the UneVEn only outperforms as the penalty parameter p get larger and larger. What's the reason behind it? How does UneVEn overcome the \"curse\" of suboptimal stuck in VDN and QMIX? \n\n3. More experiments on general tasks should be provided.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review for UneVEn",
            "review": "##########################################################################\nSummary:\n\nThis paper studies the problem setting of cooperative multi-agent RL (coop-MARL) under centralized training with decentralized execution (CTDE). Additionally, it assumes that the reward function is known and is represented as a weighted linear combination of a basis function.  \nThey consider Q-learning with value-decomposition as the basis for tackling such problems and improve upon SOTA methods for coop-MARL in the domain of predator-prey (PP). Specifically, they claim to improve upon monotonic value-decomposition methods such as VDN and QMIX by better addressing the issue of relative overgeneralization which stems from their monotonicity constraint in representing the joint-action value function. This paper has a similar objective as Weighted QMIX, but it achieves it through implicit weighting by learning over a set of related tasks (reward function weights).    \nTheir approach is to extend Value-Decomposition Networks (VDN) by combining it with a multi-agent version of USFs (Successor Features + reward-function-based Universal Value Functions), called MAUSFs. Then, they use a related-task sampler during exploration to sample simpler tasks and through this, they claim that more importance is placed on better joint actions implicitly.   \n\n##########################################################################\nReasons for score:\n\nOverall, I think this is, in general, a sound paper and so would like to see it accepted. As a paper to inspire and give insights on issues in coop-MARL through a creative approach, I like this paper. However, regarding the general applicability and its practical usefulness, I'm not fully convinced yet. Additionally, I have some other concerns which, hopefully, the authors can address during the rebuttal period. \n\n##########################################################################\nPros:\n\n- The approach is innovative.\n\n- The experiments are targeted and illustrative. The zero-generalization experiment is also useful.\n\n- Paper is well-written and covers the literature appropriately.   \n\n##########################################################################\nCons:\n\nPlease see my questions below.\n\n##########################################################################\nQuestions during the rebuttal period:\n\n1) How can we ensure pi_z is optimal on task z? Isn't the optimality of pi_z on task z based on which the SF is learned a necessary condition for the GPI update to apply?\n\n2) I'm not fully convinced how in general this method can be expected to improve regarding the issue of relative overgeneralization? How do we know that these related tasks would bias the monotonic approximation towards \"more important\" joint actions? Is there a guarantee in the limit of tasks sampled by some distribution D? \n\n3) I have some concerns regarding the general applicability of this approach. To the best of my understanding, this work is limited to domains where the reward function is known and represented as a linear combination of some basis function. To me, this seems like a very challenging issue to overcome as breaking down the reward function based on experienced reward signals is perhaps as hard as learning the optimal action-value function (loosely speaking). \nSo can this approach really lead to more generic extensions that would work on unknown and general reward functions? \n\n4) Why does VDN's gap w.r.t. UneVEn narrows down (instead of widening) in p=0.004 (nonmonotonic) vs. p=0 (monotonic)?\n\n5) Why does QMIX perform so much worse than VDN in p=0.004 (in fact, on all tasks in Figures 2 & 3 this is the case)? Shouldn't QMIX's ability to represent a larger class of decompositions improve performance over VDN?\n\n##########################################################################\nMinor comments:\n\n- Section 1, paragraph 3: \"QTRAN (Son et al., 2019) and WQMIX (Rashid et al., 2020a) addresses...\" -> address\n\n- Remove the first comma in Proposition 1: \"For, the predator-prey game defined above,...\"\n\n- I think \\pi*_z should be used to indicate the optimal policy on task z is meant.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not well motivated and lack of significance ",
            "review": "This paper employs the linear value factorization proposed by VDN and extends universal successor features with GPI to multi-agent reinforcement learning. It also proposes an exploration method, called universal value exploration, that biases an agent’s action selection by utilizing related tasks. Empirical evaluation demonstrates its outperformance over baselines in predator-prey tasks. \n\nHowever, I have some major concerns about this paper.  First, the motivation of this paper is confusing, that is, the monotonic restriction leads to inefficient exploration. Exploration and the expressiveness of value factorization do not have a causal relation. Some exploration strategies can allow VDN and QMIX to learn the optimal strategy even with non-monotonic returns. One of such trivial exploration policies is the optimal (or nearly optimal) policy.  \n\nThe proposed MAUSFs approach looks like a simple extension of single-agent successor features to multi-agent settings by using linear value factorization.  Are there challenges for such an extension? \n\nI don’t think the proposed UNEVEN exploration method can address the relative overgeneralization pathology. The relative overgeneralization pathology is caused by the limited function class of some value factorization methods. QTRAN and QPLEX can potentially address this issue, but they may not perform well in some tasks that require more efficient exploration.  \n\nI don’t think it is fair to compare UNEVEN with most of the baselines in this paper. It is because UNEVEN uses the linear parameterized representation of reward functions that allows generating related tasks. In contrast, some baselines do not aim to address the exploration problem or do not take additional information. \n\nThe experimental evaluation can be improved. For example, it can include more complex settings, like SMAC.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "clever idea, limited experiments",
            "review": "The paper develops and evaluates an algorithm for decision making in the CTDE MARL setting (centralized training and decentralized execution for multiagent reinforcement learning). That is, the concern is how to use closely supervised training to produce agents that can work independently toward a common goal. The problem is formalized in the DEC-POMDP (decentralized partially observable Markov decision process) setting.\n\nThe proposed solution uses elements of several recent and promising approaches including value decomposition networks, QMIX (a nonlinear monotonic combination of individual utility functions), linear reward functions, universal value functions, successor features, generalized policy improvement. The novel idea is combining these ideas to create multi-agent universal successor features.\n\nThe power of the resulting algorithm was demonstrated in a set of predator-prey games with increased difficulty of learning to coordinate due to an increasing penalty value for accidental miscoordination. It was shown that other methods for this problem were not able to handle learning in the setting of high costs for coordination, but the proposed algorithm could.\n\nThe paper is quite good for what it claims to do. But, my impression is that the contribution is not very large. Specifically, the main demonstration was (significantly) improved performance on a predator-prey task. However, I am skeptical that these results will generalize  to other domains---these kinds of problems are quite hard and it's not clear if ANY algorithm would be expected to solve a diverse collection of real-world challenges. In the context of what the paper was trying to do, additional domains (3 distinct ones, say?) would go a long way toward characterizing the space of domains where this approach is appropriate.\n\nI guess I'm saying that this algorithm might be THE algorithm you'd want to use (it creatively combines a number of promising component ideas), but the paper didn't provide an argument for why the reader should be convinced that the positive results generalize.\n\nSome detailed comments:\n\n\"We now propose two novel action-selection schemes based on related tasks with probability 1-eps, and thereby implicitly weighting joint actions during learning.\" I'm afraid I wasn't able to parse this sentence. Reword?\n\npg. 6: Maybe putting the graphs on the same y-axis would aid in comparisons between settings (different values of p).\n\n\"However, both QMIX and VDN fail to learn on other three higher penalty target tasks\" -> \"However, both QMIX and VDN fail to learn on three other higher penalty target tasks\"?\n\n\"between different action selection\" -> \"between action-selection schemes\".\n\n\"learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and propose\" -> \"learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and proposes\"?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}