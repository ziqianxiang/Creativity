{
    "Decision": "",
    "Reviews": [
        {
            "title": "The contribution seems meaningful, but I have a few things unclear",
            "review": "***\n\nSummary:\n\nThe submission studies the application of the information bottleneck (IB) principle in reinforcement learning. The proposed approach is summing up the IB loss and RL loss and optimize their total jointly. The paper derives certain optimality guarantees when the objective can be effectively minimized, and the solution is close to the optimum in terms of the state-representation mutual information. As a randomized neural network is applied for representation generation, the aforementioned problem might be hard to optimize in practice. A solution to this is to follow the variational information bottleneck (VIB) method and leverage the objective's lower bound. Stein variational gradient descent is then applied to the optimization problem. And the resulting method improves vanilla RL algorithms' performance and outperforms the VIB and an IB-RL algorithm based on MINE, a popular method for mutual information estimation. \n\n***\n\nReasons for score:\n\nMy current score is weak reject, indicating that the paper is marginally below the threshold. I think the idea of deriving an information-theoretic framework for reinforcement learning is meaningful and worth exploring. The paper's writing is also good and easy to follow, with relevant materials in each (sub)section. I have a few concerns regarding the assumptions, formulation, experimental settings, and significance of the main theorems. Please refer to the section of the cons for details. \n\n\n***\n\nPros:\n\n \n1. The IB principle is a useful tool for designing statistical algorithms. At a high level, it assumes a Markovian relation, $X\\to Z\\to Y$, and aims to find a proper mapping from $X$ to $Z$ that approaches $Y$ but has little additional information about $X$. In the paper, $X$ is the state, $Z$ is the state's representation (generated by a randomized NN), and $Y$ is the reward. Hence, it could be rewarding to apply the IB principle to RL tasks, showing the research problem's value.\n\n2. The proposed algorithm successively updates the parameters of the representation NN and the policy, which is efficiently implementable and generalizable (Equation (18) and (19)). In particular, we can consider different NN architectures and RL policies. The theoretical reasoning shows the design principle and intuition behind the algorithm, while the empirical evaluation in Section 5  demonstrates its effectiveness over existing ones. \n\n3. The paper also provides an algorithm based on the popular MINE mutual information estimator. Appendix C.5 compares this algorithm with the one mentioned above, which is a good plus. \n\n***\n\nCons:\n \n1. A major concern of mine is the significance of the paper's theory. \n\n- First, Equation (4) seems like a powerful assumption, saying that the distribution of the reward $R_t$, conditioned on the representation $Z_t$, is simply a distribution of form $P(R_t\\vert Z_t) \\propto \\exp(-\\alpha(Y_t - V^\\pi(Z_t)))$, where $V^\\pi$ is the expected return of the corresponding state.  The reason for this particular choice, as stated in the comments after Equation (4), is to make the probability decrease \"as $V^\\pi(Z_t)$ gets far from $Y_t$.\" I do not find this reason convincing and would like to understand the model choice more, especially about how practical such an assumption is. On the other hand, this distribution model greatly simplifies the RL objective, which is an expectation of $\\log P(R\\vert Z)$ and now reduces to $-\\mathbb E[\\alpha(R-V^\\pi(Z))^2]$, or essentially the negative MSE.  \n\n- Second, in the main theoretical claim, Theorem 1, the solution compared with the optimum is the optimal solution of a (usually) non-tractable optimization objective (Equation (8)). It seems that this objective is slightly far from that being addressed in the subsequent sections. I wonder if this creates a problem. Besides, the guarantee is stated in terms of mutual information closeness, which doesn't seem straightforward. \n\n \n2. Another concern that I have is the novelty of the paper. As mentioned in Section 2, \"related work,\" and page 5 (before Theorem 2), similar results have been derived. I suggest the authors provide more discussions and comparisons as the present statements are not quite informative. \n\n- For example, when comparing the proposed algorithm to VIB, the last paragraph on page 2 says that \"our method can learn arbitrary distributions.\" This seems to be a strong claim. May I know what exactly the word \"learn\" refers to? \n\n- For another example, page 6 says that Liu et al. have similar derivations as those shown in Equation (10) and (11), but the underlying principle is Bayesian inference instead of IB. It might be helpful if the paper can clarify (at least intuitively) how different derivations lead to similar results so that readers can see the differences. \n\n\n3. Below are a few additional comments/suggestions/questions.\n\n- Regarding the hyperparameter $\\beta$ for the information bottleneck loss (Equation (3)), Appendix C.6 shows that it can affect the proposed method's performance. Besides, Section 5 states that different tasks require different $\\beta$ values for the best results. I wonder how this parameter was chosen in the experiments. \n\n- Appendix C.5 compares the IB-MINE algorithm with the proposed one and shows that the former often has worse performance. Based on this, the abstract and page 2 (point 4 before Section 4) claim, \"we find that using MINE (Belghazi et al., 2018) for optimizing IB is not suitable in RL\". However, since this is just a particular IB-MINE algorithm, I wonder if the claim is too strong. If so, is there a way to improve the MINE-based algorithm? \n\n- I wonder if there is an explanation for choosing $U(Z)$ independent of $\\phi$ when constructing the variational lower bound (page 5, Equation (13)). \n\n- A suggestion: It might be better to give an example for $\\xi$ when defining the neural network $f$ on page 3. In the actual implementation, $\\xi$ is often a Gaussian random variable with small variance. \n\n- Typos:  Page 3, first paragraph of Section 3, change $R:X\\times A\\times X$ to $R:\\mathcal X\\times A\\times \\mathcal X$? Page 4, before Equation (5), change \"Equation equation 3\" to \"Equation 3\".\n\n*** ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "lack of comparison with other methods",
            "review": "This paper proposed using an information bottleneck (IB) regularizer in reinforcement learning (RL). The authors (i) formulated an objective with IB regularizer as in Eq. (10); (ii) proposed using a lower bound of the original objective as in Eq. (12); (iii) used the Stein variational gradient descent (SVGD) method to optimize the lower bound objective; (iv) did experiments on Atari games and showed the regularized methods outperform A2C and PPO.\n\nPros:\n1. Using IB regularizer in RL to learn features seems to be a reasonable and novel idea. \n2. The paper is well written, and the presentation is clear.\n3. The experimental results on Atari games using A2C and PPO look promising.\n\nCons:\n1. The main point of this paper is to claim IB regularizer is helpful for learning good features. But it seems the experiments are not enough to show the learned feature are better. For example, how does the learned feature generalize to a slightly different task (like what has been done in other representation learning papers)?\n\n2. As a paper which proposed an IB regularization method for learning features, the comparison is only conducted with A2C and PPO baselines, and another IB variant method. I think it is important to show how the IB regularizer benefits learning features comparing with other methods, such as successor feature [1], and Laplacian method [2]? This is related to the first point since in those work generalization ability of learned features has been shown to argue that the learned representations are good.\n\n3. The theoretical results are actually not insightful. Theorem 1 is some basic fact that holds for any objective with regularization. Theorem 2 actually is almost identical with results in entropy / KL divergence regularized RL literature. From these theoretical results, there is no much insight provided about why using IB regularizer has benefited from theoretical perspectives.\n\n4. The adaptation of the SVGD method is confusing. Why is this method necessary here (what kind of problem needs to be tackled by SVGD that cannot be resolved by other optimization methods) and why is this part a novel contribution to this paper (what is the difference that this paper made compared with the original SVGD paper)?\n\nOverall, I found the idea reasonable and the empirical results promising. However, there are not enough results from both the theoretical and the experimental sides for why the IB regularization helps learn better features comparing with other methods.\n\nReferences:\n[1] Improving generalization for temporal difference learning: The successor representation, Dayan.\n[2] A Laplacian Framework for Option Discovery in Reinforcement Learning, Machado et al.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear benefits of the proposed method",
            "review": "In this paper, the authors discuss a variational lower bound formulation for RL that is based on the information bottleneck framework. The authors use a stein variational gradient descent based approach to optimize the variational lower bound obtained with generic representations.  They then use the learned representation to accelerate the learning in an actor-critic algorithm, to be specific, A2C and PPO. To be more specific, the paper starts with a discussion of information bottleneck in reinforcement learning and then explains how to derive an objective using it, arguably using a heuristic choice of P(Y|Z), then obtain a lower-bound and optimize it via amortized SVGD (Feng et al 2017, Haarnoja et al 2017). \n\nWhile the framework is interesting, I have concerns about the empirical section of the method:\n\n- The evaluation is performed on only 20 Atari games, with A2C and PPO and not more recent and perhaps more effective RL algorithms on Atari domains. Moreover, the proposed method only obtains marginal benefits in sample efficiency. I would suggest trying such a framework for other actor-critic algorithms or Q-learning algorithms on Atari to make a stronger case.\n\n- The values of $\\beta$ for different games is chosen differently! On Atari, when testing on only a subset of the games, if different hyperparameters per game are used, there are multiple problems: (1) it makes it incomparable to compare across algorithms (2) it is very easy to overfit to a small number of games with multiple hyperparameters. So, essentially the empirical comparison is *extremely* weak due to this reason.\n\n- The mutual information estimation plots using MINE are interesting, but I do not see a major difference in Figure 11, it all seems very similar in Assault and roughly similar in Pacman. The authors say in the caption that it is clearly faster that vanilla A2C, but that is not reflected. Given mixed results for MI estimation on the three games, I would want to see more results on other games for this experiment to make a conclusion.\n\n- Baselines: A number of representation learning methods have been proposed -- though from other perspectives -- for e.g., see SLAC, Lee et al. 2019 or DeepMDP, Gelada et al. 2019. I would want to see a comparison to these approaches especially in terms of efficacy, MI values, and also some intuitive comparison of differences in the representations learned. The current paper just compares to vanilla A2C and PPO, which seems insufficient to me.\n\nOverall I think the paper will benefit from, a more rigorous empirical section, which as it stands right now is quite weak. Comparison to other methods, unifying hyperparameters, etc are at the minimum needed.    ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Applying information bottleneck in RL",
            "review": "Summary: The authors provide information bottleneck in reinforcement learning.  The author first transfer reinforcement learning into a variational information bottleneck (IB) problem. Focusing on the lower bound part, they apply the Bayesian sampling algorithm, in particular, Stein variational derivative, to conduct the optimization step. Several examples demonstrate the effectiveness of the algorithm. \n\nPro: 1. Transfer reinforcement learning into an information bottleneck problem.\n         2. Conduct the optimization by one minimization problem, the lower bound of IB,  by gradient descent methods in Stein metric. \n\nCons: Changing the IB problem to its lower bound may change the optimizer of the RL problem. \n\nHere I still have questions based on your approach. \n\n1. For transferring information bottleneck into a pure minimization of its lower bound, how much accuracy is lost? In other words, how does the solution of your lower bound problem being close to the one in the original minimizer of the RL problem? Is there any simple example to demonstrate the potential benefit of the proposed algorithm analytically? \n\n2. For the optimization approach, can we apply any other MCMC algorithm for minimizing the KL? If so, what is the advantage of using Stein variational derivative compared to other methods? \n\nE.g. some other methods may be also useful in the optimization steps. \n\nY. Wang, W. Li, Accelerated Information Gradient flow. \n\nY. Wang, W. Li, Information Newton's flow: second-order optimization method in probability space. \n\n3. How do you choose the kernel for SVGD in your optimization step? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}