{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper uses the double oracle method from game theory and applies it to GANs.\n\nThis idea is interesting and Double Oracle actually seems like a good fit to train GANs. This could lead to interesting results in the future.\n\nReviewers disagree on the clarity of the paper, probably because the game theory vocabulary is not something that is common among the papers published at ICLR, so extra care should be taken to explain these notions.\n\nThey also point out that the method only applies to some GANs and not all (in particular the loss needs to be zero-sum).\nThe experimental section is too weak and the metrics used need to be changed. Results are too far from the state of the art to be convincing. The authors based their experimental setup on DCGAN: this is too old, too many improvements have made since then. Other criticisms of the experimental section include: comparison to newer methods must be made, analysis and discussion of the results must be pushed further.\n\nGenerally, the average score of the reviews is too low for acceptance. The reviewers agree that the idea is both interesting and pertinent, but this paper cannot be published as it is now. The theoretical part mostly consists in applying double oracle to GANs, and the experimental section is too weak. At least one of these parts (preferably both) must be strengthened for this paper to be impactful."
    },
    "Reviews": [
        {
            "title": "DO-GAN",
            "review": "Summary of paper:\n\nThis paper proposes a GAN training method that involves keeping a table of historical losses for the generator and discriminator across training iterations and calculating the loss using a double-oracle framework inspired by game theory.\n\nStrengths:\n\n-- To my knowledge, this is a novel approach which uses new ideas in a related field to potentially help the unstable training of GANs.\n\n-- The background preliminaries are thoroughly explained, which is important, as much of this might be new to the target audience.\n\n-- The idea of computing losses across iterations to save on the total number of iterations that must be performed is a good one given the high computation costs of GAN training.\n\n\nWeaknesses:\n\n-- I found this paper to be poorly written and hard to understand. The game theory language used here is not standard GAN terminology and thus made it difficult for me to follow. \n\n-- If model weights themselves are compared/combined across iterations, for example, took me several read-throughs to understand because of unnecessary terminology like calling them \"policies\" and \"meta-game\" which are not used in work on GAN training.\n\n-- Not all GAN loss functions are zero-sum (in fact the best performing models e.g. Big-GAN do not use a zero-sum loss), but as I see that is required for this approach, or at least is the only thing  that is considered.\n\n-- Loss values themselves are not necessarily indicative of generative performance. There are stable equilibria that do not yield good generation, and a different equilibrium at a lower value may or may not be better. As such, only so much can be done with historical loss values.\n\n-- I don't understand how this is supposed to help mode collapse, which is claimed as a beneficial result several times in the results section. To combat mode collapse, it will have to change the way gradients are differentiated for different points. This seems prima facie unrelated to the proposed method, and if it really is causally related, that would be valuable but this needs to be investigated with deeper experimental analysis for it to be claimed.\n\n-- The experiments are weak. The toy example is entirely vacuous, as getting a regular GAN to match a handful of modes in 2D is quite possible with any number of small tricks that have been around for years and are not onerous.\n\n-- The natural image results are only on CIFAR and CelebA, and are low quality at that. Qualitatively, the generated images still look bad, and the use of a DCGAN as the main baseline is misleading as the vanilla DCGAN is nobody's standard to beat anymore. Models that have achieved state-of-the-art results in the last couple years like Big-GAN should be included.\n\n\n\nConclusion: The motivation for this particular meta-game strategy is not very clear, and the results are not good enough to be relevant to where the current state of GANs are, and as such I vote to reject.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "It will be better if there are more experimental investigation.",
            "review": "This paper proposes a  new training framework for GAN, inspired by the double oracle (DO) algorithm in game theory. The authors design many mechanisms to make it possible to employ DO in GAN training. The motivation is clear, and the experimental results support the claim. \n\nFor the proposed training framework, I have the following concerns:\n\n1. It is unclear what tasks are suitable for the proposed DO-framework. In the paper, several experiments are conducted in the laboratory and real-world environment. However, some of the more common GAN scenarios at this stage (2020) have not been proven to be suitable for DO-framework, such as high-resolution image synthesis, e.g., CelebA-HQ and LSUN scene generation [1] and conditional generation [2]. I am curious about the performance of the DO-framework in these tasks.\n2. Will computational complexity be an obstacle to the application of this method? Will the proposed method require more storage space?\n3. In figure.13, the DO-GAN stopped before 300 epochs. However, its FID still seems to be declining. If DO does not terminate the program, will it be better?\n\nReferences:\n[1] Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4401-4410).\n\n[2] Park, T., Liu, M. Y., Wang, T. C., & Zhu, J. Y. (2019). Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2337-2346).\n\nUpdates:\n\nI have read the author's response and the comments of other reviewers. Although as an \"approach as the proof of concept\", limited experiments on large-scale GAN models are still necessary. Just provide evidence to prove it is feasible. This is a little disappointment for me. For my other questions, I agree with the author's response. Since I have given a positive initial rating (6), I will keep this rating. Thanks to the author's reply and AC's efforts.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Decent ideas but evaluation is lacking",
            "review": "This paper proposes to use the well-known Double Oracle methods for solving large scale games for computing the equilibrium in GANs. The main idea of a mixed strategy being a mixture over generators (and mixture over discriminator for toher player( is from Hsieh et al. The double oracle approach is shown to yield superior results on three image datasets.\n\nPros:\nThe idea of using double oracle algorithm for GAN is novel though surprising that no one has tried it earlier.\n\nCons:\n- I feel it is very important to compare (experimentally) to Hsieh et al. ICML19. That work is also computing NE in the exact same model of mixture over generator and discriminator, which this paper borrow. Not sure why this is not done. In fact, the authors have this line about Hsieh et al \"The sampling approach may be inefficient to compute mixed NE as the mixed NE may only have a few strategies with positive probabilities in the infinite strategy space\" - do the authors have any evidence (theory or experiment) to support this?\n- The evaluation, while exploring many GANs, is missing a few recent ones. Have the authors tried LOGAN (achieves same scores almost of CIFAR). Also, wondering why WGAN is not considered - WGAN also is a game and actually better than some of the GANs tested.\n- Clearly the NE is not being computed exactly because of the many approximations (e.g., approximate best response). \n- It is very surprising that only 10 pure strategies are enough in the mixed strategy support when the pure strategy space is infinite - there are results on small support for approximate NE (please cite those also). I am not sure though if it is so small, the lack of rigor here is unconvincing.\n- There is no new technique in the way double oracle is used. That makes it seem like an engineering effort.\n- Can some of the other approaches in PSRO framework work better than double oracle? (why do you the authors call PSRO use as \"beyond games\", PSRO is still being used in a game, just a very large game)\n- I do not think comparing time by comparing epochs is fine - doesn't the double oracle epochs include Nash equilbrium computation. Please state these clearly.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper applies Double-Oracle (DO) / PSRO to training a GAN, a 2-player zero-sum game.",
            "review": "This paper applies Double-Oracle (DO) / PSRO to training a GAN, a 2-player zero-sum game. DO cannot be applied directly \"out-of-the-box\". Instead of an exact oracle, the generator and discriminator are trained using local gradient optimizers for a finite number of steps. Also, in DO, the meta-game matrix can grow very large and maintaining and training against a large population of neural networks is expensive, so the population is pruned throughout training.\n\nThe authors list 4 \"key contributions\": 1) treating D and G as players and substituting gradient optimization (Adam) for oracles to create a meta-population, 2) computing the NE of the resulting meta-game as an LP, 3) pruning the meta-population to keep the computation tractable, and 4) evaluating this approach on several GAN architectures and datasets of interest. I really only view the last one as a key contribution. The rest are contributions of prior work (McMahan, Lanctot, Cheng & Wellman).\n\nOverall, the value of the paper lies in applying DO to GANs. I have been eager to see this tried and I view it as a necessary piece of research. The authors confirm that this approach can indeed improve performance: better Inception & FID scores on MNIST, CIFAR-10, and Celeb-A. The training speedup supported by Figures 2 and 4 is relatively meagre. The improved robustness to mode dropping in Figure 3 is good. These aren't criticisms against the paper. However, in order to draw more general conclusions on these phenomena, I would have liked to see more architectures tried. For example, not all DC-GANs are created equal. One challenge in getting GANs to work is finding the right number of layers, neurons per layer, filter sizes, strides, etc. I'm curious to learn if DO makes GANs more robust to these choices. In my opinion that would be an important contribution.\n\nI would also like to see a discussion of training time for DO versus vanilla training. It's clear that DO is much more expensive than standard training, but by how much? Given the extra computation required, is training in 17 epochs vs 20 worth it?\n\nAs I said, there is not a lot of originality on the algorithmic side. Had the authors proposed a novel pruning scheme with superior performance on GANs, for example, that could have helped strengthen the paper.\n\nAlso, I found it interesting that the Nash distributions found in the appendix were sparse. This means that despite maintaining a population of players, we might be able to get away with a small number at test time.\n\n\nQuality:\nThe quality of the paper is at a high enough level for ICLR.\n\nClarity:\nThe paper is generally clear.\n- Please make it clear that \"generatorOracle()\" and \"discriminatorOracle()\" are approximate oracles obtained by an Adam optimizer, not \"true\" oracles.\n- Figure 3: Should show Epoch 17 in top row as well to support your argument.\n- In \"Results\" under Table 1, the authors state that DO-SGAN obtains much higher FID scores. I think you mean lower.\n\nOriginality:\nIn my opinion, the first 3 \"key\" contributions are really just 1: the idea of applying DO to GANs. This idea is novel, although the most interesting problems that must be solved in order to apply DO have already been tackled in previous work (PSRO - Lanctot 2017, Pruning - Cheng & Wellman 2007). The authors do not propose any significant modifications to the previous approaches. Also, the formulation of Nash of a 2-player zero-sum game as an LP is standard even within the recent DO / meta-game literature (e.g., \"Open-ended Learning in Symmetric Zero-sum Games\", Balduzzi 2019), so not a contribution.\n\nSignificance:\nDO provides a principled training regime for GANS, that although potentially expensive, does show meaningful improvement.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}