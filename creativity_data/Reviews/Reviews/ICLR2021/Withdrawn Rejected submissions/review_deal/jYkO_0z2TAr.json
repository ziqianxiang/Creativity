{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The submission proposes to leverage a commonsense knowledge graph and an attention GNN based model to aggregate the node features on the graph for the problem of zero-shot learning. It received three reviews two of them recommending rejection, and another review was initially borderline however they moved to acceptance after the rebuttal. The meta reviewer finds that the paper is not yet ready for publication and recommends rejection based on the following observations.\n\nAlthough the model is interesting, as agreed by the reviewers the initial version of the paper fell short on convincingly evaluating the method, e.g. generalized zero-shot learning (GZSL) setting as pointed out by R5, ImageNet and small scale dataset results as pointed out by R1. Similarly, the main paper (without the annexes) has been found to fall short on providing enough details of the model as pointed out by R1. \n\nThe authors ran additional experiments during the rebuttal phase which showed some promise, however one more review round may be necessary to carefully validate these results. As R1 pointed out, the paper only reports results on two small datasets i.e., AWA2 and aPY, which contain classes similar to ImageNet. It would be interesting to observe the behaviour of the model on more challenging scenarios on other publicly available benchmark datasets of fine-grained nature whose distribution are far from ImageNet. This would indicate the generalisation ability of the model.\n\nFurthermore, as pointed out by R1, moving the details on the implementation and the architecture details from appendix and from python scripts to the main paper may be beneficial. However, this would end up significantly extending the paper. Hence, one more review round may be necessary for this paper."
    },
    "Reviews": [
        {
            "title": "Limited technical contribution and experiment results not convincing",
            "review": "The submission proposed to leverage a commonsense knowledge graph and an attention GNN based model to aggregate the node features on the graph for the problem of zero-shot learning. \n\nThe main concern is the technical contribution is limited:\n\n- The authors mention \"Existing methods for zero-shot object classification such as GCNZ and DGP cannot be\nadapted to common sense knowledge graphs as they do not scale to large graphs or require a directed\nacyclic graph such as WordNet.\" Can the authors elaborate on why GCNZ and DGP can not scale to large knowledge graphs? To my understanding, GCNZ and DGP use WordNet knowledge graphs which are large-scale. And [a] also proposed to leverage GNN structure and knowledge graph, can the authors explain more comparisons with respect to this paper?\n\n- The proposed GAT model is similar to the previous works, i.e., GCNZ, and DGP. To me, the model GNN and attention (Transformer in the paper) model have already been discussed in these two works and the only difference seems to be the knowledge graph, where the previous paper uses the WordNet while the current submission uses the ConceptNet. (I think WordNet is a more straightforward knowledge graph format, can the authors explain why extra efforts on making a ConceptNet is better?) Also, [e] this paper has already proposed to leverage ConcepNet and GNN structures for ZSL, can the authors explain the difference with it please?\n\n- The authors claim the structure of Transformer while actually, it is the structure of dot product attention. The appliance of the attention of ZSL has been a lot, such as [a,b,c,d]. So I think the technical contribution is limited and it is better to also include the discussions with these works.\n\nBased on the previous three points, I believe the technical contribution is limited for this submission.\n\nThe experiments do not support the claims:\n\n- The authors claim the model can scale to large size while in the experiment section,  the results on ImageNet is not reported. The most related two works GCNZ and DPN both reported their results on ImageNet, thus i believe the performance result on this dataset is necessary. \n\n- I am expected to see performance on the generalized zero-shot learning since this setting is more practical and includes the prediction on both seen and unseen classes. The most recent papers including the previous works listed in the experiment table have the results on GZSL, so do the GCNZ and DPN. Thus, I think the results on GZSL are also necessary to support the claims in this submission.\n\n\nWriting:\nEven the basic format is not well organized, such as the tables are out of columns and spelling errors.\n\nRelated works:\nThe current related works section is a little bit precise, and I would recommend the authors to include the related works mentioned in my previous reviews and discuss the difference and how this submission is different from previous works. Since there is plenty of previous literature that is somewhat related to the current submission, a more detailed explanation of the relationships to previous works is recommended.\n\n[a] Attribute Propagation Network for Graph Zero-shot Learning, AAAI 2021\n\n[b] Attentive Region Embedding Network for ZSL, CVPR19.\n\n[c] Semantic-Guided Multi-Attention Localization for ZSL, NIPS19.\n\n[d] Attribute Attention for Semantic Disambiguation in ZSL, ICCV19.\n\n[e] TGG: Transferable Graph Generation for Zero-shot and Few-shot Learning",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Novelty of the proposed model is limited and more analyses are needed in experiments",
            "review": "The paper proposes to tackle the zero-shot learning problem by learning class representation from commonsense knowledge graphs. To capture knowledge in graphs, the paper proposes to perform Transformer-based aggregation. \n\nPros:\n- The motivation of using commonsense knowledge graphs to enhance zero-short learning is interesting (although it has been explored in the previous work). \n- The paper is in general clearly written and easy to follow.\n\nCons:\n- In term of methodology, the novelty of paper is very limited---it uses Transformer to aggregate knowledge over graphs. This is pretty straightforward, and the previous work (e.g., (Kampffmeyer et al.)) has tried Graph Attention Networks (GAT)-based models for zero-shot learning. Transformer and GAT are very similar, particularly in the setup of this work (e.g., without sequence/position embedding and the graphs are not fully connected). The differences claimed in this paper between GAT-based and Transformer-based models are rather marginal.\n\n- The empirical comparison to GAT-based models in the experiments is not clear enough. For example, Table 4 shows that the proposed model is better than ZSL-KG-GAT, but detailed analyses were not provided to make this clearer to help understand why that is the case (and hence better understand the contributions of this paper). For example, whether the performance differences are related to the use of layer normalization, the two-layer feed-forward nets in equation (4), or other reasons? Previous work suggested DGP is better than GAT-based models (Kampffmeyer et al.) and in this current submission GAT and transformer-based models are better. More discussions will be helpful. \n\n- It will be more helpful if the paper describes more details about the models in comparison, e.g., details about ZSL-KG-GAT such as the setup of multi-heads.\n\nMore comments:\n- The title of the paper may be made more specific, particularly given much work has been done by using commonsense/knowledge graphs for zero-short learning. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "Summary\n\nThis paper tackles zero-shot learning by leveraging the large-scale knowledge graph i.e., ConceptNet to propagate the knowledge learned from seen classes to unseen classes. The authors propose a novel propagation rule that aggregates node embeddings by the self-attention technique. It is infeasible to run GCN on such large-scale knowledge graph. Therefore they reduce the knolwedge graph by adopting a neighborhood sampling strategy based on random walks. The method is evaluated on multiple zero-shot learning tasks including object classification, intent classification and fine-grained entity typing. The SOTA results are achieved.\n\nPros\n\n-This paper is well motivated. The idea of leveraging a large-scale common sense knowledge graph that contains richer semantic relationship is reasonable and novel. I agree that previous GCN methods for zero-shot learning are limited by the wordnet. \n\n-The self-attention or transformer-based aggregator is novel as well. Applying such non-linear aggregation makes graph capture complex relationships between nodes. Therefore, I believe the technical contribution is significant. \n\n-The evaluation is conducted on multiple zero-shot learning tasks and the results show that the proposed approach generalizes  well to different tasks. \n\n-The paper is written well and easy to follow in general, although some important details and justification of the results are missing (I will points out the issues below).  \n\nCons\n\n-I am particularly not convinced by the object classification results (Table 1). This paper only reports results on two small datasets i.e., AWA2 and aPY, in the zero-shot learning setting, while the GCN baselines i.e., GCNZ, SGCN and DGP, are all evaluated on the large-scale ImageNet in both zero-shot and generalized zero-shot learning settings. I strongly suggest the authors to compare with GCNZ, SGCN and DGP on the ImageNet.\n\n-The results on the BBN dataset (Table 3) are not impressive. The proposed approach performs much worse than DZET \nin two metrics i.e., Loose Mic. and Loose Mac. The authors fail to give some justification for these bad results.  \n\n-The neighborhood sampling technique is rarely evaluated. I believe it is interesting to ablate different sampling strategies e.g., uniformly ramdom sampling, importance sampling (Chen et al., 2018a) and random walks. Implementation details for sampling are not provided either which makes it hard to reproduce the results. \n\n-Writing can be further improved in the following perspectives. A discussion on the difference between the common sense knowledge graph and the standard knowledge like WordNet adopted by previous ZSL works should be included. The justification for the bad results on the BBN dataset is missing. There are not enough implementation details in order to reproduce the results e.g., what are the text features used for the intent classification? How many nodes are selected in the graph sampling step?  \n\nMinor questions\n\n-In the last paragraph of Section 4.1, I do not get why other GCN approaches are referred as general-purpose methods? \n\nJustification of the rating\n\n-Overall, I think this paper has significant technical contributions. However, due to the issues I have pointed out above, my initial score is only 6 (marginally above the acceptance threshold.). If the authors can address my concerns, in particular, providing better results on the ImageNet dataset, I will be very happy to upgrade my score.  \n\n----------------------------------------------------\nPost-rebuttal\n\nThanks for the new ZSL and GZSL results on ImageNet. The results are convincing to me. My other concerns are properly addressed too. I read the concerns from R4 and R5. To my knowledge, using common sense knowledge graph and the GCN with self-attention are novel in the zero-shot learning literature. I decide to increase my score to an accept.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}