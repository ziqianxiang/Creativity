{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review - Dynamic Probabilistic Pruning",
            "review": "This paper proposed dynamic probability pruning (DPP), which incorporates Gumbel top-K sampling and mutual-information-based metric to dynamically select a subset of connections between input and output channels. It prunes model from scratch for high sparsity. The method proposed in this paper uses Gumbel top-K for forward pass for structural sparsity, and adopts Maddison et al. 2017’s softmax-temperature approximation of conventional “hard” max to allow gradients to flow through. The empirical results demonstrate the capability of the method, but it would benefit significantly by clarifying the issues below and additional experiments. \n\n### Advantages\n1. The idea of incorporating techniques from compressed sensing and neural network pruning is original and orthogonal with other dimension-reduction methods in CNN compression.\n2. The mutual information (MI) metric is interesting, especially on the capabilities of indicating the diversity and confidence of neurons in a layer. The history of entropy and MI is also considered in this paper. \n\n### Disadvantages\n1. This paper is not novel and it seems to be a straightforward adoption of (Huijben et al., 2020a) onto structural pruning. There are many existing publications that uses Gumbel-based stochastic data-paths in neural networks [12-14].\n2. The evaluation of the DPP is quite weak. Although it achieves high compression rate with low accuracy degradation on small datasets, there are insufficient results on larger datasets (e.g. ImageNet). The authors emphasized the algorithm is hardware-oriented, but provided no theoretical FLOPs and real performance results on hardware platforms.\n3. Some algorithmic details are not clearly explained. For example,\n\t* “During inference, one mask M(i) per layer will be drawn from the trained log-probabilities that is then used to prune the model.” What is the strategy/criterion for extracting the mask? Is an additional fine-tuning stage needed to recover the performance of the pruned model?\n\t* “Data augmentation is used for this experiment.” It is better to elaborate on which augmentation methods are used in the experiments for a fair comparison. \n\t* “Across these experiments, we set a K value per layer, which determines the exact number of active inputs assigned to each output neuron.” It is not clear how K was selected in each experiment.\n\n### Other issues\nSome claims are misleading to some extent, and they should be carefully revised. For example:\n1. In the abstract, “Importantly, most of the pruning methods do not provide a structural sparsity, resulting in inefficient memory allocation and access for hardware implementations.” In fact, structured pruning has been well-explored [6-11] in recent years, also motivated by hardware friendliness considerations.\n2. “All weights are initialized based on He normal initialization”\nIt would be better to insert the reference. \n3. “The second disadvantage of current pruning methodologies is the lack of integration of existing pruning algorithms with other compression techniques such as quantization and its extreme binarization.” Itʼs worth mentioning the existing works that already integrated pruning and low-bit quantization [1-5].\n4. Section 2.1, the reference to SNIP (“Lee et al.”) does not have a year and conference name.\n\n### Summary\nIt would be a significant improvement if the authors could extend the evaluation of the proposed method and address the above issues. It would also justify the real contribution of this work if the authors could present the motivation for incorporating parameterized generative sampling and substantial benefits to pruning. The authors could further provide a detailed algorithm to illustrate how the additional parameters are updated in the pruning pipeline. \n\n### References\n* [1]: Wang, Tianzhe, et al. \"APQ: Joint Search for Network Architecture, Pruning and Quantization Policy.\" CVPR 2020.\n* [2]: Yang, Haichuan, et al. \"Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach.\" CVPR 2020.\n* [3]: Tung, Frederick, and Greg Mori. \"Clip-Q: Deep network compression learning by in-parallel pruning-quantization.\" CVPR 2018.\n* [4]: Yang, Li, Zhezhi He, and Deliang Fan. \"Harmonious Coexistence of Structured Weight Pruning and Ternarization for Deep Neural Networks.\" AAAI 2020.\n* [5]: Yiren Zhao, et al. “Focused Quantization for Sparse CNNs.” NeurIPS 2019.\n* [6]: Yihui He, et al. “Channel pruning for accelerating very deep neural networks.” ICCV 2017.\n* [7]: Zhuang Liu, et al. “Learning Efficient Convolutional Neural Networks through Network Slimming.” ICCV 2017.\n* [8]: Hao Li, et al. “Pruning Filters For Efficient ConvNets.” ICLR 2017.\n* [9]: Xitong Gao, et al. “Dynamic Channel Pruning: Feature Boosting and Suppression.” ICLR 2019.\n* [10]: Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" NIPS 2016.\n* [11]: Luo, Jian-Hao, Jianxin Wu, and Weiyao Lin. \"Thinet: A filter level pruning method for deep neural network compression.\" ICCV 2017.\n* [12]: Wang, Yulong, et al. \"Dynamic Network Pruning with Interpretable Layerwise Channel Selection.\" AAAI 2020.\n* [13]: Kang, Minsoo, and Bohyung Han. \"Operation-Aware Soft Channel Pruning using Differentiable Masks.\" arXiv preprint arXiv:2007.03938 (2020).\n* [14]: Dong, Xuanyi, and Yi Yang. \"Network pruning via transformable architecture search.\" NeurIPS 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More comparison needed to establish the gains",
            "review": "The paper proposes a method to prune the neural networks using a trainable masking layer technique. This technique can produce un-structured as well as structured model after pruning depending on the generated mask. The results are demonstrated on CIFAR and MNIST datasets for both fully connected networks as well as for CNNs.\n\nHere are my concerns:\nThe over-all scheme of trainable mask layers and its benefits have been established in (Liu et al., 2020) and earlier works. The paper has tried to learn the parameters of the masked layer in a different way using Gumble top-K sampling (from Huijben et al. (2020a;b)) but the results does seems to improve over when compared to Liu et al., 2020 (Table 1 & 2).\n\nThe paper mentions that having same number of weights retained for each output activation also provides a level of structured pruning. It’s not clear how this structured (which is in fact un-structured) pruning can be used by the hardware during inference for speedup.\n\nThe paper presents results only on small size datasets like MNIST and CIFAR. The model needs to be tested on bigger dataset like ImageNet to establish it’s efficacy.\n\nThe experiment section looks weak as the paper doesn’t compare their method with other state-of-the-art algorithms. For eg. in Table 2, the results for (Liu et al., 2020) looks better than DPP for VGG on Cifar-10 but the paper doesn’t include this method in rest of the comparisons. Also, for convolution layers the DPP is essentially performing filter pruning and therefore it can be compared with other filter pruning methods like “Pruning Convolutional Neural Networks for Resource Efficient Inference (https://arxiv.org/abs/1611.06440)“ and “Pruning Filters for Efficient ConvNets (https://arxiv.org/abs/1608.08710)“\n\nTypos:\nPage 1 -> to prevent instead of toprevent\nPage 4 -> convert instead of covert\nPage 8 -> facilitate instead of faciliate ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok submission. More discussion needed for main contributions. ",
            "review": "### Overall\nIn general, this paper presents a idea which utilize the gumbel top-K sampling method to build a dynamic mask for each layer in the neural networks for the pruning purpose. Experiment results show the good performance with DPP. \n\n### Pros\nClear presentation of main idea. Nice illustration to show how the idea works for a neural network piece-wisely.\n\n### Cons\nBasically, this paper tries to solve the neural network pruning problem with DPP tech, but the materials are not well organized to support the main idea. With the main constributions listed in section 1, readers may still get confused after following the whole work. \n1) It is a good point to support both full-connected and convolutional layers with the same pruning tech. The first contribution metions \"a hardware-oriented sparse structure\". How does the DPP address the idea of \"hardware-oriented\" in practice? It is not enough with showing some case on fully-connected net or CNN to support the method can be applied for a specially designed hardware architecture. Especially for the experiments, practical study on a memory-limit or computation-limit system with DPP could be more convincible for users. And a proper guidence on how DPP works for these real systems can also be helpful for users to generalize the idea on their specific resource-limit systems. On the other hand, since the CNN could be converted to the simliar expression as fully-connected network, this work is supposed to show how DPP outperforms those existing pruning methods on both fully-connected net and CNNs.\n2) For the second main contribution, the paper tries to adopt \"layer-wise sparsity\" in the method. However, more hyper-parameters mean more workloads on tuning a network to fit a specific hardware. How could DPP overcome this issue?\n3) This work emphasizes \"our framework allows re-wiring during training\". How does DPP perform comparing to other step-wise pruning method which could handle the re-wiring during training as well?\n4) For the last point, this work try to propose \"a new information-theoretic metrics\" to \"capture the confidence and diversity of the pruning masks among neurons\". As this idea is introduced by the \"probabilistic nature of DPP\", could this measurement work for other pruning methods? Is it possible to compare different methods under the same measurement of the proposed metrics? It would be good to append the diversity results for each table in the comparison.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A mask learning technique (variant) with weak experimentation and lack of clarity.",
            "review": "Decision:\nThe review might seem harsh to the authors, but I sincerely hope you take it in good spirit and improve the paper. Mask learning has a lot of potential and stable mask training with exploration could be of interest. The metrics proposed for the confidence and diversity of masks could benefit from more investigation. \n\nI think the paper is not ready for publication from both writing and technical perspectives. The lack of clarity and weak experimentation push me to reject this paper. \n\nSummary: \n\nThis paper proposes a variant in mask learning for sparse neural networks. For any weight tensor W, M is a mask that decouples sparse structure learning from the weights themselves. So the effective weight for forward propagation is W.*M (element-wise product with appropriate broadcast in case of structured pruning). M typically has only {0, 1} (is binary) and adheres to sparsity constraints for the prediction. Until recently, the masks were generated based on magnitudes of the weights (top K) (Han et al., NeurIPS 2015, Zhu & Gupta 2017) or other important factors like gradients (Evci et al., ICML 2020), momentum (Dettmers & Zettlemoyer 2019), or through trainable sparsity (Liu et al., ICLR 2020, Kusupati et al., ICML 2020) based on magnitude. \n\nHowever, there have been works that successfully decoupled mask learning (M) from the explicit importance of the weights W. Zhou et al., NeurIPS 2019, and Ramanujan et al., CVPR 2020 showcase the learning of masks or supermasks without dependence on the weight magnitude itself. However, each of these masks was a soft variant of the binary mask and the relaxation was used to do a magnitude-based pruning for M to learn it smoothly while ending up with a binary mask. Above all Savarese et al., NeurIPS 2020 (has been public since December 2019 -  https://arxiv.org/abs/1912.04427 (please check both the current version and v2)_ made mask learning continuous and jointly trained them with the weights of the network. M was independent of magnitudes or other importance factors of W as well.\n\nThis paper proposes Dynamic Probabilistic Pruning (DPP). a variant of mask learning where M (if it is DxC dimensional) is parameterized by learnable unnormalized log-probabilities ($\\phi$) of D (≥ 1) independent multinomial distributions with each C classes. This ($\\phi$) is trainable and is perturbed using a scaled ($\\alpha$) Gumbel(0, 1) noise distribution before taking a top K out of the C based on the magnitude. Because the underlying weights are not zeroed out, the noise perturbation and learning could activate some dead weight during the course of the training allowing the sparse structure to change over time. This can also be extended to structured sparsity with appropriate reformulation and can also be applied to Binarized Neural Networks where magnitude pruning can't be used (as claimed by this paper). The paper also provides some information-theoretic metrics for the confidence and distribution of pruning masks. \n\nThe paper evaluates DPP on a small scale and midscale pruning tasks on MNIST (LeNets, Binarized LeNets), CIFAR-10, and CIFAR-100 (VGG-16 and MobileNetV1). The paper claims to effectively induce structured or unstructured sparsity (with focus on hardware) due to non-reliance on magnitudes of the weights while also resulting in ultra-compressed models. The paper also claims to be the first to combine\n\nPros:\n1. The idea of DPP which is based on DPS is simple and grounded in theory. It is an intuitive choice to use the components used in DPP to make discontinuous problems differentiable. This works well in practice. \n2. The proposed information-theoretic metrics and the supporting experiments do provide promising direction to explore the theoretical aspects of the pruning masks. \n\nIssues:\nI will go sequentially in the paper and will include non-technical issues as well. \n1. The paper is not well written and takes significant time to read and understand due to a lack of clarity in conveying technical details and the insights. The paper also will benefit from a thorough vocabulary check and grammar check.\n2. Abstract: I assumed binarized neural networks e.g., binary settings. It was not clear until the end of the introduction what it meant.\n3. VGG-16 has 138 M parameters.\n4. Memory typically means working memory or RAM. I think you want to say model size as unstructured pruning rarely reduces working memory in CNNs.\n5. The claims made in contributions are overreaching and I will explain why in the following issues. \n6. While the related work looks adequate the authors are encouraged to read the related work sections of Gale et al., 2019 and Kusupati et al., 2020 for a better picture. \n7.  There are also techniques for filter pruning that use proxy importance factors based on batch norm etc - Liu et al., ICCV 2017. While they are still magnitude-based, they do not depend on the filters themselves so fall into the category of DPP.\n8. I do not understand the claim of the paper saying that \"On the other hand, the structured pruning approach has not been transferred to fully-connected layers, which for certain architectures consume the largest percentage of memory\". If there is a fully connected layer of size DxL. I don't see a problem in pruning few rows making it KxL or vice-versa. Fully connected layers also benefit highly from low-rank parameterizations if needed. So structured sparsity in fully connected layers is not as bad as portrayed. \n9. I would also point the authors to Elsen et al., CVPR 2020, and Gale et al., SC 2020 where they argue the speedups for unstructured sparse kernels. However, I completely agree with the paper that structured sparsity is much more superior for hardware utilization.\n10. Section 3.2 would benefit from re-writing as it was very hard for me to parse through the first couple of times. Maybe a pseudo-code for the algorithm would be of great help.\n11. While the paper claims independence from magnitude-based pruning (Albeit for weights), DPP still uses magnitude-based pruning for the mask top K. \n12. I would recommend the authors to look at Straight-Through-Estimators (STE) (Bengio et al., 2013 - https://arxiv.org/abs/1308.3432) and its use for pruning in (Wortsman et al., NeurIPS 2020). STE helps you pass through gradients even through the dead weights making the optimization more stable and circumvents the relaxation troubles. \n13. Use of distributions to sample masks or perturb the masks is not completely novel and has been used in both Zhou et al., NeurIPS 2019, and Savarese et al., NeurIPS 2020 (please check Alg 2 in https://arxiv.org/pdf/1912.04427v2.pdf). They use Bernoulli distribution with the learnt mean rather than the scaled Gumbel noise.\n14. Most mask learning methods can be extended to the 3.3 (a) (b) and (c) effectively without much trouble. This makes the novelty of the paper limited. \n15. I do not understand anything in the last paragraph of section 3.3. The claims seem to imply that the same sort of sparse structure induced by different mechanism behaves differently - I am not sure why that is the case.\n16. The experimental setup is very weak due to the choice of the dataset, models, and baselines.\nDatasets: While MNIST, CIFAR-10 and CIFAR-100 might be good for checking correctness the generalizability of the results to other settings depends on the models being tested the level of overparameterization. Even in that case, I suggest authors not to claim results on MNIST due to the ease of the task as even shown by the experiments in this paper.\nModels: VGG-16 has 138 M parameters and is highly over parameterized compared to CIFAR-10 and CIFAR-100. The sparsity levels for this model can be taken up to 99.9% with minimal loss in accuracy for these two datasets (you can see this in multiple recent papers including DST). Similarly, MobileNetV1 has 4M params which is still in the same regime. The results on these models x datasets do not generalize to other settings like ResNets with ImageNet etc., One good architecture for CIFAR-10/100 pruning is ResNet20 proposed in Frankle & Carbin ICLR 2019. 10% is non-zeros is a lot of VGG-16 and 40% is a lot for MobileNetV1 to be very much overparameterized and not have any effect.\nI strongly encourage authors to reconsider the choices.\nBaselines: The paper claims to induce both structured or unstructured as required. But the baselines are very limited with DST (Liu et al., ICLR 2020) MNIST and VGG-16 + CIFAR-10. The paper also does not compare with any structured sparsity approach in CIFAR experiments making the validity of the claims less.\n17. The sparsity ranges seem to be chosen randomly for the experiments and the classifier is left dense in these experiments while the baselines have them pruned.\n18. I do not understand the rationale behind inducing structured sparsity into convs and unstructured sparsity into FCs in CIFAR experiments.\n\nI suggest an overhaul of the entire experiment section with a better and sound methodology.\n\n19. Lastly, the paper claims to be the first to do pruning + binarization. I would suggest the authors search for \"pruning binarized neural networks\" which might give more solid baselines to compare with. Also, the experiments are done on MNIST with LeNets and need to be scaled up to have some amount of generalizability.\n20. Adding to the point of pruning for binarization. Even for BNNs, there are intermediate states where the weights are non-binary. One can always use those smooth values as with any relaxation for discontinuity to do magnitude-based pruning effectively. Binarization doesn't effectively stop pruning as we can still do mask learning over top of it if not magnitude pruning. \n21. The claims in the discussion are overreaching and readers would appreciate a toned-down version. \"concussion\" -> \"conclusion\".\n22. Lastly, there is an increasing consensus that overall sparsity level per layer might be the only thing that matters rather than particular weights. So a good strong baseline would be random masks induced gradually overtime while increasing sparsity and training weights simultaneously and maybe with STE. \n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}