{
    "Decision": "",
    "Reviews": [
        {
            "title": "Low novelty and impact. Specific instance of more general problem. Paper needs significant proof reading and lit review.",
            "review": "**Summary**\nThis paper introduces a method for NAS of GNN architectures under hardware constraints. They describe results on a few datasets.\n\n**Strengths**\nThe authors point out that GNNs architectures obtained from NAS can be inefficient when deployed on hardwares and that there is an opportunity to jointly optimize the architecture and the hardware together.\n\n**Weaknesses**\n\n*Very poorly written*\n\na) Figures and legends not explained properly.\n\nb) Proper literature survey for multi-objective NAS not done. e.g. see https://arxiv.org/abs/1804.09081 which links to several other papers.\n\nc) Terminology not introduced. \n\ni) What is LUT, FF and DSP?\n\nii) Sec2P1L4 should be h* \\in \\mathcal{H}(a*) \n\nd) Riddled with typos and grammar mistakes\n\n“...Using *reinforcemnt* learning…”\n\n“...We test the *seaching* efficiency of our method…”\n\n“...the joint search *ahieves* the best accuracy ..”\n\n“...and *shortes* searching time…”\n\n“...The best *accuacry* result on *differnt* datasets.”..\n\n“..distribution of decent *sampels* ..”\n\n“...adds up to a *coniderable* level….”\n\n“..more *sampels( are …”\n\n“...that is *perferred* to the controller…”\n\n\n*Low novelty*\na) The problem formulation is not that interesting. It boils down to multi-objective NAS with an expanded hardware search space.\n\nb) Their NAS algorithm is not novel. Mostly uses the original NAS from Zoph et. al.\n\nc) Hardware constraints are implemented with the rejection sampling. Samples with hardware realizations outside spec are awarded score zero. It would be much more interesting to solve the joint problem or find the best hardware configuration for a given architecture.\n\n**Low impact and weak experiments**\na) Focus on GNNs and FPGAs seems unnecessary. The problem is more generic in reality i.e. multi-objective NAS. The GNNs and FPGAs are only a specific application.\n\nb) Experimental results are weak. A small search space defined in Table 2. Few baselines - random and phased search.\n\nc) Figure 3 should also plot accuracy.\n\nd) The authors don’t report SOTA accuracy results. What’s the delta from SOTA?\n\ne) The results are not tested on real hardware and the hardware search space seems arbitrary. Do the results generalize to arbitrary hardware spaces e.g. those with very non-linear objectives?\n\n**What could make the paper better**\n\na) Paper writing needs to dramatically improve. The manuscript is very premature and needs a lot of proofreading and polishing. \n\nb) Proper literature review is also needed.\n\nc) The core method lacks novelty. It’s simply an instance of multi-objective NAS for which there are many methods. \n\nd) Experiments are lacking.\n\ni) Need more than 2 baselines. Random and phased search is not sufficient.\n\nii) Need more generic hardware architectures and complicated search spaces.\n\niii) Need to test realizations on real hardware to show real world impact. Otherwise optimization remains theoretical.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited Novelty",
            "review": "The paper provides a method for hardware-aware GNN architectural search using reinforcement learning, showing some improvements over random search and disjoint architecture/hardware optimization. \n- Pros: The paper is well-structured and easy to follow and the methods are fully conveyed to the reader. The general topic of the paper is of practical importance, however, the proposed methodologies are not entirely novel and rather borrowed from prior works in NAS for CNN/GNN design (mode details below).\n- Cons: \n1. Perhaps my biggest concern about the paper is the limited novelty. The GNN architecture search components (i.e., search-space) as well as the reinforcement learning method used are previously studied in the literature (e.g., Gao et al. 2019 in the paper). The FPGA implementation of the GNN is also based on a previously published work (Zhang et al. 2015 in the paper). The utilized FPGA cost model and optimization scheme has also been studied in the context of NAS for CNNs with a very similar (even slightly more complex) setup [1]. The effort to transition from a CNN to a GNN is not enough to justify the methods in the paper as a standalone contribution.\n2. The hardware cost in its current shape merely discards invalid configurations but does not perform any ranking on the \"valid\" configurations. As such, the term \"joint optimization\" of hardware and GNN is misleading since the hardware aspect only performs a sanity check on the design constraints but does not differentiate between configs that comply with the said constraints. Different hardware configs certainly have different characteristics in terms of latency, memory, power, area, etc. A better formulation thus would have been to model the hardware cost such that it differentiates between valid configs in terms of the above characteristics. \n\n[1] Jiang, Weiwen, et al. \"Accuracy vs. efficiency: Achieving both through fpga-implementation aware neural architecture search.\" Proceedings of the 56th Annual Design Automation Conference 2019. 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper targets an important topic, but what I am concerning is its novelty.",
            "review": "While previous GNN NAS work is purely at the software level, this paper claims to be the first that takes hardware constraints into consideration in the NAS process. Such a software-hardware co-design often leads to a better-searched model (i.e., with higher accuracy). Overall, I think this paper is targeting an important problem, but I do not see enough technical contributions that make it a topic conference paper.\n\n1. Why using FPGAs as your hardware backends? Can your framework be applied to other backends like CPUs and GPUs? Would GPUs and CPUs be better choices considering that regular uses would have direct access to them? \n\n2. Compare to previous work on hardware-aware for CNNS, what are the new challenges and opportunities incurred by GNNs? Could the previous infrastructure be applied to GNN NAS? What kind of extensions needs to be made? Key technical contributions need to be better articulated.\n\n3. The hardware-software co-design parts are not clear. In particular, I do not get the synergy between the software and hardware design. The hardware designs are more pretty much ad-hoc. Could you elaborate on the insights behind your co-designs? For instance, when/why would you get better performance compared to the separate search as shown in Table3? A more detailed explanation is needed.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper shows an interesting NAS method, but results are inadequate ",
            "review": "This paper presents a hardware-algorithm codesign NAS framework for graph neural networks. The NAS algorithm considers hardware constraints and is able to explore an architecture-accelerator pair for the targeting task. The paper is nicely presented and the method is novel, however, the presented results are limited to small datasets and makes it hard to evaluate the scalability and effectiveness of the proposed approach.\n\nStrength:\n\n1. The paper is written nicely and easy to follow.\n2. The idea of building a hardware-algorithm codesign NAS for GNNs is novel.\n3. Since GNNs are pervasive in solving graph related tasks, designing a systematical approach to accelerate them on reconfigurable devices is surely an important research direction.\n\nWeakness\n1. My main concern about this work is its evaluated datasets. The considered citation datasets are too small. It is not persuading to show hardware performance on small datasets like this and this also does not show scalability of your proposed approach. \nThe authors could consider larger datasets presented in the GraphSaint paper [1].\n2. What is the major GNN application you are focusing on? If you are interested in doing fast-inference, which seems like to be the case because you are not exploiting batch-wise parallelism, what is the application? Do you intend to use GNNs on mobile systems? Or you want to provide faster inference for cloud applications? Different use scenarios indicate very different FPGA platforms and might heavily impact the results.\n3. I have concerns regarding the scalability of the approach when the dataset is larger. The proposed RL-based NAS iteratively builds FPGA implementations using HLS. It is a known issue that hardware synthesis with HLS is very time-consuming, if the searched GNN becomes more complex, the hardware mapping might take a large amount of time and the NAS algorithm might take a very long time to converge.\n\nMy suggestions & confusions:\n\n1. Do you do mapping to actual FPGA devices? What is the time cost? What if your synthesis results do not agree with your hardware fitter? Is there a case when synthesis passes but hardware mapping fails so that the design cannot be actually mapped or have to be mapped with slower clock rates? This is possible due to things like large fanouts, routing congestions and etc..\n2. Why the accuracy of the FGNAS is not matching state of the art? I realised you mentioned that you have hardware constraints, but theoretically speaking, if your hardware design is flexible enough and you give a large enough hardware budget for FGNAS, the results in Table 4 should match GraphNAS [2], AutoGNN [3] and PDNAS [4]. The performance gap between FGNAS and other Graph NAS on Cora is huge (>10%). Is this performance gap simply from having the hardware constraints? Or is it because of the more complex dual-optimisation in the hardware space? Or is it because of the lossy quantisation? I would recommend you to assign infinite hardware resources and perform FGNAS to see whether it is actually on par with other graph NAS methods. If we have an accuracy vs hardware resources vs latency curve, you are not exploring the whole range of possible accuracies?\n3. When you say ‘it is necessary to divide the vector-wise operation into sub-tasks’, is this simply loop-tiling or am I misunderstanding? Effectively I assume you are picking tile sizes here, please correct me if I am wrong.\n4. It might be worth to show the quantisation search space in main text.\n5. Some minor writing mistakes on Page 3: ‘gnn model’, ‘nei’hbouring’\n\nIn general, I like the idea of this paper and am happy to change my scores if the authors can address my major concerts in a) larger datasets, b) explain further the motivation with respect to targeting applications and c) show FGNAS has the same performance to state of the art NAS when given a large hardware budget. \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}