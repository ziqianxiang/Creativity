{
    "Decision": "",
    "Reviews": [
        {
            "title": "Potentially interesting result on the utility of failure trajectories, but needs clarification",
            "review": "This paper describes an algorithm that stores an agent's raw experiences within a task then feeds them into a neural architecture to aid action selection when that task reoccurs. In this work the tasks are meta-RL-style tasks wherein the agent must learn to explore a set of options to discover which one is rewarding. The results show that using experience from past occurrences of a task is beneficial, and that recording failures in particular provides a benefit over recording successes.\n\nI have some major concerns about this paper\n\n1.) The paper claims SOTA, but in fact doesn't present results on any previously used benchmark task. The results are for a new task setting the authors devised, wherein tasks identifiably reoccur. This was not the case as far as I know in the work to which they compare (esp. SNAIL), and constitutes a major difference from the setting for which SNAIL was designed. In my view it's inappropriate to apply phrases like \"outperforms the existing state-of-the-art\", when there effectively is no previous SOTA for this setting. Not only is it overclaiming, it's also very confusing. It made it hard to figure out what the paper is really about.\n\n2.) It's not clear exactly what this new task setting is. How often do tasks reoccur? How many tasks are there in total? Is there a specific schedule of reoccurrence? Is the agent tested on held-out tasks (i.e. new colors, or at least the rewarding color is a color that wasn't rewarding in the training set)? It's very difficult to evaluate the results without this information.\n\n3.) If I understand the task setup correctly, the authors have shown two things: (1) that providing more data on a particular task leads to better performance and (2) that specifically using data from failed trajectories is especially helpful. (1) is not very novel, but (2) is surprising and intriguing. My concern here is that I don't follow the author's explanation for why this happens. Perhaps I can boil down their explanation to this quotation: \"Because the agent has no memory of mistakes across trials, it may again make mistake A when it re-encounters the task.\". In the tasks in this paper that's true, but if the agent (again, in the particular tasks in your paper) has a single example in memory of a successful episode, it should be able to solve the current task. In other words - why can't the agent learn to go to the correct block every time when it has an example of receiving reward after going to that block in its memory bank?\n\nIn addition to explaining what the setup is, I'd be able to more easily evaluate the paper if the authors motivated why they chose this setup. Does this setup of identifiable reoccurrence have some bearing on the real-world or some class of applied tasks? Or is it of interest because it reveals a surprising property of failure trajectories? If it's the former, it would help to explain this early on in the paper. If it's the latter, point 3 above needs to be answered convincingly. \n\nIf the authors can clarify these points in compelling way, I'm happy to increase my rating.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary\n\nThis paper proposes using memory to assist meta-learning of multiple tasks. More specifically, the memory is used to store trajectories from failed episodes. Intuitively, this strategy encourages the model to learn from past mistakes, which provides a helpful training signal. The method is tested on a range of procedurally generated tasks against the state-of-the-art SNAIL algorithm [1].\n\nPros\n1. While memory has been demonstrated to be useful in many areas, this work represents an interesting exploration of memory for multi-task learning.\n2. The proposed method utilises a number of recent innovations, such as transformers, and demonstrated that the attention module is useful for hierarchical memory processing.\n\nCons\n1. Memory in the form of replay buffer has been widely used in reinforcement learning (e.g., [2, 3]), which addresses the problem of learning from highly correlated trajectories that motivate this work as well. However, it is unclear how is the proposed method conceptually different from those earlier work.\n2. While the idea is interesting and intuitive, it is unclear how to determine whether the agent fails when the rewards are dense. This limits the application of the method. [3] may provide potential generalisation from TD-errors.\n3. The method may not be scalable, as it requires one memory bank for each task, so the memory size grows linearly with the number of tasks.\n4. The empirical section lacks comparison with numbers independently reported from other papers. [3] already provides a number of experiments that can be benchmarked against. Direct comparison with them would make the result more convincing.\n\nOther comments\n\nRegarding the comment on RL^2 and SNAIL (... are special cases of our formulation …). This may not be true, as the hidden state (h) in those models can be any unobserved factors, including the memory bank.\n\n[1] A Simple Neural Attentive Meta-Learner. Mishra et al. 2017\n[2] Human Level Control Through Deep Reinforcement Learning, Mnih et al. 2015\n[3] Prioritised Experience Replay, Schaul et al. 2016",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a technique for learning meta-RL agent by augmenting it with an external memory bank of past experiences --- including failures --- as well as its own experiences within a trial/task. When the agent faces the same task shown previously, it extracts information from a memory through two levels of attention mechanism; the memory embedding and the current episodes’ features are combined to derive a memory-conditioned policy.\n\nStrength:\n------------\n\n- This paper studies, to the best of my knowledge, a novel idea of using failure experiences for improving the performance of meta-RL.\n- The proposed approach makes a significant improvement over the SNAIL baseline on simple Gridworld and more complex continuous control environments. Ablation studies are also carefully designed.\n\nWeaknesses and/or detailed comments:\n--------------------------------------------------\n\nFirst, I do not think it is very clearly motivated why failure episodes need to be stored, and how those failure trajectories are utilized to improve the meta learner’s performance. The relevant explanation in Section 2 --- providing the agent with a set of mistakes to prevent from making and oscillating between the same mistakes --- but it is less straightforward why the agent should be able to avoid making the same mistake. To me, how the agent actually can “learn from failure” sounds like a black box. I failed to find a specific part of the algorithm where such data (m_o^i) is treated as a “failure” experience, or something like a contrastive learning --- is it just through smaller rewards of such trajectories?\n\n(Regarding section 3.4) What would happen if we only store “success” (a similar idea to [Oh et al., 2017; Self-imitation learning])? Would the agent still be able to benefit from that? If not, what would be a logical explanation that would differentiate the proposed method from this variant?\n\nLimitations of the approach:\n(1) This method is a direct modification to SNAIL with an explicit, external memory structure. The approach sounds pretty specific to SNAIL rather than being a generic approach for meta reinforcement learning. One possibility would be directly using this data for updating the policy/value function within a meta-RL context, rather than feeding their embedding. Another limitation is the existence of such a binary function that tells if an episode was successful; in some tasks (e.g. sparse reward) it is feasible, but when the task involves multiple phases how to define a failure would be not clear.\n(2) A memory bank is associated per task, which can be of very much memory usage if the number of tasks are large. For tasks represented by continuous representations, this method is not applicable due to this assumption.\n\nPresentation:\n- (Section 2.3) The paper should be self-contained: if the memory conditioned policy is based on SNAIL, a description of how it is working should be explained to an enough extent.\n- Training details and network architecture details are not presented.\n- Empirically, the presented environments (Gridworld/Miniworld) look good, but this paper did not experiment on more standard environments such as standard control suites or ProcGen, etc., as in recent meta-RL works (SNAIL, MAML, PEARL), etc.\n\n\nOverall evaluation\n------------------------\n\nOverall, this paper presents an interesting idea of providing failure experience to improve the efficiency of meta-RL agents. The method is quite simple, though There are promising results, but the method does not seem very convincingly explained or presented. Hence, my initial recommendation is towards rejection but I would like the authors to provide more convincing evidence on why and how failure experience can be exploited to stabilize and improve the learning.\n\n\nAdditional comments and feedback:\n--------------------------------------------------\n\n- Additionally, it would be interesting to qualitatively visualize which failure experience the agent put more attention to. Since the number of failure data can be large, what is the data that got the most attention and represents the memory-conditional part (in section 2.3)?\n- The paper could also benefit from citing and discussing in comparison to more related works, including: Oh et al. Self-Imitation Learning, ICML 2017 (stores successful trajectories)\n- Figure 1 could be improved. I do not think this convey any core concept/idea other than the architecture diagram itself. Also, it would be even better to put such a policy architecture diagram near the method section (2.1-2.3).\n- Sec 2: Please consider using more standard notations for state-action-reward sequence: (o0) -> (a0) -> (s1, r1) -> (a1) -> (s2, r2) (a2).\n- Experiments: Plots and their labels are too small for readers.\n- The method name (Memory) referred in the experiment section could be improved with a better name.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, needs further work.",
            "review": "The paper provides an interesting direction in the meta-learning field. In particular, it proposes to enhance meta learning by augmenting agents with external memory banks that specifically store failures from past experiences. The paper claims two main contributions. The first one is a new architecture with two steps of attention and the second is the idea to store failures in memory to learn from past mistake. Results are shown on 3 task: (1) mini world (2) gridworld (3) robot manipulation\n\n##########################################################################\n\n\nReasons for score: \n \nOverall, I vote for rejecting. Although in principle I like the idea of using previous negative experiences to correct mistakes, I think the experiments used to test the idea are too limited to effectively prove that this is general enough. In particular, the exact same reward structure is used across the three tasks, so they are only a tests for the generality of the architecture with respect to the inputs. Moreover, this reward structure is such that negative rewards are very prominent and so this make the storage of negative trajectory particular important for the task. However it is unclear what would happen if there is no negative reward, would the algorithm still produce better results?  \nI think the paper is interesting, but it should provide more experiments and more analysis to be accepted.\n \n##########################################################################\n\nPros: \n \n1. Interesting new approach with potential applications in domain where negative rewards are particularly important to avoid (e.g. robotics)\n\n2. New architecture which use self-attention to summarise trajectories.\n\nCons:\n\n1 The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. The paper claims that the method is effective in 3 different domains, however all the experiments are basically testing the same problem (same reward structure). So it’s all like one big experiments. I think testing the architecture with different reward structures would be critical. Also the fact that with this reward structure f_(\\tau) is always clearly defined critically undermines the effectiveness of the experiments.\n\n2. The method heavily depends on being able to define f_(\\tau). What if no negative reward is present? How would you extract negative trajectories?\n\n3. Considering the limited results, a deeper analysis of the proposed method would have been nice. In this respect no real ablation of the architecture has been performed. \n\n4. The paper is poorly written. There are no details about many of the components of the architecture (e.g. how many layers in the MLPs?), there are limited details about dimensionality or learning rates. This would make almost impossible to replicate these results, at least at the current stage. The website claim that the code will be released, but so far it has not.\n\n5. There are no analysis on how attention is used, this would have been important to see given that the method is heavily centred around this mechanism.\n\n\nMinor comments: \n\n(1) pag. 6: “Our method does not require precise knowledge of how many steps of failed episode should be stored in memory” -> this might be true for fairly short trajectories (like 80 steps as in the paper), but what about very long trajectory, let’s say in the order of 1000s?\n\n(2) par 6. The last sentence in 3.1.4 seems like a misplaced sentence as it would make more sense to claim that after section 3.4 or at least link it here. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}