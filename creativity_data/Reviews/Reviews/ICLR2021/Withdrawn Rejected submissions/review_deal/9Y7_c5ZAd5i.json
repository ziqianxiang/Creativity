{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.\n\n"
    },
    "Reviews": [
        {
            "title": "Review of Paper 374",
            "review": "This paper studies learning in stochastic games, which are extensions of Markov decision processes (MDPs) from the single-agent setup to the multi-agent one. Here the objective of each learner is to optimize her own reward function. Similarly to the case of MDPs, here one can devise learning algorithms with controlled sample complexity or regret (or both simultaneously) even when reward and transition functions are unknown. \n\nThe main contribution of the paper is a model-based algorithm called Nash-VI, enjoying simultaneously a regret bound of $\\widetilde O(\\sqrt{H^3SABT})$ after $T$ steps and a PAC-type sample complexity of $\\widetilde {\\mathcal O}(H^3SAB/\\epsilon^2)$ for finding an $\\epsilon$-approximate optimal policy. \n\nThe paper also presents an extension of Nash-VI to the case of reward-free exploration, which is called the VI-Zero algorithm. The sample complexity of VI-Zero depends logarithmically on the number of candidate reward functions, but has a worse dependence on $S$ than that of Nash-VI. \n\nMain Comments:\n\nThe paper is very well-organized and well-written. There are a number of minor easy-to-fix typos that are reported below. The paper overall delivers a clear presentation of algorithms and results, and expect for a few unclear sentences (listed below), it is very a nice read. \n\nOn the technical side, one strong aspect of Nash-VI is that its sample complexity (almost) matches the lower bound except for dependencies on the action set cardinalities. Achieving a near-optimal regret bound is another strong aspect. Finally, the main algorithm outputs a single Markov policy, and not a history-dependent policy. \n\nFor the MDP setup, it is already well established that model-based algorithms are minimax-optimal in terms of both regret and sample complexity. The paper makes a good step towards understanding the benefits of model-based algorithms for stochastic games, thus indicating that their sample complexity could almost match the existing minimax lower bound. \n\nIt turns out that the presented algorithms mostly rely on existing tools already developed for the MDP setup, which are by now fairly standard. That said, the paper does not present any fundamentally different technique than existing ones for MDPs. However, I believe suitably combining all these tools is not a trivial task and deriving such sharp sample complexities requires care. This is in particular true for bonus $\\gamma$ discussed in p. 5. Overall the paper conveys interesting messages advancing our understanding of model-based algorithms for stochastic games, and in my opinion is worth accepting.  \n\nI was unable to check the proofs in such a limited review period, but they appear correct to me. \n\nMinor Comments:\n\n- In p. 6 “during the training phase” please clarify. \n\n- In the statement: “would not hurt the overall sample complexity up to a constant factor”: Did you mean “would hurt the overall sample  complexity only up to a constant factor”\n\n- In p. 7, when referring to $\\mathcal M(\\mathbb P, r)$, it is not clear which reward function $r$ is. Does it one belonging to a set of reward functions? Please clarify.\n\nSome typos:\np. 1: with the problem multi-agent -> … the problem of multi-agent\np. 1: Model-free algorithms has -> … have\np. 3: Markov games is -> … are\np. 3: distribution of actions -> … over actions \np. 6: the policies … is -> … are\np. 6: such policy -> such a policy\np. 6: a $\\epsilon$-approximate -> an …\np. 8: since MDPs is a special case of -> since MDPs are special cases of \np. 8: rerunned -> rerun\n\nThanks for updating the paper in light of my earlier comments.\n\nAfter a long discussion with other reviewers and ACs, we concluded that the paper would require another complete review process in view of newly added proofs, which were unfortunately missing in the first round. My lowered score signals this to the ACs.\n\nI would like to highlight that I became very upset to find out that many important details were skipped and left as exercise to readers. While this would make sense for some repetitive details within a paper, it may not apply to non-trivial proof details, such as the generalization from 2 to arbitrary S or similar.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Excellent theoretical paper",
            "review": "Summary\n-------\n\nThe authors introduce new algorithms to solve two-players zero-sum Markov games, as well as two-players Markov game in the reward-free setting. The approach is model-based, based on successive episods of planning and counting for updating the model estimate. It involves solving a matrix game at each iteration, looking for a notion of equilibria that is computable in polynomial time (unlike Nash equilibria) An extension to multi-player games is proposed for both reward and reward free setting (in the appendix).\n\nThe sample complexity of the zero-sum Markov game algorithm (Nash-VI), has a\nsample complexity that is closer to known lower-bounds than existing work, and which is better in the horizon dependency than the best model-free algorithm.\n\nIn this zero-sum setting, the improvement to existing work (VI-UCLB) are the following:\n-  use of an auxiliary bonus that is proportional to the gap of upper-confidence and lower-confidence value function, that allows to use much-smaller standard Hoeffding/Bernstein bonuses\n- use of a relaxed notion of equilibria when looking for the next policy given the Q functions of both player. This relaxed notion is introduced in Xie et al. 2020\n\nThe reward-free setting is simpler, in that it simply use greedy policies for each player, with each player maintaining artifical rewards based on Hoeffding bonuses.\n\nReview\n------\n\nThe paper is very well written and presents some exciting results. It is completely theoretical, but provides two different algorithms for two different settings, in both the two-player and n-player setting. The improvement in existing bounds is significant. I have only slight concerns:\n\n- The algorithmic contribution (Alg. 1) could be seen as incremental, as it changes two elements in known algorithm, one of which (coarse correlated equilibria), having been used for a similar purpose in a previous paper. Similarly, Alg 2. is at the end of the day a rather naive extension of zero-reward exploration in single player MDP. Yet the notion of auxiliary bonus is very original, and the reduction of complexity non-trivial.\n\n- The absence of experiments, even in toy setting, is regrettable. It is\nespecially true as the use of coarse correlated equilibria may be expensive, and\nI would have appreciated seing Alg. 1 implemented. As this is ICLR, extensions\nto function approximations would also be interesting. In particular, comparison\nwith model-free approaches would be welcome, as constants before the sample\ncomplexities may vary.\n\n- Some parts of the text could be further explained: in particular the\nintuitions behind coarse correlated equilibria, which is introduced only mathematically.\n\n- The paper theoretical content is rather heavy, which may make this manuscript more suitable for a journal venue, where it would be more thoroughly reviewed. I must admit that I could not proof read the entire appendix.\n\n I have several questions, as follow:\n\n- I do not understand the note \"Our results directly generalize to randomized reward\nfunctions, since learning the transition is more difficult than learning the reward.\" Could you elaborate on this aspect.\n\n- Is there any reason why we would like to use Hoeffding bonuses instead of Bernstein bonuses in the rewarded case ?\n\n- In the multi-player, rewarded case, it appears that using coarse correlated equilibrium instead of Nash equilibrium yields non-product policies, which is unfortunate. Is there any way that we could obtain product policies solving a relaxed notion of equilibria that would be computable in polynomial time ? Similarly, is there a foreseeable way in which $\\Pi A_i$ could be transformed in a sum ? Lower-bounds in this case are not discussed in this case, is there any ?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Impressive theoretical claims, some editing needed",
            "review": "The authors study reinforcement learning in two-player (and more) Markov games, providing new algorithms and bounds. They reduce the dependence on horizon H and states S from H^4 S^2 to H^3 S, matching information theoretic lower bound for these factors.\n\nOverall, my impression is that the results are very valuable. In a short paper like this, it's difficult to convey (and absorb) the fundamental theoretical insights behind it.\n\nSome additional work should be done to improve the presentation (see below).\n\nDetailed edits:\n\n\"Model-free algorithms has also\" -> \"Model-free algorithms have also\"\n\n\"finds eps-approximate Nash equilibrium\" -> \"finds eps-approximate Nash equilibria\"\n\n\"multi-player general-sum Markov games\": I thought that's what we've been talking about since the beginning. Explain? Does \"multi-player\" mean more than two? Was it clear we were talking about only two up to this point?\n\n\"the runtime is PPAD-complete\" -> \"the algorithm requires solving a PPAD-complete problem\"??\n\n\"We remark that the current policy is random\" -> \"We remark that the current policy is stochastic\". (To me, \"is random\" reads as \"is chosen at random\".)\n\n\"in Azar, et al. Azar et al. (2017) can not\" -> \"of Azar et al. (2017) cannot\".\n\n\"In order to achieve so,\" -> \"To do so,\"\n\n\"needs to scale as large as\" -> \"needs to be as large as\". Also, shouldn't the big O here be a big theta? After all big O means \"is not bigger than\", so \"needs to be as large as not bigger than\" conveys no useful information.\n\n\"a relaxation of Nash equilibrium\" -> \"a relaxation of the Nash equilibrium\".\n\n\"Coarse Correlated Equalibirum (CCE) instead, a\" -> \"Coarse Correlated Equiliibrum (CCE)---instead, a\".\n\n\"Therefore, executing such policy requires the cooperation of two players during the training phase.\": Wouldn't it also require cooperation during any other phase as well? Also, shouldn't this observation be part of the problem definition? You are learning policies only in the setting where the algorithm is controlling both (all?) players. That's different from the earliest work (Rmax), which provided bounds even when the players are independent.\n\n\"using Bernstein bonus instead of Hoeffding bonus\" -> \"using a Bernstein bonus instead of a Hoeffding bonus\".\n\n\"with Bernstein bonus\" -> \"with the Bernstein bonus\"\n\n\"achieves optimal dependency\" -> \"achieves the optimal dependency\"\n\n\"pair of Markov policy\" -> \"pair of Markov policies\"\n\n\"can not\" -> \"cannot\" (throughout).\n\n\"into the reward-free exploration setting\" -> \"for the reward-free exploration setting\"\n\n\"Due to space limit\" -> \"Due to space limits\".\n\n\"comes from the sample complexity only scales\" -> \"comes from the sample complexity only scaling\".\n\n\"rerunned\" -> \"rerun\".\n\n\"any more\" -> \"anymore\"\n\n\"have regret guarantee\" -> \"have a regret guarantee\"\n\n\"Since MDPs is\" -> \"Since MDPs are\"\n\n\"both exploration\" -> \"both the exploration\"\n\n\"allows arbitrary\" -> \"allows an arbitrary\"\n\n\"in Theorem 5 scales\" -> \"in Theorem 5 scaling\"\n\n\"we defer the detailed setups\": Not just the details... you are claiming to provide Algorithms 3 and 4, which only appear there.\n\n\"How to design\" -> \"How can we design\"\n\n\"We would like to leave these as future work.\" -> \"We leave these problems as future work.\"?\n\n\"On the complexity of approximating a nash equilibrium\" -> \"On the complexity of approximating a Nash equilibrium\".",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review of A Sharp Analysis of Model-based Reinforcement Learning with Self-Play",
            "review": "-Summary \nThe authors consider self-play in tabular zero-sum episodic Markov game. In this setting, the goal is to learn an \\epsilon approximate of the Nash equilibrium of the Markov game while minimizing the sample complexity, i.e. the number of episode played by the agent.  They present Optimistic Nash \nValue Iteration (Nash-VI) that output with high probability a pair of policies that attains an \\epsilon approximate Nash equilibrium in O(H^3SAB/\\epsilon^2) episodes where H is the horizon, S the number of states,  A and B the number of actions for the max-player respectively min-player. This rate matches the lower bound of order \\Omega(H^3S(A+B)/\\epsilon^2) by Jin et al. (2018) up to a factor min(A,B). They extend this result to the multi-player setting with Multi-Nash-VI algorithm of sample complexity O(H^4S^2\\prod_i A_i/\\epsilon^2) where A_i is the size of the action space of player i. The authors also provide VI-Zero an algorithm for reward-free exploration of N-tasks in Markov game with a sample complexity of O(H^4SAB log N/\\epsilon^2) and a lower bound of order \\Omega(H^2SAB/\\epsilon^2).\n\n\n-Contributions\nalgorithmic: Nash-VI (significance: medium)\ntheoretical: Nash-VI sample complexity of order O(H^3SAB/\\epsilon^2) (significance: high)\nalgorithmic: VI-zero (significance: low)\ntheoretical: VI-zeros sample complexity of order O((H^4SAB log N/\\epsilon^2) (significance: medium)\nalgorithmic: Multi-Nash-VI (significance: medium)\ntheoretical:  Multi-Nash-VI sample complexity  of order O((H^4S \\Prod_i A_i/\\epsilon^2)  (significance: medium)\ntheoretical:  lower bound for the sample complexity of reward-free exploration in Markov game of order \\Omega(H^2SAB/\\epsilon^2)\n\n-Score justification/Main comments\nThe paper is well written. The authors use the same technical tool as Azar et al. 2017 to get a sharp dependence in the horizon H and the state space size S. Precisely they use Bernstein bonuses in combination with the Law of total variance in the Markov game to obtain the H^3 and only concentrate the empirical transition along the optimal value function to push the S^2 into second-order terms.\nI have mixed feelings concerning this paper. On one hand, I think the contributions deserve to be published on the other hand I not convinced by the proof. Indeed the proofs are dangerously close to a sketch of proofs (see specific comments below) when it is not the case (see proof of Theorem 6). Even if I think most of the issues are fixable, considering the number of corrections required, I would need to read the updated version to assert if the results are correct.\n\nThe algorithm that attains a dependence A+B instead of AB is almost adversarial, do you think it is possible to obtain the same result with a model-based algorithm that uses the stochastic assumption, or do you see a fundamental reason why it will be impossible? \n\n-Specific comments \nP1: What do you mean by information-theoretic lower bound?\n\nP2, Table 1: It could be interesting to compare these algorithms on a toy example. At least implement Algorithm 1 to prove it is feasible.\n\nP3: precise what is \\pi when you introduce D_{\\pi}Q\n\nP5: Which (estimation) error the bonus \\gamma is compensating precisely?\n\nP6, Algorithm 1: The definition of the empirical transitions and the bonuses are not clear when N_h(s_h,a_h,_b_h) = 0.\n\nP7, comparison with model-free approaches: could you also compare them in term of computational complexity.\n\nP12, Non-asymptotic […] assumptions: precise what do you mean by highly sub-optimal because at the end Algorithm 1 is also sub-optimal by a factor min(A,B).\n\nP15, (Approximate CE): I think that \\phi \\circ\\pi is not a good choice of notation since it is not really composition.\n\nP15, Th 15: Could we, as in the two-player, deduce a Nash equilibrium by only computing CCE and get a polynomial-time algorithm?\n\nP19,Lemma 19: I do not understand the statement of the lemma, since \\bar{V}^k_{h+1}-… is a random variable you need to precise in which sense the inequality |V|(s) \\leq … holds. In particular, considering how you use it in the sequel it is not almost surely. Furthermore in the proof, you need to deal with the case when N_h^k = 0 and you cannot apply the empirical Bernstein Bound from Maurer and Pontil like that since you have a random number of samples. An additional union bound over the possible value of N_h^k is required and you need to prove that conditionally to this choice the samples are independent…. \n\nP19, proof of Lemma 19: you need to precise which event you consider in order to be able to invoke Lemma 18. \nP20, top of the page: the two equalities are wrong in (10) they are inequality because of the clipping for the first one and because \\hat{p}_h^k (\\bar{V} ….) \\geq 0. What do you mean by “with high probability” exactly? And since it is Hoeffding inequality the second term in 1/n is not necessary.\n\nP21, proof of Theorem 3: Again you need to precise with the event you consider to be able to call Lemma 18 or 19 … and guarantee that this event is of probability at least 1-p. Currently, a lot of union bounds are hidden whereas they should be treated properly.  \n\\zeta_h^k and \\xi_h^k are martingales with respect to which filtration?\nWhen you apply Azuma-Hoeffding inequality you implicitly imply that you can upper-bound all the \\zeta_h^k \\xi_h^k by the same constant (moving the bi O outside the sum) but you cannot because they are not necessarily positive.\nFor the pigeon-hole argument again you should consider the case N_h^k=0\nAfter (12) it is \\sqrt(H^3SABT).\n\nP22, Lemma 20: same remarks as Lemma 18 and 19.\n\nP23, Lemma 21: Because of the big O after “these terms […] separately” it seems that the constants in Lemma 21 are wrong. Furthermore, here you apply Hoeffding inequality plus a union bound over an \\epsilon-net or control in KL to obtain the H^2\\sqrt{S/n} bounds. You need to explain this properly.  \n\nP24, proof of Theorem 4: same remark as for the proof of Theorem 3. For (i) it is not only the Law of total variation but also because of the Hoeffding Azuma inequality ( and in Azar et al. it is only proved for stationary transition).\n\nP27, proof of Theorem 6: since you only prove the theorem for H=S=1 you should only state this case and keep the general case as a conjecture. \nS=H=1 in Markov game (where there is nothing to learn) is not equivalent to a matrix game. What do you mean exactly by reward-free matrix game? The agent does not observe the rewards, but in this case, there is nothing to learn, no? I do not think it easily generalizes to the Markov games setting. Could you also obtain the factor \\log(N) in the lower bound?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}