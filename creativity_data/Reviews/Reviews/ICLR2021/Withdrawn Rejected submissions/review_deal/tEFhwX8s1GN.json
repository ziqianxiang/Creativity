{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper primary theoretical contribution claim is to establish the constant size SGD converges linear to the optimal solution in non-convex settings. This is shown in the interpolation regime for over-parametrized situations when starting from points nearby to the optimum. The paper's empirical claim is to use relatively larger learning rates for SGD in common deep learning settings and claim that they can do well. \n\nMy recommendation is based on the overall low scores provided by the reviewers - which did not change post rebuttal. The concerns raised by the reviewers amounting to my decision recommendation is summarized below - \n\nOverall the reviewers found the connection between the theoretical results and the overall claims of the paper unconnected. The reviewers found the theoretical contribution of the local convergence weak - particularly in the context of an analysis of constant learning rates and taking into account existing work on the convex case for such results. Furthermore, the experimental contribution of the paper is incremental as the proposed algorithm is standard with just a larger than typical initial learning rates. This factor is usually searched over during Hyper Parameter sweeps in all the large scale learning setups. In this context, SGDL performing favorably, is an interesting observation but not enough of a contribution. Further the reviewers objected to the fact that SGDL does not connect with the theory presented as SGDL in experiments still uses learning rate schedules.\n\n"
    },
    "Reviews": [
        {
            "title": "The paper lacks of innovation and theoretical justification for their claiming points.",
            "review": "Main idea: As a classical and effective optimizer, vanilla SGD can always compete with or even outperform its momentum or adaptive variations when training over-parameterized DNNs. The paper aims to theoretically justify this claim and empirically compare the performance across multiple tasks.\n\n1.\tWhat is exactly the overall advantage or difference between the SGDL and the vanilla SGD? Particularly, in how to choose the stepsize alpha?\n2.\tThere is no theoretical comparisons between SGD and its momentum and adaptive variations. Because the paper claims to theoretically justify the claim that “SGD is better”, can authors point out how they justify it theoretically?\n3.\tRemark 1 that follows theorem 2 gives particular conditions to make Eq.(7) hold. Can authors explain more on how to derive these 3 conditions 8(a)-(8(c)?  Does the proof assume that the initial point is close enough to the optimal solution? How can SGDL globally converge (converge from any starting point)?  \n4.\tIn experiments, what is SGDM? \n5.\tIn experiments, the paper uses decaying learning rate, so a large initial stepsize can quickly decay into a small number, so how does this become an advantage?  SGD has better generalization which has been observed in many prior works.\n6.\tThe paper has some typos, and the meaning of some sentences is puzzling.  For instance, (1) there are multiple uses of N in definition 2; (2) the index used in the paper is not consistent, i=1:N and i=1,…,N are both used in the manuscript and other format has been used in the proofs; (3) Eq.8(b) and the condition of theorem 2 are not consistent.\n\n#####################\nupdate:  I have read authors' response to my comments and also read other reviewers' comments and discussions.  The main concern of my comments is still not clear. I will keep my rating unchanged. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Reviews for The simpler the better: vanilla sgd revisited",
            "review": "This paper studies the smooth finite-sum problem under suitable conditions in the non-convex case. They show the necessary condition for the minimizer $x^*$ being a point of attraction, and Theorem 1 provides a sufficient condition for the strong minimizer $x^*$ to be a point of strong attraction with high probability. Based on the results, they introduce a modified SGD algorithm with a large initial learning rate (SGDL), and provide extensive experiments on various popular tasks and models in computer vision, audio recognition and natural language processing. \npros: 1, They give a sufficient condition for the strong minimizer $x^*$ to be a point of strong attraction with high probability. \n2, Extensive experiments are presented to show the effectiveness of SGDL. \n\ncons: 1, In Theorem 2, it is better to show how to choose the $\\epsilon$ explicitly and what linear convergence rate can be achieved, i.e., how small the parameter $\\rho$ can be. \n2, Even though this paper considers the non-convex case, the assumptions seems very restrictive. Assumption A.3 means that in a neighborhood of $x^*$, the objective function is essentially strongly convex. Furthermore, all $\\nabla f_i(x^*)$ need to be zero. \n\nminor comment: In the first inequality of the proof of Remark 2, why the bound is not zero? Since under the condition $\\tau=\\infty$, $||X_k-x^*||$ should be no larger than $\\epsilon$. \n\n---------------------After the rebuttal------\nThe authors partially addressed my concerns. I remain the current score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting (albeit a bit disconnected) theoretical and empirical results",
            "review": "This paper presents a theoretical analysis of SGD with constant step size (SGD-CS) and presents conditions under which SGD-CS leads to parameter updates that converge to a local minima, including parameters of non-convex functions.  The authors then show, in context of some special functions, that the step size can be fairly large yet convergence is achieved.  This is followed by a number of empirical studies of SGD with large (but annealed) step-size (SGDL) on a variety of tasks.\n\nI find the connection of SGD-CS with SGDL tenuous, and it is not clear that the theoretical analysis helps in selecting largest possible step size.  However, I do find the following contributions of the paper valuable:\na) Analysis of SGD-CS sheds lights on conditions under which the minimizers (local) of objective functions act as attractor (or strong attractor) of SGD updates.  This is worth sharing.\nb) The empirical results with SGDL show a very consistent pattern of SGDL outperforming other optimization approaches (including ADAM, SGD with momentum, etc.).  While this is not really SGD-CS, I find the consistent behavior of SGDL worth noting and sharing.\n\nSome clarifications and minor typographical errors:\n* SGC in last sentence of Section 3.1 is not defined.\n* Section 3.2 assumptions A.1 and A.2 should refer to Eq. (1b) and not (1a) I think\n* In Figure 2 which optimization approach is used to derive the curves?\n* In Fig. 6, the two curves in (a) and two in (b) … are they train and test perplexity results?\n* Last sentence of conclusions is unclear, please restate.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Weak analysis, strong and unrealisitc assumption, and unsupported claims",
            "review": "This paper investigates the SGD with constant step size (SGD-CS) on non-conex optimization problems. Theoretically, the paper shows the conditions under which a minimizer $x^*$ is a point of attraction in a local neighborhood under the algorithm SGD-CS with sufficiently small step-size. Furthermore, the paper experimentally shows that vanilla SGD-CS with relatively large step-size performs well, or even outperforms its momentum and/or adaptive counterparts, on several popular deep learning tasks.\n\n[Comments]:\n\n1: Claim and analysis are not consistent. The authors claim in the introduction that convergence of SGD-CS on non-convex functions is shown. However, the analysis only focuses on points of attraction and the stay within a local neighborhood. I would like to point out that the latter concepts are not equivalent to convergence. To show convergence of an algorithm, the missing piece of the paper is that, starting from the initialization point, the algorithm can be guaranteed to find such a local neighborhood around the point of attraction. Without this guarantee, the analysis of points of attraction is meaningless in the sense of algorithm convergence. Hence, I disagree that convergence of SGD-CS is theoretically shown in the paper.\n\n2: Considering the necessary condition for points of attraction, Theorem 1, the assumption A.3 barely holds true in practical cases. As stated in Theorem 1, $x^*$ being a point of attraction implies interpolation property. For most real world tasks, interpolation can be only achieved when the model is over-parameterized, i.e., number of parameters is greater than the number of data samples. (For example, consider solving a system of linear equations). As pointed out by the work [Liu et al. 2020], most of the minimizers are not isolated, instead they form a low-dimensional manifold. In this case, none of the minimizers satisfies Assumption A.3, because the Hessian matrice H at the minimizers always have zero-eigenvalues (flat directions).\n\n3: The main theoretical result, Theorem 2, relies on the fact that step-size is sufficiently small. However, one of the main claims of the paper is the convergence under large step size, as discussed in Section 5 and experimented in Section 6. I don’t see the connection between the small step-size theoretical result and the large step-size experiments. The theory seems not to explain the experiments.\n\n4: The paper frequently talks about large learning rates. However, it is not clear to me what is the criteria to be large or small. Especially, in section 5, the paper provides a certain range of step size values (e.g., step size $t \\in (0, 1/\\lambda_{max}(H))$, within which the SGD-CS converges on a few simple examples. What are the reasons to claim these step-sizes are large?\n\n[About clarity]:\n\n1: It should be reader friendly to enlarge some of the figures.\n\n2: Providing an intuition of the error function, defined in Eq.(4), should be helpful.\n\n3: Notations can be improved.\n\n[References]:\n\n[Liu et al. 2020] Liu, Zhu, and Belkin. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. arXiv:2003.00307.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}