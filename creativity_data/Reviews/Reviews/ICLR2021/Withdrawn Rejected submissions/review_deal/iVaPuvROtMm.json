{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "While the reviewer's noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity. I agree. (Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion. Later on page 1, you use the term \"the dynamics of density\" but you should not assume that the reader knows what that means. There are typos as well, e.g. \"make all predictions base[d] on Equation (6)\" on page 4. It would be helpful to know something about why you chose the experimental setups in Synthetic-1, 2, and 3. )\n\nRegarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future."
    },
    "Reviews": [
        {
            "title": "Interesting problem, but requires more work",
            "review": "The paper addresses an interesting problem: learning stochastic dynamics from aggregate data. The aggregate data refers to the setting when the data is anonymized. For example, one has data about the location of the birds at different instants in time, but the birds are not labeled and can not be distinguished from each other. For the purpose of learning, the paper proposes to use a generator, in the setting of WGAN, to learn the drift term in the dynamics. The novelty of the proposed approach is to consider the weak form of the Fokker-Planck equation to express the loss function. \n\nThe problem is interesting, but I found several issues about the proposed approach that need to be addressed.  \n\n1- The paper consider a special setting where the dynamics is given by a diffusion process. It is assumed that the diffusion coefficient is known and the objective is to learn the drift function. I think this is a strong assumption and learning diffusion coefficient is also very important and need to be included in this work.  \n\n2- Learning the drift function from aggregate data may have fundamental limitation that the paper needs to address. It seems impossible to learn mixing terms (the terms in the dynamics that do not effect the distribution) from aggregate data. In other words, even in and ideal setting with infinite capacity for neural network and infinite amount of data, the learned drift function differs from the actual one. This can be verified by running a simple experiment with dynamics [[0,1][-1,0]] so that the distribution does not change and the learned drift function should be zero. \n\n3- In the proposed formulation, eq (8), it is not clear which one is real data and which one is generated data. \n\n4- Theorem 1 is not stated correctly and it is missleading. It states that eq (7) is the Wasserstein distance. However, eq (7) is just an approximation. And I really don't think this should be stated as a result in a Theorem. This is basic definition of Wasserstein distance and eq (1) and it is more appropriate to be presented as part of text.  Also, Lemma 1 is well-known and a reference should be given. \n\n5- The numerical experiments are nice, but they should also consider the long-term behavior: If the learned dynamics results to the correct stationary distribution or not. Because the stationary distribution is one of the important characteristics of a stochastic dynamics.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Accept",
            "review": "This paper proposes a new approach to learn the dynamics of density evolution of objects from aggregated data. The basic idea is to derive a closed-form Wasserstein distance between the empirical data distribution and the predicted distribution generated from the weak form of Fokker Planck Equation (FPE). Based on this measure, an objective function is developed to learn the underlying drift coefficients and the discriminator simultaneously using neural networks. Some theoretical results are provided and numerical experiments are carried out to illustrate the effectiveness of the proposed method.\n\nThe paper is overall well written and solves an important problem. I have only a few minor questions.\n\n1. How prediction is made based on the estimated model? One step ahead or multiple-step ahead prediction? Please provide more details on this issue.\n\n2. Some notations are unclear in Theorems 1-1. In Theorem 1, what are the definitions of $N$ and $x^{(k)}$? Do you generate data for N times? In Theorem 2, I  assume that $n$ stands for $n$ steps ahead from t_{m_0}?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A paper on an interesting topic, but with strange similarities with an already-published article ",
            "review": "The paper is about an interesting setting, where observations correspond to lots of individuals evolving over time. The twist is that individual identifiers are not available.  Thus from one time point to the next, the observer does not know which individual is which. The problem is to learn a time series model from this, and models considered here are in the form of an SDE.\n\nIn my first reading of the paper, I thought the topic was very interesting but I struggled to follow some parts of the manuscript, which I thought was overly unclear. Some of the notation is undefined or ill-defined, the general reasoning is hard to follow, the notation does not help identifying the objects that are observed from the objects that are simulated, there is a lack of acknowledgement of limitations of the proposed method, the synthetic examples are not well-motivated, and various other fairly standard concerns.\n\nHowever, during my second reading I looked at the reference Wang et al, \"Learning Deep Hidden Nonlinear Dynamics from Aggregate Data\", published in UAI in 2018.  From this I got more serious concerns.\n\nThe authors' abstract starts with \"Learning nonlinear dynamics from aggregate data is a challenging problem since the full trajectory of each individual is not available, namely, the individual observed at one time point may not be observed at next time point, or the identity of individual is unavailable due to technical limitations, experimental cost and/or privacy issues.\" \n\nWang et al's abstract mentions: \"However, in most of the practical applications, these requirements are unrealistic: the evolving dynamics may be too complex to be modeled directly on observations, and individual-level trajectories may not be available due to technical limitations, experimental costs and/or privacy issues.\"\n\nThe authors write \"In the work of Hashimoto et al. (2016), a stochastic differential equation(SDE) was adopted to capture the dynamics of particles directly on observations, in particular the drift coefficient is parameterized by a recurrent neural network.\"\n\nWang et al. write: \"Modeling the dynamics on aggregate observations have been investigated recently in (Hashimoto et al., 2016), where a stochastic differential equation (SDE) has been used to capture the transition directly on observations Yt.\"\n\nThe authors write: \"There are many existing models to learn dynamics of full-trajectory data. Popular ones include Hidden Markov Model (HMM)(Alshamaa et al., 2019; Eddy, 1996), Kalman Filter (KF)(Farahi & Yazdi, 2020; Harvey, 1990; Kalman, 1960) and Particle Filter (PF) (Santos et al., 2019; Djuric et al., 2003). These models and their variants (Deriche et al., 2020; Fang et al., 2019; Hefny et al., 2015; Langford et al., 2009) require full trajectories of each individual, which may not be directly applicable to the aggregate data as we mentioned earlier. \"\n\nWang et al. write: \"Existing models such as Hidden Markov Model (HMM) (Eddy, 1996), Kalman Filter (KF) (Harvey, 1990) and Particle Filter (PF) (Djuric et al., 2003) are popular methods with hidden variables. However, these models and their variants (Langford et al., 2009; Hefny et al., 2015) require individual-level trajectories, which may not be available, as was mentioned earlier.\"\n\n--\n\nBased on these strong similarities I do not think that the proposed manuscript is suitable for publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}