{
    "Decision": "",
    "Reviews": [
        {
            "title": "AnonReviewer4",
            "review": "This paper proposes a simultaneous similarity-based self-distillation (S2SD) approach to alleviate the higher retrieval brought by high dimensional embedding. Through S2SD, this paper can achieve higher results on several benchmarks. Overall, I vote rejection at this time given the following considerations:\n\nPros:\n1) The experiments of S2SD seem to effective in several datasets and the experiments detail is very clear to me.\n2)  The methodology part is easy to follow, combine 3 types of distillation make sense to me.\n\nCons:\n1) The first concern is about the motivation of this paper.  From the abstract, it seems the motivation is reducing the high dimensional embedding for retrieval task. However, I did not see explicit experiments to discuss the cost of reducing by the proposed method.\n\n2) The proposed combination of distillation is not very novel to me. It combines with relation distillation, self-distillation and feature distillation. These components have been investigated in the literature.\n\nThanks to the author response. However, the novelty of this paper for me is not good enough to accept in ICLR. As the authors agree on the relational distillation, self-distillation or feature distillation is existing components. Simply leveraging them in a new context without a very strong and clear motivation is not good. I noticed the discussion between the Authors and Reviewers 3 about the novel part.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Deep metric learning (DML) with focus on making a low-dimensional embedding space",
            "review": "This paper tackles deep metric learning (DML) with focus on deriving a low-dimensional embedding space. The main idea is to leverage knowledge self-distillation in two aspects: from high-dim embedding space to low-dim embedding space, and from feature space to embedding space. Overall, the idea of using knowledge distillation for compressing the embedding space for DML is valid and stand. The proposed method is evaluated on three image retrieval datasets with a good setup following a recent work from Roth et al 2020(b).\n\n\n*** Method ***\n\n1) The description\toverall is clear but some details are hard to understand. Concretely, how is the feature distillation formulated in Eq (6). In particular, how is the normalised backbone and how it generates normalized feature representation?\n\n2) The method itself is totally based on self-distillation, hence more self-distillation methods in literature should be included, e.g. [a,b]. -- [a] T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anand- kumar, “Born again neural networks,” ICML, 2018. [b] X. Lan, X. Zhu, and S. Gong, “Self-referenced deep learning,” in Asian conference on computer vision. 2018.\n\n3) How about combine similarity (pairwise relation) based distillation with more common logins based KL loss?\n\n\n*** Evaluation ***\n\n1) All three datasets selected are not large scale, smaller than 25,000 images. They are clearly not suitable to demonstrate the performance and efficiency of large scale image retrival as the paper aims to solve. I know that the authors mainly followed the previous studies in data selection. However, it is still an issue in general for the gap between the claims and the experimental setup.\n\n2) The setup following Roth et al. 2020b is a good choice, and evaluation basis.\n\n3) A good set of ablation studies have beed done, which is informative. One missing piece is the performance of auxiliary high-dim embedding space.\n\n4) No time complexity analysis across different embedding dimensions.\n\n\n*** Minor ***\n1) Quite a number of references have no publishing venues. \n\n-----------------------------------------------------------------------------------\n\n*Final comments*\n\nThank the authors for detailed response to all the comments including mines. \n\nAfter reading the author responses and the comments of all the reviewers, I would keep my overall rating (learn to accept), although  knowledge distillation is not the first time used for metric learning. Most of my concerns are resolved. The authors have also pointed out the main differences with the most related works raised by peer reviewers. Meanwhile, I agree to some degree that the novelty is not so significant as all the components are from existing works. However, the idea of using KD for dimension compression is valid and the model presented in this work is effective. This would provide the community useful contributions in this specific field at least. \n\nAn extra comment. As clarified by the authors in response, the feature space and embedded space are connected by a dimension reduction layer. This type of design is very similar to the projection head in recent contrastive loss based self-supervised feature learning works (e.g. SimCLR, ICML20). It would be interesting to draw this connection in the final version. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice work combining metric learning and self-distillation",
            "review": "This paper presents a similarity-based self-distillation method to boost the performance of metric learning by distilling the knowledge from high dimensional embeddings. The proposed will not introduce additional computational cost during inference while improving the retrieval performance. The proposed can be served as a general approach to improve low dimension embeddings, which may be useful in real-world applications. Experiments on standard benchmarks show consistent improvement over several popular baseline methods such as MultiSimilarity and Margin.\n\nI think this is a good paper overall with a new and reasonable method and decent experimental results. Considering the high-quality of this paper, I would like to recommend acceptance. The strengths of this paper are:\n- The idea of combining metric learning and self-distillation is interesting and new. Compared to previous methods based on standard knowledge distillation, this method is more flexible and lightweight.\n- The experiments look convincing. A consistent improvement over baselines is achieved. The proposed self-distillation method can even outperform Teacher-student distillation.\n- This paper is well written and easy to follow. \n\nAlthough this paper overall is nice, I still have some concerns/suggestions:\n-  This paper uses \"Simultaneous Similarity-based Self-distillation\" as the main contribution. However, the idea of simultaneous self-distillation is not completely new. A line of work on simultaneous self-distillation [r1, r2] should be compared and discussed. The differences should be highlighted to show the real contribution of this work.\n- Recent work [r3] shows RP and mAP@R are more reasonable metrics to evaluate embedding performance. It is suggested to add the results under the two new metrics. \n\n[r1] Be your own teacher: Improve the performance of convolutional neural networks via self distillation. CVPR 2019.\n\n[r2] MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation. ECCV 2020.\n\n[r3] A Metric Learning Reality Check, ECCV 2020.\n\n\n--------\nfinal review:\n\nThanks for your response.\n\nAfter reading the authors' feedback and other reviews, I still think it is an interesting work with some new technical contributions and decent improvements based on fair comparisons. I think it is a lightweight and generic method to improve various metric learning methods. Therefore, I would like to keep my initial score.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "I vote for rejection because this paper has not grasped multiple underlying studies, lacks novelty, and brings small performance improvement.",
            "review": "**Summary**\n\nThe paper delivers a KD-like method that could yield a better performance metric (e.g., R@1) when the embedding dimension is small. The method, S2SD, brings about +3.5%p (CUB200) and +4.5%p (CARS) performance boost for the R@1 metric.\n\n**Strong points**\n\n- Although additional computing resources required to use the proposed method are not described in the main text and supplementary, additional projections ($g$s) of S2SD lead to performance gains even with small computing resources, presumably.\n- This paper has verified the proposed method through a new deep metric learning benchmark presented in academia recently [1], making the result excels in terms of fairness.\n\n**Weak points**\n\n- To say that its method is KD-like is because its teacher network also updates its parameter during the training, and such technique is already well established in the term of *mutual learning* [2]. There is a terminological problem that arises because of this: if the teacher model is also trained with the student, then neither model can be called a \"teacher\" or \"student\". Totally omitting the literature related to mutual learning acts as one of the major weaknesses of the paper.\n- Very similar work has been published in ECCV 2020 workshop [3]. [3] also brings several auxiliary models to distill their knowledge to each other using in-batch *relational KD* as S2SD does, yet reports better performance than S2SD. Moreover, [3] experimented with more diverse cases such as heterogenous backbones (using both Resnet-50 and BN-inception) or the scalability of the number of models participating in training, showcasing a various aspect of when mutual learning meets deep metric learning, which is rarely covered in this paper.\n\n**Recommendation**\n\nI vote for rejection because this paper has not grasped multiple underlying studies, lacks novelty, and brings small performance improvement.\n\n[1] Roth, Karsten, et al. \"Revisiting training strategies and generalization performance in deep metric learning.\" arXiv preprint arXiv:2002.08473 (2020).  \n[2] Zhang, Ying, et al. \"Deep mutual learning.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.   \n[3] Park, Wonpyo, et al. \"Diversified Mutual Learning for Deep Metric Learning.\" arXiv preprint arXiv:2009.04170 (2020).  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}