{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Knowledge distillation (KD) has been widely used in practice for deployment.  In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co-train the teacher and student models. The proposed method is novel and interesting. Empirical results showed that the proposed method can perform better than several KD variants.  However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal.   Besides this,  a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments.  In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task. On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model,  and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer. [1] also contains a  theoretic analysis  (mean-field analysis based) to support their method. Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient. \n\n[1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020."
    },
    "Reviews": [
        {
            "title": "Intriguing method but needs more work before publication ",
            "review": "This paper suggests an interesting approach to knowledge distillation, which uses architectural properties rather than the loss function to encourage knowledge transfer between a teacher and a student. A student and teacher net are jointly trained on a prediction task, with forward connections from layers in the student net to layers in the teacher net. At test time, the teacher net can be stripped away. The method results in improvements in student performance compared to training the student without the teacher.\n\nThe main positives I see in this paper are:\n+ modest improvements over baselines in the one-stage KD setting\n+ provides a different perspective on KD compared to recent works that are based on loss functions\n\nAlthough the results look promising, I think the paper is not yet ready for publication. The main issues I see are:\n- no comparisons against network pruning and compression methods, which are arguably more relevant than KD baselines\n- little insight or analysis of why the method works\n- concerns with the experiments, detailed below\n\nThis paper positions itself as a KD method, and argues that a novelty is in doing KD via architectural tricks rather than via a loss function. This may indeed be a new perspective for the KD literature, but it isn’t without precedent. Methods that use architectural scaffolding at training time, which is removed at test time, do exist, simply under other names such as weight pruning or model compression (many of these papers are cited in the intro of the current submission). The current paper needs to make it clearer how the proposed method relates to those methods, and how it goes beyond them. Ideally this should include quantitative comparisons, or an explanation for why the other methods are not applicable.\n\nMy second major concern is that this paper provides very little in the way of an explanation for why the method works. There is an intriguing statement that the method “enhances the learning performance of the student model due to the backward gradient flows from the teacher,”  but nothing to back this statement up. Some analysis of how these gradients achieve desirable effects would greatly strengthen the paper.\n\nMy third concern is with the experiments. First, the numbers in Table 2 are somewhat lower than those reported in prior papers (see Table 1 of Tian et al. 2020). Appendix A2 states that the code from Tian et al. 2020 was used to run the comparisons. Why then the discrepancy in performance? Second, many of the prior KD methods perform best when their objective is combined with the original Hinton KD objective (see Table 7 of Tian et al. 2020), but this comparison is not provided in the current paper. These two concerns mean that I’m not sure the proposed method is really outperforming competitive baselines from prior work. Lastly, I did not find enough details about ECD* to be able to really evaluate if those results are strong or interesting.\n\nStylistically, I think the paper would be improved by adopting a more even-handed tone. Statements like “compared to existing KD methods, ECD is more friendly to end users thanks to its simplicity” or that the method is “simple and neat” come across more as advertisement rather than as scientific analysis. The intro argues that the one-stage nature of the proposed approach makes it more applicable than two-stage approaches, but I would say one and two-stage methods are simply targeting qualitatively different applications. Two stage approaches are useful when you are _given_ a big model, which maybe you do not have the resources or data to train, and want to compress it or adapt it, e.g. for mobile deployment. One stage approaches are useful when you are able to train the big model yourself. There are interesting tradeoffs between these two paradigms and one is not better than the other. The current paper should acknowledge this. In general, the advantages and disadvantages of the method should be discussed equally. I also think the paper overemphasizes how simple the method is. I don’t personally feel this method is any simpler than methods that use loss functions, and in fact I find the proposed method more conceptually complex since I don’t know why it works.\n\nDespite these criticisms, something interesting does seem to be going on with this method, and I encourage the authors to pursue it further.\n\nMinor comments:\n1. Abstract: “temporally” —> “temporarily”?\n2. Table 1: what is the \"Baseline\"? The student?\n3. Table 2: citations should be added for each method",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and novel work knowledge distillation, with some unanswered concerns",
            "review": "Summary:\n\nThe paper proposes new KD framework, i.e., Explicit Connection Distillation (ECD), which unlike existing methods, designs teacher network that is well aligned with the student architecture and trains both the networks simultaneously using explicit dense feature connections. The proposed method is evaluated on CIFAR-100 and ImageNet datasets.\n\nStrengths:\n\n- The proposed method neither requires any explicit pre-trained teacher network, nor any distillation loss. So, the method overcomes the problem of selecting teacher network or alternatives of distillation losses for the task at hand.\n- By design, the generated teacher network has features aligned with the student network at every layer.\n\nConcerns:\n\n- Though existing works involves complex optimization in terms of losses but the hyperparameters involved in distillation like the weight on distillation loss or the temperature value is not so sensitive like learning rate. Even without careful tuning, decent distillation performance can be achieved with moderate temperature, high weight on distillation loss and low weight on cross entropy loss. So, this is not a major limitation in existing methods.\n- In the proposed ECD framework, both the teacher and student networks are trained simultaneously, so number of trainable parameters (teacher parameters + student parameters) would be large. So, the method may not work well in case of limited amount of training samples.\n- Selecting an optimal value of kernel number ‘n’ is a concern.\n- The gain in performance in Table 1 for WRN-40-2 is marginal. So, it seems the proposed method may not be effective on some architectures like wide ResNets where the network channels are widened.\n- In Table 5, marginal improvement using ECD over stage wise feature supervision.\n- Table 4 shows shallow layers migrate more information than higher layers and dense connections are preferred on shallow layers only to get optimal performance. But identifying the layer from which high level semantics would be captured is non-trivial.\n\nQueries for authors:\n\n- Any restriction or range of values that alpha can take?\n- All the experiments are done with n=16, how the performance changes by varying ‘n’?\n- Is the performance of the teacher reported in Table 1, obtained through auxiliary teacher involving feature connections with the student network?\n- While training using the proposed ECD, how to decide number of epochs for training (based on either teacher or student performance on validation data)?\n- Details about ECD* and how learnable ensemble is applied is not mentioned in detail even in Appendix.\n\nGeneral Remarks:\n\nWhile the creation of auxiliary teacher directly from the student network removes its dependencies from pre-trained teacher but dependency on several design choices like the number of dynamic additive convolutions for the first module and appropriate places for adding connection paths in the second module for explicit flow of layer to layer gradients remain.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An Interesting Knowledge Distillation Method. But I Do Not Understand Why It Works",
            "review": "The authors propose a new knowledge distillation method applicable to convolutional neural networks. Given the architecture of a student network, the teacher network is constructed by replacing each convolutional layer with a dynamic additive convolutional layer. In this way, the teacher network is guaranteed to be more capable than the student network. Then, the teacher and student models are trained together, minimizing their own training losses. In order to \"distill\" the student model, the student and teacher model are inter-connected in a way such that the student model receives gradient flow from the teacher network.\n\nPros:\n1. The proposed method is straightforward and easy to implement.\n2. By adding an additional distillation loss on the logits (the ECD* method), it achieved competitive distillation results.\n\nCons:\n1. The authors claim that the gradient flow (from the teacher model to the student model) helps improving the student model. But I do not see why. The gradient is from the teacher's loss. So by applying the gradient on the student model, only the teacher's loss can be improved. This is my greatest concern. Similarly, from Equation (2), I do not see why optimizing \\theta_T would help the generalization ability of the student model.\n2. The authors made several claims about existing knowledge distillation methods. For me, many of the claims are not meaningful. For example: \"..in real applications, for two-stage KD methods well pre-trained teacher models are usually not available, ...\" This is true. But what is the drawback of \"well pre-trained teacher models are usually not available\"? All distillation methods require a teacher model. The proposed method still need to train a teacher model. In fact, I might claim that two-stage KD methods can utilize existing pre-trained teacher models, whereas the proposed method always has to train a teacher model during distillation. Similar claim was made on \"designing complex distillation losses\" as a drawback. But a user does not have to design new forms of losses if he/she stick to existing methods. On the other hand, if the proposed method became popular, researchers may add auxiliary losses on top of the proposed method, and that would not seem like a drawback for the proposed method. \n\nSome confusions:\n1. When comparing to different distillation methods in Table 2, are all teacher models have the same architecture?\n2. When generating the teacher model, do the authors initialize the weights of the teacher model randomly, or according to the student model?\n\nAfter reading the author feedback, I upgrade my rating to 6. See responses in this thread for reasons.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3 ",
            "review": "Overall, I vote for marginally below the acceptance threshold. I think the idea is somewhat novel for the Explicit Connection Distillation, especially for cross-network layer-to-layer gradients propagation. This paper proposes a new strategy of Knowledge distillation called  Explicit Connection Distillation (ECD), and the ECD achieving knowledge transfer via cross-network layer-to-layer gradients propagation without considering conventional knowledge distillation losses. Experiments on popular image classification tasks show the proposed method is effective.\nHowever, several concerns including the clarity of the paper and some additional ablation studies ( see cons below) result in the decision.\n\n##########################################################################\nPros: \n1) The knowledge distillation by cross-network layer-to-layer gradients propagation is somewhat novel to me.\n2) This paper is easy to follow, the methodology part is clear to me.\n3) The experiments part show detail ablation study of each component and the supplementary lists almost detail of experiments which help the community to reproduce the proposed methods.\n\n##########################################################################\nCons: \n1) The first concern is about motivation. (1): The author claims conventional KD methods leads to complex optimization objectives since they introduce two extra hyper-parameters. To my best of the knowledge, these two parameters have not too much search space. e.p Temperature is from 3-5 and the weight is T^2 from Hilton's paper and following paper.  (2): The drawback of one-stage KD methods is a little bit overclaim,  Both ONE and DML can be applied to a variety of architecture. In my opinion, the teacher design of  ECD  follows a similar strategy with ONE and its variants, which is the teacher is wider than the student.  Overall, I think the motivation of this paper needs to be very careful to clear.\n2) The fairness of comparison. Is the Dynamic additive convolution component used to in student network, does this will influence the comparison of  Table2, Does ONE and DML also use that?\n3) Why the automatically generated teachers of ECD is much lower than other methods in Table2 in term of performance and results in higher student performance. Is there any explanation here, like such as [1][2]?\n4) Could you provide the computation cost comparison of the proposed method and other one-stage methods in Table2.\n5) Some recently SOTA work is missed [3][4], although I know the performance of this paper is outperformed. I think they need to be discussed.\n\n\nReference:\n[1]: Improved Knowledge Distillation via Teacher Assistant\n\n[2]: Search to distill: pearls are everywhere but not the eyes\n\n[3]: Online Knowledge Distillation via Collaborative Learning\n\n[4]: Peer Collaborative Learning for Online Knowledge Distillation\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}