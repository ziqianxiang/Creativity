{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers have some following concerns: \n\n1) There is lack of experimental result. The experiment on MNIST with small CNN architecture is definitely not sufficient to verify the efficiency of the proposed method. Moreover, the advantage of the proposed method is not very clear due to the choices of the parameters. The choice of the learning rates is quite sensitive. \n\n2) It is not clear why the authors could argue that $ \\mathbb{E}(V_T) = \\mathcal{O}(T)$ without any theoretical and empirical support. Even if this is correct, this term could dominate the first term unless $ \\mathbb{E}(V_T) \\leq \\mathcal{O}(\\sqrt{T})$, which is too strong. If assuming $\\mathbb{E}(V_T) = \\mathcal{O}(T)$, the convergence results are upper bounded by some constant (note that $\\epsilon$ is a constant in this scenario, not arbitrarily small). Hence, the authors failed to show the convergence to a stationary point. \n\nThere are some suggestions to improve the paper as follows: \n\n1) Show $\\mathbb{E}(V_T) = \\mathcal{O}(T)$ and revise the theory properly to make it rigorously by showing upper bounded by some function $R(T) \\to 0, T \\to \\infty$ rather than showing the convergence to some fixed neighborhood. (Note that $\\frac{C_4}{\\sqrt{N}}$ is a fixed constant). \n\n2) Do more experiments on various datasets and network architectures to verify the efficiency of the proposed method and show the clear advantages compared to others. \n\n3) Provide convergence rate comparisons with other decentralized algorithms (e.g., as a table). It would be nice if the authors also provide the assumptions and the dependent constants so that the readers could really see the differences. \n\n4) Explicitly derive the convergence measure based on the standard one, that is, $\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E} [ \\| \\nabla  f (  X_t )  \\|^2 ] $ and add the dependency of $G_{\\infty}^2$ to the bound. \n\n5) Revise the paper and implement all necessary comments from the reviewers consistently with the content. \n"
    },
    "Reviews": [
        {
            "title": "Weak theoretical results with weak experiments.",
            "review": "\nThis paper studied the decentralized adaptive gradient methods and provided convergence guarantees. Experiment on MNIST is conducted to show the effectiveness of the proposed approach.\n\n1. The theoretical result is weak. The linear speedup result is not proved as in (Lian et al. 2017), the benefits of adaptive gradient methods are also not illustrated in the bound in Theorem 2 and Theorem 3.\n\n2. The learning rate scheme is not practical and does not hold in practice. As illustrated in Theorem 2 and 3, the learning rate $\\alpha$ is set to be less than $\\epsilon^{0.5}/16L$. \n\n3. The LHS of Theorem 2 and Theorem 3 are not the standard gradient squared norm but the scaled version. It is unclear what is the bound if the LHS is the standard gradient squared norm as in (Lian et al. 2017). It is important to use the same measure as in the previous literature for fair comparison.\n\n3. The experiment is weak. Doing distributed training only on a tiny dataset on MNIST is not sufficient. I would like to see results on larger datasets such as CIFAR and ImageNet.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper still need many improvements.",
            "review": "In this paper, the authors attempt to use adaptive gradient methods in decentralized training paradigm. They develop a general framework to convert an adaptive gradient method from a centralized one to its decentralized variant. Specifically, they propose a decentralized AMSGrad algorithm. They also point out a potential divergent problem of an existing method and investigate the conditions to ensure convergence. Finally, they conduct some experiments to verify the performance of their algorithm.\n\nPros:\n1. This paper is the first one to use adaptive gradient method to decentralized training paradigm and the authors also provide a completed convergence analysis for their algorithm.\n2. This paper reveals a divergent problem of DADAM in offline situation by taking some intuitive examples. Based on this issue, they find a solution that makes consensus on the adaptive learning rates.\n3. This paper proposes a general framework to apply adaptive gradient methods to decentralized optimization. The authors also investigate the conditions to make sure these decentralized variants will converge.\n\nCons:\n1. In section 3.2, the paper claims AdaGrad and AMSGrad satisfy the condition to guarantee the convergence of Algorithm 2 while Adam does not. It seems not to be an obvious conclusion from the reference Chen et al. (2019). Is there more explanation or proof about why AdaGrad and AMSGrad satisfy the condition? And does it mean Adam still diverges even after using the algorithmic approach proposed in this paper?\n2. In Theorem 2, the convergence analysis result of Algorithm 2 is given. However, the convergence of common adaptive gradient methods such as AdaGrad and Adam is still not clear. Therefore, the “convergent adaptive gradient method” in the title is very misleading. Do most of the adaptive gradient methods have the same theoretical guarantees as AMSGrad?\n3. \\mathbb{E}[\\sum_{t=1}^T \\lVert (-\\hat{V}_{t-2} + \\hat{V}_{t-1}) \\rVert_{abs}] = o(T) is the key condition to ensure the convergence. But when the above equation is O(\\sqrt{T}), the convergence rate is worse than the centralized counterpart. Is that case possible?\n4. In section 3.4, the experiment is divided into homogeneous and heterogeneous data, which is very confusing. What is the reason for doing this and what will happen if we just deal with the dataset normally? The heterogeneous data is treated very intentionally. Is there any discussion about when the treatment of heterogeneous data is important?\n5. In the homogeneous data experiment, the performance of DADAM and decentralized AMSGrad are similar. What is the reason that the learning rates on different node tend to be similar? Is that a common case? Maybe the experiment on more dataset is needed to address this concern. Besides, how will such similarity among data impact the theoretical convergence?\n\nMinors:\nThe algorithm proposed in Lian et al. (2017) is called Decentralized Parallel Stochastic Gradient Descent (D-PSGD).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A well-written paper that addresses an improtant problem in distributed learning literature",
            "review": "The paper introduces a decentralized framework for adaptive momentum-based gradient descent optimizers, such as ADAM. The proposed method is novel and is among the first works to consider a decentralized communication graph without a master node. The author discovers the divergent properties of the recent work of DADAM (Nazari, 2019) and proposes a way to fix it by adding a similar consensus step for the adaptive learning rates of agents. The mathematical derivation seems to be correct to the best of my knowledge. Finally, the author tests their method on a simple CNN and show their superiority compared to DADAM to achieve a close to the centralized performance.\n\nHowever, I have some minor comments to improve the manuscript.\n1. The experimental evaluation of the work is quite limited. I understand the space limit but it would have been nice to see more experiments instead of showcasing Algorithm 2 with an extra example in Algorithm 3. It is important to see the convergence behavior of the method (on the training data) with respect to the DGD on various datasets/networks in practice, rather than observing how the testing accuracy behaves. Note that your method does not guarantee any specific generalization behavior and therefore I believe it is more suited to report the experiments only in terms of training performance when you are out of space.\n\n2. What are the drawbacks of this method? I can see more memory requirements for the agents due to the new variable \\tilde{u} for instance. Do you have any quantified evaluation in this respect? I suspect it can be significant especially if the trained model is large and the agents have limited memory/computational resources\n\n3. In Section 3.2, the author says \"Algorithm 2 can become different adaptive gradient methods by specifying r_t as different functions. E.g., when we choose ..., Algorithm 2 becomes a decentralized version of AdaGrad.\" This sentence is not accurate as algorithms like AdaGrad and Adadelta do not use momentum on the past gradients. They only use the squared values of the past gradients. I believe your method, as I mentioned above, is a general framework for momentum-based techniques including ADAM, AdaMax, NADAM, etc, which brings me to the next question.\n\n4. Is it possible to generalized your method for an adaptive gradient descent algorithm that does not use the momentum of the gradients? For example, take AdaGrad with a fixed learning rate of \\eta instead of m_t. How does your convergence behavior change? ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Everything seems sound from first glance, but I did not have a chance to carefully check the proofs",
            "review": "This paper discusses the problem of adaptive acceleration in the decentralized setting. The premise is that decentralized versions, while successful in the simple SGD setting, do not extend well in the acceleration settings, e.g. Adam and Adagrad. They first present a counterexample where a simple decentralized scheme, applying Adam, converges to a nonstationary point.  (Suggestion: I would maybe add a cleaner, more flushed out version of this proof in the appendix, maybe with illustrations.) \n\nOverall, everything the paper presented seemed reasonable. The motivation and counterexample in DADAM case are solid, and the following theorems seem to suggest gradient error norm $\\to 0$ at rate $O(1/\\sqrt{T})$ which is reasonable in nonconvex optimization.  The intuition in the adjusted merging scheme is also reasonable, and makes sense that it would work better than vanilla merging schemes. However, I did not have a chance to carefully check the proofs, which are clearly the main contribution of the paper. \n\nOne thing I would suggest is a more thorough set of numerical experiments. The two examples shown, in fact all the methods converge, and while the proposed method converges faster, it isn't really verifying the paper's main point, which is that the standard distributed methods diverge and the proposed method converges. Showing this on a standard machine learning task would improve motivation.",
            "rating": "7: Good paper, accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "This paper  is very confusing and not well-written. It  cannot be accepted.",
            "review": "This paper consider the decentralized adaptive algorithms. At the first glance,  I am really happy to that the adaptive methods are used for the decentralized optimization. However,  after I read the  main document, I do not think  paper actually    analyzes the decentralized adaptive algorithms.\n\nIn line 9 of Algorithm 1, the   denominator is $\\sqrt{\\hat{v}_{t,i}}$.\n\n However, in Algorithms 2 and 3, it is changed as $\\sqrt{u_{t,i}}$. In  the proofs, the authors proved the convergence based on $u_{t,i}\\geq \\epsilon$. This is actually the DSGD. The proofs can be quite simple.  And the restriction $\\alpha=O(\\sqrt{\\epsilon})$ can be easily removed.\nThis paper does not present any insights for the decentralized  adaptive methods. It only depends on $u_{t,i}\\geq \\epsilon$. The numerical results show that the proposed method is similar as DSGD. As mentioned before, it is actually DSGD but with slightly modification.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}