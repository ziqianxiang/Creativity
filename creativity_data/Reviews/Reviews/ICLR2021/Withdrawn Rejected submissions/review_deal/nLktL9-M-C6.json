{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Although all reviewers acknowledge that the paper has some merit, the work lacks in novelty. The idea of using normalisation techniques to reduce the domain discrepancy in UDA is well established in the DA and DG community. The theoretical analysis is an interesting first step in providing some insights on this class of approaches, but it still does not support an understanding on why some of these methods work better than others. Given all these considerations, the work is not ready for acceptance at ICLR."
    },
    "Reviews": [
        {
            "title": "Interesting work, but some points remain unclear and are not convincing.",
            "review": "--Paper summary--\n\nThe authors propose extended batch normalization, called collaborative normalization, to tackle unsupervised domain adaptation. It normalizes features using domain-specific statistics and shifts the normalized features to gradually align their distributions between domains through the model. Experimental results with several datasets validate the advantage of the proposed method.\n\n\n--Review summary--\n\nThis paper is well-written and well-organized. The proposed module should be widely applicable to various DNN architectures, and it works well in the experiments with several datasets. However, some points remain unclear or are not convincing, which makes my score a bit conservative. To appropriately determine my score, I would appreciate if the authors clarify these points in their response.\n\n\n--Details--\n\nStrength\n\n- The proposed module can be widely applied to modern DNN architectures.\n- This paper is well-written and well-organized. \n\n\nWeakness and concerns\n\n- Some points in the design of the proposed method remain unclear or are not convincing.\n\t- The definition of \\eta in Eq. (4) is intuitively unreasonable and needs more justification. When the distributions are perfectly aligned between domains, \\eta should diverge, because the denominator becomes zero.\n\t- It should be also justified to use d in the numerator. Why do not we need any other coefficient to appropriately scale the shift value?\n\t- It seems reasonable to define A^{st/ts} using m^{t/s} as shown in the end of \"With respect to the second challenge.\" However, instead of them, the authors use cosine similarities between Gs to compute A^{st/ts}, and it is not justified theoretically or empirically.\n\n- As far as I understand, the mini-batch from target data is assumed to contain all class candidates, but this assumption can be easily violated if the number of class candidates is large or the size of mini-batch is small. Does the performance of the proposed method heavily depend on the number of class candidates and the size of mini-batch?\n\n- Just after Eq. (7), the authors state \"since y^s has the highest cosine similarity with y_i^t,\" but why is it?\n\n- The existing studies on subspace-alignment based domain adaptation (e.g., [R1]) should be referred.  \n[R1] \"Unsupervised visual domain adaptation using subspace alignment,\" ICCV 2013.\n\n- Figure 2 should be further improved. For example, \\hat{y}^S should be \\tilde{y}^S, and \\tilde{y}^S should be \\overline{y}^S.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Straightforward idea with clear contribution",
            "review": "This paper proposes a novel normalization method termed Collaborative Normalization (CoN) to eliminate domain discrepancy for unsupervised domain adaptation. In practice, CoN firstly exploits domain-specific statistics to normalize features from source and target domain. Then it calculates the collaborative representations across domains and gradually aligns samples from one domain to the other by using these representations. Extensive experimental results demonstrate the effectiveness of the proposed normalization method in various unsupervised domain adaptation benchmarks.\n\n\nPros: \n+ Neat motivation; \n+ Novel methods with clear contributions;\n+ The writing of this paper is excellent;\n+ Extensive experiments and good performance.\n\nCons:\n\nI would like to hear more about the implementation details. \n- Are all of the models fine-tuned from ImageNet? If yes, is it stable to replace CoN from BN in the training process?\n\n\n- Do you replace all of the BN layers in ResNet50? Is it reasonable to apply global pooling for low-level features that haven’t captured enough semantic information?\n\n\n- The authors aim to estimate the location of the source (target) features in the target (source) domain. In practice, the Collaborative Translation is realized across the mini-batch in each iteration. Is the proposed method sensitive to the mini-batch size and the distribution of the categories in a mini-batch, especially compared with TN? How about the performance of the proposed method and TN by using a smaller batch size?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A review for collaborative normalization for unsupervised domain adaptation",
            "review": "**Summarize what the paper claims to contribute.**\n \nThe paper proposes a normalization technique called collaborative normalization, which can substitute batch normalization in any neural network and achieve better performance on domain adaptation tasks than the previous works. The proposed method introduces an additional domain attention, collaborative translation, which calibrates the output of domain-specific batchNorm to exploit common domain information by exploiting cross-domain projection error.\nThe author gives an geometrical interpretation of the proposed method and it is the first trial to analyze a normalization in domain adaptation in a theoretical manner.\nThe performance of the proposed method is evaluated on the several domain adaptation benchmarks and it achieves the best performance compared to its counterpart techniques.\n \n**List strong and weak points of the paper.**\n \n*strong points*\n \nThe proposed method introduces collaborative normalization a closed-form domain attention to calibrate the latent features in the batch normalization. It boosts performance without additional learning parameters.\n \nFor my best understading, it is the very first paper trying theoretical analysis on normalization for domain adaptation, while the other works [R1,R2,R3,R4, AdaBN, TransNorm] only show empirical performance.\n \n*weak points*\n \nThe proposed method adds an incremental contribution compared to TransNorm (Wang et al. (2019a)). The versatility of the proposed method is limited to only two domain cases, although domain-specific BN is applicable to multi-source domain adaptation.\n \nAs my understanding, the proof is not directly related with domain discrepancy. and the proof don’t compare the proposed method with the other methods, BatchNorm, TransNorm.\n \n**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice**\n \nI give “Ok but not good enough - rejection (4)” to this paper. The paper is well-organized and it firstly tries to give the proof on normalization for domain adaptation. But, the proposed method is incremental to the previous work [TransNorm]. I think the proof is not in a complete form. I write the questions on the proof in below section.\n \n**Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.**\n \nI have questions on collaborative normalization because I can’t fully understand the description in <section 3.2 collaborative normalization>. Thank you in advance.\n \nFirst, in the end of the section 3.2, you mentioned that “effectively eliminates the cross-domain discrepancy”. I have a confusion because domain descrepancy usually is usually formated as a kind of distance function between source and target distribution. (1) Could you explain how the sample-specific error term leads to the entire domain discrepancy?\n \nSecond, could you give an inquality that CoN achieves better approximated error (or domain discrepancy) than TransNorm and AdaBN in terms of approximated error $\\tilde{\\epsilon_i}$ or related terms?\n \nThird, a distribution on latent features can not be fully represented by mean and standard deviation of batch normalization. If the underlying distribution on latent feature is Gaussian distribution, then it can be represented as mean and covariance. In this reason, I think that the given proof is a kind of very good draft for future proof.\n \n**Provide additional feedback with the aim to improve the paper.** \n \nSeveral related works for domain adaptation with batch normalization are missing. Although I know that there are too many relevant works for domain adaptation, this work misses too many works on normalization-inspired works for domain adaptation. The works [R1, R2] can be an alternative for AdaBN, TransNorm.\n \nThere are works on combining several normalizations [R3, R4] to achieve better performance on several tasks even for domain adaptation [R3]. What do you think about those works? and how they are related to your work?\n \n[R1] Domain-Specific Batch Normalization for Unsupervised Domain Adaptation, CVPR 19\n \n[R2] Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss, CVPR 19\n \n[R3] Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks, NeurIPS 18\n \n[R4] Switchable Whitening for Deep Representation Learning, ICCV 19\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}