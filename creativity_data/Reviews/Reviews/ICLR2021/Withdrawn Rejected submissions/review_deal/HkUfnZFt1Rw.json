{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies various graph measures in depth.  The paper was reviewed by three expert reviewers who complemented the ease of understanding because of clear writing. But they also expressed concerns for limited novelty, theoretical justification, and unrealistic setting. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers."
    },
    "Reviews": [
        {
            "title": "Unrealistic setting",
            "review": "This paper tests performance of different node similarity measures when used in K-means for clustering LFR graphs. It provides recommendations for which measure is more appropriate for different parameter spaces. \n\nIt is easy to read paper and well organized. A rich set of graph similarity measures is studied. \nThere are however critical issues with the design of the experiments. First, the space of parameters considered is very unrealistic and hence most experiments and recommendations are not applicable. This makes, for example, most of the space in figure 6, irrelevant. For instance the exponent of the degree distribution is between 2 and 3 for most real world networks. This goes up to 100 in the experiments. Where did you use the transformed [0,1] version?\nThe average degree is much smaller whereas graphs are usually much larger than the setting here. A negative modularity is result of poor parameter choices, and generally, signifies there is no cluster structure in the graph. Generally, given this is focused on studying the space of LFR graphs, one expects more careful understanding of what there parameters mean. On a minor point, LFR is not using preferential attachment mechanism and is achieving power-law degree distribution by directly sampling degrees and using configuration model. \nI suggest fixing the parameter settings and making sure the modularity is at least 0.1 for all the graphs. The common link prediction measures, e.g. number of common neighbours, could also be added as simple baselines, as well as more recent embedding based models where k-means could be applied directly in the embedded space. Another possibility is to also include another clustering measure, say a density based one. The motivation needs to be expanded. What justifies going n square when a graph based clustering could do the same job? Also in the results, using ranking is good, but the actual numbers are also meaningful. It is hard to see if these methods are recovering anything meaningful when only the relative performance is reported. Basically, if the ARI is too small, which means no correlation between results and the ground-truth, you can still have a ranking of all the close to zero numbers. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting empirical analysis of different graph measures for graph clustering. The choice of LFR though limits the generalization of the observations. Also, the proposed approach lacks theoretical justification.",
            "review": "The paper deals with the problem of community detection on graphs, examining the impact of graph measures. To do so, the paper proposes an experimental framework where clustering is achieved using the kernel k-means algorithm, and the performance of graph measures is examined on various instances of artificially generated graphs using the LFR benchmark. The overall approach is empirical, supported mainly by the experimental results. The main observations concern the consistent behavior of particular graph measures across multiple settings of the dataset.\n\nStrong points:\n\n-- The paper addresses an important problem in network analysis, which also concerns practitioners in a wide range of disciplines.\n\n-- Various graph measures are considered in the evaluation. This is very interesting, since, in my view, some of them are not very well-known among the graph clustering / community detection communities.\n\n-- The paper is well-structured and well-written. Most of the concepts, including the experimental framework, are clearly presented.\n\n\nWeak points:\n\n--- My main concern about the paper has to do with the consistency of the proposed evaluation framework under different evaluation criteria and graph data beyond the LFR benchmark. Firstly, as the paper also mentions, focusing only on LFR graphs can definitely reveal important properties of algorithms, but limits the generalization of the observations in the case where other generators might be used (e.g., SBM) or even real-world graphs. Besides, the argument made in the paper that the LFR benchmark generates graphs similar to real-world ones is not very accurate. LFR focuses on the clustering structure as well as on the degree distribution but might miss other key properties including graph diameter or the number of triangles (or the clustering coefficient). Is there any evidence that could support the observations of the paper in the case of real graphs? \n\n--- A closely related point has to do with the observation that real-world graphs do not have a clear clustering structure, i.e., well-defined cuts. Focusing only LFR graphs might not be enough to capture such instances. \n\n--- The modularity criterion is used to evaluate the quality of communities, which overall is a widely used criterion. Nevertheless, modularity has been shown to be prawn to the particular structure of the communities (e.g., resolution limit). How is this taken into account in the evaluation of the communities?\n\n--- Another point is that the paper is purely empirical. Definitely, this is a good starting point, especially when the focus is on experimental settings that have not been used before. Nevertheless, despite the interesting observations, I would expect to have a (theoretical) justification or some reasoning of why SCCT performs well on LFR graphs, or for instance why PPR which is a widely used measure, shows poor behavior.\n\n--- In the paper, all graph measures are considered as kernels. How valid is this argument? Why not just using the pure k-means for graph measures which are not kernels? \n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper empirically evaluate the distance measure between nodes in a graph under the setting of kernel k-means and LFR data generation.",
            "review": "\n1. No good reasons to choose settings of evaluation, particularly kernel k-means and LFR. \n\n2. I think this paper may think about something very obvious. The setting is kernel k-means, and if the similarity measure is given as some kernel, it might be more clearly shown what kernel is good under what condition, under kernel k-means.  In other words, we may find some connection between a kernel and the condition of data generation under the kernel k-means already before doing some experiments.  I think this type of investigation is missing in this paper.\n\n3. So what is the reason why SCCT is the best and/or why highly-ranked methods are so? I think that would be simply connected to the scheme of data generation of LFR or another feature in generating data or noise. I think this point might be obvious but might become some good contribution, while just data generation and comparison would not be something people can say contribution.\n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An interesting yet limited comparative review",
            "review": "Using 7500 LFR-generated graphs as a benchmarks suite, the authors compare 25 graph clustering measures, determining the best measure for every area of the parameter space. The paper is well written, mathematically sound and interesting, and definitely useful to the graph theory community. However, as acknowledged by the authors, the study is limited by the structure of the benchmark suites, which is restricted to networks that can be generated by LFR rules. Overall, I rate it as a weak accept.\n\nPros:\n- the analysis is clear and grounded, with a sufficient level of mathematical details\n- the authors point out a clear winner out of the set of compared metrics\n\nCons:\n- the amount of novelty in the manuscript is limited \n- the benchmark suite is very specific, and no real world example (that would have added great value to the submission) is provided\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}