{
    "Decision": "",
    "Reviews": [
        {
            "title": "official review by AnonReviewer4 ",
            "review": "\n\nIn this paper, the authors investigate two temporal attention modules, including pre-hoc modules and post-hoc modules. This modules can be plugged into traditional memory–augmented recurrent neural networks to improve their performance in QA task.\n\nThe usage of temporal attention modules adds interpretability attributes absent in the original model, by calculating the attention weights over all memory blocks from different facts. Besides, this work shows that the temporal attention modules to a recurrent model with external memory increases performance in QA tasks.\n\nHowever, I think the paper is still limited for the following reasons:\n\n1 The authors should present a brief description of  the Entity Network in section 3, so that the readers clear the difference and relation between the original operations in Entity Network and their proposed model.  \n\n2  The “ans” used in the post-hoc temporal attention module is predicted by the original Entity Network. I suggest the authors describe the overall framework assimilating the Entity Network.\n\n3   Although equation 13 lists the complete loss function of the proposed models, I think it is more helpful for the authors to depict the loss function L_sf(D, Θ) associated with supporting facts in detail.  \n\n4 While the authors compare their model with the Entity Network using the  bAbI dataset, they ignore other related memory models.  Besides, I am confused why the experiment results of base Entity Network shown in the submitted version are different  from the original paper.  Moreover, I think the analysis about the experimental results is limited, including the integration of the temporal attention modules into different memory architectures or efficiency analysis of the temporal memory modules in different QA tasks (such as Yes/No questions) . \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Some interesting results",
            "review": "The paper proposes to extend the existing memory network with a self attention mechanism. The self attention mechanism produces a weighted sum of memory blocks across different time steps. The weights are computed through a two-level hierarchical softmax function, named as intra- and inter-temporal attention. Experiment shows that the extended model outperforms the original Entity Network, that the proposed model is built upon.\n\nThe idea is straightforward, and results seem promising. \nMy major concern is about the novelty of this paper. The so-called temporal attention modules are not substantially different from other self-attention techniques. While the author claims that interpretability is one of their major advantages, I didn’t find enough proof, except the two examples given in Fig.4, to support their claim. The Fig.4b also seems to indicate that the model didn’t attend to the sentence that supports the model’s prediction “bedroom”.\n\nMinor points:\nIn the experiment section, the author didn’t compare the model with other sota models on the babi task. \nThe equations in section 3 are difficult to read.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes to use temporal attention modules in memory-augmented neural networks to improve its interpretability. The general idea of this paper makes sense to me, but it has the following weaknesses:\n1) the paper claims that the module is a plug-and-play module to any attention-augmented recurrent models, but the following experiments are only verified on a specific implementation called Entity Network. I think the author needs to plus their modules to a more broader set of attention-based networks. Is your module applicable to the widely used models like Transformer, BERT, ROBERTa, etc? I would be eager to see your results on 'improving interpretability for BERT prediction'.\n2) the proposed method lacks novelty, it seems it's just a 2-D attention matrix (global-local attention). I think such kind of strategies has been exploited in dialog or other long-sequence NLP tasks. At least it's not new to me.\n3) the paper abstract claims that the model can \"improve performance on natural language processing tasks.\", however, they only experiment on the bAbI diagnostic tasks. The bAbI datasets are mainly synthetic, which has a huge divergence with real-world natural language datasets. I agree that its performance can be used for some preliminary study, but the whole paper doesn't go further than that. There are plenty of NLP tasks (QA) with supporting evidence, like Natural Questions (https://ai.google.com/research/NaturalQuestions/), its long answer can be used as the supporting fact in your case. It would be great to see how your model performs on that dataset. In order to verify the generalization of your model, I would suggest adding more NLP experiments on tasks like Fact-Checking (https://arxiv.org/abs/1803.05355, it also has annotated supporting evidence), entity linking (KILT, https://github.com/facebookresearch/KILT). \n4) the paper reports that Entity Network has an error rate of 29.6% in Table 1, but the original paper reports is 0.5%. Why is there such an inconsistency, am I missing something? \n5) In general, I would like to see more convincing results over different backbone models and realistic NLP tasks to accept this paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear on benefit of proposed approach given previous work",
            "review": "This paper proposes to augment Entity Networks (EntNet) with attention. The model is trained with and without strong supervision (relevant fact annotations). Experiments are conducted on the bABI dataset, where the method is shown to outperform EntNets.\n\n### Strengths\n\n1. The paper demonstrates attention can improve EntNets\n\n### Weaknesses\n\n1. The primary benefit of EntNets over something like End-to-End Memory Network [1], is that the model does not need to store all facts in memory at prediction time. Adding attention as done here negates this advantage. Furthermore, the performance reported is worse than previously proposed models which attend to all facts. For example, the best result from [1] in the bAbI 1K setting achieves 12.4% average error without strong supervision. This is significantly better than even the strongly supervised results presented here. As such, I am unsure under what scenarios the model here would be preferred.\n2. The technical novelty is very low. The incorporation of attention in recurrent architectures in the context of the bAbI dataset has previously been explored with strong supervision in [2], and with weak supervision in [3].\n3. Experiments are limited.\n4. I don't believe the interpretation of the MemNN SS presented in Appendix D is correct. That model contains a component that is explicitly trained to predict the correct supporting facts, but it is not supplied with supporting fact labels at test time. As such, the MemNN SS result is comparable to the strongly supervised results in the main paper, not the version in the Appendix. See Appendix A.2.1 of [4] for more information.\n5. Methods are not clearly described.\n   1. The inputs to the loss functions are omitted. I assume the binary cross entropy fact annotations supervision mentioned in Section 4 is applied to the inter-temporal attention values, however this isn't explicitly stated. I also find it odd the paper uses a softmax to normalize the inter-temporal attention scores given that answers have multiple supporting facts.\n   2. In the appendix the 1k and 10k bAbI variants are mentioned, however I couldn't find anything stating what version was actually used in the paper.\n6. The paper talks about improved inductive biases in several places, but doesn't actually state what inductive bias the proposed modification is intended to impart.\n7. In Table 1, the label \"Accuracy Error\" doesn't make sense.\n\n### Recommendation\n\nI recommended rejection. The paper has issues with methodology, clarity, understanding of previous literature, and is not ready for publication.\n\n### Other Issues\n\nThe paper contains a number of grammar mistakes. Some I noted are below.\n\n- The sentence \"This work successfully shows that the temporal attention modules to a recurrent model with external\n  memory increases performance...\" doesn't make sense.\n- There is punctuation missing in \"to supervise the QA task On the other hand, a loss function\"\n\n### References\n\n1. Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"End-to-end memory networks.\" *Advances in neural information processing systems*. 2015.\n2. Kumar, Ankit, et al. \"Ask me anything: Dynamic memory networks for natural language processing.\" International conference on machine learning. 2016.\n3. Xiong, Caiming, Stephen Merity, and Richard Socher. \"Dynamic memory networks for visual and textual question answering.\" International conference on machine learning. 2016.\n4. Weston, Jason, et al. \"Towards ai-complete question answering: A set of prerequisite toy tasks.\" arXiv preprint arXiv:1502.05698 (2015).",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}