{
    "Decision": "",
    "Reviews": [
        {
            "title": "Over-claimed statements with low clarity and weak theoretical results",
            "review": "Algorithm 1 and 2 are not in arbitrary sampling setup as claimed. In this paper, a classical uniform sampling is relaxed to nonuniform sampling allowing different probabilities for each node $i$. This setup is far less general than arbitrary sampling setup. The usage of Arbitrary Sampling in this paper including the title is not justified and misleading. I think, Non-uniform Sampling might be more appropriate.\n\nThe clarity of the overall writing is very low. For instance, distributed variant of SVRG is not motivated well and is not presented with enough details. The first two paragraph of section 3.1 explaining Algorithm 1 and the description of Algorithm 1 itself are confusing. There is no mention on what $\\tilde{w}_k$ is and how it initialized. How distributed setup works in Algorithms 1 and 2, i.e. what masters does and what compute nodes do ? when and what information is communicated ? Which side samples variables $\\zeta_k, \\xi_{k,t-1}$ ? Why there is a summation in line 8 of Algorithm 1 with only a single summation term ? Although Algorithm 1 is designed to be distributed, there is no step when more than one node are in communication/computation process at the same time.\n\nIs the Lemma 1 a new result or something known ? There is no proof of it and no citation around the statement. At the current form it looks more like a definition of expected smoothness.\n\nLemma 2 and Proposition 1,2 are more in an exercise nature. It was almost trivial that in the presence of works on SVRG/SAGA under different settings, the behaviour depends on the average smoothness not the maximum. Besides, the proof of Lemma 2 uses convexity of each function $f_i$ (the first inequality of the proof). However, neither in the statement of the lemma nor in the Assumption 1 convexity of $f_i$ is not assumed.\n\nIt is strange that the solution (4), discussing minimum-cost variant, depends only on the single index of the smallest cost. What if there are multiple nodes with the same minimum cost ? Why probabilities in (4) do not depend on the magnitude of the cost variables $c_i$ ? So, if the costs of all nodes but one are increased by any factor, then the minimum-cost variant is not changed at all as the sampling probabilities are the same.\n\nIn the appendix A, new variables $v_{k,t}$ are introduced to be the same as $w_{k,t}$ with the reason that it simplifies some parts of the proof. What parts of the proofs are simplified and in what extent ?\n\nIn the proofs, the standard notation $\\mathbb{E}[\\cdot | \\xi]$ of conditional expectation is used as expectation with respect to $\\xi$.\n\nIt is hard to gauge the novelty of the paper due to the clarity issues. It has some ideas not properly developed and explained. In its current form, the paper looks a slight modification of the original paper of SVRG from 2013. Thorough revision is needed in terms of notations, terminology, explanations and comparisons.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A solid paper, contributions can be summarized as a rather trivial post-analysis of SVRG based on some environmental factors only. ",
            "review": "\nSummary: \nThe paper considers a finite sum minimization problem using the SVRG algorithm. Given a cost $c_i$ of computing a gradient of the $i$-th component of the finite sum, the paper proposes an importance sampling strategy for SVRG that minimizes the expected cost to run the inner loop of SVRG. Next, it is argued that the mentioned constants $c_i$ can efficiently model the stragglers in the distributed optimization. Next, the SVRG algorithm is tuned for when applied in the distributed scenario with wireless congestion, and a novel optimal minibatch size is derived. \n\nOverall, I believe that the paper's contribution is not substantial enough for acceptance. I find the contributions to be more appropriate for a workshop paper rather than ICLR. \n\n\nMain issues: \n\n1) It is not clear to me why the problem (3a)-(3d) makes sense in the first place. Specifically, why would one want to minimize the cost of performing an iteration of SVRG+ with a constraint that the rate should be better than the rate of vanilla SVRG? Wouldn't it be better to minimize the cost of one iteration multiplied by the number of iterations required to get sufficiently close to the optimum? In such a case, I would expect the optimal probabilities to be proportional to $\\frac{L_i}{c_i}$, i.e., $p_i \\propto\\frac{L_i}{c_i}$.\n\n2) Congestion in wireless communication: I believe this section requires a significant rewriting. I find it impossible to understand without looking at supplementary and hard to understand even with the supplementary. Please say in the main body of the paper what $r_1, r_0$ are and explain. \n\n3) {\\bf New results for minibatch SVRG with arbitrary/importance sampling}. Unfortunately, the results on the rates of SVRG that are provided in this paper (entire Section 3) are not new; and even more, general/better results were already provided. Specifically, a variant (with a random inner loop length that follows a geometric distribution [1]) of SVRG -- LSVRG --  with importance sampling that allows for minibatching was already proposed in [2] (note that [2] allows for the acceleration too). While these results hold for LSVRG only, it is easy to derive a similar result for the classical SVRG; see Section A.9 of [3], for example. Next, note that one can analyze SVRG under a more general matrix smoothness assumption that appears naturally in many different scenarios such as logistic regression or dual SVM's. In such a case, one can get a tighter rate for the minibatching than what was proposed here [4]. Note that the aforementioned papers discuss the (strongly) convex case only similar (same setup as the one in this paper); while the literature on the non-convex setup is vast too. To summarize, I see the results of Section 3 only as non-significant advancement over prior work. \n\n\nContributions: \n\nI find that this work's only solid contribution can be seen as a post-analysis of SVRG based on some environmental factors. These factors include the cost of evaluating and receiving the gradient of the $i$-th summand of the objective for the problem with stragglers or the wireless channel communication protocol constants $r_0, r_1$ for the wireless congestion problem. However, both of these can be seen as a rather straightforward problem formulations with a very simple solution, given that a rate of SVRG is provided. Therefore, I see the contributions just as a minor enhancement of SVRG theory. \n\nWriting: The paper is well written and easy to follow in most of the parts. The results are transparent and easy to understand. The paper is neither hiding flaws nor over-claiming the results. Overall, I am very satisfied with the writing. \n\nCorrectness: I did a high-level check and all results seem correct to me. \n\nLiterature: Related literature is mostly well covered. \n\nExperiments: I find the experiments to be well designed to demonstrate the strengths of the proposed approaches.  \n\n\n[1] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien and Brian McWilliams. \"Variance reduced stochastic gradient descent with neighbors.\" Advances in Neural Information Processing Systems. 2015.\n\n[2] Qian, Xun, Zheng Qu, and Peter Richtárik. \"L-SVRG and L-Katyusha with arbitrary sampling.\" arXiv preprint arXiv:1906.01481 (2019).\n\n[3] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. \"A unified theory of sgd: Variance reduction, sampling, quantization and coordinate descent.\" International Conference on Artificial Intelligence and Statistics. 2020.\n\n[4] Filip Hanzely, and Peter Richtárik. \"One method to rule them all: variance reduction for data, parameters and many new methods.\" arXiv preprint arXiv:1905.11266 (2019).\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting new take on optimizing the cost of SVRG optimization; simplistic framework of analysis and toy results.",
            "review": "The paper introduces an interesting new angle of analysis for SVRG optimisation: in real applications, what really matters might not the amount of bits sent or the total amount of iterations, but rather local power consumption or bandwidth. These issues will become more and more relevant as ML loads are increasingly carried out in a decentralized and distributed fashion. \n\nOriginality\nThe new angle on optimization analysis is (to the best of my knowledge) original, as most practitioners concentrate mainly on the iteration count, and have only turned recently to taking the amount of transmitted bits into account. The paper aims at going even further and providing theory that is closer to the real use cases. This direction is both promising and increasingly relevant, and is one of the strong points of the paper.\n\nClarity\nOverall the paper is well-written and not hard to follow. There are however a couple of key points that do suffer from a lack of clarity, and are quite damaging to the overall contributions.\nFirst, while the new analysis angle is explicitly targeted at distributed optimization, the framework of analysis is actually a functional equivalent of the sequential case. Critically, there is no allowing for delayed updates, which means that all updates are inherently sequential, and parallel workers do not effectively work in parallel. This is partially redeemed by the mini-batch analysis, but in the main use cases where this interesting new type of theory would apply, one would hope for some kind of way of dealing with asynchrony. The reader is confused at first by the choice of framework of analysis, and in the end the results are less convincing because of this choice.\n\nAnother important point is the claim that the method is roughly twice as fast as vanilla SVRG, because instead of sampling a random iterate in a given epoch at the end of the epoch, the iterate is sampled beforehand and the epoch is stopped at this stage. This seems to be one important selling point of the new method. However, this is not actually what happens in practice. SVRG users typically do not sample a random iterate at each epoch, rather, this trick is an artefact of the analysis. This mostly invalidates the improvement, even though from an analysis standpoint it makes sense.\n\nSignificance\nAs the framework of analysis is limited to the sequential, synchronous case, the results are very incremental on top of arbitrary sampling. Going from the maximum Lipschitz constant to its average is, as noted by the authors, a classical results for this type of algorithms. Again, the iterate sampling trick is not relevant for actual applications, which very significantly reduces its significance too.\nFinally, the experiments are not actually distributed, and are applied on a toy dataset where vanilla SVRG would be fine. To make a convincing case for the method, actual distributed experiments would make a much stronger case for the proposed method.\n\nQuality\nWhile the paper is well-written overall, the couple of major imprecisions we've underlined bring down both the relevance and the general quality of the paper, unfortunately.\n\nConclusion\nThe new analysis direction aims to be closer to practical applications and is (as far as I am aware of) the first approach of this sort. \nUnfortunately, the framework of analysis is inherently sequential and thus negates most of the benefits of the new approach, making it unapplicable to precisely the use cases it targets. As a result the contributions do not match the initial promise of the paper. Thus, this version feels too early to clear the bar for publication at this stage.\nI want to stress that this research direction is very relevant, and encourage the authors to try to improve the paper by taking the following advice into consideration:\n- add theory to take asynchrony into account, thus avoiding unused worker computations\n- run actual distributed experiment to showcase the advantages of the method.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Results are not significant enough",
            "review": "This paper studies the distributed learning setting where each worker is associated with a cost to stand for, e.g., computation and communication speed. To deal with such a setting, this paper proposes cost-efficient SVRG with arbitrary sampling (SVRG-AS+). Convergence analysis is proposed for this method. And it is theoretically studied about how to choose the sampling distribution to minimize the cost of optimization. Finally, some experiments are conducted to show the superiority of the proposed method over SVRG-AS.\n\nFirst, the algorithms and analysis of this paper are almost the same as the existing method, namely, SVRG with arbitrary sampling. The only difference that I can observe is the introduction of the costs, where each worker is associated with computation or communication cost, and the sampling probability is determined according to the cost. To me, it seems that there is no new algorithm in this paper, only a new way to do sampling. So I feel the results are not significant enough.\n\nBesides, the experiment results are also not convincing enough. Though there is some reduction in the cost, the difference is somehow too small compared to the existing method. \n\nFinally, though the mini-batch version is proposed, it lacks theoretical analysis. Many existing works about SVRG has suggested the choices of some hyper-parameters (e.g., step size) are heavily related to the batch size. I doubt if the optimal sampling distribution also depends on the batch size. This should be studied since the mini-batched algorithm is more commonly used in practice.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited Technical Novelty and Unsatisfactory Experiments",
            "review": "In this paper, the authors proposed a SVRG based algorithm with arbitrary data sampling for distributed optimization over networks. Compared to the conventional SVRG method with the uniform sampling, the authors show that the proposed SVRG-AS+ can achieve better efficiency when the sampling distribution is chosen properly in terms of the smoothness parameters of each individual loss function. A mini-batch version of SVRG-AS+ is also considered. In addition, the authors propose a minimum-cost SVRG-AS+ by choosing its sampling distributions as close-form solutions of a constrained problem. The experiments show that the proposed algorithms can achieve similar convergence behaviors as conventional SVRG and SARAH, but with a lower cost (in terms of network utility).\n\nStrength: \n\n(1) The proposed algorithms exploit the possibility of non-uniform data sampling in the distributed optimization over networks and try to figure out the optimal choices of sampling distributions under different settings.  \n\n(2) The authors show that the proposed algorithms can achieve an improved network utility without scarifying the convergence rate.\n\nWeakness:\n\n(1) My biggest concern about this paper is the novelty of the proposed algorithms. Based on my reading, the proposed SVRG based algorithm with arbitrary sampling uses the almost same idea from Horváth & Richtarik, 2019, as well as the technical proofs (of course with minor adaptations to distributed settings). Although Horváth & Richtarik, 2019 considered the nonconvex setting and this paper considered the strongly-convex setting, the key step to handle the non-uniform distribution should not be that different. Furthermore, the extension to the mini-batch case is also standard, because there are already extensive studies on such a topic.\n\n(2) My another concern is that the proposed algorithm heavily depend on the non-uniform sampling. However, based on their results, the chosen sampling distribution depends on the smoothness parameters of loss functions. In the experiments, the authors estimate such parameters based on some easy-to-obtain bounds on simple logistic regression objective function. However, estimating such smoothness parameters for more complicated objective function (e.g., with deep neural networks) can introduce an extra computation burden or may be infeasible in practice. For this reason, I suggest that the authors can run more experiments with more complicated objective function. \n\n(3) The experiments are not convincing enough.  First, the authors only compare the proposed algorithms to other variance-reduced methods such as SVRG and SARAH. They should also provide a comparison to SGD or ADAM types of methods  if possible. Second, the authors should provide more details of the competing algorithms, i.e., SVRG and SARAH in the experiments (e.g., whether such methods also use a mini-batch sampling for the inner loop? How they choose the hyper-parameters like stepsize, batch size, etc?)  \n\n(4) Some references may be missed. Since this paper focuses on variance-reduced methods, some related papers on SPIRDER-type and SARAH-type methods should be mentioned. I provide some as follows. \n\n     1) SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator.\n     2) SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient.\n     3) SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms.\n\n(5) One minor comment:\nLine 7 of Algorithm 1 on page 4:  whether g_{\\xi}(w)/Np_\\xi  is g_{\\xi}(w)/(Np_\\xi) or (g_{\\xi}(w)/N)p_\\xi? Based on my reading, it should  be (g_{\\xi}(w)/N)p_\\xi? If so, I suggest the authors can write it as p_\\xi  g_{\\xi}(w)/N.\n\nIn summary, I am not convinced by the results in this paper, and tend to reject the current version due to the limited technical novelty and the unsatisfactory experiments. However, I am open to change my mind based on the response and other reviewers’ comments.  \n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}