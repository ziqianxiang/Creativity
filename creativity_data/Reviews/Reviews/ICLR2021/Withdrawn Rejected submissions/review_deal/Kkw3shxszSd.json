{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper tests out some straightforward data augmentation strategies on the protein inputs to the transformer used in the TAPE paper.  Overall, there is insufficient intellectual merit to warrant publication at ICLR. As a side-note, the quality of the manuscript in terms of scholarliness of presentation was overall lacking."
    },
    "Reviews": [
        {
            "title": "Strong paper that advances data augmentation regime for protein sequence data",
            "review": "Summary:\nThe paper explores the impact of different types of data augmentations for protein sequence data, and does a thorough benchmark analysis on them. The authors used a pre-trained transformer model, fine tuned the model on augmented data using two approaches, namely, contrastive learning and masked token prediction. This finetuned model was evaluated with an added linear layer on a range of tasks.\n\nOverall, I vote for strong acceptance as I am genuinely excited about the paper. My reasons are below-\n\nPros:\n1. There are no known good augmentations for protein sequences. This is probably the first paper that tries a wide range of data augmentations for protein sequences, and tests these augmentations on a wide range of benchmarked tasks.\n2. The design of the data augmentations is creative. Specially, random dictionary and random alanine replacement augmentations seem to improve the downstream performances quite consistently. \n3. The paper has done comprehensive experiments showing most possible configurations of the augmentations and their combination with contrastive learning and mask token prediction. \n\nThe paper is so comprehensive in its experiments in the scope of a conference that I can’t really think about any strong cons. I would just ask the authors if there are any plans to bring these augmentations to a library so that the community can start trying them rather than implementing the augmentations themselves? Thanks for this wonderful paper.\n\n\nUPDATE on 12/7/2020:\nI read the other reviews and the authors’ replies.\nI hold onto my score and recommend the paper to be accepted. Here are some points I would like to highlight.\n\n1. Anonreviewer1 suggests the work is incremental, yet there’s no citation to previous work on protein data augmentation. This is in fact the first major work tackling this issue.\n2. Anonreviewer1 points out the authors did not use a SOTA model. This is true, but the model they are using is little more than 1 year old. Moreover, while I agree with Anonreviewer1 that different models need to be used to see how generalizable the data augmentations are, I don’t see this as a reason to reject a paper that is tackling an orthogonal issue of what modes of protein data augmentations work.\n3. Both anonreviewer1 and anonreviewer3 are pointing out that different augmentations seem to have worked best for different tasks, and hence the results are not strong or useful. But how would we know this if no one had even tried it? The authors found the results as such and that’s an important contribution to the community's knowledge.\n4. I am not seeing the problem of data augmentation done on the validation set that anonreviewer1 is suggesting. The test set results should not be affected in a corrupt way in this case.\n5. I disagree with anonreviewer3’s comment that the augmentations are with unreasonable intuitions. I don’t think that’s an objective reason to reject a paper.\n6. I agree with other reviewers that the contact prediction task should have been evaluated with whatever dataset that was available. Even then, the other 4 tasks in my opinion are enough as a first attempt at this area.\n\nOverall, investigating the protein data augmentation regime is something that is not fancy like building a new model or beating the latest SOTA yet a necessary task for which we should thank the authors. There are of course problems that will indeed need to be addressed in further work but there are also important insights in the paper.\n\n\n\n\n\n\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental; lack of rigor ",
            "review": "A suite of data augmentations is presented for improving protein language models.\n\nStrengths\n- Data augmentation and contrastive models are underexplored areas in protein language models. The authors are able to improve upon the TAPE baseline. It would be interesting to see if these data augmentations could be used on recent models (ESM-1 and ProTrans) to achieve a new SOTA.\n\nWeaknesses\n- All evaluation was performed on the TAPE model, which was released >1 year ago. Since then, there have multiple models released that have much better performance (ProTrans from Elnaggar, et al.; ESM-1 from Rives, et al.).\n- Related work is incomplete. For example, it says SimCLR is the current SOTA contrastive learning technique. There is already SimCLR-v2 that should be cited, among others. Even though the authors use the models from Rao, et al. 2019, the authors should also cite Alley, et al. 2019, Heinzinger et al. 2019, and Rives, et al. 2019 which were first to introduce semi-supervised learning for protein language models.\n- The data augmentation is done on the validation set. This means there is no longer a validation set! Their model performs better than the ones presented by Rao, et al. but that could be because additional data was incorporated here. The authors include a baseline where they continued training Rao, et al. on the validation set, but it would be better practice to augment data from the training set.\n- The authors do not evaluate on contact prediction because CASP12 \"has an incomplete test set due to data embargoes.\" However, CASP12 ended 4 years ago and the data is now available. In fact, the authors use CASP12 for secondary structure prediction, so clearly they have access to the data. Furthermore, the authors could consider additional structural sets, such as structural holdout at family level; temporal holdout to CASP12/CASP13; other at the very least a sequence based split.\n- The data shuffling will be predominantly shifted to the right because beta = min(N, alpha + 50).\n\nAdditional\n- In the abstract, the authors refer to fine-tuning semi-supervised protein models. This is not very specific, as the authors actually continue the pre-training procedure with the data augmentations. \n- For the replacement augmentation, why do you randomly replace with a single amino acid instead of rotating between, e.g. all hydroxyl or all cyclic amino acids?\n- Specify that secondary structure is \"3-class\" \n- On your table, you should clearly write the three test sets used for remote homology. \n- The authors may want to see \"Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization\" by Lu, et al. which came out around the time of the ICLR deadline.\n- It would be helpful to summarize on a table which data augmentations worked best for each method\n\nOverall, this is an underexplored area, but is incremental to previous work. If the authors had shown better results, I would have increased my rating. However, the lack of rigor, novelty, nor impressive results means this paper does not meet the high bar for an impactful paper at ICLR.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing key components",
            "review": "The authors suggest that training self-supervised protein sequence models with string manipulations as data augmentations will lead to better downstream performance.\nThey study several different types of data augmentations, (1) dictionary replacement, (2) global/local random shuffling, (3) sequence reversion and subsampling, and (4) a combination thereof.\nThe authors show promising results, where contrastive training improves benchmark results on 3 of the four tasks proposed in TAPE.\nAdditionally, data augmentations show better results on all tasks, beating the TAPE baseline significantly.\n\nI find it exceptionally interesting that sequence reversal has such a negative impact on pretraining.\nI would never have expected the order of protein sequence has such a large effect on downstream tasks.\nAdditionally, this is a very good line of work to pursue, since contrastive methods are known to work on images, but have not shown to beat MLM on NLP tasks.\nIt is interesting to find out whether they do well on protein language modeling tasks.\n\nHowever, there are a few glaring weaknesses in the paper.\n\n(1)\nThe authors do not even attempt to test on contact prediction, citing a data embargo.\nIn fact, many of the CASP12 structures are available, and are tested on in various papers, including the TAPE baseline, trRosetta, etc.\nI believe contact prediction is a main problem to show gains on out of the 5 TAPE tasks, and without showing it in some form, this paper should not be accepted. (See CASP competitions)\n\n(2)\nThe results are reported to be best across all data augmentations taking the best per task.\nThis seems unfair, as the representations learned should not depend on the downstream tasks.\nSince each augmentation seem to have extremely different effects on each task, I would not say that on the whole data augmentation is actually useful in building a protein representation.\nFor example, checking the best model for stability, RD(0.01), I find that this augmentation only does better than the baseline on fluorescence and RH@family.\n\n(3)\nI have a few worries about the augmentation strategies used.\n- For dictionary replacement, a much more natural choice is to follow probabilities outlined in BLOSUM or PAM substitution matrices.\n  I believe this would be a stronger result than Alanine substitution, though the hand designed replacement scheme is quite interesting.\n- GRS and LRS do not make much sense to me.\n  It's quite unnatural to shuffle amino acids, and it's surprising to me that these representations would do as well as it does on secondary structure.\n  In fact, I don't think GRS should be able to do better than a linear model on just amino acid identities on secondary structure prediction, so either there is a bug here or this is a quirk of the dataset.\n  I also think LRS with a percentage of the sequence length is more interesting, and probably ends up being similar to the MLM loss if you use 15%.\n- In figure 4 - it's not clear why the contrastive models all have a drop in performance at p=0.1. I don't see any reason that this should happen across multiple tasks and augmentation strategies.\n\n\nI also offer some small suggestions for a more clear and better paper:\n- Sec 2, last sentence of paragraph 1 claims protein sequences do not need to preserve contextual meaning.\n  This is simply wrong. \n  They must preserve the ability to fold into useful biological structures, and augmentation strategies like random shuffling remove this property - please remove this sentence.\n- The 'MT' task is more commonly referred to in literature as MLM (Masked Language Modeling), I suggest to use this term instead.\n- Figure 3's color scheme is counter-intuitive and not consistent across tasks.\n  Why is gray the best on two tasks, where as light red is best on remote homology and dark red best on fluorescence?\n  Please use a more intuitive color scheme, where perhaps the bold colors show where the best results are.\n- Please permute the numbers for remote homology so it is ordered (fold, superfamily, family) - this is the natural ordering for this task.\n- Table 7: why are the linear evaluation results now reported by cross entropy rather than classification accuracy as in previous tables?\n\n\nAs a summary, my main criticisms of this paper is (1) not including contact prediction, (2) picking the data augmentation to use for each task, and (3) augmentations with unreasonable intuitions.\n\nEdit: Read responses, bumped score up to a 4, check full response below",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A paper tackling an important topic in protein deep learning",
            "review": "### Summary\n\n- This paper experimentally investigates the effects of data augmentation for protein deep learning.\n- The authors first pretrained a transformer-based NN without data augmentation; then trained the model using data augmentation; finally, each TAPE task was fine-tuned.\n- For data augmentation strategies, they used the Random dictionary, Random Alanine, Global/Local Shuffling, Sequence Reversion, and Subsampling.\n- In the data augmentation phase, the Masked-token prediction (MT) and the Contrastive Learning (CL) have been studied.\n\n### Strong Points\n\n- I agree that investigating the data augmentation effect is important in protein deep learning because effective strategies are not very straightforward compared to other application domains, such as image processing.\n- Experiments are conducted against wide downstream tasks. The authors found that CL with random replacement would be an effective data augmentation.\n- The paper is well-written and easy to follow.\n\n### Weak Points\n\n- Randomness in the experiments: Are the reported values averaged over several attempts with different random seeds (i.e., initial weight of neural network, or randomness in data augmentation)? The authors have (seemingly) reported standard deviation only by the bootstrap over test data. I think this is important because, for example in Fig.4, the reported values seem to be affected by randomness (i.e., at p=0.1); the authors do not provide a reason for this (so I understood that it comes from randomness).\n- Possibility of the other data augmentation: This paper argues that the random replacement strategies (RD/RA) work well for downstream tasks. However, this direction is not investigated deeply. It would be better if the authors will compare it with other replacement strategies. For example:\n  - What happens if we use the other amino acid mappings described in Section 3.2? The authors reports only the best mapping, i.e., [[A,V], [S,T], [F,Y], [K,R], [C,M], [D,E], [N,Q], [V,I]]; how is the impact when using the other mapping?\n  - What happens if we replace an amino acid with the other one that is uniform-randomly chosen from the other 19 possibilities? (Although the authors do not investigate it due to the lack of biological reasons, the following paper reports that this strategy also improves the accuracy for the secondary structure prediction task; Neural Edit Operations for Biological Sequences, NeurIPS 2018)\n- The insights found and argued in this paper are not very strong. Actually, the authors say \"different downstream tasks benefit from different protein augmentations\" at the conclusion. This is a very well-known fact in the deep-learning community.\n\n\n### Minor Points\n\n- In \"Replacement (Dictionary/Alanine)\" paragraph in Section 3.2, the period \"(A).\" should be removed.\n\n\n### Evaluation\n\nOverall, this paper provides an interesting direction for protein learning, where effective data augmentation strategies are not very straightforward compared to other application domains (e.g., image processing). Although I agree that this is an important topic, I have several concerns as mentioned in the \"Weak Points\" above. Please clarify and address my concerns.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}