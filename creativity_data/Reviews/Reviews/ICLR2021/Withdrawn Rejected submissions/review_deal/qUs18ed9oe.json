{
    "Decision": "",
    "Reviews": [
        {
            "title": "A policy optimization method for CMDP using a Lyapunov-based barrier function",
            "review": "The paper considers the model of constrained Markov Decision Process (CMDP) for reinforcement learning with safety constraints. CMDPs are more challenging than regular MDP due to the safety constraints. This work proposes a Lyapunov-based barrier function method LBPO to satisfy the constraints of CMDP while doing policy optimization. LBPO builds on a previous Lyapunov-based approach to safe policy optimization. The idea is to use the cost-value function as a Lyapunov function and optimize the policy while satisfying the Lyapunov constraints. Previous work handles the Lyapunov constraints by some projection methods. This paper proposes a new barrier function approach to convert the Lyapunov constrained optimization problem to an unconstrained optimization with Lyapunov barriers. \n\nSolving CMDP is important in safe reinforcement learning, and the proposed LBPO has much less constraint violations compared with previous methods in the conducted experiments. However, I have the following concerns on the comparison with previous methods and the experiment results.\n\n- From the description, LBPO is mostly the same as SDDPG, and the only difference is on how to optimize the constrained optimization (8). Improving the optimization method is important, but little discussion is provided on why the barrier function approach is better than the projection method besides experimental comparison. The logarithmic barrier seems to make sense, but it's not clear if it's better than some other primal dual methods used in previous approaches.\n\n- Intuitively, the projection method in SDDPG may not provide the best policy in terms of reward performance, but it should mostly satisfy the constraint given the projection on the safe policy set. So my expected results would be that LBPO may have similar constraint violations as SDDPG, but LBPO may produce better returns than SDDPG. However, the experimental results is very different from what I expect: LBPO violate much less constraints than SDDPG. This phenomenon is discussed in 4.3 and the reason provided is the Q-function errors, but the explanation is not convincing besides the numerical comparison. It could be true that SDDPG indeed has more constrain violations, but I have high concerns on the experiment implementation. In particular, it is claimed that all baselines are initialized by the same safe baseline policy. So one would expected that all methods behave safely at the beginning of the training. According to Figure (6) in the supplementary, this is true for most cases, but not true for PointGoal2. In the PointGoal2 plot, SDDPG violates the constraint by a big margin at the beginning and violation stays high throughout training. This seems to be wrong given the supposedly safe initial policy, and it bings up concerns on the correctness of the implementation.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An slightly new algorithm with good empirical result but no theoretical results.",
            "review": "This paper proposed a new improved policy optimization method for continuous-action CMDP in a sense that the policy being updated towards is not far away from some human-defined fixed baseline policy. The paper proposed an approximate to the objective proposed by a previous paper by Chow (2018) using the barrier function and proposed an algorithm which approximates the solution of the approximate objective. Empirically the proposed algorithm reduces the number of constraint violations during the learning process but still gets high returns asymptotically, compared with previous approaches. \n\nOverall I think this is an ok paper. Although the proposed algorithm mainly built upon Chow (2018), and the paper didn't show any new theoretical analysis about its proposed algorithm, the paper empirically showed that the proposed algorithm significantly reduces violating constraints in the learning process while obtaining an OK return after learning.\n\nSeveral concerns:\n\nDid the induced policy set include optimal policy (or some policy close to the optimal policy) by setting \\epsilon in a way described in the paper? If so, why are the returns shown in table 2 still quite low? Maybe a smaller discrete domain would help to illustrate this point.\n\nEquation 13 has not been justified. It is not used in baselines like SDDPG. How important is this technique in their algorithm? Did the authors use the same technique in other baselines?\n\nChow (2018)  didn't call their algorithm SDDPG. I don't know why this paper makes this name.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper is built on previous ideas for safe RL using Lyapunov functions, but proposes to use the logarithmic barrier function instead of  Lyapunov constraints.",
            "review": "\nThis paper focused on safe reinforcement learning within the constrained markov decision process(CMDP) framework. The proposed method is built on the lyapunov-based approach which can guarantee safe policy updates. Specifically, this paper proposed to convert the primal objective with Lyapunov constraints to the unconstrained one by utilizing the logarithmic barrier function as the penalty indicator. The experiments show that the proposed LBPO method can achieve competitive performance with fewer constraint violations.\n\nThe paper is well-written and explains the existing Lyapunov framework introduced by Chow et al. (2018) clearly. The paper makes a small modification on the optimization method within the same framework, and show that the proposed method performs well in terms of the number of constraint violations during training.\n\nHowever, my main conern about the paper is that the work is mainly performed under the same  lyapunov-based framework for safe RL which is devoloped in Chow et al. (2018), and the contribution of this paper is relatively incremental, with a modification of an alternative optimization method, that is, a logarithmic barrier function instead of  Lyapunov constraints and  combining it with trust region-based methods for better performance.  Practically, such modifications may be useful but their theoretical contribution is limited as the penality methods are existing methods and popularly used.\n\nThe experimental results are mainly conducted based on some simulation data instead of a real situation that critically relies on the safety of RL.  \n\nFigure 5 nicely shows that by turning the parameter for the barrier, the tradeoff between safty and performance could be achieved. However, in practice, is there any suggestion on how to set this parameter?\n\nSome typos:\n1. A right bracket is missing in Eq.11\n2. In Fig.5, the hyperparameter β is chosen from {0.0001, 0.001, 0.005, 0.01}, however, in section 4.4, this paper states that \"we do a hyperparameter search for β in the set (0.005, 0.008,\n0.01, 0.02)...\";",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "New experimental results for CMDPs",
            "review": "This is an experimental paper. The paper studies the constrained MDP problems using the Lyapunov-based safe policy iteration approach [1]. To ensure the constraint satisfaction during the training, the authors propose using Lyapunov-based constraints for CPO [2] or DDPG with the safety layer [3] as described in Section 4. Some computational results are provided to show the effectiveness of the algorithms, especially a good performance for the constraint violation. \n\nStrong points:\n\n(1) The proposed algorithms could be an improvement of the Lyapunov-based approach [1] for CMDPs with continuous action spaces.\n\n(2) The proposed methods also extend existing RL algorithms, e.g., DDPG or PPO to deal with constraints. \n\nWeak points:\n\n(1) The key component of the proposed algorithms is incorporating Lyapunov-based constraints into DDPG or PPO. This is done by modifying existing methods in the references [2] and [3]. The Lyapunov-constrained PG method is similar to CPO except for the Lyapunov-based constraints. The Lyapunov-constrained safety layer is also similar to the safety layer in [3]. What are the technical challenges? This makes me feel the novelty of this paper is weak. \n\n(2) The main results are based on the reference [1] in which the constraint satisfaction relies on information on baseline policy and MDP transition models to ensure a Lyapunov function. As we know, CPO [2] and DDPG with the safety layer [3] already perform well for dealing with constraints. If we use more conservative Lyapunov-based constraints, it is not surprising to see a good constraint violation that is already ensured by the Lyapunov theory in [1]. Thus, such comparison experiments seem not that meaningful. Instead of comparing to slightly modified algorithms, I recommend comparing the proposed methods with algorithms in the state-of-the-art references [4, 5, 6]. It also would be more interesting to see what theoretical guarantees are. Since no convergence theory is provided, the impact and significance of this paper could be limited. \n\n(3) Several technical concerns: Why can you utilize the Lyapunov-based method to deal with continuous state space? Why the constrained optimization in Section 4.1 is still feasible? What do you mean by data efficiency?\n\nOverall, this paper has reported some combination results. Due to the above concerns, my current rating is OK. \n\nOther comments:\n\n(1) What do you mean by the ‘Lyapunov-function-based policy optimization framework’? The name ‘framework’ seems to be overstated. \n\n(2) Section 3 just repeats results in [1]. The notation is too heavy and makes it difficult to read. It needs efforts to summarize the key results [1]. What are assumptions and how practical they are?    \n\n(3) Missing inner product brackets in the first argmin formula in Section 4.1. The math symbols look not taking the same size; this issue happens throughout the paper.\n\n(4) Why do you state a direct calculation as Proposition 1? I don’t see any point to emphasize it. \n\n(5) Figures are not in a good view. \n\n(6) Since this is an experimental paper, one example seems not enough to demonstrate the merits. Any other continuous control tasks experimented with? How about the scalability of your algorithms in terms of problem dimension and computational complexity?  \n\n\n[1] Y. Chow, O. Nachum, M. Ghavamzadeh, and E. Duenez-Guzman. A Lyapunov-based approach to safe reinforcement learning. NIPS, 2018.\n\n[2] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv preprint arXiv:1705.10528, 2017.\n\n[3] G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa. Safe exploration in\ncontinuous action spaces. arXiv preprint arXiv:1801.08757, 2018.\n\n[4] R. Cheng, G. Orosz, R.M. Murray, and J.W. Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. AAAI, 2019.\n\n[5] T.Y. Yang, J. Rosca, K. Narasimhan and P.J. Ramadge. Projection-Based Constrained Policy Optimization. ICLR, 2020. \n\n[6] A. Stooke, J. Achiam, P. Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. arXiv preprint arXiv:2007.03964. 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}