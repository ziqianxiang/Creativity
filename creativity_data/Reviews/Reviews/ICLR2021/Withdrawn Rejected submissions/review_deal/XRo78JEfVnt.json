{
    "Decision": "",
    "Reviews": [
        {
            "title": "Contributions are marginal, needed to improve the writing",
            "review": "This paper studies gradient-based optimization over orthogonal polynomials. The authors utilize the three-term recursion of orthogonal polynomials and characterize them by two different types of coefficients, i.e., alpha and beta. In particular, they propose efficient forward and backward passes by amortizing computations of vector-Jacobian products and this reduces the memory complexity from O(n^2) to O(n) where n is the number of data points. They also applied the proposed method to (1) image compression by learning parameters of JPEG and (2) Minimal value of Optimization Problem (MOP) with promising results.\n\nThe main contribution of this work is reducing the memory cost of vector-Jacobian products for evaluation and interpolation. Although it can contribute both fast and memory-efficient operations in a practical setting, I feel that it is fairly marginal. Intuitively, one can be readily obtained this advantage from the three-term recursion as it keeps two consecutive terms for evaluation. If I missed something, I ask authors for describing the key component for reducing memory complexity in their response.\n\nThe experimental results are weak to support the proposed method. For image compression, more comparisons with recent methods are needed. And it would be better to provide the real images to check whether the algorithm correctly can compress. Also I am wondering how the proposed algorithm is faster or slower than the JPEG with DCT.\n\nIn addition, the writing quality of the paper needs to be improved and many problem settings are unclear. I encourage authors to re-arrange the paper so that key contributions and problems that they deal with describe very clear and straightforward.\n\nOverall, this paper proposes an efficient algorithm for optimizing orthogonal polynomials, but the motivation, writing, empirical results should be improved. Hence, I rate this paper to reject.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Efficient method for optimizing over families of orthogonal polynomials, but limited applicability to machine learning",
            "review": "Summary: The paper proposes to learn over all the families of orthogonal polynomials by deriving a fast and memory-efficient method to evaluate the backward pass of orthogonal polynomials evaluate and interpolation. While standard auto-differentiation naively applied to the forward pass will yield a backward pass with O(n^2) memory (where n is the maximum polynomial degree), the proposed method only requires O(n) memory and results in a fast implementation. The paper demonstrates the utility of learning over orthogonal polynomials in two applications. In JPEG compression, replacing the discrete cosine transform (DCT) by orthogonal polynomials can improve compression quality. In approximating the minimal value of a general nonconvex objective, the proposed method shows promising results on toy examples.\n\n======\n\nStrengths:\n1. The proposed method is memory efficient (O(n) instead of O(n^2)), and leads to fast implementation. This will be useful for other applications in machine learning that uses orthogonal polynomials. The speed claim is validated by measuring forward and backward time of the custom implementation against those from an auto-diff engine (Jax).\n2. The experiment of improving JPEG compression with orthogonal polynomials is interesting. It demonstrates the utility of learning over families of orthogonal polynomials instead of using a fixed family (in this case, DCT, or the Chebyshev family). The resulting compression loss is lower when the family is learned rather than fixed.\n\nWeaknesses:\n1. Unclear how the method could be applied in machine learning. I'm unsure how relevant the two experiments are to the machine learning community. For example, the second experiment, finding minimum value of an objective function, was motivated by debugging whether model training has reached the optimal value. However, the experiment only shows results for small dimension (N=2 or N=16), which in the motivating example would correspond to models with 2 or 16 parameters. In general machine learning models tend to have more parameters than that, and I'm not sure how the method would scale with respect to dimension N (I think it's exponential).\n2. Unclear how better memory scaling would benefit the two applications. Even though the proposed method is motivated by reducing the memory of the backward pass from O(n^2) to O(n), the applications shown don't require very large degree n. For example, in the learning JPEG experiment, n = 8. In the experiment in section 5, n is at most 12 I think.\n3. Lacking comparison to baselines such as trying multiple families of classical orthogonal polynomials. The paper argues that it's useful to learn over families of orthogonal polynomials, for example by gradient-based methods. However, it's unclear that is better than just trying a different different families of classical orthogonal polynomials (Jacobi, Laguerre, Hermite). For example, in the learning JPEG experiment, if one can choose among these 3 or 4 different families, how well does that do against learning with gradient-based method?\n4. It's unclear how the proposed method to compute the backward pass was derived. Section 3 seems to lack intuition and insight on how the backward pass can be done efficiently.\n\n======\n\nOverall, I vote for rejecting. Though the idea is interesting, I'm not yet convinced that the proposed method has wide applicability to machine learning. More empirical evidence, on more realistic datasets, would make the paper stronger.\n\n======\n\nAdditional feedback and questions:\n- Section 4.1: are the alphas and betas initialized to those of the DCT?\n- Section 5: it's not clear to me how the dimension N affects the algorithm? How does the algorithm scale with respect to N?\n- Sentence right above 5.2: exponential in N, not n?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Optimizing Over All Sequences of Orthogonal Polynomials",
            "review": "This work introduces developments connecting the theories of orthogonal polynomials and optimization. First, it shows how to differentiate through the orthogonal polynomial transform in terms of the parameters of the orthogonal polynomial (OP) family. This allows using learnable OPTs instead of being constrained to a fixed transform, such as the DCT. Second, it considers finding the minimum value of a function by leveraging recent techniques showing that this can be reduced to a problem of orthogonalizing the measure.\n\nThis paper builds on a line of work connecting orthogonal polynomials to machine learning, particularly on using parameterized structured transforms as learnable components of ML pipelines that can be used to replace hand-crafted structure. The contributions are useful as they show improvements on applications as important as JPEG compression. The main weakness of the work is that the second half on minimizing functions through orthogonalizing the probability measure feels disconnected from the first half, and seems of limited applicability.\n\nDetailed questions and comments:\n\nLearnable OPs:\n- The paper implies that it chooses to use the $O(n^2)$ instead of $O(n\\log^2 n)$ algorithms because they are parallelizable. However, the $O(n\\log^2 n)$ algorithms should also be parallelizable (e.g. FFT-like algorithms such as the DCT can have only log(n) depth). The paper only compares naive and custom implementations of the $O(n^2)$ algorithms against each other, but does not compare the $O(n^2)$ vs $O(n\\log^2 n)$ algorithms; what is the speed of AnyPT vs. a custom DCT baseline? Even a comparison just for the forward pass (inference) would be useful\n- Initialization: do you initialize your layer close to the DCT for the JPEG task? what happens if it's initialized very far?\n\nMop:\n- Section 5.1:\n  - The notation is confusing (e.g. X is not defined) although ultimately understandable\n  - the way $\\lambda$ is defined, it does not involve $f$. Should the sentence \"Let α and β be the (unique) recurrence coefficients of the sequence of polynomials orthogonal with respect to ρ\" say $\\mu$ instead of $\\rho$?\n- If I understand correctly, the proposed algorithm potentially makes two stochastic modifications to Lasserre's algorithm:\n  1. Using approximate instead of exact moments, through sampling\n  2. Addressing stochastic instead of exact functions\n  Is it correct that using either of these modifications will have no proven convergence rates?\n- It is strange that the experiments don't have a comparison to (Lasserre 2020) as a baseline, which the proposed algorithm is based on\n- As stated, Mop seems to only apply to 1-dimensional problems. How is it modified to work in higher dimensions? In general, one of my main concerns is that the use case of the algorithm seems limited.\n\nMisc.:\n- The work [1] may be interesting as it presents several families of $O(n \\log^2 n)$ algorithms for evaluation, inversion, and transpose multiplication of families of transforms that generalize orthogonal polynomial transforms, based on the (\\alpha, \\beta)-recurrence parameterization of OP transforms.\n[1] De Sa et al. \"A two-pronged progress in structured dense matrix vector multiplication\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}