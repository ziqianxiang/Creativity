{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors argue that uniform priors for the high-level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero-shot domain adaptation and few-shot meta-learning.\n\nPro:\n- A simple yet effective method\n- Signifiant gains in experimental study\n\nCons:\n- Close variants of this approach were proposed in previous works, and so the novelty of the current work is limited.\n- There is no accompanying analysis which may shed new light on the advantages of the approach."
    },
    "Reviews": [
        {
            "title": "Simple yet effective regularization trick with numerous applications",
            "review": "The authors argue that uniform priors for the high-level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero-shot domain adaptation and few-shot meta-learning. The authors propose a uniformity regularization term on the latent representation, implemented as an adversarial discrepancy. The results show consistent improvement in the different tasks.\n\nStrengths\n- The method is simple, yet effective.\n- The paper is easy to follow and well presented.\n- The approach is relevant and can be applied to multiple problems.\n- The evaluation is comprehensive, showing consistent moderate gains.\n\nWeaknesses\n- The observation that uniformity helps transference was already observed by previous works, as the authors acknowledge. From that, the derivation of the regularization term is relatively straightforward.\n- There is no qualitative analysis to illustrate and provide insights about why uniformity helps nor how the regularization term changes the latent representations.\n- The actual methods used in different tasks are often not explained. For example, the method for the out-of-distribution task is not explained. Please provide concise yet helpful explanations.\n\nIn my opinion, the work shows an interesting contribution, but it would benefit from qualitative analysis that could provide more insights.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple and effective approach, however some comparisons are missing.",
            "review": "#### Summary\n\nThe authors propose a regularization technique that maximizes the entropy of the learned representation by regularising it with uniformity prior. The uniformity prior is imposed via an adversarial objective function.\n\n#### Strong Points\n\n1. The proposed regularization can be easily added to the existing frameworks to improve their generalization ability.\n2. The methods improve upon the results of the baselines.\n3. The paper is well-written and easy to follow.\n\n#### Weak Points\n\nAlthough the method clearly improves the results, my main concern is with the novelty of the proposed method. Secondly, the paper needs to position itself better with respect to related work which proposes more or less similar regularisations for meta-learning frameworks. \n\n1. The idea of maximizing the entropy of the learned representation to increase its generalization capability has been explored before. In addition to the representation learning perspective in [Wang and Isola] and [2], [1] explicitly studied entropy maximization regularizations for generalization in meta-learning, although their regularization objectives are different. I would appreciate it if the authors could compare their proposed regularization with the ones in [1].\n\n2. For uniformity regularisation, the authors propose an adversarial learning scheme. This way of inducing uniformity in the representations is also not new. For e.g. Hjelm et al [2] used the exact strategy to induce uniformity in their self-supervised representations learning method.\n\n3. As noted by the authors, [Wang and Isola] show that the self-supervised contrastive objectives also induce uniformity in the learned representations (although in hypersphere). Such contrastive objectives have recently been used to improve transferability of meta learned representations for e.g. in [3, 4]. I would appreciate it if authors could comment on how those regularizations are different and why they should not be compared with the proposed method.\n\n\n[1] Jamal et al. Task-Agnostic Meta-Learning for Few-shot Learning.\n\n[2] Hjelm et al. Learning Deep Representations by Mutual Information Estimation and Maximization.\n\n[3] Medina et al. Self-Supervised Prototypical Transfer Learning for Few-Shot Classification.\n\n[4] Doersch et al. CrossTransformers: spatially-aware few-shot transfer.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "In this paper, the authors claimed that uniformity in embedding space if the key for good generalization, and then propose an adversarial training based method to improve the uniformity of feature space. The claim is from previous work, thus the key contribution is the way to impose such regularization. The method itself makes sense to me. \n\nOne lacking aspect is that the authors provide no evidence on how the method works, neither quantitatively (the distance between uniform distribution and learned feature distribution) nor qualitatively (e.g. t-sne visualization on the learned feature). \n\nAnother point could improve is that I suspect the effect of uniformity is quite like imposing margin on loss function (such as AM-softmax, arcface, etc), it is better to discuss and compare with them. \n\nI am not familiar with the dataset and SOTA performance used in evaluation. The results look reasonable to me, and could demonstrate the effectiveness of the proposed method.\n\nAbove all, I think this paper may contain some ideas that publishable, however the authors fail to dig deeper into it, and lack sufficient ablation experiments to demonstrate the method works as expected. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of AnonReviewer2",
            "review": "Summary:This paper broadly discusses about learning good transferable features in deep networks to perform few or zero-shot \nadaptation to novel tasks. The paper proposes a uniformity regularization scheme that encourages better transfer and feature reuse. The method is validated on several benchmark datasets.\n\n+ves:\n+ The paper is overall well-written and easy to follow. \n+ The paper has shown experimentations on different datasets under different learning setups, including deep metric learning and zero shot domain adaptation.\n+ The overall idea of using a uniform regularizer on manifold is interesting and supported by theoretical considerations coming from GAN and VAE-like formulations.\n\nConcerns:\n- Equation 6, which is the final proposed loss, seems incomplete, which seems like a key issue. As I understand, Eqn 6 must consider the discriminator loss described in Eqn 4 too. Considering only the generator (that is the network q(z|x) till the last layer) is a necessary condition but may not be a sufficient condition to optimize the end-to-end network. I will be happy to know if I have missed something here.\n- The paper seems to have missed some key references that have addressed similar problems - [1] and [2] listed at the end of this review. Both of these efforts show task transferability for computer vision tasks. This is important, considering the similarity of objectives.\n- The paper states that it performs transfer between MNIST, SVHN and USPS (model trained on source data and tested on target data). It was not clear how MNIST --> SVHN was done. Wouldn't there be a mismatch of input channels in this case (SVHN is color, while MNIST is not). \n- A feature space visualization of the proposed method and other baselines would have been very useful to directly compare and understand the claim and advantages of imposing uniform regularizer.\n- One broader question that may be relevant (may not directly relate to a decision on the paper): How will uniformity on task space be affected if we leveraged a method such as Mixup [3]? How will the task space, in that case, look like? \n\nMinor comments:\n1.  Abstract: learn-ing --> learning,  transfer-able -->  transferable\n2. Please check the line \"... for improved generalization: For Unsupervised Representation Learning\"\n3. Section Uniformity Regularization: isn't \\U(-\\aplha, \\beta) a correct formulation considering the previous \\U(-\\alpha, \\beta) in section Prior Matching?\n4. Page 7 Zero shot Domain Adaption: Please remove ``(\" from the ``(LeNet''\n\nReferences:\n[1] Zero-shot task transfer, Pal et al, CVPR 2019\n[2] LSM: Learning Subspace Minimization for Low-level Vision, Tang et al, CVPR 2020\n[3] mixup: BEYOND EMPIRICAL RISK MINIMIZATION, Zhang et al, ICLR 2018\n\nPOST-REBUTTAL:\nI thank the authors for their response.\n\nAt the outset, I am satisfied with the authors' responses to my questions - all the questions were answered. I do agree with other reviewers that the idea is incremental. Learning the prior across tasks is not very novel, as pointed out in references cited by other reviewers. However, on the bright side, the authors have done a good job in answering the questions, and the comprehensive experimental results are promising. The overall idea looks a bit incremental from the GAN literature side but maybe a good lead for meta and incremental learning literature.\n\nI change my decision to \"Weak Accept\".",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}