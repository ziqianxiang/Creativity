{
    "Decision": "",
    "Reviews": [
        {
            "title": "Solid paper, marginally below acceptance threshold",
            "review": "The work at hand addresses the pruning of neural networks, which has become popular in recent years (e.g. due to networks being deployed on mobile devices). The authors propose a \"dataset-dependent structured pruning model\", which yields pruning masks for a given dataset and target network. A meta-learning framework (STAMP) is proposed to train this mask generator within a few gradient steps for a new task (Algorithm 1). Experiments are conducted on three data sets (CIFAR 10, SVHN, and Aircraft); here, meta-training is conducted on CIFAR100. Thus, weights are removed of a model trainined on CIFAR100 based on information (few gradient steps) from the three aforemented new datasets and the learnt meta information (set encoder).\n\nPositive: \n- The performance and time improvements are significant\n- The baselines seem reasonable\n- The paper is well written and structured\n\nNegative:\n- Only very shallow networks (vggnet-19 and ResNet18) are considered; it is unclear if this idea works with larger architectures (more weights)\n- Only small datasets are considered (CIARF10, SVHN, and a downsampled version of Aircraft)\n- Very poor performance on the Aircraft dataset (normal performance is around 95% accuracy; might be due to subsampling)\n\nOverall, I think that the paper is a solid piece of work, but it might not meet the very high ICLR standards. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "review",
            "review": "### Summary\n\nThis paper proposes STAMP, a novel method that can adapt the network architecture (number of channels) to different datasets fast. To achieve, a set-based encoder summarized the information needed to prune the network and this information is propagated and used by a gating function similar to that of prior art (BBDropout). In order to achieve fast adaptation, the trainable parameters under this framework are trained with meta gradient. Empirical results on various datasets have demonstrated the effectiveness of the proposed method over other baselines.\n\n### Reasons for score\n\nI find the idea of fast adaptation in both architectures and weights interesting, and the empirical results support the proposed method. However, I'm probably missing something and have concerns regarding the current paper listed in weaknesses.\n\n### Strengths\n\n- The proposed solution is well-motivated and novel in the sense that it learns to adapt both the weights and the architecture fast to target dataset.\n- The empirical results look good in the sense that it outperforms recently proposed AutoML-based search method (MetaPruning) with significantly lower cost for adaptation.\n\n### Weaknesses\n\n**The method can be presented more clearly**\n\n- Without looking into the supplementary material, one cannot really understand why the set encoder is and what the gating function is. In addition, $h_l$ used in equation 3 is not defined in the main text. I had a hard time understanding the method until I read through the supplementary. To me, those material are \"necessary\" as opposed to \"supplementary\". \n\n\n- It is not clear how $\\kappa$ is set for line 6 in algorithm 1. Do we need different $\\kappa$ between training and pruning? Since the paper mentioned that the proposed method aims to train once and use everywhere. It is not clear if the sparsity constrained should be the same of different during meta-training and meta-testing. It is also not clear what line 12 in algorithm 1 does.\n\n\n - It is not clear how to run STAMP for Table 2 and Table 3 as Algorithm 1 has two inputs: source dataset and target dataset. This makes it hard to reason about why STAMP can possibly outperform methods such as BBDropout and MetaPruning given no transfer is happening. The better performance of STAMP over MetaPruning makes me wonder if this is a general approach that improves pruning. I think clear discussion regarding this performance gap is needed.\n\n**Main results demonstrated using unnecessarily large networks**\n\nWhy use VGG and ResNet-18 for CIFAR? In the resnet paper, a family of resnets were proposed specifically for CIFAR, i.e., ResNet-20, ResNet-26, ..., ResNet-56, which have much fewer parameters than ResNet-18. Additionally, ResNet-56 can achieve even better performance [1,2] compared to the numbers here in Table 1 and Table 2. In this case, the performance improvements obtained might due to poor baselines. Having ResNet-56 in the paper allows benchmarking with other channel pruning paper to better position the proposed approach and rule out the possibility that the performance improvements come from poorly trained baselines (potentially overfitting too much).\n\n[1] He, Yang, et al. \"Soft filter pruning for accelerating deep convolutional neural networks.\" arXiv preprint arXiv:1808.06866 (2018).\n\n[2] Chin, Ting-Wu, et al. \"Towards Efficient Model Compression via Learned Global Ranking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n**Evaluation can be better**\n\nSimilar to few-shot image classification setting, this paper in addition can adapt the image classifier. As a result, it would be more informative to adopt the evaluation protocol adopted in few-shot image classification such as N-way classification using CIFAR and miniImageNet. In this way, we can understand if adapting the architecture helps or hurts compared to just adapting the weights.\n\nAs opposed to these common benchmarks and evaluation protocol in few-shot learning, this paper evaluates on CIFAR, SVHN, and Aircraft. While this setting has no problem, it gives little information on comparisons to meta-learning without adapting the architectures and I think this can greatly benefit the manuscript. Without this experiment, it might appear that we need both \"pruning\" and \"meta-learning\" to do well compared to the full network baseline (for the Aircraft case).\n\nOne important baseline I would like to see is pruning meta-learning trained models. Specifically, in algorithm 1, train W with meta gradient. After this procedure, use BBDropout during fine-tuning on target dataset. I'm wondering the improvements obtained in the proposed method might mainly come from meta learning as opposed to the novel aspect: \"adapting architectures with meta-learned encoder\".",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "## Summary \nThis work focuses on improving fine-tuning performance(training time, inference time) through learning a mask generator that is used for pruning. The generator is trained through meta-learning (using CIFAR-100) and performance is measured in 3 different datasets (Cifar-10-, SVHN, Aircraft). Although I find the motivation interesting and relevant I have serious concerns about experimental evaluation and writing needs to be improved significantly.\n\n## Pros \n- Authors do a diligent comparison on training efficiency by using FLOPs count and run-time, which makes the work quite applicable to the real-world.\n\n## Cons \n- I think baselines should be improved. Authors say: \"...our main focus...to significantly reduce the training time by obtaining a near-optimal compact deep networks for unseen targets on the fly, which is not possible with any of the existing approaches.\" I don't this is accurate. To begin with, many early pruning work did indeed focused on this. See for example this [8] or this [7]. You can use existing pruning techniques during fine-tuning. I would recommend magnitude based pruning as a baseline. It is important the regularize the initial training (probably with weight decay), too while doing this. Similarly SNIP would probably perform better if used on the pre-trained network; not from random weights. There are also much better baselines to pick beyond SNIP. \n\n- There are many efficient training methods [1, 4, 5] that came up after SNIP. Those should be mentioned and compared against if possible. \n\n- \"However, they are limited in that they perform unstructured pruning which will not result in meaningful speedups on GPUs, either at inference or training time.\" This is true given the current state of hardware/software. But I don't think this is a fundamental limitation of these methods. See recent work on acceleration [11, 12]. \"unstructured pruning, they can not reduce FLOPs which remains equal to the original full networks.\" Why SNIP FLOPs are same as the dense training? This should be updated as a 90% sparse network would require only (assuming uniform sparsity distribution)  significantly less FLOPs. It is also not clear why they take more time than the regular training? \n\n- \"Moreover, they underperform state-of-the-art structure pruning techniques with pretraining.\" There are many work that shows sparse network perform better than same size dense networks. See [1, 2, 3]; One need to consider total training time when comparing different methods. \n\n- I think focusing on structured pruning and obtaining small dense networks since they are accelerated with current hardware/software is a valid point; but one needs to consider [4] when discussing these methods; which shows that the pruned networks can be trained from scratch and thus the structured pruning algorithms should be thought of as architecture search methods. I would like to see this baseline method: i.e. small-dense model that is the same size as the pruned STAMP network trained from scratch.\n\n- \"For fine-tuning, we follow the standard setting from Zhuang et al. (2018) and perform mini-batch SGD for 200 epochs where the batch size is 128.\" I am not sure how standard these fine-tuning hyper-parameters are but it would be nice to mention the learning rate. If it is small, I would propose a higher learning rate. Results are usually significantly better with higher learning rates. See this work: https://openreview.net/forum?id=Cb54AMqHQFP&referrer=%5BTasks%5D(%2Ftasks). \n\n- I recommend using more established benchmarks like VTAB-1k [9] or [10]. Only 10 task (from CIFAR-100) is probably not enough for meaningful transfer. Definitely not for \"any\" task.\n\n## Minor \n- \"Thus we exclude the meta training time of STAMP (15h on VGGNet and 30h on ResNet) and MetaPruning (1.2h) per task in\" This is maybe ok, but I recommend authors to use than more tasks for evaluation; currently I don't think this pre-training cost is amortized.\n- Does the set function require 640 images always?\n- \"needs of pretraining,' -> need for pretraining\n- \"such that it can generalize well to any unseen datasets within a few gradient steps of training.\" any should be removed here I think; We don't even know all unseen datasets; yet to measure performance.\n- \"by removing its weights and activations (unstructured) or\" What does removing activations mean? Isn't that structured pruning? If you mean Activation pruning/sparsity,  that is different than removing parameters and therefore not a pruning method.\n- \"without consideration of its structure.\" -> without considering its structure\n- \"repeats between\" repeats\n- \"two widely used network architectures, namely VGGNet-19\" I don't think VGG is widely used in practice if any. Maybe MobileNet?\n- \"VGGNet-19 with 16 convolution layers and a single fully connected layer\" Why not global average pooling? Why the 3 fc layers are replaced from the original network.\n- \"These results are consistent with the findings in Frankle & Carbin (2019), which showed that most of\nthe subnetworks require larger number of training iterations over the full network.\" I am not sure LT paper showed this. Can you be more specific by what you mean here? LTs converge significantly faster. Random sparse networks doesn't match the pruning performance.\n- Does the full-dense baseline use regularization? I think with the right regularization full network might approach to smaller ones.\n\n[1] Rigging the Lottery: Making All Tickets Winners, https://arxiv.org/abs/1911.11134\n[2] Efficient Neural Audio Synthesis https://arxiv.org/abs/1802.08435\n[3] Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers\n https://arxiv.org/abs/2002.11794\n[4] Picking Winning Tickets Before Training by Preserving Gradient Flow\n https://arxiv.org/abs/2002.07376\n[5] Pruning neural networks without any data by iteratively conserving synaptic flow https://arxiv.org/abs/2006.05467\n[6] Rethinking the Value of Network Pruning https://arxiv.org/abs/1810.05270\n[7] https://arxiv.org/pdf/2002.08307.pdf\n[8] (https://arxiv.org/pdf/1611.06440.pdf)\n[9] (https://arxiv.org/abs/1910.04867)\n[10] Meta-Dataset (https://arxiv.org/abs/1903.03096)\n[11] https://arxiv.org/abs/1911.09723\n[12] https://arxiv.org/abs/2006.10901",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}