{
    "Decision": "",
    "Reviews": [
        {
            "title": "Paper is marginally below acceptance threshold",
            "review": "*****  Paper's Summary  *****\n\nThis paper considers the multi-armed bandits problem, where the rewards are non-stationary and depend on past actions and contexts. For dealing with exploration and exploitation in such problems, the authors proposed a recurrent neural network (RNN) based algorithm that uses the energy regularization method. Their experimental results show the effectiveness of the proposed algorithm in various benchmarks and real-life recommendation systems.\n\n\n*****  Paper's Strengths  *****\n\nAs the proposed algorithm uses RNN with energy regularization, it can be useful for non-stationary multi-armed bandits problems. \n\nThe experiments consider different scenarios to measure the performance of the proposed algorithm. It is demonstrated that the proposed algorithm is better than other algorithms.\n\n\n*****  Paper's Weaknesses  *****\n\nThe weak point of the paper is its contributions. The only contribution of the paper is an RNN based algorithm that uses energy regularization. \n\nOnce the neural network is fixed (number of layers and size of hidden layers), it may not be possible to estimate any arbitrary reward function. Therefore, the proposed algorithm can have linear regret for the cases where the reward function can not be estimated.\n\nUnlike existing methods, there is no theoretical guarantee for the proposed algorithm. Without such guarantees, it is not sure that the proposed algorithm will work for every problem. \n\n\n\n*****  Comments  *****\n\nThe authors can look at [1], which uses a deep neural network for solving contextual bandits. The paper uses recent theoretical results from deep learning to drive regret bound for the proposed algorithm (NeuralUCB). A similar result may be useful for driving the regret bound of the proposed algorithm. This paper will become much stronger if it has theoretical regret guarantees.\n\nIt would be nice if the proposed algorithm's performance is compared with NeuralUCB for at least contextual problems. Also, adding a confidence interval in the plots gives an idea about the variability of different algorithms.\n\n[1] Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with UCB-based exploration. ICML 2020.\n\n\n*****  Questions for the Authors  *****\n\nPlease address the above weaknesses of the paper.\n\nWhat is your intuition behind using a 2-layer stacked GRU with the size of the hidden layer = 128 for all experiments?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "RNN for Non-stationary MAB",
            "review": "This paper studies the non-stationary non-oblivious adversarial multi-armed bandit problem, in which the reward distribution may change over time and is relative to the historical arm-selection sequence. The main contribution is the regularizing term that punishes extreme selection distribution (in other words, extreme arm selection criteria is punished) so that the trade-off between exploitation and exploration is balanced. Experiments on both synthetic scenarios and real-world datasets show convincing results.\n\nThe problem studied in this paper is clearly described and well-defined. The main contribution is a novel regularization term to balance the exploration-exploitation in non-stationary MAB, though the idea itself is not novel.\n\nThere remain some severe problems. \n1. Firstly, the structure of the paper is not good enough. No formal formulation and descriptions for the non-stationarity that is trying to handle until the experiment part, which may confuse the readers. Actually, if the reader only read Section 1-3, it is very possible to think that the non-stationary model studied in this paper only fits the rotting bandits.\n\n2. Secondly, in the related work part, the paper lists several non-stationary scenarios while ignoring their solutions and may lead to misunderstandings that these scenarios may not be solved by now, which is not the case. There are recently many efforts devoted to the non-stationary bandits, and it seems that the paper does not mention such advancement. To name a few,\n* MAB setting\n(1) A. Garivier and E. Moulines. On upper-confidence bound policies for switching bandit problems. ALT 2011.\n(2) C.-Y. Wei, Y.-T. Hong, and C.-J. Lu. Tracking the Best Expert in Non-stationary Stochastic Environments. NeurIPS 2017.\n(3) R. Allesiardo, R. F´eraud, and O.-A. Maillard. The nonstationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics 2017.\n* Linear bandits setting\n(1) W. C. Cheung, D. Simchi-Levi, and R. Zhu. Learning to optimize under non-stationarity. AISTATS 2019.\n(2) Y. Russac, C. Vernade, and O. Capp´e. Weighted linear bandits for non-stationary environments. NeurIPS 2019.\n(3) P. Zhao, L. Zhang, Y. Jiang, and Z.-H. Zhou. A Simple Approach for Non-stationary Linear Bandits. AISTATS 2020.\n(4) B. Kim, and A. Tweari. Randomized Exploration for Non-Stationary Stochastic Linear Bandits. UAI 2020.\n* Contextual bandits setting\n(1) H. Luo, C.-Y. Wei, A. Agarwal, and J. Langford. Efficient contextual bandits in non-stationary worlds. COLT 2018.\n(2) P. Auer, Y. Chen, P. Gajane, C.-W. Lee, H. Luo, R. Ortner, and C.-Y. Wei. Achieving optimal dynamic regret for non-stationary bandits without prior information. COLT 2019.\n\n3. Thirdly, what the theoretical bound trying to say is that pure exploitation rarely happens since there's a balanced rate bound for the probabilities that arms are chosen, while this theoretical result is not relevant to the regret bound that we truly want. \n\n4. Lastly, and most importantly, there exist several problems in the experimental settings. In Bernoulli MAB, SWA as the compared method, is not designed for Bernoulli MAB but Rotting Bandit; the same problem also appears in the Yahoo!Today's experiment where contenders (LinUCB, LinTS, BootstrappedUCB) are not designed for non-stationary bandit problems. Actually, these approaches are designed for the stationary bandits setting, so it is not surprising that the algorithm of this paper can outperform them. As raised in point 3, the authors seemingly overlook many related works in non-stationary bandits, in which many other algorithms of non-stationary bandits can serve as the baselines. Besides, the rotting bandits experiment uses the prior information on how the distribution changes, in order to perform the change of dropout. While such prior information may not be available when it comes to real-world situations. Also, there's a serious typo that may lead to misunderstanding, such as \"the mean accumulated reward\" before \"presented in Fig.1(b)\" should be \"the mean accumulated regret\". \n\nTo ensure more convincing empirical results, the following two experiments are believed necessary: \n- Rotting Bandits with dropout (it does not matter if the result are not good enough, the key is to avoid using prior information about changes)  \n- Non-stationary bandits algorithms (such as sliding window/discounting/restarting methods, ADA-ILTCB+, etc) as the baselines on Yahoo's real-world datasets.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok but not good enough submission",
            "review": "##########################################################################\n\nSummary:\n \nThe paper proposes to solve non-stationary multi-armed bandit problem where rewards could depend on past actions or contexts with RNN. The author introduces an energy minimization term to limit the gap between the maximal and minimal probabilities to prevent the network from being too confident. Simulation experiments are done on some variants of the bandit problem and a real world recommendation dataset.\n\n\n##########################################################################\n\nReasons for score: \n \nOverall, I vote for rejecting. While the experiment results are encouraging, the paper has several drawbacks, detailed below. \n\n\n##########################################################################\n\nPros:\n\n1. The idea of solving non-stationary bandit with RNN is relatively new.\n \n2. This paper provides experiment results on several bandit problems and a real world dataset, and the majority of the results are encouraging, showing that the proposed method is better than other baselines. However, there are situations in which the proposed method is not the best or fails to converge, e.g., Figure 1(b), or 3(a).\n \n\n##########################################################################\n\nCons: \n\n1. The justification for introducing the energy minimization term looks problematic to me. It is not clear to me why Equation 10 is satisfied when you do the optimization, since the objective function is the sum of REINFORCE loss and energy loss. This is particularly problematic to Theorem 1, which assumes that Equation 10 is satisfied. Moreover, I am not sure how meaningful the bound in Theorem 1 is.  Only plotting the bound in the figure on page 4 is not very informative, as it could be that even without this energy minimization term the actual ratio between the maximal and minimal probabilities is already not very large. \n\n2.  Overall the experiments are not convincing enough. For example, it is not clear if the results except those in figure 1(a) and 4 are averaged over multiple runs. Also, in the Bernoulli bandit problems (results in figure 1), it seems like the proposed method is only at best marginally better than the Bernoulli Bandit baseline, which is not designed for handling non-stationarity.\n\n3. The paper has a number of presentation issues that makes it hard to follow. I will specify a few of them below.\n\n\n##########################################################################\n\nPresentation issues:\n1.“The method has been successfully applied for recommendation tasks(Li et al., 2010) and non-linear task”. What does “non-linear task” mean here?\n\n2.Is the definition of regret in Equation 3 dynamic regret? Usually people consider static regret, measured against a constant action. What is the motivation of considering dynamic regret here?\n\n3.“So the output of classification layer cannot become biased”. What does “biased” mean here? And why does the output cannot become biased? If “biased” mean the maximal probabilities is too large or the minimal probabilities is too small, I would consider a maximal probabilities that could be 10 times larger than the minimal probabilities to be “biased”.\n\n4.The y-label in Figure 3(b) is confusing, why would the cumulative regret decrease over step?\n\n\n##########################################################################\n\nOther minor comments: \n1.It would be good to see some ablation study on the RNN architecture as well, such as LSTM vs GRU, hidden size or number of layers, etc.\n2.Is there any particular reason why the proposed method much more successful than other methods on the Yahoo dataset?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "There are major flaws in this paper",
            "review": "This paper proposes an original way, the use of Recurrent Neural Networks, for handling a very general problem containing almost all bandit problems: stochastic bandits, non-stationary bandits, rotting bandits, contextual bandits…\n\nThe related work is incomplete. The authors have to explain what is the fundamental difference between their framework and the standard Reinforcement Learning. The authors do not reference an important line of works that is clearly related to their setting: the adversarial bandits, where the sequence of rewards is chosen by an adversary (see for instance [1]). In this reference, the authors will find for instance Exp3, and Exp3.S, which could (should) be used for controlling the exploration of their model with some regret guarantees. The authors should also position their work with respect to restless bandits [7]. The state-of-the art concerning the use of Neural Network for the contextual bandit problem is almost empty. The authors should position their work with [2,3].\n\nThe method used for controlling the exploration is based on regularization terms.\n\nA term for taking care of actions with a low probability to be sampled and an energy conservation term for preventing the output of the RNN to be biased (by the way if the authors take a look to Exp3, that is exactly that it does but with regret guarantees). The third regularization that is used is dropout, which is a pure regularization method, and its action in favor of exploration is not clear in the paper. The authors have to explain how, may be by referring “follows-the-perturbed-leader” [8].\n\nThe author proposes an analytical result to show that the proposed method guarantee a lower bound of the lowest probability of an action of being sampled. This result is weak, but worst it is wrong.\nThis result is weak because it does not guarantee that the values of the probabilities of actions lead to low regret.\nThis result is wrong because the use of a regularization term such as L_{EC} does not guarantee that its value at the end of the learning process is zero. So the first line of Theorem 1 does not hold: \\sum_i p_{i_t}z_i \\neq 0. \n\nIn the experimental section the authors compare the proposed method on different MAB problem: stationary, non-stationary, contextual, rotting bandits…\nIn Figure 1, the reviewer does not understand why the mean cumulative regret is decreasing. Usually it is an increasing function. So the reviewer is not sure that the authors really plot the cumulative difference between the mean reward of the optimal policy and the average reward of the tested policies.\nSame remark for Figure 3 and 5. \n\nFor the Yahoo dataset, the authors present the final CTR but say nothing about what happens before. The CTR during time could be more informative. The use of non-linear contextual bandits as baselines could be fairer (see for instance [2,3,4,5,6]).\n\nOverall, the authors propose an interesting idea, the use of RNN for generalizing a lot of bandit problems. However, this paper cannot be published at this time. There are too many issues. The reviewer encourages the authors to improve their work.\n\nMinor comment:\nIn the experimental section in the paragraph Bernoulli MAB, the authors use \\hat{\\theta}, which is misleading: usually \\hat{\\theta}, is used to mean an estimation of \\theta.\n\nReferences:\n[1] The non-stochastic multi-armed bandit problem, P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire, Siam computing, 2002. \n\n[2] Neural Contextual Bandits with UCB-based Exploration, D. Zhou, L. Li, Q. Gu, ICML 2020.\n\n[3] A Neural Networks Committee for the Contextual Bandit Problem, R. Allesiardo, R. Féraud, D. Bouneffouf, ICONIP 2014.\n\n[4] Random forest for the contextual bandit problem, R. Feraud, R. Allesiardo,  T. Urvoy, and F. Clerot, AISTATS 2016.\n\n[5] Filippi, S., Cappe, O., Garivier, A., and Szepesvari, C. Parametric bandits: The generalized linear case, S. Filippi, O. Cappe, A. Garivier, and C. Szepesvari, NIPS 2010.\n\n[6] Finite-time analysis of kernelised contextual bandits, M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini, UAI 2013. \n[7] Restless bandits: Activity allocation in a changing world, P. Whittle, Journal of Applied Probability 1988.\n\n[8] A. Kalai, S. Vempala Efficient algorithms for online decision problems, A. Kalai, S. Vempala,  In Journal of Computer\nand System Sciences, 2005.\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}