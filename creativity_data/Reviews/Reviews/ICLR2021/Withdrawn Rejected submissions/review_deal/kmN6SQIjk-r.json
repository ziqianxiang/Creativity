{
    "Decision": "",
    "Reviews": [
        {
            "title": "MoCo-Pretraining Improves Representations and Transferability of Chest X-ray Models",
            "review": "This paper investigates whether MoCo-pretraining leads to better representations or initializations for chest X-ray interpretation. \n The main contribution of this paper is that a linear model trained on MoCo-pretrained representations has higher performance on a chest X-ray interpretation task than one trained without MoCo-pretrained representations, with greater improvements in low labeled data regimes and a model fine-tuned end-to-end with MoCo-pretraining has higher performance than one without MoCo-pretraining for small label fractions, and comparable performance for large label fractions.\n\n+ The idea of tranferbility of MoCo-pretraining  in x-ray is interesting. \n+ Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. However, the method having limited novelty (tranferbility of MoCo-pretraining vs imagenet), which is marginally acceptable in ICLR2021\n\n+ In this paper MoCO was selected due to small batch size. In general, queue with memory bank is one of the most important factors in MoCO. However, this paper don't mention it. \n+ Why the author used pleural effusion label of chexpert? Please add additional labels' performances.\n+ There are marginal improvement of this MoCo-pretraining to ImageNet pretraining. However, it is very interesting in stress test.\n\nIn addition, detail result of Shenzhen dataset should be evaluated\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper is a comprehensive study exploring the benefits of MoCo pretraining for chest xray interpretation. While the paper is well motivated and well written, more experiments/ablations need to be performed to clearly validate the claims made in the paper and draw firm conclusions.",
            "review": "Clarity\n\n- Well written paper, figures are informative\n- For non medical imaging audience, it would be good to provide figures with illustrative examples from the dataset demonstrating difference from natural images like the different views, etc\n\nQuality\n- All results have been statistically validated\n\nOriginality\n- This is the first paper to comprehensively study the benefits of MoCo pretraining for chest xray interpretation\n- However, there have been several prior works studying the potential of self supervised learning for medical imaging. The related work section does not cite any of them. For example: i> https://arxiv.org/pdf/1907.02757.pdf ii> https://arxiv.org/pdf/1806.09521.pdf iii> https://www.sciencedirect.com/science/article/abs/pii/S0968432817303037 iv> https://link.springer.com/chapter/10.1007/978-3-030-00928-1_37 \n\nSignificance\n- Investigation of effectiveness of recently proposed SSL techniques like MoCo is important research direction for medical imaging as i> often unlabeled data is vast especially for chest xrays ii> annotations are expensive as it requires access to experts and also has potential to be noisy\n\nQuestions/Comments/Clarifications:\n\n- It seems the plural effusion task was chosen because of prevalence. It would have been much more interesting to understand the benefits of MoCo pretraining for diseases with low prevalence. The data efficiency experiments would have been more natural there.\n- Not sure FixMatch is an instance of contrastive learning when compared to SimCLR, MoCo and PiRL as mentioned in the related work section.\n- Should be pointed out that while Kornblith et al (‘Do Imagenet Models Transfer better?’) and He at al (MoCo) performed experiments on natural image datasets, Raghu et al’ Transfusion work was focussed exclusively on medical imaging datasets.\n- SimCLR pretraining can also work with smaller batch sizes but would likely need to train longer.\n- The choice of using Imagenet initialized weights for MoCo pretraining does not seem well motivated. Raghu et al showed convergence gains for finetuning but not for contrastive learning. Some ablations to test this hypothesis further would have been nice.\n- Why were the images scaled down to 320x320? Were there any experiments performed by pretraining on larger resolutions as it seems to be important for medical imaging?\n- Was random rotation always fixed to 10 degrees? Why not higher?\n- Random cropping could still be beneficial data augmentation as long as the cropping % is kept low\n- Was the hyperparameter selection done on the small chexpert validation set or on a bigger subset?\n- Why was the network depth/width not varied? It is well known that higher depth and width can benefit contrastive pretraining. For example - see https://arxiv.org/abs/2006.10029\n- Why were the label fractions drawn a different number of times for the Shenzen dataset?\n- Finetuning using the same hyperparameters for the supervised setup does not seem correct. The initial weight scales are different and would call for different learning rates and decays. A hyperparameter sweep is necessary here to validate claims. \n- “These findings support the hypothesis that MoCo-representations are of superior quality, and is most apparent when labeled data is scarce.” -  this is a somewhat strong claim. From figure 1, it seems benefits are only statistically significant when label fraction used is 0.1%.\n- “These results demonstrate that MoCo-pretraining yields performance boosts for end-to-end training, and further substantiate the quality of the pretrained initialization, especially for smaller label fractions.” -  This is again a strong claim. Would remove “especially” as the results demonstrate benefits only for smaller data fractions.\n- The results on transfer to Shenzen also do not seem to be statistically significant for both linear evaluations and end to end finetuning.\n- For the different pretraining learning rates, what was the final contrastive learning task performance? Was there any sensitivity observed to learning rates for the core Chexpert task itself?  \n- Graphs demonstrating learning dynamics would have been nice for both pretraining and finetuning. Convergence speed could also have been used a valid measure of initialization quality.\n\nOverall this is a promising effort exploring the effectiveness of MoCo pretraining for Chest XRay interpretation. However, the current gains especially on larger data fractions and for end to fine tuning are not sufficient to make the fairly strong claims made in the paper. \n\nMore experiments/ablations including varying i> Network width/height ii> Hyperparameter sweeps for finetuning iii> studying benefits of MoCo pretraining for other diseases/pathologies with lower prevalence could make this paper stronger.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper for a clinical oriented workshop",
            "review": "Authors evaluate the impact of pre-training deep models for the task of Chest X-ray interpretation. Particularly, they investigate whether resorting to self-training (i.e,. Momentum contrast (MoCo)) can improve the initial representation of deep models. To evaluate their learning strategy, authors employ two public X-ray datasets, CheXpert and Shenzhen.\n\nStrengths:\n- The paper is well written.\n- The idea of unsupervisedly enhancing the feature representations is interesting/useful.\n- The performance of the proposed learning strategy outperforms the baseline. \n\nWeaknesses:\n- There is no methodological contribution. This paper basically evaluates whether a well-known self-learning strategy (i.e, Momentum contrast) can improve the performance of a mainstream task, in the context of X-ray interpretability. \n- Authors motivate this work by arguing that the applicability of self-training strategies in natural images is limited in X-ray images. Further, they also state that their application to medical imaging settings has been limited. I kindly disagree with this. There has been a large body of relevant literature that has applied self-training in the context of medical imaging. Just to give few examples, these include: solving a Rubik's cube [1,2], prediction of the position of anatomical patches [3], image reconstruction [4], or some others [5-6]. \n- Related to my previous comment, literature on self-supervised training for medical imaging is poorly-conducted. Authors have missed an extensive list of relevant recent works on this topic.\n- Furthermore, which is the interest of just evaluating MoCo instead of all the prior work on self-training?\n- As authors state in their conclusion, this work is the first to show the benefit of MoCo-pretraining across label fractions for chest X-ray interpretation. Thus, the impact on the machine learning community will be low, since they only evaluate one self-training strategy, among many, and in the niche of chest X-ray images.\n\nTherefore, I support my decision of rejecting this work based on the limited (if any) technical contribution and limited scope and interest of the findings.\n\nMinor:\n\n- This reference is repeated: Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020d.\n\n-------------------------\nReferences:\n\n- [1] Zhu et al. Rubik’s Cube+: A Self-supervised Feature Learning Framework for 3D Medical Image Analysis. MedIA'20.\n- [2] Zhuang et al. Self-supervised feature learning for 3D medical images by playing a rubik’s cube. MICCAI'19\n- [3] Bai et al. Self-supervised learning for cardiac mr image segmentation by anatomical position prediction. MICCAI'19.\n- [4] Chen et al Multi-task attention-based semi-supervised learning for medical image segmentation. MICCAI'19.\n- [5] Zhou et al. Models genesis: Generic autodidactic models for 3D medical image analysis. MICCAI'19.\n- [6]  Spitzer et al. Improving cytoarchitectonic segmentation of human brain areas with self-supervised siamese networks.MICCAI'18.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Technical contribution is very limited",
            "review": "Summary:\n--------------\nThe paper investigates the performance of a well-known contrastive learning algorithm (MoCo) on X-ray images classification when labeled data samples are limited. The paper contains experiments in different settings to demonstrate the performance which are: after MoCo pretraining on unlabeled CheXpert, 1)  fix the network + train a linear model on top w/ few labeled CheXpert, 2)  fine-tune whole model w/ few labeled CheXpert, 3) fix the network + train a linear model on top w/ few labeled Shenzhen, 4) fine-tune whole model w/ few labeled Shenzhen. These experiments are compared with the counterparts where MoCo pretrained weights are replaced with ImageNet pretrained weights. The conclusion of the experiments is that MoCo training learns better representations compared to ImageNet pretraining  that can be used for training on the same dataset and transferred to other datasets with limited labels.\n\nComments:\n----------------\n1 - My main concern is about the technical contribution of the paper. The paper contains investigations of an existing and well-known method (MoCo) on chest X-ray image classification rather than a technical contribution as also mentioned in the abstract. Moreover, there is not much modification on the existing method to apply on chest X-ray images. Therefore, I think the contribution of the paper is quite limited.\n\n2 - The only modification to MoCo is on the augmentation part when constructing positive and negative samples. The paper motivates this modification in Sec 3.2. by mentioning that the data augmentations used in the original MoCo may not be suitable for chest X-ray images since e.g. random crop and Gaussian blur could change disease label. This sounds reasonable to find data augmentation strategies that are suitable for chest X-ray images. However, this motivation is not supported with experimental analysis. I would expect seeing some experiments that shows how performance changes with different data augmentations.\n\n3 - The paper mentions that the choice of MoCo is due to computational resources since MoCo performs better with lower batch sizes compared to other well-known contrastive learning method SimClr. I think comparison with SimClr to justify the choice of MoCo would be very useful to show if this is the case for chest X-ray images as well. A recent paper [ref1] that uses SimClr for image segmentation reports that larger batch sizes lead to diminished performance on medical images where they achieve the best performance with batch size of 40. \n\n[ref 1] - Chaitanya et al. \"Contrastive learning of global and local features for medical image segmentation with limited annotations\"\n\n4 - There are two possible approaches that are used in semi-supervised learning setting which can be used during fine-tuning steps: 1) data augmentation, 2) pseudo-labeling. These methods can be used as complementary to pre-training approaches which can lead to further boost in the performance. I wonder if using these approaches would close the gap between MoCo and ImageNet pre-training. Especially, given that the performance improvement obtained by MoCo is marginal in the end-to-end fine-tuning experiments and the gap gets closer towards the 100% setting, using such additional tools can be crucial when comparing these pretraining strategies.\n\nOverall: \n-----------\nI think the lack of technical contribution is the main weakness of this paper. The modification that is done in the augmentation strategy is well-motivated but not justified. I believe that the experimental analysis are limited due to the points I mentioned in (3) and (4) above. Therefore, I suggest a reject for this paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}