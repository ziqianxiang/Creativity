{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies the problem of multi-agent meta-learning. It can be viewed as extending Al-Shedivat et al. (2018) by incorporating the dynamics of other agents. The reviewers praised clear writing and theory. There were two main concerns. The first concern is the novelty when compared to Al-Shedivat et al. (2018). The second concern are experiments, which could be more ambitious and are not always clearly described.\n\nThe reviews of this paper were borderline and this was not enough to get accepted."
    },
    "Reviews": [
        {
            "title": "solid paper, evaluation could be more ambitious",
            "review": "This paper considers the problem of meta-learning in a multi-agent environment under the assumptions that:\n* the learning agent's policy evolves over time as a function of the other agent's actions\n* the other agents' policies evolve potentially using the learning agent's actions.\nThe policy learning problem is assumed to be Markovian. The meta-learning problem is considered to be that\nof finding the best initial policy parameters (that will subsequently be evolved according to the learning dynamics) as to maximize the agent's cumulative marginal payoff.\n\nThe paper is very well written, easy to read and relatively straightforward in its exposition. I do not have any big remarks about writing except that the authors may want to rethink the term \"defective persona\" to avoid the weird double meaning. A sufficient amount of related work presented and the lineage of the ideas is traced convincingly well. \n\nThe main contribution of this paper is to extend the ideas of Al-Shedivat et al. in a way that exposes the other agent's learning dynamics to the policy optimization (as opposed to treating them as a non-stationarity). The policy gradient form corresponding to this setting is derived in Theorem 1. The approach is evaluated in a synthetic experiment using iterated games as well as a somewhat less synthetic experiment on a quarter-cheetah problem (each agent controls a leg of the half-cheetah).\n\nI think that while the paper is incremental, the point that is raised within is rather intriguing. If anything my main criticism is that the authors could have gone for a more challenging setting that iterated games. E.g. recent results (https://arxiv.org/pdf/1901.08654.pdf) indicate that in settings like collaborative exploration, being aware of the other player's learning dynamics is important for achieving a better outcome.  Perhaps the policy gradient approach can solve issues that cannot be addressed straightforwardly within the bandit framework. Another question is whether the approach can be used successfully to tune the inner learning process, e.g. by incorporating the policy gradient step size and other hyper-parameters into phi_0.\n\nOverall I think this is a solid paper, which would benefit significantly from more ambitious problems.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unclear experimental setup",
            "review": "This paper makes a full derivation of the meta-learning gradient estimation for multi-agent adaptation.\nThe resulting algorithm combines the meta-learning of the opponent's updates (existing in LOLA) and of oneself's futur updates (existing in Meta-PG).\n\nWhile the theoretical part of the paper is clear and well explained, the experimental setup is missing a lot of details to be interpreted:\n- In each experiment, it seems (but never explicitly formulated) that \"agent i\" (agent 1, since all experiments are involving 2 players) is doing the meta-learning algorithm (meta-MAPG, meta-PG or LOLA) while the other (agent 2) is a naive agent initialised with defective/cooperative policies.\n- In that case: how are naive agent updated? with simple policy-gradient?\n- How many lookahead are used (denoted by L in algorithms)?\n- Why did LOLA failed at learning to cooperate with the cooperative opponents? (it should have learned to cooperate, unless naive agents are still doing selfish PG updates --and in that case, meta-MAPG results are very impressive)?\n- Are the opponent's policies given or learned (i.e. with opponent modelling)? \n\nAlso, I would have been interesting to see an ablation study showing the importance of the \"own learning\" and \"peer learning\" terms in equation 6 (from the same implementation with fixed HP). Does the authors have tried it?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review 3",
            "review": "This paper studies meta-learning in multi-agent reinforcement learning. It proposes a meta multi-agent policy gradient method that considers the learning processes of other agents in the environment for fast adaptation. This method can be seen as a unified framework of previous methods (Al-Shedivat et al. (2018) and Foerster et al. (2018a)). The method outperforms previous methods in two matrix games and 2-agent HalfCheetah.\n\nPros:\n- The method is simple and well motivated. It additionally takes into consideration peer learning, comparing to Al-Shedivat et al. (2018).  \n- The method unifies the benefit of Al-Shedivat et al. (2018) and Foerster et al. (2018a). \n- The method greatly outperforms these two methods in two matrix games. \n\nCons:\n- Like LOLA, the method needs the access to policy parameters of other agents, while Al-Shedivat et al. (2018) do not. This may be impossible in mixed and competitive environments. How to deal with this? \n- In experiments, most questions are answered by the two matrix games. It is not fully convinced since the state space is very limited. Why not choose RoboSumo in Al-Shedivat et al. (2018) as an experiment? \n- For two matrix games, opponent policy is limited compared to complex environments, for example, halfcheetah. Although the out of distribution is tested, it is less informative for generalization. Why not test the out of distribution for halfcheetah? \n- \"The out of distribution has a smaller overlap between meta-train/val and meta-testing distribution.\" What exactly is the out of distribution?\n- The experimental results need to be more elaborated. Why do Meta-PG and LOLA perform similarly to REINFORCE?\n\n---\n**After rebuttal**\n\nThe responses address my main concerns. I have increased the score to 6. But, I also agree with other reviewers that the novelty of this paper is somewhat limited. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper addresses the non-stationary policies of other agents in MARL, by the idea of meta RL meta-trained on a distribution of other agents' policies.",
            "review": "This paper points out that a key challenge in MARL is the non-stationarity of other agents' policies, as opposed to previous papers which only account for non-stationarity of the environment. The paper extends (Al-Shedivat et al., 2018) by directly conditioning the meta-policy on a distribution of other agents' policies. In my opinion, the major contribution of this paper is a new multiagent meta learning theoretic framework that explicitly accounts  for the dynamics of all agents. \n\nStrengths of the paper:\n1) A new perspective in MARL that considers nonstationarity of MARL in terms of dynamics of the other agents' policies\n2) A new theoretically grounded algorithm that explicitly models the policy dynamics of all agents\n\nWeaknesses of the paper:\n1) Except for the new perspective of incorporating policy dynamics of other agents, the backbone of the paper (i.e., meta-RL based framework to mitigate non-stationarity of MARL) is inherently the same as (Al-Shedivat et al., 2018). The novelty is somewhat limited. \n2) In experiments the paper answers several questions that show the effectiveness of the new algorithm. However, this is subject to the two-agent setting. It is questionable whether such a framework can perform well in settings where there are multiple agents. \n\nQuestion: does the proposed framework generalize to >2 agents scenarios? if yes, what is the reason that the authors did not conduct empirical evaluations in these scenarios? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}