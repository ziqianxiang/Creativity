{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper considers the problem of searching over the joint space of hardware and neural architectures to trade-off accuracy and latency. \n\nReviewers raised some valid questions about the following aspects:\n1. Low technical novelty\n2. Prior work on hardware and neural architecture co-design, and closely related work are not addressed\n3. Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory)\n\nOne additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints. \n\nOverall, my assessment is that the paper requires more work before it is ready for publication."
    },
    "Reviews": [
        {
            "title": "Good exploration with weak technical contributions.",
            "review": "##########################################################################\n\nSummary:\n\nThe paper proposes an algorithm to search for hardware designs and neural architectures jointly.\nThe results show that joint optimization improves accuracy and latency, reduces the search samples.\n\n##########################################################################\n\nPros:\n\n- The paper introduces a new dimension, hardware design, to the neural architecture search domain.\n- The results show that joint optimization outperforms phased optimization.\n\n##########################################################################\n\nCons:\n\n- The used techniques are not novel\n- The hypothesis \"joint optimization is better than two-phased optimization\" seems obvious.\n\n##########################################################################\n\nOther comments:\n\n1. Can you describe more of the hardware. Is it a publicly accessable platform? How long does it take to switch between different configurations?\n2. Can you add wall-clock time or used computing resource in table 3?\n3. I also have concerns about the motivation. Is it typical to design an accelerator for just one network? I think an accelerator should be optimized for at least a range of neural networks.\n\ntypo: Section 3.1 \"The objective for NAHAS it to\" -> \"The objective for NAHAS is to\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "NAHAS review ",
            "review": "The authors describe an automated system for co-designing neural architectures and HW accelerators. The system is able to find the best solution under latency and chip-area constraints.  A highly parameterized (commercial) edge accelerator defines the hardware search space.  Results are compared to MnasNet, platform-aware NAS and EfficientNet.\n\nThis is an interesting area and the authors demonstrate clear advantages of their approach. \n\nA claim is made that \"although previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly.\". Does the work outlined in the \"co-design\" paragraph of section 2 not attempt to do this?  Some other examples that could have been discussed include: \n\n* “FPGA/DNN co-design: An efficient design methodology for IoT intelligence on the edge”, Hao et. al. DAC’19\n* “Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator”, Abdelfattah et al, DAC’20\n* When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design\nhttps://ieeexplore.ieee.org/abstract/document/8839421\n\nIt is a little unclear why the papers by Jiang and Yang and the papers above are dismissed? It would be good to understand better how the author's NAS approach improves on these previous works? \n\nLimited information is provided regarding the architecture of the accelerator. What fraction of the HAS search space is unavilable due to restrictions imposed by the compiler? Perhaps these design points are all uninteresting?\n\nIs there anything to learn by a discussion of the good hardware configurations that were found? Are these surprising or unexpected? How do they differ from the baseline configuration?\n\nThe work is interesting but the claimed contributions perhaps need to be clarrified. \n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "This paper proposes NAHAS for co-designing neural network architecture and hardware architecture. It shows performance improvements on ImageNet compared to previous hardware-aware NAS methods which only optimize the neural network architectures. \n\nPros:\n1. Co-designing neural network architecture and hardware architecture is a promising and important direction. \n2. The proposed method clearly outperforms hardware-aware NAS. \n\t\nCons:\n1. The technical contribution of this paper is limited. The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. The other components, such as the training method, search algorithm, and learning objectives, are borrowed from previous papers with minor modifications. \n\n2. Co-designing neural network architecture and hardware architecture is not new. Similar ideas have been explored in [1]. The authors should discuss the difference between this paper and [1]. \n\nIn summary, co-designing neural network architecture and hardware architecture is not new. The authors do not discuss the difference between this paper and [1]. Besides, I think the technical contribution of the proposed method is limited. Therefore, I recommend rejecting this submission. \n\n[1] Neural-Hardware Architecture Search, NeurIPS 2019 Workshop on Machine Learning for Systems.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper needs to improve its clarity",
            "review": "##########################################################################\n\nSummary:\n\nThe paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) \nfor software-hardware co-design. It uses PPO with joint search space: model accuracy and hardware constraints.\n\n##########################################################################\n\nReasons for score: \n\nThe paper claims it demonstrate effectiveness of hardware aware NAS for first time, which is dubious.\n\nThe paper should reference the industry-standard parametrized accelerator used. And reference or briefly present the in-house simulator.\n\nIn the beginning of section 3, it is unclear about the entire workflow. Explain the high-level workflow in Figure 1 caption.\n\nTested only on Imagenet, mobilnet and efficientnet. It didn't demonstrate that the method can be generalized across different the vertical stack as claimed in the introduction.\n\nIn 1. introduction, typo: Intel’s Nervana\n\n##########################################################################\n\nPros: \n\n- Demosntrated improved accuracy and latency upon other work\n\n##########################################################################\n\nCons: \n\n\n- The paper needs to improve its clarity by explaining the figure workflow in section 3.\n\n- It also needs to revise the claim that it demonstrate effectiveness of hardware aware NAS for first time. E.g: NSGA-NETmultiobjective\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}