{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes to use context-based metric learning, where an attention/Transformer-based mechanism is used to incorporate neighborhood information for deep learning-based metric learning. This was initially demonstrated on two simpler datasets, although larger ones were added during the rebuttal. On the whole, reviewers appreciated the simplicity and intuition behind the idea, but the consensus among all of the reviewers found several aspects lacking, including: 1) clarity of the descriptions in the paper, 2) novelty compared to existing work, especially that of Set Transformer for clustering, 3) lack of convincing results compared to baselines, or at least analysis/justification for negative results. While the reviewers appreciated the authors' rebuttal and experiments, it did not address many of these concerns. The idea is interesting and seems to hold some promise, so the authors are encouraged to refine these aspects in order to fully explore this idea and submit to a future venue. "
    },
    "Reviews": [
        {
            "title": "Recommendation to Reject",
            "review": " Summary:\nThe hypothesis of this paper is that learning a contextual metric (allowing pairwise distances to depend on the data) can improves clustering, and is motivated by two examples (Omniglot, intersecting circles).  The paper proposes a new method - Attention based clustering (ABC) that incorporates context to learn a metric in the form of an embedding and kernel similarity layer (predefined). The embedding layer uses repeated self attention blocks (SABs) from the transformer architecture and is theoretically shown to make the clusters more condensed. An off-the-shelf clustering algorithm (e.g. spectral clustering) is used to cluster the similarity matrix (number of clusters pre-specified or inferred). The experiments show favorable results on the toy dataset and are competitive with methods that use a prespecified clustering.\n\nReasons for score:\nThe paper proposes a novel combination of approaches to improve clustering using a supervised learning method. Given that ABCâ€™s results are surpassed for the Omniglot task by methods leveraging context for clustering, it would help to have a real world task where the approach taken (learning the similarity metric followed by off the shelf clustering) is the best.  This would help better motivate the proposed approach compared to previous approaches in the literature. It would also help to have further experiments on other datasets to increase the range of validity of the experiments.\n\nOther suggestions:\nIt could be interesting to visualize (say with T-SNE) what the geometry of the embeddings layer of the intersecting circles looks like. Further insight (theoretical or experimental) on the choice of kernel and/or other kernels would also be valuable.\n\nMinor:\nPerhaps displaying plots from the eigengap method of inference to validate that the inferred gap (number of clusters) is significant.\n\nClarity:\nSome details for the other methods compared to in the experiments would be helpful. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Attention based clustering",
            "review": "The authors propose a metric learning and clustering method based on the idea of learning the metric from the context. They use the self-attention block module of the multi-head attention based transformer to embed the data and learn a kernel using the ground truth labels.  \nThey demonstrate their idea on a toy dataset and present results on the Omniglot dataset. \nAlthough their results look reasonable, I am concerned that the idea seems very incremental and simply uses a combination of techniques that have been proposed in the literature. It is unclear to me what the authors' primary contribution is to the field of metric learning \nThe following are my other main concerns with this paper: \n1. It is unclear if the proposed ABC method uses all the ground truth labels of the dataset to train the similarity kernel. If so, this would make their method highly impractical. Why would one cluster data using a method which requires it to have all the cluster labels in advance? In the Introduction the author say \" We use ground truth labels, only in the form of pairwise constraints, to train a similarity kernel, making our approach an example of constrained clustering.\" Constrained clustering techniques are semi-supervised and do not require all of the data set to be labeled.  The authors need to clarify and further discuss this point.\n2. In Section 5.1 the authors do not cite or mention what pairwise metric learning method was used. Was any hyperparameter tuning or other optimizations done for this method? It is very surprising that a clustering done after learning a metric performs worse than out-of-the-box spectral clustering.\n3. The authors demonstrate their results only on one real data set and there is no clear discussion of the results. They need to provide more details on why they chose the three tasks (variable number of clusters known and unknown and fixed number of clusters). If the training is independent of the task then why do they observe a slightly different NMI for the three tasks? Wouldnt the same kernel be learnt for all the tasks and thereby the same number of clusters chosen as specified in Section 3.4? There is a need for clarity in the description of the experimental setup and results. I would also recommend adding a few more data sets to the results. \n4. The description of the Ominiglot data set can be improved. For instance, they use the words alphabet and language interchangeably. The Omniglot data set only refers to alphabets and the authors should be consistent. \n5. Why are different metrics used for the two data sets (Rand Index and NMI)? \n\nIn summary, I believe this idea is incremental and possibly has some potential, but the authors have not done justice to it in this paper. There are a lot of unclear aspects that need to be better explained. \n\n\n\nReview update after rebuttal: The authors have addressed some of my concerns in the rebuttal and the modified version of the paper. They have added results on more real datasets and explained some aspects that were unclear in the first version. However, I am still not convinced that this is a comprehensive enough contribution as it is right now. The results on the new datasets have, in fact, raised more questions. Their proposed method seems to perform worse than the baselines in some scenarios. Though this is not bad and it is important to show negative results, I did not see any discussion or insights into the poor performance. I would recommend the authors run some baselines themselves and compare rather than copying the results from the baseline papers. This would ensure that the experimental setup and environment are similar leading to a fairer comparison, and possibly some insight into their methods' performance. In summary, I still believe that this is an idea with potential but the authors still have ways to go in putting their idea clearly on paper and justifying it completely and comprehensively. I am sticking to the score I had assigned before. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper considers using the context in clustering which is an important problem however the proposed solution is not new. ",
            "review": "This paper proposes Attention-Based Clustering (ABC) that learns latent representations to adapt to context within an input set. They use ideas from the metric learning literature and the Siamese network on how to learn compatibility scores, and the transformer architecture and the Set Transformer on how to use context to make decisions.\n\nThe paper is well written and considers using the context in clustering which is an important problem however the proposed solution is not new. \n\n1- Transformer already being used as the Set Transformer to improve clustering (Lee et al., 2019b).  \n2- Also they mentioned that their model uses context to output pairwise similarities between the data points in the input set. They use the ground-truth labels and they train ABC in a supervised manner using those labels which is not realistic in clustering tasks!! \n3- The paper also claims that the ABC model is agnostic to the number of clusters. The known eigengap method (von Luxburg, 2007) is used to find the number of clusters.\n4- The paper only shows the result on OMNIGLOT and Olympic circles problems. Clustering is a very old problem and there are  known datasets from different domains to evaluate a new approach. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Missing details and needs improved evaluation",
            "review": "This paper proposes a method for producing representations for clustering that take into account global trends in the dataset, rather than considering each pair of instances in isolation. They claim to achieve competitive clustering performance on omniglot, which they attribute to the use of these contextualised embeddings. They also present a theoretical justification for using transformers in metric learning\n\n## Strengths\n* The need to make use of context is well motivated, and the authors show that their method does make use of context effectively.\n* Both the algorithmic and theoretical work described by the paper appear to be technically correct.\n\n## Weaknesses\n* Important details pertaining to both the proposed architecture and experimental evaluation are missing (see questions below).\n* The novelty seems limited, given that Lee et al. (2019a, 2019b) have already proposed using a set transformer to compute contextualised embeddings for clustering.\n* While technically correct, it is unclear to me how the theoretical analysis directly relates to the SAB components used in the architecture. In particular, assuming all within cluster weights are equal to $\\alpha$ or $\\beta$, while all between cluster weights are equal to $\\gamma$ does not seem like a realistic model of an SAB module.\n* The experimental evaluation could be improved. Currently only one real-world dataset is used in the evaluation, which makes it hard to say whether the findings will generalise to other situations.\n* The way the experiments on omniglot are set up does not match the motivation for the method. In particular, one would expect context to be useful when elements from different clusters can sometimes look very similar (e.g., letters from different alphabets that resemble each other). However, the omniglot experiments consider each alphabet in isolation, so this impies that context will be useful when individual letters in the same alphabet can sometimes look similar.\n\n## Questions\n* How does $\\mathcal{T}$ map from $\\mathbb{R}^{n \\times d_x}$ to $\\mathbb{R}^{n \\times d_z}$ when the input and output of each SAB is the same dimensionality? Does this mean $d_x = d_z$?\n* In the circles experiment, are multiple datasets (i.e., \"instances\") synthesised in order to train the embedding block?\n* It is said that the \"pairwise\" baseline, the embedding block is removed, but also that it is a metric learning method. What metric learning is going on here?\n* When would one expect additive attention to outperform multiplicative attention, and vice versa?\n\n## Other comments\n* Siamese networks were proposed by Bromley et al. (1994), not by Koch et al. (2015).\n\n## After Author Response\nMy concerns are only somewhat addressed by the authors' response. While the results on extra datasets are encouraging, I still think there is limited novelty in the proposed approach and some of the important details remain unclear. In particular, terminology seems to be used inconsistently, which results in ambiguity when describing variants of the model used in experiments.\n\nJane Bromley, et al. Signature verification using a \"siamese\" time delay neural network. NeurIPS, 1994.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}