{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of \"MIXUP TRAINING AS THE COMPLEXITY REDUCTION\"",
            "review": "The paper under review addresses the question of providing the theoretical underpinning of the \"mix-up\" approach to regularisation in two specific machine learning setups: linear classifiers and neural nets.  \n\nThe results are based on Rademacher complexity bounds. The results concerning linear classifiers are of interest and sheds more light on the reasons why the \"mix-up\" approach is improving the performance in classification. \n\nThe results of neural nets are based on a bound by Neyshabur et al. which seem a bit old to me. Showing that the \"mix-up\" strategy improves a complexity bound which is not tight in every situation, might not be seen as a satisfactory answer to the initial theoretical question. It might be the case that compression strategies could be put to work in order to obtain tighter and more useful bounds. \n\nAlso, it seems to me that mixing two images is a bit counterintuitive whereas taking an appropriate average on the manifold they are lying in could be much more relevant, as advocated in (Verma et al., 2019). Extending the study to the manifold case could be a real additional bonus for the present contribution.\n\nThe results are presented with great clarity and are worthwhile sharing with the community. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper in its current form is not clear to me in many places",
            "review": "The paper aims to explain why and when Mixup works from a statistical learning perspective, through quantifying the change of empirical Rademacher complexity before and after applying Mixup. The paper concludes that the effectiveness of Mixup’s generalization effect is due to the complexity reduction. \n\nThe paper provides a new and interesting perspective to look at Mixup’s regularization impact. The research topic is well motivated, highly relevant to the machine learning community, and important. But the paper in its current form is not clear to me in many places, as follows.\n\n1.\tThe paper proves that with ReLU activation functions, Mixup can reduce the gap between the average training loss and the testing loss. But that how such change will result in the major regularization effects of Mixup is not clear to me. That is, how such reduction linking to the improvement of predictive accuracy, model calibration, and robustness to adversarial attacks is not discussed or experimentally showed. \n\n2.\tThe paper claims in page6 that “the generalization performance is higher when the [mixing ratio  \\alpha in Mixup] is a small value” and show that empirically using  Cifar10. In this experiment, I sense an inconsistency between the reduction of the training loss and testing loss gap and the improvement of the predictive accuracy and calibration. In the original Mixup paper, the \\alpha was set as 1.0 to have the best accuracy and calibration results for both Cifar10 and Cifar100. I would appreciate that if the authors could share some insights here to discuss the potential inconsistency.\n\n3.\tThe paper is missing some important related works. The following two papers provides theoretically analysis on Mixup: one from the Adversrial training perspective and another leveraging Taylor expansion to approximate Mixup's loss. I think it would be useful to differ this work from them: \n“mixup as directional adversarial training”, Perrault-Archambault et al., 2019\n“On Mixup Regularization”, Carratino et al., 2020\n\n4.\tSome main claims are not supported by empirical evidence. Such as “the greater the variance in the distribution of the data, the higher effect of Mixup” in page4, and “mixup regularization for neural networks is more effective when there are outliers in the samples.” In page 5. I think it would significantly improve the paper if empirical evidence is provided. \n\n5.\tThe experiments setups are not clear to me. Such as what is SFHN in Appendix 1 (Table1)? Would it make sense to run the experiments multiple times and then provide the mean and deviation for each dataset/network architecture?\n\nMinor issues:\n1.\tIncorrect citation for the original Mixup method in section2.1.\n2.\tTypo in section2.1, “…layer if neural networks…” --> “…layer of neural network…”\n3.\tThe paper calls a sample B as a set of training samples in Section3, which might be a bit confusing. \n4.\tAlso, the related work section could be extended to include some recent advances along this research line. For example, PuzzleMix (Kim et al., ICML 2020) leverages saliency for Mixup. Also, Mixup has also been extended to nonlinear mixing (Non linear Mixup, Guo, AAAI 2020).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice contribution on part of parameter optimization, but the others just simple observations for investigating mixup",
            "review": "Summary\n\nThis paper try to theoritically explain why mixup regularization works. They investigate the effect of mixup in different situation (e.g., iterations, parameter of beta distribution), and also contribute how mixup searching for optimal parameters. \n\n\nComments\n\n1. Lots of observations for investigating the effects of mixup are provided, but seems to be a simple analysis.\n2. Nice contribution to connect mixup data distribution with bregman divergence to explain how mixup searchs the optimal parameters. In my humble opinion, this part is the first work to theoritically analyze between the parameters of beta distribution and generalization of mixup, which is a nice contribution to community of data augmentation.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well-motivated, but most probably contains some errors in its core theorems. Not good enough for publication at ICLR.",
            "review": "This paper aims to give a theoretical analysis for generalization under Mixup (Zhang et al., 2018), a popular regularization technique for deep learning frameworks. The main idea is to consider a number of classic complexity-based generalization bounds, for example: bounds based on Rademacher complexity, and then investigate how these complexity measures are changed after applying mixup.\n\nThe core idea is somehow interesting and paper is well-motivated. However, results and theoretical achievements are not interesting enough to be published at ICLR. In particular, \n1) authors have tried to simplify their analysis by fixing the mixing parameter $\\lambda$ to a constant value throughout the training stage. However, this strategy is in contrast with the nature of mixup regularization. In fact, the randomness within $\\lambda\\sim\\mathrm{Beta}\\left(\\alpha,\\alpha\\right)$ is a crucial factor for mixup regularization and forces the learner to NOT completely rely on a single atomic data point in the training set. By fixing $\\lambda$ to a constant value, again learner only experiences a finite number of data points (this time, a new set of data points different from the original training set) which contrasts the very fundamental purpose that mixup is proposed for.\n\n2) I am not sure about the mathematical correctness of some of the main Theorems in the paper. For example, in Theorem 3 and after Eq. (14), the chain of inequalities that include (15) and (16) do not seem correct. Assume we have $\\tilde{A}\\leq\\max_{i}\\left[A_i\\right]$ and $\\tilde{B}\\leq\\max_{i}\\left[B_i\\right]$. Then, we do not necessarily have $\\tilde{A}-\\tilde{B}\\leq\\max_{i}\\left[A_i-B_i\\right]$. Similar arguments might apply to Theorem 2 as well. In fact, authors' bounds for the maximum divergence between the original and altered Rademacher complexity values are not symmetric w.r.t. $\\lambda$ (for example, the bound given in Theorem 1 is of the form $\\frac{\\Lambda\\left(1-\\lambda\\right)}{\\sqrt{n}}$) while any possible bound concerning the generalization of mixup for constant coefficient $\\lambda$ is expected to behave similarly w.r.t. $\\lambda$ and $1-\\lambda$.\n\nBased on the above-mentioned shortcomings, I cannot recommend this paper for acceptance.\n\nThere are several minor issues or grammatical mistakes throughout the paper. Some are listed below:\n\nPage 2: receive\n\nPage 3: \"... holds for any $h \\in H$\" is misleading and undermines the true strength of inequalities (5) and (6).\nOne should say \"... holds for all $h \\in H$, simultaneously\" or \"... holds over $H$ uniformly\". \nPage 3: Proof of Theorem 1 is not needed here. It can be easily found in almost any basic learning theory handbook. Authors can just reference to an already-published version of theorem 1.\nPage 3: ...is data-dependent due to the {fact that} empirical Rademacher ...\nPage 3: Rademacher complexity-based bounds are known to be loose in many practical situations, specially for the important case of deep neural networks. In this regard, even providing a relatively smaller error upper-bound as an alternative, when both bounds are not tight might not be that much interesting. \nPage 3: \"The difference between the two Rademacher complexity\" -> please consider rephrasing.\nPage 3: Eq. (8) has some confusing notations. Please correct $S^2\\left\\Vert\\boldsymbol{x}\\right\\Vert_2$.\n\nPage 4: \"with the $L$ layer\" -> please consider rephrasing.\n\nPage 5: suppressed -> should be replaced with \"bounded\".\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}