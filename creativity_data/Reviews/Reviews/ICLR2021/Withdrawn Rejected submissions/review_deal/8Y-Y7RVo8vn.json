{
    "Decision": "",
    "Reviews": [
        {
            "title": "Proposes an approach to improve generalization; analysis needs more work and issues with experimental sections",
            "review": "Summary: This paper proposes an approach to improve the generalization performance of mini-batch SGD. The idea is simple: the mini-batch gradient is replaced by a convex sum of two mini-batch gradients where the mini-batches are sampled independently from each other. Using this \"noise-enhanced\" approach, the authors claim that this enables a way to monitor the intrinsic noise of SGD without scrutinizing on the batch-size and learning rate hyper-parameters. \n\nMy overall assessment of this paper is that it is not at the level of ICLR; while the proposed method seems like a neat idea, there is no theoretical justifications as well as some serious flaws with the experimental validation. \n\nDetailed Comments:\n\n- The paper appears to be somewhat unfinished; ends at 6.5 pages?\n\n- The sentence at the end of Section 2 - \"noise enhancement is also applicable to other variants of SGD like Adam\". I am really not sure about this- there are many works in the last few years concerning about the distinctions between Adam and SGD. These two algorithms behave very differently from each other; for both optimization and generalization performance. With regards to this paper, the intrinsic noise of SGD in Eq. 4 is very different from the intrinsic noise of Adam. I have derived the intrinsic noise of Adam in my own research (following the steps in the derivation of Eq. 4 + a lot more tedious algebraic manipulations) and the final form is very different from Eq. 4. Hence, I am skeptical of how the analysis in the remainder of Section 2, which are completely SGD specific, carry over to Adam. \n\n- Since the authors are proposing a new algorithm, it would be better if the authors have a formal \"Algorithm box\" describing their algorithm in Section 2 with proper pseudo-code\n\n-  In C1 description of the network architecture at the beginning of Section 3, the authors mention that ghost-batch normalization is used; I believe that this is the one that is defined in [1`]. It should be properly introduced and defined in the paper. GBN is typically used in the large-batch regime; I am wondering at which batch-sizes the authors are applying this? Also, is GBN not used in architectures F1, C2?\n\n- The experiments are all done in Adam and not SGD? I believe that this leads to a serious gap between the experiments in Section 3 and all of the analysis and discussions in the rest of the paper. Almost all of the papers in the literature which study SGD noise, batch-size/learning rate effects on generalization conduct their experiments with SGD. \n\n- Since this paper proposes to modify mini-batch SGD with a noise term to improve generalization, I would encourage the authors to do some empirical comparisons with other papers which also have done this; for example, [2] and [3].\n\nReferences:\n\n[1] Hoffer, Elad, Itay Hubara, and Daniel Soudry. \"Train longer, generalize better: closing the generalization gap in large batch training of neural networks.\" Advances in Neural Information Processing Systems. 2017.\n\n[2] Wen, Yeming, et al. \"Interplay between optimization and generalization of stochastic gradient descent with covariance noise.\" arXiv preprint arXiv:1902.08234 (2019).\n\n[3] Zhu, Zhanxing, et al. \"The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects.\" arXiv preprint arXiv:1803.00195 (2018).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but needs better experiments",
            "review": "In this paper, the authors propose a method for enhancing the noise in an SGD update to improve generalization, without changing the learning rate or the batch size hyperparameters. Experiments are run on three datasets to show improvement in generalization performance when changing the hyperparameter controlling the amount of noise.\n\nI find the main idea interesting and potentially useful because of its simplicity. The paper is also nicely written and easy to read. The main limitation of the paper is its experimental analysis. Only small models are considered that do not get up to high accuracy for datasets like CIFAR-10 and CIFAR-100. So the applicability of the idea on bigger models is not explored. In addition, a standard training setup is not used for these models. Instead of training for a fixed number of epochs with a preset learning rate schedule determined by the number of steps, the authors define two loss values L* and L**, which are seemingly randomly chosen, to determine when the learning rate is decreased and when training stops. So it is not clear to me whether the proposed idea works in general even on the small models considered.\n\nI would encourage the authors to do a more thorough experimental study of their idea to show it robustly works across different models and datasets under standard training setups. Please also see a list of my questions below, which would be good to clarify.\n\nA few important questions:\n- It is not immediately clear to me that the analysis shown here also extends to Adam. Can the authors elaborate on it a bit more and show if the same derivations hold or not? This seems especially relevant given that all experiments are run with Adam.\n- Is the initial learning rate changed as the batch size is increased in the experiments? If it is, how is it changed? This seems important to consider since the effect of alpha would depend on this.\n- The analysis depends on the minibatches being randomly sampled from the dataset. What sampling strategy was used for the experiments? In practice, random orderings are used instead of random sampling, particularly as the dataset size increases. Does this affect the results?\n\nAdditional questions about the experiments that would be good to clarify:\n- Is the learning rate kept fixed as alpha is changed? How is this initial learning rate chosen?\n- There seems to be an optimal alpha (alpha = 2) for the experiments shown in figure 1. However, for the rest of the experiments alpha = 1.5 was used. Was this value optimal for the other problems? It would have helped to have done a sweep of alpha across different problems to show how much the optimal alpha varied.\n- Another relatively minor point: standard regularization techniques like data augmentation or weight decay are not used in the experiments. Does the effect of the noise enhancement still exist when using weight decay?\n\nOther minor comments:\n- In the paper, it is mentioned that the covariance of the SGD noise is proportional to eta^2/B (please add a citation when this statement is first made in the introduction). From what I understand of the prior literature, the particular scaling relationship between the learning rate and the batch size depends on the structure of the noise that is assumed, and empirically, the eta/B scaling seems to hold up more consistently in the literature. Given that the authors do not really use this scaling relationship in their method (except to make the point that the noise can be controlled by changing either the learning rate or the batch size), it is probably best to mention both scaling laws in the introduction and elsewhere with the relevant citations.\n- It would have been interesting to explicitly show in the experiments how different the performance of using a batch size of B_eff = B/(alpha^2 + (1-alpha)^2) is compared to using noise enhancement using parameter alpha.\n- Typo right above equation 2: \"In the SGD\" -> In SGD",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns with novelty of proposed method",
            "review": "This paper proposes \"noise enhancement\", an algorithm intended to fix generalization issues with large mini-batch sizes by adding additional mean-zero SGD noise to the update computed using an additional batch of samples. Experimental results show that adding this noise to the optimization procedure improves the final test performance. The authors intend to use this method in cases when it is hard to increase the learning rate, and decreasing the batch size would result in reduced parallelization. \n\nPositives:\n- This method is clearly motivated, as a simple calculation shows that adding the noise would cause the covariance to scale up proportionally. Prior work which studies on the SGD noise covariance implies that this would lead to better generalization. \n\nNegatives:\n- My main concern regards novelty of the proposed method, as experiments with artificially adding noise have been done a lot in the past literature [1, 2, and possibly others]. These papers perform extremely similar experiments based on adding some mean-zero noise to the updates to improve final test performance. \n- Some details about the computational cost are unclear. For example, the paper plots convergence time v.s. batch size, but how is \"convergence\" measured? Also how does the noise enhancement affect the wall-clock time? \n- More justification is required that simply increasing the learning rate eta is not sufficient, which was claimed on the first page \"However, when Î· becomes too large, the training dynamics often becomes\nunstable and the training fails.\" In particular, prior work [3] shows that the learning rate can be scaled linearly with batch size for ImageNet, up to quite large batch sizes. \n\nIn summary, the novelty of the proposed method is the main reason for my rating. \n\n[1] Wen et. al 2020. An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise.\n[2] Wei et. al 2020. The Implicit and Explicit Regularization Effects of Dropout. \n[3] Goyal et. al 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting trick, but lack of large scale verifications",
            "review": "This paper follows the line of study on the regularization effect of SGD noise, and proposes an interesting trick to tune the noise scale. The trick is very simple, to enlarge the noise scale, do an extrapolate between gradients of two mini-batches.  Yet it has advantages that it does not involve the batch size and learning rate, which can potentially benefit large batch training. \n\nPros:\n- The paper proposes a very simple and potentially useful trick for tuning SGD noise magnitude by an extrapolating of two mini-batches. The trick can be useful because (1) it is computational feasible; (2) it keeps the particular structure of SGD noise; (3) it serves as an additional hyperparameter, and does not affect learning rate and batch size. (3) is rather important since enlarging LR can hurt convergence, and reducing batch size can reduce the parallelism. From this point of view, I think this trick really worths to explore in depth.\n\nCons:\n- With the above being said, the empirical studies in this version are really disappointing. They only test CIFAR-10 / FashionMNIST with ConvNet / VGG. As an empirical paper, those are far from convincing. I would expect at least (1) ResNet and (2) ImageNet.\n- Page 2, footnote 2. I do not think there is any complication for measuring the magnitude of SGD noise. The difference is simply because of the different definitions of SGD noise:  in stochastic modified equations, they took an $\\sqrt{\\eta}$ as the step size, and treated the rest as the noise.\n- Page 4, experiment setups. You discuss SGD in the whole paper, but do experiments with Adam??? Please provide formal experiments at least ablation studies for SGD. These are the basic requirements for science.\n- Several claims are too vague and lack of backups, especially in the Sec. 4. Please do not make statements without justifications. Moreover, I think Wu et. al 2019 do claim the class of noise is not important for regularization, which makes me confusing why you cite this one in the second to the last paragraph.\n\n\nOverall:\nI think the paper has potential, given a complete and rigorous large-scale empirical study. However, the current version is too weak for an acceptance. I suggest the authors to re-write the paper with less vague statements as well.\n\n\n \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}