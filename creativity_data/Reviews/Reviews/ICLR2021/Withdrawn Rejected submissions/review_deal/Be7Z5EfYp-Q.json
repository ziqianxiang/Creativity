{
    "Decision": "",
    "Reviews": [
        {
            "title": "The Poisson likelihood estimator is classical method for image recovery (contribution is not very significant)",
            "review": "Summary: The paper presents a method to recover signal in a coherent diffraction imaging setup, where Fourier intensity of an unknown object and additive known reference is observed at the detector. In the low-light situation, measurement noise follows Poisson statistics. To recover signal from the measurements, the authors propose to use a Poisson maximum likelihood estimator. To further regularize the problem, the authors propose to use a deep decoder as an image prior. The paper presents simulation results on some sample images. \n\nNovelty and significance: It is a well-known fact in optics and signal processing that noise statistics in low light follow poisson distribution. The log-likelihood estimator for Poisson measurements is a standard optimization problem that is solved in such settings. In that respect I do not find the contributions of this paper novel or significant for publication. The paper uses a deep decoder as an image prior in the same Poisson estimator, which provides slight improvement in terms of reconstruction error. I do not consider that a significant or novel contribution of this paper, because similar ideas have been explored in other inverse problems. \n\nCorrectness: The authors assume Poisson noise in the measurements, but then they compare their method with mean squared error-based optimization problems, which is not fair. Mean squared error-based optimization and weiner filter assume that noise is additive and Gaussian. I give that authors credit that they have discussed the relation between Poisson and Gaussian noise (i.e., in high photon count, the Poisson noise can be approximated with Gaussian), but including the comparisons and making claims that the proposed method performs better than classical methods is misleading. Poisson likelihood-based reconstruction is a classical method as well, and that is exactly what this paper is proposing. \n\nQuality: The paper is well written and I find it easy to read and understand. The experimental results are not impressive or surprising for the reasons I described above. \n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Seems that ICLR is not the most appropriate venue for this topic",
            "review": "The paper proposes a Poisson maximum likelihood framework to  the holographic phase retrieval problem in the low-photon regime using untrained deep image prior.\n\n\n1) The holographic phase retrieval problem studied in the paper might not be of great interest to the community of ICLR, so it is probably better to submit the paper to other venues.\n2) In section 4, it would make the paper more self-contained if the author could list the size/statistics of the datasets used in the paper\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Paper about more realistic phase retrieval ",
            "review": "Summary: \nThis paper studies the problem of phase retrieval. In contrast to previous work, this paper proposes to model the observations via a Poisson process, incorporates a beamstop, and also considers a variant with a known reference (holography). The resulting log-likelihood is tractable and optimized via gradient descent either in pixel space or in the parameter space of a deep image prior.\n\nStrengths:\n+ The addition of Poisson process and beamstop make the setting more realistic\n+ The two algorithms (pixels, deep-decoder) show qualitatively different behavior. Deep-decoder can work with less data, but saturates, while the pixel space optimization requires more data but does not saturate.\n+ Holography is a neat idea for improving performance of inverse problems. Although mature in the phase retrieval literature, this idea seems relatively under-explored in the recent neural-network-based approaches for inverse problems.\n\nConcerns / weaknesses:\n- Most plots in the main paper are created for single images making them and any interpretations only applicable to that image. It would be more useful to show plots averaged over a many images so that more robust comparisons between different approaches can be made.\n- All results presented in the paper use simulated measurements. Given that the goal of this paper is to make phase retrieval practically applicable, the paper would be significantly stronger if the benefits of the proposed approach were to be demonstrated on a dataset with real measurements.\n- At high noise levels, the qualitative and quantitative results seem to disagree. Why is this the case? Perhaps it would be useful to look at a perceptual similarity metric in addition to the L2 reconstruction error?\n- In the text, it is discussed why baselines are sensitive to the separation parameter and why the proposed methods should not be . However, the empirical results show the opposite (Fig 6). Why is this the case?\n- It would be nice to compare the results under a realistic (Poisson) measurement model with or without holography to quantify the benefits of holography for phase retrieval under the various approaches.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Realistic setting, questions on experimental validation",
            "review": "### Summary:\n\nThis paper proposes a method for solving the holographic phase retrieval problem (similar to classical phase retrieval but with a known reference) under realistic conditions of high noise levels and unavailable low-frequency data. \n\nThe method incorporates the physical forward model into a Poisson likelihood objective function that allows a deep neural network deep decoder prior. Through a variety of experiments the authors compare their method against baselines and investigate when the neural network prior is particularly helpful.\n\n\n########################################\n\n### Strong points:\n\nThe paper considers a ractical setting which entails a beamstop and Poisson model. Moreover when learning is used, no training data is required which is useful in scientific imaging scenarios where training data may not be available.\n\nThe method is able to incorporate the physical forward model even when learning is used.\n\nThe Poisson model is experimentally compared against the more widely used MSE to justify the development of this Poisson-based method for handling low photon count scenarios.\n\nThe authors point out where learning is helpful and where it is not helpful via a variety of experiments.\n\n\n########################################\n\n### Weak points:\n\nAs the Poisson model and deep decoder are not new on their own, a thorough experimental evaluation is important:\n\n\\- Most of the results are for the three sample images. As the method performance varies depending on the image (as acknowledged by the authors in the paper), the conclusions would be strengthened by evaluating average performance over a larger set of images / signals. While Figure 15 shows this for one set of experiments, this is not done for the beamstop and separation experiments.\n\n\\- Following on, looking at generalization to other signals / images is especially critical as a separate deep decoder architecture (Table 1) was tuned for each of these three images. No validation set was used. In a practical setting we will not be able to tune the architecture according to the image that we wish to recover.\n\n\\- The authors vary the ill-posedness of the problem by varying the number of photons which is helpful. Are there also results on the performance of the method for other variations in ill-posedness (such as varying the oversampling ratio)?\n\nIs there an error in Figure 15? The errors get worse as the photon count on the horizontal axis increases.\n\nIn Figure 4 and the other Figures for this type of experiment, why do the Wiener and inverse filtering errors start to fall as the beamstop area is increasing and the problem gets harder? Furthermore, there are sharp jumps in the HolOpt methods after a certain beamstop size when the number of photons is higher. Is there some critical threshold?\n\n\n########################################\n\n### Recommendation:\n\nOverall I lean towards rejection. I like the more realistic setting as well and the examination of where the deep decoder is particularly helpful. However, the experimental validation should be strengthened to better understand the proposed method.\n\n\n########################################\n\n### Additional questions and clarifications that will help assessment:\n\nPlease address and clarify my above questions and queries.\n\nOne of the listed contributions (number ii) in Section 1.3 is the efficient auto-differentiation implementation. Please can the authors expand on the specific contributions and challenges that were overcome to do this?\n\nA single reference is provided in section 1.1 for the beamstop. Please can the authors discuss and motivate the scenarios where a beamstop is used in more detail?\n\n\n########################################\n\n### Additional feedback that does not necessarily impact recommendation:\n\nWhat is C in equation 2 / how is it determined for the experiments?\n\nHow is the random initialization for the deep decoder input done?\n\n[Minor] The font size on the graph axes and some figures is quite small.\n\n[Minor] Some references contain URLs (PyTorch one even extends beyond the margins) and others do not.\n\n[Minor] For completeness it would be nice to state the MSE loss used in the experiments.\n\n[Typo] Euclidian on page 13 section A.2\n\n\n######### After author response #########\n\nI thank the authors for their responses and updates to the manuscript which clarified some of my queries. However, I still feel that there are gaps in the experimental evaluation which impacts the understanding of the method from a machine learning perspective (e.g.  architecture / iterations is varied across noise levels according to Table 1). I retain my original score for this reason.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}