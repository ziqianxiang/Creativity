{
    "Decision": "",
    "Reviews": [
        {
            "title": "The proposed task is interesting, but more literature survey is required",
            "review": "- Summary:\nThis paper proposes a new task for training a model for out-of-distribution (OOD) classification and clustering. The proposed task is called class homogeneity, where it determines whether given k samples are from the same class or not. Experimental results show that the model trained on the proposed task works better than simple baselines.\n\n- Reasons for score:\n1. I think literature search is not sufficiently done. To me, this paper aims at improving transfer learning by representation learning (by casting the problem in a different form). However, no transfer or representation learning works are discussed or compared in this work. I recommend to survey works on transfer learning, metric learning [Movshovitz-Attias et al.; Sohn], and self-supervised learning [Chen et al.; He et al.].\n2. I think it is hard to say that a random subset of classes is OOD of the others on ImageNet, because some classes are fine-grained, like dogs. I recommend to consider hierarchy of classes, for example, dogs as in-distribution and others as OOD. Also, the purpose of this work (identifying unseen classes) looks similar to some prior works leveraging the hierarchy of classes like [Lee et al.; Deng et al.], so I recommend to review and discuss them.\n\n- Other questions:\n3. What hyperparameters are tuned with the validation dataset?\n4. Does the model work if it is trained from scratch on the proposed task?\n\n[Chen et al.] A simple framework for contrastive learning of visual representations. In ICML, 2020.\n[He et al.] Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n[Lee et al.] Hierarchical novelty detection for visual object recognition. In CVPR, 2018.\n[Movshovitz-Attias et al.] No fuss distance metric learning using proxies. In ICCV, 2017.\n[Sohn] Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS, 2016.\n[Deng et al.] Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition. In CVPR, 2012.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Out-of-Distribution Classification and Clustering",
            "review": "The paper proposes an out-of-distribution (OOD) classification and clustering method. It can detect the OOD class when trained on the subset of data and also can detect the OOD dataset. The proposed model is simple and can be applied for the large scale dataset.\n\nComment: \n1: The proposed approach is scalable and works well for the out-of-distribution class and dataset, while many works focus on detecting on the out-of-distribution dataset. Prediction of the OOD class is a more challenging setting.  \n\n2: How inference is performed it is not clear to me. Once the model is trained, is it, we will take samples of all the training class with OOD/Non-OOD samples then pass to the ResNet-18 then transformer and transformer will predict all samples are from the same class or different class? It looks like too much costly. Please clarify that.\n\n3: The author has not compared with any OOD (class/dataset) paper. I suggest to the author; please compare your result with the paper [a] [b] [c]. If suggested papers are not scalable at least compare on the small scale dataset with the OOD class and dataset. It isn't easy to quantify the paper without compared with the strong baseline.\n\n[a] Detecting Out-of-Distribution Examples with Gram Matrices, ICML-20\n[b] Robust Out-of-distribution Detection for Neural Networks, ArXiv-20\n[c] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, NeurIPS-18\n\n4: How the transformer is used in the proposed approach? There is no detail provided in the paper; I believe the result reproducibility will be a challenging task. The author does not provide the code. Also, I request the author please provide brief details about the training setup and inference procedure and everything necessary to reproduce the result in the appendix of the paper.\n\n5: Why transformer is used? What are the advantage and disadvantage of using a transformer? What is the alternate of the transformer?\n\nI request to the author; please address the raised concern.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments are not sufficient - Presentation needs improvement",
            "review": "SUMMARY:\n----------------\nNeural networks are not capable of predicting class labels when given an input from an unseen class. The paper defines this problem as out-of-distribution classification and propose a new task to train neural networks to solve this problem. It also extend the method for clustering of unseen data (out-of-distribution clustering). The paper contains 3 different experiments to demonstrate the performance: 1) train a Resnet+transformer on 800 Imagenet classes on the proposed homogenity task and perform predictions on all 1000 classes. 2) use the same Imagenet model and classify MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 datasets, respectively. 3) Cluster unseen classes of Imagenet.\n\nTo my understanding, the proposed method for out-of-distribution classification method works as follows: a network M is trained by giving a set of 25 images as input and binary label as output where binary label indicate whether the 25 images belongs to the same class or not (this is called class homogeneity task). Once this network is trained, it is evaluated by taking 24 samples from all classes (including the unseen ones?) and the test image x' to be classified. The class label giving the highest score is selected as the class label of x'.\n\nCOMMENTS:\n-----------------\n1 - I think one of the major problems of the paper is about the clarity of the presentation. I wrote a short summary of my understanding above to double check with the authors that I understood correctly, because it seems there are some missing details.\n1.1 - In Sec. 3.2., it is stated that Y also includes classes not used during training. This statement is ambiguous since the set of classes not trained on is infinite. Therefore, I assume that the method requires availability of some samples from unseen classes. I think this part should be written in a more clear way.\n1.2. - If my assumption in 1.1. is true, how many samples does the method need from each unseen class? \n1.3. - In Sec. 5.2., the paper states that \"results were averaged by classifying one thousand images from each class\". Does this mean that the method requires 1000 images from each class including the unseen ones? What if there are less samples available?\n1.4. - Each sample during training use 25 images. How these images are given as input to the network? Is it given like 25-channel image?\n\n2 - Out-of-distribution clustering method is not very clear to me either. \n2.1. - In Sec 5.3., L is chosen as empty initially. But L is never updated in the description. Is it updated with the samples added to the new cluster Q?\n2.2. - Why do you need a different variant of class homogeneity task for clustering experiments? In the beginning of page 4, it is stated that both variant could be used for both classification and clustering. Using two different variants contradicts with this statement.\n\n3 - I think the paper requires more experiments to validate the claims and demonstrate the performance.\n3.1. - One, and crucial, hyperparameter of the method is k. How does the performance changes as a function of k?\n3.2. - The paper describes quite a lot of work in the related work section. However, experiments does not include comparisons with any of them. I understand that the other methods requires re-training whereas the proposed method does not. However, I think comparisons with at least a few of them are still required to position the proposed method in the relevant literature. The comparisons only include nearest neighborhood-based methods which I believe are not sufficient.\n3.3. - Why using a modified version of Imagenet in the experiments but not the original Imagenet?\n3.4. - In Sec. A.1., the paper states that batch size of 2048 is used during training but smaller batch sizes work as well. However, there is no experiments with smaller batch sizes. \n3.5. - What is the computation time of the proposed method? This is especially crucial for the clustering experiments since it requires comparing each pair to be clustered.\n\nOVERALL:\n--------------\nI found the introduced new task is quite interesting and there might be some potential of using it for out-of-distribution classification. However, I believe, the paper is not ready for publication yet with its current form due to the points I mentioned above. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: OUT-OF-DISTRIBUTION CLASSIFICATION AND CLUSTERING",
            "review": "##########################################################################\nSummary: \n\nThe paper proposes a technique that can be used to classify and cluster out-of-distribution samples.\n\n##########################################################################\n\nReasons for score: \n \nOverall, the paper in its current form is below the acceptance threshold. The proposed idea looks interesting, however, the paper has serious drawbacks, some of which are detailed in the comments. \n##########################################################################\n\n\nPros:\n\n1. Overall, the paper is well written, easy to follow and understandable. \n\n2. The idea of not retraining to make predictions on unseen classes is interesting.\n\n\nCons:\n\n1. The justification of positioning the paper in the body of the literature is reasonably convincing.\n\n2. “many different work aim to make this ...” doesn’t flow well, please modify\n\n3. My major concern with Figure 1, ignoring a few classes of ImageNet in training and making the predictions is not really a convincing case to consider it as out-of-distribution. Because, the ignored images still belong to the same distribution as the ones in the training set with 800 classes/999 classes. Instead, the convincing out-of-distribution example would be to train on MNIST and test on CIFAR, like Hendrycks and Gimpel, 2016. Therefore, picking up the features of those unseen classes is not surprising. \n\n4. In section 3.1, the sentence “... whether the samples are from the same distribution/class or not.” the same distribution or not makes sense,  but a prediction on the same class or not, what does that mean?\n\n5. Again, I have serious reservations on the very definitions described in the paper for out-of-distribution classification and clustering. \n\n6. More on clustering, the clusterer seeing labels during training? In clustering we never have labels.\n\n7. “... we think new methods to avoid overfitting training classes will have to be developed.” Dropout, ensembles, mixup, augmix, and so on are the methods. In fact, Mixup, Augmix already proved to be better on out-of-distribution.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}