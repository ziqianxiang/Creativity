{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper is concerned with sequence segmentation. The authors introduce a framework which they call 'lexical unit analysis' - a neural network is used to score spans and then dynamic programming is used to find the best scoring overall segmentation. The authors present extensive experiments on various Chinese NLP tasks, obtaining better results than the systems they compare to.\n\nReviewers raised concerns, including about novelty. In my view, beyond beating the state of the baselines on the chosen tasks, it is hard to extract an actionable insight or novel conceptual understanding. Therefore, the paper is not recommended for acceptance in its current form."
    },
    "Reviews": [
        {
            "title": "This paper presents a new sequential segmentation method by making all possible spans in a sentence as candidate segments and applies dynamic programming to find the best segmentation.  The method is tested with various sequential tagging problems and achieved state-of-the-art results for most of the problems.",
            "review": "The paper proposes a new algorithm for sentence segmentation which can be applied to various sequential tagging problems.\nThe motivation and the description of the algorithm are clearly given, and the proposed method achieved state-of-the-art results for most of the problems and datasets.\n\nThe proposed method tries to find all possible segments in the given input sequence, to estimate the scores of the segments using pre-trained BERT representations, and to find the best sequence of segments using the dynamic programming algorithms.  The proposed method is general enough to apply to various sequential tagging problems and natural language sentence analysis.\n\nWhile the proposed method looks new to apply to the sequential tagging problems in natural language processing, the dynamic programming approach to sequential analysis is a well-known method in the speech recognition community where a sequence of phonemes are segmented into a word sequence.  Also, a similar method has been applied to the segmentation of character sequences into word sequences for the languages that have no delimiters between words, such as Chinese and Japanese.   In these views, the novelty of the paper is not high.   On the contrary, the application of the BERT-based representation to the sequence segmentation tasks such as sentence segmentation and sequential labelling may be new, and the finding that this method can attain a state-of-the-art performance in those problems could be worth reporting.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not a clear contribution",
            "review": "The paper is well-written, easy to follow and clear. However, the novelty and main contribution of the paper is not clear. The authors used a scoring model to score the composition of each segment, as well as the probability of having a specific label for the segment. The BERT language model is used in the paper to encode the input sequence. The training part is a more like a supervised training and a dynamic programming (DP) approach is used for inference. It is not clear how DP contributes to the success of the model, as the scores for segments are derived during the training (which seems most of the success is coming from the labeled data (i.e. supervised training) and BERT encoding). One other thing about formatting and citing references, some of the references are published in conference proceedings, not sure why authors cited their arxiv version.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Segmenting Natural Language Sentences via Lexical Unit Analysis",
            "review": "This paper presents a method called LUA, Lexical Unit Analysis for general segmentation tasks. LUA scores all the valid segmentation of a sequence and uses Dynamic Programming to find the segmentation with the highest score. In addition, LUA can incorporate labeling of the segment as an additional component for span labeling tasks. \n\nPros:\n1. LUA overcomes the shortages of sequence labeling as a token-based tagging method and span-based models as well, by treating them separately.\n2. The decomposition of scoring label and scoring span allows the pre-computation of the maximum label score for each span, reducing the complexity.\n3. This method achieve the state of the art performance on 13 out of 15 data sets empirically.\n\nCons:\n1. The novelty is incremental, as the idea of calculating span-based score and label-based score with DP has been used widely in constituent parsing, which applies interval DP in a similar way. Also check semi-CRF model (Sunita Sarawagi and William W. Cohen, 2004).\n2. The way of using neural model to calculate the span-based scoring seems to be very arbitrary (Eq3), without any explanation why it is designed in this way.\n3. Label correlations are used to mimic correlation scoring, however the transition between spans are not explicitly modeled.\n\nQuestions:\n1. LUA is only used in inference stage. Do you think by using LUA in training as well, though slower, the performance can be further improved?\n2. Do you have any intuition of why designing the scoring function (Eq3) in that way?",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}