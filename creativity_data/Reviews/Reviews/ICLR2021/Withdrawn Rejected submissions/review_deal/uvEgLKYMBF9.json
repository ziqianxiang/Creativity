{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper develops a smoothing procedure to avoid the problem of posterior collapse in VAEs. The method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers' concerns. However, there is one remaining issue that would require additional discussion. As identified by Reviewer 1, the analysis in Section 3 is only valid when the number of layers is 2. Above that value, \"it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically\". Thus, the scope of the analysis in Section 3 deserves further discussion. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue."
    },
    "Reviews": [
        {
            "title": "Good theoretical analysis; doubts on practical performance ",
            "review": "This paper proposes a Hermite variational auto-encoder which use Ornstein Uhlenbeck Semi-group to p(z_i|z_i+1) which i denotes the latent layer number. It has clear theoretical inspiration and had solid analysis on variance reduction. \n\nPros:\nQuality: The paper's generic theoretical motivation and analysis is with high quality. \nClarify: The paper's presentation is clear. \nOriginality: This paper provides a new perspective and used mathematically tools of Hermite expansion etc to inspire and proposed new method for variance reduction which prevent dying unity problem in Hierarchical VAE. Although motived by advanced tools, but the application of the method in vanilla version of hierarchical VAE (in term of implementation) seems very simple. Thus, the method looks easy to adopt. \n\nCons and questions:\nSignificance:\n1) Does the method work for vanilla VAE with only one level of z? It seems that it is only applicable to the hierarchical version as the operator is applied in  p(z_i|z_i+1) and if it is one level, it lost the point due to single Normal prior.  This may limit the application impact as VAE is much more widely adapted in different applications comparing to hierarchical VAE. \n2) I am not sure making unit not dying at all is desired (such as shown in last column of table one or Figure 3. Being Bayesian with the prior, there is a natural model selection behavior (implicit Occam's Razor) , thus, behavor such as the method with active unints (40,40,40,24) in table one may not be desired and rather a bit weird as only the last layer have dying units. Behavior such as VAE+KL (2,3, 11, 37) looks more natural to me as simpler model is needed in high hierarchy. \n3) Experiments only compared to Ladder VAE in number of dying unit but not in term of ELBO for performance. As LVAE is one of the most known work in this domain and also mentioned first in the related work section), this is weird. In LVAE paper, the MNIST performance is reported as -81-82 while in the paper it is about -85 which is significantly worse. Although there is a chance due to minior setting differences, I doubt the method's performance can match LVAE.  (Again with 2), I don't think that puring comparing the number of units without reporting performance makes sense). \n4) in term of performance of ELBO, most of the time, it does not match simple KL annealing either. \n5) there are more highly related work anaylsis the variance-bias trade off such as Tighter Variational Bounds are Not Necessarily Better are not discussed in the paper. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good theoretical contribution, however the experiments are not sufficient",
            "review": "1. Summary\nThis paper studies the training of deep hierarchical VAEs and focuses on the problem of posterior collapse. It is argued that reducing the variance of the gradient estimate may help to overcome posterior collapse. The authors focus on reducing the variance of the functions parameterizing the variational distribution of each layer using a layer-wise smoothing operator based on the Ornstein-Uhlenbeck semigroup (parameterized by a parameter $\\rho$). The operator requires additional Monte-Carlo samples. The authors provide an analytical analysis of bias and variance. Last they train multiple VAEs models, measure the posterior collapse and observe a phase transition behaviour depending on the parameter $\\rho$.\n\n2. a Strong Points\nThis paper introduced a theoretically grounded solution to the problem of posterior collapse. In particular, it is discussed that variance may be an issue. Great efforts were invested to study the behaviour of the Hermite VAE in theoretical terms and the authors provide analytical results on the Bias and Variance for this estimator. \n\n2. b Weak Points\n* Complexity \nIn the main text, it is written that \"*experiments show that 5 or 10 samples suffice*\". This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed, nor it is studied empirically. Given 5 MC samples, I interpret that HVAE is 5 times more expensive than other approaches -- please clarify this point. \n* Empirical study of the variance \nThe problem of the variance is discussed in the paper but left apart in the experimental section. I would expect the authors to measure the variance (and/or SNR) of the gradients for the HVAE objective, the VAE, for advanced estimators such as STL and DReG. A study is required to corroborate the claim that reducing variance overcomes posterior collapse.\n* Experiments on posterior collapse\nI am surprised to see that none of the existing methods (KL warmup and freebits) allows overcoming posterior collapse (Figure 1). At least using the right amount of freebits should improve the results (the number of freebits is not reported). Furthermore, the authors should report the KL divergence in the benchmark experiment.\n* Experimental protocol \nI don't understand why VAE models trained in section 5 only have 2 layers whereas HVAE uses 4 layers: this is not a fair comparison. Furthermore, LVAE should be studied on the basis of posterior collapse -- not only in terms of likelihood.\n\n3. Recommendation\nUnfortunately, based on the current form of the paper, I recommend rejecting this paper.\n\n4. Recommendation Arguments \nDespite the good theoretical contributions, I do not find the experimental section to be strong enough to support the claims. In particular, the cost induced by the additional MC samples is not discussed and methods are hence not compared on the same basis. \n\n5. Questions to the Author\n- What is the complexity of HVAE? Do the VAE models use multiple MC samples as well?\n- Why using only 2 layers for the VAE models?\n- How are the freebits and KL-warmup applied in figure 1? \n\n6. Feedback \nYour work is very relevant and the theoretical insights are very interesting, this work would greatly benefit from an improved experimental section.\n\nIn the first page, two typos:\n-  you defined $q(z | x)$ and not $q(x, z)$\n- The KL divergence in equation 1 should depend on $q(z_i | z_{i-1})$ and $p(z_i | z_{i+1})$\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Idea is sensible, but analysis is incomplete",
            "review": "Post-rebuttal update\n---------\n\nThank you for your response.  Now I understand that the algorithm works by smoothing the Gaussian parameters $\\mu_i,\\sigma_i$ w.r.t. the centered Gaussian rv (as described in my last reply, second part of bullet point (1)), so my original concern regarding the bias _in the Gaussian parameters_ does not hold.  However, I still cannot recommend acceptance at this point, because of a newly discovered issue in the theoretical analysis:\n\nThe analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers.  The text only considers two layers of stochastic latents and the KL part of ELBO, but in the deeper case, the smoothing of $\\mu_i(z_{i+1})$ will additionally have influence on the layers below $i$, through the nonlinear functions $\\mu_{i'},\\sigma_{i'}$ for $i'<i$.  More concretely, consider the following scenario: $\\mu_i(z_{i+1})\\equiv z_{i+1}, \\sigma_i(z_{i+1})\\equiv \\epsilon$ which is very small. Further assume that $z_{i+1}$ is high-dimensional and approximately follows $\\mathcal{N}(0,I)$, so $\\\\|z_i\\\\|_2 = \\\\|\\mu_i(z_{i+1})+\\sigma_i\\varepsilon_i\\\\|_2 \\approx \\\\| z_{i+1} \\\\|_2 > 100$ with probability $1-\\epsilon_1$, where $\\epsilon_1$ is also very small. In this case, it is possible to achieve a low KL in the original ELBO, by using a $\\mu_{i-1}$ which only has sensible values in the region $B := \\\\{z_i: \\\\|z_i\\\\|>100\\\\}$; in the complement set $B^c$, $\\mu_{i-1}$ can be \"arbitrarily\" bad so long as its impact on the ELBO does not outweigh $\\epsilon_1$, the probability its input falls there. However, in the smoothed estimator with $\\rho=0$, the input to $\\mu_{i-1}$ only have norm $O_p(\\sigma_i(z_{i+1}))=O_p(\\epsilon)$, so the value of $\\mu_{i-1}$ on $B^c$ will have a far higher impact, easily exceeding the original by $O(1/\\epsilon_1)$.  To summarize, *it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically*.  Moreover, even in the shallow case, $z_i$ will be fed into a final decoder block to generate the reconstruction image, so a similar issue exists, although it will be in the reconstruction likelihood part of the ELBO as opposed to the KL part.\n\nA less important issue is that parts of the analysis are written in a confusing way.  Apart from the abuse of notation $U_\\rho$ which leads to my original confusion, in Section 3 the $\\hat{\\mu}_p$'s should have a suffix of $z_1$, to signify the fact that they are coefficients of a function that depends on $z_1$ (see the last response from he authors).  Also it is unclear to me why there is no mention of $\\mu_p^4$, in the analysis of the variance of an estimator for $\\mu_p^2$.  But given the aforementioned issue, I don't think it is necessary to look further into this case.\n\n\nSummary\n-------\n\nThis work proposes to smooth the mean and variance parameters in the decoder of hierarchical VAEs with the O-U process. It is shown that the smoothing procedure reduces variance of the ELBO, alleviates posterior collapse, and improves on model likelihood on CIFAR-10 under a fixed number-of-parameter budget.\n\nThe idea to investigate the impact of ELBO variance in hierarchical VAE performance is sensible, and the experiments seem to show improvements. However, I have concerns regarding the theoretical claims, and the empirical results also seem to need clarification.\n\nMajor Concerns\n--------------\n\n- The claim that the smoothing doesn't change the expectation (of functions acting on the latents) doesn't seem correct. Prop.1 and 2 only holds when the expectation is taken wrt the standard normal distribution, while all but the top-level latents (i.e., $z_i$ for $i<L$) come from a mixture of Gaussian. Intuitively it also seems incorrect: what if $\\rho=0$?\n\n- The variance analysis works by assuming $\\sigma_q$, the decoder variance, is constant. This ignores the problem of unbounded likelihood [1], where posterior variance goes to zero, thus driving the ELBO and its variance to infinity. It would be helpful to include a plot of the decoder variances in the most realistic model, to see if this issue is relevant in modern hierarchical VAEs (and thus whether the analysis here provides a complete picture).\n\n- The conclusion of the analysis does not seem helpful: the bias is $O(1-\\rho^2)$ and the variance is $O(\\rho^2)$, so it is unclear from the bound whether there will be a $\\rho$ that decreases the overall MSE.\n\nMinor \n--------------\n\n- It is worth mentioning that there are several types of posterior collapse and not all of them are undesirable [2]: sometimes it is superfluous units rightfully pruned [3, 4]. This also implies that the number of active units is not a good measure of model quality; it is helpful to include reconstruction error in Section 5.1.\n\n- The observed phase transition of KLD connects to the fact that ELBO-trained VAE acts like a thresholding operator; see [2].\n\n- Why didn't Table 3 mention NVAE [5] and IAF-VAE [6], both of which have better BPD values? Seeing where those models are on the #parameters-BPD curve helps to put the results here in perspective.\n\nReferences\n----------\n\n[1]: Mattei and Frellsen, Leveraging the Exact Likelihood of Deep Latent Variable Models, in NeurIPS 18.\n\n[2]: Dai et al, The Usual Suspects? Reassessing Blame for VAE Posterior Collapse, in ICML 20.\n\n[3]: Lucas et al, Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse, in NeurIPS 19.\n\n[4]: Dai and Wipf, Diagnosing and Enhancing VAE Models, in ICLR 19.\n\n[5]: Vahdat and Kautz, NVAE: A Deep Hierarchical Variational Autoencoder, in NeurIPS 20.\n\n[6]: Kingma et al, Improved variational inference with inverse autoregressive flow, in ICLR 16.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}