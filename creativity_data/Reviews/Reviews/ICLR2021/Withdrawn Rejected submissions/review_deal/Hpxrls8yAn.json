{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary:\n\nThe paper proposes a new method CLION - aimed at improving the performance of model-based RL from pixels on the DeepMind Control (DMControl) benchmarks. A leading model-based RL Dreamer is used as a baseline. The paper proposes an improvement over Dreamer by trying to use augmented latent states and rollouts as a consequence of these perturbations. In order to deal with the potential issues that may arise out of these perturbations such as irrelevant rollouts, they have a GAN-like discriminator with a heuristic evaluator scheme for what maybe considered useful trajectories. Additionally, they use a data augmentation consistency loss in the latent space. The imagined rollouts are combined with TD(lambda) for the policy learning. The paper shows better results than Dreamer and DrQ on the data-efficiency benchmarks of DMControl.\n\nStrong Points:\n\nImprovement in model-based RL from pixels.\nWorks on an important problem - data-efficient continuous control from pixels - important for robot learning.\nResults on well established and calibrated benchmarks.\nWritten well - was easy to get a hang of what's going on.\nResults look good.\nBenchmarks on robustness to rotation - good idea.\nWeak points:\n\nThe method looks complicated and has many moving pieces. Would be nice to understand why and how the different components are useful. For eg - what happens if you don't do the trajectory filtering with the evaluator- how much drop in performance, how much is the pixel space augmentation constraint helping stand-alone (without other pieces), how much is the latent space augmentation helping stand-alone (without other pieces, how much does TD(lambda) help, etc How were hyperparameter choices made, etc. - important to know.\nHow many seeds were used for experimental results? Currently not mentioned. Please do report results over 5 seeds.\nMethod seems fairly non-trivial to be able to reimplement - would code be provided by authors?\nsome of the bold numbers on tables should be updated - ex 942 +/- 71 (DrQ), 956 +/- 124 (CLION) - not clear then which is better. that's also why more seeds are important. Similarly, 660 +/- 96 (DrQ), 631 +/- 21 (CLION).\nCould add performance uncertainty benchmarks for DrQ/RAD as well.\n\n\nRecommendation: Weak Reject - I am happy to revise based on author feedback / updated draft with ablations / more details of experiments.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "Summary:\nThe paper suggests strengthening model-based RL agents that leverage stochastic state-space models by perturbing initial rollout states (confusingly called 'memories' in the paper), and using a heuristic to compute a weighting score. \n\nOverall, the architecture is similar to the Dreamer, but the proposed changes do seem to improve performance over Dreamer. \n\nHowever, I don't believe the paper is publishable in its current state:\n- It is confusingly written (as an example, there is a mention of perturbing input observation (section 2.2), but the way the inputs are perturbed does not seem explicited; this idea seems is present in figure 2, but missing from Algorithm 1). It is possible the perturbations are similar to DrQ, in which case a worry is whether the algorithm is essentially a simple combination of Dreamer+DrQ.\n\nIn terms of writing, the algorithm often makes mention of memories, virtual episodic memories, for tasks that require very little memory (and the connection to episodic memory seems rather nebulous).\n\n- There is little if no attempts at understanding the source of increased performance (is it more robust representation? Better understanding of model uncertainty? Exploration? Model robustness?), no ablation between the various additions (observation noise, initial rollout noise, weighting of trajectories). Many choices appear very ad-hoc (gaussian noise on input state). The most interesting bit is the attempt at understanding model uncertainty through weighting trajectories using a trajectory evaluator, though again, seems somewhat ad-hoc. \n\nThe theoretical analysis stays very close to Janner et. al, and in the current state of writing, is a bit disconnected from the rest of the paper and seemed unconvincing.\n\nThe paper appears to have interesting ideas. A significant rewrite would help clarify what the contribution of the paper precisely is.\n\n\nMinor: \n-numerous typos throughout. \n- Differentiating through the model is done through numerous papers, see e.g.\nModel-augmented actor-critic, backpropagating through paths, \nImagined Value Gradients, \nand the Dreamer paper itself\n- The paper lacks citations in model-based RL, and gives too much credit to the Dreamer paper (which is not the only paper on image tasks, even when limiting to continuous control tasks)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting results | Sufficient Ablation Study | Curious about  Fair comparison to Dreamer",
            "review": "##########################################################################\n\nSummary:\n\nThe paper is based on Dreamer which is the current state of the art for model-based reinforcement learning in visual control tasks. Superficially, Dreamer involves dynamics and behavior learning. The current agent interacts with the environment to collect data. This data is used to learn the dynamics and the agent is trained over the trajectories generated by the learned dynamics which comprises of latent states instead of actual observations.  Also, the state representation comprises of stochastic and deterministic components. \nThe author argues the imagination trajectories lack diversity due to the deterministic state component. This makes agent fragile to changes and uncertainties of the dynamics. They address this by augmenting the trajectories generated by the dynamics with a set of gaussian noises added to the deterministic latent state component. At the same time, they constrain the decoded observations from deterministic and noise-induced-deterministic-states to be similar.  The agent learns in imagination from trajectories generated by with/without noise-induced states. This augmentation induces diversity in the trajectory and thereby makes the agent more robust leading to better sample efficiency.\n\nActor-Critic:  Also, this noise could make a trajectory unreliable, thereby, they also re-weight the updates to the policy/value based on the reliability of the trajectory. The reliability metric was learned by a GAN like framework which distinguishes between real and noise induced trajectories. Other than this, they simply use lambda-value return for value regression and analytic gradients for policy update as done by Dreamer.\n\nThe authors showcase sample efficiency and performance of their method over 6 dm-control tasks.\n \n##########################################################################\n\nReasons for score: \n\nOverall, I vote for accepting.  I feel the experiments and ablations shown by the authors justify their approach.  At the same time, I find the work to be a blend of recent advancements in RL.\n\n##########################################################################\n\nPros: \n\n- The data-augmentation has shown to improve generality and performance in model-free methods where augmentation is done over input observations. However, The author identifies the need to have augmentation in the latent state space over which policy is learnt. \n- The author also shows the importance of regulated noise with the help of constraints on decoders.\n- The author also identifies the need to check for unreliability of trajectory and induce necessary weight metrics to balance it. \n- The results shown for both with/without uncertain environment looks convincing.\n\n#########################################################\n\nQuestions:\n\nMy only major concern is the number of updates made to policy in either case of Dreamer and CLION? Are they equal? I wonder if the number of updates done to the policy in CLION is more due to augmentation. If so, Itâ€™s important to ensure that both cases receive equal number of updates for a fair comparison. This could be done by just sampling more imagination trajectories in the case of Dreamer.\n\nSuggestion: You may want to try out noise as a function of time-step. Further in the rollout, less noise must be induced.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Writing needs improvement, no clear intuition for empirical performance",
            "review": "## Summary\nThe paper proposes an augmentation method for recurrent latent space models used in model-based RL approaches. Concretely, random noise is added to the latent state of the model before rolling it out during policy learning. From reading the paper it was not apparent to me what the exact motivation of this is, but likely it should help improve the robustness of the model-based RL approach to noise in the input observations.\n\n## Strengths\n- the approach is evaluated on multiple representative environments of the DeepMind Control Suite and seems to be compare to state-of-the-art methods (even though the evaluation setting with input noise is different from what these methods were designed for)\n- the related work section captures a good amount of relevant work, particularly for the model-based RL literature\n\n## Weaknesses\n\n- **unclear intuition of the proposed method**: the paper claims that a trained Recurrent State Space Model (RSSM) does not capture enough diversity in the rollouts it is generating, therefore the paper adds noise to the latent states to increase the diversity. However, the method then uses a GAN discriminator to downweight the contribution of trajectories that are outside the support of the training data. Yet, a well-trained RSSM should already capture all trajectories that are within the support of the training data, therefore it is unclear which benefit the proposed method is supposed to bring.\n-  **unclear main contribution**: while reading the paper I was under the impression that the addition of noise augmentation to the latent state of the RSSM is the main contribution of the paper, but Section 3.3 states that the main difference to the Dreamer approach of Hafner et al. 2019 is the introduction of the discriminator-based reliability weight estimation. Therefore, it remains unclear to me what the claimed main contribution of the paper is.\n- **use case for method not clear**:  the concept of environment uncertainty is first mentioned in the experimental section, but it seems that this is the setting that the approach is designed for and that it actually performs better in than the baselines (in the \"normal\" evaluation setting most of the gains are within the error bounds). If the approach is indeed designed for environments with observation uncertainty this should be mentioned much earlier and other baselines that can improve robustness to such uncertainties (like simple data augmentation in image space for Dreamer) should be compared to.\n- **writing is not clear**: it is often hard to follow the argumentation. As a result I had trouble following the core ideas of the paper and proposed method. Some examples: the introduction does not clearly explain the shortcomings of prior work and it is therefore also hard to follow the description of how the proposed method is trying to address the shortcomings. Another example: section 2.2 talks about an augmented input observation without ever mentioning that augmentations should be performed outside the model's latent space.\n\n## Questions\n- from Fig.3 I understand that augmentation is performed when computing target values for a given state; however, since the augmentation changes the latent representation of this state, how can we use rollouts based off of this altered latent state to compute values for the original state?\n- why are augmentations only performed on the initial hidden states of the model, not the intermediate transition states?\n- are there any augmentations applied to the input images for CLION (since Fig2 includes augmented input observations)? If so, what augmentations are performed?\n\n## Suggestions to improve the paper\n- improve the clarity of writing, in particular more clearly detail the motivation, main use case, contribution and intuition of the method\n- add a model-based baseline that performs simple image augmentations (like done for DRQ) to see the importance / benefit of performing the augmentation in latent space in CLION\n- if possible, perform experiments that analyze *why* CLION gets better empirical performance than the baselines (eg try to analyze when the baselines fail but CLION works, or plot performance as a function of input disturbance etc)\n- if the method is directly building onto Dreamer it might be good to have an early section explaining the Dreamer method for completeness (that might also help to explain its shortcomings)\n- it is unclear what fig 1(b) is demonstrating: since there seems to be an explicit constraint that the output of the augmented branch should match the output of the non-augmented branch it is not surprising that the reconstructed images match the originals -- but it is also unclear what use the augmentation has if the output is constrained to match the non-augmented output. --> explain this more clearly\n\n## Overall Recommendation\nIn its current form, the exposition of ideas in the paper lacks substantial clarity. I found it very hard to even grasp the basic, high-level ideas of the paper which made it impossible to adequately judge its novelty or the thoroughness of experiments. Since I believe that a thorough restructuring of the paper is necessary to clearly explain its ideas, I think it requires another cycle of reviews and I can therefore not recommend it for acceptance in its current form.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}