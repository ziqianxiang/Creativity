{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper suggests a simple variant for BERT training that improves classification for smaller training samples.  So it has a very specific applicability unlike other published variants which generally improve a broad range of tasks.  The variant adds a self-supervision classification task based on clustering.  Experiments are done but it only shows improvement for small training sizes.\n\nAnonReviewer4 suggested a BOW experiment/baseline which was done by the authors in an updated version.  This confirmed the authors line.  AnonReviewer3 asked for computational details, which were added.  AnonReviewer1 lists a number of limitations which the authors need to address and rephrase the statements in their paper.\n\nSo it is publishable work, but somewhat marginal due to its specialised nature and thus rejected."
    },
    "Reviews": [
        {
            "title": "Simple BoW related baselines are missing",
            "review": "The paper proposes to use a simple intermediate task - clustering - to improve the generalization ability of BERT in low resource text classification settings. The idea is simple. We can perform clustering using BoW representations to generate pseudo labels, which will be used to fine-tune the pre-trained BERT model. The results show that the proposed method is effective in topical classification problems.\n\nPros:\n - the idea is very simple and we can easily adapt the idea to different classification tasks.\n- the results are promising, especially on topical classification problems.\n\ncons:\n- My main doubt about this paper is that I think the improvements in the topical classification problems were rooted in the fact that the authors used the \"BoW representations\" to conduct the clustering. The BoW representations are known to be more effective for topical classification problems compared to BERT-based sentence representations. So what the authors did maybe actually infuse/distill the BoW knowledge into the BERT representations.\n- I suggest the authors include a set of BoW representation based baselines. E.g., simple BoW based SVM model, average of GloVe embeddings of BoW representations based classification.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-written, comprehensive experiments and analyses. Will be applicable for any topical dataset without a plenty of labels.",
            "review": "This paper proposed an inter-training framework of BERT, an unsupervised training method applied between the conventional pre-training method via Masked Language Models and the fine-tuning method by using labeled data in the target task. The proposed inter-training methods are an unsupervised method by first running clustering with BoW features for the target task and then fine-tuning by training BERT over pseudo-labels generated by clustering results. This inter-training framework significantly improves prediction accuracies especially when the task is topical, i.e., the task to classify texts based on a high-level distinction related to what the text is about, and the labeled data is scarce.\n\nThis paper is well-written. The motivation is reasonable and the proposed methods make sense. Experiments are comprehensive and analyses are well designed. The contribution of this paper is obviously above the ICLR borderline.\n\nI think it is better if the authors mention the computational cost of the inter-training framework in detail. While the computational cost of the clustering is negligible, the authors did not mention the computational cost of the fine-tuning in the inter-training framework. Compared with the final fine-tuning process, it is better to clarify how much additional cost we need to perform the inter-training method. Moreover, it is better if the authors mention the effect of the size of unlabeled data in the inter-training method. Is more unlabeled data improve target metrics a lot, or are hundreds of texts sufficient for the inter-training method. It is better if the authors show the plot by changing the size of the unlabeled data used for the inter-training method.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good way to fine tune BERT for topical text classification with fewer labels.",
            "review": "Quality\n\nThe paper proposed using unsupervised clusters to help boost BERT performance in text classification tasks that lack labels. The method make good sense. Experiments well designed and showed clear advantage.\n\nClarity\n\nThe paper is well written. I have no trouble following all details.\n\nOriginality\n\nIt might be the first for BERT, but using unsupervised clustering to help classification seems an old topic in NLP. \n\nSignificance\n\nIt's less significant. First, it limits to topical classification. Second, it fits the scenario with more data for clustering (thus not truly low resource) but just too few labels. Performance gain diminishes quickly as number of labels get over a few hundreds.\n\nWhy not higher rating? The method seems too intuitive from NLP research. It's about BERT but the idea is not that novel. It's only helping topical cases with quick diminishing gains. \n\nWhy not reject? Paper is well written, evaluation done thoroughly and showed good improvement. Authors analyzed the result in a useful way.\n\nDetailed comments:\n1) The authors should be careful about the \"low resource\" claim, as we still need fair amount of data, unlabeled, for the clustering.\n2) Did you use training set for clustering, without considering their labels? Seems so but not clarified in the paper.\n3) Why use accuracy, not P/R as the metric? are all test sets balanced?\n4) For Figure 3, consider using shapes or in-figure labels. Color dots are hard to read when we have many labels, and not friendly to color-blind people either.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper that lacks context and baselines",
            "review": "This paper proposes a novel domain/task adaptation procedure for BERT-style language models (LMs). Inspired by computer vision, the authors propose to specialize LMs to a particular domain and task with an intermediate clustering task. A large unlabeled dataset is clustered to generate pseudo-labels then a BERT model is specialized to predict the cluster id of each example. More precisely, the authors use GloVe embeddings (bag-of-word representations) and the sIB clustering algorithm with a constant number of clusters (50). \n\nThe additional clustering task is shown to help for low-resource topic classification. In the paper setup, the whole dataset (up to 15000 examples) is used for clustering (without the labels), while the fine-tuning phase relies on a small subset of the training data. The authors show nice gains when using 64 examples for fine-tuning. Graphs show that the gains are lower as more fine-tuning data is used, and usually disappear with more that 192 to 384 examples. The authors go on showing that their approach works best when the cluster ids correlate well with the original labels.\n\nOverall the paper is clear and well-written. It proposes an interesting and, to the best of my knowledge, novel idea.\n\nConcerns:\n- Some claims feel overly broad. I believe the claims should be limited to improving low-resource/few-shot *topic* classification tasks, not text classification in general, which is especially unclear in the title and abstract.\n- While the domain transfer literature is well introduced, no low-resource/few-shot literature is mentioned, making it hard to relate the results to similar research (although to be fair, it is true that CV has much more literature on this topic than NLP).\n- There is little discussion on the motivation and practicality of this approach, which makes the results seem almost anecdotic. If the motivation is to improve few-shot learning, the relevant literature should be introduced and compared against. If the motivation is topic classification, the practicality of this work seems limited: since a larger dataset is available for clustering, manually labeling a few hundred examples (human expert, MTurk) feels more promising. For example, BERT with 256 examples yields better results on most datasets than BERT:CLUST with 64 examples.\n- Several ideas mentioned as future work seem relatively straightforward to try and could have been interesting for inclusion in this paper.\n\nQuestions:\n- The correlation between cluster ids and task labels seems critical for the approach to work. Did the authors try using a number of clusters equal to the number of labels? If so, how did that change the results?\n- The vocabulary size used for clustering (10000 word stems) seems low. What are the reasons for choosing that number? Also, is the vocabulary task-dependent?\n- Why did the authors use a BoW approach for clustering? For example, why not embed each example with BERT (which should capture topics better than GloVe-BoW) using the output [CLS] vector?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}