{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose to use counterfactual (a.k.a as contrastive, as they do not account for causal mechanism) explanations  to explain the errors of an already trained predictive model with images as input data. To this end the authors rely on the manipulation of the latent space of  a VAE with disentangled representations. In general the idea is simple (and based on approaches from prior works) and extends work on counterfactual explanations which have been broadly studied in other domains like decision making where the input data have often semantic meaning (in contrast with the pixel of an image).  While the technical contribution is quite limited, I believe that the general approach of the paper is interesting. \n\nHowever, even after the rebuttal and reading the updated version, it is still unclear what exactly means key concepts for the paper like trivial and actionable, and more importantly how to use the proposed approach in practice, beyond checking/correcting for potential gender bias in  the data (given that you have access to gender information). In particular, it is not clear how you measure \"non-triviality\" for the predictive task when you do not have additional knowledge (like the gender) or when the disentangled latent representation do not correspond to semantical features (which as far as I understand they do not need to).  Similarly, while actionability and diversity have been broadly discussed in the decision making domain, it is again not clear what an actionable feature means here to me. Is it just that you can perturb the latent space? \n\nFurthermore, I believe it is worth exploring the connection to approaches for adversarial examples. As it has already discussed in the literature, in terms of formulation,  counterfactual explanations resemble the problem of adversarial examples, but it seems substantially different semantically. At times when reading the paper, it feels that it is indeed more related to adversarial examples than to counterfactual explanations, as the explainability part seems quite superficial. Thus, I would encourage the authors to better position their paper. \n\nIn summary, I believe that the paper requires further work before being ready for publication. In particular, the paper would significantly from: i) a better positioning of paper with respect to the literature; ii) formally introduce  key concepts like actionability, diversity and triviality, explaining what they mean in this context, and how to measure them; and more importantly, iii)  explaining how the proposed approach (which by the way involves training a generative model)  can be used in general to 'understand' a model. On a final note, I believe that the paper would benefit from from bringing back to the main body of the paper the experiments that were moved to the appendix during the rebuttal. \n"
    },
    "Reviews": [
        {
            "title": "interesting but execution needs substantial work",
            "review": "Summary: The authors propose interpreting the decision of a black-box (BB) image classifier using diverse counterfactual explanations. The proposed model consists of a pre-trained β-TCVAE, which learns to extract a disentangled latent representation for the input image. To generate explanations for a given image, the model optimizes to find n latent perturbations. Each decoded output from β-TCVAE is similar to the original image and produces a desired outcome from the BB classifier. To ensure the diversity among the n latent perturbations, the model minimizes the pairwise similarity loss between the latent perturbations. The model further performs spectral clustering to partition the latent space into different attributes. Thus, at inference time, for the same input image, multiple counterfactual images can be generated as explanations by changing different dimensions of the latent space. The experiments demonstrate the realistic quality of the explanations and their ability to discover bias in the BB classifier.  \n\n\n•\tThe idea of generating multiple images as counterfactual explanations is interesting. If multiple explanations differ in multiple attributes, it can help identify un-wanted correlation or biases in the model and datasets. \n•\tThe paper lacks detailed experiments to quantify the importance of diverse explanations. The experiment should quantify what attributes, apart from trivial counterfactual changes, differ across the explanation images. The successful explanation experiment confirms differences in the explanation images, but ``what\" is different is not apparent.\n•\tThe author's definition of a valuable explanation is misleading. In the introduction, the authors describe a valuable explanation as an explanation that is proximal, i.e., it is much similar to the input image, and actionable i.e., it can be derived by performing feasible changes to the input image. In the experiment section on \"Beyond trivial explanations,\" authors defined a valuable explanation as the one for which the BB model and human or its proxy (an oracle network) have different outcomes. For the two definitions to be consistent, the authors assume that the oracle network can uniquely identify feasible features in an image and consider such features in its classification decision. \n•\tFeasible features in an image are hard to define. For example, adding/removing sunglasses is a trivial example of a feasible change. While changing some pixels around the face's lips, to add/remove the smile is a more complex change whose feasibility is hard to define. As the change in the region around the lips may add/remove an expression from a face that is hard to quantify. Also, the authors didn't perform any experiments to show that the explanations generated by their method correspond to feasible changes in the input image, in contrast to other methods like PE or xGEM. Since all the methods involve a generative process, which is prone to perform un-realistic changes to the image, the author's claim of restricting perturbation to only feasible changes is vague. \n•\tIt is not clear what training data is being used to train the encoder-decoder in the proposed model. If the method uses a different dataset from the training dataset of the BB classifier, please state that explicitly. Also, to compare against the existing methods, the authors can design an experiment where they consider a dataset (explain-dataset) different from the dataset used for training the BB classifier (BB-dataset). They can then use the explain-dataset to train and compare the different explanation models (DiVE, PE, xGEM). \n•\tThe authors consider a perceptual reconstruction loss instead of a standard pixel-wise reconstruction. An ablation study to compare the different reconstruction losses is required to justify the proposed model's additional dependency on a pre-trained network R.  \n•\tThe term \"adversarial loss\" is misleading. The adversarial loss defined in equation 5 constrains the model to learn a perturbation that results in the desired probabilistic outcome. Hence, the name should reflect this constraint and its dependence on the BB classifier. As mentioned in the text, there are no adversaries here; hence no min-max game to be solved.  \n•\tThe authors claim that the sparsity constraint on the latent perturbation results in proximal and actionable explanations. The explanations' actionability is defined in terms of sparsity in the number of attributes that are modified in the latent perturbation. Since β-TCVAE is trained in an unsupervised manner, the latent space is not explicitly disentangled in measurable attributes (e.g. presence of sunglasses). Hence, the disentangled attributes learned by β-TCVAE, and discovered by the spectral clustering, may not correspond to discrete human-understandable concepts (e.g., sunglasses). An actionable explanation enables the human end-users to modify discrete concepts (e.g., remove sunglasses from the face) and observe changes in the BB's behavior. Experiments are required to quantify the actionability of the explanations. \n•\tA replication of the \"Beyond trivial explanations\" experiment on real images can also demonstrate the disagreement between the prediction from the BB classifier and the oracle classifier. It's not clear how experimenting on the counterfactual images provided any more/different information than the same experiment performed on real images.\n•\tA valid counterfactual image should produce an opposite prediction from the BB classifier compared to the input image There are no experiments to quantify the validity of counterfactual explanations.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A method that counterfactually explains predictions of classifiers by exploiting perturbations on data samples.",
            "review": "\n\n\nIn this paper, the authors present a method based on counterfactuals that learns a perturbation using constraints to ensure diversity in explanations.\n\nThe authors argue that explanations produced by their method are more “actionable, diverse, valuable and proximal than the previous literature”. However,\nit is unclear how they quantitatively measure these attributes, given that FID scores only captures the similarity of generated images to real ones.\n\nI would like to understand the motivation on using the perceptual reconstruction loss. The authors should clarify the usage of this loss in their method and\nhighlight its importance on their explanatory method. The author briefly mentioned the gains in terms of image quality, when compared with GANs in PE.\nHowever, I would like to see a more deeper discussion. \n\nSince interpretability is closely related to users/humans, it is difficult to assess the quality of the generated explanations without human evaluations.\nAn initial setup could be the one used in PE.\n\nOverall, assuming the above limitations, the experiments help to understand the contributions of the article.\n\nTypos:\n \n- Sec. 3.3: “Since these mask are …” -> “Since these masks are…” \n- Sec. 4.2: “In Figure 2b, …” -> “In Figure 2, …”",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "## Reasons for score\n\nOverall, I really liked your proposed method and would appreciate seeing your paper published. However, as of now, it does not pass the acceptance threshold. If you address my questions and requests, I would be willing to change my score.\nI think that it is critical that you improve the experimental section in terms of writing and presentation of the experimental protocol, metrics, results and especially section 4.3. Also, I think that it is necessary that you improve the related work section and highlight the limitations of your method.\n\n\n## My background\n\nMy research is focused on detecting data biases (or spurious correlations) learned by deep neural networks using explainability methods. This is the exact scope of this paper. However, although I have a solid understanding of the attribution methods, this paper develops a counterfactual one which is related to but not directly within my area of expertise.\n\n## Summary\n\nContext:\nThe paper focuses on counterfactual explainability methods that aim at improving the reliability of machine-learning systems and help for model debugging (finding spurious correlations or model biases). Given an input example and a target prediction score, this class of methods generates counterfactual examples.\n\nProblem:\nThe authors identify and tackle issues of state-of-the-art methods, xGEM[23] and PE[41]:\n- they combine multiple biases of the model in a single counterfactual example (not disentangled),\n- they exaggerate or remove the presence of the attribute being classified (trivial and not valuable).\n\nSolution and novelty:\nThe proposed method generates $n$ counterfactual examples for a given input example and a target prediction score. It is composed of :\n- a pretrained encoder-decoder architecture with $\\beta$-TCVAE[5] that produces a disentangled vectorial representation of the input example,\n- an algorithm that produces $n$ perturbations of this representation that are decoded to generate counterfactual examples.\nThe perturbations are obtained by minimizing a loss composed of:\n- a binary cross-entropy loss to generate examples that match the target prediction score,\n- an L1 loss between the input example and each generated example to force small perturbations in input space,\n- an L1 loss on each perturbation to force small perturbations in latent space,\n- a structural mechanism based on the Fisher information matrix and spectral clustering to force diversity of perturbations\n\nClaim:\nThe proposed method generates counterfactuals that are diverse, non-trivial (i.e. not just exaggerate or remove an attribute), high quality (i.e. in distribution) and valuable explanations about the model's prediction (i.e. biases can be detected by humans).\n\nExperiments:\nThe method reaches state-of-the-art results on two existing benchmarks. The paper also introduces a new benchmark to evaluate how valuable the explanations are.\n\n## What I liked the most\n\n- meta-problem of explaining neural networks is critical\n- mostly well contextualized\n- mostly easy to read\n- mostly easy to understand\n- mostly well illustrated\n- relevant issues have been identified\n- novel, simple and interesting method to tackle them\n- novel experimental benchmark\n- I really liked the hat section of 4. Experimental Results where you describe the 3 different aspects that you evaluate and you point to the associated sections\n- improvements over state-of-the-art are significant\n- ablation study mostly validates each proposed components\n\n\n## What could be improved\n\nMy cons are expressed per section, but the listing is random (I did not write the critical cons at the top of each section).\n\n1. Abstract and introduction\n- I find it surprising to assume derivability for a black box in the context of explainability methods. I know PE[41] uses this definition, but at least they clearly state that they assume derivability. For me, it does not correspond to the commonly admitted definition of a black box (e.g. Wikipedia: \"implementation is opaque\" \"without any knowledge of its internal workings\"). See the influential Ribeiro et al. 2016 \"Explaining the predictions of any classifier\" (4000 citations) for a definition that implies no assumption about derivability (i.e. \"model-agnostic\").\n- I had to write a detailed summary of your paper to better understand it. It is not critical, but I think that your writing can be further improved/structured to better frame the issues of previous state-of-the-art that you tackle, the novelty of your contributions (for instance, is it novel to structure diversity with FisherMatrix and spectral clustering?) and the claims that you validate in the experiments.\n\n\n2. Related work\n- First of all, note that reading the extended related work in the supplementary material (which is not mandatory) did not address the points I am going to make. I am convinced that your related work section could be improved as follows: 1) I would briefly mention that many post-hoc explainability approaches to detect biases exist and that you focus on counterfactual ones. 2) I would write about counterfactual methods that are not generative (see Papernot et al. 2018 \"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning\" which may be related). 3) I would write about state-of-the-art generative counterfactual methods and their limitations. 4) I would write about explainability methods that focus on diversity (you should mention that only a few exist if it is the case). \n- I do not agree with \"[attribution methods] do not explain how to modify [input features] to change the model outcome\". I think that is exactly what they do (by applying perturbations/masks). See Fong et al. 2017 \"Interpretable explanations of black boxes by meaningful perturbation\" that you already cite. However counterfactual generative methods of your kind apply perturbations that look real from a human standpoint or \"in-distribution\". I would like to see a discussion about the need for this kind of perturbations.\n\n3. Proposed Method\n- From a first read, it is not easy to map Dive, DiveFisher and DiveFisherSpactral onto their definition (around Eq. 7 and 8). Paragraphs Diversity loss and Beyond trivial counterfactual explanations could be better structured in this respect.\n- Figure1: You should make it clear that counterfactuals at the top are non-valuable to detect biases (because considered not bald by humans and by the model), whereas the bottom are valuable to detect biases (because considered bald by humans but not by the model).\n- Figure1: I do not understand why the bottom right has a black border.\n\n4. Experimental results\n- Overall, I do not find it clear what your training, validation and testing sets are made of. Do you use a testing set? How do you tune your hyperparameters? Is it standard?\n4.1\n- What does FID stand for? \n- I do not understand this: \"we train a second-order spline on the trajectory of perturbations produced during the gradient descent steps of our method\"\n- I think I understand, but it could be more clearly stated: \"even though DiVe is not explicitly trained to produce examples at intermediate target probabilities\". Actually, you can choose the target probability, isn't it?\n4.2\n- Table2: metric is not included in the caption\n- Table2: I do not understand the reason why some numbers are bold\n4.3\n- Overall I think it is critical to improving this section. I did not clearly understand how your novel experimental protocol can be used to validate \"the ability to identify diverse valuable explanations\".\n-\"Because it is costly to provide a human evaluation of an automatic benchmark, we approximate both the proximity and the real class with the VGGFace2-based oracle.\" Why do you think that we can trust the approximation?\n- Why don't you compare against PE and xGEM even tho it's not the same decoder as xGEM+ and DiVE?\n- I would make it clear what \"success rate\" means (you could make it bold where you define it)\n- Figure3: Overall, I should be able to mostly understand the Figure by reading the caption. It is not the case at all. I did not find it clear that you explore different hyperparameters.\n- \"We show results for all explanations in Figure 3a and only when the generated images are counterfactuals in Figure 3b.\" It is not clear. From what I understood your method generates counterfactuals only! Some of them are just valuable to detect biases (i.e. misclassified by humans) or not. What does \"successful counterfactuals\" mean?\n- It would be interesting to have a baseline with random masks to compare against your proposed method (Fisher and FisherSpectral).\n- There are no experiments on Dive-Fisher or Dive-FisherSpectral except for Figure 3 (and even for Figure 3. You don’t explain the difference between results for the different versions of Dive). I did not understand which version of your method works best. Also, you should define Dive-F and Dive-FS or not use them. I find that using Dive_Fisher and Dive_FisherSpectral improves clarity.\n\n5. Conclusion section\n- I would like to see a discussion about the limitation of your method and issues that need to be addressed in future works. In particular, what happens if your encoder-decoder is biased? What happens when your encoder-decoder is not able to produce disentangled representations (it seems often the case on real and complex datasets)? How to calibrate the many hyperparameters (to train the encoder-decoder and to weigh your loss functions)?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work not entirely validated and with missing references",
            "review": "The present work proposes an explanation method returning actionable, proximal, diverse, and not trivial counterexamples as explanation. The work is well written even though various concepts are detailed only in the Appendix.\nThe proposal is interesting, sound in the formulation, and valuable from the experiments reported. The examples reported are nice and quite convincing. The bias detection case study is effective and well presented.\nHowever, the paper lacks some major points that make it not ready for publication.\nFirst, even though theoretically the proposed DIVE method can be employed on any type of data it is developed and tested only on image data. The future work discussion is missing and the possibility to employ it on other data types is not treated. The fact that experiments are reported for a unique dataset is a limitation. Various simple datasets with more simple features can be adopted (mnist, cifar10, fashion mnist) or cifar100 imagenet by considering categories and specific classes for the different features.\nSecond, the paper misses various work on counterfactual explanations (some of them are listed in the following) and consequently also a comparison against them. In particular, it would be interesting to test DIVE against methods using a different logic for finding the counterexamples than against methods using a similar approach. Furthermore, in DICE (cited) are presented many evaluation measures not adopted in this paper and it is not justified why.\nFinally, the trivial/valuable explanations, which are the main motivation for this wor, are not formally defined in Section 4, nor in Section 5.\n\nMinor issues:\n- The optimization problem solved by DIVE to find the exemplars is not sufficiently detailed in the main paper.\n- It is not clear (or not easy to find) the dimension of the latent space of the VAE and how it affects the performance\n\nMissing Related\nKarimi, Amir-Hossein, et al. \"Model-agnostic counterfactual explanations for consequential decisions.\" International Conference on Artificial Intelligence and Statistics. 2020.\nPoyiadzi, Rafael, et al. \"FACE: feasible and actionable counterfactual explanations.\" Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020.\nGuidotti, R., Monreale, A., Matwin, S., & Pedreschi, D. (2019, September). Black Box Explanation by Learning Image Exemplars in the Latent Feature Space. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 189-205). Springer, Cham.\nPawelczyk, M., Broelemann, K., & Kasneci, G. (2020, April). Learning Model-Agnostic Counterfactual Explanations for Tabular Data. In Proceedings of The Web Conference 2020 (pp. 3126-3132).\nDhurandhar, A., Chen, P. Y., Luss, R., Tu, C. C., Ting, P., Shanmugam, K., & Das, P. (2018). Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In Advances in Neural Information Processing Systems (pp. 592-603).\nVan Looveren, A., & Klaise, J. (2019). Interpretable counterfactual explanations guided by prototypes. arXiv preprint arXiv:1907.02584.\nGuidotti, R., Monreale, A., Giannotti, F., Pedreschi, D., Ruggieri, S., & Turini, F. (2019). Factual and counterfactual explanations for black box decision making. IEEE Intelligent Systems, 34(6), 14-23.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}