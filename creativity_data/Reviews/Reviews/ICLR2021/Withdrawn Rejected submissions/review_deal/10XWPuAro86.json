{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nThe paper considers exploiting low-rank structure in Q-function and the Hamiltonian Monte-Carlo (HMC) to approximate the expectation in Q-learning to reduce the stochastic approxiamtion error, and thus, achieves \"efficient RL\". The authors tested the algorithm empirically within some simple environments. \n\nAs reviewers (R1, R3, R4) mentioned, the major bottleneck of this algorithm is the assumption that the dynamics is known up to a constant, which is extremly strong, and thus, limits the application of the algorithm. I suggest the authors to consider the common RL setting, without any knowledge about the transition models, and make fair empirical comparison with baselines in the same setting.\n\n"
    },
    "Reviews": [
        {
            "title": "Only applicable for tabular DP; experiments are weak",
            "review": "This work focuses on dynamic programming in the tabular setting. It proposes to use Hamiltonian Monte-Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low-rank Q matrix. It shows theoretical convergence. Experiments on discretized problems (CartPole and an ocean sampling problem) show that HMC and low-rank learning can behave more benignly compared to IID samples.\n\nThere are several issues with the current manuscript.\n\n1. It can be applied to only tabular problems with known dynamics, which is restricted as many control problems are continuous, and their underlying dynamics are unknown. Q-learning can be applied in the RL setting, but not the method proposed in this paper.\n\n2. Some technical details are not clearly described:\n- What is the distribution used for Fig.1?\n- Why is the \\euler{P} in Eq.(5) necessary? Why not directly use \\mathbb{P} as in Eq.(7) and (8)?\n\n3. Experiments:\n- It seems that the covariance matrices are defined arbitrarily for both CartPole and Ocean experiments.\n- Do Fig.2(a) to (c) correspond to the same (optimal) actions? Or the optimal actions wrt their own Q values?\n- Fig.2(d) is not very informative as we can have zero consecutive difference yet the Q function is far from the true optimal Q. Showing distance to the optimal Q values would be more meaningful.\n- For the Ocean experiment, it is not clear how the action will affect the dynamic. More specifically, it is clear in Eq.(16) that the action $a$ will affect the state dynamic, but not for the ocean experiment equations on page 14.\n- More descriptions about the ocean sampling problem would be helpful for the readers to understand the task. It seems that only the first term of the reward function on page 8 depends on the state (not sure where the action fits in this equation).\n\n4. Paper organization. Both Sec.3.4 and Sec.4 are experiments, so why not put them into the same section?\n\nMinors\n- In Fig.1 caption, 1 should be 1/3. \n- In reference, McAllister and Rasmussen: ta-efficient -> data-efficient",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting title but contributions are not convincing",
            "review": "In this paper, a Hamiltonian Q-learning is proposed by combining Hamiltonian Monte Carlo with matrix completion. Evaluations are compared with Q-learning.\n\n1. How to interpret Eqn.(3)(9) as a Hamiltonian equation (usually consisting of coordinates and momenta) is not clear. The so-called Hamiltonian Q-learning takes minimization optimization and essentially claims an equivalence of energy in physics model, which might not always make sense. What is kinetic energy in your case? A Hamiltonian equation is usually defined as a functional of some parameters, is your problem defined over the policy?\n\n2. In experiments, the proposed method is compared against Q-learning, but not with more advanced RL algorithms, e.g. DQN, DDPG, etc. The scalability of proposed method can be further investigated. More environments are also necessary.\n\n3. It is not clear that how accurate it is to assume the Q-table to be a low-rank matrix. I agree it is a reasonable assumption, but highly doubt its practical performance, especially in large-scale problems. \n\n4. The analysis in Section 3.3 is trivial, not enough to justify an acceptance.\n\nTherefore, I am not convinced by the contributions.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " Convergence contributions (Theorems 1-2) do not exhibit gains over vanilla Q learning. ",
            "review": "\nStrengths:\n\n1) To my knowledge, this is the first credible effort to apply Hamiltonian Monte Carlo to Q learning in order to avoid some degree of the random sampling required for its almost sure convergence. At least the algorithm novelty is apparent.\n\n2) The experiments clearly demonstrate the merits of the proposed importance sampling scheme for obtaining improved convergence of Q learning, and for yielding coverage of the space comparable to exhaustive sampling. \n\n3) The idea of using importance sampling *during* training is a key distinguishing feature from a majority of other works which use it for, upon the basis of a fixed prior policy, improving the current policy.\n\n\n\nWeaknesses:\n\n\n1) The preliminaries section is disjoint/fragmented. The limitations of Q learning (equation (1)) should directly motivate use of Hamiltonian Monte Carlo (HMC) in Section 2.2, but instead the manner in which HMC is presented is as additional preliminary material. This is not inherently disqualifying, but a number of exotic terms are introduced in the ``preliminaries\" Section 2.2 without explanation or otherwise connection to the RL problem, such as momentum variable, leapfrog integrator, Hamiltonian, etc. Why are these entities pertinent to MDPs and Q learning? This needs to be more carefully and conscientiously connected. A similar comment is true for Section 2.3 -- matrix completion or the need for addressing matrix estimation problems is nowhere to be found in 2.1 and 2.2, which leaves the reader confused as to why matrix completion is being discussed. This overall disjointedness then makes the conceptual innovation of this work more mysterious to understand, which is a concern. \n\n2) The reasoning in the paragraph before Section 3.1 seems incorrect. The fact that samples in a limited pool of IID samples concentrate around the region with high\nprobability density is not evidence that this is a poor estimate for the expected value, but rather that the expected value is not representative of a uniform distribution across  the space. Thus, I think the authors might have intended to  be talking about higher-order moments of this conditional distribution, such as the skewness, kurtosis, etc. or otherwise risk measures such as CVaR.\n\n3) The main convergence theory (Theorems 1-2) do not exhibit any discernible complexity or sample efficiency gains over vanilla Q learning. Moreover, a coherent discussion of the technical innovations required to establish these theorems is absent from the manuscript. These limitations alone are very concerning.\n\n4) The matrix completion step (equations (9)-(10) is playing the role of a proximal operator on the Q learning update, or otherwise some projection of the Bellman error onto a low-dimensional subspace of features. Therefore, the steps conducted in Section 3.2 are very similar mathematically to entropic regularization, which may also be seen as a special case of a proximal operator on the space of Q functions. This connection is not made in the paper, as well as the more computational/statistical motivation for where the matrix competition step comes from. I strongly suggest the authors consider better explaining the links between step (4) of Algorithm 1 and proximal methods in any revision of this work.\n\n5) Proofs seem fairly routine in my reading. What is new or innovative here? Again, this is not explained anywhere.\n\n\nMinor Comments:\n\n\n1) References missing on distributional/representational aspects of Q learning:\n\nDearden, R., Friedman, N., & Russell, S. (1998, July). Bayesian Q-learning. In AAAI (pp. 761-768).\n\nKoppel, A., Tolstaya, E., Stump, E., & Ribeiro, A. (2018). Nonparametric stochastic compositional gradient descent for q-learning in continuous markov decision problems. arXiv preprint arXiv:1804.07323.\n\nJeong, H., Zhang, C., Pappas, G. J., & Lee, D. D. (2019, August). Assumed density filtering Q-learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (pp. 2607-2613). AAAI Press.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper studies appling Hamiltonian sampling to computing Bellman equation approximately. I have a few questions:",
            "review": "This paper studies appling Hamiltonian sampling to computing Bellman equation (thus Q-learning iterations) approximately. I have a few questions:\n\n 1. The Hamiltonian sampling approach requires knowing the distribution in advance. How is this possible in our setting? If we are estimating this distribution, then since it is high dimensional, we still need large amount of data. This is the most confusing point from my point of view.\n2. As for the matrix completion approach: why is this reasonable? Why does the Q-table have low-rank property? I think at least this assumption should be justified empirically, otherwise, the theoretical result doesn't make any sense.  Also, the definition of the low-rank property itself is very suspicious, because it needs to hold for any policy. As a result, this may be very hard to verify rigorously. But at least some preliminary evidence should be presented.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}