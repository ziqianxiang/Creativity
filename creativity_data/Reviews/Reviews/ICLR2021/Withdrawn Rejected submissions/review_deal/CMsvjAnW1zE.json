{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies gradient descent with weight decay and momentum for scale-invariant networks. While this is an interesting research direction, the clarity of the paper suffers from poor writing. In its current form, I doubt this paper would have a large impact in the community. In addition, some reviewers pointed out that the analysis is not rigorous (some steps are missing, the authors re-use some unjustified steps from previous papers, ...). The authors claim these steps are obvious, I certainly don't think that's the case and at the very least, one would expect that detailed and rigorous derivations in the appendix if the lack of space is the real issue in the main paper.\n\nI think the feedback provided by the reviewers is valuable and I strongly encourage the authors to take advantage of this feedback to improve their work and submit to another venue.\n\n"
    },
    "Reviews": [
        {
            "title": "Comments",
            "review": "Unfortunately, I am not qualified enough in the literature of learning dynamics of the neural network to comment on the novelty and contribution of this paper. Therefore I may take the author’s word and would like to rely on the comment on other reviewers.\n\nIn general, the motivation of this paper is clear, since the theoretical result  in literature mainly relies on the assumption of equilibrium condition but does not discuss why this equilibrium can be reached. \n\nThe main contribution comes from  Theorem 1 and Theorem 2.  This paper proves that weight norm can converge at the linear rate under mild assumption . It also defines another index called angular update to measure the change of normalized neural network in a single iteration and prove its convergence with linear rate. Overall the theoretical result looks novel and the experimental result matches the theory.Hence, I lean toward accepting.\n\nMy only concern is whether this theory can help us to train the deep neural network or give us some insights to design certain layers of the neural network. But it maybe that the overall novelty/contribution outweights this concerns. Can the author further explain that?\n \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Some Comments",
            "review": "The equilibrium condition is important during the theoretical analysis of spherical motion dynamics. \nThis paper focuses on the reasonableness of the equilibrium condition. \nConcretely, the authors first show that the weight norm can converge to its theoretical result under some assumptions (with SGD, SGDM settings).\nFinally, the experiments verify their conclusions.\n\n### CONTRIBUTIONS\n\na) The analysis of the equilibrium condition is important. The authors analyzed the equilibrium conditions under different settings, including SGD, SGDM, angular. \n\nb) The problem is motivated well. The authors gave a detailed introduction to the question of why we need to analyze the equilibrium conditions.\n\nc) The authors conducted several experiments to show the reasonableness of the theorems/assumptions. And the theoretical findings agree well with empirical observations.\n\n### MAJOR CONCERNS\n\nI am still concerned with the explanations in Eqn.17. (Of course, the same concern in Eqn.20.)\nIn Eqn.17, the formula (left) is the MSE form, which is the combination of the bias and the variance. \nThe authors explained it like “the square of weight norm can linearly converge to its theoretical value in equilibrium, and its variance is bounded by the variance of the square of unit gradient norm multiplying the square of learning rate”. \nCould the authors tell us why the first term (vanish as t \\to \\infty) on the right hand is actually the bias? \nIn my view, one cannot say the estimator *converge* to its theoretical value if the right-hand does not converge to zero.\n\nI think the most important point in the paper is the convergence of the estimator to the theoretical value, which is also the thing the authors tried to verify in Figure2 (b, e). \nTo reach the conclusion, one needs to at least claim “V\\eta^2/l” is small. As shown in Figure2 (a), I do not think V is such small. \nAlso, the claims in the main text that “the learning rate \\eta is small” is not satisfying. I am not sure if I have skipped some important claims in the main text. \nI would like to raise my score if the authors can explain it better.\n\n### OTHER CONCERNS\n\na) Eqn.12 (as well as its statements) is not clear. It is still unclear to me why one needs to use an angular update. \n\nb) There are several typos (and also something like log->\\log, min->\\min) in the appendix. I believe the conclusions are mostly correct (although I did not check the details), but I still spend a hard time reading the appendix.\n\nc) In Theorem3, the assumption “when V=0” is a little bit strong and unrealistic (shown in Figure2(a)). The importance of the theorem will be larger if the authors could use a mild assumption.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting direction but confusing experiments ",
            "review": "The main goal of the paper is to establish theoretically some previous known results that for scale invariant networks the weight norm has a fixed point with ||w||^4=eta/lambda ||\\tilde{g}|| . They also discuss the angular update, which because of scale invariance is basically equivalent to arccos (1-eta lambda) |w_t|^2/|w_t+1|^2 and it thus comes mainly from the gradient. They have some experiments which they compare with the predicted equilibrium values for the angular update/ weight norm. \n\nThe theorems are the main results and I don't think they are very powerful given what is known in the literature (the van Laarhoven paper but also the more recent Li-Arora and Lewkowycz-Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate). Also theorem 3 assumes that the normalized gradient norm is constant through training, but it actually changes a lot. \n\nThe experiments do not scale invariant networks (even if they have batch-norm, the last layer for example is clearly not scale invariant) so it is not that clear why the theory applies to that case, and the authors should probably discuss this. \nIn the learning rate decay setup, the experiments don't include the equilibrium value for the weights and it would be good to include them, and it does not seem like the equilibrium analysis is valid there as the authors discuss. The authors propose to rescale the weights together with decaying the learning rate, it would be good if they could mention how this changes the model validation accuracy. \n\nI find it really intriguing that in figures 2ef , the weight norm tracks so well the equilibrium one (which by definition has \\Delta |w|=0) but at the same time it, the weight norm changes considerably? Could the authors comment on how this is possible? I suspect that this is because the predicted one is actually really noisy due to batch noise (as shown in figures 2ad) and thus the system is never at equilibrium. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "paper is not well-written and the analysis is not rigorous",
            "review": "This paper tried to analyze gradient descent with weight decay and momentum for scale-invariant networks. Convergence rate regarding the norm of the iterate is provided.\n\nI think this paper has several weaknesses and hence do not recommend accepting the paper at the current point.\n\n1. The paper is poorly written. The analysis and arguments are very poorly explained.\n\nPage 2:\n\n\"If $w_t \\approxeq w_{t+1}$, then the weight norm can be approximated as $\\|w_t \\|_2 \\approxeq $...\"\n--- Why is that?\n\n\"while the gradient component provided by WD always tends to reduce weight norm.\" \n--- Why? Is there any proof to support this argument?\n\nThe authors say \"one can obtain\" equality (4). But why (4) holds? it is very unclear and more explanation is necessary\n\nThe authors say \"one can easily speculate from eq (5) the magnitude of update in SGDM cases should be ...\" but the reasoning is unclear.\n\n\nPage 3:\n\"Eq.(7) implies gradient norm is influenced by weight norm, but weight norm does not affect the output of DNN\" \n--- Why?\n\nHow to derive (8)?\n\n\"If \\| w_t \\| \\approxeq \\| w_{t+1} \\|_2, one can obtain (10)\" \n--- What do you exactly mean when you write \"\\| w_t \\| \\approxeq \\| w_{t+1} \\|_2\"? and why the condition leads to (10)?\n\nThe authors say \"one can derive\" (12). But again this is unclear.\n\n\"From Eq.(7) we can infer that increasing weight norm can lead to smaller gradient norm if unit gradient norm is unchanged.\" \n--- This needs more explanation.\n\n\"Arora et al. (2019) proves that full gradient descent can avoid such problem and converge to a stationary point...\" \n--- To avoid which problem?\n\n\"Besides, practical implementation suggests training DNN without WD always suffers from poor generalization\" \n--- This needs references. \n\nPage 4:\n\n\"We can easily derive (15)\" How? Also, what does $\\approxeq$ on (15) mean? Can you give a precise statement?\n\n\"Notice that the core assumption mentioned above is unit gradient norm is unchanged. In fact this assumption can solve the contradiction we present in Section 1: the convergence of weight norm is not equivalent to convergence of weight, steady unit gradient norm can also make weight norm converge\" \n--- I found the paragraph hard to parse. What do you mean by \"unit gradient norm is unchanged\"? Of course, convergence of weight norm is not equivalent to convergence of weight. So why is it a \"contradiction\"?\n\n\n2. The proof is not rigorous and the writing of the proof is very sloppy.\n\nI believe the proof in the appendix is not rigorous and I feel unease. The authors use the following phrases (not an exhaustive list) to skip a lot of detailed proof, which does not meet the standard of mathematical writing.\n\n\"we can easily prove\" (page 11)\n\n\"we can prove\" (page 13)\n\n\"can be formulated as\" (page 14)\n\n\"it's easy to prove\" (page 14)\n\n\"can be explicitly expressed\" (page 14)\n\n\"which it's easy to prove\"  (page 15)\n\n\"it's easy to obtain\" (page 16)\n\n\"therefore it is easy to derive\" (page 17) \n\n\n3. The theorems are not very insightful.\n\nWhat insight does the convergence of weight norm provide? for example, it doesn't rule out the case of converging to a bad local min.\n\nWhat is the meaning of the value $w^* = ( L \\eta / 2 \\lambda)^{1/4}$? Can you explain why w^* should have the dependency on each parameter?\n\nDoes the theorem say that it will converge to the value $w^*$ regardless of any initialized point?\n\nWhat does the condition $\\lambda \\eta << 1$ exactly mean?\n\nWhat does the condition $\\| \\tilde{g}_t \\|^2 > l$ mean? Why does it hold?\nWhen the iterate reaches at a stationary point, isn't it zero?\n\n\n4. Comparison to the related works are not clear.\n\nRemark 1 and 2 state the results of this paper are \"consistent\" with some related works but did not provide any elaboration. What results exactly are being compared with? If the results are consistent, then what is new here?\nI hope the authors can explain more.\n\n5. Experiment does not really support the theorems.\n\nIt seems to me that the authors didn't really show that the weight norm converges to $w^*$ of Theorem 1 or 2.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}