{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Paper proposes an approach for scene autoregressive layout generation. Four expert reviewers evaluated the paper outlining the following pros/cons of the work. \n\n> Pros:\n- Good performance across different domain [R1,R2,R3,R4]\n- Formulation is general [R1,R2]\n- Clever separation of different attributes [R1]\n- The idea of using transformers is interesting [R4]\n\n> Cons:\n- Missing related works [R3]\n- Unclear comparison with baselines that [R2]\n- Lacks of  hyper-parameter tuning on the baselines [R2]\n- The quantitative results do not outperform the state-of-the-art models consistently across all metric [R4]\n\nAuthors have addressed some of the concerns in the rebuttal and generally reviewers are more convinced after the rebuttal than before. The fairness of comparison to baselines remains an issue for two of the reviewers, and quality of results for one. AC acknowledges and agrees with these concerns. As such, given the large number of highly qualified submissions to ICLR and in comparison to those submissions, the paper fell slightly bellow the acceptance threshold. \n\nThat said, AC believes the approach, overall, is interesting and warrants re-submission after the appropriate revisions are implemented. "
    },
    "Reviews": [
        {
            "title": "Clean architecture and good cross-domain applicability",
            "review": "This paper presents an auto-regressive method for generating layouts by sequentially synthesizing new elements. The architecture is not dramatically new, but it is well-justified and analyzed, and there are some interesting tweaks. The results are strongest in that they show good performance of essentially the same architecture and hyperpameters across quite different domains: to my knowledge such variety has not really been demonstrated for any of the assembly-based generative models I'm familiar with.\n\nThe discretization aspect is not completely clear. As I understand it, each scalar geometric attribute of a node (but not the feature vector s_i) in the layout (x_i, y_i, w_i, h_i) is quantized to 8 bits, so that it can be thought of as selecting from one of 256 discrete options. How is the projection to d_model dimensions then performed, if d_model is 512 in one domain and 128 in another? What is the \"categorical distribution\" referred to here? Could you please explain this section more clearly?\n\nAlso, would it be possible to do an ablation study where no discretization is done at all?\n\nThe following paper is relevant as a baseline which also does sequential part assembly for shape generation. It would be nice if the authors could compare to this work.\n\nSung et al., \"ComplementMe: Weakly-Supervised Component Suggestion for 3D Modeling\", SIGGRAPH Asia 2017.\n\nThis is another paper that presents a fairly general framework for an autoregressive layout generator. The current paper should be compared with this as well, via experiments if possible:\n\nRitchie et al., \"Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models\", CVPR 2019\n\nAnd this paper is more recent than the ones compared to in the paper for document layout generation:\n\nGadi Patil et al., \"READ: Recursive Autoencoders for Document Layout Generation\", CVPR 2020 Workshop on Text and Documents in Deep Learning Era.\n\n(Various flavours of layout generation have seen a huge amount of research in the last few years, so I am not totally confident of my assessment especially vis-a-vis prior work.)\n\nThere are minor typos and grammatical errors -- a thorough proofread would help.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good Message and Empirical Performance, but Rather Limited Novelty",
            "review": "=====Post-Rebuttal Comment=====\n\nI thank the authors for their detailed response to my concerns.\n\nWhile my opinion of this work remains largely similar, I raised by score from 5 to 6 for the following reasons:\n\n- I do buy the argument that the proposed method \"allow both developers and researchers to start from a strong method with a low barrier to entry in diverse domains\".\n- I do not share the concerns of R2 & R4 regarding the quality of results and fairness of comparison. As my primary concern (amount of technical innovation) is not shared by other reviewers, I am swayed to change my initial \"on the borderline\" rating to the positive side.\n\nI still would recommend adding the following evaluations:\n\n- more diverse initial state for Figure 10.\n- a more interpretable dataset for Figure 15: I think ShapeNet would demonstrate this point better.\n\n\n=====Summary=====\n\nThis work introduces a general framework for generating scene layouts in 2D or 3D. The key idea of this work is that one can represent an arbitrary layout as a sequence of objects, where each object contains a set of attributes (location, sizes, category, shape, etc.). By projecting these features into the same (high dimensional) embedding space, one can thus represent the entire layout as a sequence of embeddings. Consequently, it is possible to borrow architectures commonly used in natural language processing and design a generative model for such sequences. Here, the authors use a simple model with masked self attention and trained with teacher forcing. The proposed model is shown to have performance comparable or slightly better than SoTA methods in four different tasks under multiple metrics. It is also demonstrated that the learned embedded space displays some structure that respects the semantics.\n\n=====Strengths=====\n- The idea behind this paper seems to be general enough and can be applied to any scenarios where one can represent each entity in the layout share the same set of attributes.\n- I like the idea of separating different attributes a lot --- probably my favorite part of the paper. It makes sense intuitively that doing so will allow the attention module to more easily focus on the attributes that matter.\n- Strong empirical performance: comparable results for 3D shape synthesis and seems to achieve slightly better performances for the other three tasks evaluated.\n- Clear exposition and comprehensive discussion of related works in multiple areas\n\n=====Weaknesses=====\n- Would like to mention that one of the main novelty (to my understanding) in this work has already been attempted in an earlier work “PolyGen: An Autoregressive Generative Model of 3D Meshes”. The vertex model of PolyGen, in particular, uses a similar transformer decoder to generate the positions of points, a task that is very similar to predicting the position of objects in the layout. (the coordinates are also discretized in a similar way to this work, and the ordering strategy is similar, but these are much more minor points).\n- Following previous point: I think the novelty in this work is quite limited in general, using existing architectures and relying on ideas that have been explored (albeit in slightly different settings) before.\n- The empirical performance of the method is not strong enough to convince me that it can replace the more domain specific methods compared here. The reason is two-fold. First, it is unclear to me whether the proposed method is flexible enough to handle all the other tasks that those domain-specific method could, as only generation results are shown here. Second, I am not sure if the proposed method is flexible enough / has the right inductive bias for the layout tasks. For example, not taking hierarchy into account seems to be a minus for me when one want to interpret / manipulate the outputs. The sequential nature of the model also put constraints on the type of partial input acceptable e.g. it seems to be hard to handle input objects with only partial information available)\n\n=====Reasons for Score=====\n\nI’m right on the borderline for this paper and think this can go either way. \n\nOn the positive side, the paper shows a very clear message (again) that models borrowed from NLP can be surprisingly effective if we find a way to convert other structures (layout here) to a sequence. The empirical performance is also quite decent. On the negative side, the amount of novelty behind the main message of this paper is questionable: neither the architecture nor the idea of converting stuff to sequences are completely new, and the empirical performance is not strong enough for one to favor this approach over others (in their respective tasks). \n\nTo me, the question boils down to whether the main message of this paper is a very important one that deserves to be heard more by the broader community. I am not particularly convinced here, and lean slightly towards rejecting this paper. \n\n=====Additional Comments & Questions=====\n\nOther minor questions in addition to what I listed above:\n\n- I am not sure if it makes sense to map different types of features onto the same embedding space e.g. I don’t see how spatial coordinates and color can share any feature, and they would ideally just occupy distinct regions of the embedding space. Could the authors explain the design choice here? Why can’t one use different embedding spaces (and modify the attention module to handle that)?\n- Would like to see examples demonstrating that the model can generate diverse outputs, as opposed to always do the same thing for the same (partial) input. The shape completion tasks in figure 3 seems to be a good task to do this on. (The authors mentioned in conclusion that diversity is a problem, but I would still want to see how much of a problem it is, as I think some of the other works compared here don’t suffer too much from diversity problems)\n- Would also like to see a figure showing the nearest neighbor from the training set(s) to make sure that the model is not simply memorizing the input data. \n- It is mentioned in section 3.1 that discretization helps learning symmetries, are there concrete evidence towards this?\n- Figure 6: I get that there is structure here, but does the method learn better structure than other works?\n- Figure 5: I don’t get the message of this figure - all the images look equally bad to me.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Are the comparisons with the baselines fair?",
            "review": "### Summary\n\nThis work proposes a model to generate scene layouts by treating the scene as a composition of primitives, such as instance class, coordinates or scales. The model is a Transformer architecture, that attends on all previously predicted or given instance primitives. The probability of a scene layout is defined with a joint distribution, modeled as the product of conditional distributions using the chain rule. The model predicts an end of sequence token, that allows the generated layouts to have variable size. Moreover, the model allows to either complete an existing incomplete layout or to generate one from scratch. The paper presents experiments in four datasets, spanning different data domains, including 2D and 3D data.\n\n\n### Strengths\n+ The motivation and objective of this work are clear.\n+ The general framework is simple and gives enough flexibility to apply it to different types of inputs and datasets.\n+ Providing results on different data domain datasets helps supporting the work. \n\n### Major concerns\nSome important points of the method, experimental setup and the results are not clear (and need to be clarified), as they bring some important concerns:\n- Using test set as validation set: It seems that the same split was used for both finding hyper-parameters (considering number of trained epochs due to early stopping also a hyper-parameter) and for reporting final results, at least for COCO-Stuff. It is not clear from this paper this is also the case for all the other datasets. In general, this is a bad practice and gives unfair advantage over baselines such as LayoutVAE, that uses a separate validation set to find hyper-parameters. \n- Unclear comparison with baselines: Given that some metrics/datasets were not used in the original papers, I have some concerns regarding fairness of comparisons; (1) were the methods re-trained for this paper or taken as is from the code of original authors? (2) If the answer is the former, were hyper-parameters tuned independently for all datasets, as to give the chance for all baselines to do better on each given task? If the answer is the latter, it is unfair to use the same setup and hyper-parameters designed for one task and apply it for all.  (3) how were the proposed method's parameters tuned? are these parameters tuned for each dataset? Related to these questions, Figure 5 shows really bad results for LayoutVAE. It is unclear to me if this is what one would expect from this method and it makes me suspect the baseline is not properly used, as it seems as if this baseline is generating random layouts. It could help to see the generated layout for this baseline, and not only the final image. \n\n### Other concerns\n- In page 4, it is said that at validation time, teacher forcing is used. Does this mean that for the final reported results teacher forcing is used as well?  This is important to clarify, as always having access to the ground-truth layout is an unrealistic assumption at test time.\n- The paper states in several parts that the framework allows to \"generate a new layout either from an empty set or from an initial seed set of primitives\". Which experiments show the method generating a layout from an empty set? It seems that the COCO-Stuff and 3D objects start from an initial set.  (1) Are the experiments on documents and applications from an empty set? (2) why not test the empty set start for COCO-Stuff and 3D objects? (3) when starting from an initial set, how many objects are used to start with? is it always the same number for all methods and setups? \n- I would like to see standard deviations for all reported metrics, as it is unclear for some of them whether the position with respect to other baselines is meaningful or not.\n- In Section 3 \"other details\", it is stated that nucleus sampling is used. However, throughout the paper, beam-search (Figure 2) or greedy-search (Figure 3) are mentioned. Which sampling method is actually used?\n\n\n### Additional questions/comments\n- As I understand, an EOS token is generated at some point and the layout generation stops. How is it prevented to generate a EOS token in the middle of an instance bounding box, let's say, after \"h\" and before outputting a \"w\" value. Is it only possible to generate an EOS token only when the next step generation is a class ID?\n\n- At the end of Section 2, one of the listed advantages of the proposed method is \"The autoregressive nature of the model allows us to generate layouts of arbitrary lengths as well as ...\", while in fact other existing approaches (for instance, LayoutVAE) can also generate arbitrary length layouts, and it is not something introduced in this paper.\n\n- In terms of general format:  Starting from the paragraph \"3D Primitive Auto-encoding\" in Page 5, this should go in another Section about experimental setup, not in the main method. Similarly, the conclusion should also be formatted as a separate section. \n\n- Regarding the sentence: \" The function that projects node i to latent space s_i can be learned independently or jointly with our layout generation framework.\" Where is this discussed in the paper or supported in experimental results? I could not find it and I am unsure of why is it mentioned here.\n- Given that geometry coordinates (x,y, w,h) are discretized, which primitives remain continuous? Equation 4 contemplates the case where primitives are continuous, but it is not clear which primitives remain continuous after the discretization step. \n\n- Why can't LayoutVAE and ObjGAN be applied to PoseNet? I would like to understand what is the limitations of these methods with respect to the proposed method.\n\n### Reason for score\nIt is a simple approach based on existing work, just slightly adapted for this layout generation task. Moreover, I had to infer most of the experimental and comparison details, as it is not clear in the paper. Additionally, given the important concerns already mentioned above regarding the provided results, I cannot discern whether this simple approach brings any actual improvement over existing methods. \n\n### What can be improved\n- A clearer discussion of what are the advantages of the proposed method. Although other methods did not provide results on such diverse data domains, it seems that LayoutVAE and ObjGAN were adapted as baselines for all except the 3D dataset. Therefore, the flexibility to different data domains of the proposed method, a major selling point, could in fact also be present in other methods. If baselines are allowed to tune parameters for each specific dataset, do they provide consistently worse results than the proposed method?\n- I would like the authors to extensively discuss the \"Major concerns\" and paint a better picture of the experimental setup. In this line, these details should also be included in the paper, whether it is in the main body or in the appendix. \n- If all other concerns raised are solved, this could also be clarified in the text. \n- Include results as mean and stdev over different random seeds for all experiments, providing more robust comparisons.\n\n\n### After authors response\nAlthough authors have addressed one of my main concerns and some minor ones, I still have doubts regarding the fairness of comparison with the baselines (lack of proper hyper-parameter optimization) and therefore cannot trust the results. All in all, I keep my rating. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Results not state-of-the-art",
            "review": "The paper presents a transformer based architecture to model the probability distribution of scene layouts. The authors evaluate the model in four different application domains. The model performs competitively with state-of-the-art methods with respect to appropriate metrics.\n\n**Strengths**\n+ The paper is written clearly and the implementation details are appropriately described\n+ The idea of using transformers is interesting\n+ The evaluation of the method is thorough\n\n**Weaknesses**\nWhile the evaluation conducted by the authors is thorough, my main critique of this paper is that the results are not convincing enough to show the value of the proposed model.\n- The quantitative results do not outperform the state-of-the-art models consistently across all metric. For instance, the mdoel does not outperform PointFlow and PQ-Net across all metrics in Table 1. Same is the case for ObjGAN (IS=7.5) vs proposed (IS=7.1) in Figure 7. The authors fail to comment on why is that the case. \n- The qualitative samples are also not realistic in some cases. Some of the COCO results in Figure 4 and 5 do not look realistic -- layouts are too cluttered leading to incomprehensible scene. (row 2,col 3) in Figure 4 and (row 3,col 4) in Figure 5). From the layout samples presented in the paper, it seems the model produces cluttered layouts when the model is trying to generate higher number of objects.  The authors do not discuss this aspect in the paper. Is the quality of the layout dependent on the number of entities in the scenes? If yes, I think it is important for a scene layout generation model should be robust enough to handle flexible lengths.\n\nOverall, while the paper has interesting approach to model the relations between the elements of a scene, the results are not convincing enough to demonstrate the effectiveness of the proposed model. Therefore, my initial rating is 5.             \n                                                                                                                                                                                      \n=========================================**Post-rebuttal comments**=======================================\n\nI appreciate the revisions and additional visualization provided the authors. The revised version of the paper provides clarifications that make the paper easier to follow. While the paper presents an interesting idea, I am not convinced of the effectiveness of the proposed method based on the results provided by the authors. Therefore, my final rating as 5 (same as the initial rating).  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}