{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper studies Knowledge Distillation (KD) to better understand the reasons behind the performance gap between student and teacher models. The analysis is done by conducting exploratory experiments. The paper establishes that the distillation data used for training a student can play a critical role in the performance gap apart from the model capacity. Building on this idea, the authors propose a new approach to distillation, KD+, utilizing out-of-distribution data when training a student. Extensive experiments are performed to demonstrate the efficiency of KD+. Overall, the paper studies an interesting problem. The results provide a more in-depth explanation of how the distillation data and model capacity play a role in the performance gap between student and teacher models in KD.\n\nI want to thank the authors for providing the rebuttal and sharing their concerns about the quality of one of the reviews.  The reviewers appreciated the paper's ideas; however, all the reviewers were on the fence with borderline scores. In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers' feedback to better position the work w.r.t. the existing literature and provide clear reasoning behind the gains for KD+ in experiments. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers' feedback when preparing future revisions of the paper."
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "This paper investigates why conventional student network is not outperformed to teacher network. Based on several experiments design, this paper argues this is due to the student only fit the local, in-distribution shapes of the network rather than model compacity.  Besides, this paper design a go beyond in-distribution distillation approach to overcome this issue. Overall, I think this paper has some merits, but also some concern need to be addressed. \n\nPros:\n1）This paper is well-written. The motivation and the organization of the paper are easy to follow. \n2)   The experiments design to prove the assumption part is good to read and make sense, e.g. Tabel1, Tabel2 CST/ IST\n\nCons:\n1）The first concern is the result of Table 1, I was very supervised the loss of teacher and student is equal to 0. For my experience, the distillation loss in very small, but it still has the same value, which means the student output can not total the same as teacher output.\n\n2）The second concern is about the novelty of KD+, the selection of Out of distribution sample is similar to the technique applied in the current state of art semi-supervised learning methods: 1) mixmatch[1] 2) remixmatch[2]. I suggest the author add these reference and discuss. They also use KD and use middle points.  \n\n3) I also concern about the fairness of Table 4, since using the middle points is a regularization technique, like, mixup[3]. Does it fair to compare with KD without mixup.\n\n4) The last one is the student can outperform to teacher has been studied some literature? I remember some of the paper point out this. \n\nReference:  \n[1]mixmatch: a holistic approach to semi-supervised learning \n[2]remixmatch: semi-supervised learning with distribution alignment and augmentation anchoring\n[3]mixup: Beyond Empirical Risk Minimization\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors insist that this is the first work to explore the root reason for the performance gap in knowledge distillation and the distillation data matters instead of the model capacity. But this paper presents some unconvincing arguments that the designed experiments cannot support firmly. This paper lacks the most important references and the proposed KD+ is not superior to the existing approach.",
            "review": "This paper presents an argument that model capacity differences are not necessarily the root reason for the performance gap between the student and the teacher, and the distillation data matters when the student capacity is greater than a threshold. Based on this, the authors develop KD+ to reduce the performance gap between them and enable students to match or outperform their teachers. In addition, this paper designs experiments to confirm the proposed arguments. \nHowever, this paper should be rejected because:\n(1)the results of Table 1 cannot confirm the authors’ argument that the widely used students are CSTs. It also depends on the model capacity differences whether a CST is able to fully fit the teacher outputs or not. \n(2) some arguments in this paper are unclear and lack verification. For example, the paper states“This suggests that these students have well captured the knowledge on sparse training data points but have not well captured the local shapes of the teachers within the data distribution.” in Definition 4.2. How did you come to this conclusion?\n(3) the most important or relevant references are not cited, for example [1] Xu G, Liu Z, Li X, et al. Knowledge Distillation Meets Self-Supervision, 2020. And the proposed KD+ is not superior to the approach of [1], for example, some results in Table 4 and Table 8.\n(4) This paper lacks experimental settings and details.\nPlease refer to the above comments and answer these questions.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review from AnonReviewer4",
            "review": "This paper explores why the large gap existing from the viewpoint of data distribution rather than student model capacity. Base on the exploratory experiments and some analyses, the paper proposes KD+ by sampling out-of-distribution data points and achieve better results than current KD methods on the experiment.\n\nStrengths: \n1. The exploratory experiments on out-of-distribution data is interesting, which is meaningful to discuss why the gap between the student and teacher model exists from a new viewpoint of data distribution rather than the student capacity.\n2. KD+ achieved promising results.\n\n\nSome concerns:\n1. A recent KD work [3] also achieved promising results on ImageNet by adopting a discriminator to judge the output distribution is from the teacher model or student model, I think it shares a similar idea with this work, but it uses GAN-style learning to make student's output be more like teachers. This paper fails to compare their results with [3] or discuss the difference by using a GAN style to learn the output distribution of the teacher. \n2.  KD+ will sample data on the original training data, so it would result in more training data than KD or other methods. How does the training time change? The paper fails to discuss training time changes on more training data.\n3. The key point of KD+ is to sample from training data $x$, while it only gives an illustration figure (Figure3) on one dimension data $x$ rather than on real images. Namely, the paper doesn't discuss how to sample $p$ images from two images $x_1$ and $x_2$, by interpolating from $x_1$ and $x_2$ or some other methods? \n4. In Table 2, the simulation results of students (CSTs) are better than the teacher models. I want to know what if use the teacher model to teach itself on the same simulation experiments, as the work in Born Again Network [1] or Tf_self [2].  The author would like to consider this improvement as regularisation [3] or some dark knowledge from out-of-distribution? \n\n\n[1] \"Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks.\" Shen, et al. arXiv:2009.08453 (2020).\n\n[2] \"Born again neural networks.\" Furlanello et al, ICML 2018.\n\n[3] \"Revisiting knowledge distillation via label smoothing regularization.\", Yuan et al, CVPR 2020.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #3",
            "review": "The paper studies knowledge distillation. In particular, it tries to disentangle the effect of student model capacity and distillation dataset on the performance of the student. The paper goes on to present KD+, a knowledge distillation approach that goes beyond in-distribution data. Experiments on multiple image recognition models and datasets show that KD+ outperforms KD consistently.\n\nThe study is very interesting in how it tries to characterize the impact of  student capacity and distillation data on the performance of  the distilled student. The ablations studies are interesting and shed light on several angels including few-shot learning scenarios, different algorithms for distillation, etc.\n\nThere are some important questions though that arise:\n\n- Is the effect of improved student performance coming from having out-of-distribution data or simply more data for distillation? This also relates to the bigger gains with few-shot learning settings. \n\n- When using p=2 for data augmentation (adding a middle point between two samples), are classes considered in any way? In other words, could it be that the model is benefiting from adding harder examples to the distillation set? Similar observations were made in self-training where there has been a lot of work on leveraging how hard an example is, or how  confident a model is to select samples for self-training?\n\n- The gains seem to vary given the distillation algorithm, with bigger gain when using basic KD. Could other distillation algorithms that try to align the representations of the student and teacher by using different criteria be accomplishing similar generalization effect to the proposed approach\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}