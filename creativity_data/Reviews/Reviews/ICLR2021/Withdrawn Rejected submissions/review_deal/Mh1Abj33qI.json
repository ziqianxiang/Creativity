{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a simple yet powerful generalisation of graph scattering transforms that allows a flexible scale dilation structure, retaining the stability guarantees of dyadic transforms. Experiments with strong empirical performance are reported on a variety of biochemical tasks.\n\nReviewers acknowledged the soundness of the approach as well as the quality of the empirical evaluation, but also raised some concerns about lack of novelty. Ultimately this AC believes that, although this work solidifies Graph Scattering Transforms as a good alternative to GNNs on certain structured physical domains, it provides little advancements on the theory front. Unfortunately not all good papers can be accepted, and therefore the AC recommends rejection at this time, encouraging a resubmission."
    },
    "Reviews": [
        {
            "title": "Interesting connections between GSP and GNNs but with a few issues",
            "review": "Summary:\n\nThis paper proposes a parameterization of the scatter transform on graphs and builds graph neural networks based on this parameterization. Authors also demonstrate that this scatter transform could theoretically lead to stable hidden representations of GNNs. Experimental results on biochemical datasets are conducted to support the arguments of the paper.\n\nPros:\n\n1, The connection between the recent advance of graph signal processing (i.e., the generalization of the scatter transform on graphs) and graph neural networks are inspiring and interesting.\n\n2, I agree with the motivation of designing band-pass or high-pass graph convolutional filters since the original ones used by GCNs indeed act like low-pass filters and tend to over-smooth signals on graphs. Although I am not so sure whether the scatter transform is the way to go to design data-driven/learnable graph convolutional filters which satisfy this purpose, the exploration along this line is still valuable and could inspire others in the community.\n\n3, Theorem 1 is interesting since it reveals the stability of hidden representations of the hand-crafted GNN. \n\n4, The overall paper is written in a clear manner.\n\n5, Authors provide experimental results on a wide range of datasets.\n\nCons & Questions & Suggestions:\n\n1, If I understood correctly, Theorem 1 holds for the construction shown in Eq. (1). However, it is unclear whether Theorem 1 still holds for the relaxed construction of filter banks. This part is not discussed in the paper. If the theoretical guarantees on the stability do not hold anymore, then the contribution of the proposed method will be degraded significantly.\n\n2, I’m concerned about the emphasis on significantly fewer learned parameters of the proposed model. If the performances of your proposed model are on par with or superior to other GNNs on many datasets, then having fewer parameters would be a merit. However, on common social networks, as shown in the appendix, the performance of the proposed model is worse than other basic GNNs, not mentioning the recent ones. This also links to the somewhat less convincing experimental comparison on biomedical datasets as discussed in point 6. On the other hand, it seems that there are a few options to increase the number of parameters (as discussed in point 4) which arguably could improve the model capacity.\n\n3, Some important references on the intersection of the graph signal processing and graph neural networks are missing, e.g., [1,2,3]. In particular, it would be great to discuss the relationship between your work and [2,3] which also go beyond low-pass filter on graphs by exploiting learnable spectral filters and capturing the multi-scale diffusion. Comparing the scatter transform with their approaches would help better position your contribution. \n\n4, Regarding the design choice of learning the selection index F, it is less expressive to share the selection index among different graphs. In other words, the current design of using the same set of \\theta for all graphs is less satisfying as one can imagine that some graphs may require capturing long-range dependencies (corresponding to the higher power of diffusion matrix P) whereas some may just require short-range dependencies. A better option would be designing a model that takes graph data as input and predicts the selection index. Therefore, the selection index would be graph-dependent. Moreover, compared to [2,3], if one wants to explore larger diffusion steps, the computational cost would be much higher than [2,3] since they avoid the direct computation of matrix powers by using the approximated top eigenpairs of the diffusion matrix P.\n\n5, For the aggregation part, there are quite a few simple options like max/mean/attention. It would be more convincing to include these options and conduct an ablation study to justify the design choices like the RBF aggregation.\n\n6, In the experiment section, it would be more convincing to add comparisons with some recent competitive methods, especially those motivated from the graph signal processing like [2,3]. Could you shed some light on why GraphSAGE has similar std as your method and other baselines on graph classification tasks while significantly larger std than yours and others on graph regression tasks? Also, why does the proposed LEGS-RBF have a huge performance drop on the MUTAG dataset compared to any other baselines? It seems that the proposed methods perform comparable or better than other baselines on biochemical datasets but worse on social science datasets (as shown in the appendix). Could you explain why this is the case?\n\n[1] Defferrard, M., Bresson, X. and Vandergheynst, P., 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS.\n\n[2] Liao, R., Zhao, Z., Urtasun, R. and Zemel, R.S., 2019. Lanczosnet: Multi-scale deep graph convolutional networks. In ICLR.\n\n[3] Luan, S., Zhao, M., Chang, X.W. and Precup, D., 2019. Break the ceiling: Stronger multi-scale deep graph convolutional networks. In NeurIPS.\n\nConclusion: Based on the above merits and issues, I am currently on the borderline and would like to hear feedback from the authors.\n\n===================================================================================================\n\nThe response and the updated version clarify and address many of my concerns regarding contributions and empirical conclusions. Overall, I lean towards acceptance.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper extends geometric scattering network by relaxing its scattering construction to enable training / data-driven learning. There are three major modules in the proposed network architecture: diffusion module, scatter module, and aggregation module. They conduct experiments on two tasks: whole graph classification and graph regression.\n\nThe idea to relax geometric scattering network is novel to the best of my knowledge. However, I have the following concerns:\n\n* Why is it that LEGS only outperforms other GCN methods on biological datasets? The major advantage of LEGS compared with other low-pass filter based GCNs is that it goes beyond low frequencies and consider richer notions of regularity. Why doesn't this advantage manifest on performances on other types of graph data (e.g. social networks)?  \n\n* The result in Table 2 does not seem promising. If LEGS only performs well on graphs that exhibit certain properties, showing results on synthetic datasets would help.   \n\n* I suggest that authors should report results on larger datasets like QM9. All experiments are conducted on datasets with no more than 5000 instances. Or is that due to computational complexity and scalability issues? \n\n* What are the advantages of the proposed method when compared with scattering-GCN [1]?\nThey also address the problem of oversmoothing and scattering-GCN is also learned in a data-driven fashion. How does scattering-GCN  perform if we obtain whole-graph features by aggregating node features obtained by their method? Why is it not included in the baseline?\n\n[1] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in graph convolutional networks. arXiv preprint arXiv:2003.08414, 2020.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Notable contribution in learning the scales of graph scattering transforms.",
            "review": "Summary:\n\nThis paper proposes a novel graph neural network-based architecture. Building upon the theoretical success of graph scattering transforms, the authors propose to learn some aspects of it providing them with more flexibility to adapt to data (recall that graph scattering transforms are built on pre-designed graph wavelet filter banks and do not learn from data). By dropping the dyadic distribution of frequencies within the wavelet bank, the proposed architecture actually learns a more suitable frequency separation among the different wavelets.\n\nStrong points:\n\nThe paper is well written and technically solid.\n\nThe idea of flexibilizing scattering transforms while retaining key theoretical contributions is a notable contribution. In particular, the idea of learning the dyadic nature of the frequency bands in the wavelet bank is a key aspect, since the frequencies in graphs are rarely evenly spaced, and thus the ability to partition the space of frequencies in other form than a dyadic one is bound to bring substantial improvements.\n\nThe numerical experiments in the paper deal with new biochemical datasets which are much more interesting and practically useful than the de-facto benchmarks of semi-supervised learning in citation networks. This is a welcome change to see actually useful applications of graph neural networks.\n\nWeak points:\n\nNo real weak points except for a common misunderstanding of the low-pass nature of filters in a graph convolutional neural network. Please, see below for details. I seriously encourage the authors to re-write parts of the abstract and introduction to avoid perpetuating the misconception that graph convolutional neural networks can only learn low-pass filters.\n\nRecommendation:\n\nThis is a very interesting contribution and, provided the clarifications that I mention below, I firmly support the acceptance of this paper into ICLR.\n\nMajor comment:\n\nThe claim that GCNs rely on low-pass graph filters is, at the very least, misleading. As a matter of fact, even a graph convolution with an order-one polynomial can be a high-pass. To see this, assume we adopt the graph Laplacian as the matrix description of the graph. If this is the case, we know that low-eigenvalues correspond to low-frequencies and high-eigenvalues to high-frequencies (according to the notion of total variation). If this is the case, then a filter like H(L) = 0*I+1*L gives a frequency response of h(lambda) = lambda which is 0 for the zero eigenvalue, and grows for larger eigenvalues. This is an example of a high-pass filter. Likewise, assume that we use the adjacency matrix. If this is the case, then the highest real eigenvalue is the one with the lowest frequency, and depending on how far we are from that eigenvalue (measured as modulus operation in the complex plane), the higher the frequency is. So, for the sake of argument, let us assume that the adjacency matrix has real eigenvalues and is normalized by the largest eigenvalue, so that all eigenvalues are contained in [-1,1]. For this situation, eigenvalues closest to 1 will be low frequencies, and eigenvalues closest to -1 will be high frequencies. Then, a filter of the form H(A) = 0.5*I - 0.5*A gives a frequency response of h(lambda) = 0.5 * (1-lambda) which has value 0 for the lowest frequency, and value 1 for the highest possible frequency. This makes such a filter a high-pass filter. As we can see, we have just constructed two filters (one for the Laplacian and one for the adjacency) where both filters are of order one, and still high-pass filters. Of course, it becomes easier to build high-pass filters if the orders of the polynomials are higher (i.e. ChebNets). Therefore, since the coefficients of the graph convolutions are learned from data, it is very hard to argue that the resulting filters will be low pass (i.e. they need not be, there is nothing preventing an order one graph filter to be a high-pass and there is no enforcement of such a constraint in the original formulation of graph convolutional neural networks).\n\nThis claim is first found in the abstract, and later repeated in the introduction. With the clarifications from the introduction, I may understand where the misinterpretation arises. Kipf's GCNs use as graph matrix a normalization of (I+A) and consider an order one polynomial with the zeroth coefficient set to zero: h*(I+A). This forces h_0 = h_1 in an order-one graph filter on the adjacency matrix, which would otherwise be written as h_0 * I + h_1 * A. By forcing h_0 = h_1, we are indeed forcing the filters to be low-pass, i.e., we always learn filters of the form h(lambda) = h*(1+lambda) where we only learn the coefficient h. However, this is a design choice of Kipf's GCNs. The general formulation of a graph convolutional neural network (see Defferrard's ChebNets, for instance), does not require h_0 = h_1 and thus can also learn high-pass filters as discussed above. Interestingly enough, even if we would consider the normalization of (I+A) as the graph matrix, but we don't force the zeroth order coefficient to be zero, we would be able to have filters of the form h_0 * I + h_1 * (I+A) which would actually allow the learned filters to be high pass (i.e. h_0 = 1, h_1 = -0.5, see above). In any case, what I mean here, is that learning low-pass filters is a self-inflicted problem by Kipf's GCNs that can be very easily avoided by just using a more general formulation of graph convolutional neural networks (for instance, Defferrard's ChebNets).\n\nIn summary, I would suggest the authors to avoid any mention that graph convolutional neural networks only learn low-pass filters. Otherwise, this would perpetuate an important misunderstanding that has been around for a while now and is misleading research efforts. In other words, yes, Kipf's 'GCN' implementation of graph convolutional neural networks can only learn low-pass filters. However, the general definition of graph convolutional neural networks (suggested in Defferrard's ChebNets, and formalized in the signal processing literature regarding GNNs) by no means implies that the learned filters are low-pass. Thus, 'graph convolutional networks' are not oversmoothers, or only learn low-pass filters. Kipf's GCN does (and it can be avoided by just using a different implementation).\n\nIn any case, this does not alter the contribution of the paper. I would just request the authors to correct this aspect to avoid perpetuating the misunderstanding surrounding graph convolutional neural networks.\n\nMinor comments:\n\n1) The authors suggest two learnable adaptations of graph scattering transforms. The parameter alpha in the graph matrix, and the scales. However, the authors find out that the parameter alpha does not contribute anything in training and is thus arbitrarily set to 1/2. I would suggest, then, that the authors focus on the scales as the learnable adaptation, and only mention in passing that alpha can be potentially trained, but that the numerical experiments in this paper show no improvement by doing so. I believe this would put the focus and draw attention to, probably, the main contribution of the paper.\n\n2) When referring to GCNs, the authors cite Kipf's paper, Veličković's paper and Abu-El-Haija. The GAT architecture in Veličkovic's paper is not a graph convolutional neural network, so it shouldn't be cited here (GATs are, probably, the first case of a popular graph neural network architecture that is not convolutional). Also, I believe it is unfair not to cite the two main contributions to GCNs: Bruna's 'Deep Spectral Networks' paper from ICLR 2014, and Defferrard's 'ChebNets' from NeurIPS 2016. (Omission of ChebNets would explain the misunderstanding with respect to the graph convolutional neural networks being low-pass filters).",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Principled approch, minor contribution",
            "review": "Post discussion period:\n-------------------------------\nI read the other reviews and the author's response. Thank you for performing these extra experiments - they make the experimental section more complete. I still feel that the technical contribution of this paper is marginal and stick to my original rating.\n\n\n-------------------------------\nSummary: \n\nThe paper suggests a new graph neural network architecture based on a recently suggested wavelet-like transform for graph data called geometric scattering transform (Gao et al., 2019; Gama et al., 2019a; Zou & Lerman, 2019).  The Geometric Scattering Transform is a fixed feature extractor with a GNN structure and fixed wavelet filters that can extract both high and low-frequency features. The resulting features were shown to be effective in various graph analysis and learning tasks. Importantly, this transform relies heavily on two fixed choices: (1) A row -stochastic diffusion matrix P= ½ (I_n +WD^{-1}) that is used to extract features (2) A specific choice of dyadic frequency bands to partition the spectrum and governs the wavelet filter bank. \n\nThe main contribution of the paper is to relax these two hard-coded choices (1-2) and suggest a mechanism that will learn them from data. Concretely, for (1) the authors suggest adding a learnable parameter \\alpha  that controls the laziness of the operator P, i.e. P=\\alpha* I_n + (1-\\alpha)WD^{-1}. For (2), the authors suggest learning (soft) frequency bands using a softmax mechanism. The authors then show that two useful properties of the original transform (stability, equivariance) are still valid The authors present results of their method on graph classification and regression tasks. The results indicate that the method performs well.\n\n \n\nStrong points:\n\nThe idea of taking a fixed architecture that works well and relaxing it to have components that are learned from data is a good idea that has worked well before.\nThe approach is principled: (1) Take a good fixed extractor (2) Relax it to better represent the data (3) show that the good properties of the original transform still hold (4) compare to the original transform. \n\nWeak points:\n\nThe main contribution of this paper, namely relaxing the laziness parameter and learning the frequency bands, is minor. I don’t think that it is sufficient for an ICLR paper.\nThe relaxation of the laziness parameter is not actually used throughout the paper since it did not work well.\nThe evaluation is rather limited, especially when considering the relatively minor novelty of the model. For example,  the authors do not compare to state of the art models such as GIN.\n\nRecommendation:\n\n Although the idea of the paper makes sense, and the method seems to work, I believe its contribution is limited.\n\n\nMinor comments:\n\n- Equation 2 is difficult to parse, please explain all notation.\n- Section 2 - it might be useful to exemplify/illustrate the transform on simple graphs. \n- Lemma 1 - why self-adjointness is important?\n- What is a nonexpansive frame?\n- When learning the F matrix - how do you make sure there are no repeating rows?\n-  “A somewhat less explored domain for GNNs is in biochemical graphs that represent molecules and tend to be overall smaller and less connected than social networks.” - I think that the datasets used in the paper are quite standard (DD, ENZYMES,...)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}