{
    "Decision": "",
    "Reviews": [
        {
            "title": "A simple and clever trick, but the paper could be made much stronger",
            "review": "This paper presents a simple trick in the family of MixUp-like methods: rather than predicting $f(x) \\rightarrow y$, predict $f([x_1,x_2]) \\rightarrow y’$, where $y’ = (y_1+y_2)/2$. And that’s it! This is delightfully simple, and seems to work quite well.\n\nI like this paper but I also think it could be made much stronger with more work. It feels like a preliminary draft of an interesting idea.\n\nThe main positives I see are:\n+ simple, widely applicable\n+ consistent performance gain across a lot of different settings\n\nNegatives include:\n- it’s a minor variation on the theme of MixUp, CutMix, etc\n- framing this as contrastive learning is not fully justified\n- little insight or analysis of why it works\n- few comparisons to prior methods\n- some concerns with the fairness of the experiments\n\nFrom the perspective of technical novelty, this paper is only a tiny step away from prior work, such as CutMix. If that tiny step is meaningful, and yields new results or understanding, then I’m all for it. But with the present paper, what I get out is less exciting, something like: here is a slightly better way to do CutMix, without new justification or better understanding of why it works.\n\nI found the connection to contrastive learning to be intriguing but forced. Especially in the intro of the paper, I think the contrastive story just obfuscates what’s going on. The method is so simple, I would state it right up front, in the abstract, and in the first paragraph of the intro. And I would connect it immediately to the “Mix” line of work, since its connection to those methods is much tighter, in my opinion, than its connection to contrastive learning.\n\nNonetheless, it is interesting that this method might be implicitly learning about similarity and dissimilarity. I like that angle and wish it were analyzed further. I think the paper could be made much stronger by adding some theory as to _why_ the method works, which is supported by math or experiment. Right now there are only vague statements that implicitly the network must be comparing the two input images. Can this computation be identified in the learned network weights? Is it expected from first principles? Is it somehow the optimal strategy?\n\nAside from the framing and analysis, the results look strong, but I have some concerns about them as well. First, while the experiments are effective at convincing me that ImCLR boosts performance over CE, they are not convincing in showing that ImCLR outperforms alternative regularizers and mixing strategies. There are now a lot of papers published that do some similar kind of data mixing, e.g., MixUp, CutOut, CutMix, AugMix [Hendrycks et al., ICLR 2020], and some unpublished papers that have nonetheless been online for some time, e.g., CowMask (https://arxiv.org/abs/2003.12022). Within this line of work, the present paper only compares against CutMix (and only on CIFAR, while Imagenet is arguably more competitive and interesting). How does ImCLR stack up against the other contenders? Second, there are two potential advantages ImCLR has over the baselines: 1) it has more parameters, 2) it sees twice as many images for a fixed number of forward/background passes. The paper argues that the increase in parameters is marginal, but then again so are the gains in performance. Section 3.5 runs a nice experiment where the same image is concatenated to itself. I feel like this can be considered a control to see if simply having more parameters helped. However, that point is not discussed. I was left a bit uneasy about the increase in parameter count, although I admit I doubt it makes a difference. I would also like to see a control for the fact that ImCLR sees two images on each forward pass rather than one. Does this mean that ImCLR is effectively trained on twice as many epochs as the baselines? Or was this factor controlled? One control would be to train the baselines on batch sizes that are twice as big.\n\nI think the extension to concatenating k images is quite intriguing. I wonder why it didn’t work well? Perhaps rather than averaging the $y$ vectors they should be concatenated, e.g., there could be k output heads. In the present setup, there is ambiguity as to which index of $y$ corresponds to which image. Perhaps that ambiguity is good — maybe it has a regularizing effect — or perhaps it is bad. I think it would be interesting to experiment with this.\n\nIn sum, the proposed method looks promising, but I think the paper still has a ways to go before it makes a strong contribution. I encourage the authors to continue pushing it forward.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "The authors propose a straightforward strategy to improve the performance of supervised classification models. Instead of passing in a single image, two images are concatenated together and the model is trained to identify both image classes. This is similar to MixUp which averages pixel values together instead of concatenating them. At test time, a target image is concatenated with itself and passed through the network to get a prediction.\n\nStrengths:\n\n+ This is a simple, easy-to-implement idea that adds a consistent boost to performance across experiments and ablations.\n+ Writing of the paper is clear and easy to follow\n+ Experiments cover the standard suite of classification settings comparing various backbone models and results are shown to be complementary with existing sophisticated data augmentation strategies.\n\nWeaknesses:\n\n- The formulation requires odd use of the network at test time inference. It is unusual and a bit of a waste to have to concatenate two copies of the image together to get the network's prediction. How does the network perform if you only pass in a single copy of the image?\n\nIf you have to do double the work at test time, you could arguably get a better return by leveraging standard test time augmentation techniques. How does performance w/ ImCLR compare to a baseline with a simple test time augmentation that results in running inference twice (e.g. flipping left/right and averaging the result)? This seems like a more fair comparison since the resulting FLOPs are comparable.\n\n- While I'm not one to care about cutting-edge, state-of-the-art numbers, I have to point out that the error rates reported in the paper seem unusually high. In Table 3 we observe that CIFAR10 error is improved from 7.65 to 7.51 and CIFAR100 error from 25.93 to 25.29. But in my experience you can easily train out-of-the-box resnet with standard data augmentation and see errors closer to 4-6% for CIFAR10 and 20-22% for CIFAR100.\n\nThat's not to mention numbers from competitive data-augmentation papers. AutoAugment (Cubuk et al) which the authors include in their ablations report error rates of 1.5% and 10.7% for CIFAR10 and 100 respectively. On one hand, I know reproducing the very best numbers on a benchmark can be difficult, but on the other, the numbers reported in this paper fall short of what I would consider reasonable expectations for baseline results.\n\n- I am not convinced about the framing of this paper from the outset as an implicit way of performing contrastive learning. This paper is not a self-supervised learning paper and the use and evaluation of this method is not comparable to papers like SimCLR, I think readers are likely to come in with false expectations given the title and abstract.\n\n- There is almost no discussion at all of mixup (Zhang et al) which seems much more relevant to compare to and discuss in depth than say, all the discussion on contrastive learning papers in the related work.\n\nSummary:\n\nWhile the authors have put together a clear case for a simple strategy that results in consistent gains on standard classification benchmarks, due to the concerns outlined in the weaknesses (wasted compute at test time, poor benchmark performance, misleading framing of the method) I don't recommend this paper for acceptance.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel method, good empirical results, however the claim of contrastive learning is not really justified",
            "review": "Summary: \n\nThis paper proposes a novel approach for learning with neural networks. The main idea is to concatenate two (or more) items (in this case images) to make a paired input to the network. The label for the paired item is set to a two-hot vector of two original labels. Two-hot vector takes two values of 0.5 on the positions of two labels, while other elements of the vector are set to zero. The claim is that the proposed structure can implicitly learn similarity/dissimilarity of images, making it a novel and competitive approach for contrastive learning.\n\nPros:\n\n1- The idea is novel and up to my knowledge has not been investigated before. \n\n2- Authors provide an extensive set of experiments to inspect the benefits of the proposed learning approach. \n\n3- ImCLR improves the supervised classification results on many datasets, including more than 3% improvement on tiny imagenet dataset with a resnet network. \n \n\nCons:\n\n1- The main message of the paper is that concatenation of items helps the network to learn similarity or dissimilarities implicitly. However, it is not clear what it exactly means. How does it lead to implicit learning? Thus, it is needed that authors elaborate more on the implicit constrastive learning.\n\nThe claim could be more systematically investigated. For example, checking the t-SNE plots of the final embeddings could be helpful. Observing a slight improvement in accuracy does not necessarily mean that the proposed method learns embeddings consistent with the semantics.\n\n2- The distribution of data in training and test sets are fundamentally different. During the training two samples (probably without replacement) are taken to be concatenated as the input to the network. So we feed a concatenation of two different images, most probably with different labels. However in test time, the same image is concatenated to itself (see the last sentence before Section 2.1). There is a significant difference between the distribution of training input and test input to the network. How can the network still work properly in such a setting?\n\n3- Authors claim that their approach is a sort of contrastive learning. However, the assumptions they made are quite different to well-known contrastive learning approaches (eg. SimCLR). The setting of recent contrastive learning approaches is completely unsupervised, for feature learning. Various augmentations of an instance is considered as a positive item, while other instances are negative ones. In such a scenario, either a pairwise loss or triplet loss learn surprisingly informative representations for the images. Given such a representation in full unsupervised scenario enables us to use a simple MLP, and acquire competing classification results.\n\nComments and questions:\n\n1- The abstract on openreview and the abstract of submitted paper do not match.\n\n2- In table 4 what does CE stand for? Is it base without ImCLR?\n\n3- Page 5, last paragraph, “Understanding performance with varying epochs.” There is probably a mistake, as this paragraph describes the performance with a varying labeled set.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Data augmentation for vision networks with limited novelty and weak baselines",
            "review": "This quality of writing of the paper has some issues: the aims of the paper are not completely clear at first reading. It is argued that ImCLR is an implicit contrastive training method, but in my opinion it is more fair to describe it as a data-augmentation approach.\n\nThe idea is to take multiple (distorted) images from dataset, concatenate them and predict all the labels from the concatenated image.\n\nMy first issue is that the naming; the method misuses the notion of \"contrastive learning\". Although it is somewhat undefined what \n\"contrastive learning\" means, there is common theme:\n- The network only decides whether two (or multiple instances) of the same class. The advantage then is that it is relatively simple to add new classes to the system, since one just adds instances with new labels. The system does not learn to do a multi-way classification into a fixed set of categories.\n\nHowever, this is not true in this setup: the classifier is trained by producing multiple labels from a fixed set of classes. So this method is not really contrastive at all. It is not even clear whether it is \"implicitly\" contrastive, or what the semantics of that notion should be.\n\nGenerally, I find that the paper describes a training data augmentation technique rather than a new paradigm or any kind of contrastive learning.\n\nOriginality: The methodology is very similar to CutMix, although the paper makes the claim that it is independent of it and can be combined with CutMix, I fail to see the fundamental differences compared with CutMix. They present an experiment that should back their claims, in which they outperform CutMix and in which the combination of CutMix and ImCLR outperforms both. However,  the causes of this are unclear. I think the paper needs much more extensive explanations about the differences of ImCLR and CutMix.\n\nAn important strength of the paper is the extensive set of experimental results that should back the claim of significant gains by ImCLR.\n\nAnother major weakness of the paper is the relatively weak baselines. For example 7%/26% error on CIFAR-10/100 are certainly not a very strong baselines. Even with relatively modest augmentation, one can easily train networks with 5% and 21% error respectively.\n\nThe writing style of the paper is relatively confusing, various pieces of information are given at ad hoc places, the approach is not clearly motivated and it requires a lot of guessing to read the paper in one go.\n\nConclusion: The novelty of the paper seems very limited compared to CutMix, the categorization and naming of the approach is somewhat misleading (not really contrastive training), the baselines are weak and the writing is somewhat confusing. I don't think that this paper should be accepted at ICLR.\n\nMinor remarks:\nTypo on page 3: implicitlly",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}