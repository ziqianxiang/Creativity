{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper was reviewed by 3 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere."
    },
    "Reviews": [
        {
            "title": "compositional reasoning under uncertainty",
            "review": "This work introduces the \"compositional reasoning under uncertainty\" benchmark. \n\nThis is motivated by the observations that standard [classification] benchmarks: \n 1) consider only a fixed set of category labels, \n 2) do not evaluate compositional concept learning  \n 3) do not explicitly capture a notion of reasoning under uncertainty\n\nThe work proposes a task that requires several aspects of compositional reasoning, such as boolean operations, counting, etc. The author(s) introduce a notion of a \"compositionality gap\" to quantify the difficultly of each generalization type. \n\nI buy the vision of the work and I consider it a promising direction. However, I am not completely convinced by the work and its contribution to the existing works, especially the recent related work. \n\nWhat is the source of \"uncertainty\" in this challenge? What kind of \"uncertainty\" are we dealing with? The only place that you refer to it is the 2nd paragraph of your intro, but otherwise, I don't see anything else (while there are 18 mentions of \"uncertainty\" in section 1-2, there is only one mention of \"uncertainty\" in the whole main body section 3-5 and that is only in the title of Section 3!) \nWhy I am not convinced this is a good (useful?) notion of \"reasoning under uncertainty\". Just because two formulas/concepts can describe one scene, implies \"uncertainty\"? If so, what is the \"reasoning\" under this defined \"uncertainty\"? \n\nIn terms of writing: the intro/abstract are very clearly written and are easy to follow. But the rest of the work could be improved. \n\nOverall, good work but requires a bit of work polishing to make the contributions stronger. \n\n==============\n\nMinor issues: \"Producitivity\": typo? \n\nI felt that you're citing so many works that at times make it difficult to read. \n\n\"disentangling\": clarify or cite. It's not clear how it's related to the tasks listed in Fig 5. \n\nDefine \"productive generalization\" and \"systemic generalization\".\n\n======= \nUPDATE after reading author response:  The authors' example answered my question about the nature of the \"uncertainty\" in their setting. I have increased my confidence score and have retained my marginally-positive evaluation. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Initial Review",
            "review": "Summary:\n\nThe following work presents a CLEVR-based compositionality benchmark. The task of the model is to verify logical statements about an image, and in order to achieve such, must learn how to map individual statements to a composition of functions over the image checking for color, placement, shape, etc. Specific to this dataset is that it is explicitly few-shot, which forces the models to generalize very quickly and to infer under uncertainty.\n\n\nStrengths:\n\n-Introduction of novel compositionality gap to measure the extent to which any learned solution needs to extrapolate on each train/test split \n\n-Dataset is very carefully split in multiple different ways to individually test for generalization across various aspects\n\n-Extensive analysis, testing a wide range of the standard baseline methods\n\nWeaknesses and Concerns:\n\n-Like CLEVR, the statements are generated based on a grammar over a controlled set of relations/attributes/terms. While this is necessary for ruling out linguistic complexities from the analysis, one wonders how useful the dataset will be in the long run. Specifically because the dataset is setup to be used in a few-shot setting, models that perform increasingly better will do so by incorporating strong priors that match the true distribution of the dataset, at which point would we not simply be overfitting to this dataset due to its limited linguistic scope as we did for CLEVR?\n\n-It's not clear to me what the  benefit of the audio modality is. Specifically, I imagine that there are a number of different ways to encode the relational information in audio, with each possibly giving different results. Any conclusions are likely specific to the particular encoding rather than the audio modality as a whole.\n\nAdditional questions:\n\n-What were the main conclusions that we can draw from this dataset that would have been impossible in CLEVR?\n\nTypos:\n\nAppendix Figure 18: y axis has no tic marks\n\nOverall, I like the clean setup of the dataset's split and the some of the few-shot analysis on exsiting models. However, the novelty of the dataset as a whole is limited, as it is effectively a modification of CLEVR. It is not yet clear to me that the analysis has yielded any particularly groundbreaking insights, though I would love for the authors to correct me on that. Most importantly, I have strong doubts about the long-term usefulness of this dataset as a benchmark before we start overfitting, given its limited scope and few-shot setting.\n\n---------------------------------------------------------------------------------------------\n\nPost rebuttal update:\n\nAfter reading the clarifications from the authors, it is now more clear that the dataset is about the learnability of certain hypotheses as opposed to that of test-time verification. I am generally satisfied with the responses given by the authors, and willing to buy the statement that having a per-concept breakdown of learnability can be seen as a feature rather than a weakness pertaining to the use of a \"toy\" dataset. I think there are some valid insights as provided by this work, though I am more skeptical regarding the superiority of transformers on disentanglement tasks as the improvement gap was relatively small and there are various implementation details in the used transformers and relationship networks that could presumably shrink or even invert that gap. \n\nOverall, I think the writing clarity is still a significant concern. Several detailed passes over the paper were necessary just to get the general picture of what was going on. I think the toy example provided in the author's response is certainly helpful, and should improve the paper in this area somewhat. \n\nBased on the above, I am upgrading my rating to marginally above threshold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting Question with Design Flaws and Missing References",
            "review": "This work proposes the CURI dataset to measure productive concept learning under uncertainty. The dataset is designed using a concept space defined by a language and formulated as a few-shot meta-learning problem to tell apart in-concept samples from out-of-concept samples. The authors also design several out-of-generalization data splits that test models' ood generalization performance. Together with an oracle model, the authors show using the prototypical network that the compositional concept learning and reasoning problem in CURI is challenging.\n\n---\n\n1. The proposed dataset touches the problem of generalizable concept learning, which is important, as more and more methods have been shown to be able to well fit the iid distributions.\n2. The dataset is well-designed in terms of generalization. It explicitly separates the concept space and proposes several aspects for the generalization problem.\n3. A good metric for the generalization gap that can measure how difficult each data split is by using oracle models.\n\n---\n\n1. One of the major problems I consider in this work is how to make sure the positive samples follow a unique concept. The authors mention that the actual concept space is unbounded and uncertain. Therefore, there could potentially be an infinite number of concepts where the positive samples are satisfied. Is it possible to make sure that the positive cases and the negative cases can prune all other concepts to make sure the actual concept tested in $D_{supp}$ is unique? If not, during testing, how to make sure a ''negative'' sample is truly negative rather than coincidentally satisfying the concept? If the problems can not be properly addressed, then the dataset may contain fundamental errors.\n2. Another major problem is the missing references to existing few-shot meta-learning works. There has been a plethora of related papers and this work only compares with the most basic prototypical network. See the following references for examples. Besides, a couple of recent works also try to address the compositional learning problem. These models are also missing. Without sufficient experiments, it's hard to paint a full picture of the dataset.\n3. The statements regarding CLEVR and PGM are inaccurate. The paper states that questions to ask need to be inferred in CURI, which I believe is not true because the question is simply asking a model to tell an odd one out. It's just that CLEVR provide questions that can be parsed to programs while CURI does not. For PGM, it's stated that \"once the constraints of a puzzle are identified\" ... However, the uncertainty problem and concept learning problem are right at identifying the constraints, same as in CURI where the concept description needs inferring.\n4. Typos: Page 4 at the bottom \"to to\" -> \"to\", \"yeild\" -> \"yield\"\n5. How is $P(H=h | D_{supp})$ defined? I think this could be a major part impacting the performance. If you have a proper reasoner to do it (say an oracle model), it doesn't make sense to have such a low accuracy. But if it is just a multi-way classification setup, then the performance can be poor.\n6. A very important and highly related work is missing: Andreas, Jacob, Dan Klein, and Sergey Levine. \"Learning with latent language.\" ACL 2018. This work is basically in the same setup as CURI and the models used, the methodology, the problem it considers \"at least\" need some in-depth discussions. \n\n[1] Lake, Brenden M. \"Compositional generalization through meta sequence-to-sequence learning.\" NeurIPS. 2019.\n[2] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.\n[3] Bertinetto, Luca, et al. \"Meta-learning with differentiable closed-form solvers.\" ICLR. 2018.\n[4] Lee, Kwonjoon, et al. \"Meta-learning with differentiable convex optimization.\" CVPR. 2019.\n[5] Sung, Flood, et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}