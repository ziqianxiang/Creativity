{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "Summary: \n\nIn this paper, the authors proposed a new meta-learning algorithm with base learner structure search. The greedy idea is applied in the algorithm design. They conduct the experiments on miniImagenet and FC-100.\n\nPros:\n(1) It is interesting to investigate the best structure in meta-learning, which benefits the progress of automated meta-learning.\n\n\nCons:\n(1) The technical contribution of this paper is quite limited. Besides the pruning part, the proposed method CAML (Algorithm 1) is the same as the simple version of T-NAS (i.e., Auto-MAML), which has been proposed and discussed in T-NAS (see Algorithm 3 in [1]). For the pruning part, I think it is a common practice and the motivation of layer confidence is quite straightforward.\n\n(2) The performance of CAML is worse than T-NAS and CAML++ is worse than T-NAS++ (see the results of Table 3 [1]). It would be more convincing if the authors could report the results under a fair setting rather than compare CAML++ with T-NAS.\n\nOverall, I think the main contribution of this paper is entirely the same as the literature. It should be rejected.\n\n[1] Lian, Dongze, et al. \"Towards fast adaptation of neural architectures with meta learning.\" International Conference on Learning Representations. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting paper, but unclear if there's any contribution/novelty compared to previous work",
            "review": "This paper proposes an approach (CAML) to meta-learning that combines neural architecture search (NAS) with meta-learned weight initialization. Unlike most previous work combining NAS and meta-learning, CAML jointly learns the best architecture and weight initialization, iteratively alternating between learning updates for each. This is enabled by the proposed progressive connection consolidation (PCC) technique and leads to strong empirical performance and better sample efficiency.\n\nThe paper is clear and I think the motivation is sound, and the experimental analysis covers a number of benchmarks. However, I do have a number of concerns that I believe need to be addressed.\n\nMy main concern is that it seems unclear whether there is any contribution of this method over the previously published Meta-NAS work (Elsken et al, 2020); and I have a few other comments:\n\n1) Meta-NAS is surprisingly not discussed in the related work section, and only cited as an entry in the experimental results table. It is quite similar: it also jointly optimises architecture and initialization weights and applies soft pruning, which are stated as two of the contributions here. What is the technical contribution of CAML over this existing work (eg. is it just PCC?), and if so, how does PCC compare to the soft pruning / annealing strategy in Meta-NAS?\n\n2) Experimentally, the performance of CAML on mini-Imagenet is shown to be a little better than Meta-NAS (small), but this could just be explained by the fact that Meta-NAS uses Reptile in the inner loop rather than MAML (the performance margin between them is similar). I think some ablations are needed to clarify and quantify the improvement over Meta-NAS, eg. an ablation with CAML using REPTILE, an ablation comparing PCC with Meta-NAS’ soft pruning/annealing, and any other differences to Meta-NAS (these should also be clearly stated in related work). If the performance gap can indeed be explained away by the difference between MAML and REPTILE, then I'm not sure that there's a valuable contribution being made.\n\n3) The experimental results tables are also misleading, because other approaches are within the error bounds and should also be highlighted. Ie. for mini-Imagenet, CAML++ does not show statistically significant improvement over MAML++ on 5-shot, and Meta-NAS (big) on both 1- and 5-shot. Similarly in Table 2, CAML++ actually falls within the error bars of T-NAS for all cases.\n\n4) Further, the paper mentions that different within-task-sets are used for the meta-learning loops for architecture and adaptation - is this then comparable with the analysis in other previous papers (again including Meta-NAS)?\n\n5) I find Figure 2 confusing and I don’t think it explains MAML as well as it could: what do thetahat_m and phihat_m refer to? (I don’t think they’re introduced in the text, shouldn’t it just be without the hat?)\n\n6) There are also quite a few issues with writing - a few examples are below but I recommend the authors do a detailed proofread to check for grammar/spelling and clarity.\n\n\nWriting issues, typos, grammar, etc:\n\n- “With searching for only once” in the abstract\n\n- Page 2, “... need to perform whole meta-learning process”, should be “... the whole …”\n\n- Page 2, “mete-learning”.\n\n- Page 3, “... (MAML) try to find …” should be “tries”; “In i-th meta-train task” is missing a “the”\n\n- Page 3, MAML refers to Figure 9, which I don’t believe is the correct one.\n\n- Conclusion says “all three datasets”, but the third is in the appendix and not mentioned in the main text.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Connection-Adaptive Meta-learning",
            "review": "This paper proposes to meta-learn both the architecture and the weights of the model instead of applying a two-stage approach. The proposed method Connection-Adaptive Meta-learning (CAML) takes both the advantages of MAML and DARTS. Results on some benchmarks show that the CAML performs well.\n\nThe motivation is clear, and the paper is well-written.  It is reasonable to jointly search architectures and train the meta-weights on consolidated connections. \nThere are several concerns on this paper:\n1. Algorithm: although both the architecture and the weights are meta-learned jointly, it is still optimized in an alternative way as shown in Eq. 5-8. The authors should show if we perform those two-stage comparisons methods multiple times, could they get better results?\n2. Experiments:\n2.1 The authors compare the CAML with the ConvNets results on MiniImageNet, but the performance is not good enough. First, some strong MAML variants and embedding based methods with ConvNets backbone should be compared, please consider some more recent methods; Second, the authors should also try CAML on deeper backbones and see whether the proposed CAML can work for them.\n2.2 The authors compare CAML with other methods on the few-shot learning benchmarks. To show the ability of CAML, some NAS tasks should also be considered.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Timely paper, good results.",
            "review": "### Summary\n- **The first contribution** of this paper is to propose a meta-learner to co-optimize both meta-weights (initialization) and the architecture simultaneously in few steps. Current algorithms Instead use a two-steps procedure that consists in first searching the architecture and then retraining the weights fixing the searched architecture. The authors claim that this two-steps optimization would lead to suboptimal solutions. \n- **Second contribution:** to gradually prune the supernetwork during meta training using a confidence score computed on each layer. Every 5 epochs, the layer with largest score is selected and the operation with the largest weight value is kept while removing the others: the kept operation is called meta-connection. \n- **Third contribution:** I’m not sure that the authors can claim their algorithm is achieving state-of-the-art performance because they are only comparing against non-SOTA baselines. I would restate the contribution saying that the method can improve x% on the baseline and that it can find architectures with much fewer parameters wr.t to the baselines with better performance. \n\n### Considerations\n- In eq. 4, I think that the bi-level optimization problem should be defined with respect to two different loss functions: $\\mathcal{L}^{train}$ for the inner problem and $\\mathcal{L}^{val}$ for the outer one. The architecture is considered to be a hyper-parameter selected to maximize the performance on the validation set, training with data of the training set in the inner loop, hence the nested optimization. In few shot learning scenarios the Training set is the support and the validation is the query. \n- I would find some space to describe all the methods that you compare with. For instance, you never mention MetaNas from Elsken et al 2020, but it only appears in the experiment table. This is actually the closest to your method in terms of training time, even if the number of parameters of the final model is higher.\n- Maybe it would be helpful to move algorithm1 (CAML) in the main paper. \n- Minor spelling error in AppendixB title: compete algprithm ->  complete algorithm.\n- I think the paper is nice and well written, the experimental setting and the ablation study are ok. I'll wait for the authors' response to my questions for my final decision.\n\n### Questions\n- Q1: Why do you need two different splits for updating the connections and updating the meta weights? Also, I suggest you use more informative split names, eg. NAS-split, MAML-split ...\n- Q2: What is the interplay between PCC and the number of parameters of the final model? I don't think I've found in the paper any reference to it.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}