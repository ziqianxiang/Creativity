{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies synthetic data generation for graphs under the constraint of edge differential privacy. There were a number of concerns/topics of discussions, which we consider separately:\n1. Theoretical contributions. There are not that many theoretical contributions in this paper. I think this is OK, if the other components are compelling enough. On the theory, the authors mention that accounting for the constants is important in the analysis of DPSGD. On the contrary, I would say that these constants are not very important: if one requires specific constants, numerical procedures can determine values, otherwise for the sake of theory, no one generally needs these constant factors.\n\n2. Empirical/experimental contributions. This was the primary axis for evaluation for this paper. None of the authors were especially compelled by the results. The methods are essentially combinations of known tools from the literature, and it is not clear why these are the right ones to solve this problem in particular. If the results were very exciting, that might be sufficient to warrant acceptance, but it is still not clear how significant the cost of privacy is in this setting. The experiments are not thorough enough to give serious insight here. It is a significant oversight to not provide results on DPGGAN without the privacy constraint, as this is the best performing model with privacy. The omission of something as important as this (and lack of inclusion in the response, with only a promise to include later) is indication that the experiments are not sufficiently mature to warrant publication at this time. The decision of rejection is primarily based on concerns related to the empirical and experimental contributions.\n\n3. Privacy versus link reconstruction. Reviewer 4 had concerns about the notion of privacy, claiming that it does not correspond to the probability of a link being irrecoverable. This is differential privacy \"working as intended\", which is not intended to make each link be irrecoverable: it is simply to make sure the answer would be similar whether or not the edge were actually present, so it may be possible to predict the presence of an edge even if we are differentially private with respect to it (e.g., the presence of many other short paths between two nodes are likely to imply presence of an edge). Some discussion of this apparent contradiction might be warranted, as this might mislead reader who are specifically trying to prevent edge recovery. It might also be worthwhile to have discussion of node DP in the final paper. The authors comment \"we focus on edge privacy because it is essential for the protection of object interactions unique for network data compared with other types of data\" -- the stronger notion of node differential privacy might also be applicable here. It would indeed be interesting to know whether it can preserve the relevant statistics (some of which seem more \"global\" and thus preservable via node DP).\n\n"
    },
    "Reviews": [
        {
            "title": "Secure Network Release with Link Privacy",
            "review": "This paper considers the problem of releasing sensible structured data, where there are two inlined challenges: 1. The global network structure should be effectively preserved; 2. The link privacy should be rigorously protected. This paper looks at the secure release of network data with deep generative models. Specifically, the paper develops two models, DPGVAE and DPGGan, which can be viewed as a combination of DP-SGD and graph generation techniques. Extensive experiments are carried out on real-world network datasets, and the positive results have shown the effectiveness of the new models.\n\nI think the problem this paper looks at is both interesting and fundamental. Most of the current DP works focus on estimating one specific property of the dataset's underlying distribution, e.g., mean of the distribution. However, in many real-world applications, the tasks can not be known beforehand, and a potential solution is to release synthetic datasets under DP constraint. As far as I know, there are few papers in this area of privately releasing synthetic datasets with deep generative models.\n\nThis paper has proposed two models (DPGVAE and DPGGan), both reasonable to me. Meanwhile, experiments are carried out on real-world network datasets, and it has shown that for many structural statistics, the algorithms have competitive performance. I have little background knowledge of graph generation, and I can not give many technical comments:\n1. I will be appreciated if the authors can provide the results when $\\varepsilon = \\infty$, which I expect to be better than the case when $\\varepsilon = 10$. Does it mean your algorithm outperforms the current best algorithm in IMDB when there is no privacy constraint?\n2. I think it is also interesting to include some experiments on synthetic datasets. I believe for some extreme graph structure, DP should have more impact. For example, if the objective is to estimate the number of triangles in the graph, I expect the privacy to incur more loss when the underlying graph is complete compared with a sparse graph.\n3. From the algorithmic perspective, I  think both models can be viewed as a simple combination of DP-SGD and graph generation techniques. I am not sure this paper has made many theoretic contributions.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for paper #130",
            "review": "The paper considers the problem of learning graph properties with differential privacy (DP) constraints on edges. The problem is well-motivated in the paper with potential applications in social network learning where interactions between users (nodes) are sensitive. \n\nThe techniques involve recent advances in graph generation networks and DP deep learning. The network structure is based on recently proposed Graph VAE and VAEGAN. DPSGD is used during the training process to ensure the privacy guarantee of the process. Experiments are conducted to compare the proposed algorithm to a few nonprivate algorithms and the performance is comparable in certain applications.\n\nAn interesting observation of the paper is that it is enough to preserve privacy when training the generator network since the inference is only based on the generated network. \n\nMy main concern about the paper is its novelty. It seems to combine results from different areas. I hope the novelty can be further explained in the response.\n\nOther comments:\n1. In the experiments, it would be better to report delta values for the privacy guarantees for each experiment.\n2. It seems the exact nonprivate counterpart of DPGGAN is not included in the experiments. It would be nice to have results on it to see the exact influence of privacy. For example, in Table 2, Both DPVAE and DPGGAN seem to perform better than NetGAN and GraphRNN algorithms. It would be better if the authors can decouple the influence of privacy and network structure.\n3. It seems unnecessary to include the whole proof of DPSGD in the appendix since it is almost identical to the original paper. Including the sensitivity analysis would be enough to me.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Does not deliver on its main promise.",
            "review": "This paper proposes a method, Differentially Private Graph Generative Nets (DPGGAN), to release graph in way the preserves utility while preserving privacy. This method trains a deep graph generation model in a differentially private manner, by injecting Gaussian noise to the gradients of link reconstruction module, thereby claiming to guarantee edge privacy, while ensuring utility via a structure learning component based on a variational generative adversarial network (GAN) architecture, to enable structure-oriented graph comparison to the original.\n\nThe motivation of the work is sound, yet the work does not deliver on its main promise: while the claim is made that link privacy is protected, there is no experiment to justify this claim. While an experiment is mentioned that purportedly does this, the mentioned results are not shown, neither in the main paper, nor in the appendix. The closest the work comes to reporting such results is this statement:\n\n  \"... links predicted on the networks generated by DPGGAN are much less accurate than those predicted on the original networks (26%-35% and 15%-20% AUC drops on DBLP and IMDB, respectively) as well as the networks generated by all baselines.\"\n\nThese results are not presented, and the privacy parameters Îµ that lead to them are not discussed. Thus, it is hard to know how much utility has to be sacrificed for the sake of the privacy gains mentioned. Further, a drop of 15% does not corroborate the claim that the released data useless for link prediction.\n\nOverall, while the paper claims to offer robust privacy guarantees towards various graph attacks, such guarantees are not explicitly spelled out. It seems to be taken for granted that a differentially private link reconstruction provides the intended guarantees, yet that begs the question of what protection is achieved in practice, under a learning-based attack. The fact that learning under differentially privacy can be surprising successful, and thus constitutes an attack on differential privacy, has been established in previous work [1]. This paper should reflect more thoroughly on what privacy means in the proposed setting, how it is shown, and what utility it corresponds to. On the other hand, there have been efforts to define explicit guarantees regarding link reconstruction [2, 3], which this work does not take in consideration.\n\nIncidentally, the paper claims that graphs lack efficient universal representations, citing [Dong et al., 2019]. It is not clear how the cited paper, which studies a form of universal graph representation, supports this statement. Besides, there is work explicitly dealing with efficient universal graph representations [4], based on similar principles to those studied in the cited work.\n\nLast, previous work [5] has already established, to a higher degree that it is done here, that any kind of structural identification attack can effectively be prevented using random edge perturbation, even while important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated on the perturbed data. Given these results, it is unclear what this work adds to what has been already established.\n\nReferences:\n[1] Personal privacy vs population privacy: learning to attack anonymization. KDD 2011.\n[2] k-isomorphism: Privacy-preserving network publication against structural attacks. SIGMOD 2010.\n[3] L-opacity: Linkage-Aware Graph Anonymization. EDBT 2014.\n[4] NetLSD: Hearing the Shape of a Graph. KDD 2018.\n[5] Delineating social network data anonymization via random edge perturbation. CIKM 2012.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This work consider the problem of link privacy when releasing models that are trained on graph data. It achieves this by making a generative graph model based on a VAE differentially private. Differential privacy is obtained by adding noise to gradients during training (DPSGD approach). The paper uses graph metrics and a classification downstream task to evaluate the utility of generated graphs. Comparison of algorithms with related work and details of the mechanism could be expanded to articulate novelty and significance of the work.\n\nI. The paper is well written and nicely merges ideas of DP + VAE + graphs for edge privacy.\nII. It would be good to see the main body of the paper articulating what the difference is between papers that use DPSGD from Abadi et al. as is and this paper. That is, what exactly had to be changed for graph data. Even after reading the appendix it was not obvious.\nIII. Experimental results could include accuracy of classification tasks of graph network models as it seems baselines in the paper use non-private generated networks only for downstream tasks. It may suggest that accuracy of generated networks on these tasks is already not optimal and the utility impact of DP is smoothened as a result.\n\nDetailed comments:\n1. Theorem 1 and page 5. Please consider adding what is different from DPSGD mechanism; what was the challenge in applying DPSGD in this setting.\n2. A2 states \"However, in a more complex task like graph learning, a minor change in the training dataset can probably induce a different gap according to the chosen measurement.\" The use of word \"probably\" is worrisome. Sensitivity should be properly analyzed when guaranteeing DP and adding noise based on it.\n3. Lemma 1: please articulate how s and C depend on each other or relate, if at all. How is this result  different from that in Abadi et al. What would noise addition/clipping be?\n4. Theorem 1 proof in Appendix: please explain how this is different from Abadi et al. given that s=C.\n5. Does Algorithm 1 make use of s? If not why Lemma 1 was needed.\n6. Please explain how Algorithm 1 differs from DPGAN work in Xie et al 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}