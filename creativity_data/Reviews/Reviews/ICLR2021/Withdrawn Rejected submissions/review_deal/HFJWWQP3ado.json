{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting direction but the results are not convincing enough.",
            "review": "##########################################################################\n\n\n[Summary]:\n\n- This paper attempts to understand the underlying mechanism of neural network pruning. In concrete, the authors attack this problem from a decision boundary perspective, which unifies the analysis of different pruning techniques. Built upon this, the authors further propose a hamming-distance based metric to detect when can we prune the network and a method for pruning redundant neurons based on the cosine similarities. \n\n##########################################################################\n\n[Overall]:\n\n- Overall, I vote for rejecting. I agree that studying neural network pruning from the decision boundary perspective is a promising direction, as it directly relates to the final performance of the neural network. However, this paper overclaimed too much, and also I doubt the motivation of the proposed method for pruning. Lastly, the paper has tons of typos. (See cons below) \n\n##########################################################################\n\n[Pros]:\n\n- I like the idea of studying pruning from the decision boundary perspective, and the visualization in Figure 2(a) for comparing `neuron` level pruning and `weight` level pruning is insightful. It shows the redundancy in the network and thus explains why pruning is possible.\n\n##########################################################################\n\n[Cons]:\n\n- Overclaiming: This paper claims several contributions in the abstract, such as how theoretically study pruning. However, I did not see which section reflects this. Also, the result of the first contribution ([C1]) is well-known. The optimization for narrow networks is difficult, and its generalization performance is poor. That's why people seek to prune overparameterized neural networks. \n\n- Spline pruning policy: The extension to global pruning is problematic. I would expect the relationship between neurons at different layers is non-linear. However, the authors first do a PCA for neurons at different layers and then use the eq (3) to measure the similarity. This process should only hold in the linear case. \n\n- The experiments need to be improved, and the authors should at least provide the 1-standard error when reporting the numbers. For example, the difference between the performance of `Random Init`, `EB train`, `LW pretrain` is fairly small (up to 0.5%). Are these results statistically significant or robust enough to support the claim?\n\n##########################################################################\n\n[Questions during rebuttal period]:\n- Please address and clarify the cons above. \n\n\n##########################################################################\n\n[Some typos]:\n\n- Finally, we conduct extensive experiments to shown... -> Finally, we conduct extensive experiments to show...\n\n- the main driving factors of DN performances haves... -> the main driving factors of DN performances have\nthrough a serie of L layers... -> through a series of L layers...\n\n- lastly, overparameterized also... -> lastly, overparameterization also\n\n- reducing the amount of updated... -> reducing the amount of updates\n\n- and cross-validation as overparameterizing... -> and cross-validation as overparameterization\n\n- this methods outperforms SOTA methods... -> this method outperforms SOTA methods...",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach, very disappointing execution",
            "review": "**Summary**: The main idea is to utilize the perspective of max-affine spline for pruning, i.e., by considering the input-space partition formed via the max-affine splines due to each neuron hyperplane and its ReLU, and their successive subdivisions as you go through the layers. This can be thought of as providing a \"dual\" way to removing redundancies caused by over-parameterization, in contrast, to say the \"primal\" view of removing redundant weights directly.\n\nThe authors give the analogy of the optimal positioning of subdivision lines in the input space to optimal initialization in K-means. The point is to remark that over-parameterization is currently needed due to the lack of suitable \"data-aware initialization\" of the weights in a neural network. \n\nFurther, they consider using this perspective to detect Early-bird tickets (winning tickets that can be found early on in training) by measuring how much the input-space partitioning changes relative to the points sampled from the dataset. While this whole thing about max-splines, input-space partitioning makes sense, but, in the end, this is *nothing but a (nice) story of pruning redundant neurons via cosine-similarity between their weight vectors*.\n\n**Pros**:\n\n -They try to establish pruning from a principled perspective of how it affects the input-space partitioning. \n\n -Their criteria to detect early-bird tickets is independent of the desired pruning ratio.\n\n**Cons**:\n\n-The end product of their (coarse) approximation to this max-splines perspective is basically cosine-similarity between neurons, which is far from novel. \n\n-Weak empirical analysis, misleading comparisons with baselines, then the few baselines selected are very old, and missing crucial empirical details.\n\n-Over-selling of claims about absence of \"better initialization\" without convincing results and proper methodology.\n\n-Poor presentation: the paper is plagued with lots of typos, grammatical mistakes, vague statements and the discussion of related work is not proper. \n\n**Detailed comments**:\n\n-> End product used for pruning: Despite the interesting perspective about max-affine splines and the initial emphasis on finding principled ways of pruning, the measure used at the end is quite disappointing. They essentially end up using cosine similarities of neurons/filters, which I highly doubt to be novel. E.g,. a quick google search will show you many papers using cosine similarity for pruning, one such is https://arxiv.org/abs/1802.07653 . I guess if you used L2 distance of the weight vectors for neurons (which is a common criterion), you will probably get more or less similar results. Besides, can you give some concrete idea of how loose is your approximation?\n\n-> Weak empirical analysis and misleading comparisons: The comparison with ThiNet in Table 3 is wrong, and is hidden under the carpet! You are taking numbers from their paper, but you are missing that their unpruned network has accuracy 72.88%. However, you measure improvement wrt your unpruned accuracy of 75.99! As a matter of fact, ThiNet performs better resulting in 0.84%  drop for 30% and 1.87% for 50% as compared to 0.91% and 2.62% for your method. This also makes me suspicious that the results for global pruning in Table 6 as well. \n\nAlso, otherwise, I don’t think calling network slimming and these other methods like ThiNet (papers from ~ 2017), as the SOTA is correct, particularly in this topic with so much ongoing research. How does your method compare to more recent SOTA methods?\n\n-> Missing experiment details:\n-When using EB spline for ImageNet what criterion do you pick the EB tickets? The hamming distance like criterion that you mentioned before or something else?\n-What is spline w/o EB detection? You first train the network, prune via your cosine criterion, and then fine-tune?\n-EB: At what epoch do you find the EB ticket? After that, do you retrain it from the start with the same number of epochs as other methods?\n-Table1: I guess you use (Liu et al 2017) as a pruning criterion? You should make it clear what you used. Further, over here in Table 1, these experiment results seem to be single runs. How much is the standard deviation?\n-I guess you use average hamming distance between the codes over multiple samples. How many samples do you need on average? And how does the performance vary as you increase it?\n-Do you prune all the layers, including the input and final fully connected layer?\n-Can you add the results for just “EB” alone for Table 2 and Table 3?\n\n-> \"Absence of smart-initialization\" is over-sold: The authors claim that over-parameterization is needed as current networks lack a suitable data-aware initialization of parameters, which consequently hinders optimization if the networks are small-sized. Yes, I largely agree with this remark. However, I think this is more due to the nature of current optimization methods than the initialisation. Nevertheless, the main thing is that you seem to be over-selling this whole point about initialization, given your comparisons with “smarter initialization” are very questionable.  \nBecause, it is verified against pruning techniques which prune early on in training. However, the SOTA is obtained by post-training pruning or dynamic pruning methods. Therefore, it does not really explain that the gap between over-parameterised-then-pruned networks versus training a small network of the same size is merely due to the absence of a good initialization. \n\nFor example, DPF https://openreview.net/pdf?id=SJem8lSFwB shows that VGG16 on CIFAR10 can be pruned to 95% pruning ratio without any loss in accuracy, so I am not convinced by Table 1. Can you please show some results of post-training-pruning plus fine-tuning (at least in the case of structured pruning; DPF is unstructured), and compare that with other methods here?\n\n-> \"Designing principled pruning techniques”: IMO, Optimal Brain Damage/Surgeon (Lecun et .al. 1990; Hassibi & Stork 1994) are also based on a principled foundation, as they try to remove those parameters which leads to least decrease in loss, measured by the second-order Taylor series expansion. This has also been verified at the scale of current networks as well, EigenDamage (Wang et al 2019) and WoodFisher (Singh & Alistarh 2020). The community still needs even better methods, and those which can give further insights, like your paper tries to do here. However, depicting that there has been a lack of tools and no principled methods yet, is far from true and should be corrected. Likewise “a novel line of research”: I would not use the word ’novel' as a lot of pruning methods were proposed in the 90’s and the current line of research has been more of a resurgence/revival of that line of work. \n\n-> I must say that there are quite a few typos and grammatical mistakes. In fact, I should rather say, from the 4th page onwards there are typos in almost every other line, which makes it very annoying.\n\n\"to shown the” -> to show the;\n“though a serie of L layers” -> “though a series of L layers”;\nCirculant block circulant (repetition);\n\"open new avenues” -> \"opens new avenues”;\nHyphens missing at a lot of places;\n\"simple strategy that simply” (repetition) ;\n\"a posterior to” -> \"a posteriori to”;\n\"Let first consider” -> \"Let us first consider”;\n\"greater than the true one” -> \"greater than the true number”;\n\"vanishing of exploding” ;\n\", once can remove”;\n\"First, the case of employ”;\nMNIST: 764 dimensional space -> 784 \n\n-> There are some statements which are quite strange and vague. \n-\"which in turn emerges naturally from architecture search and cross-validation as overparametrizing….” \n-\"why pruned networks are less expressive than the overparametrized variants (Sharir & Shashua, 2018)”: Here, I don’t see how Sharir & Shashua paper applies here as they talk about overlapping and strides in case of ConvNets?\n-Besides, there are other vaguely written statements that need to be fixed: e.g., \"when to stop the first training step”.\n  \n*Hence, in the current shape, unfortunately, I can only recommend reject.*",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper2734 AnonReviewer3",
            "review": "Overview：\n\nThis paper revisits the connection between winning tickets and overparametrization in deep networks. Based on analysis of Continuous Piecewise Affine DNs, it  demonstrates that the final decision boundary does not always depends on all the existing subdivision lines. Hence it removes the subdivision lines that are not crucial for the decision boundary and has a small effect on the final classificaiton performance.\n\nPros:\n\n1.This paper proposes a new  avenue to study the pruning techniques.\n\n2.The training is efficient and energy-saved for model pruning.\n\nCons:\n\n1.The expriments are not convinced. As I know, the results of [1] and [2] are much better than SFP and ThiNet, which are not the SOTA.\n\n2.Is there a more efficient method to remove the units that do not affect the layer input space? Can you merge the information between the units with small enough N_p(k,k')?\n\n[1]Chin T W, Ding R, Zhang C, et al. Towards Efficient Model Compression via Learned Global Ranking[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 1518-1528.\n\n[2]Li B, Wu B, Su J, et al. Eagleeye: Fast sub-net evaluation for efficient neural network pruning[J]. arXiv preprint arXiv:2007.02491, 2020.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "contributions are often either trivial, limited, or unclear with insufficient evidence.",
            "review": "This paper is motivated to develop theoretical analysis for deep network pruning that is claimed to be lacking in the literature. Specifically, the paper chooses to use spline theory to investigate several ideas in pruning including the effects of overparameterization and initialization, interpretation of pruning, and detection of early-brid tickets. The paper demonstrates some experimental results for the performance of pruned models using spline early-bird (EB Spilne) and compared it against a few baselines. The paper however has several critical issues, and some of them are briefly outlined in the following.\nMany of the main findings/interpretations are either studied already quite extensively in existing works or often too trivial; e.g., good initialization leading to good pruning results. The contribution or message is unclear. For example, the paper argues that they study the difference of different pruning techniques (unit vs. weight pruning), yet the analysis presented in Figure 2 (a) doesn’t seem enough to show any significant/new insights. Remark 1 (winning tickets are the result of employing overly parameterized DNs) is not supported by any clear evidence either; K-means results do not directly support this idea, or frankly it has nothing to do with overparametrization. The paper is often contradicting itself (e.g. “being tied with the DN input space partition does not always depends on all the existing subdivision lines. Pruning will not degrade ..”). This in a sense means the analysis using spline theory is not suitable for analyzing pruning. This paper, as opposed to what’s claimed as theoretical, is a highly empirical work; e.g., the method of leveraging arbitrarily selected convergence of partition changes to draw “better EB tickets”, using biases and slopes (or “relaxed” version) to identify redundant units. The experiments/results are considered insufficient/weak; the performance gains are achieved for often too low pruning ratio (30%) leaving the question of whether pruning based on splines would be effective for difficult settings; the comparisons are too limited to be considered empirical competitors to existing methods.  The paper has writing issues. It reads as if no proofreading is done before submission.\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}