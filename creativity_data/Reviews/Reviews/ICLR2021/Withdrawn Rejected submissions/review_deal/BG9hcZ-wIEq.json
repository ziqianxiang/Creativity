{
    "Decision": "",
    "Reviews": [
        {
            "title": "The contribution may be questionable",
            "review": "This paper proposed a method to detect adversarial example using noise feature maps generated by steganalysis rich model. The authors conducted extensive experiments on different datasets and attack methods. The proposed method is interesting in general.  However, I think the contribution is limited. My reasons are as follows.\n1. The authors used almost the same filter and the same (or even simpler) network structure as in Zhou et al. CVPR'18.\n2. Zhou et al. CVPR'18 tried to detect image manipulation but this work tries to detect adversarial examaple, which in my opinion is much harder. One thing missing in this work is to consider adaptive attacks. Specifically, I think the in the experiments, the attacks do not consider the noise stream and noise conv layers. In other words, the add-on detection module may also be fooled by adversarial attacks. I think the authors may need to consider the following adaptive attack. Basically the same loss as CW attack, but also consider the gradient on the input through noise conv layers to noise stream input and the original input. When non-differentiable operations, such as quantifying and truncating, are encountered, the authors can use techniques similar to BPDA attack in Athalye et al. ICML'18. I believe when the whole model, including the detector, is taken in to account, it is not hard to fool the detector. Nowadays, many works on adversarial example detection and defense have been proven ineffective (consider works by Carlini). So I will be conservative when evaluating such methods without guarantees. I refer the authors to follow the guideling in Carlini et al. \"On Evaluating Adversarial Robustness\".\n3. (Minor) 0.05 epsilon for attacking CIFAR in Figure 3 may be too large. I think usually people use 2/255 or 8/255. The adversarial examples' distortion are perceptible in Figure 3.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review title",
            "review": "Summary: \nThis paper considers the task of adversarial sample detection. Authors propose to use steganalysis rich models (SRM) to generate noise feature maps and then train a separate network to classify adversarial samples from benign samples. Authors argue that detection-only defences are good because they are model agnostic and do not bear the cost of retraining in presence of new attacks.  Originally, SRM were used to find stegocovers using noise residuals acquired through high-pass filtering with different quantisation steps. \n\nFeedback:\nThe authors present three filters used for noise-feature extraction, each appearing to be a specific Gabor filter, looking at very local features. E.g. one of them looks at three neighbouring pixels and makes sure there is little difference between them. It's unclear why those particular filters are used, but I can imagine those can easily be learned and used as a secret by the defender. It is also clear that the attacker can also optimise the detection away in the attack stage. \n\nUltimately, the approach can be summarised as adding a fixed kernel bank right after the input. Although the paper talks about quantisation step in the related work it doesn't mention if it uses quantisation in any form. In general, the related work could be improved to position the paper among the large number of different detectors that came out over past few years each with their own drawbacks. \n\nThe evaluation itself could be improved. It is currently performed with large perturbations to craft adversarial examples (MNIST with eps 0.3  and Cifar10 0.05).  This conflicts with the imperceptibility claim in the introduction, since the adversarial samples considered in the paper can be noticed with human eyes. This is confirmed in Fig 3, which shows three such examples. \n\nIn addition, I wasn't sure how to parse some of the conflicting claims made in the paper. First, it is argued that the data-set only detectors are good and that without relying on the targeted model, the proposed method can still effectively identify adversarial examples`, but then the proposed approach uses adversarial samples generated from one of the models to perform detection. There is clear variation in model performance across the architectures even for such large perturbations, suggesting that the effect of the architecture choice will be more pronounced for smaller perturbations.\n\nRelated to this, could it be that the excellent performance on MNIST can be explained by the fact that there is little to no high frequency noise in the dataset and any non-focused perturbations will have a large noise impact?  That could also explain why in Figure 5, for Red and Blue curves, the detector doesn't touch 0 even for 0% FP. This is the case even for methods that should cripple detection performance otherwise.\n\nCould you better justify the choice of benchmarks? It may be inadequate — sometimes benchmarks appear to work as well as random guess  and sometimes even worse (?)  e.g. Fig 4 black and green lines.  If they can't see such large perturbations as the ones used in the evaluation I would question their usefulness all together. Furthermore, it was noticed previously in the literature that Metzen et al. cannot detect CW and small PGD, whereas in the evaluation it shows >0.8 performance. Could you explain why the proposed approach performs differently here?\n\nOddites: \n- This is novel–while a number of detection only methods have been proposed to improve the robustness of models, no prior work in defending against adversarial examples has investigated learning from noise distribution in detection.``\n- Could you relate the work to literature going back five years that looked at the noise residuals of adversarial samples  without any model dependency (actually even attack independent ) e.g. MagNet and Hendrycs and Gimpel\n\nNitpicks:\n- And, our approach can be combined with different models repeatedly after once training.``\n- Have been proposed to design the worst-case perturbations.`` Worst case is subjective, what is meant is smallest perturbations \n- When a trained model is being applied, the cost is huge of retraining it to deal with new attacks.`` Not always true, certifiable defences (outside of inverse adversarial samples) do not concern with additional data retraining. Where there are individual data point errors fine-tuning helps. \n- Carlini & Wagner (2017) converted adversarial examples into the argtanh space, making it more flexible to use the optimization solvers.`` Not sure what that means\n- proposed Deepfool to find the shortest distance from the clean images to the decision boundary of the adversarial images.`` not really true, it assumes linear classification boundaries. \n- For MNIST dataset, we design two classifiers, as shown in Label 3 in Appendix, a local model and a black model. A``\n- The classifiers for CIFAR-10 and CIFAR100 are shown in Lable 3 and 4 in Appendix`` Should be Table?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach but requires reliable comparison method",
            "review": "This paper considers the problem of adversarial example detection problem in a post-hoc manner. The authors propose a two-stream pseudo-siamese network to detect adversarial examples. Steganalysis rich model (SRM) us used to generate noise feature maps along with RGB images. Experiments show the approach to have good performance on detecting both seen and unseen adversarial examples.\n\nPros:\n1)\tThe idea is interesting, and results are reasonable.\n2)\tThe paper is well written and easy to read.\n\nCons:\n1)\tThe authors need to cite and compare to Steganalysis based adversarial example detection paper (see [1] etc.).\n2)\tThe comparison of generalization to unseen adversarial attacks warrant more detailed analysis – which attacks transfer to which and why? Choose a diverse set of adversarial attacks for this comparison. \n3)\tA major drawback is that existing method is listed as their dependence on the protected model – why is this a drawback? No concrete use case is given where these approaches are suboptimal or cannot be applied. Why should not one be implementing a detection method specific to the model that he/she is deploying?\n4)\tOne of my major concern is a lack of reporting the performance on adaptive adversarial attacks. I would recommend authors to take a look at [2,3] and adopt the recommended guidelines to evaluate the performance. \n5)\tFigure 2 should be made more informative or removed. \n6)\tThe authors should also evaluate their approach on low-frequency adversarial examples [4].\n7)\tThere are a large body of work on runtime adversarial example detection. See a relevant survey on the same [5]. The author should discuss and use more baselines listed in there. \n\n\n[1] J liu, et. al., Detection based Defense against Adversarial Examples from the Steganalysis Point of View.\n[2] F Tramer, et. al. On Adaptive Attacks to Adversarial Example Defenses.\n[3] N Carlini, et. al., On Evaluating Adversarial Robustness.\n[4] C Guo, Low Frequency Adversarial Perturbation.\n[5] S Bulusu, Anomalous Example Detection in Deep Learning: A Survey.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "**Summary**\nThe authors propose a method to detect adversarial examples by leveraging tools from steganalysis. Specifically, they train a pseudo-siamese network that takes in the original RGB image and an enhanced noise image (calculated using the Spatial Rich Model) as inputs, and predicts if the input is adversarial. The auhors show that their method is effective at detecting both white box and black box attacks. They also show that their approach generalizes well, that is, the trained network is able to succesfully detect attacks from different algorithms despite only being trained with a single type. \n\nThe authors support their claims with experiments on MNIST and CIFAR-10/100. \n\n**Strengths**\n1. The paper has extensive experiments on small datasets showing both the effectiveness of the algorithm and generalization.\n\n2. The method is simple, and shows very promising results. The model used for detecting adversarial examples is small, and can be easily trained.  \n\n3. The writing is clear and references are adequate.\n\n4. I appreciate that the authors shared their code, making it easy for checking validity of results. \n\n**Weaknesses**\n1. The novelty of the approach is limited. Liu et al.[1] have previously used SRM features for noise enhancement along with a classifier for detecting adversarial examples.\n\n2. The method seems to be a direct modification of Zhou et al.[2] which trains a network with a similar RGB and noise augmentation to detect locations where an image may have been manipulated arbitrarily. The take-away form that is that such modifications can be detected by SRM as they are high-frequency and lead to inconsistencies in the image. \nConsequently,I am concerned that RGB-N may simply be reacting to any noise that is added to an image. The generalization results also seem to be a consequence of this. That would also lead to a large number of false positives in case of benign noisy images.  I would be glad to see the authors address this by repeating some experiments with random noise added to the test images and reporting their results.\n\n**Clarifications and Minor Errors:**\n1. What are the dataset sizes that the detector network is trained on? Do you attack all the images from the classifier training dataset followed by picking the successful ones for the detection network?\n\n2. `Table is misspelled as Lable in many cases. \n\n\nRefs:\n1. Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, Nenghai Yu; Detection Based Defense Against Adversarial Examples From the Steganalysis Point of View. CVPR 2019.\n\n2. Peng Zhou, Xintong Han, Vlad Morariu, and Larry Davis. Learning rich features for image manipulation detection. CVPR, 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}