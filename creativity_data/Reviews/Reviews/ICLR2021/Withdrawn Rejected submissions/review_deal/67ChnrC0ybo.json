{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting theoretical perspective, but not that practically useful",
            "review": "This paper proposes a new estimator for second-order DARTS [1] by replacing the inverse Hessian in the architectural gradient computation with the Hessian and show that simple modification provides a more stable search, in the sense that the found architectures are better than the original DARTS algorithms when searching for a larger number of epochs.\n\n- Even thought the fact that DARTS sometimes produces degenerate architectures has been noticed before [2], the theoretical perspective presented in this paper is still interesting. However, I think in general there is no clear evidence about the dynamics of the bi-level optimization loop and the approximations to the response function in the lower level, in particular for highly non-convex objectives. For example, Francheschi et al [3], show that in gradient-based hyperparameter optimization, solving inexactly the inner problem can act as a regularizer and can find better solutions of the bi-level problem rather than solving it exactly. In DARTS this becomes even more complex as the weights are shared between all architectures.\n- The empirical results are okay, but of course a NAS practitioner may very well ask why would he/she use this method instead of other ones [4, 5] that circumvent the instability issues in DARTS and find better solutions in a shorter search runtime. Also compared to PC-DARTS [6], as shown in Table 2 and 3 the proposed method runs for longer and still finds an architecture with comparable performance. Maybe it would be useful to evaluate on other benchmarks (including tabular NAS benches).\n- In the footnote of page 3, you say that Zela et al [2] \"believed a super-network with higher validation accuracy must be better, and owed unsatisfying sub-network performance to the final discretization step of DARTS. \". Also in Appendix A.1 this is again repeated. I think that paper clearly demonstrates that this is not the case, so I would suggest the authors to correct this sentence. They empirically show that the super-net performance is not correlated with the sub-network performance.\n- Is there any empirical or theoretical evidence that your method does converge to a stationary point?\n\nOther:\n\n- In introduction you say: \"extending the default number of 50 epochs to 200 epochs, almost all DARTS-based approaches converge to a dummy architecture in which all edges are occupied by skip-connect\". However I do disagree with this claim if it is not backed up with empirical evidence or references to demonstrate it. A counterexample is the SNAS[7] paper.\n- This sentence in the introduction seems it needs to be rephrased: \"how instability happens in mathematics and how to maximally avoid it.\"\n- Would the proposed gradient estimation transfer to other approaches, e.g. GDAS [8]?\n\n-- References --\n[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR 2019\n[2] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search. In ICLR 2020\n[3] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In ICML 2018\n[4] Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-based regularization. In ICML 2020\n[5]  Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, Cho-Jui Hsieh. DrNAS: Dirichlet Neural Architecture Search. In ArXiv 2020\n[6] Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, Hongkai Xiong. PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. In ICLR 2020\n[7] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR 2019\n[8] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In CVPR 2019\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "One main concern must be addressed",
            "review": "The authors propose a solution to the problem of instability in differentiable neural architecture search.\n\n## Strengths\n1. The paper is well motivated, as it identifies a reason for a commonly encountered problem in differentiable neural architecture search.\n2. It is generally well written and quite easy to follow.\n3. The experiments make use of standard benchmarks, compare to sensible baselines, and the ablations confirm where the benefit comes from.\n\n## Weaknesses\n1. My main concern is with the derivation in Section 2.3. In particular, it is noted that $\\nabla_{\\vec \\omega}\\mathcal{L}_{\\text{val}}(\\vec \\omega^\\ast(\\vec \\alpha_t),\\vec \\alpha_t)  = 0$ for all values of $\\vec \\alpha_t$, because $\\vec \\omega^\\ast(\\vec \\alpha_t)$ is a critical point. Given that this quantity appears in the products in Equations 4 and 5, this implies that $g_2$ and $g_2^\\prime$ both evaluate to $\\vec 0$. I suspect the reason this method performs better than classic DARTS is precisely because a poor approximation of $\\vec \\omega^\\ast(\\vec \\alpha_t)$ is used in practice---this approximation being a single step of SGD. This results in the gradient not being zero, and therefore $g_2$ and $g_2^\\prime$ are also not equal to zero.\n\nIf this weakness is remedied, and no other issues come to light, I would be inclined to change my score to an accept.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A DARTS fix backed by reasonable experiments but dubious math.",
            "review": "Summary:\nThis paper attempts to improve the stability of DARTS-like NAS by making two modifications that align the search process more closely with its objective: (1) amending the architecture gradient update and (2) using the same hyperparameters during search as during re-training. The authors evaluate these changes on standard NAS search spaces. While the empirical contribution seems sound, I do not support accepting the paper due to several dubious/unrigorous claims in the motivation and justification that bring the method into question.\n\nStrengths:\n1. Experimental studies for search over larger spaces (where cells are not constrained to be the same) and using the same hyperparameter settings during search as during training.\n2. State-of-the-art results on PTB.\n\nWeaknesses:\n1. My main concern is the lack of justification for replacing the inverse Hessian in Eq. 4 by a coefficient eta times the Hessian in Eq. 5; this is the main methodological contribution of the paper. The only justification provided is that this ensures that the angle between the approximation and the desired quantity is at most 90 degrees, but (a) this is a fairly weak justification, (b) the proof of this result in Appendix A.3 is non-rigorous (among other things, the second-to-last paragraph uses the fact that a_kb_k>=0 holds in *most* cases to make a claim that only holds directly if a_kb_k>=0 holds in *all* cases), and (c) this property may hold for a variety of other substitutions for the inverse Hessian, making the choice made in the paper rather arbitrary (in particular, 2nd-order DARTS substitutes the identity instead, which the authors claim does not satisfy the 90 degree property but provide no proof).\n2. Assumptions underlying mathematical results are left unstated; for example the loss functions L are undefined and not provided any regularity properties such as differentiability, the Hessian at optimality is taken to be invertible without proof or assumption, and Appendix A.3 seems to assume that there are more architecture parameters than shared weights, which is reasonable but should be stated formally.\n3. The motivation seems muddled. The authors claim to focus on the optimization gap between the supernet and its subnetworks and draw a contrast with Zela et al. (2020), who focus on better validation loss. However, the proposed gradient amendment seems to be more in service of better optimization (the latter goal) rather than anything to do with subnetworks. Indeed the authors report better validation loss due to their method in the experimental section.\n4. The method is substantially more expensive than other DARTS-like methods.\n5. Code is not provided with the submission. Will an implementation be made available?\n6. (minor) Grammatical errors occur throughout. The notes include some of those found in the first two sections.\n\nQuestions:\n1. Perhaps the best known failure case of DARTS is on NAS-Bench-201 (Dong & Yang, 2020, https://arxiv.org/abs/2001.00326). Why not evaluate the proposed modification on this search space?\n\nNotes:\n1. Abstract: “owes such instability to” -> “claims that this instability is caused by”\n2. Abstract: “from two aspects” -> “in two ways”\n3. Intro: “slacked” -> “relaxed”\n4. “Although some practical methods (Chen et al., 2019; Nayman et al., 2019; Xu et al., 2020) have been developed to reduce search variance” - unclear that Nayman et al. (2019) focused on reduced search variance or instability.\n5. Intro: “state-of-the-arts” -> “state-of-the-art”\n6. 2.2: what setting are these experiments in? DARTS search space on CIFAR-10?\n7. 2.2: “Despite dramatically bad sub-networks are produced” -> “Despite producing dramatically bad sub-networks”\n8. 2.3: “we note that ω*(α) has arrived at the optimality on the training set” - “note” -> “assume”\n9. 2.3: “H is symmetric and positive-definite, and thus invertible” -> this is not necessarily true even if the loss is convex, as H can have zero eigenvalues.\n10. 2.3: “Note that no approximation has been made till now” -> arguably the biggest approximation made is the assumption that w is at optimality.\n11. 2.4: “Eqn equation 4” -> Equation 4”\n12. 2.4: “Let us denote the approximated term as g2^2nd , then the property that <g2^2nd,g2> doesn’t hold.” - proof?\n13. 2.4: “Besides amending the architectural gradients to avoid ‘over-fitting’ the supernetwork” - how is optimizing better avoiding overfitting?\n14. 2.5: “introduced a few human expertise to stabilize search” -> “introduced human expertise to stabilize search”\n15. 2.5: “XNAS and DARTS+, by adding human expertise, somewhat violated the design principle of AutoML, in which one is expected to avoid introducing too many hand-designed rules.” -> unclear how either paper introduced any more “human expertise” than the current paper. How is early stopping violating the design principle of AutoML more than amending the architecture gradient?\n16. 2.5: “Our approach sheds light on introducing a similar property, i.e., robustness to approximated ω*, which helps in stabilizing differentiable search approaches.” - where is robustness to this approximation discussed?\n17. 3.3: “As far as we know, this is the best results ever reported in the DARTS space” - RSWS (Li & Talwalkar, 2019) achieve 55.5 test ppl on this benchmark, which is also worse but much closer to this result and should be included in the table.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This is a paper that barely meets the acceptance criteria.",
            "review": "This paper focuses on Differentiable Neural Architecture Search, tries to solve the optimization gap caused by using the identity matrix instead of the Hessian matrix in darts. The author claims that this can solve the structural degradation in Darts after a long search. This is proved by a lot of experiments\nStrong point: \n1.\tA rather simple approximation method of Hessian matrix is proposed, and the theoretical basis is given (although the theoretical basis is not very strong)\n2.\tThere are plenty of contrast experiments, ablation experiments, and simple numerical experiments to verify the theory\nWeak point:\n1. Although the authors believe that supernetworks with high verification accuracy may not produce good subnetworks, this method is still committed to finding a good supernetwork \n2. The author thinks that the importance of using Hessian instead of I is far more than that of accurate w * (α), and it is proved by numerical experiments, but the exact w * (α) is used in numerical experiments, which is difficult to prove the difference of the importance of the two.\n3. The author thinks that the criteria of artificial design (such as early stop) are against the original intention of Auto ML. I do not agree with this subjective opinion, because it is also artificial design to approximate H ^ - 1 to H\nMinor questions:\n1.\tCan the author improve the numerical experiment to make up for weak point 2?\n2.\tThe experimental results shown in Figure 5 show that your method also has a tendency towards nonparametric layers at 500 epochs. Does this mean that η in your method is an important parameter to avoid collapse over time? We hope to see the experimental results of adding η to darts\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}