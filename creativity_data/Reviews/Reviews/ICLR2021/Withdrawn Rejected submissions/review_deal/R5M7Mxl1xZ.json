{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper deals with unsupervised image-to-image translation and proposed a geometric constrains for better structural similarity between the source and the target. Experiments are done using multiple GAN frameworks and demonstrate reduction in distortions in the generated images.\n\nThe reviewers appreciated the contributions, but were overall not very enthusiastic about the paper, with two rejection recommendations. In particular, the criticism regarded\n- limited applicability; shape similarity does not always translate into a good visual result; scenes with multiple similar objects might be severely distorted\n- some results show a strange mixture of styles\n- missing implementation details\n- only small quantitative improvement\n- similarity to prior works on perceptual loss\n- lack of clarity about the use of mutual information for geometry preservation, and implementation details\n- unconvincing baselines\n\nThe authors provided an extensive rebuttal addressing some of the above comments. However, many of the doubts remained because of which we believe the paper cannot be accepted. "
    },
    "Reviews": [
        {
            "title": "Novel idea about geometry structure constraint and thorough experiments",
            "review": "This paper presents a geometry-distortion constraint for the unsupervised image-to-image translation for a better structural similarity between the source and the target, which is deducted from the pixel correlation. The experiments on multiple GAN frameworks and datasets show its effectiveness in reducing the shape distortions in generated images.\n\n## Pros\n\n- This paper is well-written and easy to follow. The main paper is well-organized in describing the problem and proposed methods. And the appendix provides more interesting details. \n- The experiments are quite thorough. This paper includes almost all well-known unsupervised image-to-image translation works as baselines and show significant improvements on them. The sensitivity study clearly shows the influence of mutual information by changing its weights.\n- The idea of using pixel-wise geometric in-variance to constrain the shape distortion is quite novel. \n\n## Cons\n- In some datasets, simply maintaining the shape similarity doesn't always mean visual-appealing results. Sometimes it leads to a weird mixture of styles&structures of both domains for some datasets, e.g., selfie2ainme. I would be appreciated if the author can discuss more the limitations of this method and extends the analysis in A.6.\n- A minor problem: it might not be a good choice to run experiments on edge2shoes as in Figure 7, since there is actually no shape distortion between the source and target (ditto for Cityscape, Maps). Besides, the figure annotation is not clear: which row is MUNIT? Which row is MUNIT+MGC?\n- More implementation details are needed: epochs, learning rate, weight $\\lambda_{mgc}$, whether trained from scratch, etc. for these datasets. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Needs better exposition",
            "review": "This paper presents an image to image translation framework that addresses the problem of preserving geometric details. Specifically, the goal is for the framework to translate the color details, however preserve the structural cues. To this end, the main idea is to derive a mutual-information constraint between the pixel colors of the input and translations, which is added as a regularization in the standard adversarial GAN loss. Experiments are provided on several examples and show some promise. \n\nPros:\n1. The idea of preserving structural information in I2I translations is interesting.\n2. Some of the translated images look appealing. \n\nCons:\n1. It is unclear to me how precisely is the paper tying the geometry-preservation with mutual information? Aren't we supposed to capture the higher-order gradients to preserve the structure (for example, the edges in the images, and such)? However, what the paper is suggesting is to compute the relative squared-loss mutual information over all pixels. How will this preserve the geometry? \n\n2. The quantitative results show only very marginal improvements over other methods. The qualitative results are also not significantly different from say cycle-GAN (in quality). e.g. Figure 4 U(light) + MGC on the zebra image.\n\n3. The proposed approach appears to be very similar to methods that attempt to minimize the perceptual loss; such as for example \"Generating Images with Perceptual Similarity Metrics based on Deep Networks, Dosovitskiy and Brox, NIPS 2016\". It would be good to contrast the approach to such methods. There are several recent works in this area that the paper could compare to as well.\n\nMinor comments:\na. The paper could benefit from better organization, and thorough polishing.\nb. After (1), it is said that \"it is unclear how we can backpropagate through the histogram\". There are standard methods to do that like computing a soft-histogram. \nc. What is P_{V^Y'} just before (2)? I think it must be P_{V^\\hat{Y}}.\nd. How precisely is the E_{\\beta S_i + (1-\\beta)Q_i} characterized in (4)? How do you sample from this distribution? \ne. It is unclear why it is called minimal geometry distortion constraint. It is more of a regularization, and technically there is no specific geometric information in the constraint. It is more of enforcing perceptuality.\n\nOverall, the contribution of the paper is unclear. The technical presentation does not seem to match what the contribution is claimed to be. Experiments show very marginal benefits and needs comparison with more recent and relevant methods.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed constraint based on relative Squared Mutual Information (rSMI) would be useful in correcting geometric distortions in unsupervised image-to-image (I2I) translation tasks.",
            "review": "This work introduces the geometry-distortion constraint (MGC) for mitigating undesired geometric distortions that may often occur in the current unsupervised image-to-image (I2I) translation approaches. The MGC is formulated using the Mutual Information (MI) between intensity values from original image and translated image. Authors claimed that the maximization of MI enables for suppressing the randomness of color transformation, leading to geometric-distortion free results. To integrate the MI into GAN based deep networks, authors leveraged the relative Squared Mutual Information (rSMI) that is differential, proposed in (Sugiyama et al., 2013). Experiments support the effectiveness of the proposed MGC in various image translation tasks, including digit translation, image-to-parsing, and style transfer.\n\n* Pros\n1) The MGC based on rSMI seems to be beneficial to several I2I translation tasks.\n2) Detailed ablation studies on various tasks are provided.\n\n* Cons\n1) The proposed MGC is applicable only for some limited tasks, where geometric layouts are strongly preserved. Namely, deforming objects in a geometric fashion does not belong to such cases.\n2) For general scenes with multiple similar objects, this constraint may lead to severe distortions. For instance, the results in the last row of Figure 4 demonstrate that the proposed MGC yields even worse results than original methods. Failure cases and analysis on the limitation of the proposed constraint should be provided carefully.\n\nMinor comments\n1) Appendix A.1 seems to be from existing works, and thus references would be needed.\n2) A.3 has no contents.\n3) What is Gc+Cycle+MGC in Table 3?\n4) What is GAN+Contextual in Table 3?\n5) Setting lambda_mgc is described below. Please specify how to choose lambda_mgc in more details.\n'A practical strategy of choosing lambda_mgc is to find the largest lambda_mgc with normal style information using binary search.'\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Minimal Geometry-Distortion Constraint for Unsupervised Image-to-Image Translation",
            "review": "#####################################################\n\nSummary: \n\nThe paper focuses on the geometry distortion problem of unsupervised image-to-image translation. To combat this issue, authors approach a new I2I constrain: Minimal Geometry-Distortion Constrain (MGC). In practice, performing estimation and maximization of MGC is challenge, then the paper provides an approximate representation of mutual information: relative Squared-loss Mutual Information (rSMI). To evaluate the proposed method, various kind of datasets are leveraged and compared to SOTA. \n\n #####################################################\n\nPros: \n\n+ From both quantitative and qualitative results, the proposed obtains batter score compared to baselines.\n\n+ A new mutual information is proposed and utilized for Image-to-image translation.\n\n+ The paper conducts extensive experiment on the various kind of datasets.\n\n+This paper is well-written and easy to understand.\n\n  #####################################################\n\nCons:\n\n-For me, the reason why the mutual information preserves the structural information  is unclear. I fail to understand why the proposed method  only focus on the geometry information and ignores the other (e.g. color, style and pattern).   If authors utilizes the pre-trained vgg (on ImageNet or celeba) to extract the feature of both the input and output image, and then use reconstruction loss to constrain the output images, which I think still be helpful to constrain the geometry and ignore the colour information. I would like the new mutual information objuective, but do not agree it is suitable for Image-to-image translation. \n\n-The improved result (e.g. KID, Table 3) is weird. I think the proposed method is added directly to current frameworks. For example, using CycleGAN + MGC makes the model less freedom to generate the target domain, which probably results in less performance than CycleGAN.  \n\n-The most dataset maybe not suitable for the proposed method, since the  dataset except for the tiny experiment on digit are allowed to change the geometry and should be altered to adapt the target domain.  In fact more papers[1,2] give effort to change the geometry, since current methods is limited to change the geometry.\n\n-I am not sure how authors select baselines. In the first paragraph of Experiment section, authors introduce the compared baseline in this paper. The following result (Table1, 2), however, present different ones (e.g. GcGAN, CoGAN, BiGAN/ALI, SimGAN, DistanceGAN), which makes me confused.   Besides, some datasets are performed on GcGAN-rot/vf (Table 1), but other one only contains GcGAN-rot (Table 2), and Table 3 show GcGAN (w/o rot/vf), which is weird for me.\n\n\n\n[1] Cross-Domain Cascaded Deep Feature Translation, ECCV2020\n\n[2] TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation, CVPR2019",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}