{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting ideas but it requires more analysis and ablation study",
            "review": "This paper interages adversarial samples into the knowledge distillation (KD) framework to improve the model performance. These adversarial samples are generated based on comparison with logits between teacher and student nets, and then incorporates them into the original training dataset for the student net training. The proposed algorithm is evaluated on both MNIST and GLUE showing its effectiveness.  Adversarial training [1,2,3,4] have shown promising results on both pre-training and finetuning in NLP. It is great to see the improvement on model compression. Overall, the paper is very interesting and well organized. \n\nHere are a few comments:\n\n--FreeLB, SMART and VAT have shown promising results in NLP, and them should be included baselines showing the importance of the large teacher models. \n\n--Lack of ablation study. As mentioned in the paper, the proposed approach includes two steps: pre-training on the student model and then fine-tuning KD. It is not clear the contributions of each step.\n\n--Estimation of additional samples requires backward and it is expensive. It will be great to report a comparison of training costs.\n\n--Perturbation on embeddings has been used in [1, 2, 3, 4]. Thus the second contribution is questionable.  \n\n--Please cite each dataset in GLUE as officially suggested.\n\nTypoes: \nroBERTa -> RoBERTa\n\n\n[1] Zhu et al. FreeLB: Enhanced adversarial training for natural language understanding, ICRL 2020.\n[2] Jiang et al. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization, ACL 2020.\n[3] Liu et al.  Adversarial training for large neural language models, https://arxiv.org/abs/2004.08994. \n[4] Miyato et al, Virtual adversarial training: a regularization method for supervised and semi-supervised learning, TPAMI 2018.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper shows that augmenting the training set with a synthetic dataset generated by an adversarial objective can improve the performance of knowledge distillation. The authors conducted experiments on MNIST, CIFAR-10 and GLUE. The improvements are consistent. \n\nThe idea is very similar to that of VAT which is a semi-supervised learning method. Since VAT can achieve great results on semi-supervised learning and that the objective of semi-supervised learning and knowledge distillation is quite similar, the results do not look surprising to me. Due to the limited novelty, it would be better to achieve state-of-the-art results to show the effectiveness of the method.  \n\nIn this paper, the auxiliary training data is generated by the backward pass. What if you simply use data augmentation such as RandAugment? This could be a strong baseline. \n\nSome ablation studies would make the method easier to use. For example, how would the value of \\eta change the performance of the model?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Weak baselines",
            "review": "# Summary\n\nThe authors in this paper propose to generate auxiliary training samples by extracting knowledge from the backward pass of the teacher in the areas where the student diverges greatly from the teacher. They evaluate this method on MNIST, CIFAR-10, and GLUE benchmark. \n\nOverall, the paper is easy to follow. But it would be better motivated by finding a realistic setting and show that it is more suitable than other traditional KD methods on large-scale problems. \n\n# Strength\n\n- The idea of utilizing the backward pass knowledge of the teacher is novel.\n- The synthetic data experiment setting in Section 4.1 is intuitive, and we can understand the behavior of the proposed method better.\n\n# Weakness\n\n- Though the authors perform experiments on diverse datasets and show that the proposed method is consistently better than original KD on MNIST and CIFAR-10, it is not clear if the results are statistically significant or not.\n- They only test it on CIFAR-10-level datasets and only compare it with original KD (based on soft labels) which is a weak baseline. They should consider datasets like ImageNet and other recent KD variants. Otherwise, it is difficult to convince me that this is a useful new KD method. After all, there are so many KD variants and why would we bother trying this new one?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A KD method that uses some auxiliary data to improve accuracy.",
            "review": "The paper proposes a new KD method that first generates some auxiliary data to capture parts of the input space where the divergence between teacher and student is maximum. After adding this data to the original dataset, the performance of KD is improved. The results are shown on MNIST, CIFAR-10, and NLP tasks.\n\nPros:\n+ The paper is well-written and explains the idea clearly.\n+ The idea of mapping out the input space where the divergence between teacher and student is maximum is interesting.\n\nCons (see below for details):\n- No comparison against any SotA KD methods.\n- More complex baselines using some naïve modifications to existing methods must be constructed and could have been used for comparison.\n- More experiments must be conducted on large-scale image classification datasets such as Imagenet.\n- No theoretical/empirical grounding for how many iterations must be done while generating new samples (so that the new samples do not go out-of-distribution).\n- Other minor issues.\n\nDetailed Comments:\n1.\tThere are too many papers that have come out since the original KD paper (and that improve results significantly beyond the original KD). Many of these papers do not require any additional dataset-generation stage and yet still improve results over the original KD. The claim that the proposed method will improve just about any method needs to be justified with concrete results, or a head-to-head comparison must be shown against SotA KD methods (one example: CVPR 2019 paper: https://openaccess.thecvf.com/content_CVPR_2019/papers/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.pdf -- there are many others). None of the latest papers have been used for comparison.\n2.\tIf the authors do insist on generating additional samples for improving KD performance, they could have easily constructed a strong baseline for comparison. The proposed approach basically boils down to two things: (a) Generate new samples that have some property that may not be present in the original dataset; (b) Add these samples to the original training set and train the student with additional data. Essentially all of the Zero Shot KD (e.g., https://arxiv.org/abs/1905.09768 and https://arxiv.org/abs/1905.07072, etc.) work does the step (a) already. And some of the newer work like adversarial belief matching also generates samples that are at the boundaries of various classes. All one needs to do is to add those new samples to the training set and see how much the accuracy improves over ZSKD works. If the authors’ concern is with using a generator network for this purpose, that is not really a concern because such samples can also be generated without any explicit generator (and having an additional generator is not a concern anyhow since we do all this image generation stuff offline (and not on-device)). Such baselines must be constructed and compared against. Since otherwise, one can easily point out that comparison against the traditional KD methods is unfair (since those methods do not use additional data for training).\n3.\tExperiments on MNIST and CIFAR-10 are not sufficient for demonstrating the effectiveness of the proposed approach on vision tasks. Experiments on Imagenet must also be presented.\n4.\tAt the top of page 5, the authors point out that if the image generation stage is trained until convergence, the samples may go out-of-distribution. However, this will be a particularly important issue for practical use of this algorithm. How can one know how many iterations to train the image generation stage? Is there any theoretical insight behind this? Just doing trial and error to find a suitable tradeoff sounds not too useful.\n5.\tFig 1.(a) and 1(b) are exactly the same. In the synthetic experiments in section 4.1 (Fig. 4), was the teacher also trained on the same set of data points as shown in Fig. 4 (looks unlikely)? Will this experiment still work if the student network was trained on the same set of data points that were available to the teacher (which is generally available in traditional KD).\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}