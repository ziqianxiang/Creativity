{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The novelty of the claims of the paper has been challenged by one of the reviewers, and in addition, one reviewer also raised concerns about the validity of the proof of the main result (Theorem 1). I looked into the proof myself, and I agree with these\nconcerns. The authors did not use the rebuttal phase to clarify these issues."
    },
    "Reviews": [
        {
            "title": "claims of this paper are widely known",
            "review": "This paper argues that as the cross-entropy loss goes to zero, since the correct logit increases in magnitude the entries of the Hessian diminish to zero. Such overfitting on the training set and a small spectral norm of the Hessian should result in poor generalization error. Motivated by this, the paper experimentally evaluates the effect of weight decay on the Hessian controls the magnitude of weights, increases the spectral norm of the Hessian and improves the generalization.\n\nThis paper presents the beginning of an interesting argument but the thesis of the study is unclear. Hessian-based measures of sharpness come with caveats, as has been explored in the literature cited in the paper. The conclusions of the paper have been widely discussed in these existing papers and it is therefore the incremental value of this paper’s results is difficult to ascertain.\n\nI do not believe I understand what the paper is arguing here. It is clear that exponential losses such as the cross-entropy loss result in weights that can have a large magnitude. It is also clear from Section 3 that entries of the Hessian go to zero if the weights increase. It is also known that the spectral norm of the Hessian can be made arbitrarily small/large without changing the input-output mapping of the deep network and thereby the generalization performance. So although the experiments are sound, the conclusion that “flatness is a false friend” does not follow from these experiments.\n\nThe experiments are sound but the numerical accuracies are very poor; Fig 2 shows 3.3% validation error on MNIST, Fig 3 shows 45.2% error on CIFAR-100, Fig. 4 does have a stronger error of 19.4%. The authors are advised to use state of the art data augmentation and regularization techniques in these experiments.\n\nThe paper advocates the need to understand the magnitude of weights in addition to spectral properties of the Hessian. The reviewer agrees with this point of view but the present paper does not offer any concrete insights into this issue.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A review",
            "review": "### 1. Brief summary:\nThe authors look at the Hessian related measures of flatness such as the spectral norm, trace and the Frobenius norm. They ask the question of whether flatness at the end of training is a meaningful metric of generalization and answer no. To to do that they use a simplified model and a mathematical derivation and several numerical experiments where they evaluate the flatness measure for networks trained with varying amounts of L2 generalization, showing that solutions with lower flatness can generalize better.\n\n### 2. Strengths\n* The question the authors are asking is interesting and very relevant.\n* The introduction is very good in clarifying prior work, the reasons for why some people believe flatness is relevant and the assumptions it rests on.\n* The range of architectures and datasets tested is pretty good.\n* The paper does both theory and empirical experiments, which is a great sign and gives more gravity to the theoretical claims.\n* The paper likely dispels some preconceptions that presumably a part of the community holds about the role of flatness.\n* I like that you notice that the regularized Hessian has eigenvalues offset by the regularization strength -- a small thing, but made my appreciate the level of care you take.\n\n\n### 3. Points of my confusion\n\n#### a) The validity of the shift motivation for flatness as a relevant measure. \nIn the intro in page 1 you mention that the original motivation for considering flatness is the assumption that the train and validation minima are offset by delta w. This is not a weakness of your paper, I would just like to know why is it that people assume this would be the case?\n\n#### b) I do not get the Gedanken experiment. \nI am not sure I understand the point of the Gedanken experiment. Why is the loss exponential dependent on the output of the net? Where is the target y for the (X,y) pair? Are you just trying to make the output of the net as small as possible and exponentially punishing the deviations from that? I get that the you get the X W1 W2 ... WL polynomial structure there the same way you'd get in normal cases, but what is the motivation for this experiment? And what is the argument here? Is it that for L -> 0 the polynomial w1w2w3 must go to -infty, and in the expression ww exp(wwwX) the exponential decay kills off the powerlaw growth, taking the Trace -> 0? I am just generally confused about this setup and what it shows us. If you wouldn't mind clarifying this, I'd be grateful.\n\n#### c) How general is the derivation in Section 3? \nI see that you are using the fact that the weights go to infinity for the loss to go to 0. How general are the results you derive here for the Loss = epsilon. To my mind that is actually way more interesting because as I rightly say practically we're not going to get to L=0 anyway.  I see that you derive that as L->0 so do the second derivatives, but what about L = epsilon << log(# number of classes)?\n\n### 4. Relevant papers that could be of interest\n[1] I found a paper that looks at the Trace(H) and the ration Trace(H) / |H|_F and links this to the weight space norm of the network. It seems very relevant to what you are doing here. The paper is The Goldilocks zone: Towards better understanding of neural network loss landscapes by Stanislav Fort, Adam Scherlis (AAAI 2019, https://arxiv.org/abs/1807.02581). They link this measure to the easy of optimization on random low-dimensional sections of the weight space.\n\n[2] There is a nice overview of the structure of the Hessian in Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra\nby Vardan Papyan (https://arxiv.org/abs/2008.11865). In your spectra you seem to be observing (C-1) outlying eigenvalues as observed / expected in many papers. \n\n[3] Emergent properties of the local geometry of neural loss landscapes by Stanislav Fort, Surya Ganguli (https://arxiv.org/abs/1906.04724) forms a model of the Hessian based on the logit gradient clustering properties and also looks at measures of flatness in that regime and its dependence on the weight norm.\n\n### 5. Summary\nOverall I like the paper. I am a bit unsure about the generality of the derivations and the gedanken model, but the experiments show that at least in some cases flatness does not correlate well with generalization. I think it is important to appreciate the role of the weight norm and given that (possibly) some do not, this is a useful addition to that effort. I'm not clear on whether this insight is original (I am ready to be corrected) but if it were I think this is a valuable paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting experiments on the interplay of flatness and generalization, but with limited novel insight",
            "review": "The authors of this article experimentally investigate whether flatness of the loss surface can be a good measure for generalization capabilities of neural networks. They present theoretical reasoning that suggests that weight regularization can lead to sharper local minima and better generalization although it is expected that flatter minima generalize better. Several experiments are conducted that support this interplay of weight regularization with sharpness. Finally it is demonstrated that also different optimization techniques lead to results that question the validity of purely flatness based measures.\n\nThe experiments are well-described and show the interesting behavior how weight regularization affects generalization and flatness for several models and datasets (logistic regression on MNIST, 2-layer neural network on MNIST, 9-Layer CNN on Cifar100, ResNets on Cifar100). The experiments take advantage of an open-source Pytorch implementation (described in a preprint and based on Laczos algorithm) to find the spectrum of the Hessian. The theoretical reasoning is nicely presented first for a linear 1D model explaining the intuition behind the approach and then considered in the more general setting. The theoretical argument is based on large weights during convergence and supports that regularization by weight reduction increases generalization and sharpness. The conclusion of the article that flatness alone is a misleading measure and cannot explain the generalization performance of deep neural networks.\n\nDespite these interesting experiments, I recommend to reject for the following two reasons:\n\n1.) The conclusion of the article is not novel and therefore does not offer any new insight. There are several articles, also mentioned in the paper, that realize the same fact and suggest different measures (not considered in this submission) to address this short-coming of purely flatness-based measures. Taking the results of other articles into account (e.g. Neyshabur et al., 2017; Tsuzuku et al., 2019; Rangamani et al., 2019) the result that adding L2 regularization increases sharpness and generalization is not counter-intuitive. Therefore, the paper only offers additional experimental evidence that flatness alone is not a good measure.\n\n2.) The proof of the theoretical result (Theorem 1) appears to be incorrect in its current form (see below). If the theory can be saved, the theoretical application does not seem to explain the observations as the argument is based on (the product of) weight norms converging to infinity, while the L2 norm is bounded and decreasing in some experiments (see Figure 6 and 7).\n\nI would appreciate a clarification on the extent of the contribution and clarification of the theoretical part.\n\n\nIssue in the proof to Theorem 1:\nAt the bottom of page 4, the weights cannot be bounded below by a value strictly larger than epsilon without a loss of generality: The symmetries of Dinh hold at every finite point, so at each weight setting we can reduce one weight arbitrarily close to zero while accommodating the change in another weight. \n\n\n\nOther issues:\n- Equation (1) only holds at a critical point of the loss surface\n- Below equation (1). It is not because of small gradient that higher order terms can be dropped in an approximation. Instead, bounded higher derivatives and small delta w are necessary for the approximation to hold\n- End of Motivation: The argument that reparameterizations are not an issue under L2 regularization is incorrect. The flatness measure can still be altered in that setting.\n- Below equation (9): A ReLU network is not real analytic, and it is unclear what the \"specifically\" sentence should say. It is obvious that ReLU networks are differentiable everywhere except at a set of measure zero, is this all that is meant here?\n- Equation (10) is incorrect at several places (A_{i,j} - what is j? What is gamma \\setminus a product? ....)\n- Equation (10) To denote a higher derivative with products in the denominator is nonstandard\n- Third line on page 3: The bracket is bounded by 1 by definition, this requires no explanation about contributing paths. Maybe this can be explained by the typos in (10)?\n- Figure 6&7: Does it show the val accuracy (according to text), val error (according to axis label) or test error (according to caption)?  \n- Figure 6&7, please label the two curves of identical color (although one can deduce which curve shows what)\n\n\n\nTypos\n- Around equation (5) both layer and vector components are indexed with ‘q’\n- After equation (5), typo „the the“\n- one line below that, the dot is at the wrong position\n- before equation (7), z denotes both softmax input and softmax output\n- Equation (10)\n - Line 5 of Experimental setup, verb is missing\n- Figure 6 caption „Aadam“",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}