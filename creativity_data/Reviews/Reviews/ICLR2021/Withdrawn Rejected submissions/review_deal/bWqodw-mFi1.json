{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "All four reviewers raised concerns on the limited technical novelty and insufficient experiments. They unanimously recommended a rejection. I carefully read the authors' rebuttal but did not find strong reasons to go against the reviewers' recommendations. The reviewers made excellent points to further improve the paper. The authors are encouraged to incorporate those for a future submission."
    },
    "Reviews": [
        {
            "title": "The paper experiments with H and A estimation as an additional loss in the image recognition task. Marginal improvement is reported.",
            "review": "The paper test a hypothesis that adding  affine transformation and homography estimation as an auxilliary loss in an image recognition task will improve performance. In experiments on CIFAR10, CIFAR100 and SVHN, improvement of a up to a few % points is observed.\n\nThe technique is close to augmentation and experimenting with A and H is not novel. \n\nThere are issues with the experiments:\n1. a certain representation of A and H was chosen. Other decompositions are possible. Is the ad hoc chosen one suitable? Have other options been considered?\n2. the representation of A and H influences the sampling. \n3. the three problems used for demonstrations are quite similar. It is unclear what type of vision problems would benefit from this technique.\n\nParts of the paper state well-known facts (description of H and A, there properties), some are irrelevant - for instance par 2. in the intro about transfer learning. More details should be provided about SimCLR and BYOL to make the submission more self-contained.\n\nI see very limited benefit to the reader.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper needs stronger and more extensive experimentation results to be in a publicable state.",
            "review": "Summary: the paper introduces a novel technique for self-supervised learning where additionally to the self-supervised loss, the model is forced to predict a parameter of the homography between two images given the difference of its embedding vectors. Authors evaluate the proposed method in BYOL and SimCLR.\n\nStrengths:\n\n- Self-supervised learning has shown a lot of promise these last years. I believe that this work explores an interesting addition to traditional self-supervised methods. \n\n- The paper evaluates this addition in two of the state-of-the-art self-supervised models (BYOL and SimCLR).\n\n- Authors provide very detailed explanation of the experimental set-up, which is very good for reproducibility. \n\n\nWeaknesses:\n\n- In my opinion, the paper needs to evaluate the methods in more large-scale benchmarks. Authors only use CIFAR and street view house numbers dataset, which are very specific benchmarks. However, the community is moving towards using larger and larger benchmarks and I think that the paper should follow this trend to be publicable. I think authors should at least apply their method to ImageNet training. \n\n- The performance improvement shown does not seem enough to merit publication, given the benchmark used. In particular, larger differences are shown only when one method does worse in absolute terms (Table 3). I think the combination of these two factors (the numerical improvement and the benchmarks used) make the paper weak.\n\n- I think it would be useful for the reader to see some examples of the homography to estimate in the main paper. I believe it helps the reader understand the task the model is attempting and how hard or difficult this is?\n\n- Have the authors considered using using a concatenation of the two vectors x1, x1' instead of its difference to predict the homography? I am missing some experiments motivating the choice of architecture. \n\n\nConclusion: I think the research direction is interesting but the paper needs to evaluate in larger dataset to show results that merit publication. Furthermore, I think authors should motivate better their architectural choices. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Adding a secondary loss on contrastive SSL that explicitly predicts an affine augmentation",
            "review": "====================================================\n\n**Update after rebuttal**\n\nI want to thank the authors for a long and highly detailed rebuttal. They clarified a lot of my questions and hopefully in the process they were able to improve the paper. However, my listed weaknesses still stand:\n* there is no transfer learning experiments\n* there not seem to be any consistent gains with the proposed approach over other methods, as seen in Table 4, after 500 epochs (when the models indeed have probably converged). Gains can be seen for 100 epochs, but from the absolute numbers  it is obvious that models havent really convered at that time. Furthermore, as the authors clarified,  they do require  30% higher training time, something that make the claim that they learn faster even weaker\n* There is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results. \n\nI will therefore retain my rejection rating.\n\n\n====================================================\n\nSummary:\n\nThe authors propose a module and objective that can be added to recent contrastive self-supervised learning (SSL) frameworks like SimCLR or BYOL. The module p`erforms an affine or homography transformation $\\phi$ on the input $x_1$. Then the difference of the features of $x_1$ and the geometrically transformed version are the input to an MLP that tries to regress to the parameters of  transformation $\\phi$, using an MSE loss. The authors show gains on CIFAR and SVHN by adding this module over SimCLR and BYOL. \n\nStrong points: The authors show that adding this module speeds up SSL pre-training during early epochs.\n\n\nWeak points: \n\nA) The authors do not measure transfer learning performance and evaluate *only* on a very superficial setup, where they train linear classifiers on top of encoders that are pre-trained with contrastive SSL learning on the exact same datasets.  Although this is indeed an experiment popular among SSL papers,  in all recent SSL works like SimCLR and BYOL the linear evaluation on the same dataset (usually imagenet) is only a small part of the evaluation suite that also involves testing performance on many different tasks and datasets. Transfer learning performance here is not measured, nor is performance on  semi-supervised learning tasks.\n\nB) The authors show some gains and faster learning for the first epochs of SSL pre-training. However, results should be taken with a pinch of salt as their gains are only visible in a realm where performance is still clearly suboptimal and learning is far from converged (eg for CIFAR100,  31.33 at epoch 100 vs 42.53 at epoch 500) To their credit, the authors themselves clearly say this in the text and show in Table 4. However they only present results at 100 and then 500 epochs, so it is hard to understand when (at which epoch) the two lines \"cross\" - at which epoch does the proposed approach stop giving gains? \n\nC) There is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results. In Table 5, the authors show that most of the gain can be achieved by only predicting the 2-dim sheer tranformation; this is something potentially interesting that the authors however do not investigate any further.\n\nQuestions and notes:\n* Although not used for SSL, the authors should cite and discuss the Spatial Transformer module of [Jaderberg et al 2015]\n* The notation could be clearer (eg although clearly explained in the text, it is a bit confusing that augmentation/transformation $a_2$ is sampled from set $T_1$ and $a_\\phi$ from $T_2$)\n* Design ablations: The authors use vector difference as the input to MLP h to regress to the transformation parameters, but other reasoning modules could also be used. How would performance be affected for other input functions beyond difference, eg concatenation or elementwise multiplication?\n* There area number of hyperparameter choices in Table 1 that do not match the corresponding papers (eg BYOL uses LARS not SGD). Why wasn't the original protocol followed, and were all the differences ablated?\n* How do you balance the two losses in Eq(5)? Is there no hyperparameter? what would happen if you favor one or the other loss?\n* How would performance change if this module was added after g() instead of f()?\n* Why only on $x_1$? Could there be extra gains if the MSE loss was also applied over $x_2$?\n* What is the added cost of the module in terms of training time? \n* It is unclear to me what the variance measures: Is it for results after multiple runs, and if so, how many runs? Were the multiple runs from scratch or only for the linear evaluation?\n* In Figure 2, how often was performance tested? markers would help understand where datapoints are.\n* The datasets used are relatively small and more importantly in very low resolution. Would the same hold for higher resolution datasets?\n* How dataset biased are the results presented? E.g. invariance to homographies might not be beneficial to digits (6 and 9 is a good example) but it would be useful in eg landmark datasets. Maybe a more suiting dataset/task would be something related to localization\n* [Tian et al 2020] is another related paper that could be discussed - there the authors explore different augmentations to get invariance to, and show highly increased performance even after longer training.\n\nReferences:\n[Jaderberg et al 2015] Spatial transformer networks.\" Advances in neural information processing systems. 2015.\n[Tian et al 2020] What makes for good views for contrastive learning, Arxiv May 2020, accepted at NeurIPS 2020 (not officially published yet)\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "**1. Summary:**\n\nThe authors propose a module that regresses the parameters of an affine transformation or homography as an additional objective in the contrastive self-supervised learning framework. The authors argue that the geometric information encoded in the proposed module can supplement the signal provided by a contrastive loss, improving both performance and convergence speed. The authors validate their claims with two recent contrastive self-supervised learning approaches (i.e., SimCLR, BYOL) on several benchmark datasets showing effective results.\n\n**2. Strengths:**\n\n+ The authors extend the typical contrastive objective with an additional homography estimation objective to enforce non-invariance to the affine transformation or homography. This extension is well motivated, and the authors provide a clear intuition for the proposed module.\n\n+ The authors evaluate the proposed module on several benchmarks showing nice results.\n\n**3. Weaknesses:**\n\nWhile the authors demonstrate the effectiveness of the proposed module on several considered datasets, some points make me concerned about the actual usefulness of this module:\n\n- The authors conduct the empirical study on three small-scale datasets (i.e., CIFAR10, CIFAR100, SVHN). This is fine in itself, however, without experiments on large-scale datasets like ImageNet, it is unknown whether the conclusion still holds since most of the previous effective contrastive learning methods (e.g., SimCLR, BYOL) are originally conducted on ImageNet. It would be better for the authors to provide such experiments to further validate the claims. In this case, there is no need to perform experiments with large batch size to achieve state-of-the-art performance, just a normal batch size (e.g., 256) can validate the effectiveness of the proposed module.\n\n- The authors show significant improvements across all datasets with the proposed module in Table 2 and Table 3. However, as shown in Table 4, when pretrained for longer epoches, the relative benefit diminishes and even degrades the performance a little. Although the authors give a brief explanation on this phenomenon, I am concerned whether the proposed module is actually useful when SimCLR and BYOL have sufficiently converged.\n\n- The authors use the linear evaluation to measure the quality of learned representations by pre-training and fine-tuning on the same dataset. Although it is one of the common evaluation protocols, pre-training and fine-tuning on the same dataset may induce some inevitable biases. Therefore, it would be better to provide more empirical evidence by transferring to other datasets to evaluate the effectiveness of the proposed module.\n\nTo sum up, although the paper is interesting, I think it requires more work (see points W1-W3 above) in order to become more complete and convincing. Therefore, I am leaning towards rejection.\n\n====================================================================================\n\n**Post-Rebuttal**\n\nAfter reading the rebuttal and the other reviewers' comments, my concerns persist:\n\n- The technical contribution is limited. Adding a pretext task of homography prediction itself brings little insights regarding how it improves upon contrastive representation learning from a different perspective. \n\n- The experimental results are not convincing compared to the recent advances. The authors are encouraged to include ImageNet results as well as transfer learning evaluation.\n\nTherefore, I would like to keep my initial rating as rejection.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}