{
    "Decision": "",
    "Reviews": [
        {
            "title": "A work that proposes a few good engineering tricks for self-supervised video representation learning yet may lack sufficient insights",
            "review": "This work elaborates on self-supervised video representation learning, a task that attempts to initialize a spatio-temporal model from inherent cues in videos and then transfer / finetune the model on another video dataset (for example, Kinetics to UCF101). The proposed method implements three key ideas: new data augmentation (playback rate distortion, temporal drop etc.), temporal memory bank, and contrastive learning. \n\nOverall this work represents a few engineering tricks that successfully demonstrate non-trivial accuracy elevation (for it I am indeed not sure). In particular, data augmentation is known to be crucial for the final performance. This paper obviously explores more temporal augmentation than previous methods, which to me may contribute most to the reported performance. However, I am not sure that the insights brought by this work are sufficiently significant – memory bank and contrastive learning are both well-known in the deep learning community.\n\nThe experimental results are very promising. I have no complaint regarding the accuracies since it beats previous methods by large margins. However, it is not trivial to figure out the true source that the improvements are from. For example, the proposed method uses more frames than standard methods (see “A+B“ in Table 4) in order to perform temporal augmentation. To ensure convincing experiments, it is important to make all comparisons fair and rule out the impact of different engineering tricks in different methods. I regard this as a typical borderline paper.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Self-supervised Temporal Learning\"",
            "review": "This paper focuses on a temporal-based contrastive learning framework that includes temporal augmentations, temporal memory bank, and a new SSL loss. The results are promising that significantly outperform the state-of-the-art. I have the following comments:\n\n1. PRP (Yao et al.) also leveraged playback rate prediction task. Thus the novelty of L_{spd} is limited to me. I suggest the authors adding more discussions to PRP regarding the technical improvements. \n\n2.  From the results in Table 2 and Table 3, it seems temporal augmentation significantly boosts the performance in video retrieval. Can the temporal augmentation technique boost the performance in other state-of-the-art methods? (e.g., MemDPC).  Besides, the improvemenst of UCF-101 is relatively minor compared to HMDB-51. More explanations are needed to better understand the results.\n\n3. Furthermore, the discussions to CVRL is missing. Figure 3 of this paper looks similar to Figure 2 in CVRL. However, the discussions are missing.\n\n4. The temporal memory bank introduced in this paper looks similar to the design of MoCo.What's the technical novelty to MoCo?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper, but still need clarifications on both writing and experiments",
            "review": "This paper proposes a method for self-supervised video representation learning. The main contributions include 1) temporal augmentation; (2) temporal-aware memory bank; and (2) some video SSL losses. Experiments are done on standard benchmarks, e.g. pre-train on Kinetics-400 and evaluate on UCF101 and HMDB51. Overall, the novelty is good which focuses on the temporal aspect of videos, presentations and experiments need further clarifications. I am leaning to vote for accept this paper with subject to satisfied clarifications from the author(s).\n\n(+) Pros:\n- Novel aspect of SSL, e.g. investigating on different temporal augmentation, memory bank specially designed for videos (temporal aware which was guided by motivated observation), a set of losses: contrastive losses for clip-2-clip and clip-2-video, the \\mathcal{L}_{spd} is similar to SpeedNet [Benaim et al. CVPR 2020], the author(s) may need to give them some credit here.\n- Experimental results are good compared with previous approaches. The ablation experiments are also good enough to cover most of the design choices.\n\n(-) Cons:\n- Some experimental setup are not clear and need further clarifications.\n1. In section 4.1, \"\"following prior works, we use split 1 for pre-training and evaluation in this paper\". This is not quite correct, the most popular protocol (up to now), is to pre-train on Kinetics-400, and evaluate (fine-tune or KNN) on split-1 UCF101 and HMDB51 for ablation. But for comparing with previous work, most people reported on 3-fold cross validation, e.g. average 3 splits results.\n2. Table 1 has a couple things need to be explained: is this on split 1 or 3-split, if it is only split-1, then it needs to be re-rendered to be averaged of 3 splits. Also, Input Size column need to have number of frames, e.g. 16x112x112 for SSTL. Also, it is not sure what PRP numbers are underlined, but not MemDPC?\n3. Motivation fig 2 is clear, but Fig 1 is still vague and need better explanation (caption and inline text)\n4. Eq(3) needs more in text explanation the intuition. If the reviewer understands it correctly, notations i, k in Eq(3) are the video indices (as defined in page 4 of dataset V = v^{i}, i=1..N), assume h_{i, x} and h_{i, y} are features from two augmentations x, y of an identical clip from video i, does Eq(3) has anything to account for clips sampled from the same video but not identical (e.g. temporally different?) because I assume h_{k, x} and h_{k, y} are from video k different from video i? Then where the memory bank helpful for? Is it only used for Eq(5)?\n* Some minor corrections:\n- page 6 bottom paragraph says 74.5 and 44.5 for UCF101 and HMDB51 while Table 1 wrote 79.1 49.7 (results mismatched for SSTL).\n\n[a] Benaim et al. SpeedNet: Learning the Speediness in Videos, CVPR'20.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This submission proposes three techniques (1) temporal augmentations (2) temporal memory bank and (3) SSTL loss for self-supervised temporal learning. Despite performance seems good at first glance, this submission lacks of experiments to justify their claim. The writing also needs substantial improvement. ",
            "review": "Pros:\n1. The motivation of the paper is clear. The main problem in current video representation learning is lack of temporal modeling.\n\n2. Video retrieval performance is strong. But can authors provide an AUC curve or per-class results to analyze why your approach is better than previous literature? Using long term temporal information could be one reason, but an 100% relative improvement needs justification. Some visualizations here could be nice, e.g, a t-SNE plot. \n\nCons:\n1. Writing needs substantial improvement. For example, the submission claims to have done experiments on 4 datasets, but actually they only do 2. There are also typos and missing words throughout the paper. \n\n2. Lack of experiments. This submission doesn't have linear probe results for Kinetics400 dataset, which is an important metric to evaluate the effectiveness of learned video representations. Because if the backbone of the network is not fixed during finetuning, the only thing you can claim is you have a better model initialization. Only by freezing the backbone, we can justify if the learned features can be directly extracted and used somewhere else. \n\n3. Why choose Kinetics600 to draw in Figure 2 as mentioned in Appendix? Most of the experiments done in this submission is on UCF101. Is there a reason why not use UCF101 or Kinetics400 to draw the statistics in Figure 2? \n\n4. The proposed temporal augmentation techniques have been introduced in PRP and other recent action recognition literatures. Could authors clarify the difference? \n\n5. I have a question about the temporal memory bank. It has L columns. Is L a fixed number? How to design L for videos with different duration? For example, if video A has 300 frames, video B has 600 frames, and the interval is set to 60 frames, does that mean video A has L=5 and video B has L=10? Also, how does this relate to temporal segments? Have authors tried other fusion approach other than simple averaging? \n\n6. Are comparisons in Table 1 fair? Because as shown in this submission, large batch size helps to improve performance a lot. Most previous methods only do pretraining on small batch sizes. Maybe that is the reason they do not perform well. Can authors provide insights here? \n\nIn conclusion, despite the proposed techniques seem useful, this submission lacks of experiments and analysis to justify their claim. The submission at its current form is below the acceptance threshold unless significant rewriting. The most important experiment should be added is the linear probe results on Kinetics400. \n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}