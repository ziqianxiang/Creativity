{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting paper suffering from a weak a experimental protocol ",
            "review": "# Summary of main claims\n\nThe authors proposing using mean embeddings of distributions as subjects for contrastive learning instead of instances. The variance of the distribution embeddings is used as a score to detect corrupted, anomalous, and out of distribution data as well as a measure of uncertainty.\n\n# Merits of the paper\nThe paper does a good job of motivating the importance of uncertainty prediction in unsupervised learning, introducing self-supervised learning in general, and SimCLR in particular, as well as discussing the trade-offs between generative and discriminative models for the considered tasks.\nWhile the idea of using mean embeddings of distributions and their variance as an uncertainty/anomaly detection score is an established idea, its application to self-supervised models, is to the best of our knowledge, novel. \n\n# Areas to improve\n- The embedding interpretability experiment, while interesting and ranging over a wide array of corruptions, does not have baselines. We recommand comparing against the euclidean and cosine distances over SimCLR embeddings.\n- Augmenting the anomaly detection experiment with the more standard one vs all scenario over CIFAR-10 and ImageNet-30 will allow comparing the merit of the authors' approach against recent deep anomaly detection literature.\n- The OOD image detection experiment should be compared against recent self-supervised OOD approaches. At the very least, comparing against SimCLR features + KNN [1].\n- The mathematical discussion in section 3 uses probability distributions over function spaces. In general, it is difficult to guarantee the existence of distributions over non-parametric spaces. We recommend modifying section 3 to reflect the fact that the authors used conditionally Gaussian distributions in the experiment section. \n- The paper does not mention any work on distribution embeddings done in the kernel literature. Although, we cannot fault the authors for this oversight, tracing back the recent ancestry of the main idea behind the paper does not have any downsides and can reveal additional insights.\n\n[1]Tack, J., Mo, S., Jeong, J., & Shin, J. (2020). Csi: Novelty detection via contrastive learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Can you compare your work to “Modeling uncertainty with hedged instance embedding”?",
            "review": "**Summary**\nThis paper proposes a Deep Uncertainty Model (or DUM) that estimates the uncertainty of representations from the model. It employs a contrastive learning framework that learns to distinguish between the distribution of the embedding of positive distribution views of an image to the random negative distributions. The method was useful in visualizing which examples are more representative to the model, and also useful in detecting the noise or anomalies in the data.\n\n**Originality and significance aspect**\nI do want to ask the authors to compare their work with [1]. Although the exact formalism and motivations are not exactly the same, both of this work and their work assumed a) an embedding is now a random variable with a distribution b) used a contrastive objective to learn such random variables c)  both work used Gaussian distribution for the random variable. I am aware two works have differences such as [1] used Variational Information Bottleneck formulation while this paper employed the bound assumption on the negative distribution (at the end of Page 3) to ignore and simplify the contrastive objective. \n\nWhile some can argue these two works contribute to the community in a slightly different angle (and I also do agree somewhat), I still want to ask the authors to make a solid discussion and also empirical comparison to compare these two works. I do feel this is necessary for a conference paper.\n\n[1] Oh, Seong Joon, et al. \"Modeling uncertainty with hedged instance embedding.\" arXiv preprint arXiv:1810.00319 (2018).\n\n**Quality and clarity aspect**\nThe paper felt pretty solid. Here are a few recommendations to make the paper even stronger.\n\n* The readability of Section 3 could be improved further by adding more higher-level views on each assumption. e.g. a) what does this \"negative\" distribution do? b) how do you connect L_Dist to L_DUM. c) how could you connect this t_i(x_i) and t_j (x_j) in L_DUM to the previous contrastive loss (2)? How do you construct these two positive pairs?\n* Circular covariance: I think \"isometric covariance\" is a more common term? I could be wrong here.\n\n**Recommendation**\nBecause of the lack of closely related work [1], I am leaning toward rejecting this paper; however, I would be happy to revisit my score, looking forward to seeing the author’s comments.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The problem is an interesting problem but I believe this method is not well explained, seems to be restricted to post-processing and gaussian embeddings have already been proposed in the past.",
            "review": "## Summary of the method\nThis paper aims at introducing the notion of uncertainty in the context of contrastive learning. They build upon the hypothesis that embedding a given input is not sufficient in practice and that one might want to have a notion of confidence or uncertainty over the obtained embedding.\n\nTheir model is based on replacing the mapping of an input (image or other) to a point vector (embedding), by a mapping to a density instead and consider the specific case of gaussian distributions with diagonal covariance matrices.\n\nEven though the method is inspired by contrastive learning and it is mainly used on top of simCLR embeddings, it does not train jointly the means and covariances of those distributions. Instead they consider the means to be equal to the pre-trained embeddings and only learn the covariances in a post-processing fashion.\n\nThe objective function is inspired by contrastive learning. Take a batch of inputs, transform them at random and obtain m gaussian densities. Combine them all into a single gaussian density via POE to obtain another combined gaussian density. The objective is to maximize the dot product between the mean of this gaussian with the mean of the gaussian obtained by applying different random transformations over the same batch. The covariance weights are trained to maximize this objective. For any given input they can now compute the covariances along the diagonal and use the norm of the covariance matrix as a measure of its uncertainty.\n\nBy doing so they are able to apply their measure of uncertainty to 3 different tasks: explainability, anomaly detection and OOD detection.\n\n## Clarity\nThe paper overall is not very well written and quite sloppy on the notations and equations. With the exception of the introduction which I found very clear.\n\nSection 3 is especially hard to follow which is a pity since it describes the method. It starts with equations that are acknowledged “not to be useful”, then it claims to be grounded on contrastive learning but gets rid of the contrastive part of it by removing the negatives and eventually introduces some models that are “loosely motivated”. Important equations such as the one giving the mean of the POE are given in the text even if it lies at the core of the model. On those POE equations, it seems to me like the products are in the wrong order it should be $\\left(\\sum_{i=1}^{m}{\\omega_i} \\right)^{-1}\\sum_{i=1}^{m}{\\omega_i \\mu_i}$.\n\nThe notations are also hard to follow and often poorly chosen. For instance it is not clear why there is a $\\phi$ in $f^2_\\phi$ (the POE) while it seems that this transformation is not parametric. And even in $f^1_\\phi$, the parameter $\\phi$ seems to be a renaming of the inverse variances $\\omega$ above. In the DUM equation (that should have a number), it seems to me that the inputs $x_j$ in the second part of the batch should be the same as in the first batch, since only the applied transformations should be different and otherwise, there is no reason why different inputs should have aligned embeddings.\n\nOverall, it requires a lot of effort to understand the method and the model and the paper does not even explain well the rationale behind it, which IMO is not about contrastive learning so much. The idea, as I understand it, is that when applying different transformations over the same input, the obtained embeddings can be quite different, meaning that the variance of the underlying distribution is high. That is what they want to capture with this objective function. It works because the mean of the POE distribution is actually a linear combination of individual embeddings of the batch, which weights are functions of the variances of each density: the higher the variance the lower the weight. By aligning the two transformations of the batch, they push to decrease the weights (increase the variance) of inputs whose embeddings are unstable under transformations. The fact that inverse variances are acting as weights is key and should probably be explained.\n\nThis observation also means that the method would probably work as well by applying a simple linear combination over the embeddings directly, trying to align two transformed batches this way. This would highly simplify the method description and would remove the long and painful path to justify the method through the lens of contrastive learning.\n\nFor clarity sake, I would also recommend adding an algorithm explaining actually how to reproduce the proposed method, which is far from behind obvious by reading the current version of the paper.\n\n## Quality\nThe strength of the method is to provide an easy-to-fit uncertainty measure over embeddings. \n\nThe first weakness IMO, beyond clarity and sloppy justifications, is that it seems like this method has to be applied in a post processing manner only. Can the uncertainty be learnt together with the embeddings? One may also question the reason for restricting this method to the context of contrastive learning, since it seems like it could be applied on any embedding, including the ones obtained in classification for instance. Or if not it would be interesting to understand and specify why.\n \nThe second weakness is the novelty of the work. Indeed gaussian embeddings have already been proposed and learned end-to-end, in previous works that have not been cited in the paper. Namely “Word Representations Via Gaussian Embedding” by Luke Vilnis, Andrew McCallum in 2014 or “Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions” by Boris Muzellec and Marco Cuturi 2018, which are both mathematically more founded. I would like to understand what is the advantage of the DUM method compared to those, if any.\n\nThe evaluation of the method is somewhat debatable, since the 3 applications are quite similar and are all answering the same question : is a given input coming from the training data?\n- In case of the first application, it is about figuring out if the data has been corrupted or not. Note that to be fair, the range of noise should be explicitly given in section 4.1. \n- The second application is the one for which the results are the best, but interestingly those are not based upon embeddings obtained by contrastive learning.\n- As for the last application, the table first does not clearly show that the method is underperforming compared to other methods. Best results for each experiment should be in bold, as it is the case in table 2. Second, the justification for underperforming invoking the other methods to be supervised is a bit far-fetched in my opinion. Indeed the task here is a binary classification of whether a given input comes from a given dataset or not. The hypothetical advantage of supervised learning is not obvious at all. I do not see a clear reason why this should be true.\n\n\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "**Summary**\n\nThe paper proposes a deep uncertainty model (DUM), which assigns the uncertainty upon the features in a post-hoc manner. Motivated by contrastive learning, DUM considers the objective that approximates the distribution version of the contrastive objective. The paper presents the qualitative results for certain/uncertain samples and quantitative results on anomaly / out-of-distribution detection.\n\n\n**Pros**\n\nI. DUM can be applied to any features with low cost\n\nDUM assigns uncertainty to pre-defined/computed features, which is much cheaper than training a feature extractor from scratch. However, DUM can be applied to *any* features, not only pre-computed contrastive features, as mentioned in concern II.\n\nII. Considering the uncertainty of representation is an important problem.\n\nExtending the idea of prediction (aleatoric) uncertainty [1] to representation learning is an interesting and meaningful direction.\n\n[1] Kendall & Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? NeurIPS 2017.\n\n\n\n**Concerns/questions**\n\nI. Missing a simple yet strong baseline\n\nDUM maps each augmented sample $t_i(x)$ to a distribution, then aggregate them over augmentations to define the final distribution of a sample $x$. However, there is a simpler way to define the distribution/uncertainty: instead of assigning a distribution for each $t_i(x)$, just aggregate the deterministic features {$g_\\theta(t_i(x))$}. Then, one can define a Gaussian distribution of a (aggregated) sample $x$, by calculating the mean $\\hat{\\mu}(x)$ and variance $\\hat{\\Sigma}(x)$ of the features {$g_\\theta(t_i(x))$}. Here, the uncertainty of a sample $x$ and augmentation $t_i$ are easily computed, by the norm of covariance $\\lVert\\hat{\\Sigma}(x)\\rVert$ and the distance of a feature from the mean $\\lVert\\hat{\\mu}(x) - g_\\theta(t_i(x))\\rVert$, respectively. The paper should verify that the proposed distribution-ensemble approach is better than this simple baseline method.\n\n\nII. Not a framework for contrastive learning\n\nWhile DUM's initial motivation was from contrastive learning, the final objective is simply the weighted average of augmented samples' features, due to the crude approximation that eliminates the effect of negative distributions. Unlike the paper's claim, DUM's key component is the product-of-experts ensemble, which gives small variance for well-clustered (easy) samples and high variance for hard samples (see Fig. 1d). In fact, DUM can be applied to *any features*, agnostic to contrastive learning. In Section 4.2, the paper shows that DUM can be applied to the UCI features, indeed.\n\n\nIII. Weak experiments, missing lots of prior work\n\n- In Table 1, the paper does not compare the results with other baselines; hence it is hard to grasp the difficulty of the problem. In CIFAR10-C, most AUROC values are around 60%; are they meaningful?\n- In Table 3, AUROC of DUM is worse than prior work, such as Mahalanobis or Gram.\n- Furthermore, several related work consider the uncertainty / anomaly / out-of-distribution detection of contrastive learning [2,3,4]. The paper should discuss the relation and compare results with them.\n\n[2] Liu & Abbeel. Hybrid Discriminative-Generative Training via Contrastive Learning. arXiv 2020.\\\n[3] Tack et al. CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances. NeurIPS 2020.\\\n[4] Winkens et al. Contrastive Training for Improved Out-of-Distribution Detection. arXiv 2020.\n\n\nIV. Organization/presentation could be improved\n\n- Visual corruption detection (Section 4.1, Table 1), anomaly detection (Section 4.2, Table 2), and out-of-distribution detection (Section 4.3, Table 3) are essentially identical tasks but scattered among subsections. The paper could reorganize them while clearly state the definition and difference of the scenarios. In particular, while the title of Section 4.1 is about qualitative results on embedding interpretability, it also contains the results of visual corruption detection, which is somewhat confusing.\n- If Table/Figures are inside the text (instead of top), it would be better to put them *after* explaining the experimental setup. For example, Table 1 appears after the discussion on qualitative results, and the definition comes after six lines, which was hard to follow for the first time.\n- Some important equations, such as the final objective, are missing the corresponding number.\n\nV. Questions\n\n- In the UCI dataset, how the augmentations $t_i$ are defined? Which network architecture is used?\n\nVI. Additional comments\n\n- In Section 1, the paper claims that \"generative approaches are Bayesian\" - there are many non-Bayesian generative approaches (e.g., likelihood)\n- In Section 4, the paper says \"we focus on three applications,\" but lists four applications: (a) embedding interpretability, (b) novel noise detection, (c) anomaly detection, and (d) out-of-distribution classification, where (b) novel noise detection is missing in the subsection.\n- In Section 5, the paper claims that \"[5] require class information\", but [5] also shows the results on zero label setting (e.g., one-class CIFAR-10).\n\n[5] Hendrycks et al. Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. NeurIPS 2019.\n\n\n**Rating**\n\nDue to the concerns above, I recommend rating 4 for the current submission.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}