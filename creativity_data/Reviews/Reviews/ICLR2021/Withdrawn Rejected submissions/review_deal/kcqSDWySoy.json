{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper solves a PDE using an additional penalty function between the derivatives of the function. On toy examples and two PDEs it is shown that these additional terms help.\n\nPros: - The motivation is to include derivatives in the computationa\n          - Implementation and testing on several examples, including high-dimensional ones\n          - Timing is included in the latest version\n\n\nCons: -The loss is Sobolev norm of the residuals of the equation. \n           - The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation. For example, for the Poisson equation, the problem is selected in such a way the solution is analytic. However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities. Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases). The L2-norm then is not the right functional for the solution to exist, not to say about the higher-order derivatives. So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution.\n    - Convergence. There are quite a few papers on the convergence of DNN approximations to solution of PDEs. The presented methods might have converged to a local minimum. An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks. Neural Networks. 2017 Oct 1;94:103-14.\n  "
    },
    "Reviews": [
        {
            "title": "applying Sobolev training to the PDE learning problem ",
            "review": "Sobolev training of neural networks, which augments the standard loss function with terms that penalize discrepancies between the derivatives of the network and target functions, has been shown empirically to improve data-efficiency. Intuitively, one would expect that it also aids generalization in settings where the target function is sufficiently smooth. This manuscript proposes augmenting the loss functions used to represent the solutions of partial differential equations with terms penalizing the Sobolev norm of the solution, its initial condition, and the boundary condition. The motivation for this approach is clear because data -efficiency is of the utmost importance in PDE learning problems where the data could be very difficult to access. \n\nThe experiments in this paper clearly show that a target accuracy can be achieved with fewer overall training points when using Sobolev training, very much consistent with the established understanding of the effect of penalizing the Sobolev norms in typical supervised machine learning problems.  Some of the examples are high-dimensional and non-trivial. \n\nThe theoretical results are not particularly compelling, but they serve a reasonable justification for the proposed scheme. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting problem, but incremental and not enough support for claims",
            "review": "The idea of using neural networks to approximate the solutions of the pdes is very interesting, specially in high-dimensional setting where classical approaches fail to scale. Although there has been many efforts in this direction, there are still open venues to explore. One of the most important aspect is the choice of loss function to guide the training of the neural network. And the paper's aim is to address this issue by proposing Sobolev norm as the loss function instead of the commonly used $L^2$-norm. The Sobolev norm includes additional term about derivatives of the error. The main claim is that with the inclusion of the additional term, the convergence of the neural network training becomes faster. This is the basic promise of the paper.  \n\nAlthough an interesting proposal, I think the paper did not address the full aspects of it. In particular,  \n\n1) training with derivatives in the loss function should be costly compared to $L^2$ loss. This becomes worse as the dimension increases. If the dimension is $d$, the first derivative scales with $d$ and the second derivative scales with $d^2$, .... It seems that at most, one can try only the first derivative.   \n2) No comparison is provided in terms of computational time. \n3) Comparing the convergence speed may not be fair, because for $L^2$ loss, one might be able to use larger learning rate which would yield faster convergence, while large learning rate might make Sobolev training unstable. \n4)  Including the derivative requires strong smoothness assumption about the pde data (the forcinig term, the BC, IC) which are not standard in the pde literature. \n\nGiven this, I think the contributions of the paper are incremental. Moreover, the theoretical results do not really support the claim that Sobolev loss function is better. It would be interesting to have a negative result about the L^2 loss function that motivates the application of the Sobolev norm. \n\nMoreover, it seems that the appendix is not written with care. \n\nSubsection A1 does not really include a proof. If the result is already known, it seems better to cite the reference (with exact pointer to the result) in the main body of the paper and do not include it as the contribution. \n\nSubsection A2 also includes the result Porp A.3 without proof or reference. The proof of Thm. A4 is not written with care. The bounds in A19 and A20 are obtained without explaining the steps. What type of Poincare inequality is used? \n\nIt will be good to include a definition of the norms and Poincare inequality for the reader unfamiliar with pde analysis.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction but could be strengthened by more theory and numerics",
            "review": "Overview: \nThe paper proposes a novel loss function using Sobolev norms to decrease the computational costs when solving PDEs using neural networks. I think the idea of the work is very interesting and relevant and in a useful direction, however I think the paper in its current form is not yet suitable for publication and it should be strengthened by incorporating more theoretical and numerical aspects. This will make the concept a lot more convincing. I present some ideas and comments for improvement below. \n\nComments and ideas for improvement as well as clarification questions:\n- You state “it requires relatively high computational cost compared to traditional mesh-based schemes in general”. Would you have a reference for this? In part I agree with the notion that neural network training could be computationally-heavy, but on the other hand, as you also mention, it should not suffer from the curse of dimensionality compared to mesh-based methods which would seem that it is computationally efficient?\n- The main claim of this work is to introduce Sobolev training to speed up convergence, or as you mention in the introduction “overcoming the issue of high computational cost when solving PDEs using neural networks”. Theoretically the results in Section 4 are not showing this. I know that in the original Sobolev training paper there is a result on how Sobolev training has a lower sample complexity than regular training. Extending such a result to this setting would be necessary to make the claims in the introduction rigorous. \n- The results in Thm 4.1 and 4.2 are only for 1D equations. I understand that higher order could be more complex, and perhaps the 1D equations are sufficient to convey the intuition, however in that case at least a comment is needed on how these results could be extended to higher orders. \n- In Figure 1, what is the reason for H2 loss not speeding up convergence with the ReLU? Is it the differentiability? \nThe results in 5.2 are again only for 1D equations. I think that if theoretically you do not prove the results for high-dimensional PDEs, the value of the proposed methodology for high-dimensional PDEs should at least be shown in extensive numerical experiments. I do think the example in 5.4 is in the right direction, but a more rigorous analysis would be needed. \n- I would like to have more information on how the “true” (PDE) values of the gradients of the boundary and interior differential operators are computed, and whether this is always possible. \n- My last comment is a general one, but given that the research in this area is growing rapidly with various approaches to improve convergence, a stronger literature review would be necessary. I give two examples of papers which could be of interest below. \n\nSome references which may be of interest:\n- Ito, Kazufumi, Christoph Reisinger, and Yufei Zhang. \"A Neural Network-Based Policy Iteration Algorithm with Global H^ 2 H 2-Superlinear Convergence for Stochastic Games on Domains.\" Specifically, Remark 4.2 could be of interest. The authors also discuss how certain norms cannot guarantee convergence of the derivatives of the numerical solutions. \n- van der Meer, Remco, Cornelis Oosterlee, and Anastasia Borovykh. \"Optimally weighted loss functions for solving PDEs with Neural Networks.”. The authors discuss the choice of loss functions to also speed up / improve convergence and the solution accuracy. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}