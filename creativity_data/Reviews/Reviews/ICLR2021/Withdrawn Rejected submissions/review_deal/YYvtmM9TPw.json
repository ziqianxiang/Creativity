{
    "Decision": "",
    "Reviews": [
        {
            "title": "Well organized paper, with limited novelty and insights. ",
            "review": "This paper investigates the popular contrastive learning algorithm in the self-supervised learning community. More specifically, this paper studies the impact of nonlinear project head, strong data augmentation and negative samples. Based on the analysis, this paper suggests good practices for self-supervised learning.\n\nThis paper is very well organized and well written. I enjoyed reading this paper as it highlights a few very important problems for self-supervised learning, which lead to significant quality differences if they are not considered properly. \n\nHowever, the main concern I have is lacking novelty. The good practices this paper suggested are well understood in the literature, and are widely used everywhere (SimCLR, SimCLRv2, MoCo, MoCov2, et.al.) before this paper. More details: \n(1) SimCLR has done significant ablation tests (Fig4, Fig5) on data augmentations; \n(2) The main focus of MoCov2 is \"MLP head\" and \"stronger augmentation\", which overlaps significantly with this paper; \n(3) The last point \"negative samples\" is also the main focus of MoCo paper, which decouples negative samples from mini-batch size. Overall, the added value of this paper to the self-supervised learning community is unclear, given the existence of MoCo paper and SimCLR paper. \n\nMinor suggestions:\n(1) This paper uses reconstructed quality to demonstrate the impact of MLP layers. It would be nice to show more images of different MLP layers, e.g. do more layers lead to slightly worse image quality? Besides that, it reminds me of the representation quality of GANs [1, 2], where high quality image generation may not lead to better image representations. It would be interesting to connect the image generation analysis with representation from GANs. \n\n[1] Large Scale Adversarial Representation Learning\n[2] Self-Supervised GANs via Auxiliary Rotation Loss\n\n(2) It is a bit disappointing that this paper reinforces that the practices proposed by SimCLR/MoCo are sufficient, as all the analysis and ablation leads to worse experimental results. It would be nice to get novel insights from such an analysis paper, which lead to improved results. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work, but need more results",
            "review": "**Summary**: this paper aims to characterize the \"good practices\" to help guide the design choices of self-supervised representation learning. The experiments are conducted surrounding three aspects: the effect of a nonlinear projection head, \"semantic label shift\" caused by aggressive augmentation, and factors that help reduce the number of negative samples required.\n\nThe questions this paper studies are of practical importance and the experimental results will be valuable to the community. However, the paper in its current form is not sufficient for publication.\n\n**Pros**:\n\n- Explains the effect of nonlinear project head as an information filter, and shows empirical evidence of this claim via image reconstruction.\n- Introduced the semantic label shift problem to explain why contrastive learning is not hurt by aggressive data augmentation.\n- Overall the paper is clearly written and easy to follow.\n\n**Cons:**\n\n- the title should be changed from \"self-supervised learning\" to \"contrastive learning\", since self-supervised learning contains many forms other than contrastive approaches.\n- \"good practices\" is a broad claim that needs to be supported by extensive experiments on various datasets and frameworks. The paper asks the correction questions , but the results seem incomplete.\n\nQuestions:\n\n1. the projection head as a filter: more explanation is needed on why this is the case. It'd be interesting to see more experiments to clarify the following questions:\n    1. How would the dimension of the projection head affect reconstruction? What information gets thrown away when we reduce the dimension?\n    2. The projection head may be filtering information because of its architecture choices, e.g. reduced dimension, or the use of ReLU. If we let the projection head to preserve the dimension (i.e . the projection head has square weight matrices), and use tanh activations (which is invertible, unlike ReLU), then in theory, the projection head cannot \"filter out\" information unless the weight matrices are almost singular. Can we verify this in the experiments?\n    3. When we change the type  or strength of augmentations, how would the reconstruction quality of the features before and after the projection head be affected?\n    4. How would the number of layers in the MLP affects the reconstruction? It'd be good to include a comparison among 0 / 1 / 2-layer projection head; e.g. $g(h) = \\sigma(h)$ (0-layer), $g(h)= \\sigma(W h)$ (1-layer), and $g(h) = \\sigma(W_2 \\sigma(W_1 h))$ or $g(h) = W_2 \\sigma(W_1 h))$ (2-layer).\n2. the first paragraph of section 3 says it is common for the nonlinear MLP projection head to have 1 layer; could you provide citations for this? As far as I know, both SimCLR and MoCov2 use a 2-layer MLP ($g(\\mathbf{h}) = W_2 \\sigma(W_1 \\mathbf{h}))$ where $\\sigma$ is the ReLU activation.\n3. The t-SNE plots show that linear classification boundaries break down when the augmentation is too strong for supervised training, however this does not mean the same effect would not happen for contrastive learning.\n\n    It would be better to provide the same set of t-SNE plots for features learned with contrastive loss, i.e. plots with no augmentation (i.e. a positive pair consists of two copies of the original image), weak and strong augmentation. Would these results affect the current hypothesis?\n\n4. Section 5 lacks a clear answer. The experiments conducted are important, but appear to be intermediate results that could be included in an appendix.\n5. Typo: last sentence on page 5: it should be \"fewer\" (rather than \"less\") negative samples.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A shallow analysis of some interesting aspects of recent SSL methods",
            "review": "Summary:\nThe authors present a set of \"good practices\" for self-supervised learning (SSL), focusing on the MLP head and the very strong augmentations that SimCLR introduced that were also shown useful for MoCo in MoCo-v2, as well as the negative samples. They \n\nStrong points:\n* The authors explore some of the design choices for the top SSL methods.\n\n\nWeak points:\nThe paper's analysis lacks in a number of ways.\n\nA) It is unclear what the MLP head analysis offers beyond (Chen et al 2020a). What is more, early works on transfer learning [Azizpour et al CVPRw 2015] have shown the effect of overfitting in the last layer for (supervised) Imagenet pretrained models; Given that the proxy tasks are much simpler for SimCLR/MoCo-v2 (practically learning augmentation invariance) it makes sense that this would also follow for SSL pre-training and (Chen et al 2020a) have already experimentally validated that. \n\nB) The authors use feature inversion to visualize the invariances encoded by the MLP head, but their results are very few and not really clear. They show some qualitative results in Figure 2, that support the claims in general, they however fail to go deeper and try to understand which invariances are lost and at which level (eg , by showing more targeted examples that measure specific invariances)\n\nC) Analysis on strong data augmentation: The behaviour in Fig3 is expected for any model and there are zero insights with such a coarse analysis. Specifically: It is unclear what exact model was used for Fig.3, what is \"weak\" and \"strong\" augmentations and whether these were covered by the SSL pretraining task.  The results in Table 2 suffer also from a coarse protocol: the fact that performance breaks down when keeping 2-10% of the image as crops is far from surprising. For color jittering, it seems that the peak has not been found for MoCo-v2. Similar to the analysis for the MLP head, the authors do not seem to offer some useful insights beyond what the current state of the art SSL papers have already shown.\n\nD) Negative sample analysis: this analysis lack in a number of ways:  Fig 4 is essentially the same as Figure 3 from MoCo (He et al 2020), but with lines added for SimCLR (which is possible misleading/wrong -see below), MoCo-v2 and a MoCo-v2 variant with hard negative mining, where there doesnt seem to be much real difference between the two for different queue sizes.  One would expect more gains for smaller K for the hard neg mining case, but it is unclear whether hard negatives are actually mined.\n\nE) The way hard negatives are \"mined\" is unclear to me: if I am not mistaken, it seems that there is no mining, but a thresholding of the already existing negatives to keep only hard ones, but the text there is unclear and surprisingly laconic. If the previous sentence is correct, these negatives already contributed to the loss, as they were part of the queue. I can only assume that dropping many other negatives would increase their contribution to the loss because of the softmax, and this is why this method shos slightly different performance, but all this is not really discussed. Moreover a) The sentence \"we over-sample in order to keep the overall Q size same as MoCo-v2\" is highly unclear and underexplained.  b) Fig5 is probably hand crafted (ie to illustrate in very high level what hard negatives and \"semi-hard\" negatives mean) c) \"We mine hard and semi-hard negative exemplars from dynamic Q using a skew-norm distribution\" is also unclear - what are the parameters of the skew-norm distribution? \nd) It is unclear how the authors train SimCLR with batchsizes of 16k or 65k fior the results in Fig4. \n\n\nQuestions and Notes:\n* The interesting \"fixed projection head\" variant from Table 1 bares resemblance to the experiments in the \"Fix your Classifier\" paper of [Hoffer et al ICLR 2018]; It would be nice to discuss and connect the two.\n* Although cited, the work of (Chen et al., 2020b) is not discussed in terms of their findings wrt the MLP head. This is not a weak point of the paper, as this is concurrent work, but it would be nice to discuss.\n* Although cited, the work of (Tian et al., 2020) is not discussed in terms of their exploration of harder augmentations. This is not a weak point of the paper, as this is technically concurrent work, but it would be nice to discuss.\n* In SimCLR, the negatives come from the same batch. How are simCLR results in Fig 4 computed for K > 4k?? Did the authors train with such big batchsizes?? \n* Variance is neither reported nor discussed.\n* how are \"semi-hard\" negatives defined?\n\nReferences:\n[Azizpour et al CVPRw 2015] Azizpour, H., Sharif Razavian, A., Sullivan, J., Maki, A., & Carlsson, S. (2015). From generic to specific deep representations for visual recognition. CVPR DeepVision Workshop (best paper)\n\n[Hoffer et al ICLR 2018] Hoffer, Elad, Itay Hubara, and Daniel Soudry. \"Fix your classifier: the marginal value of training the last weight layer.\" ICLR 2018\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Raises interesting questions, but claims need more rigorous experiments",
            "review": "This paper analyses three features shared by recent work in contrastive self-supervised representation learning, that have been found to be important to performance but are still not well-understood: i) the role of the non-linear projection head ii) why aggressive data augmentation does not end up adversely affecting performance, and iii) whether large amounts of negative samples are actually needed, and proposes an alternative that mines hard-negatives. It designs experiments to answer each of these questions, in an attempt to lay out “best practices” for research in this area.\n\nStrengths \n\n– The paper does well to identify and study less-understood aspects of recent contrastive learning methods, that have been found to be critical to their success \n\n– The paper is well-written and easy to follow in most parts\n\nWeaknesses\n\n– Using Deep Image Prior to diagnose the role played by the nonlinear projection head is a promising direction and an interesting experiment. That said, only looking at two images without any sort of quantitative analysis seems insufficient to arrive at the stated conclusion, and more rigorous experimentation is needed. Two suggestions: does a model trained on images reconstructed using z after MLP do substantially worse? Does a TSNE visualization before and after the MLP respectively demonstrate very different trends?\n\n– Sec 4: \"From Table 2a, we can see that as the cropping augmentation becomes more extreme … cropping augmentation is essential to this process”: Some elaboration is needed on the point being made. If MoCoV2 is learning occlusion invariant features (as shown in prior work) via the cropping augmentation, why is performance worsening at increasing severity? Or is the point that the drop is less severe compared to supervised learning? But that too is not entirely clear, as the performance drop is identical at medium cropping and very high at extreme cropping for both self-supervised and supervised methods in Table 2a. So I am not convinced that this experiment backs the claim that the semantic label shift problem does not apply to instance discrimination.\n\n– The findings with hard-mining negative presented in Figure 4 are unclear to me. First, since MoCoV2 performance is already stable at low values of negative samples (=512), it is unclear to me why that is the method this technique is applied to, rather than SimCLR or moco for which performance clearly degrades at low K. Second, since the hard-negative approach oversamples using the skew-norm distribution so as to keep Q size the same, what is the number of *unique* examples used as hard negatives? Without quantifying that, it is difficult to validate the claim being made.\n\nOverall comments\n\nThis paper studies interesting questions regarding contrastive learning, but I have concerns about the experimental protocol and validity of some of its claims.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}