{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents a method to make CNN focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation. \nThe paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. Tables are not readable and results tend to be cluttered and confusing.  Some comparisons seem to be cherry picked as pointed out by some reviewers.\nAlthough the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach (that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others).\nI strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. In my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper."
    },
    "Reviews": [
        {
            "title": "An ok paper",
            "review": "Summary: The paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. However, unlike in dropout the masks are kept fixed throughout training/inference and applied to the bottom layers of the network. This shifts the focus of the network away from texture and towards shape features resulting in the adversarial examples being fooling humans as well.\n\nI am inclined to vote for accepting the paper. The approach is novel as it tackles the issue of adversarial examples from a different angle than the usual denoising appraches, however, there is some room for improvement.\n\nPossible improvements and questions:\n- EGC-FL (Yuan&He'20) performs equally well in the experiments, the authors argue that the main advantage of their method is the lower runtime but do not give any quantitative information on runtime. This information needs to be presented/commented upon in more detail.\n- writing/clarity can be improved in the experiments section, sometimes the claims seem a bit exaggerated (e.g., p8 saying \"This corroborates the phenomena...\" when there was no result showing the said phenomena)\n- the authors claim Defective CNNs can help saving a lot of computate and storage, is this hypothetical or is this a real option with current deep learning libraries?\n- why in some tables (2&3) keep probability is 0.1 while in others (4) it is 0.3 & 0.5? Without explanation this seems like cherry picking.\n- from the experiment with shared defective masks it seems that the Defective CNNs actually encode the texture as well, the information is just spread across channels, or can the result be interpreted otherwise?\n\nSmall corrections:\n- Eqs 1,3,7 could use more standard notation for convolution to improve readability ($*$ instead of the current $\\bigotimes_{conv}$)\n- Eq 6 could spell out $M_{dp}$ as $M_{dropout}$ to be consistent with $M_{defect}$ in Eq 5\n- Sec 3.2: \"by both two CNNs\" -> \"by both of the CNNs\"\n- p5: \"... and Gaussian noises usually hard to affect\" -> \"...and Gaussian noise usually hardly affect\"\n- Sec 4.1.1: \"approaches try to erase\" -> \"approaches that try to erase\"\n- Sec 4.2: \"the chose of\" -> \"the choice of\" and \"Without loss of generality\" -> \"Here\" and \"v.s\" -> \"vs.\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A great paper that defends texture-related attack the with simple solution.",
            "review": "##########################################################################\n\nSummary:\n\nStudies suggest that CNNs that overly rely on texture features are more vulnerable to adversarial attacks. The authors of this paper propose a simple yet effective method \"defective convolution\" that randomly \"disables\" neurons on the convolution layer. The authors argue that by doing so, the CNN is encouraged to learn less from object texture and more on features such as shape. The authors support this statement by empirically evaluating the proposed model under multiple perturbation methods.\n \n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for accepting. This paper provides a simple yet novel method to force CNNs from learning texture-based features. But I found it more important to understand why such a simple method would achieve this effect rather than using it to defend against adversarial attacks. I hope the authors could provide more motivation and experiments to understand the effect that defective convolution layers have on the CNNs.\n\n \n##########################################################################\n\nPros:\n\n1.  This paper is addressing a very fundamental question of CNN: how can we change convolution filters so that the model will learn certain visual features (shape) while less likely to learn others (texture)?  To answer this question requires a better understanding of the underlying mechanism of CNNs. This work could serve as the initial step for answering this question. \n\n2. The authors have conducted experiments on synthetically altered images to show that the defective convolution indeed tends to learn less from texture while more putting more emphasis on edges. The comparative experiments in section 4 also empirically support the author's statement that CNNs with such property is more robust against transfer-based attacks.\n\n3. This paper is well-written. The introduction section provides sufficient background for the problem with clear intuition for the method the authors proposed. The literature review is sufficient and well-organized. \n \n \n##########################################################################\n\nCons: \n\n1. This paper needs better motivation. Would this work be applicable to other real-world CV application where the texture of the object of interest is a confounding factor?  \n\n2. The author argues in 3.1 that by using M_defect, neurons of the conv layers are masked out and therefore local features are hard to preserve. While this is intuitive, a more rigorous analysis would increase the credibility of this statement. What would be a proper mathematical definition of texture? How it is related to the locality? Why masking out spatial location in the conv layers would impact locality? \n \n \n##########################################################################\n\nSuggestions:\n\n1. Please address the concerns in the cons section.\n\n2. The ablation study mostly explores the effect of p and the layers where defective convolution is inserted. Intuitively, I think the spatial location where the defective neurons are placed would also impact the model's behavior. For instance, if all defective neurons are on the edge of the filter, we essentially reduce a large filter to a smaller one. On the other hand, if the defective neurons are at the center of the filters then we could discourage the model from learning some continuous patterns. It would be interesting to see how this would affect the model's performance. \n\n\n \n#########################################################################",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "circumstantial evidence. adversarial attacks not strong enough.",
            "review": "The authors present a modification of a convolutional layer that they claim forces their model to be more robust to texture attacks. The authors perform a variety of experiments attempting to show that non-texture features are being \n\nThe authors make a very strong claim in this work: that by deforming the output of convolution, they force the model to rely on other cues, such as shape. However, their analysis showing this to be the case is circumstantial. In none of their experiments do they explictly show that the model is focusing on shape outlines. For example, in the case of the random shuffling experiment, there might be a dependency on shape that is being obstructed, but there could also be a dependence on spatial relations. This is just one possibility.  It also doesn't make sense to use a convnet in the case of a 8x8 random shuffle; how do 1x1 convolutional layers on this experiment do? The Stylized-Imagenet experiment is potentially compromised by virtue of being very out of distribution to the ordinary ImageNet. \n\nThe authors should perhaps examine the visual attention literature to understand which pixels are being used to make classification decisions, such as Grad-CAM. Another set of experiments can involve using a dataset consisting solely of objects to be classified (ie, no background context), and which are ideally composed only of simple shapes. Would the model require training to be robust to such deformities.\n\nWrt the adversarial robustness experiments, it is not clear that their method yields more robust results or simply obfuscates the gradient attack so that a stronger attack is required. I would expect to see a plot showing their model's inference accuracy against attack strength, as measured by the number of PGD iterations. Their accuracy will decrease and should stabilize somewhere. Another possibility is to run gradient-free attacks, as in [1, 2]. We can then see whether their method yields more robust accuracies against an attack.\n\n[1] Jonathan Uesato, Brendan Oâ€™Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk and the dangers of evaluating against weak attacks. In ICML, 2018.\n\n[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018.\n\n\n==================================================================\n\nI thank the authors for their thorough comments and experimental details. I am more satisfied after reading them. I am raising my score from 4 to a 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}