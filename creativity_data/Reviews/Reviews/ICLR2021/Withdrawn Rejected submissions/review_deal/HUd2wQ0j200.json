{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good exploration, but the conclusions seem not to be clear and confident",
            "review": "The authors present TransNAS-Bench-101, a benchmark with 7352 networks fully trained and evaluated on 7 tasks across two search spaces, which is designed for facilitating transferrable NAS research with universal solutions across tasks and search spaces. \n\n\nStrength\n-  The benchmark expands the traditional NAS research that focused on a few datasets on image classification to 7 different tasks. \n- It extends the cell based search space to marco-level based search space.\n- It shows some observations on the difficulty of transferability of learned architectures across tasks.\n\n\nWeakness\n- There is no clear definition of the “transferability” of NAS architectures or algorithms among tasks. The benchmark is focusing on investigating the transferability of across-tasks NAS with the same data domain, but it would be great to have the discussion of the transferability across datasets in the same/different domains as well. \n- While the total GPU hours spent on constructing this benchmark is huge and appreciated, the huge number of GPU hours spent is not necessarily counted as contributions. It is not very clear whether the size is large enough for NAS benchmarking, e.g. there are only 7K architectures evaluated, in which the cell-based search space is much limited (4 operations instead of 7)  in comparison with DARTS. The authors noted in Figure 4 that reducing 50% architectures results in a quick shrinking of correlation scores. Therefore it is not clear whether the NAS results on this benchmark can be directly compared with DARTS-based results and how they can be evaluated across tasks on this benchmark.\n- The results obtained from Section 4 (Figure 4 and Figure 5) are interesting but the conclusions are not quite clear. How to define the correlation among tasks? Which tasks are most transferable and why? “Some search spaces can dramatically lower the difficulty of NAS transfer, and some might have inherent disadvantages”. Can the result justify that cell-based search space is not a good one for NAS transfer in comparison with marco-based one? A deep dive in this would be valuable to practitioners.\n- It is not quite clear how this benchmark can be adopted to evaluate existing NAS algorithms such as DARTS.\n- The calculating process of the Spearman Rank Correlation Scores among the tasks are not quite clear.\n\nIn general, I think this paper adds value to the study of NAS transferability among tasks, however, the results seem to be preliminary and no clear findings or guidelines are provided for better understanding or further improving the transferability of cross-task NAS.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "[Recommendation to Weak Reject] Concerns about the search space and hyperparameters ",
            "review": "This paper proposed a new benchmark for the NAS community -- TransNAS-Bench-101. The authors studied the relationship of 7K architectures between 7 different tasks, and also benchmarked some algorithms on this new benchmark. As a dataset/benchmark paper, I believe we should make a higher standard and make sure all the settings are reasonable. I totally agree that Transferrability and Generalizability are important directions and such a new dataset/benchmark is valuable to the community. However, many settings in the paper are impractical and unconvincing to me, thus my initial rating is weak reject.\n\nPros:\n- a new and useful benchmark is valuable to the community\n- this paper explore an interesting and important direction of NAS -- Transferrability and Generalizability\n- the usage of Taskonomy to study Transferrability might inspire some following NAS papers.\n\nCons:\n- in this paper, the search space is much smaller than existing NAS datasets, to be specific, the search space is at 10X less than the search space of previous NAS benchmarks:\n|                 | # search space | Type |\n|:------------|:--------------:|:----------------------:|\n| NAS-Bench-101           | 510M                   | cell |\n| NAS-Bench-201           | 15.6K                   | cell |\n| NATS-Bench (S_t)        | 15.6K                  | cell |\n| NATS-Bench (S_s)        | 32.8k                  | size (macro) |\n| TransNAS-Bench-101 | 4k                        | cell | \n| TransNAS-Bench-101 | 3.2k                     | macro |\n\nIs a small search space enough to evaluate the \"Transferrability and Generalizability\"? How could the conclusion be generalized to the realistic setting -- a much larger search space? How easy it is to hack the small search space? From the current manuscript, I did not find the answers for these questions. I'm thus concerned whether this proposed dataset can be used to evaluate NAS algorithms or not.\n- the training hyperparameters look impractice. In the caption of Table 1, the authors claimed \"all architectures have been fully trained\". How do the authors define \"fully trained\"? For the selected tasks, the authors train them using 10, 25, or 30 epochs with a constant learning rate. Following existing NAS benchmarks, we usually use warmup+cosine-decayed learning rate to train the model by at least 100 epochs to obtain its evaluation performance. From my personal experience, accuracy obtained by the training hyperparameters in this paper is far from the hyperparameters used in NAS-Bench-101 or NAS-Bench-201. If so, the reported correlations might be an incorrect estimation for the ground truth correlations, and so does the benchmarking.\n- The popular weight-sharing algorithms are not evaluated in this benchmark, which will lose a lot of attention. The authors made some excuses in the paper: \"most of them operate on cell-based search spaces only, and it would take non-trivial modifications to the algorithms for them to search on the macro-level search space.\". First of all, the authors are supposed to at least evaluate them on the cell-search space. Second, the previous benchmark -- NATS-Bench -- has reported the results of weight-shariing methods on the macro search space. There are some weight-sharinig methods that are designed for the macro search space, I think the authors can try to benchmark them on the proposed macro search space.\n\nMinor comments:\n- Do the 7 datasets use the same vision data? If so, I would recommend the authors call it 1 dataset, 7 tasks.\n- I'm a little bit confused about Table 3. In my opinion, we should have 7 sources tasks and 7 target tasks, thus for each algorithm, we could have 7*7=49 results? Among these 49 results, which number is used in Table 3?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "# Summary\n\nThe paper presents two new tabular benchmarks for neural architecture search (NAS) based on an offline evaluated exhaustive search.\nThe first benchmark is based on a cell search, and the second benchmark represents the macro structure of convolutional neural networks with multiple stages.\nCompared to previous work, architectures are not evaluated across datasets but across different tasks, such as image classification or semantic segmentation on the same dataset.\nThis allows to benchmark NAS strategies that transfer knowledge across different task.\n\n\nIn general, I do think the paper addresses an important research direction in NAS and, looking at the large impact of previous NASBench benchmarks, might lay the foundation for interesting future work. However, there are a few points (see below) that are unclear. My major concerns is that the correlation between well-performing architectures is often low or even negative, and that benchmarks might not allow to transfer knowledge across tasks.\n\n\n# Pros\n\n1) Arguably the main strength of the paper is to enable future research in NAS that aims to transfer knowledge across different problem types than just across datasets.\n\n2) The paper systematically analysis the ranking of architectures across tasks, showing that there is no single architecture superior on all tasks, and there is often only a rather small correlation across tasks.\n\n3) Compared to NASBench201 and NASBench101 the paper also proposed the first tabular benchmark that mimics the macro level optimization of convolutional neural networks.\n\n\n# Questions for the rebuttal\n\n1) The amount of architectures per task is relatively small, even compared to other NASBenchmarks. Also, each architecture is only evaluated once, i.e. the benchmark is deterministic, which might put NAS methods that can handle noise into a disadvantage.\n\n2) The paragraph \"Evaluation of direct architecture transfer\" in Section 5 is somewhat confusing. It says 'After running 500 x 7 trials, each target task has 6 curves, ...\" shouldn't it be than 500 x 6 trials per target task? It then goes on with \"We then take the average of each curve across all 7 tasks\", shouldn't it be averages across the 6 source tasks? Or does the target task also serve as a source task?\nI am also confused by Figure 6. What is the target task here? Is it sem. segment. or Surface Normal? Why are there only 5 instead of 6 tasks?\n\n3) Looking at the experiments it seems that transferring knowledge often hurts performance, i.e negative transfer. REA being the only exception, where the transfer version achieve slightly better results on some benchmarks. Also Figure 4 shows that for the better performing architectures (top 50%) there is only little performance between the most tasks, and sometimes even negative. This raises the question whether the proposes benchmarks actually allow to benchmark transfer NAS algorithms.\n\n4) I would be keen to see the correlation between task for the top 20% / 10% / 5%? Does the correlation decrease even further?\n\n5) What leads to actually a better performance, optimizing the cell search space or the macro-level search space? From Table 3 it seems that the macro-level search space contains for the most tasks substantially better architectures?\n\n\n# Typos\n\n- Section 5 first paragraph: evolution algorithm -> evolutionary algorithm\n\n- Section 5 second paragraph: \"... we first run RS on a task for 50 epochs....\": shouldn't that be iterations?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great new NASBench but questions regarding the experimental evaluation",
            "review": "#### Summary\nThe authors created a novel NASBench dataset that allows to investigate NAS across different tasks. With \"task\" they refer to a change in the machine learning task but not in the dataset. 7 different vision tasks are explored: object classification, scene classification, semantic segmentation, autoencoding, room layout, surface normal, and jigsaw. TransNAS-Bench-101 considers both cell-based and macro-level search spaces and has results for all architectures within this search space on all 7 tasks. Furthermore, the authors compared classical NAS methods to some simple transfer methods.\n\n#### Detailed Comments\nI think the idea of TransNAS-Bench-101 is very useful and the paper is in many part clearly written. I have some comments regarding the experiments conducted. They are important in my opinion since they demo the usefulness of the generated data.\n\nIt is not clear for every experiment how often experiments have been repeated. In some cases the number of repetitions might be too low. There are various cases where the expected network ranking does not align with what one would expect:\nIn Figure 7 there seems to be a significant difference between random search and REA-tfs for the very first trials. However, in both cases the initial trials are chosen uniformly at random.\nThe curve for RS in Figure 6 is very different compared to the curves for RS in the other figures. Why is that the case?\nOut of about 4000 candidates, all optimization techniques seem to find the global optimum within 50 trials. How is this possible?\n\nFigure 6: Why does it contain only 5 source tasks and not all 7? I think it would be useful to either i) create a plot like this and show the horizontal line that indicates how well the best architecture for one task would do on average or ii) add these lines to Figure 6.\n\nIs it possible to make any statements regarding statistical significance? Is REA-transfer statistically better than REA-tfs or RS? Which methods are statistically better than RS?\n\nThe motivation for RSDT is completely unclear to me. Why not run the random search a length of 500*50/7 and use the top 500/7 architectures from each of the 7 tasks?\n\nThe authors mention that TransNas-Bench-101 offers detailed network training information. One could be more explicit what this information contains.\n\nThere is no explanation for Table 2. Why does this table show 7 datasets? I understand that the labels are different but the input images are the same. I guess it is a matter of how you define a dataset. But if there are 7 datasets and 4 tasks, I'd expect that there are 7x4 combintations. The clustering into the 4 tasks also seems arbitrary. E.g. auto-encoder could also be a regression task. Jigsaw and auto-encoder are considered the same task but have different loss functions and hyperparameters. I would consider this to be 1 dataset and 7 different tasks which would align better with the Taskonomy paper that deal with this as a multi-task problem. It would also better align with the paper because the authors sometimes speak about 7 tasks.\n\n#### Conclusion\nConcluding, I have quite some concerns regarding the experiments conducted but I personally think that the TransNAS-Bench-101 data would outvalue them since it has the potential to fuel lots of new research ideas.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}