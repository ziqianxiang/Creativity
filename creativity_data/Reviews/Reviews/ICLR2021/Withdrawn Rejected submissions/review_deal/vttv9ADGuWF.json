{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper provides a simple prediction procedure to defend against (rectangular) patch attacks, and also a method to obtain some random estimates of the certified robustness of the method. The simplicity of the method is certainly appreciated. On the other hand, there are a number of issues preventing the acceptance of this paper. The main problem is that the paper deals with a randomized predictor, yet the certification guarantee developed for deterministic predictors is applied. This leads to several problems, starting from the target being undefined to unfair comparisons. While the authors made an attempt to address this in the rebuttal, more work is needed to properly settle this issue."
    },
    "Reviews": [
        {
            "title": "An elegant defense. But text and discussion must be improved.",
            "review": "### Summary\n\nThe paper proposes a robust neural network architecture to defend against\nadversarial patch attacks --where the attacker can freely modify a small patch\nof size $s_{patch}$ of the input images-- by training the net on random\ncrops of a pre-defined size $s_{crop}$ (in general $\\neq s_{patch}$), and, at\ntest time, use majority voting over several such random crops for every image.\nThey show that, as long as $s_{patch}$ is small ($\\leq 2.4\\%$ of total image),\nthese architectures achieve usual and **certified** robust accuracies that are\ncompetitive with the SOTA among the techniques and architectures that were\ntailored to certifiably defend against such attacks.\n\n\n### Overall evaluation\n\nThe defense is elegant for its simplicity and seems efficient, both in terms of\nperformance for small attack patches (which, arguably, is the most interesting\ncase), and in terms of computational speed/complexity (for small enough sample\nsize of crops). But the paper is still unclear, vague and/or too\napproximate in some parts (see points below). It seems to have been written in\na rush (Fig.2 on the right is obviously not the one intended) and some\nadditional experiments and/or discussions would be useful to complement the\nevaluation of the current method (see point 8. below). So, overall, I don't\nrecommend publication in the current state but am inclined to reconsider when I\nwill see appropriate changes in the paper.\n\n\n### Detailed remarks/questions\n\n1. Since you repeatedly refer to de-randomized smoothing and PatchGuard in the\n   text (and their combination), I suggest adding a short self-contained\n   description (in appendix with a reference) at the beginning (or in appendix\n   with a reference at the beginning) for the non-informed readers.\n2. I suggest to clearly mention that what is certified is the robust **test**\n   accuracy, not the distributional robust accuracy.\n3. I think that you are sampling the crops **uniformly** at random **with**\n   replacement, but I don't recall reading this explicitly in the text. Anyway,\n   it would be good to compare both approaches (with and without replacement),\n   at least in appendix.\n4. The description of Table 1 is in Sec.4.1 with title \"Without patch\n   transformation\", yet the caption says (and Table 2 confirms) that it shows\n   results **with** patch transformations. Which one is right? And if it's with\n   patch transformation, please add the same table for the setting without patch\n   transformation (at least to appendix).\n5. Eq. 6: capital $N$ in $C^N_i$ should be $n$.\n6. Please do an additional pass of proof-reading\n7. Sec 3.2: the description of how to compute the certifiable robustness bound\n   could be **significantly** improved and seems to be written in a big rush. In\n   particular, you never explicitly say which formula is used to compute the\n   robust bounds.\n8. _Crop size vs patch size_: The trade-off between crop size and attack patch\n   size is discussed in a rush at the end of the experiments (Sec. 4.2): it\n   deserves attention and an intuition explanation of the trade-off much earlier\n   in the paper, f.ex. when explaining the overall attack. Also, the paper could\n   benefit a lot from a finer analysis of the dependence of the optimal crop-size\n   on the attack's patch size (and probably on the typical size of \"relevant\n   information\" in the images). This could be either empirical (studying the\n   optimal crop-size as a function of the patch size) or more theoretical (with an\n   image model, or by using the average size of relevant objects in the image).\n   Also, it could shed more light on why your attack works better on ImageNet than\n   on CIFAR10 (probably because the typical size of the relevant parts of the\n   images are different in both datasets).\n9. _Sec. 3.1 Â§Training randomized cropping classifier_: the sentence \"the only\n   trainable part of the randomized cropping classifier is the crop\n   classifier\" deserves a proper Proposition/Theorem (with precise and explicit\n   hypotheses, s.a., f.ex. the assumption that crops are sampled **uniformly**\n   at random with replacement) and a proper, well-delimited proof. \n\n------------------------------------\n\nUpdate after rebuttal\n-----------------------------\n\nThe updated version is clearly better than the first one. However, the concerns\nof the other reviewers regarding the randomness of $p_c$ (and therefore of the\ncertification method) have convinced me that there are, indeed, some further\nclarifications and discussions needed prior to the publication, which is why I\nwill keep my initial recommendation.\n\nTo be more precise, the root issue here seems to be that the proposed\nclassifier is not deterministic, which means that the standard definitions of\nadversarial examples and adversarial accuracy do not apply and therefore, that\nthe problem that you try to solve is unclear and/or not well defined.  In\nparticular: what is it that gets certified?  what does it mean to get\ncertified? and, more generally, is the word \"certified\" really appropriate in\nthis context?\n\nHowever, whether an analysis of the distribution of $n_{2to1}$ and $p_c$ will\nbe needed (as asked by other reviewers) might depend on how the authors will\ndefine adversarial vulnerability in the random setting, and what they try to\ncertify. Let me explain what I mean.\n\nA reasonable start might be to define adversarial risk as\n$$\n    E_{(x,y)} E_{\\phi} \\mathcal{L}(\\phi(x), y) \\ ,  \\tag{1}\n$$\nwhich is the usual definition, but with an additional expectation over the\nvariability of the classifier $\\phi$.  Adversarial accuracy would then be the\nadversarial risk for the 0-1-loss $\\mathcal{L}_{0-1}$. Then the authors could, f.ex., set as goal\nto construct a (provably) unbiased estimate of this quantity.\n\nThe advantage of such a method is that one doesn't forget the fact that, what\nwe actually want to certify is this \"distributional\" robustness (i.e. where\nexpectation is taken over the true underlying, unknown distribution), not the\nrobustness on the test set. Even methods that have a non-random certification\nprocess (so-called \"provable robustness guarantees\") will never be able to\ncertify this quantity: they'll only deliver certificates on test example. The\n\"certified robustness on the test set\" that they yield is also just a random\nvariable which we hope \"generalizes to\" (1). Reviewers almost never ask authors\nto analyze/certify this generalization gap. Similarly, here, one could see the\nrandomness over $n_{2to1}$ and $p_c$ as just another source of randomness\ncontributing to the variability of the generalization gap, in which case, maybe\nno rigorous analysis could be acceptable, as long as it is clear what the\nauthors want to certify (unbiasedness of the estimator, f.ex.). Therefore,\nwhether this source of randomness could or should be explicitly captured/used\nby the authors' method is, I think, a question of how the authors frame the\nproblem and their goal.\n\n### Minor points:\n\n- even the revised version still contains quite a few grammatical errors,\n  especially in the new/re-worked sections, where many articles (\"the\", \"a\")\n  are missing.\n\n- End of p.5, \"to maximize number of certified robust images, the randomized\n  cropping classifier should maximize n2to1, which is equivalent to maximizing\n  classification accuracy of $g_\\theta$\": not sure about this equivalence.\n  Maximizing the classification accuracy is equivalent to maximizing n1, not\n  necessarily n1-n2.\n\n- are DRS and PG also probabilistic certifications (i.e. certifying robustness\n  with some probability, f.ex. pc>.95)? This should be clearly said in the text\n  and the captions, especially since it would make the comparison a bit unfair\n  if their certification were 100% sure. (This issue is obviously related to my\n  previous major remark on randomness.)\n\n- the name \"worst-case certified accuracy\" in the caption of Table 1 is very\n  unclear at that point. It becomes clear in Sec. 4.3, but you refer to Table 1\n  in Sec. 4.1 already. So this term should be clearly explained in the caption,\n  or there should be a clear reference to the relevant part in the text.\n\n- don't always re-cite Levine&Feizi and Xiang et al. every time you mention\n  de-randomized smoothing and patch guard. Cite them the first time, and then\n  say that you'll refer to de-randomized smoothing and patch guard as DRS and\n  PG in the rest of the text.\n\n- in the conclusion: \"This paper proposes a new architecture for defense\n  against\" -> \"This paper proposes a new defense against\". (You are not really\n  proposing a new architecture.)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposed a simple way to defense adversarial patch attacks.",
            "review": "The method can be basically summarized as a majority voting of crops of an image. Moreover, a new certification of the proposed method is introduced, not similar to the conventional adversarial robustness certification on perturbation under $\\ell_p$ ball , the method using simple geometry and probability problem to certify the results instead of any relaxation of the neural networks.\nHowever, the paper is poorly written, and many experimental setups are missing.\n\nUpdate after rebuttal:\nAfter reading the rebuttal, I don't think my questions are addressed very well, especially about the confidence probability, $p_c$. The certification is defined as a guaranteed yes/no problem but the $p_c$ will relax the certification to a probabilistic problem. Also, PatchGuard with patch transformation is out of scope for the original paper, so I think the experimental results in Section 4.1 and 4.2 are more like a fair comparison. However,  refer to the results in table 3, the proposed method yields worse performance than PG-DRS although the computational cost is saved. Hence, I will keep my rating.\n\nPros:\n\n+The proposed method is quite general and intuitive.\n\n+Using a simple geometry and probability model, the proposed method can certify the robustness of the adversarial patch attack in a very efficient way.\n\nCons:\n\n-What's the training time of the crop classifier $g$ compare with other baselines?\n\n-The whole system highly depends on the test accuracy of $g$. What's the result if no attack involved in?\n\n-From the experimental results, the proposed method has margin improvement compare with PG+DRS, and the basic idea of the certification algorithm also follows PG+DRS. So, the contribution is not enough for ICLR.\n\n-The hyperparameter $p_c$ is set as 0.95 in the experiment. However, the certification needs guaranteed results. So, if the $p_c = 0.95$, is that means there is a 5% possibility that this image can be attacked? Although this may not change the results so much, the ablation study of this parameter setting should be performed to make the comparison fairly.\n\n-Figure 2 (right) seems to use the wrong plot.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Update after the rebuttal:\n\nI read other reviews and response from the authors and I decided to keep my score. Overall, I think paper still needs more\nwork. For example, incorporating details on confidence into the main paper and not just a section in the appendix is quite\nimportant as otherwise the paper is misleading.\n\n==================================================\n\n-> Summary: \nIn this paper, authors propose a new certified defense against adversarial patches. They propose a model\nwhich samples different patches from the original image, performs classification of these patches using neural network classifier and performs a majority vote to compute the output label. The guarantee is obtained by computing a probability that none of the sampled patches intersect adversarial patch.\n\n-> Reasons for score:\n\nI vote for rejecting this paper. The main issue I have is that their probabilistic guarantees lack confidence intervals and \ntherefore it is not clear how meaningful they are. Further issue is that the method works better than prior work only if \ncertain image transformations are applied, such as rotation. Yet, this critical part is not described formally: what is rotated, entire image or just a patch? Same holds for aspect ratio. Due to these problems, I cannot recommend acceptance for this paper.\n\n-> Pros:\n\nI think explanations of the method are easy to follow and writing is solid, with some parts that require more clarifications.\n\n-> Cons:\n\nThe biggest problem I have with this paper is that authors do not consider confidence intervals for their guarantees. In particular, probability p_c computed in Equation 6 is tied to the particular n patches that were sampled which determines the value of n_{2to1}. This means that n_{2to1} is random variable and p_c that authors compute holds for just one sampled value of n_{2to1}. So for this bound to be useful, we need some form of confidence interval. For example, guarantee could be that with 99% probability p_c is interval [p_c_low, p_c_high]. And then, only if p_c_low is greater than some probability threshold (which is set to 95% in Section 4), we can report that this sample is certified. If I misunderstood something, it would be great if authors can clarify what kind of guarantee is provided.\n\nAdditionally, I find method used here relatively trivial and I think more technical contributions are necessary (e.g. computing confidence intervals above). In terms of empirical results, it seems that this method works better than prior work only in the case of additional image transformations and otherwise it performs worse or same. As this is the critical thing, I think authors should provide more explanations on how are these transformations applied, more formally instead of just describing it.\n\n\n-> Questions:\n\n- In Equation 6, summation is over i in the set {0, n_{2to1}}. Should this summation be over 0 <= i < n_{2to1} instead?\n- Can you clarify what is the difference between the experiments in Table 1 and Table 2? If I understand correctly, the experiments in Table 1 have additional transformation that is applied besides the adversarial patch? \n- Can you write mathematical formulation of transformations in Section 4.2? I am not sure whether these transformations are applied over the entire image or only over the patch, so it would be good to write more formally what exactly is the transformation.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Surprisingly simple approach to an important problem",
            "review": "The authors propose a surprisingly simple statistical defense, that can certify the robustness of a classifier against patch attacks. This is achieved by randomly sampling small rectangular subregions of the perturbed image and classify these samples individually. \n\nThe paper is mostly well written. Certification methods, able to handle patch attacks, are of significant interest in real world applications like autonomous driving. The paper is charming because of the simplicity. However, some questions remain unanswered and the experimental evaluation is not very thorough, hence this paper is borderline. \n\nQuestions:\n- Have the authors considered some kind of adversarial training? \n- As far as i understood the paper, the certification method is probabilistic due to the probabilistic intersection of sampled subregions with the adversarial patch. Could this be circumvented by defining a fixed set of subframes that get classified? Then the number of overlapping subregions could be calculated precisely in advance and the certification would not be probabilistic. \n- How does the right figure in Figure 2 look like? The current one seems to be the wrong one. \n- In Table 1, Table 2 and Table 3, the authors provide certification rates. Could the authors also provide how many images of the not certified ones can be successfully attacked?\n- When is $p_c$ as in Equation 6 considered to be close enough to 1, is it $0.95$ as in Section 4?\n- How do the certification rates change for larger/smaller adversarial patches?\n\nComments:\n- $C_i^N$ in Equation 6 should be defined. \n- A vertical line between the CIFAR10 and the ImageNet results in Table 2 & 3 would improve readability.\n\nI am willing to increase my score if my questions and concerns are addressed. \n\n-------------------\n\nAfter reading the authors response, i think that this work would benefit from an experimental comparison of their random method against a method relying on a deterministic crop selection, even if the certification rates of the deterministic method are inferior, because the certificates both methods are yielding are different: The resulting certificates from the deterministic method would be deterministic instead of probabilistic. Hence i will retain my score. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}