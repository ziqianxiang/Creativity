{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for single image depth estimation based on spectral consistency and predicted view",
            "review": "This paper deals with an important problem in computer vision of generating depth estimates from a simgle input image. Banking on the fact that single image depth estimation is usually inferior to depth estimation from stereo, the authors propose a novel architecture to infer accurate disparity by leveraging both spectral-consistency based learning model and view-prediction based stereo reconstruction algorithm.\n\nThe first part where the authors build the spectral consistency based depth estimation seems to be entirely similar to the development mentioned in [Godard et al. 2017]. The L_{cs} term just checks for the upper bound of the disparity in addition to the flow mentioned in the reference paper.\n\nThe single image stereo by predicted view is mostly the Cycle-GAN [Zhu et al. 2017] formulation with no new novelty in it.\n\nFinally, the fusion algorithm to generate the combined disparity map seems to be mostly from [Tosi et al 2017] paper.\n\nThe technical contribution of this work is quite limited as it just computes the outputs for two established methods and then proposed to use another paper to fuse / conditionally select one of the outputs from the disparity generation methods.\n\nThe results reported seem to support the idea that the combination of two strong methods with a nonlinear combination algorithm beats each one of them individually. The novelty of this method remains a major bottleneck for this paper.\n\nFor this venue, I would go with a reject decision since this work does not propose anything to further the research field in this domain.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Single Image Depth Estimation Based on Spectral Consistency and Predicted View\"",
            "review": "### Summary\nThis paper proposes a self-supervised framework to tackle single view depth estimation problem.\nDifferent from previous methods that directly estimate depth map from a single image, this paper proposes a depth fusion method which fuse depth estimations from (1) single-view (2) stereo matching of a synthesized stereo pair.\nSpecifically, \n(1) a disparity network is trained to estimate both left and right disparity maps.\n(2) a view synthesis network is trained to synthesize left/right view from a given view. Then SGBM is applied on the synthesized pair to estimate the disparity map.\n(3) To fuse the disparity estimations from (1) and (2), a confidence network is trained to predict the confidence for each prediction. (4) Finally, a winner takes all appraoch is adopted to select depth prediction from (1) or (2).\nTo train the networks,\n(1) single view depth estimation network is trained in a self-supervised manner, i.e. photometric-based loss (spectral consistency loss) as the main supervisional loss\n(2) GAN-based loss is used to train the view synthesis network\n(3) To train the confidence network, conventional confidence measures are used as the label.\n\n### Strength\n#### interesting idea of combining single-view depth and single-view stereo estimation\nPersonally, I like the idea of single-view stereo estimation though sometimes I think that it is a little bit \"unrealistic\" (no offence). However, if we can do single view depth estimation, why not single view stereo estimation.\nMoreover, I believe stereo matching should produce more accurate depth estimation than single view depth estimation, though depending on the quality of the synthesized image.\n\n#### higher accuracy when compared to single-view depth estimation frameworks\nIn Table 2, the authors have shown better performance when compared to prior arts. However, I believe the comparison is based on the \"without ImageNet pretrained\" setup. This work has shown better performance than the popular monodepth2 framework under this setup. It will be more convincing if the authors can show better performance in the \"with ImageNet pretrained\" setup thus it will be the real STOA result.\n\n### Weakness\n#### Overclaiming of contribution\nThe authors have stated three contributions, including introduce a self-supervised method based on the spectral consistency for image consistency.\nHowever, the only difference I can see is extending the left-right consistency [proposed by monodepth] by a robust loss, i.e. BerHu loss.\nThis doesn't create a significant contribution, especially under the circumstance that no experiment support.\n\n#### incomplete diagram illustration\nIn figure.1 (Inference), the diagram over simplifies the inference process. e.g. the view synthesis network doesn't output a depth map directly but the network+SGBM outputs the depth map. Also, if I understand correctly, the confidence network is missed for the fusion as well.\n\n#### unconvincing qualitative result / depth fusion scheme\nThough the authors have shown improvement on quantative result using the proposed method (with stereo fusion), the qualitative result looks worse than my expectation.\ni.e. from Figure 3, the output from single image stereo shows artefact on ground (textureless regions). In row 1, the artifect are passed to the full output after fusion. In row 2, the artefect are not passed to the full output in contrast. The confidence prediction/fusion scheme looks a little bit random.\nCould authors shed some lights on the fusion scheme? e.g. what scenes will it be useful/fail?\n\n#### writing issue\nI would suggest the authors proofread before submission. There are some writings/sentences are not clear to me / typo.\ne.g.1 Sec 4.2 (para. 2): \nFor the predicted view-based single image stereo network, We train the IT from scratch ...\n- We-->we\n- train the \"IT\": what does it mean?\n\ne.g.2 Sec 3.4 \nThese two networks can be combined for joint training once being trained to obtain the ability\nof geometric reasoning for the task of view synthesis and stereo matching separately. End-to-end\ntraining of the whole pipeline can These two networks can be combined for joint training once being\ntrained to obtain the ability of geometric.\n- incomplete sentence\n- repeated sentence\n\nThese low-level writing issues give an impression that the article is written in a rush and lower confidence on the paper quality.\n\n### Conclusion\nGiven the insufficient experimental support and explanation, I will give rejection at this stage.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but a bit over-claimed. Detailed experiments are required to validate the contributions.",
            "review": "Summary: This paper proposes a two-stream network to tackle the single-view depth estimation problem. One stream predicts disparity map directly from the input image, while the other stream synthesizes a novel view image and then estimates the disparity via stereo matching algorithms. The outputs of the two streams are then fused together according to the estimated confidence maps, to generate the final result.\n\n\nStrength:\n+ The idea of synthesizing another view and then do stereo matching seems interesting and novel \n+ The proposed method achieves good performance on the KITTI benchmark\n+ The proposed method seems to transfer well to other driving scene data quite well visually\n\n\nWeakness:\n- In the introduction, the authors list three key contributions. However, the authors cannot really say they “propose” a self-supervised method for disparity prediction. The actual contribution is the improved disparity consistency loss (Eqn 3). However, no experiments are conducted to validate the effectiveness of this loss.\n- More details are required for the last paragraph of Section 3.2. In addition to SGBM, how do the authors incorporate ELAS and the Bayesian inference strategy?\n- In Section 3.3, the authors claim “confidence maps are trained separately in a self-supervised fashion without using ground truth depth labels”. However, how exactly the authors train this network remains unclear to me.\n- The text in Section 3.4 seems messy. Also, “end-to-end” usually indicates training the whole system in one stage. I feel that the authors should not use this term here.\n- Table 1 is a bit confusing. I am not sure what “stereo only” is and what “mono only” is. Are they actually the two streams of the whole system? The improvement over Ferrera et al. seems quite marginal (3.03% v.s. 3.01%). My concern is whether the improvement is only on the KITTI dataset or generalizable.\n- In the first paragraph of Section 4.2, the authors say that they use post-processing to eliminate the effect of dis-occlusions. However, I did not find relevant descriptions in Table 2 and the corresponding description. So I assume “Our full” uses this post-processing. It is necessary to show the raw performance without post-processing. Also, I wonder if the authors can show the performance of the stereo-matching stream (w/ and w/o post-processing). Another interesting study will be, replacing the synthesized image with the ground truth right-view image, so that we can know the upper bound performance of this idea. This might be more informative than Table 3.\n- Comparing to some of the methods in Table 2 that use only a single-view during training, the proposed method uses stereo pairs. It is better to mention this in the table.\n- I wonder if the proposed method can only work well on the driving scene. To test the generalizability, previous approaches test their method on other depth estimation dataset (e.g., Make3D) without fine-tuning. Maybe the authors can also show such a study.  \n\n\nOther comments:\n* Why use the term “spectral consistency”? “Photometric consistency” or “appearance consistency” seems to be more commonly used by the community.\n* The images in Figure 3 are not spatially aligned.\n* I am a bit not sure the purpose of Figure 7.\n\n\n---- Post-rebuttal comments----\n\nThanks for the response. However, the rebuttal does not fully address my concerns. The current presentation is not good enough, and the technical contribution seems limited (very likely due to the presentation). Thus, I keep my initial score.\n\nI encourage the authors to re-write the paper and submit it to the next venue. In particular, the authors should use (ablation) experiments to support/validate the proposed technical novelty.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}