{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces a self-training strategy for semi-supervised learning for few shot sequence learning.   It builds on ideas from an existing work on robust deep learning that adaptively reweights examples for learning to reduce impact of noisy examples, here the noisy examples are introduced to the student network training by the teacher network.  Two main novel points, one is on  selectively constructing the validation set used for adaptive reweighting.  Another idea is to move from the sentence level reweighting to token level reweighting.   The paper shows strong results suggesting the proposed method can effectively learn under few-shot learning. \nA primary concern from the reviewers is that the paper has limited novelty given that it primarily applies existing ideas to a slightly different problem.  Another concern is that the system consists of many components, each of the choices could have other viable options.  The ablation studies indicate these components are useful compared to when removed, but fail to explore possible alternative choices. One of the questions is whether token-level reweighting is necessary. It would have been nice to see an ablation study comparing against a baseline using sentence-level reweighting. \n\n"
    },
    "Reviews": [
        {
            "title": "The limited novelty, lack of sufficient rigor around proposals make it less publish-worthy.",
            "review": "The authors propose adaptive self-training that uses self-training + meta-learning for few-shot training of neural sequence taggers. Specifically the authors focus on reducing noisy training data for student models and reweighting them.\n\n\nADAPTIVE LABELED DATA ACQUISITION: \n* The motivation was not clear to me. \n* The authors proposed a way to select example based on teacher loss, but it's unclear to me why this was done in this way. I did not find sufficient discussion around this. What happens if we don't do this?\n* Overall, It would've been nice to see a deeper discussion around whether this is needed in the first place, what benefits does it give, and if so what are all the ways of solving this problem and why the particular approach is the right one.\n\nRE-WEIGHTING PSEUDO-LABELED DATA\n* I could not find sufficient novelty about the \"token\" aspect. What's special about perturbing weight for each token vs instance.\n* the entire section was a bit unconvincing, I could not find the motivation for diversity, nor sufficient rigor for the choices made.\n \n \nExperimental Results\n* It's nice to see several experiments. \n* Have the authors considered introducing a pretraining objective on the unlabeled data? I wonder how much of the \"gap\" from pretrain+finetune would go away if the teacher models were pretrained on the in-domain unlabeled data.\n* Have the authors compared with an explicit distillation step of a teacher (E.g. BERT+finetune + distill)? I am asking because this might really show whether we need 'reweighting', noise reduction etc.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ablation study missing",
            "review": "SUMMARY\nThe paper presents a series of strategies for self-supervised learning for sequence labeling tasks.\nThe proposed model is a teacher-student network.\nA teacher model is trained on a small set of labeled data, and then is used to pseudo-annotate a lot of unlabeled data. The student model is trained on the pseudo-labeled data.\nThis paper introduces a strategy to select informative labeled examples to use as dev set for the student model, and adapts an existing re-weighting mechanism for pseudo-labeled examples to the sequence labeling setting.\n\nThe overall approach is tested on different sequence labeling datasets with different characteristics.\nIn low resources scenarios, the proposed model significantly outperforms previous models.\n\n------------------------\nREVIEW\n\nPlease, make clear that the model is thought for a low resource setting.\nIt would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled.\nFollowing the experiments in table 5, this should not work as good as in a low resource.\nThe authors should give a intuition, or an answer on why this is the case.\n\nThere is an important experiment missing.\nThe impact of adaptive label data acquisition is not tested.\nWhat happens if you keep all the examples?\n\nThe argument in the introduction about self learning not suitable for sequence learning model is a bit weak. Self learning has been extensively studied and successfully applied in sequence labeling tasks (for example, https://arxiv.org/pdf/1804.09530.pdf).\n \nWhy do you need to sum in equation 8? do you have same instances and same tokens in different batches? why do you need that?\nIn general section 3.2 could be a bit  more clear.\n\n- Presentation\n\nSome sentences are very long and hard to read, i.e., \"To address such issues stemming from noisy labels and training set biases, learning to re-weight noisy examples (Ren et al., 2018) lever- ages a meta objective with the basic assumption that the best weighting strategy should minimize the loss on a held-out clean labeled validation set.\"\n\nRen et al. 2018 has been published at ICML 2018, please update the references.\n\n\n-- UPDATE\nThanks for the clear and exhaustive response.\nMinor, regarding the ablation study in A.1, with S=3 you get the best results, why not try with more?\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A self-learning framework for sequence labeling with solid experimental results",
            "review": "Summary\n\nThis paper proposes an adaptive self-training framework, called MetaST, for tackling few-shot sequence labeling tasks. The framework consists of several components: a teacher model that finetunes with the few-shot training data and generates noisy labels for the unlabeled examples; a student model that learns from re-weighted noisy labels (at the token level), and an iterative process to update the teacher with the trained student. It also uses a meta-learning mechanism to adjust the token-level weights based on a subsampled set of clean data. This subset is sampled based on the student model’s uncertainty to improve learning efficiency.\n\nThe proposed system is evaluated on a few sequence tagging tasks for slot filling or named entity recognition. It outperforms previous semi-supervised learning systems across all the evaluated tasks.\n\nStrengths \n\n- Very solid experimental results on both English and multilingual datasets. The comparison against previous systems (including ones using BERT, similar to the proposed model) seems quite thorough.\n- Ablation studies that showcase the effectiveness of each model component.\n\nWeaknesses\n\n- The framework is quite complex with many subcomponents: uncertainty-based data acquisition, meta-learning based token-level re-weighting, iterative updates, etc. It is nice that the paper contains a fairly complete ablation study to analyze the effectiveness of each of these components, though.\n\nOther questions/comments:\n\n- In Algorithm 1, how do you check convergence? How many steps does that usually take?\n- In the last paragraph of section 4, “Analysis of pseudo-labeled data re-weighting”, does “step 100” refer to the step of the student model or the outer loop (teacher re-initialization)?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}