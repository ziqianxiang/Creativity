{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "While the paper's topic is on a topic of interest and presents an evaluation on three synthetic datasets,  PartNet-Chairs, Shop-VRB-Simple, CLEVR dataset, several concerns and weaknesses remain after the author response.\n\nMain Concern and Weaknesses:\n* The main improvement comes from the additional supervision provided by language, which provides a strong supervision signal as the language is scripted and the parser has nearly \"perfect accuracy (>99.9%) on test questions/captions\", as the authors state.\n* Limited contribution: combination of MONet/Slot-Attention with NS-CL; \n* Experiments limited to synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). This is especially concerning when the task is segmentation and the supervision comes from templated language, making it a strong supervision signal.\n* The positive impact of the objectness score on performance was not sufficiently demonstrated\n* Additionally, in the final discussion phase, reviewers were concerned that the with limited visual reasoning training on a subset of 25% of the data, reduces the performance [I note that I did not take this as the decisive reason for rejection as the authors did not have a chance to respond to this concern but the authors should discuss this in any future version of the paper]\n\nThe paper initially received borderline and reject scores and the authors took significant effort to address several of the comments of reviewers. While the paper was improved several of the main concerns remained and reviewers recommended reject after reading the author response and each others comments.\n\nI agree to the concerns and recommend reject."
    },
    "Reviews": [
        {
            "title": "Interesting paper, but critical information is missing. ",
            "review": "This paper proposed an interesting idea that uses language to learn the concept and aid downstream tasks such as segmentation and referential expression interpretation. The authors combine the unsupervised segmentation method (MONet and Slot Attention) with neural symbolic concept learning (NS-CL). By joint training these two objectives, the authors show improvements in the object segmentation and several downstream tasks. \n\n1) What is the language objective for the PartNet-Chairs dataset? The dataset contains templated captions instead of questions, but the paper didn't mention any of the associate target or loss. \n\n2) A lot of the paper needs more clarification, for example, how to calculate the  \"Whiteness\" using the concept embedding? How many programs are there? I can not find that information even in the supplementary materials. It is hard to understand the exact algorithm of the proposed method.  \n\n3) The performance of pretrained semantic parser is not reported in the paper. Since there are only a few templates, will the model achieve 100 percent performance? \n\n4) The semantic parser info is necessary since the model will know the exact additional information of the image given the question and captions (especially captions), assuming there is no error in the parsing procedure. This will harm the overall novelty of the paper since the proposed method can not generalized to more complicated questions and captions. \n\n5) If the pretrained semantic parser can provide very high accuracies, for a fair comparison, it will be good the authors can show the performance of MONet and Slot Attention with additional supervision. For example, Instead of having a Neuro-symbolic program executor, simply use the concept embedding to filter the object-centric representations and then do image decoding. This will show the effectiveness of the Neuro-Symbolic Program Executor.\n\n6) Missing related work, [1] also learns the visual concept through question answering. The authors should discuss the difference between this work. \n[1] Yang et.al Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition (CORL)\n\nI really like the paper's idea, but a lot of critical information experiments are missing. I am happy to increase my rating if the authors can resolve my questions. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Language mediated, Object-Centric Representation Learning",
            "review": "Focus of the work: The paper tries to tackle the problem of learning object centric representations using language. The authors note that most of the previous work which tries to use language  assume pre-trained object detectors to generate object proposals in the visual image. \n\nMethodology: The proposed method consists of 4 modules. \n1. Object Encoder, which tries to generate a modular representation of the scene. \n2. Object Decoder, a module to reconstruct masks corresponding to individual objects given the high level factorial representation from the object encoder.\n3. A pre-trained semantic parser: a pre-trained module to parse the input (for ex. a question) into semantically meaningful program which can be easily executed.\n4. Neural-symbolic Program: This modules takes a factorial representation (i.e., the output of the encoder), as well as some other intermediate information from module 2 and module 2 and outputs an answer to the question. \n\nThe paper mentions that this is a \"principled\" framework for object centric learning. By keeping the module 3, and module 4 fixed, the authors evaluate different methods for learning object centric representations like MONET and SLOT attention. The authors evaluate how inclusion of the language can improve the performance of MONET and SLOT attention.\n\nExperiments: The authors evaluate the proposed work on two visual reasoning datasets for image segmentation evaluation: Shop-VRB-Simple, and PartNet, as well as a subset of PartNet called PartNet-Chairs. The paper shows that inclusion of the language improves the performance of both MONET as well as SLOT attention, albeit the increase in performance corresponding to SLOT attention is more as compared to MONET.\n\nInteresting points:\n\n1. I like the problem which the proposed method hopes to achieve i.e., learning the representation of high level variables (i.e., objects), which are often associated with language.  \n\n2. It's interesting to note that the gain in performance for Slot attention is more as compared to MONET. This point was also emphasized in the work of RIMs (https://arxiv.org/abs/1909.10893), where they used a \"top-down\" representation to learn object centric representations.\n\n3.  I like the ablation in table 4 i.e., fine-tuning the entire model improves the performance of the model, as compared to separately training the different components. \n\nMajor Points:\n\n1.  I think the paper presents nice preliminary experiments for showing that incorporation of language can help learning object centric representations. Since the contribution of the paper is to actually evaluate different methods for object centric learning (MONET/SLOT Attention) and combining with the framework of neuro-symbolic learning. It would be interesting to evaluate the LORL + Slot Attention as well as LORL + MONET for more difficult tasks to see how well the learned representations transfer to new environments which share some common structure. It could be in the form of  language conditioned scene generation such that by using language one can generalize in a compositional way to new scenes or in the context of instruction following (instruction is in the form of language) where their are objects in the scene, and the goal is to put the objects in  a particular spatial configuration or just navigation (https://arxiv.org/abs/2003.05161).\n\n\nMinor Comments:\n\n\"We have proposed Language-mediated, Object-centric Representation Learning (LORL), a principled\nframework for learning object-centric representations from vision and language.\"\n\nI'm not sure about the use of the word \"principled framework\" as the proposed method is not really a framework. As authors note in the paper, the goal of the paper is to see how incorporation of the language can be used to learn or improve representations of the high level variables (i.e., objects).\n\n======\n\nAfter Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I very much agree with the authors that the problem is very interesting, but as of now more work needs to be done in terms of \"downstream applications\". \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Intuitive approach mostly based on existing components, somewhat limited evaluation to support all the claims.",
            "review": "=Summary\nThe paper proposes a framework for object-centric representation learning with additional language supervision such as e.g. questions and answers, denoted as Language-mediated, Object-centric Representation Learning (LORL). The authors combine two ideas from prior work, the unsupervised object-centric representation learning and the neural-symbolic concept learning, in one architecture. The model obtains object representations by learning to reconstruct the input image (as in MONet and Slot Attention). The learned representations are used as input to the neural-symbolic program executor, which learns to answer questions about objects. The entire model is trained in three stages: first the reconstruction objective, then the QA objective, and, finally, jointly. Experiments on two datasets demonstrate that the obtained object segmentations have better quality that those of the original unsupervised models. The learned representations are also shown to be effective in several other down-stream tasks.\n\n=Strengths\n\nThe intuitions are clearly conveyed and the method is well motivated: language introduces strong inductive biases, providing concept names which can in turn be grounded in images, helping the model understand what these concepts should look like (especially for heterogenous objects such as “coffee maker”).\n\nThe authors have collected language descriptions for the PartNet-Chairs dataset.\n\nAdding LORL on top of Slot Attention leads to notable improvement in object segmentation performance on two datasets.\n\nThe learned representations (using Slot Attention) allow to retrieve similar objects that belong to the same object category, showing that they capture object semantics much better than the unsupervised models.\n\nIt is interesting to see that the learned representations can be effectively used for other tasks, e.g. referring expression comprehension w/o additional fine-tuning.\n\n=Weaknesses/High-level comments\n\nI am not sure how unexpected the main result really is (the improved segmentation quality), considering that LORL receives additional (language) supervision. Comparison of vanilla unsupervised models vs. LORL is not completely fair or informative, in my opinion. It would be interesting to compare LORL vs. some other forms of supervision (or even fully/partially supervised segmentation models) or show how VQA supervision compares to caption supervision (more on that below), or using attributes instead of language, etc.\n\nThe authors claim that “LORL can be integrated with various unsupervised segmentation algorithms” (P1). In practice almost all the experiments are carried out with just Slot Attention. The only reported result with MONet (Table 2) is somewhat weak (as also stated by the authors, P7). This seems to undermine the authors' claim. \n\nThere are no experiments on the popular CLEVR dataset, only on the two recent datasets introduced by the authors (ShopVRB-Simple is obtained by selecting easier scenarios from ShopVRB). Specifically, they could have used CLEVR for MONet, as MONet is not applicable to the ShopVRB-Simple data.\n\nIt is in unclear what the language task on the PartNet-Chairs actually is, it is only stated that the authors collected descriptive sentences. It is mentioned that the output can be True or False; does it mean that the task is to predict whether the sentence is true to the image? No”negative” examples are included in the paper, nor is there any discussion on that. \n\nWhy have the authors decided against collecting question-answering data for the PartNet-Chairs, similar to the ShopVRB-Simple? Is there an advantage of using a specific form of supervision (e.g. question-answering supervision)? Some analysis on which form of supervision is more effective would have been interesting.\n\nThe overall framework mostly relies on existing components, the main technical novelty is bringing them together and an additional objectness score used in both objectives. It is not entirely clear whether the reported Slot attention and MONet performance is obtained with the vanilla implementations or after stage 1 training of the proposed model (which also includes the objectness scores). Overall, would be interesting to see an ablation of the proposed objectness score.\n\nWould be great to learn more about the referring expression comprehension experiment (what the test data looks like, how similar it is to the observed training data, how was the model adapted to the task, etc).\n\nThe authors show that the final training stage significantly improves the model’s QA abilities, saying that it demonstrates the importance of joint training. I am not entirely sure about the purpose of this experiment. Would be also interesting to see how the obtained performance compares to the vanilla neural-symbolic approach.\n\nHere we have synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). I am somewhat skeptical about the generality of the proposed framework (which also concerns prior works on which this work builds), i.e. can we expect that this model will work with real cluttered images and real natural language? What do the authors believe is the main limitation of their proposed approach?\n\nNothing was stated about making the code available.\n\n=Detailed comments (P# - page number)\n- P1: perhaps “(for) learning disentangled, object-centric scene representations”?\n- Throughout the paper the authors say “referential expression interpretation”, while this task is more commonly known as “referring expression comprehension”.\n- The authors say “object” to refer to both objects (in ShopVRB-Simple) and object parts (in PartNet-Chairs). I wonder if there is a better way to explain this, which encapsulates both scenarios.\n- Fig 1 is misleading as it misrepresents the task posed on the PartNet-Chairs dataset (it is not question-answering). \n- P2 ShopVRB should be ShopVRB-Simple.\n- P2 PartNet should be PartNet-Chairs.\n- P3: should o_i be z_i?\n- P3 white object Fig. 2 => in Fig. 2\n- P4: should {z_i} be {z_k}?\n- P4 which query the attribute => which queries the attribute\n- P4 All concepts appear => All concepts that appear\n- P4 “We use the same domain-specific language (DSL) for representing programs as CLEVR “ : in fact it is not the same but extended, as stated in the appendix.\n- P5 an filter => a filter\n- P5 “we find that it better highlights the quality of learned object-centric representations for various models.” - not clear what this means\n- P7 “The quantitative results are summarized in Table 1.” - make it clear that this is for the ShopVRB-Simple\n\n==================================================================\n\nPost-rebuttal comments:\n\nI thank the authors for an extensive response and the other reviewers for brining up many relevant questions!\n\n= Main positive additions: \n\nLORL was successfully combined with another unsupervised approach, SPACE, on the CLEVR dataset.\nLORL+SA outperforms IET, which has the same amount of supervision. It does lose to NS-CL slightly, which has access to pre-trained object detectors.\nI like the added analysis of the QA types, data efficiency etc.\n\n= Some of the remaining issues:\n\nThe positive impact of the objectness score on performance was not demonstrated. To show its benefit the authors had to propose yet another evaluation scheme (precision and recall of the reconstructed scene graph).\nThe training objective for PartNet-Chairs should be discussed in the main paper, not in the appendix. Also, perhaps I am missing something, but would not one still need some negatives to train it?\nMinor: Fig 1 in the revision is still wrong, i.e. the example for PartNet-Chairs dataset still illustrates the QA task.\n\nOverall, I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited. The authors have considerably expanded their evaluation, but at the same time have introduced another confusion (as pointed out by R3 during post-rebuttal discussion): it appears that using 25% of supervision leads to lower segmentation performance, contradicting the main claim of the paper. \nI therefore decrease my score to 5. I hope to see an improved version of the paper (with more exciting technical contributions) in a future venue!",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction, but limited novelty of current approach and unsurprising results",
            "review": "### Summary\n\nThis paper proposes to combine the neuro-symbolic concept learner for visual reasoning from language (NS-CL; Mao et al., 2019) with recent unsupervised approaches to learning object-centric representations such as MONet (Burgess et al., 2019) and Slot-Attention (Locatello et al., 2020). While NS-CL normally relies on pre-trained object-detectors (in a supervised fashion) to extract visual representations, the proposed combination (dubbed LORL) use MONet or Slot Attention for this. By additionally back-propagating error signals from language-driven visual reasoning tasks obtained via NS-CL into MONet/Slot-Attention, it is shown how LORL is better able at learning object-centric representations and perform instance segmentation. \n\n### Pro’s / Con’s / Justification\n\nOverall the paper is well written and easy to follow. Moreover, using other modalities such as language to improve visual perception is an interesting research direction and timely due to recent advances in object-centric representation learning\n\nHowever, the significance of this contribution is rather limited due to two main reasons:\n\nLORL is a very straightforward combination of MONet/Slot-Attention with NS-CL and therefore not very novel --- the encoder part of NS-CL is simply replaced by the MONet/Slot-Attention, while everything else remains pretty much the same. Note that I don’t consider the “objectness scores” to modulate the NS-CL reasoning process very novel, since it is essentially a straight-forward heuristic to cope with a limitation of MONet/Slot-Attention in that sometimes the object-centric representations may containing background information or are “empty”. Were object-centric representations correctly inferred (as in the original NS-CL), this heuristic would therefore also not be needed \n\nThe results are not very surprising: it is found that better object-centric representations can be learned by fine-tuning on a visual reasoning task that uses language. However, since this task is learned in a supervised fashion and the dataset contains questions of the form “what is the name of the white object?” (parsed by a pre-trained semantic parser using a custom DSL), this provides a substantial degree of supervision to the representation learning part. In general, it is then not very surprising that using supervised data for fine-tuning improves representation learning, which limits the significance of this contribution further. \n\nDue to these reasons, I can only recommend rejection at this point in time. \n\n### Potential Improvements \n\nI don’t think that it is straightforward to improve the current submission, although I have some detailed comments that I would like to see addressed below.\n\nMore generally I would encourage the authors to see if visual reasoning _alone_ (i.e. running solely the fine-tuning stage, but without the perception loss) would cause object-centric representations to emerge. Firstly, in that case LORL would not have to rely so much on existing contributions from prior approaches like MONet/Slot-Attention (a perceptual loss for which we know it already yields reasonably good object-centric representations). More importantly, in this case it is less obvious that a supervised language-based approach would work, since there is no guaranteed separation into approximate object-representations due to using a perception loss. This would almost certainly make the contribution more significant.\n\nIt would also be interesting to analyze in what situations fine-tuning on VQA is helpful and whether there are certain types of questions that are particularly helpful in this case. I could image that questions focusing on individual object properties are more useful than those that focus on more global information (like the total number of ceramic objects). More generally it would be interesting to quantify the amount of supervision given to the representation learning and comparing this to the amount of improvement gained.  \n\n### Detailed comments\n\n* Regarding the objectness score an ablation is missing. What is the effect of this when learning object-centric representations? I expect it to be marginal, but it would still be good to demonstrate this. It would also be good to add the reasoning accuracy when the objectness modulation is not used to Table 4.\n\n* Similarly, is it correct that the baseline approach in Tables 1a and 1b includes the objectness score, and is thus used for the proceeding training stages to arrive at the results for LORL? If not then please add to these tables the performance of LORL after the perceptual training phase (and if yes, please add a baseline that does not include the objectness score as per my previous comment)\n\n* The standard deviation in Tables 1b and 2 is generally quite high, which makes it difficult to compare in some cases (although I agree that LORL generally outperforms the purely perceptual approach as is also expected). Therefore I would ask that you add additional seeds at least for Table 2 to allow for a better comparison.\n\n* The proposed metrics (GT Split and Pred Split) seem rather arbitrary and I don’t think provide much additional insight. The ARI score is consistently in favor of LORL, and although the magnitude of the difference when comparing is sometimes greater when using GT Split and Pred Split it is unclear to me how to interpret that magnitude. More generally, the choice of assigning a mask to an object if it claims at least 20% currently seems arbitrary, while this could be an important hyper-parameter for these losses. For example, what do the results look like when using a threshold of 10% or 30%?\n\n* I don’t find the results in Table 3 very surprising, since isn’t this what LORL is exactly trained for given the kind of questions used for the fine-tuning stage? What else could explain the improved ARI and the results in Table 4 other than that the representations have become better for semantic retrieval? Perhaps I am missing some alternative interpretations.\n\n* I would appreciate a comparison to NS-CT in Table 4, even though the latter is supervised. It would help understand how good the score obtained by LORL-SA is. \n\n* Section 5.4 essentially feels like an after-thought and details are missing. It would be helpful if the baseline IEP-Ref approach could be explained in some more detail. \n\n* Regarding the conclusion, I don’t see why LORL is a “principled framework”. What is the principle that is being applied here? And why is this desirable over other approaches?\n\n* I noticed in Appendix B that on PartNet-Chairs the second training phase (where only the reasoning part is trained) is skipped. What is the reason for this? What happens if this is also done for the Shop-VRB-Simple dataset? \n\n### Post Rebuttal\n\nI have read the other reviews and the rebuttal. I appreciate the extensive revision and response of the authors. Indeed, several of the minor issues/clarifications that I had raised have now been addressed.\n\nHowever, as noted in my initial review, my main concern with this work is the highly limited novelty and the significance of the findings:\n\n* LORL is essentially a simple application of unsupervised object-centric representation learners (like MONet, Slot-attention) to the language-guided visual reasoning framework proposed in MAO et al (2019), where the pre-trained vision module is interchanged. As I have previously argued, the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules, which is not very interesting. Indeed, this score is not needed for segmentation (which is the primary measure of success that the authors have adopted).\n* The main finding, which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance, is not very surprising. Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations, the considered type of questions (and DSL) provide a sizable amount of supervision I believe (as is also evident from the observed fluctuations in Table 9).\n\nOne issue that I noticed in the revision is when comparing the results in Table 9 and Table 10. It can be seen how when training on the visual reasoning task using only 25% of the provided data (i.e. 22.5K as opposed to 90K) actually reduces segmentation performance, i.e. from 83.51 (image only) to 81.01. This is surprising, and perhaps somewhat concerning, since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance. The authors do not provide an explanation for this behavior, while it appears to invalidate the main claim regarding the benefit of LORL in the general case.\n\nFor these reasons I remain in favor of a rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}