{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "All the reviewers agree that this paper was poorly written, which I agree upon my own reading of this paper. Section 1 is rather telegraphic and difficult to comprehend. Section 2 is cryptic in several respect, including what space of probability distributions the authors consider the Wasserstein distance, what QR task the objective function (5) for the discriminator corresponds to, especially after letting $a=+\\infty$ and $b=-\\infty$, and so on. The numerical experiment results do not seem convincing enough to demonstrate advantage of the proposal over existing methods. The authors did not respond to the reviews, so that many concerns raised by the reviewers have not been resolved. I would thus recommend rejection of this paper."
    },
    "Reviews": [
        {
            "title": "The quantile regression to GANs looks new, but not well demonstrated by either theoretical or empirical evidence",
            "review": "– Summary – \n\nThe paper proposes a new GAN method that applies the quantile regression of reinforcement learning into GAN and aims to show this helps to estimate the 1-Wasserstein distance better without gradient regularization. The idea of quantile regression presented in the paper is a way to match two distributions like WGAN-GP yet at a more grained level and need no regularization like WGAN-GP. The experiments are conducted on 2D-toy examples (Ring-8, Grid-25) as qualitative results and three other image datasets (CIFAR-10, LSUN-Bedroom, Cats) with FID scores. The proposed method is compared with some GANs baselines: SNGAN, LSGAN, and WGAN-GP. \n\n\n– Strength –\n\nS1 - The paper proposes a new idea to apply quantile regression into GANs.\n\n\n– Weakness –\n\nW1 - The paper is not well-written, and the paper representation is not good.\n\nW2 - The performance of the  proposed method does not look outperforming the WGAN-GP even though the paper strongly claims the robustness of this method. As shown in Fig. 4, 5, the proposed method converges faster, but is not necessarily better than WGAN-GP at the end. It looks WGAN-GP converges much more stable than the proposed method.\n\nW3 - It does not make sense why WGAN-GP is so bad on Cats dataset as shown in Fig. 6. It could be just the problem of parameters-tuning?\n\nW4 - It's unclear why Fig. 2 misses the WGAN-GP?\n\nW5 – The paper does not convince me why the proposed method is better than WGAN-GP in either theoretical and empirical results. In addition, the paper does not provide sufficient theoretical content to show the 1-Wasserstein distance is the same as minimizing quantile values as claimed.\n\nW6 - Many mathematical notions are not explained, e.g., What is $\\rho_{\\hat{\\tau}}$ in Eq. 4? How  do the authors implement with $a = \\infty$ and $b = -\\infty$?. How is $o_{i, \\tau}$ computed?\n\nW7 - FID scores alone may be biased, the combination with IS is required in the experiments.\n\nW8 – The experimental results are not sufficient, e.g., the results are with only standard DCGAN architecture, and the paper would need more ablation studies on some selected hyper-parameters, e.g., $a, b, N, k$ ... in the method.\n\nOverall, I think the paper is far to meet the conference's standard, e.g., at paper presentation, strong empirical or theoretical evidence to justify the claims. It also would need substantial revision to improve in writing. I tend to reject the paper.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Although the proposed method is interesting, there are many errors and shortcomings in the paper.",
            "review": "The authors propose the Quantile Regression GAN (QRGAN) to minimize the 1-Wasserstein distance between the real and generated data distributions. The proposed method avoids the mode collapse problem and obtains an improvement in the FID score compared to some existing GANs.\n\n-  Pros:\n  - Compared with NSGAN and LSGAN, the proposed method avoids mode collapse and achieves better performance than them in the FID score.\n  - In addition, compared to WGAN-GP, it achieves better FID with less training iterations while maintaining comparable performance.\n- Cons:\n  - Overall, there are so many typos and grammatical errors in this paper that they make it difficult to understand the content of the paper. The authors must look for these errors and correct them.\n  - Also, the way of citing figures and tables is inappropriate. For example, Fig. 1 is not cited in the main text, and figures and tables in appendixes such as Fig. 7 and Table 2 are cited without specifying that they are in appendixes. Reviewers don't need to read the appendixes, so the content should be complete in the main text.\n  - The authors state that the relationship between quantile regression and 1-Wasserstein distance is shown in section 2.1, but this is not explicitly shown. In particular, the authors state in section 2.1 that \"Here, minimizing 1-Wasserstein distance is same to minimizing distance between quantile values\", but Eq.1 and Eq.2 are simply p-Wasserstein distance and 1-Wasserstein distance, so it is unclear which equation represents the relationship. Also, the period in Eq. 3 should be a dot (multiplication).\n  - In Eq. (4), you state that a and b are set to +∞ and -∞ respectively, but how were these infinities implemented in practice?\n  - Why is there no WGAN-GP result in Figure 2? My understanding is that QRGAN minimizes 1-Wasserstein like WGAN-GP, so the result is almost the same. And why didn't the authors include unrolled GAN and VEEGAN results for comparison, even though they performed the same experiments as these papers?\n  - If the authors claim that WGAN-GP is computationally expensive, they should show how much less expensive it is in QRGAN. QRGAN also requires the sum of multiple quantile values, so the more of them, the longer it should take to compute them. Also, as far as I read, there is no indication in the paper of how the number of quantile values was set up in the experiment.\n  - Looking at Figure 4 and Figure 5, GRGAN appears to be less stable than WGAN-GP. Why is this?\n  - In section 3.2, the authors should show the image actually generated by GANs.\n\n- Minor comments:\n  - It is difficult to read because the author's citation is not enclosed in parentheses.\n  - Some parts of the random variables are in bold type and others are not. These notations should be consistent.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Having the discriminator output a whole distribution. Nice idea, average paper",
            "review": "# General statements\nthe core contribution of this paper is to train GAN by designing the discriminator so that it outputs a whole distribution instead of a point estimate for \"realism\". This distribution is instantiated through its quantiles, and the whole approach is thus framed in a quantile-regression framework. The nice feature is that using losses over quantiles means using a wasserstein distance, which has strong properties in a GAN setting.\n\nThe idea is interesting and is definitely worth investigating. I am not 100% sure it was not presented previously. But I trust the authors on this.\n* All in all, the paper is average in terms of english usage: totally acceptable in the beginning, there is a strong degradation for the experimental section, that has been either written in haste, or by another author than the rest.\n* The actual performance is rather disappointing, since the authors do not clearly manage to demonstrate any superiority of their proposed approach vs author (classical) methods, except on toy data. I don't see this as a real issue though, because the fact that the paper is inspiring is what I believe is most important.\n* The quantile regression loss is not presented in a way that allows people not knowing it already to understand the paper. This must be changed.\n* you didn't study the impact of the number of quantiles, although it looks like something you definitely should have done, since it's the core contribution of the paper.\n\n\n# Detailed comments\nBelow are remarks and typos found along the way:\n\n## Abstract\n* \"And we found that he discriminator should not be bounded to specific numbers.\" is unclear here.\n\n## Introduction\n * \"Text or structured data [...] of the world \" awkward\n * \"p(z|x), not p(z)\" awkward\n * \"Mode collapse is caused by unstable training and improper loss function\" : reference ?\n * \"propose to use mean square error (MSE) which does not saturate\": at this stage, you didn't introduce this concept of \"saturation\". And you don't tell where the MSE is applied\n * \"results better models\" : results in better models ?\n * \"Reinforcement Learning (RL):is to learn\": awkward. And we don't understand why you're mentioning RL at this stage. Above, you only introduced VAE and GAN for your purpose. This paragraph comprises no reference.\n * \"and gradually reach to the optimal policy.\": typo\n * The reason why you introduce RL becomes clearer after you introduced QR-DQN. I think that it's however a bit weird as it is framed currently. You should mention that he inspiration and motivation of your work originates from RL and some of its recent developments. \n * \"Discriminators whosetarget is specified\": what do you mean ?\n * \"mixture of gussian\"\n \n## Quantile Regression GAN\n * one really needs to know the trick already to understand equation (3). You must provide a reference here and explain the relationship between quantile regression and (3) as a loss.\n * \"D_{\\tau(batch)}\" instead of \"D_\\tau(batch)\" ? above equation (4)\n* it reads rather uncommon to me to write that infinity (negative or positive) is the objective of the discriminator, with a regularization that constraints its magnitude. I suspect the reason for this to work is: you don't actually provoke some strong collapse of the discriminator output to a specific value (a or b), but rather enforce that it stays somewhere in the approximative range [-k/2M k/2M]. This looks like a nice trick. But I would have appreciated some discussion about it.\n* Is there a reason why you picked 1-wasserstein rather than 2- or p-wasserstein ?\n* We replace Dτ(xreal) by ∞ to prevent it updating to decrease the discriminator output\": awkward sentence. \n* Actually, I don't really understand (7). What is \"x_real\" in the setting of trainng the generator ? You just have fake samples at this stage, and you're indeed using your discriminator for computing your los. I would have written min |a-D(x_fake)|. \n* maybe I'm missing something, but in Alg. 1, I don't clearly understand the difference between your notation o_{i,\\tau} and D_\\thau(x_i). Aren't them the same ? I understand that in your definition of D, you average over the batch. but here in this algorithm box you use a notation D(x_i), which makes it identical with o_i as far as I understand.\n\n## Experiments and results\n\n### toy\n\n* \"arranged in grid (5x5)\": the `x` doesn't render well.\n* \"by normalized computed gradients by generator loss\": awkward\n* \" ourput spaces\"\n* \"steep slope appears\": slopes appear. \"very gentle slope appear\": gentle slopes appear ? \"gentle\" reads awkaward to me.\n* This whole paragraph is extremely badly written and must be written completely, from \"As we can see\" to \"less affected by noise\". The english there is very bad, I don't understand what happened out of a sudden.\n* I don't understand what is depicted for (d) and (e) in figure 3: since the output of your discriminator is a whole distribution, what is it exactly that you decided to plot ? Did you pick a specific quantile ? \n* \"Instead, we can model a discriminator to predict distance. If discriminator predicts distance, itshould be less affected by noise.\" what should I understand here ? I am sorry but I really don't understand the discussion here. are you eventually discarding your model and changing it for something else that would predict a \"distance\", whatever it means ?\n\n### image\n\n* \"the checkerboard artifacts is\" \n* how many quantiles are you using ?\n* Inspecting your results on figures 4-6, I'd say they don't look particularly favorable. i/ For CIFAR10, they look kind of similar with NSGAN and LSGAN, and eventually WGAN-GP gets better. ii/ same result for LSUN, although the proposed method looks better than NSGAN and LSGAN at the end, after much instability. still catched up by WGAN-GP eventually. iii/  for cats, your method looks totally similar to NSGAN and LSGAN, and there was apparently some problem in the finetuning of WGAN-GP, that just didn't train for some reason you should have investigated. It really doesn't look like what happend for it with the other datasets. table 1 hence should not be taken too seriously, unless you really can tell that this WGAN-GP could not be made better on this \"cats\" dataset.\n\n## Acknowledgments: should that be part of a double blind review ?\n\n## References:\n* are not consistent. Sometimes full names, sometimes just initials. please make consistent.  \n\n## Apendixes\nI am not sure appendix B is necessary\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising Idea, but experiments need more work ",
            "review": "Summary. The paper proposes to use quantile regression as an alternative GAN loss. The idea is to evaluate the quantiles of discriminator outputs instead of just using a single scalar discriminator value: while all quantile discriminator outputs for real/fake data is pushed to +infty/-infty, the generator's cost function tries to push the quantiles of fake data to be as realistic as possible (+infty). As the regression loss leads to runaway values, a L2 regularizer is put on the discrimantor output quantiles. Experimental results show very good mode coverage on Gaussian Mixtures. Further experiments demonstrate it's in the ballpark of WGAN-GP on CIFAR-10, best on CATs, and second on LSUN-Bedroom.\n\nReasons for score: While the paper presents a reasonable and (to me) novel idea in GANs, the experiments fall short in comparing to the state of the art, and also the ingredients in the method are not sufficiently dissected to attain a sufficient understanding. Hence, I suggest to reject the paper in its current form.\n\nPro.\n- the paper proposes a simple and novel target function for GAN training, that makes intuitively sense\n- the mode coverage on the toy Mixture of Gaussian experiments look very solid\n\n\nCon.\n- experiments:\n   - on CIFAR-10 etc. lack comparison to state-of-the art (sota) methods, which is necesary to put the work in context. E.g. it should be added SN-GAN (Miyato et al.), StyleGAN(2).\n   - similar for the Mixture of Gaussian: comparison to other standard methods in the literature that tackled this problem are missing (e.g. Unrolled GANs). \n   - also, please check again WGAN-GP on cats - why is WGAN-GP seemingly not training at all (and starting at a much higher level from the start in Figure 6)?\n- the regularizer in equation 5 pushes both discriminator for real and fake towards the same values - hence potentially counteracting stability issues in training. This should be investigated independently to understand the effect of this regularizer in isolation (can even be formulated also for standard GAN KL losses, e.g. by pushing the average to 0.5). Otherwise it remains unclear if the benefits of the methods are attributable to the quantile regression or this regularizer\n- the exposition should be improved: \n  - pg 2: too much stuff on RL - this is not needed in the paper and should be shortened to a minimum\n  - the method could be written to be understandable more easily (e.g. give a high-level description of the intuition similar to my Summary above before diving deep into the formulae)\n\n\nRebuttal:\n- please address the points on the Con side.\n\n\nMinor issues:\n- the relation to standard divergence minimization remains unclear; i.e. in particular it remains theortically unclear if this really converges to the target distribution (the empirical results seem encouraging though)\n- many typos and language needs improvement - please check the document carefully again\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for QRGAN",
            "review": "1. This paper contains many typos, grammar mistakes, and format problems, which make it very hard to read. For most equations, there is no ending period. The citation format seems wrong.\n\n2. The equations (1) and (2) are for 1-dimensional random variables. They may not be suitable for high dimensional random variables. The $N$ quantile values defined in this paper are for which random variable?\n\n3. For the neural network for $D_\\tau$, does it mean the input is $(\\tau, X)$? So for different $\\tau$, they share the same weights. What is the meaning of this output?\n\n4. The \"$+\\infty$\" and \"$-\\infty$\" notation is confusing. Does this mean we choose a very big value or a very small value in practice? But this is very subjective now. Did you perform some sensitive analysis on the choices of $a$ and $b$?\n\n5. The authors claim that the WGAN training is slow. For WGAN-GP, I don't see why the training is much slower than the training of QRGAN. Did you perform some analysis on the training time?\n\n6. The arguments for QRGAN  to overcome mode collapse is quite vague. Many GANs with the encoder structure can solve the mode collapse well. It may benefit to compare  QRGAN with these methods.\n\n7. What is the implication of Table 1? Does that mean WGAN-GP is better than QRGAN? The generative images are not demonstrated. Other dataset such as CelebA can be applied to check the performance. Image interpolation can also be demonstrated for mode collapse situation.\n\n8. The Appendix B is completely not necessary. It contains only well known results.\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}