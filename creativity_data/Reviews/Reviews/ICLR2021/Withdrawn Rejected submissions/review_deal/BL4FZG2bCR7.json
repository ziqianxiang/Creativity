{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting problem-specific architecture, but needs more experiments",
            "review": "**Summary**\nThe work proposes a novel curated dataset and a model for alternatively spliced transcript abundance prediction. The key observation that motivates the model's architecture is that the probability of producing a given transcript can be composed from the probabilities / energies of producing each of its exons. The resulting model - DCEN - outperforms several baselines.\n\n**Score justification**\nWhile I really like the simple and clear idea behind the DCEN model, I have two major issues with the paper: (i) I am not certain that the underlying machine learning is sufficiently interesting for the machine learning community; and (ii) I feel that the experiments presented in the paper do not demonstrate that the model approaches the aspirational goal the authors set out in the introduction - *in silico* study of alternatively spliced transcripts.\n\n**Major comments**\n* Continuing on the above, the paper would be more convincing if it demonstrated the usefulness of the DCEN model for the study of alternative splicing. For example, can the model predict how alternative splicing would change in response to mutations? Does it generalize across tissues / disease conditions (e.g. when some RBPs are deregulated)? Are the motifs captured by the $f_{nt}$ network informative?\n\n* The authors mention the importance of the null state (no transcript produced) and the importance of learning the null state energy. It would be interesting to see an ablation study removing the parameter, as well as a discussion of why it is important. Does this parameter map to a biological mechanism (e.g. are the cases when pre-mRNA is transcribed and then degraded during splicing)?\n\n* The authors train their DCEN model with a combination of an $L_2$ regression loss on probabilities and a nucleotide classification loss. However, as far as I can tell, the relative importance of these losses (or the values of $\\lambda_{reg}$ and $\\lambda_{cls}$ in Algorithm 1) are not discussed/studied. Including an ablation study of these losses would help to support the following statement from the manuscript \"This helps the DCEN learn features on the gene primary sequence that are important for RNA splicing\".\n\n* Why did the authors choose to use an $L_2$ loss between the observed and predicted nucleotide probabilities? Isn't cross-entropy more suited when working with probabilities?\n\n**Minor comments**\n* In introduction: \"in-silico\" -> \"*in silico*\" + make it italic\n* In introduction: \"this is the intuitive\" -> \"this is the intuition\"\n* Algorithm 1 doesn't not provide a lot of additional detail beyond what's described in the main text and could be moved to the supplement. Similarly, the details of the construction of the CAPD dataset could be moved to the supplement.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A promising framework for the RNA splicing prediction task but many elements are still missing for completing this work",
            "review": "Summary:\n \nIn this work, the authors develop a regression model called DCEN based on energy-based models (EBM) for splice site classification and alternative splicing prediction from pre-mRNA transcript sequences and additional splicing regulatory features (aka expression levels of RBPs). The authors claim that the EBM approach improves upon existing algorithms by modeling a mRNA transcript as a combination of the energy levels of each splice junction which reflects real biological dynamics. The main novel modeling contribution is the representation of splicing along different isoforms as a set of splice site choices to allow whole isoforms splicing prediction, in contrast to previous works which focus on individual splicing events based on exon/junction inclusion. In addition, similar to DARTS, the model also includes additional regulatory information (RBP levels) to improve predictive power. To evaluate the performance of their model, the authors create the CAPD dataset which contains annotated pairs of sequence/regulatory features and exon inclusion levels which they derive from the ARCHS4 database. This dataset consists of 14 different tissues with 250 samples per tissue. Using this data for training and validation, DCEN is compared to several derivatives of Splice-AI, a current state of the art method, as well as several simplified variations of DCEN on the task of predicting exon inclusion levels. The authors show that their model outperforms the baselines on several metrics when considering short genes (<100k nt) but also performs favorably on long gene sequences.\n\n\nPros:\n1.     The conceptual framework is innovative because it uses a combination of sequence features and regulatory information to predict splicing at the resolution of the relative abundance of whole isoforms. Previous models generally lacked this integration of input features and could only predict discrete splicing outcomes or were limited to splicing events/exon inclusion.\n\n2.     The inclusion of the feature extraction step and generation of the latent embedding of the sequence features allows for the use of sequence context information. For example, motif sequences near a splice site will likely be captured by the embedding.\n\n3.     Overall, the paper is generally clear and well written. The background information covered is fairly exhaustive.\n \nCons:\n\n1.     The construction of the CAPD dataset is predicated on quantifying the abundance of each transcript of a given gene to determine the relative inclusion level of an exon start/end site. However, given that the source data is short read RNAseq, these quantifications cannot be trusted: a) transcript reconstruction from RNA-Seq is conceptually challenging (see: https://www.nature.com/articles/nmeth.2714)  and b) reads that are mapped to constitutive exons can be ambiguously assigned to each transcript.\n\n2.     The authors fail to show how the EBM aspect of the model improves performance or improves model interpretability. It is not clear whether the effectiveness of EMB is due to its ability to model real splicing dynamics or if the model is simply learning the strength of individual splice sites and aggregating their relative contributions. The task of distinguishing between alternative and constitutive exons is trivial and no information about other capabilities (e.g. tissue specificity) is shown though claimed. Furthermore, no additional analysis is conducted to show how the EBM model can facilitate interpretation (compared to Splice-AI’s Resnet model for example) and thus provide further utility. \n\n3. The evaluation criteria is not clearly defined, authors specify that their Pearson R and Spearman R are better but it is never specified if these are for individual junctions or transcript level PSI levels. This omission is significant since their primary claim is prediction at transcript level.  \n\n4. While the authors do a good job describing many related works, they fail to mention any work from the Gagneur group which is highly relevant (MMSpice, MTSplice). They also misattribute previous contributions. Xiong et al 2015 used a classification framework (low, med, high) for PSI. Regression for PSI and incorporation of RBP information (KD/overexpression, CLIP binding) were first introduced in (Jha et al, 2017, Bioinformatics). \n\n5. Although the DCEN model is able to predict the relative abundance of gene isoforms, how does this scale to genes with hundreds of possible isoforms? Would the prediction accuracy be substantially impacted by the addition of more classes?\n\n6.     Although the authors show their model outperforms Splice-AI based on their selected metrics, this evaluation does not represent a fair comparison because Splice-AI was designed and optimized for a different task (splice site classification). Consequently, the variants of Splice-AI that the authors design for splicing prediction cannot be considered a suitable external baseline. It also does not appear that the authors evaluate DCEN’s performance against Splice-AI on this task despite DCEN having this capability through its classification loss. More appropriate models to compare against are mmSplice (Cheng et al, Genome Biology 2020) and DARTS (Zhang et al., Nature Methods Brief communications, 2019)    \n\n7.     The paper could benefit from additional analysis that showcases the advantages of DCEN’s innovative features. For example, a comparison of DCEN to a simplified model that does not use regulatory information.\n\nOverall, we appreciated the conceptual goals of the paper which aim to use mRNA sequence and splicing regulatory information to predict splicing outcomes at higher resolutions than previous works. However, while the approach looks promising we believe the paper is still in its incipient stages due to the lack of comparisons to suitable baselines and additional analysis on both model performance and interpretability, which was described as a main benefit of the EMB model. Furthermore, although we appreciate the creation of the CAPD dataset as a resource, we are suspicious of it due to the mismatch between the limitations of the sequencing technology and desired model input. Finally, while the model offers some modeling novelty compared to previous splicing models it does not seem to offer, to our understanding, novel ML modeling aspects, thus reducing its suitability for a general ML focused conference. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting take on alternative splicing but with major issues",
            "review": "Alternative splicing (AS) of genes is one of the most fascinating mechanisms in the eukaryotic cells, for example, in human cells, AS generates hundreds of thousands of proteins from fewer than 20,000 protein-coding genes. The proposed method is interesting and has some interesting ideas, but overall there are major issues that need to be addressed:\n\n1. AS events are more complex than exon inclusion/exclusion. The proposed method and the ground truth generation process assume independence between all splice sites, which is not necessarily through. Some AS events are complicated, e.g., mutually exclusive exons or compound cassette exons/alternative 5’/3’ splice sites. AS events are actually connected components in the junction graph (see the LeafCutter paper, PMID: 29229983). One other solution is to focus only on cassette exons and use the formulation used in SpliceAI and other AS papers.\n2. The authors are using 3,971 transcript levels from what they call “regulatory” factors, or $X_{reg}$. Are they *removing* all those genes/transcripts from the train, eval, and test sets? If not, the model can use $X_{reg}$ to cheat for those genes/transcripts.\n3. Is the model multi-task? Why are the authors using AS profiles from 14 different tissues when the rationale is not clear?\nWhy are the authors using samples from 250 different individuals? Are they using the same genomic input for all of them, i.e., ignoring the sample-specific genomic variants? If not, what is the point of having all these samples?\n4. It is not clear how performance metrics are calculated. Are the reported correlations computed across all samples x all tissues? If yes, why not show tissue-level metrics as well?\n5. Why is ARCHS4 used? Heterogeneous datasets suffer from significant batch effects, which are very challenging to control. The authors mention in A.3 that they are removing batch effects and outliers but enough details are not provided. Batch effects would have been much more controllable if the authors had used a dataset such as GTEx, similar to SpliceAI.\n6. It is not clear if this model can be applied to genomic variation analysis, one of the most important features of computational AS models. For example, SpliceAI can be used to identify variant-induced creation of cryptic splice sites or abrogation of existing splice sites.\n7. ARCHS4 is based on Ensemble 90 while junctions are retrieved from GenCode v26.\n8. AS events can be quantified much more accurately using junction-spanning reads rather than normalized transcript counts (e.g., TPMs).\n9. The authors may want to focus on a highly-curated set of AS events, such as those found in http://vastdb.crg.eu. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting model but insufficiently described and explored",
            "review": "__Summary__\nThey authors introduce a new model for RNA splicing that holistically considers all of the splice sites in various isoforms of the gene in an energy framework. I believe this approach is novel, and it reflects the underlying biology well. Using this modeling framework, the authors predict alternatively spliced exon psi inclusion values using a regression loss function.\n\n__Major comments__\n* I don't think I understand exactly what the authors consider to be a single training example. My best guess is that a tuple of one RNA-seq sample and one gene compose a single training example. The RNA-seq sample determines the RNA-binding protein input vector. For each gene, a variable number of exon junctions must be embedded to feed into the transcript energy model. Finally, the transcript energies are converted back to exon junction inclusion values, and the regression loss is computed versus the RNA-seq sample measurements. If I have this right, how did the authors handle the very large range of junction numbers across genes? If I don't have this right, could the authors please more specifically describe the training examples.\n\n* In the description of the authors’ curated CAPD dataset, they describe collecting 250 x N_T tissue-type specific samples. What does the number 250 refer to? The authors state that N_T “is tissue type”. Is N_T a number or a categorical variable representing tissue? How do the 84,863 RNA-seq samples fit into this preprocessing? This may be related to the comment above.\n\n* Model interpretation is mentioned, but not demonstrated. Prior work in this field has described a variety of approaches that the authors could make use of. I would also be interested in interpretation of the RBP abundance input vectors; are any known tissue-specific functions learned by the model?\n\n* How do the test results in Tables 2 and 4 differ? Table 2 is titled “Performance of DCEN and baselines on withheld test samples”, while Table 4 is titled “Performance of DCEN and baselines on Test-Chr, test samples from chromosomes (1, 3, 5, 7, and 9) different from the training set”. As far as I can tell, the authors described one test set, consisting of these held-out chromosomes.\n\n__Minor comments__\n* I don't understand the contribution of theorem 5.1.\n* The authors might consider computing their regression directly on the transcript counts, rather than the alternatively spliced exon psi values. The counts more faithfully acknowledge that some genes have greater abundance than others and offer more trustworthy exon inclusion information.\n* I may be confused by what exactly comprises a training example, but I found the authors observation that validation set accuracy plateaus after processing 25% of the training data startling. Wouldn't this suggest that the model is underfitting the training data?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}