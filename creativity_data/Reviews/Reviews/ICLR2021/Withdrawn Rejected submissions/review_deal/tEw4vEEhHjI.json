{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers agree that the proposed method for reducing overconfidence in ReLU networks is novel and interesting. However, the presentation of the theoretical results is too informal and imprecise to warrant acceptance without a strong accompanying experimental section, which is unfortunately lacking. I therefore cannot recommend acceptance of the paper in its current form."
    },
    "Reviews": [
        {
            "title": "Interesting but may be overcomplicated",
            "review": "Update during rebuttal: I have read other reviews and authors' answers to them. My main concern about the approach to be potentially overcomplicated has been address and I was convinced. I am raising the score for the paper. \n/============================================================================================================ \n\nThe paper addresses the issue of overconfidence of neural network outputs. Pointing to the recent results, the authors state that even Bayesian neural networks (BNNs) are overconfident in their predictions far away from the training data. They propose a method how to fix that. The method is de-facto a simple post-hoc procedure but it is backed up with theory and theoretical guarantees. \n\nStong points:\n* Interesting idea of extending BNNs with GP to address uncertainty in the areas far away from training data\n* The idea is derived theoretically and theoretical guarantees are proved\n* The method is shown to work empirically in rather extensive experiments\n\nWeak points:\n* My biggest and basically only concern about the proposed method is whether it is too complicated. Why would one need to derive GP via these infinite ReLU feature maps? What stops just to declare the usage of cubic spline kernel? This indeed allows the authors to beautifully connect finite BNN with infinite GP with the statement like the last one in the paper, but is there other motivation for that? \nMoreover, if the proposed addition to a pre-trained BNN does not depend on training data, how would it compare to something very rough and straightforward, like just adding to variance of the BNN a function that would cubically grow with the distance away from training data?    \nIn any case, I believe it would be interesting to add this kind of ablation study to empirical comparison. \n\nI am voting to weak acceptance but willing to upgrade the score if the authors provide convincing evidence about necessity of all components of the proposed method.\n\nI am voting to accept because despite of the mentioned above concerns of the method to be overcomplicated, the overall idea is interesting, the paper is well-written and easy to follow, the paper addresses the important problem that has impact for the wide audience, the proposed method provides theoretical guarantees while in practice being used as a simple post-hoc procedure.\n\nQuestions to authors:\nCould you please clarify why you need deriving GP via infinite ReLU maps and why the proposed method would be better than the rough fix to the problem of overconfidence far away from the training data described above?\n\nAdditional comments/suggestions (not too important for evaluation but might be used to improve the paper):\n1.\tReLU is not defined\n2.\tReLU features are only introduced in Section 2.2 although used in Introduction\n3.\tFirst paragraph in Section 2.1. For the full picture, some of the definitions are missing, for example, x, y, N, D, and C\n4.\tMaybe to avoid using expression: “Bayesian methods, which turn standard networks into Bayesian neural networks”\n5.\tSection 3.1. What about zeros? Two kernels covers (-\\infty, 0) and (0, \\infty) where 0 is excluded in both\n6.\tDSCS abbreviation in caption to figure 2 appears before its definition as reference to figure 2 is before this definition\n7.\tFigure 3 is unclear. The authors should probably elaborate the caption.\n8.\tWhat is n in second paragraph in Section 6.1?\n9.\tFigure 4.a – does this mean that all 3 lines coincide? Or where the other 2 lines? If the former, at least a note on this should be included, or better something like using dashed lines and different line widths can make it visible on the plot\n\nMinor:\n1.\tSecond line in Section 4. “(iii)” -> “(ii)”\n2.\tLast line in Section 6.2. “In shown in Table 2” - > “As shown in Table 2”\n ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Initial review - updated",
            "review": "**SUMMARY**\nThe authors consider the issue of overconfidence in ReLU NN and BNNs, particularly for data that are far (in Euclidean distance) from the training data. They address this by modeling the residual (to the NN) in the latent space with a GP. The kernel for this GP is derived as the limit of infinitely many ReLU-based random features. Specifically, this kernel has the property that it scales cubically with the norm of the input, and so causes large uncertainty away from the origin. Crucially, the GP term changes little from its prior distribution after conditioning on the data, so no expensive inference is required under the approximation made.\n\n**PROS**\nThe authors’ proposal seems to improve the overconfidence of existing methods on the tasks they study. While there is a lot of technical motivation, the paper is easy to follow and results in a simple and efficient modification to existing methods. \n\n**CONS**\nThe main weak point of the paper is the experimental section. The OOD detection task of distinguishing a test set from uniform noise, while perhaps an interesting proof of principle, does not seem very relevant in practice. The results from Table 2 are mixed when compared to the baseline LLL in tasks with real OOD data (as opposed to noise).\n\nSome of the theoretical results and proofs are quite informal or vague. For example in Proposition 1: “are close enough to the origin” \n\n**RECOMMENDATION**\nIn its current form, I vote for rejecting the paper. First, while the theoretical motivation for the method is nice, many approximations are used to produce such a simple form and more analysis is needed to establish what in particular is advantageous about the DSCS kernel over other kernels. Second, the experimental results are quite restricted and should be expanded. While seeing what the method adds to some baselines is very reasonable, it would be useful to have some discussion comparing results to the current SotA. \n\n**ADDITIONAL QUESTIONS**\nThe significance of the kernel derived from the limit of ReLU features is unclear to me. The key properties seem to be that the kernel is small when one argument is near the origin and that it is \\omega(\\alpha^2). Would another kernel with these properties work just as well?\n\nWhat is the baseline performance (in terms of accuracy or likelihood) for the methods considered in Table 1?\n\n**MINOR COMMENTS**\nAs there may be images that are very far apart but are nevertheless quite similar, it’s unclear why Euclidean distance in the input space is important. More motivation for this would be helpful. \n\n- In eq (1), f_\\mu is not defined. Should this be just \\mu?\n- In section 2.2 both d and D are used.\n- Section 3.1: “which is one-zero only on (0, \\inf)” Which argument does this refer to?\n- “the value of k(x*,x*) is cubic in x*”: What does this mean? x* is a vector.\n- The notation f ~ GP(f | mu, V) is unnecessary. f ~ GP(mu, V) is sufficient.\n- “Under this assumption, give an input x* it is clear…”: It does affect the output of the BNN due to the link function.\n- Figure 3d is reference, but does not exist.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea for covariance correction to improve UQ",
            "review": "Working under the Bayesian Neural Network setting, the authors proposed a way to inflate the resulting posterior covariance, so as to get improved predictive uncertainty quantification for new data points that are far from the training set, while also maintain similar performance when these new data are close to the training set.\n\nOverall I think the writing is clear, but I had to revisit previous sections in order to tie-up and understand all the various approximation schemes used. I find the derivation of the double-sided cubic spline and the use of infinitely many ReLU's to boost the posterior covariance novel and interesting. This method has potential to be applied to other learning algorithms and get improved confidence statement, such as Variational Bayes where it is known to produce overconfident output.\n\nDespite the authors' claim of doing an extensive theoretical analysis, I find that theoretical arguments quite heuristic in some places. It would be more informative to quantify the approximation errors incurred when using the various approximation methods discussed in this paper, in particular, network linearlization through Taylor's theorem. It seems to me that you just treating the neural network $f$ as a typical differentiable function and ignore the network structure within it by just doing $\\approx$.\n\nThe level-wise RGPR covariance kernel in (6) does not seem to be correct. For level 1, you have \n$\\boldsymbol{h}^{(1)} (\\boldsymbol{x}_{*})$. \n\nFor level 2 however, $\\boldsymbol{h}^{(2)}$ is obtained by $g(W\\boldsymbol{h}^{(1)}+\\boldsymbol{c})$ where $W$ is the weight matrix at level 1, $\\boldsymbol{c}$ is the bias and $g$ is some activation function applied entry-wise, e.g., ReLU. Since the entries of $W$ is part of $\\boldsymbol{\\theta}$ the parameter for the entire network, $W$ is random because $\\boldsymbol{\\theta}$ is assigned a prior. This implies that $\\boldsymbol{h}^{(1)}$ and $\\boldsymbol{h}^{(2)}$ are dependent and likewise for higher levels. Hence the covariance kernel of $\\hat{f}=\\hat{f}^{(0)}+\\cdots+\\hat{f}^{(L-1)}$ is not the just them sum of the individual kernels but something more complicated because they are now dependent due to $\\boldsymbol{h}_{*}$. Can the authors please clarify this?\n\nI find the addition of a kernel function to do correction quite ad-hoc and not very Bayesian. Is it possible to incorporate this term into the prior?\n\nSome other comments:\n1. Section 2.2, line 6, $c_d$ should be $c_D$. Also, is this the same $D$ for the dimension of $\\boldsymbol{\\theta}$ the network weights? Then taking $D\\to\\infty$ means you have infinite weights?\n\n2. (3) does not seem to cover $0$\n\n3. In Proposition 1, $\\boldsymbol{\\mu}$ and $\\mathrm{\\Sigma}$ are the mean and covariance of the approximate posterior of $\\boldsymbol{\\theta}$, and for $\\boldsymbol{g}_{*}$, $\\boldsymbol{0}$ should be $\\boldsymbol{\\mu}$\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}