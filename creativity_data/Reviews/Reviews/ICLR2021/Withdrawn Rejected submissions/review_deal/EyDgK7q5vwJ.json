{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a method for unrolling the iterative expectation-maximizing steps in the EM algorithm for a Gaussian mixture model into layers in a neural network. Then, the proposed method is applied in the latent space of an autoencoder to allow deep clustering using autoencoder features. The reviewers raised concerns regarding the novelty, lack of systematic ablation experiments, and the efficacy of mimicking iterative optimization steps. Moreover, the training objective does not *exactly* correspond to the variational lower bound that EM often follows. \n\nThe additional experimental results in the revised version address some of the concerns regarding the empirical studies. However, the relationship between the proposed method and incremental EM algorithms and top-down generative models is not properly discussed. Moreover, backpropagating through unrolled iterations does not seem elegant compared to principled approaches such as variational/amortized inference that are commonly used. For example, increasing the number of unrolled steps is observed to hurt the performance due to \"vanishing gradients\" whereas in the EM algorithm increasing the quality of optimization in the M steps should provide a better fit to training data. \n\nGiven the current presentation, I believe that the paper is not ready to be presented at ICLR. However, I encourage the authors to take the review feedback into consideration to improve the paper."
    },
    "Reviews": [
        {
            "title": "Streamline EM into a differentiable network. Good idea. ",
            "review": "This paper proposes a deep clustering method called EDGaM. Clustering algorithms often suffer from cluster collapse or sample-specific details overestimation. To balance both challenges, the authors propose a differentiable GMM network in the latent space between encoder and decoder. The network is designed with the scheme of skip connection and several loss terms are applied.\n\nPros:\n+ The idea of streamlining EM into a differentiable network to learn its parameters by backpropagation is interesting. \n+ Writing is clear.\n\nCons:\n- Does the assumption of setting covariance $\\Sigma_k$ as $I$ and weight $\\pi_k$ as uniform for simplicity too strong? Is there a way to update these two parameters?\n- There seems to lack an ablation study to individually illustrate the function of each loss term in Eq. (9).\n- The authors mentioned three types of deep clustering approaches in related work. But it seems the authors only compare with the second type which they follow in their experiments. I wonder if the results can be better or comparable with the other two types?\n\nOverall I think it's a good paper but maybe more experiments need to be done to make it comprehensive.\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nAfter reading the rebuttal, I think the authors have well addressed my concern, thus this paper is good to be accepted for ICLR and I raise my rating from 6 to 7.  ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A useful and efficient method for deep clustering ",
            "review": "This paper consists of two components. One is to unroll the EM algorithm for fitting a mixture of Gaussian model into a differentiable multi-layer network. The other is to insert this unrolled network between the encoder and decoder in the latent space. The whole network can be trained end-to-end for the purpose of clustering. Experiments show that the learned network achieves strong performances on clustering tasks. \n\nI find the proposed method sound, useful, and efficient. I like Figure 2, which illustrates the whole scheme very well. \n\nMy main concern is that this paper does not break new ground conceptually. Both the idea of unrolling an iterative algorithm and the idea of using auto-encoder for deep clustering are well known. In fact, I have some reservations about unrolling an inference or learning algorithm and embedding it into a bigger network so that it can be trained by an overall loss. There is no doubt that it is convenient and useful, and it is in general consistent with learned computation, but it is conceptually less elegant than a top-down generative model with learned amortized inference. A top-down model may also enable us to determine the number of clusters in a principled way. \n\nOne may interpret the residual network and transformer network etc as unrolled iterative updating algorithms. But they do not assume any top-down model. In the case of a given top-down model such as mixture of Gaussian, embedding the model fitting algorithm into a bigger network appears less than compelling conceptually. \n\nIncidentally, the unrolled EM-like update appeared in an early version of capsule net. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice approach, additional background information on the iterative EM algorithm is required.",
            "review": "\nThe most common approach for deep clustering is to train a deep auto encoder (DAE) that is used to find latent features which are utilised for clustering with a classic clustering approach such as the K-means. The K-means is not differentiable and therefore the clustering was not part of the network. This caused alternative update of the parameters. \n\nIn this work the same pipeline is used yet, the clustering is carried out by EM-GMM algorithm. It is a nice idea since it is known that the K-means is a special case of the EM-GMM algorithm which is softer and not hard-decisioned like the K-means. The authors present an iterative EM algorithm which is part of the training procedure. \n\nHigh clustering results are presented on 4 different datasets.  The proposed skip connection was also found beneficial.\n\nIt seems like the authors are not familiar with the recursive EM algorithm proposed by Titterington [1]. In addition, in [2] it is presented that the recursive EM algorithm  is almost identical to the training (Feed-forward and back-propagation) of the mixture-of-deep-experts’ architecture.  It is interesting to see the link of the proposed algorithm to these methods.\n\nExperiments:\nIt is common to use also the adjusted rand index (ARI) as an objective for the clustering. Interesting to see the performance of the proposed data also on this measurement. \nAll datasets in the experiment are images. What is the performance on different kind of datasets such as texts ?\n\nThere is a repetition (almost identically paragraphs) between the introduction and the related work section. \n\n[1] Titterington, D.M.: Recursive parameter estimation using incomplete data. J. Roy. Statist. Soc. Ser. B 46, 257–267 (1984)\n\n[2] Chazan, S.E., Gannot, S. and Goldberger, J., 2018, July. Training strategies for deep latent models and applications to speech presence probability estimation. In International Conference on Latent Variable Analysis and Signal Separation (pp. 319-328). Springer, Cham.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Deep clustering with streaming EM",
            "review": "This paper proposed a streaming EM algorithm to learn latent representations and cluster in the latent space simultaneously, through a deep GMM model. The EM algorithm was unfolded into network layers as the forward pass, and the global parameters (GMM, encoder, decoder) was optimized through the backward propagation. Another novelty of this work is using both the original latent representation and its GMM reconstruction as the decoder input. Experimental results were provided to evaluate the proposed algorithm. The paper was written with clarity and insightful remarks.\n\nHere are some detailed comments on the technical aspects of the paper:\n1) Some readers would ask, is it possible to directly optimize the GMM log-likelihood function in equation (1), together with the reconstruction loss and some entropy regularization? This way the EM algorithm can be avoided in the first place. I think some explanation is needed here to motivate the (streaming) EM algorithm.\n\n2) I assume that N in equation (7) stands for the mini-batch size, and in the denominators the summation over \\lambda to get N_k is over the mini-batch instead of the full dataset. It would be nice to state that explicitly in Section 3, to reflect the streaming nature of the algorithm. \n\n3) The proposed algorithm assumes that \\pi is uniform and covariance matrix is identity matrix for all clusters. Hence this is more of a deep K-means model than a full GMM model. More details for updating the covariance matrix and cluster weight are needed if the authors would want to extend it to a deep GMM model.\n\n4) It is a bit unclear to me the approximate equality part (i) in equation (7), especially the re-weighting with 1/N_k. More derivation or explanation would be helpful to explain and justify that step.\n\n5) In Figure 3 we see that more EM layers actually hurt the performance. This is in contrast with the original EM algorithm, which will increase the log-likelihood function monotonically with more EM iterations,  until convergence to a local optima. It is not very clear to me why using more EM layers (T>=7) hurts the performance, and why T=3 seems to be the best. Probably more analysis and explanation are needed here, in addition to the discussions in Section 5.1.\n\n6) In addition to the entropy regularization on individual \\lambda_n in equation (9), entropy regularization on \\lambda averaged over the mini-batch is also used in other work (e.g., IMSAT). The regularization on the averaged \\lambda could drive the solution away from degenerated solution, where all samples are clustered into one or a few clusters. I wonder whether the same technique can also benefit this work.\n\n7) There are quite a few deep clustering algorithms proposed in recent years. It would be nice if the authors can compare with them as well. Here are a few examples:       Ji, Xu, João F. Henriques, and Andrea Vedaldi. \"Invariant information clustering for unsupervised image classification and segmentation.\" In Proceedings of the IEEE International Conference on Computer Vision, pp. 9865-9874. 2019.      Gupta, Divam, Ramachandran Ramjee, Nipun Kwatra, and Muthian Sivathanu. \"Unsupervised Clustering using Pseudo-semi-supervised Learning.\" In International Conference on Learning Representations. 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}