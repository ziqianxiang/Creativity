{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nThis paper presents \"Automunge\" a python library for pre-processing tabular data. \nThe authors develop a useful library that can be used by practicioners for data engineering in NNs applications. \nThe reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the \nperformance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives. \nIn the response the authors have done a rather thorough job of addressing the reviewers comments and\nadding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don't think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer. \n"
    },
    "Reviews": [
        {
            "title": "Useful software but insufficient scientific contribution",
            "review": "This paper introduces a library that preprocesses tabular data called Automunge.\n\nFor software packages I feel that one of two criteria must be met: The software implements a scientifically novel algorithm, framework, model, etc.; or the software package is so complex that a well-designed implementation in itself is of scientific significance.\n\nWhereas Automunge seems like a useful library, I am not convinced that it falls in either of these two categories. As such, I am not sure it justifies a publication at a machine learning conference. I suggest the authors target a different venue (e.g., PyCon, SysML, etc.) or elaborate on the scientific impact of their software (e.g., show experimentally that this framework allows practitioners to train better performing models).\n\nPros\n\n* The library seems useful\n\nCons\n\n* No significant novelty\n* Little relevance to the scientific machine learning community\n* Not very clearly written",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very nice featurization library --- not a topic for ICLR",
            "review": "The submitted paper describes a very nice featurization library, AutoMunge, that converts NLP into features suitable for NNs.\n\nIt's clear that the authors of the library have put a lot of thought into its construction, and it looks very useful.\n\nHowever, ICLR is about /learning/ representations, not about feature engineering. So this is off-topic for the conference. To make it on-topic, the authors could, e.g., compare using standard word representation techniques vs AutoMunge on a set of NLP tasks using some popular modern NLP architecture (perhaps BERT?). That would be a really interesting paper.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors present a python package to process data in the form of strings.",
            "review": "This package aims at automating some repetitive tasks that data analysts (especially within the NLP domain) deal with. I confirm that tasks like feature selection and string encoding are beneficial for applied researchers with textual data. I can see a wide interest in the community.\nOn the negative side, the paper lacks a literature review and comparison with any existing and relevant work. I expected to see performance plots for different tasks (whether just for this package or with comparison to some baseline alternatives.\n\nAs a minor comment, I don't think having \"String theory\" in the title is a good idea because this keyword is already taken to refer to another scientific topic ( a sub-field of physics).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}