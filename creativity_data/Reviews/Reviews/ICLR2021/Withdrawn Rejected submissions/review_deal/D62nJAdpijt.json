{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents a new attack combining trojans (backdoor attacks) with adversarial examples. The new attack is triggered only if both a trojan and the respective adversarial perturbation are present. Experimental evaluation demonstrates that neither adversarial training (as a defense against adversarial examples) nor defenses against backdoors are effective against the new attack.\n\nThe proposed method is original albeit somewhat incremental (combination of two well-known attack techniques). The main weakness of the paper, however, is its threat model. It is not clearly explained why the proposed attack would make sense for an attacker. Backdoor attacks are typically executed by model creators in order to force certain decisions on certain data. On the other hand, adversarial examples are generated by model users (or abusers) who have an interest in wrong model predictions (e.g., decisions made in their favor). The paper does not provide a convincing use-case in which such combined attacks would be feasible. \n\nFurthermore, paper's clarity can be improved. The introduction does not present a clear picture of poisoning attacks. It essentially treats poisoning attacks as equivalent to backdoor/trojan attacks. This is not true and a substantial body of research (starting from the seminal paper by Barreno et al. in 2006) has addressed indiscriminate poisoning attacks aimed at general deterioration of classifier performance. A distinction between a clean-label and a poisoned-label attacks is also not clearly presented. The notation of Section 3 is rather complex and confusing.  \n"
    },
    "Reviews": [
        {
            "title": "Simple combination of two known attacks",
            "review": "This paper presents a new attack against neural networks that combine Adversarial inputs and trojans. The key idea is to train a trojaned network that the victim might believe to be adversarially robust but the trojan will be activated when a trigger and adversarial noise are both present in the input image.\n\nStrengths \n-------------\n\n- The attack presented by the authors beat most existing defenses\n\nWeaknesses\n-----------------\n- The contribution seems very incremental given that there exist a substantial number of works in both adversarial inputs and trojaning. The core training algorithm (Alg. 1) also seems very straightforward. \n\n- The threat model seems to be somewhat unrealistic. The adversarial inputs usually require norm-bounded global (all pixel) perturbations while inserting trojan triggers require local modifications to the input image. It is not clear to me what is achieved by putting these two different types of attacker models together that cannot be achieved by a regular trojan attack with larger triggers.     \n\n- the evaluation against existing trojan detection/prevention methods seem a bit unfair as they were never designed to consider adversarial inputs\n\nPost Author response update\n----------------------------------------\nBased on the author's response, I will raise my score to 5. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Difficulties in learning using untrusted data.",
            "review": "This paper presents a very strong combined attack method, where infected\ntraining examples are crafted such the the trojan backdoor becomes very\ndifficult to detect.  I feel their approach to be relevant, informative\nand presents a significant advance.\n\nThe paper focuses on the mechanisms of cleverly disguising the Trojan training\ndata, and does an excellent evaluation.  The attack is particularly important\nin some online training scenarios, where one might wish to use non-trusted\ntraining data. \n\nThe evaluation is extensive, covering many existing and potential defenses,\nwith appendices covering different trojan types, trojan intensities, and attack\ndetectability, etc.  Readbility, and organization between main paper and \nappendix material was good.\n\nGiven the strength of the attack, what practical implications does this have?\nAppendix A addresses this for 2 cases that *fail* to defend, but a question I\nfound important was what steps would make the attack harder?  For example, \ncould App. A case (2) make the adversarial component more difficult if some\nlayers have parameters unknown, perhaps on a secure compute platform?\nDoes this have implications for how future compute platforms are set up?\n\np.7: Anomaly Index was actually *not* defined in appendix F, but in Wang et al.\n\nThis is a well-presented paper, with extensive experimental investigation\nand should be published. \n\n---\n\nI was the only reviewer who happened to imagine their threat scenario had some importance.\nAfter reviewing the authors' changes and comments, I feel that the threat scenario in the \nrevision still is insufficiently motivated/explained.  I'm downgrading to \"good paper\".\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An intersting method with too ideal assumption.",
            "review": "Based on the framework proposed by Pang et al. (2020), this paper unifies adversarial examples and Trojan backdoors into a synergistic attack. The inference results are dominated by the Trojan trigger and the adversarial perturbations. Such a mechanism extends the ability of the Trojan trigger. Incorporated with adversarial perturbations, the desired results of the adversary could be multiple classes. \n\nHowever, this paper involves a too strong assumption: the model parameter needs to be modified by the adversary. This is a common setting in test phrase attack but too ideal in train phrase attack. This assumption authorizes a superpower to the adversary. So bypassing existing defenses is not surprised.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but not well explained.",
            "review": "Summary: \nThis paper proposes a new type of attack: AdvTrojan. This new attack is activated only when the test examples contain two things: backdoor trigger pattern and adversarial perturbation. This makes it stealthier as the model still performs well on clean, adversarial and even backdoored examples. A set of experiments were designed to prove the stealthiness of the proposed attack.\n\nStrengths:\n1. It took me a while to understand what the authors tried to deliver here.  The proposed attack is indeed interesting and novel. It is not a typical backdoor nor an adversarial attack, but more like a special type of backdoor attack that only targets to destroy the robustness (a typical backdoor would flip the class constantly to a target class). \n2. The experiments confirmed the stealthiness of the proposed attack.\n\nWeaknesses:\n1. The motivation of this paper is poorly presented. Sometimes, I have to read several times to get the idea. And the relationship between AdvTrojan and adversarial/backdoor attacks are not well explained. This can be improved by adding a comparison table. The current version is a bit too diverged.\n2. Some of the definitions are not precise. For example, backdoor attack definition in Eq. (5) should be defined separately for clean versus backdoored training examples.  In Eq. (8), case 1 is incorrect: x should be x_adv, or the condition should be “if x contains Trojan trigger t and adversarial noise”.\n3. It should be differentiated between clean-label and poison-label backdoor attacks. Poison-label backdoor attacks also need to alter the class labels. Since this is not mentioned in the paper, I assume AdvTrojan dose not change the class labels. The Algorithm does not help understand the exact setting of this paper. The threat model should be clearly defined somewhere.\n3. Fig. 2 is a bit confusing. Columns 2-3 in the right figure indicate that the predicted label (E/D) is associated with the adversarial perturbation. Are the two patterns also part of the training? In other words, do they need to be trained into the model?\n4. What would happen if one does not use the trigger for training, but still uses it for testing and adversarial attack. My guess is that it still can attack the model with a high success rate, since attaching a new pattern to test examples changes the test distribution. Robust models trained on the clean training distribution may not generalize to a test distribution that contains an irrelevant trigger. \n\nSome of my understandings of the AdvTrojan the author may find it useful for revising the paper:\n1. Looking at the loss function defined in Eq. (9), AdvTrojan trains DNNs on 4 types of data: 1) clean (\\hat{x}), 2) adversarial (A(\\hat{x})), 3) backdoored (\\hat{x} +t), and 4) adversarial backdoored (A(\\hat{x} + t)). Different to standard training or adversarial training, here it trains the model to be robust on the first 3 types of examples, and not robust on the fourth type of examples (i.e. adversarial backdoored). This is more like to intentionally leave a loophole in the model. The model will memorize that the fourth type of examples are not robust.\n2. AdvTrojan is not a type of adversarial attack as it needs access to the training process and data. From this perspective, AdvTrojan is more like a type of backdoor attack. However, it is not a typical backdoor attack. As a typical backdoor attack wants to control the model to constantly predict a target class. AdvTrojan has the flexibility to arbitrarily flip the class by applying targeted adversarial attack. But the main purpose is to destroy (or fake) the adversarial robustness. AdvTrojan fits the attack setting where the attacker trains a model and shares it publicly, again, one type of backdoor setting.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}