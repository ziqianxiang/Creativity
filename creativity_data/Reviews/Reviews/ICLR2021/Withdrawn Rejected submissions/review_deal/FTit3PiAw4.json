{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents a provable correct framework, namely Universal Aggregation, for training GANs in federated learning scenarios. It aims to address an important problem. The proposed solution is well-grounded with theoretical analysis and promising empirical results. \n\nThe paper receives mixed ratings and therefore there were extensive discussions. One the positive end, some reviewers think that the authors' feedback provide clarification to confusing part of the paper; on the negative side, the authors feedback also confirms some of the concerns raised in the reviews: \n\n1.  It was confirmed that there is no guarantee that one can find an (nearly) optimal discriminator, which decreases the impact of the work, as in practice we work with non-optimal discriminators and hence some of the results couldn't apply. \n\n2. It was confirmed that no privacy guarantees can be given. This is concerning since the complexity of GANs won't prohibit skilled attackers from inferring some information. \n\nWhile it is true that some of the guarantees would be hard to achieve even for a traditional GAN, the paper sets up a high-expectation at the beginning of the paper,  but fails to satisfy the readers with enough evidence.\n\nIn addition, the writing can be significantly improved to ensure precise formulations and consistency; the added experiment results are useful, but stronger empirical results could help alleviate the issues in theoretical results. \n\nIn summary, the paper has built solid foundations for a good piece of work, but the current version could benefit from one more round of revision to become a strong publication in the future. "
    },
    "Reviews": [
        {
            "title": "An insufficiently clear paper",
            "review": "\n\n\nThis paper studies gederated Generative Adversarial Networks (federated GANs).\nIn particular, the authors propose a new method, UA-GAN, which is claimed to be better than earlier approaches.\n\nI have several concerns.  First, the writing can be improved significantly.  Second, the statements in the text are often rather vague.  This makes it hard to understand what are the results and to verify their correctness.   E.g., theorems are formulated either vaguely or unprecisely.\n\nMoreover, the theorems refer to \"optimal discriminators\", however there is no guarantee that one can find such an optimal discriminator or that one can even verify whether a discriminator is optimal.  (In fact, the paper doesn't define the term \"optimal\" precisely)   Even if it would be possible to learn an optimal discriminator, then there is no bound on the amount of time this would take (as that depends on various parameters of the learning problem and the learning algorithm).  \n\nThe paper contains more vague statements, e.g., Remark 1 claims the method preserves privacy, but doesn't define what information is kept private.   For example, it seems the size of the private datasets is needed for the central weighting and hence made public.  Moreover, there is no proof that from the information leaving the individual centers nothing about the sensitive data can be inferred.  For example, from the point of view of differential privacy, even if only aggregates are revealed, if their revealed value is exact then probably (epsilon,delta) - differential privacy is not guaranteed.\n\nIn conclusion, it is hard for the average reader to understand the paper due to a lack of precision, and the paper insufficiently specifies definitions, assumptions made, and precise formulations of results.\n\n\n\nSome details:\n\n* The first line of the abstract suggests that GANs are federated networks, while the second line of the abstract correctly states that this is called \"federated GAN\" and the first line of the introduction correctly describes GAN as generating realistic data.\n* Algorithm 1: \"Eq. equation 2.\" -> repetition\n* last line page 3: \"algorithm equation 1\" -> \"Algorithm 1\"\n* top of page 4: the generator G(z) seems to depend on an argument z of which the nature is not revealed immediately.  Later, Equation (2) suggests that z can be drawn from \\mathcal{N}(0,1) and hence is a real number.  It is unclear why the argument of G would be just one-dimensional.\n* top of page 4: while the word \"discriminator\" is used very frequently, no precise definition of this concept is provided, nor an explanation of how the discriminators are obtained.  With the help of Eq (2) some readers may be able to guess that D_j must have values in the open interval (0,1).\n* Definition 1: as the symbol q is already used for distributions, it is preferably to not use it for \"a probability in (0,1)\" too.\n* After definition 1: \"The central idea of UA framework\" -> \"framework\" needs an article\n* just before Section 3.1:  We will provide error bound -> ... an error bound\n* Theorem 1 uses \"Jenson Shannon divergence loss\", which isn't defined in the paper (and no definition is cited).  \"Jenson Shannon divergence\" is a somewhat well-known concept in probability theory, but even for those knowing this it is unclear how to get from it to \"Jenson Shannon divergence loss\".\n* Equation (6) in Theorem 1 seems to give an expression similar to the Jenson-Shannon divergence definition, but doesn't appear a statement the theorem is claiming to be true.  The rest of the sentence refers to q^* and q, but q is a bound variable in Eq (6), i.e., it has no meaning outside the scope of \"argmin_q\", and q^* doesn't occur in the formula (so why do you say \"where q^* = ...\" ?).  It is hence hard to parse the theorem statement and discover what is the claim exactly.\n* A proof is provided in appendix.  However, the proof first says \"To prove the theorem, we first introduce the following Lemma\", the text next states lemma 1, but never returns to the proof of Theorem 1 (as the next title says \"Proof of Theorem 4\").\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "review": "This paper proposes an algorithm for training GANs in federated learning setups. The federated learning task is to train a centralized generative model using the samples distributed across a group of local nodes in a network. Toward this goal, the paper proposes Universal Aggregation GAN (UA-GAN), in which at every iteration the server communicates a batch of samples produced by the generator to the local nodes and then the local nodes optimize their discriminator using their observed real and received fake samples. Overall, the UA-GAN approach is technically-sound and potentially useful for various distributed learning problems. However, I still have a few comments regarding the paper's theoretical and numerical results. I look forward to the authors' response to my comments.  \n\nMy main concern is on training the discriminator in UA-GAN. According to Algorithm 1, UA-GAN trains K discriminator functions, i.e., one discrminator per local node in the network. In a typical federated learning problem, there might exist hundreds of nodes  where each node observes only tens or hundreds of training samples. In such scenarios, the generalization error of training one discriminator per local node can be very large, since every discriminator is trained using a limited number of real data points. On the other hand, the generalization error seems to be significantly smaller if one uses standard FedAvg for training a single discriminator network.  Currently, there are no theoretical guarantees or numerical analysis on the generalization properties of UA-GAN. I think analyzing the generalization behavior, either theoretically or numerically, is needed to address this comment, especially becuase the paper mostly focuses on the theoretical properties of UA-GAN. \n\nThe paper's numerical expeirments only consider setups with non-identical distributions across nodes. Also, in the experiments K is chosen to be moderately small while the number of training samples at every node is relatively large. I recommend providing some numerical results for larger federated learning settings with smaller training sets. For example, one can consider 100 local nodes with only 500 training samples available at every node. These numbers sound closer to a real federated learning problem than the ones used in the paper's experiments. In addition, lines 9 and 14 in Algorithm 1 should clearly state the optimization procedures for updating the discriminator and generator parameters instead of only referring to the optimization problems. In the current version, it remains unclear how many gradient steps are used to optimize the parameters at every iteration and how the performance of the trained GAN is affected by the number of gradient steps applied for optimizing the discriminator parameters at every communication round.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "About the theory and experiments",
            "review": "This paper proposes federated GAN with multiple private discriminators. The authors also analyze the optimality of the proposed federated GAN. The review has several concerns on the current submission:\n1.  Is minimax problem（5)  the total loss of UA-GAN that is optimized by Algorithm 1?  To solve the minimax problem (5),  D_{i} for i=1,2,..., K is coupled!.  Hence，why D_i defined in（2）is the solution of （5）when G is fixed？\n2. When and how does the Nash equilibrium of minimax problem （5）holds？ \n3. Lack of experiments for federated unconditional GAN to verify the theory. \n \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Easy to read paper with interesting results",
            "review": "Summary:\n\nThe paper proposes a new method, UA-GAN to train GANs in a federated learning setup. The method simulates a central discriminator D_ua such that the odds values of the central discriminator is equivalent to the weighted sum of the local discriminators. The central generator is then trained based on the simulated central discriminator. The paper provides theoretical analysis on its proposed method and conducts experiments on toy datasets and mixtures of real world datasets to simulate a federated learning setup.\n\nPros:\n- The problem of training GANs in a federated learning setup is an important problem to tackle as it naturally provides a secure way to train a generative model.\n- The idea of simulated central discriminator based on the weighted odds value of the individual discriminators is quite interesting and seems to outperform a simple weighted sum of the discriminator gradients.\n- The results provided in the experimental section are reasonable and outperforms similar GAN setups on non-identical data setups. I also appreciate the inclusion of an accuracy metric in evaluating the different methods as it shows the effectiveness of generated data in training of downstream tasks.\n\nCons:\n- The paper provided a theoretical analysis of the performance of UA-GAN under conditions where some discriminators are trained sub-optimally. It would be nice to also have some experiments in the experimental section that shows the results of this setup i.e with smaller dataset size for some of the discriminators\n- The experimental results will also be more convincing if results of training the UA-GAN on the mixture of all three real world datasets (Font, MNIST and Fashion-MNIST) are shown\n\nOverall, the proposed method of training GANs in Federated Learning setup shows fairly convincing results. With some additional experimental results I believe this will be a good submission for ICLR.\n\n==================\n\nI thank the authors for the additional experiments which have marginally satisfied my initial concerns; ideally, more setup can be experimented. I keep my original rating.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}