{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Three of the reviewers are significantly concerned about this submission while R3 was positive during review. During discussion, R3 also agreed that there are concerns not only on experimental designs and results but also the proposed model. Thus a reject is recommended."
    },
    "Reviews": [
        {
            "title": "concerns about the experiments",
            "review": "The paper combines the label propagation algorithm (LPA) and graph convolutional neural networks (GCNs) and proposes a unified model with learnable edge weights utilizing both feature and label influence.\nThe theoretical analysis of the correlation between LPA and GCN is interesting. Based on the theoretic analysis they show that the key to improving the performance of GCN is to enable nodes of the same class to connect more strongly, thus they propose to use LPA to reweight the edges and then use the new edge weights for GCN. However, their final model jointly learns edge weights with both LPA and GCN, which is acceptable but might reduce the connection to the theory. \nMoreover, my main concerns are about the experiments. \n(1)\tSome of the baselines do not match the results reported in the original paper (e.g. GAT has much better performance than the numbers reported in Table 1); and the improvement of accuracy is actually marginal on most datasets. It will be better if the authors can analyze in which condition or on what types of datasets GCN-LPA works better. \n(2)\tI would like to see an ablation study which removes the LPA loss (\\lambda L_{lpa}(A)) or replaces it with l2 regularization in Equation (9). Since the main idea of this paper is to reweight the edges with LPA, it is necessary to show the effect of the LPA regularization term. Reweighting the edges using only GCN is a very natural ablation model.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "This paper aims to combine the label propagation and graph convolutional network with the modeling of their latent relationships. In the developed model, the node label is utilized to infer the edge weights between different nodes. From the evaluation results in Section 4, the performance improvement between the proposed method GCN-LPA and GDC is marginal, which can hardly demonstrate the advantage of the unified model (with GCN and LPA) over the graph diffusion network (without the restriction of information aggregation over neighboring nodes).\n\nFurthermore, this work generates another baseline with the combination of prediction GCN and LPA methods. From the evaluation results, this baseline performs much worse than GCN and GAT, which may indicate that the predict combination involves some noise. It is better to conduct further experiments to show the effectiveness of the proposed GCN and LPA integration mechanism over simplified combination.\n\nA minor note would be the lack of detailed hyperparameter tuning strategies of compared baselines. Different parameter settings may offer different performances; thus, it would be better to report how to perform the parameter tuning over various compared methods (such as GAT, GCN and GDC), to achieve good model performance and ensure a fair performance comparison.\n\nThe proposed method incorporates the label propagation to calculate edge weights, which share similar paradigm for learning node correlations with graph attention network and its extensions. In addition to the performance gap between the GCN-LPA and GAT, more clarifications about the model difference could be added, to have a better understanding of the new combined GCN and LPA framework.\n\nIt would be better to show the performance as the training/test ratio varies. It will be interesting to see is more data helpful to capture graph structural information better.\n\nIn the experiments, only model scalability comparison between the new GCN-LPA method and GCN, is studied. Is the GCN-LPA more efficient than other baselines, and which component of the new framework is more computationally expensive?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "need some improvement",
            "review": "This paper addresses the problem that edges in a graph could be noisy, containing erroneous edges. With the assumption of GCN that ‘labels/features are correlated over the edges of the graph’, it is desired that weights of inter-class edges are large, and those of intra-class edges are small. Hence, these noisy edges could impair GCN’s performance.\n\nAddressing this problem, this work propose to optimize the given adjacency matrix based on the performance of Label Propagation(LPA) on it. LPA also assume the ‘homophily’ property of graphs, hence adapting adjacency matrix on it can encourage larger weights being given to trajectories linking two same-class nodes. This step is expected to be beneficial for the performance of GCN.\n\nThis paper has following strong points:\n1.\tProposed GCN-LPA can learn to optimize the edges and perform node classification in an end-to-end manner, without causing much more training time cost;\n2.\tThe writing is clear and easy to understand. The motivation, theory part is presented step by step, with complete proofs of those theorem;\n3.\tExperiments show that the proposed model is better at splitting embedding of nodes from different classes, and achieve improvement in node classification performance.\nFollowing are the weak points:\n1.\tThe idea that LPA can help GCN is not convincing. These two methods have similar assumption over the data, as also shown in the relation between label influence weight and feature influence weight in Theorem 1. Hence, why the authors claim on Page4, after Eq(9), that only updating edges with GCN will cause overfitting? Would not they be the same?\n2.\tThe experiment is not very complete. Implemented GCN-LPA optimizes edges with gradient from both LPA and GCN tasks. The idea that LPA contains complementary information for GCN is not well justified. I would suggest that using LPA to obtain the updated edges and then fix them, before sending them to the GCN model.\n3.\tBesides, there are some other papers also seeking to adapt edges for the training of GCN, which are not mentioned. For example, \n[Jiang, Bo, et al. \"Semi-supervised learning with graph learning-convolutional networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.]\n[Yang, Liang, et al. \"Topology Optimization based Graph Convolutional Network.\" IJCAI. 2019.]\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "good paper, but still needs some clarifications",
            "review": "##########################################################################\nSummary:\n \nThe manuscript proposes a unified model which combines label propagation algorithm (LPA) and graph convolution network (GCN). The main idea is to optimize edge weights (after making edge weights trainable) by maximizing the intra-class feature influence. Introducing the theorems on the relationship between feature and label influence, and LPA’s prediction, the authors propose the unified objective function (a summation of the GCN loss and LPA loss) which combines both methods.   \n##########################################################################\nReasons for score: \n \nOverall, I vote for accepting. The manuscript is overall well written, and the motivation of the proposed method is well explained by the proposed theorems. The logic leading to the objective function sounds reasonable and interesting. However, the manuscript still seems to need some clarifications. \n \n##########################################################################Pros: \n \n1.\tThe manuscript provides a new theoretical viewpoint to combine LPA and GNN.\n2.\tThe proposed method outperforms the baselines, including some state-of-the-art Methods. \n \n##########################################################################\nCons: \n \n1. The method is a transductive method (test data points should are present during training) although the authors briefly mentioned the possibility of extension to inductive learning. I think that additional specialized effort is needed to extend label propagation in inductive setting. I think this is one of main weakness of the model.\n2. Regarding the term regularization term for the LPA loss. This is generally referring to a penalty term on a penalty on the model complexity. I think that it is more appropriate if the authors introduced the proposed objective function as a simple implementation of multi-objective optimization.   \n  \n#########################################################################\nQuestions:\nIn the experimental results. “In addition, we propose another baseline GCN+LPA, with simply adds predictions of”: It is not clear how to combine the prediction solutions from both methods (adding predictions in different scales?). \n\nFor Figure 5. What is the sparseness of edges in the random graphs? GCN-LPA has additional many parameters (the size is equal to the number of non-zero edges in the graph) compared to GCN. I expected that the time complexity of GCN-LPA would be way larger than that of GCN. Is this experimental setting close to real prediction problems?  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}