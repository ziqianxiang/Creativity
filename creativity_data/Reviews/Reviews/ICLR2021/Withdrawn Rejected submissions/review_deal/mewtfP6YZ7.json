{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors compare different model based R algorithms to see whether observation prediction is important. They show that, as expected, it is. On the other hand, they seem to show that latent space prediction is not very useful. The study is limited to domains with image data: Does this domain have something particularly special? Perhaps experiments with smaller-scale POMDP problems might actually have shown something different. It is very difficult to do a study of this type properly, and although the authors have tried, it's hard to see how this paper can be accepted. I agree with some the positive points some reviewers have raised, but I think that, at the end of the day, the paper is trying to draw too general conclusions from a handful of datapoints. Were I writing this paper, I would first try the simplest version of the hypothesis with very basic environments that are, however, more varied than the ones shown here. Would the hypothesis hold, I'd scale up to more complex environments and try to also run with more seeds to get a clearer signal.\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #3",
            "review": "- Summary:\n    - This paper presents a study on the trade-offs of image prediction for model based RL\n    - They find that image prediction loss is important and, surprisingly, reward prediction accuracy can be negatively correlated performance while image prediction accuracy is considerably more well correlated.\n- Stengths\n    - Clear presentation of results and overall well written\n    - Comprehensive series of results that all build on one another\n    - A large amount of care given to the implementations. The sentence \"Given the empirical nature of this work, our observations are only as good as our implementations.\" is very apt.\n    - The authors do a good job at pointing out the caveats and shortcomings of their study.\n- Weaknessses\n    - While a fair number of environments are studied, they are all visually very similar (all 3rd person camera, same background, etc.).  Adding in a very different environment, i.e. Visdoom, would strength the analysis.  While I don't think adding a different environment is necessary, this should be pointed out in the paper.\n    - While the presentation is overall quite clear, I do have some suggestions (see bellow)\n    - Questions:\n        - How was training stopped in the off-line setting?\n        - Were the same hyper-parameters used for both online and offline?\n- Suggestions for improvement\n    - A version of Table 1 with the online results at convergence would be great as the delta between online and offline performance is interesting.\n    - A Pearson correlation coefficient and/or a Wilcoxon signed ranked test on the ranking induced by Table 1 vs. Table 2 for each environment would be a nice summary statistic.  Although the dynamic range of the values may not be large enough for this to be sensible.\n    - If possible, color code the variant names in the tables with their line colors in plots.  Also, order the table the same as the figure legends.\n- Overall\n    - This paper presents an interesting, well-motivated, and comprehensive study on the trade-offs in image prediction for model based RL that also points to future directions of research.  \n\n\n## Post Rebuttal\n\nI thank their authors for their response.  I have decided to maintain my rating, overall I still think this is a good paper that presents an interesting set of results.  The result that image prediction accuracy correlated better with asymptotic performance than reward prediction accuracy continues to intrigue me.  My fellow reviewers have some concerns that while I don't agree, I think they could be avoided with some changes in the presentation to the paper:\n\n* The models used.  I understand the decision to move the model description to the appendix (not everything is going to fit in 8 pages), but the main paper would benefit from paragraph or two describing why which models were chosen and then referring to the proper places in the appendix for the full details.\n\n* Reward-only model.  There was considerable concern about how much smaller that models is than the others.  Additional results showing that one with a comparable number of parameters does as poorly if not worse would be beneficial.  The 0% line in Fig 6 and Fig 8 is very similar to this hypothetical (if it exactly) so I believe this will pan out as expected.\n\n* Presentation of results.  In my read, the most interesting result, anti-correlation between reward prediction accuracy and asymptotic performance, comes at the end.  Without that result, there is a way to read the paper as \"yes image prediction helps, it increases the amount of supervision given to the model\".  That result and the difference between online and offline shows that there is something unexpected going on here so perhaps leading with those and/or highlighting them more would help.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful taxonomy and benchmarks; generality of results w.r.t. taxonomy overstated",
            "review": "The paper proposes a simple taxonomy of objectives for training predictive models for planning in visual model-based RL, and evaluates them in a set of consistent experimental tasks and using a largely consistent set of models. It claims that in the context of the models investigated, backpropagating losses based on future observations yields better performance than using only the reward as training signal, and that prediction accuracy of future observations is also more predictive of task performance than reward prediction errors. It additionally empirically demonstrates the magnitude of effect exploration policies can have in this setting, showing how a lot of the models that perform worse online simply explore less well. \n\nI think the paper is largely easy to read and understand, and systematic/structured exploration of modeling choices is important especially given the steady trickle of results showing that specific modeling choices can be less important than how well-tuned the implementations are. The additional insight about exploration vs modeling efficiency is well-supported and the additional experiment that separates exploration efficiency is well-thought out. I also think it's great that less-conclusive or less-succesfful experiments (e.g. on optimism) are still included in the appendix rather than file-drawered, though I'm puzzled that the claim regarding implicit optimism is made at the end of section 4.3. considering what appendix A2 shows. \n\nThat said, I have some additional comments about ways in which the paper could be improved: \n\n1. I think the paper overstates the generality of its results, considering the usage of only one specific choice of each architecture and loss (with the exception of the R-model) and one planner. The choices seem to be reasonable and consistent with the SOTA, but they're also only one way of setting up each kind of loss in the taxonomy, and the paper doesn't make a strong argument for why they should be general. Especially given how the paper shows that there's no clear uniform winner across tasks, one could reasonably expect that the variability across other variants not explored is similarly large. Similarly, the underperforming oracle highlights the contribution of the fixed-horizon planner to the overall results, and calls into question the claim in section 4.1 about how the oracle approximates optimal performance. I think it's good that the architectures were kept consistent across variants when possible, but convergent evidence (even from scaled-down toy examples) would help make the point, in combination with more caveats about the generality. \n\n2. A related concern is whether the reward prediction models are genuine competitors or straw man models, since the remaining architectures / models come from previously published results and I'm not sure if the reward prediction models do (I think $\\mathcal{R}_{LL}$ is similar to what was used by Havens et al. 2019, though that was a non-archival NeurIPS workshop paper). It's surprising that the paper does not take any of the prior work it cites as the exemplar model for reward prediction, similarly to how it does with pixel prediction. On the other hand, if these models are credible competitors from past work, that work should be cited. \n\n3. The focus in the writing on what the model predicts is a bit misleading in that unless I'm missing something, the critical difference is which loss is being backpropagated. In order to plan, I need to simulate rollouts, so even some of the R-models have a next state predictor (except if they predict the full horizon at once, as in $\\mathcal{R}_{conv}$), and the reward-loss models can have worse reward predictions, presumably by having less training signal (e.g. Tab 2). This could be clarified in the text and Figure 1. \n\n4. The overall results are ultimately \"messy\" in the sense that there's no clear winners or patterns. This in itself is worth knowing (and a point that the paper mostly embraces). That said, there are a few missed opportunities for further insights, including: \n    - Performance across different kinds of difficulties. The paper makes the point early on that its test environments focus on different difficulties (contact discontinuities, large state spaces, long-term memory, long-term planning, sparse rewards). This point isn't revisited -- some discussion along these dimensions might be useful, beyond \"different training signals win on different tasks, except reward prediction, which never does\". \n    - The paper makes the point that using reward only as training signal can be substantially cheaper (section 4.4), and that latent-dynamics models are faster than pixel-space dynamics models (table 5). Does this mean we might expect cost-normalized performance to show some interesting patterns? \n\n5. It would be useful if a visual comparison between the online results (Fig 2) and offline results (Tab 1) could be made, either by including Tab 1 results in panels on the figure, or moving the table to the appendix in favor of a figure that compares offline and online results. Similarly, the tables do not facilitate the comparison made in the caption to Tab. 2 regarding the relationship between reward prediction accuracy and task performance. \n\n[I think the paper provides a useful case study and I appreciate the caveats added to the conclusion, but based on additional discussion with the other reviewers, I think that the paper's more general claims remain unsupported and insufficiently moderated. I am reducing my rating accordingly.]",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "In this paper, the authors compare several kinds of MBRL models, including observation prediction, to show which approach is better. In conclusion, The models that contain prediction on both a reward and an observation outperform the ones with only reward prediction. \n\n[Quality]\n\nThis paper requires a revision for enhancing clarity. For example, Section 4 seems exceedingly long, and it contains too many redundant details, which should not have been emphasized.\n\n[Originality & Significance]\n\nThis paper provides neither a new method nor a different perspective.\n\n[Strengths]\n+ The authors showed the effect of using visual observation prediction for MBRL models. It might show the way for further MBRL models.\n+ It was mentioned on the paper that prediction accuracy of observations and rewards and exploration performance is on the trade-off relationship.\n\n[Weaknesses]\n- The number of parameters on the model that predicts only rewards is much (about a hundred times) smaller than other models. Additionally, the authors used SV2P and PlaNet to implement each experiment; it is doubtable that comparisons are fair enough.\n- There are no clear and consistent differences between models with an observation prediction, which reduces this work's novelty. It is better to use four different SOTA models or four modified models from one backbone model instead of modifying two models.\n- The trade-off relationship is mentioned, but there is not enough quantitative and theoretical analysis to show this point. \n\n[Comment]\n- There is no bolded item in Table 2; it seems to be omitted, or the result is not strong enough.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Questionable empirical setup and problems with the statistical significance of the results",
            "review": "Summary\n\nThis paper empirically investigates the effect of different reward-prediction models for model-based reinforcement learning (MBRL) in the particular context of decision-time planning with the cross-entropy method and visual tasks. To that end, five models are evaluated: four models that predict rewards and observations jointly, while one model only learns to predict rewards but without observation prediction. The authors claim that predicting observations in addition to rewards improves performance of MBRL in their setting (in terms of cumulative reward maximization). This hypothesis is tested on a bunch of tasks from the DeepMind Control Suite, both in an online and offline setting. The authors conclude, based on their experiments, that their hypothesis can be confirmed.\n\n\nQuality and Details\n\nMy main problem is that I don't entirely understand the particular constellation of models to be compared with each other to begin with---see Figure 1. To my understanding, Figure 1 just depicts two different classes of models: the four on the left and the one on the right. Note however that this becomes clear only after studying the appendix (Appendix B) in more detail, since the models are not described properly in the main text. The four models on the left can be subdivided into 2 further subclasses depending on whether transitions are modelled in latent or observed space: latent-variable models refer to the PlaNet architecture and observation-space models to the SV2P architecture (each can be trained with and without additional reward prediction). The stand-alone model on the right is a specific model not related to PlaNet or SV2P.\n\nI believe the question of whether additional observation prediction helps can be investigated for each of the 2 top-level classes separately. For example, the left four models could be amended to only predict rewards---running PlaNet or SV2P but stripping away the observation prediction component? Looking at appendix Figure 13 that describes PlaNet, the observation component could be ignored in the course of training such as to only predict rewards. Similarly in Appendix Figure 12 that describes SV2P, the autoregressiveness could happen at the hidden layer without observation input/output. In a same way could the stand-alone model for reward prediction be amended to enable observation prediction.\n\nAfter studying the appendix in more detail, I came to realize that the authors actually followed one of the ideas above and trained PlaNet with only reward prediction but not observation prediction. They found that this performs worse compared to another reward-only-prediction baseline (which they finally report in the main paper). Concluding, the presentation is quite convoluted and it is still missing whether SV2P can be amended to predict rewards only, or whether the best-performing reward-prediction baseline can be extended to predict observations.\n\nA clear execution and presentation of the setting described above would allow to draw conclusions about whether predicting observations actually helps *across* different architectures, but not the setting studied by the authors that uses separate architectures for joint reward-observation prediction and reward-only prediction.\n\nAdditionally, assuming the experimental setup makes sense (which I feel it does not entirely), I am still concerned about the statistical significance of the experimental results from Section 4.2 on online learning. There are just 3 seeds per experiment---so it is not clear which joint reward-observation model is best, although it seems indicative that the reward-only prediction model is worse across tasks compared to the joint models. Statistical significance is only getting worse in Section 4.3 which studies the offline setting: because here only one performance value is reported *at the end of training* as opposed to *in the course of training* (making it even more difficult to judge the statistical significance, since RL algorithms are known to produce rapidly performance-changing policies in the course of training).\n\n\nClarity\n\nThe clarity of the paper is low. While multiple different prediction models are studied, none of them are presented in detail in the main paper, neither regarding architecture nor learning objective. The appendix provides more information, revealing how complex the models are. These details do matter in an empirical study and a missing description in the main paper makes it even harder to judge the conclusions drawn from the experiments.\n\n\nOriginality and Significance\n\nInvestigating the necessity of observation prediction (in addition to reward prediction) on RL performance is interesting. However, the paper does not entirely convince me regarding both the experimental setup and statistical significance.\n\n\nPros\n\nA lot of environments are tested.\n\n\nCons\n\nThe experimental setup is questionable and so is the statistical significance of the results.\n\n\nMinor\n\nI believe the caption of Figure 3 draws conclusions about which regions of the state space have been visited. However, only empirical reward distributions are presented, i.e. I don't see how one can draw conclusions about state-space visitation from there. \n\nAlso, the main online results from Figure 2 show in some environments that the oracle baseline (that knows the true environment dynamics) is worse compared to the MBRL algorithms. The authors mention in the text that this is due to a limited planning horizon---I still do not understand why the MBRL algorithms work better given that they also need to cope with a limited planning horizon.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}