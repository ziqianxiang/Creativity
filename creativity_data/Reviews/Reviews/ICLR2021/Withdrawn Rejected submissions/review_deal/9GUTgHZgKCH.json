{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This is a clear reject. None of the reviewers supports publication of this work. The concerns of the reviewers are largely valid."
    },
    "Reviews": [
        {
            "title": "Not a clear paper, nor contributons, lack of convincing experiments and positioning with respect to state of the art methods. ",
            "review": "The paper focuses on defining a new architecture that allows being reduced without significantly affecting the performance. \n\nIn short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art. Experiments are not convincing: Toy experiment and minimum experiments in MNIST without a clear comparison to existing neuron pruning algorithms.  ",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper is not ready for publication.",
            "review": "Summary: \n- In this paper, the authors propose a novel algorithm for pruning fully-trained ReLU neural networks. To motivate the algorithm, the authors first introduce a new network architecture with an affine skip-connection at each layer. Then the authors connect it to the theory developed in an `unpublished work`. They show that for such neural networks, the number of units in the original layer can be greatly reduced. The main idea of the proposed algorithm is basically to prune those neurons whose removal will not change the function a lot. To quantify this, the authors turn to the L2 norm of each neuron. Experiments on simple toy data and MNIST are conducted.\n\n\nOverall, I believe this paper is not ready for publication. So, I vote for rejection. \n- This paper massively refers to the unpublished work by the author, while the authors only provide only little details about the developed theory in the unpublished work. If this is a concurrent submission to ICLR, the author should still cite it anonymously.\n- To me, the proposed deep stack network is very similar to the formulation of the highway network in Srivastava et al., (2015).\n- The experiments are not convincing enough. The authors only conduct experiments on a simple toy dataset and MNIST. The numbers are fairly close to each other. To show the statistical significance, some measures, such as one standard error should be provided. I would encourage the authors to at least conduct some experiments on CIFAR datasets.\n\nTypos:\n- Exactly speaking, the learned function will not be the same same as before ->Exactly speaking, the learned function will not be the same  as before \n- Therefore the the function optimizing eq. (2) can be represented by finite number -> Therefore the function optimizing eq. (2) can be represented by a finite number\n\n\nSrivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. \"Highway networks.\" arXiv preprint arXiv:1505.00387 (2015).",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper considers a functional regularization form of neural network training problems to prune networks. There are significant issues in the presentation and clarity.",
            "review": "\nThe authors leverage a functional regularization reformulation of neural network training problems to prune networks via a reduction algorithm. They present limited experimental evidence showing that the reduction algorithm reduces the number of neurons without sacrificing too much accuracy. \n\n\nMajor comments/questions\n\n1. Clarity and correctness\nThere are significant issues in the presentation and clarity. The authors use footnotes to explain important concepts, but many definitions are missing. The material in the footnotes can be included in the main text with a more natural flow. The main observation in Section 4.1 is not presented as a rigorous result, which appears to be the most interesting result. Remark 4.1 can also be presented as a theorem.\n\n2. Insufficient comparisons with the baselines.\n There is extensive literature in pruning and sparsification of network layers. In Table 1 and Table 2 there is no comparison with standard baselines in pruning. Does the proposed method perform better than standard pruning based on  weight magnitude/gradient norm/Hessian based metrics? \n\n3. Figure 6 is not very informative. It would be better to zoom in the relevant portion of the plot. Focusing on a few examples instead of seven different examples would make a better display.\n\n4. It is not clear why the authors consider the bottleneck architecture in Figure 4 and 5. Is the bottleneck required for the theory behind pruning or reducing overfitting?\n\n5. Section 3 starts with 'the authors have shown in an unpublished paper\". Is this referring to Maennel et al, 2018? Similar results also appeared in other papers (e.g. Savarese et al. 2019. Please provide a reference or proof for the equivalence of (1) and (2). The proof is in fact straightforward, we only need n_j>=d+1 to hold for Caratheodory's theorem. The authors can be more precise for the required width n_j. This is a significant omission.\n\n6. On page 3, footnote 1, P(f) is not properly defined and it's not clear what P(f)=\\infty means. This can be clarified by providing necessary references noted above.\nFurthermore in eq (1), NN_\\theta is not properly defined. Is this a standard relu network?  \n\n7. In the introduction, the authors claim that the proposed method preserves the network output exactly as opposed to other pruning methods. However, in Section 4.2 and 4.3, the authors also resort to approximation methods involving magnitude based pruning and clustering, which are standard in the literature. The observation from Section 4.1 is also not exactly applied and yields an approximate neural network.\n\n8. What would be the computational complexity of looping through every neuron and the proposed approximation? Is there a way to justify this approximation?\n\nMinor comments\n1. The manuscript needs a careful proofreading since it contains lots of typos and grammatical errors.\npage 1. It's architecture -> Its architecture.\npage 1. authros -> authors\n\n2. There some definitions which need further explanation\npage 1. I believe what is meant by a 'large layer' is a wide layer.\npage 1. Could you please clarify what sparsity refers to in \"reduce the number of neurons by 90% to 99% without introducing sparsity\"? \npage 3. bottleneacks->bottlenecks",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Pruning technique for ReLU networks with insufficient validation and derivation",
            "review": "The paper suggests a pruning technique specific to ReLU networks by taking advantage of activation patterns and the separating hyperplanes. The technique consists of three steps: : (1) remove neurons that are never active and combine neurons that are always active, (2) remove neurons with little contribution to the output, and (3) use weighted k-means to combine other neurons. The method is evaluated on specific toy problems and the MNIST dataset.\n\nThe paper proposes a novel technique to reduce neurons in a ReLU network. The method to combine neurons takes the hyperplane arrangements (where activations of neurons change) into account and leads to much smaller networks with equal performance in the experiments.\n\nThe three major problems of the paper are that it lacks motivation of the proposed technique, it contains an insufficient experimental evaluation and important parts of the paper cannot be reviewed either due to a reference to unspecified, unpublished work or a lack of a derivation.\n\nSince the correctness of the paper cannot be evaluated and the technique is insufficiently validated, I recommend to reject. \n\n\nDetails on weaknesses:\n- The correctness of the proposed technique cannot be reviewed. \n(a) Section 3 cannot be confirmed as it refers to results from unspecified unpublished work, i.e., it is impossible to find and read through the unpublished work to estimate its validity. Moreover it is unclear why these results are important for the given paper and how the results are used. The paper states that one should only learn from this entire section that a finite(!) number of neurons is sufficient (which we always have in a practical setting so the conclusion is void?) In any way, either this section is not necessary for the rest of the paper and should be removed, or it is necessary in which case it cannot be verified.\n(b) The proposed pruning technique consists of three steps (see above in the summary) two of which are trivial: (1)&(2). The third step (3) uses weighted k-means to combine other neurons. There is no explanation, motivation or derivation of the equations how the clusters are combined. (The workings of the method are also surprising, because a cluster containing a single neuron is reduced to a new single neuron in such a way that the function changes, which is counter-intuitive. It seems therefore likely that the equations contain typos, also since they introduce square roots of square roots and it is not defined what is meant by squaring a vector in the function g.)\nTherefore, the validity cannot be confirmed and the reader must trust the experimental result section. This is unfortunate as there are opportunities to shorten less relevant parts in favor of a derivaiton of the equations.\n\n- The experiments are not sufficient. The experiments only consider specific toy problems and the MNIST dataset, which is too simple to showcase the pruning technique. The method is neither compared to any other pruning technique and it is fairly simple to prune networks on MNIST with a similar loss of accuracy. Finally, the method introduces two hyperparameters which are not tested. To validate the performance, both a more complex dataset and the comparison to other pruning techniques are necessary.\n\n- The presentation needs improvement. For example, there is a rather long explanation of a seemingly simple architecture and still details are left unclear (Is there a linear layer with weights that are trained, or is the linear skip connection only introduced when pruning the network?) It would be helpful to add an equation for the stack layer and reduce the explanations. The possibly redundant Section 3 could be removed. Instead, the proposed method could be derived and explained. The experiments could be explained in more detail (Why do the plots show the smoothened second derivative?)\n\n\n\n\nTypos:\nLine 4 Motivation „authros“\nPage 3, footnote, „bottleneacks has diomension“\nPage 4 toward the end of Section 3: „the the“\nPage 5: the first sentence is not a sentence",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not novel, below average on most aspects",
            "review": "The paper describes a way to reduce the dimensionality of a deep ReLU network. In general, the paper is not well written and hard to follow. They keep referencing \"unpublished work of the authros\" although its use in practice is not very clear.\n\nPractically, they prune a deep network by (a) removing dead ReLU neurons; (b) combine neurons for which the ReLU always acts as the identity. None of these two ideas is particularly novel, especially considering the huge amount of literature to be found on network pruning. Experiments are only done on an artificial dataset and on MNIST.\n\nMany parts of the paper are poorly described. For example, their \"stack network\" is simply a residual network with affine projections on the residual link. A \"P-FUNCTIONAL\" is not defined. The link between Eq. (4) and their algorithm is not clear.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}