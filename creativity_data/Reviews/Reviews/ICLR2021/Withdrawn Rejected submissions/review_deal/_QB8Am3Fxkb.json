{
    "Decision": "",
    "Reviews": [
        {
            "title": "Limited novelty; more insights and/or ablation studies would be helpful",
            "review": "The paper studies GhostNorm in detail, trying to explain why it works. The authors also improve GhostNorms by proposing a variant named SeqNorm which applies GhostNorm and GroupNorm sequentially.\n\nMy concerns/questions are below:\n* The technical novelty is limited. While SeqNorm is a straightforward combination of existing works, the paper did not provide sufficiently appealing theoretical insights/empirical ablation studies explaining why GhostNorm and GroupNorm are complementary.\n* Perhaps one of the most interesting observations is that a smoother loss landscape may not necessarily imply better generalization, as illustrated in Figure 2 & 3. These experimental results, however, are not comprehensive enough to challenge the existing hypotheses because variances across multiple runs are not reported. It also remains unclear whether this finding would still hold true with different learning rates.\n* Results for GroupNorms are missing in Table 1 & 2, which seem critical to demonstrate the usefulness of SeqNorm (by answering the question that whether GhostNorm and GroupNorms are truly complementary). While the results in Table 2 look encouraging, it is a bit unclear whether the gains of SeqNorm would vanish with stronger data augmentation policies such as RandAugment.\n* BatchNorms & GhostNorms are more advantageous than SeqNorm in terms of the inference cost (as the EMA statistics can be fused into adjacent layers). The computtion aspect should be mentioned in the paper.\n* Table 2: \"CIFAR-10 results are averaged over 2 runs\" -- why not 3 runs?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experimental results are not convincing",
            "review": "(1) This paper proposes an improvement normalization method named SeqNorm. In practice, the author realizes SeqNorm by using GhostNorm following with a GroupNorm. \n\n(2) Some analyses about the regularization properties of GhostNorm and SeqNorm are also presented in this paper.\n\n(3) The experiment results show the performance improvement compared with BN and GhostNorm.\n\nAlthough the idea is simple and achieves some improvement compared with its counterparts,  I vote for rejecting since the experimental results are not convincing from my perspective.\n\n\nComments:\n\n1.How about the performance of SeqNorm by changing the order of GhostNorm and GroupNorm? More ablation studies are needed to compare with this case. Some analyses about the difference between the proposed method and such a case should be given.\n\n2.How about the performance of GroupNorm in the same setting? Actually, GroupNorm should also be one of the baselines of your method.\n\n3.Why the improvement of CIFAR10 is limited? When using RandAugment, the gain of SeqNorm drops a lot in CIFAR100, why? The SeqNorm’s testing results of w/ and wo RandAugment are the same, why? It seems that the proposed SeqNorm is sensitive to the combination of G_C and G_M, how to keep good performance in practice? \n\n4.How about the performance of the proposed method compared with state-of-the-art methods on ImageNet by using ResNet 18? For example, the switchable normalization.\n\n5.How about the performance of SeqNorm by using various batch sizes, e.g. from 32 to 512?  Is the performance stable? \n\n6.When the batch size is small, e,g, 32, what is the best combination of G_C and G_M? Is it similar to the combination of using a \nlarge batch size, e.g. 512?\n\n7.How about the performance gain by using deeper networks, such as ResNet50 or ResNet101?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Straightforward method but has some flaws",
            "review": "This paper proposes a novel normalization method termed sequential normalization, which stacks GroupNorm and GhostNorm into a single layer to improve the performance of neural networks. The authors demonstrate the generalization ability of GhostNorm and proposed SeqNorm in an experimental and visualization way. The experimental results on various image classification demonstrate the better performance of SeqNorm compared with BatchNorm and GhostNorm. \n\n\nThe proposed idea is straightforward and achieves better performance than BatchNorm and GhostNorm. But it still has some problems preventing its publication.\n\n\n- In the last two sentences of Sec1, the authors say “This degradation has been widely associated with BatchNorm computing poorer estimates of mean and variance due to having a smaller sample size. However, the recent demonstration of the effectiveness of GhostNorm comes in antithesis with the above belief”. I don’t agree. Actually,  GhostNorm has been proposed to separate the large mini-batch into smaller ones to introduce some “noise” to improve the generalization ability. When the original mini-batch is small, the effectiveness of GhostNorm is limited.\n\n- The contributions should be re-summarized. (1) In my opinion, “visualizing the loss landscape” should be the evidence to illustrate the advantage of your model but not the main contribution. (2) The authors claim that they identified a source of regularization in GhostNorm that is fundamentally different from BatchNorm. However, in Sec2.2, they summarize that GhostNorm provides a regularization that is embedded during learning, rather than across learning just like BN. I think this is a process difference. Could you provide more theoretical explanations? (3) I think the authors should discuss more the proposed SeqNorm but not the GhostNorm.\n\n- The experiment results are still not convincing. (1) I think one of the critical baseline methods should be the normalization method proposed in Sec3.4 of [a], which is also mentioned in the third paragraph of Sec.1.1. (2) The proposed SeqNorm should be also compared with SOTA methods, such as [b][c][d]. (3) Some details of the experiment settings are missing. For example, when training neural networks with BN, what is the batch size? 512? If the batch size is too large, it is detrimental to the performance of BN, especially with the large spatial size (ImageNet). How about the performance of BN and GhostNorm and SeqNorm with the total batch size 128? (4) Is the proposed method effective on different network architectures, such as MoblieNet and ShuffleNet?\n\n\n[a] FOUR THINGS EVERYONE SHOULD KNOW TO IMPROVE BATCH NORMALIZATION, ICLR2020\n[b] Differentiable Learning-to-Normalize via Switchable Normalization, ICLR2019\n[c] Group Normalization, ECCV2018\n[d] Group Whitening: Balancing Learning Efficiency and Representational Capacity, Arxiv 2020",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Combining normalization techniques in sequence improves imagenet training (for ResNet-18)",
            "review": "In this paper, the authors propose sequence normalization: a normalization technique that combines Ghost Norm and Group Norm in sequence. They use this to show improved results on ImageNet for ResNet-18 (henceforth RN18). Overall, this paper mixes some interesting and impressive results with fairly weak analysis.\n\nOn the positive side, the authors show that combining batch norm (using a ghost-norm version thereof) and group norm can exceed vanilla BN's performance, which is an impressive achievement, especially on well-studied datasets like ImageNet. The authors should also be commended for presenting results on additional imagenet testing sets ImageNet, for which their method is also promising (with some caveats mentioned later below). The paper contains some interesting observations and results, including that smoothness of the loss landscape may not be as important for optimization and generalization as has been previously thought. \n\nDespite these points, there are major flaws with the current version of the work. One such flaw is that it surveys such a very small number of architectures. For imagenet, only RN18 is studied, meaning that we don’t know if the findings generalize, even to other resnets! One of the strengths of vanilla batch norm is its wide utility — for instance, it helps resnets of all sizes. In this work, the authors have not even shown that their normalization improves resnet-50 (probably the most commonly-used classification network).\n\nAdditionally, the reporting of results, in general, is not satisfying. In Table 2, the results are reported with a mean ± standard deviation — however, with only two runs, it would be better to report the raw results. Also, statistically speaking, the results do not support the conclusions: it simply cannot be said with the results presented that 68.1 ± 6.8 is better than 67.0 ± 6.9. For imagenet, results should be reported separately per test set used.  Are there interesting patterns here? Does being ‘a better model’ mean you’re better at all the test sets? Why is the variance so high in the test set?\n\nIn general, the authors don’t consider the computational costs of applying multiple norms in sequence, which would allow a fairer comparison of the benefits of seqnorm.  Another missing insight is seqnorm’s effect on optimization, in the form of training curves. This information may also shed some light on the authors’ choice of using the high value of 250 training epochs — are the effects still beneficial for 100 epoch training, say? I would expect so, as the authors used 50 epoch runs to verify parameter choices, but it is still important to know. Choices of architecture and training parameters are often not explained, either. For CIFAR loss landscape visualization, they use RN-56, whereas, for normal training, they use a wide resnet. For imagenet, it is RN-18. Even when it comes to the critical parameters of the number of ghost batches, the interesting discussion of the precise mechanisms of how to choose the number in a ghost batch is not explored. Does it depend on the size of the dataset, for instance? One might suspect that the size of the groups, rather than the number of groups, is a more important factor in the analysis. \n\nSome claims lack evidence. For instance, the claims of regularization in §2.2 are not really explored empirically or theoretically. At the end, they claim that they have provided ‘novel insight on the source of regularization’, but I think that is exaggerated. For normalization being such a fertile subfield, I would expect a wider range of references. E.G. the paper talks quite a lot about regularization but does not cite ‘Towards Understanding Regularization in Batch Normalization’ (Luo et al., 2018). Similarly, “Micro-Batch Training with Batch-Channel Normalization and Weight Standardization” (Qiao et al., 2019) presents an approach that uses BN and GN in sequence (the authors here cite an earlier version of this work and don't highlight the similarities to this method). \n\nMore minor points: \n* Why is a theoretical analysis of SeqNorm vs BN ‘beyond the scope of this work’?\n* The use of ‘noisy batch norm’ as a baseline has no presented justification — it is confusing for the reader.\n* The sequential approach for hyperparameter tuning could be backed up by experiments on CIFAR-10, which is quick to train. You could run the entire sweep and check that the optimum was findable by your approach without it being too computationally expensive.\n* Parts of the text are poorly-written, e.g.\n   “adoption of GhostNorm has been rather scarce, and narrow to large batch size training regimes’ — do you mean ‘and mainly restricted to large batch size’?\n ‘we walk towards the gradient direction’ — the wording is too casual.\n* Section 1.1 is repetitive —\n\tParagraph 2: ‘closest in spirit is... (Summers & Dineen)’.\n\tParagraph 3: ’closest line of work to SeqNorm is ... (Summers & Dineen).\n* Figure 1 is nicely-presented, except for instance norm (hard to see what is going on in that diagram)\n* Please repeat the procedure used for Figures [2,3] in the caption: it should be obvious that these are not just loss curves (and are, in fact, samples from what the loss would be after updating with different learning rates).",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}