{
    "Decision": "",
    "Reviews": [
        {
            "title": "A new solution with bag of models and Bayesian neural network to the continual learning problem",
            "review": "The paper studies the problem of task free continual learning without storing exemplars from previous tasks. The paper assumes no information on task boundaries and no information on task identity at test time which brings an advantage and flexibility. \nThe proposed solution recognizes task boundaries by the difference on the loss of a window of new batches and previous loss mean. \nwhen a new task is detected, the trained model is stored and a new model is trained for the new task. \nAt deployment time, all models are evaluated and the prediction of the model with the lowest uncertainty is used.\nExperiments on MNIST like benchmarks and SVHN with MLP shows performance improvements of the proposed approach compared to existing methods.\nThe paper compared training time to other methods but neglects the most important deployment time as all models are evaluated. When using a larger model than MLP for complex data deploying many models is a problem.\nThe idea of detecting task boundaries seems very similar to that of (Aljundi et al., 2019a).\nThe detection of a task should be crucial component and it is performance is not shown. Imagine we are in a incremental classification scenario, what do we do? is the last layer already initialized with all number of classes? Further if we fail to detect a task then the solution will be prone to catastrophic forgetting of the previous task.\nWhile not using task information, an implicit assumption is used that a task stays long enough for the loss to decrease and stabilize. \nThe reduction of the stored models seems risky and also prone to result in a catastrophic forgetting when a task ends up without an expert model. \n\nthe method is restricted to a mini batch predication which might be limiting in practice.\ndoes that 10 epochs training mean here that the same batch of a task is seen 10 times?\n\nIn the main experiments the number of models is equal to the number of tasks. Only one small experiments is done on rotated MNIST with smaller number. This crucial to the method and should be studied more. \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Promising new continual learning method with issues in methodology and evaluation",
            "review": "Paper summary\n==============\nThis paper presents a novel approach to tackle the catastrophic forgetting problem of neural networks via continual learning, focusing on the scenario where the boundaries between the different tasks are unknown both during training and inference time.\nThe main idea of the proposed approach is to train multiple Bayesian neural network (BNN) models, with each model being an expert for solving one or multiple similar tasks; this allows the method to function without requiring storage of past training data.\nThe paper describes a way to detect the task boundaries in an online fashion via a threshold on the difference in training log-likelihood across several mini-batches of data.\nIt then presents an approach to decide which of the stored model(s) to discard given a limited storage budget, which is based on the similarity of the model's predictions to reduce redundancy.\nAt evaluation time, predictions are then made by the single expert BNN with the smallest predictive uncertainty.\nFinally, the paper presents empirical evidence on a variety of commonly-used continual learning benchmarks that the proposed approach outperforms several baseline methods in terms of performance as well as memory and time requirement.\n\n\nPros\n====\n* the paper tackles an important open problem in deep learning, namely to enable neural networks to learn continually without forgetting previous acquired knowledge\n* the proposed method is novel, conceptually simple, easy to implement and efficient (in terms of both computation and memory)\n* the approach appears to perform well on a variety of relevant benchmark tasks, comparing favourably against previous continual learning methods (that are designed to work both with and without knowledge of the task boundaries)\n* the method does not require storage of past data and knowledge of task boundaries, widening its applicability\n* the paper is overall well-written and easy to follow\n\n\nCons\n====\n* the proposed method does not seem very principled; the described approaches to task boundary detection, model management and inference employ simple heuristics that are intuitively sensible, but only empirically justified, without any theoretical backing as to why they are supposed to work; thus, while the overall idea is interesting, I view the methodological contribution to be somewhat low\n* the datasets (i.e. MNIST, CIFAR10 and SVHN) and models (i.e. small single-layer neural networks and LeNet-5) used in the experiments are rather small-scale; while strong performance is shown in these settings, it is not clear if those findings translate to more realistic, larger-scale benchmarks (e.g. ImageNet) and model architectures (e.g. ResNet-50); this would be less of an issue if the proposed method was more rigorous and principled (i.e. if there was a stronger methodological contribution), but given the empirically driven nature of the method, stronger empirical evidence would be necessary to convincingly demonstrate that the proposed method is a suitable solution to real-world continual learning problems\n* the approach is fundamentally based on the assumption that it is feasible to store more than a single model, and it would likely perform badly if this is not the case; more generally, to perform well, the method seems to require storage of as many models as there are distinct tasks; again, while this might be feasible/realistic for the somewhat small-scale benchmark tasks considered in the experiments, it might not be for the large, state-of-the-art architectures that would be required to get competitive accuracy on more realistic benchmark tasks\n* it seems unsatisfactory that the proposed method can only be run with knowledge transfer either fully enabled (i.e. Ours-T) or fully disabled (i.e. Ours-NT); as the empirical results show, either approach can be superior to the other (with substantial differences in performance), depending on the particular benchmark, and it is not fully clear in which scenarios one approach is expected to work better than the other; in particular, it does not seem clear how a practitioner would a-priori decide between the two alternatives; it would thus be useful to have a more extensive discussion on this important design choice; the presented results (e.g. Table 5) show that a different setting works better for different sequences of tasks, suggesting that a combination of both Ours-T and Ours-NT might perform better than the individual approaches; to maximise practicality, it would thus be ideal to have a systematic way of deciding in an online fashion if knowledge transfer should be performed at a task boundary or not; one seemingly natural way of implementing this in the context of this method could be to e.g. use the actual value of the difference in log-likelihoods (i.e. the quantity that is used to identify task boundaries in the first place) to quantify the similarity of the tasks and thus to decide if knowledge transfer might be beneficial or not\n* while the study of the effect of different model storage capacities on performance (Table 4) is interesting, it might be more insightful and practically relevant to fix the total number of parameters that one can afford, such that varying the storage capacity effectively trades off between the number of models stored and their respective sizes; in particular, given a fixed budget on the number of parameters one can store, it would be important to know if it is better to have fewer larger models or more smaller models; in order to provide guidance to practitioners as to how to optimally make this trade-off, it would e.g. be useful to assess the full continuum of different configurations of number of models and model size for a fixed total number of parameters (i.e., ranging from a single huge model to as many small models as there are tasks)\n* the proposed inference approach of simply selecting the model with the lowest predictive uncertainty fundamentally relies on the implicit assumption that the models are well calibrated, i.e. that their predictive distribution matches the actual confidence of the model; this approach will thus break down if one (or more) of the models are heavily overconfident in their predictions, i.e. if they have lower uncertainty in their predictions than they reasonably should; I assume that by employing Bayesian neural network models, the paper aims to improve model calibration; while it is true that BNNs are generally designed to be less overconfident than point-estimated neural networks, their actual calibration depends heavily on the combination of model architecture and inference technique used; firstly, it would thus be insightful to not just report test accuracy, but also some measure of calibration such as test log-likelihood, in order to compare the calibration of the different methods; secondly, the proposed inference technique, i.e. mean-field variational inference using a Bayes-by-backprop like technique, has been reported to be poorly calibrated for image classification settings beyond MNIST (see e.g. Ovadia et al. 2019, \"Can You Trust Your Modelâ€™s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\"); while I understand that the proposed approach can in principle be used in conjunction with other, more sophisticated Bayesian deep learning techniques, it would nevertheless be important to empirically assess how model calibration affects performance on the down-stream continual learning tasks considered\n* while the paper makes the valid assumption that past data cannot be stored (and notably performs well with that assumption in place), it would nevertheless be insightful to know how much performance is lost by this assumption, and, in turn, how much better the method could be if storage of past data were allowed (which is often feasible in practice); for example, past data could be beneficial when deciding which model to discard when the storage budget is depleted; the proposed strategy for that cannot take into account the predictions of newer models on older data (leading to infinity values in the resulting distance matrix), and it would be interesting to know how this impacts performance\n* parts of the method explanation in Section 4 are a bit convoluted and require some thought due to the variety of notation and concepts used, and could be clarified and made more digestible by accompanying diagrams (in particular the descriptions of the task boundary detection and model management)\n\n\nReview summary\n===============\nAll in all, this is an interesting, enjoyable-to-read paper that proposes a novel, simple and seemingly effective method for continual learning in deep neural networks, that has the potential to be of significant practical interest.\nHowever, I see several major issues with both the design of the method as well as the empirical evaluation (as detailed above), which make me hesitant to recommend acceptance of this work.\nI am thus inclined to recommend rejection of the paper as is, although I am happy to reconsider my judgement if the authors can convincingly counter argue some of the issues mentioned above.\n\n\nQuestions\n=========\n* I am not fully sure if I understand the measure of uncertainty you propose for selecting the expert model at inference time (Section 4.2); how exactly is the standard deviation function std_k(.) defined? its argument y_d seems to be a scalar, and it's not clear how the standard deviation of a scalar would be computed\n\n\nMinor issues\n===========\n* abstract: \"what it has learned\" --> \"what they have learned\"\n* the formatting of Table 3 is not ideal; it would improve clarity to but the benchmark names as column titles on top of the table\n* the data in Table 4 would be easier to digest if presented as a plot, e.g. with the storage capacity on the x-axis and performance on the y-axis, and different lines for the different degrees / the overall performance\n* the connection to Occam's razor is not entirely clear and could either be elaborated further, or left out entirely, as it does not appear to add much to the paper as is",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Multiple BNN's for task-free continual learning",
            "review": "Summary:\n\nThe paper proposes to use multiple Bayesian neural networks for the task-free continual learning problem. The main idea is to maintain multiple BNN models and determine the task change by evaluating the log-likelihood on the current training mini-batch. Mainly, a threshold $\\theta$ is used to detect significant drop in the log likelihood. When the model capacity is full, the algorithm computes the pairwise distances among the models and discards the older model of the pair that has the closest distance. For inference, the method selects a model that has the lowest uncertainty and use it for prediction. \n\nPros:\nA simple method for task-free continual learning set-up. Some promising results on simple benchmark datasets are given. \n\nCons & Questions:\n- Detecting task shift by the change of log-likelihood is not fully justified. Why should the standard deviation used as a threshold? How do you justify it? \n- How do you justify the model discarding method? When two models have small distance on a specific batch, it does not necessarily mean that they will have similar performance on other tasks as well. I think it is too heuristic. \n- What happens the number of tasks grow larger than N? How does the performance vary?\n- It seems the test time (inference time) would be quite long, which may limit the practicality of the scheme. Any results on this? \n- Knowledge transfer does not seem to always help, as authors mentioned. At least, can you characterize when it helps? Otherwise, we cannot determine which method to apply for given data and tasks. \n- The experiments seem to be only on very simple tasks - variations on MNIST. What about testing the method on Split CIFAR-100 (10 tasks with 10 classes)? How would it perform? \n- What is the connection with Occam's Razor? The connection is not clearly elaborated in the paper. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting Idea, But Erroneous Experiments",
            "review": "## Summary\nThis paper proposes a task-free continual learning method based on Bayesian neural networks.\nAlongside the conventional assumptions of task-free CL, an important assumption distinguishes this paper from others: samples in the same mini-batch are from the same task (*even at test time*).\nI will refer to this as *the homogeneous mini-batch assumption*.\n\nThe key aspects of the algorithm are summarized as follows:\n- Expansion: Once a task boundary is detected, save the current snapshot of the model.\n- Task boundary detection during training: For a certain window of time, check if the data's average log-likelihood drops significantly.\n- Task inference during test time: Compute each snapshot's uncertainty scores over the given mini-batch and select the one with the lowest score.\n\n---\n\n## Pros\n\n- Using the uncertainty estimation capability of Bayesian neural networks seems novel.\n- Exploiting the homogeneous mini-batch assumption to improve task inference accuracy is highly effective.\n\n---\n\n## Cons\n\n### Unfair comparison with the baselines\n\nThis paper reports a surprisingly large performance gap between the proposed method and the baselines.\nAccording to the last paragraph of section 4, however, the proposed model utilizes the homogeneous mini-batch assumption even at test time. The provided evaluation code also confirms this. However, this assumption is not used in other baselines, especially in CN-DPM, which has to perform task inference similar to the proposed method.\n\nIn fact, the homogeneous mini-batch assumption will greatly improve the performance of *any* task inference method.\nEven if the individual task inference results are extremely noisy, the average of multiple (e.g., 64 in this paper) inference results is a very stable estimator.\nAccording to Table 3 of the CN-DPM paper, its main performance bottleneck is the task inference accuracy.\nTherefore, it is obvious that the homogeneous mini-batch assumption will significantly boost CN-DPM's task inference accuracy and overall performance.\n\nFor a fair comparison, I request the authors to conduct experiments that infer task ID without the homogeneous mini-batch assumption (at least at test time).\nI think the consensus on the uncertainty estimations of BNNs is that they are unreliable [1].\nFor this reason, I have a strong suspicion that the task inference relying on the uncertainty of a single example would be extremely inaccurate.\n\nAdditionally, reporting the task inference accuracy of the method will provide better insight into the model.\n\n[1] Jiayu Yao, Weiwei Pan, Soumya Ghosh, Finale Doshi-Velez. Quality of Uncertainty Quantification for Bayesian Neural Network Inference. ICML 2019 Workshop on Uncertainty and Robustness in Deep Learning.\n\n### Unverified capacity reduction algorithm\n\nAccording to the authors' claim, one of the proposed method's advantages is that the model has a finite capacity.\nThey also allocate a significant portion of the paper to describe the algorithm that selects a module to be deleted once the model capacity is reached.\n\nHowever, simply deleting the model will cause a significant loss of accuracy. Nonetheless, there is no treatment to handle such problems.\n\nAlso, no experiment demonstrates the efficacy of the deletion process. In every experiment, the maximum number of modules is set to the number of tasks.\n\n\n### Minor Issues\n\n- I do not see any strong connection to Occam's razor.\n- The title needs to be more specific.\n\n---\n\n## Overall score\n\nAlthough this paper's main idea is worth investigating, the authors committed serious errors in the main experiment and overclaimed their model's relative performance. I suspect that the accuracy would be far too low without the homogeneous mini-batch assumption at test time. Also, they propose an algorithm to keep the model capacity finite, but it does not have any experimental evidence and is suspected of suffering from a serious performance drop. Therefore, I recommend rejection.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}