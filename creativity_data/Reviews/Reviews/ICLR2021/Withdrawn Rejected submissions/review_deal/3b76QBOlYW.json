{
    "Decision": "",
    "Reviews": [
        {
            "title": "Low quality results. Limited evidence method actually helps.",
            "review": "### Summary\nThis paper introduced an iterative neural network architecture inspired by the GS phase retrieval algorithm. The algorithm is applied to (simulated) multiplane computer generated holography. The proposed method is trained on MNIST and tested on simulated MNIST and fashion MNIST data.\n\n### Strengths\nThe paper is well written and tackles a challenging problem. The idea of unrolling GS is interesting.\n\n### Weaknesses\nQuality: The reconstructions presented in Fig 4 are of very low quality. I don't see any benefit of the proposed method over GS or l2-grad. With the exception of the SSIM metric (which benefits immensely from setting the border of an image to 0), the proposed method works no better than existing methods.\n\nVery low resolution: Present results may be meaningful for acoustic CGH, but not for holographic displays. \n\nOnly trained on MNIST. Given the poor reconstructions in Fig 4(b), I wouldn't say the method successfully generalizes.\n\n### Recommendation\nWhile the concept of an unrolled GS algorithm is interesting, the proposed algorithms isn't really that (see comments). More importantly, the proposed method is only tested on toy datasets and the results, in my opinion, look to be of very low quality. It would be more informative to also include results with one, two, and three target planes, which presumably would look better and also show an improvement. As presented, I see little evidence the proposed method actually works.\n\n### Comments\nGS is not state-of-the-art for CGH. See for example [A]\nChakravarthula, P., Peng, Y., Kollin, J., Fuchs, H., & Heide, F. (2019). Wirtinger holography for near-eye displays. ACM Transactions on Graphics (TOG), 38(6), 1-13.\n\nMay be worth citing recent work on CGH with neural networks [B],[C].\n[B] Peng, Y., Choi, S., Padmanaban, N., Kim, J., & Wetzstein, G. (2020). Neural holography. In ACM SIGGRAPH 2020 Emerging Technologies (pp. 1-2).\n[C] Eybposh, M. H., Caira, N. W., Atisa, M., Chakravarthula, P., & Pégard, N. C. (2020). DeepCGH: 3D computer-generated holography using deep learning. Optics Express, 28(18), 26636-26650.\n\nAre the target images provided to C_forward CNN as extra channels? It's unclear based on the text.\n\nI would argue the skip connections, which add f_{n-1} to the output of the network, make this network structure fundamentally different than GS and closer in form to HIO.\n\nAdding row labels to Figure 4, rather than relying on captions, would make the information easier to parse.\n\n### Typos\nThe acronym GB (which I believe should read GS) is used before it is defined.\n\nPrevious work section: \"generatwthree\"\n\n\"but with include\"->\"but which include\"",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting method unrolling Gerchberg-Saxton, more network details would help",
            "review": "### Summary:\n\nThis paper proposes a learning-based method to design phase plates for computer generated holography. Specifically, the authors address the (harder) case where a single phase plate must be used to generate different images at different distances behind the plate at the same time. This inverse problem is challenging because phase pattern hologram and image plane amplitude constraints must be met which results in a nonconvex optimization problem.\n\nThe proposed method is based on the traditional iterative Gerchberg-Saxton algorithm. They unroll the algorithm into blocks that form a network. Learnable components are added to each block and the network is trained. Experimentally their method produces better quality images than the traditional methods. Other reasons for using the proposed method is that it is faster at solving the inverse problem and generalizes outside the training distribution.\n\n\n########################################\n\n### Strong points:\n\nIn showing how to unroll the Gerchberg-Saxton algorithm as a network with learnable parameters the authors have proposed a method that combines physical information (propagation operator, A_i) and a widely used traditional method (Gerchberg-Saxton) with learning. Given that this is a physical imaging problem and the operator is known, I believe it is a good idea to incorporate this in the learning method.\n\nThe method is explained well in Section 2.\n\nThe experiments use real optical hardware to evaluate the method and display images. This demonstrates the solution can be used in a real setting.\n\nMultiple loss functions are motivated and considered which help to find a good set of parameters for the network.\n\nThe authors demonstrate out of distribution generalization.\n\n\n########################################\n\n### Weak points:\n\nIt would be helpful to discuss and explore some aspects of the network in more detail:\n\n\\- What is the significance of C_forward (rather than simply using Step 2 of Gerchberg-Saxton on page 3)? C_forward modifies the phase of the field in the image space. However, as we wish to determine the phase of the field in the hologram space, why not only modify the field in the hologram space like C_backward does? How does modifying the phase of the field in the image space help? What if we remove C_forward?\n\n\\- How do the residual connections help? Step 4 of the Gerchberg-Saxton iterations on page 3 does not add the result from the previous iteration.\n\n\\- Please can the authors clarify how exactly the network handles the case when different images at different distances need to be shown at the same time? Figure 2 only shows a single $A$ and its inverse $A^{-1}$---does the figure need to be updated?. According to Equations 3 and 4, $A$ is dependent on the distance, $z$ and so a single $A$ would not work. Is the solution to use different $A_i$s to propagate the source plane field into every image plane (as mentioned at the end of Section 2 for the traditional method)?\n\n\\- How does the initialization for the traditional method at the top of page 3 differ to the zeros initialization mentioned in the caption for Figure 2 for the networks? A zeros initialization for the traditional method (top of page 3) results in zeros being iterated and so would not be feasible. I understand that the convolution layers may have some bias term and so a zeros initialization could work for the networks. However, given the nonconvexity of the optimization problem (equation 2), would a smarter initialization such as what the authors choose to use for the traditional method help the learned methods (in terms of accuracy or number of required blocks for example)?\n\n\\- The phases of the elements of the complex field that we aim to recover should be in $[0, 2\\pi)$ because a phase of $\\pi$ is the same as $3\\pi$. Is this enforced anywhere in the output of C_backward which is the phase of the hologram plate?\n\n\\- What is P_Transducer in Figure 2?\n\nThe abstract mentions that the inverse operator is estimated and the end of section 3 mentions that the authors directly invert the forward operator. Please could the authors clarify these claims? This method seems more about learning a nonlinear mapping that solves the inverse problem which is not necessarily the inversion of the forward operator.\n\n\n########################################\n\n### Recommendation:\n\nOverall I lean towards rejection though I like the idea of combining physics knowledge with learning to solve this inverse problem. I have also not seen the Gerchberg-Saxton method unrolled before. However, I believe that more details about the network structure need to be justified and its performance better understood.\n\n\n########################################\n\n### Additional questions and clarifications that will help assessment:\n\nPlease address and clarify my previous questions regarding the network structure and the estimation of an inverse operator.\n\nIt would also be helpful to understand (ideally with experiments) how the performance of the network varies as compared to the baselines when the ill-posedness or difficulty of the problem changes. If it is challenging to do this with the hardware, simulations would be useful. Given the hard nature of the problem, a more well posed simulation may also enable better results to be showcased. There are multiple ways to explore ill-posedness, some potential ideas are:\n\n\\- Vary the number of output images between 1 (easier case) to a larger number (harder case). The paper only uses four images.\n\n\\- Vary the spacing between output images. Smaller spacing is more challenging and larger spacing is easier. This would change $A_i$. \n\n\\- Furthermore what if $A_i$ at training is not exact and at test time different $A_i$ are used?\n\nIn the abstract the authors suggest that their method should generalize better outside of distribution as compared to other learning based approaches. Do the authors have experiments showing this? Or an understanding of the conditions under which this is more likely to happen ( for example is it smaller image spacing, more output images, less training data, something else…)? \n\nIt would be helpful to explain the experimental process in more detail, especially the role of hardware. For example, what is the value of u(x,y) used? MNIST images are normally 28x28, so were they upsampled or zero-padded to make them 60x60? Are the results in Figures 3 and 4 based on simulation or reconstruction with hardware that was done after designing a real hologram plate using the result of the different methods?\n\n\n########################################\n\n### Additional feedback that does not necessarily impact recommendation:\n\nThe top paragraph on page 6 says that the MNIST results are in Figures 3a, b and c. However, the captions suggest looking at Figures 3a, c and e. Similarly the Fashion MNIST results cited in the text do not match the captions in Figure 3.\n\nFigure 3 axis says LRGB, should this be LRGS?\n\nPlease proof-read the submission for consistency across sections. For example, it would be helpful to change g_goal to g_target to match section 2. Furthermore in equations 5, 6 and 7, it seems that N is the same as p in Figure 2 (number of amplitude images), however, in Figure 2 N is also the number of blocks / depth of the network.\n\nDo the loss functions need to be divided by the dataset or batch size? \nFurthermore are AF_N and g_goal,i vectors or 2D? If so, should the loss functions have norms instead of regular brackets (like equation 2)?\n\nPlease mention the distances behind the hologram plate in Figure 4? \n\nTypos:\n\n\\- Page 1, constrints\n\n\\- Page 2, i in ‘i-th image plane’ not in math mode\n\n\\- Page 3, generatwthree\n\n\\- Page 5 second last line, where. Also on page 6 fourth line.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some elements of this paper are unclear for me",
            "review": "Summary of contributions: In this paper, the authors propose an unrolled model inspired from the Gerchberg-Saxton algorithm which is a classic algorithm for computer generated holography. The model is trained for the generation of multi focus holograms. The authors claim that it outperforms existing methods both in terms of image quality and inference speed. \n\nStrengths: \n\n1)To the best of my knowledge, unrolling of the  GS algorithm seems to be new.\n\n2)The method is superior to the algorithms compared in this paper, both in terms of inference speed and image quality. \n\nWeaknesses: \n\n1)I'm far from being an expert in this field so I may have not understood well some parts of this paper.  But I think that the writing could be largely improved : after several readings, they are few points that still remain unclear. Some mathematical notations are not clear/well-defined and makes the paper difficult to parse. \n-What does P_transducer mean in figure 2 ? It is never explicitly defined in the main text.\n-Section 4.1: why is it question of acoustic waves ? This is never discussed earlier. Is it to simplify the simulation process ? More details regarding this point are necessary.  \n-Section 4.2, the notation seems to be ambiguous in equation 5 : isn't it |A_i f_N| rather than |A f_N|_i ? tracing back at equations (2) and (4) it seems that A is different for each plane since it depends on the image plane location.\n-Looking at equations (5,6,7):  it is the sum of N terms. Does it correspond to the K planes or to the N iterations of the iterative algorithm mentioned earlier (if it corresponds as I presume to the first case, the notations are very misleading). -Furthermore I don't understand the motivation for the proposed losses.\n\n2)I also have a few  concerns regarding the experimental setups. \n\n-A comparison with other neural network based approaches seems to be necessary to assess the effectiveness of the proposed GS unrolled architecture. I understand that the method from Horisaki et al. can only generates holograms with one focus plane. Why does the authors run experiments with 4 different planes only ? Wouldn't it be possible to compare their method to previous neural network based methods in the 1 plane setting ? \n-I do not understand the two proposed losses. Is it a common metric to assess holograms quality ?  \n-Qualitatively speaking, in figure 4) it seems to me that Results from LRGS are worse than NOVO-CGH and l2-grad (the number 2 is not readable). \n-Regarding the inference speed comparison : p5 \" The iterative optimization schemes where run for 1000 iterations\", p8: 8 \"The iterative algorithms were run for a maximum of 500 iterations.\" why the number of iterations is different for these 2 experiments ? Is this a fair comparison ? \n\nOther comments :\n\nCan you increase the number of unrolled iterations to more than 10 ? sharing weights ? Are weights parameters tied accross each GS iteration ? \n\np5 \"Since the loss function [...]\" why is this the case ? This is never discussed earlier in the paper. \n\np5 \"Almost self supervised setup\" ? What do you mean ? I do not understand.\n\nBecause of the difficulty to parse clearly the contributions of this paper, I rate the paper as borderline reject in this round. \n\nTypos:\n\np1: \"various constrints\"\n\np3: \"y the problem of generatwthree-dimensional\"\n\np3 \"projections scheme,this functional\"\n\np6: \"can be seen in figure 3 a), b) c)\" => a) c) e) ? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting and relevant inverse problem, but no ML methodological novelty",
            "review": "This is an interesting paper which proposes a deep neural network to design holographic plates. This is an ill-posed inverse problem with applications in optics and acoustics. The proposed network is physics-based in that it uses the forward operator in the loop which helps promote data consistency. The authors address a challenging variant of the problem which prescribes multiple amplitude distributions at multiple distances from the plate.\n\nA. Strong points:\n\n- The paper is clearly written and the numerics on simple MNIST and Fashion MNIST shapes are convincing\n- The addressed problem is relevant for the optics and the acoustics communities\n\nB. Weak points:\n\n- The main problem with the paper is that there is no innovation in the machine learning / deep learning pipeline. Methods that are by now standard (and many of which are cited by the authors) are adapted to a new application by plugging in a different operator. I believe this paper could be of significant interest for the optics or the underwater acoustics community and that it would fare very well in their journals and conferences. It is however my opinion that it is not a good fit for ICLR.\n\nC. Recommendation:\n\nUnfortunately, per my explanation above, I cannot recommend the paper for acceptance. I do believe that it would be a fine contribution to an optics or acoustics journal, especially if combined with real experiments.\n\nD. Questions / suggestions:\n\n- I would recommend adding LISTA to the literature overview as it was the first deep unrolling of an optimization algorithm for a regularized loss.\n\nE. Minor points I caught:\n\n- p1, abstract: ill-defined -> ill-posed\n- p1: \"subjecto to various constrints\" -> constraints\n- p3: \"Recently the problem of generatwthree-dimensional\" -> broken sentence\n- p3: \"but with include\" -> \"but which include\"\n- p5, bottom: \"where run\" -> \"were run\"; in several other places \"where\" should be replaced by \"were\"\n- p5, bottom: \"digit mnist\" -> \"MNIST digit\"\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}