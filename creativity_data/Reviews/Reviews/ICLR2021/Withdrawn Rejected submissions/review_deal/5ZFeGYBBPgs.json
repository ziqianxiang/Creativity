{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Dear Authors,\n\nThank you very much for your very detailed feedback and also updating the manuscript in the rebuttal phase. Your effort has highly contributed to clarifying some of the concerns raised by the reviewers and improving our understanding of your work. \n\nOn the other hand, we still think that the current work has rather limited novelty, and motivation and theoretical justification need to be further enhanced to be accepted for ICLR.\n\nFor these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. The reviewers added further comments after receiving your feedback. I hope their comments are useful for improving your work for future publication."
    },
    "Reviews": [
        {
            "title": "Interesting simple idea, some doubts regarding evaluation",
            "review": "##### Summary\nThe paper proposes a variant of Monte-Carlo Dropout that aims to obtain\nbetter uncertainty estimates by a better adjustment of the output variance. In\nconcrete, the paper proposes a novel objective function consisting of two\nterms. The first term is a simple regression loss between the model expected\noutput and the data label. The second term, named \"second-moment loss\" penalizes\nthe differences in the gap between the expected and sample output and the gap\nbetween the expected output and the data label. In other words, the output\nshould match the data label in expectation, and the output variance produced by\nDropout \"sub-networks\" should follow the variance of the prediction residuals,\nas a proxy for aleatoric uncertainty.\n\n\n##### Pros\n- I think the idea overall makes sense, it is simple to implement and could be\n  an interesting addition to the MC Dropout toolkit.\n- Experimental results on toy datasets show the models trained with the proposed\n  loss are able to effectively capture the aleatoric uncertainty.\n- On real datasets, the proposed method is compared to many uncertainty\n  estimation methods and shows competitive performance.\n\n##### Cons\n- One potential pitfall could happen when the expected output is far from\n  correct due to model limitations and the random \"sub-network\" predictions\n  become heavily biased. I think further discussion on this issue would be\n  welcome.\n- The paper is in general correctly structured but leaves some important details\n  to the appendix. Maybe it is because my lack of familiarity with these\n  evaluation metrics, but in my opinion it may be good to clarify them in the\n  main paper. For example:\n    - The \"normalized residuals\" $r_i$ in page 6 are not properly introduced in\n      the main text.\n    - Isn't the ECE, as defined in the appendix, a measure of fit of the\n      residuals to a uniform distribution? If yes, why is this a reasonable\n      assumption and why would this indicate a correctly characterized\n      uncertainty?\n    - Are the $\\mu_i$ and $\\sigma_i$ in the evaluation metrics computed from the\n      empirical predictions or from the data? If computed from the predictions,\n      then the proposed Wasserstein metric looks like a measure of\n      Gaussianity. This would be in contradiction with the ECE metric, and the\n      same questions as in the previous point would apply: why is Gaussianity a\n      reasonable model for the residuals and a desiderata for the output\n      distribution? what is the intuition for why this test indicates good\n      uncertainty estimates?\n- For a fair comparison with ensemble methods, the total number of parameters\n  and operations should be taken into account.\n\n\n**************\nAfter Rebuttal:\nI thank the authors for their extensive answers and clarifications.\nOverall, I maintain my positive outlook on this work. Although theoretical justification could be improved, I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Second-Moment Loss",
            "review": "The paper proposes an objective function i.e., second-moment loss (SML) to better evaluate the uncertainty based on MC dropout. A full network is used to model the mean, while sub-networks are explicitly used to optimize the model variance.\n\n\nAccording to the claim, it seems the main novelty is introducing a new network for variance. First, it is not new that using one network for accuracy (e.g., mean) and another for uncertainty (e.g., variance). There are existing models that should be compared and discussed. \n\nThe authors claim that the model can be to adaptive to domain shift, but there is no explanation why the model can do this?\n\nIt is unclear or at least not well motivated why the authors proposed the second-term-moment and why it is advantageous than others. The authors should discuss more recent proposed methods in introduction to show the neccessarity or clear advantages for the proposed one.\n\nIt is unclear what the gray lines (sub-networks) mean in Fig. 1? And how to obtain the gray lines? It is better to explain in the introduction or caption of fig. 1.\n\nThe authors used “prediction uncertainty” and “data-independent uncertainty”. What is the relationship between predictive uncertainty and epistemic uncertainty? What is data-inherent uncertainty? what the relationships between it and aleatoric uncertainty?  Why the authors call the distance $|f_\\tilde{\\theta} – f_ {\\theta} |$ aleatoric uncertainty?\n\nAlthough the objective is simple, in the current state, the model is still not clear enough to follow. \n\n----------------------------------------------------------------------\nUpdate after rebuttal\n----------------------------------------------------------------------\nThanks for the feedback from the authors.\n\nUnfortunately, although some parts are clarified, the main issues still exist. Overall, the novelty is limited and the motivation is still not clear enough.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new loss function for better uncertainty estimation.",
            "review": "##########################################################################\n\nSummary:\n\nThe paper proposes a new objective function to prevent underestimating uncertainties. The authors claimed that the new form leads to state-of-the-art performance in several numerical experiments.\n\n##########################################################################\n\nPros: \n\n- A new objective function is easy to understand.\n- The motivation for the work is sensible.\n\n##########################################################################\n\nCons: \n\n- Although the motivation of the SML loss in Section 3 is sensible, but the theoretical grounds for the SML loss look somewhat weak. MC dropout (Gal & Ghahramani, 2016) is to maximize the evidence lower bound and learns Bayesian neural networks. What are the properties of the optimal solution of the proposed objective function?\n- Following the question, is it restricted to the dropout networks? How can this new loss function be applied other than the dropout networks?\n- The authors use the (gaussian) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures. In case of MC dropout, the authors use posterior mean estimates $E(E(Y |\\theta, X))$ for $\\mu$, but a conditional posterior variance estimates $Var(E( Y | \\theta, X))$ for $\\sigma^2$. Thus, it makes sense the MC dropout might underestimate uncertainties because $Var(E( Y | \\theta, X)) \\leq Var( Y | X)$. However, Kendall & Gal (2017), which the author cited in the manuscript, addressed this issue and provided a better uncertainty quantification method. But this method is not considered in the experiments.\n- The paper's writing makes it very hard to understand the results. For instance, implementation details for Figure 1 or its pointer are not provided. Also, Figure 3 should be improved. The current presentation is confusing and it's hard to recognize which is better.\n\n##########################################################################\n\nI vote for rejection. I may well have missed some points in my reading, so clarification is welcome.\n\n##########################################################################\n\nAfter the author response\n\nAs the authors address the reviewer's concerns, I changed the rating.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}