{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "Paper proposes a method based on JL transform to speedup DPSGD. Specifically, the method approximates the per-example gradient norm using JL transform resulting in many fold speedup compared to current implementation.\n\nI really like this paper, in my opinion improving the training speed when using DPSGD makes its application feasible in many areas where it was not possible in the past, opening doors to wider applicability of differential privacy. Assuming the privacy proofs hold, the trade-off between the projection dimension, computational time, and privacy budget seem reasonable and impressing. My only complaint about this paper is unavailability of the source code. Given that the proposed method hinges significantly on empirical results, I would have loved to see the implementation that I can run and verify on my end. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper performs JL transform to preserves the \\ell_2 norm of the gradient, but the algorithm is not clear enough.",
            "review": "The paper studies differentially private stochastic gradient descent, a recent focus in privacy preserving ML since Bassily et al. The main idea in this technique is to subsample data points and compute gradients, clip the gradient to some clipping parameter, and then add noise proportional to the clipping parameter. The paper uses JL transform, where the projection matrix has all entries iid from a normal distribution, to first project the gradient to a low dimensional subspace so that its norm is preserved. \n\nI am little confused with the paper. \nIn Algorithm 1, the comment Clip the per-sample gradients does not corresponds to clipping the gradient, but corresponds to clipping the loss function! Clipping is done on the gradient in the moment-accountant paper. \nFrom what I understand, JL transform is used to reduce the dimension of the vector by multiplying the vector in R^d by a JL matrix of dimension r \\times d. This is what the paper also does. but since the JL matrix used is a dense matrix, how does the authors claim a speed up in computation?\nIf the primary aim of JL transform was to speed up the norm computation, there are distribution of projection matrix such that the projection requires input sparsity time when the projection matrix is picked from the corresponding distribution. There is a rich literature of it, starting from Clarkson-Woodruff (STOC 2012) and Meng and Mahoney (STOC 2012). That would give a far more efficient algorithm in my opinion. \nFinally, I do not get why the privacy proof is so complicated? Also, why are we doing it through Gaussian DP when moment's account gives a tighter composition. Further, we can use the analysis of Kenthampadi et al. that JL projection + noise preserves DP. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Differentially Private-SGD (DP-SGD) and its variations are the only known algorithms for private training of large scale neural networks. This algorithm requires computation of per-sample gradients norms which is extremely slow and memory intensive in practice. In this paper, authors present a new framework to design deferentially private optimizer called DP-SGD-JL and \nDP-Adam-JL. They use Johnsonâ€“Lindenstrauss (JL) projections to quickly approximate the per-sample gradient norms without exactly computing them, thus making the training time and \nmemory requirements of optimizer closer to that of their non-DP versions. Algorithms achieve state-of-the-art privacy-vs-accuracy tradeoffs on MNIST and CIFAR10 datasets while being significantly faster. The algorithms work for any network in a black-box manner. \n\nThe main idea is to approximate the per-sample gradient norms instead of computing them exactly. Johnson-Lindenstrauss (JL) projections provide a convenient way. Moreover, there is an efficient way to obtain such projections using forward-mode auto-differentiation or Jacobian-vector product (jvp). jvp can be calculated during the forward pass making it very efficient.\n\nFor experiments, algorithms achieve training times comparable to their non-private counterparts, are significantly faster than previous algorithms. \n\nThe experiment results are quite good. Writing is good and the analysis is complete.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}