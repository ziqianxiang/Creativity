{
    "Decision": "",
    "Reviews": [
        {
            "title": "review",
            "review": "This paper proposed to use hierarchical feature fusion for global feature aggregation. Then based on these aggregated global feature, a modified SE module is proposed to enhance the ROI features. At last, the features for the same ROI from different layers in FPN are fused together for classification and regression. \n\nThe idea makes sense to me, though these across layer aggregation and so-called context aware module are adapted from well-known work, which makes the novelty faded. Coming to the experiments, this paper lacks in various aspects, which makes the evidence not solid to demonstrate the superiority of the proposed method. At least the following comparisons must have: 1) comparison with SE net. The context aware module is quite like SE module. Though the authors emphasize the difference with SEnet in introduction, we need evidence to prove its superiority. 2) Libra-RCNN. In this method, The authors also propose a simple yet effective way to aggregate global context across different FPN layer. \n\nAnother concern is that the proposed dense global context and context aware module introduce heavy computation. It is important to compare with a model with similar FLOPs and parameters. For this view, PANet and scaling the filters for FPN are two baselines must have.\n\nAbove all, I don't think the proposed method have great advantage over existing methods (0.5~1.5map improvement), but with more heavy computation. I suggest rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Needs improvement",
            "review": "This paper proposes Dense Global Context Aware (DGCA) RCNN to deal with adding global awareness to FPN, but both the novelty and experiments are less convincing to me.\n\nPros:\n1. The motivation is good.\nOriginal design of two-stage object detectors has many hand-crafted hyperparameters and bring misalignments. Lack of global awareness is one of the misalignments in feature pyramids and this paper aims to adding global awareness to tackle with this problem.\n\n2. The illustration is good and easy to follow. \nFigure 3 & Figure 4 clearly describes the design of DGCA and feature flows in the feature pyramids, which is easy to follow and understand.\n\nCons:\n1. Effectiveness and novelty of dense connections.\nIn Table 1, adding dense connections only bring 0.1 AP improvements on COCO, which is pretty incremental and lower the effectiveness of global connections. What is more, dense global connections has been widely adopted in object detection and segmentation[1][2][3], it is an incremental improvement if the differences on depends on your implementation and usage at different stages of object detectors.\n\n2. Novelty of attention module.\nIn Table 3, it seems that origin “conv” design from Senet[4] gives the best mAP performances, which makes contributions of this paper incremental, since they take reference of attention design from backbone in SENet to classification branch in two-stage detectors. It is hard to confirm superiority of their design and a better comparison is to change Resnet50 to SE-Resnet-50 as backbone, if there is still improvements under this design, it is more convincing that attention module in classification branch can help with better global awareness.\n\n3. FPN performance.\nIn Table 8, FPN-Resnet-101 only reaches 37.3 mAP on COCO test-dev, which is more like a Resnet-50 result, since FPN-Resnet-101 reaches 39.1 mAP on val2017.\n\n[1] Libra R-CNN: Towards Balanced Learning for Object Detection, in CVPR 2019\n[2] Deep High-Resolution Representation Learning for Visual Recognition, in TPAMI\n[3] Path Aggregation Network for Instance Segmentation, in CVPR 2018\n[4] Squeeze-and-Excitation Networks, in CVPR 2018\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Dense Global Context Aware RCNN for Object Detection",
            "review": "## Summary\nThe paper presents an interesting dense global context aware module for the two-stage object detectors. The motivation of the paper is clear and it provides reasonable experiments on the COCO benchmark.\n\n## Pros\n1.  The motivation of the paper is clear. The problem of missing contexting information for the two-stage detector do exist.\n2. The experiments of the proposed algorithm based on the COCO benchmark are reasonable.\n\n## Cons\n1. The presentation of paper should be improved. For example, the discussion of  Dense Global Context (Section 3.2) is not clear. Especially, for Fig. 3, it does not provide sufficient information to illustrate the proposed algorithm. Also, there is no \"p2, p3, p4, p5\" in Fig. 3. \n2. The improvements based on the experiments are not attractive. For example, in Table 7, the improvement on res101 is only 0.6. More importantly, the proposed algoritm may involve more computational cost, it would be better to provide the FLOPs comparison between different algorithms. Also, I would suggest to have a fair comparison based on the similar FLOPs based on the baseline. \n3. The novelty of the paper is not significant. It is interesting to model the global context information. But the proposed algorithm provides an incremental approach to address this problem. For example, the attention module in Fig. 4 is similar to the SE module. The proposed dense global context seems to be a little bit complex to employ. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important problem but poor novelty & limited comparisons/analyses",
            "review": "In this paper, the authors study an important problem in object detection: The problem of integrating context into the detection pipeline. The authors start first by criticising the contextually-limited nature of RoIPool/RoIAlign operation used in object detectors. Then, they propose extracting features from whole layers in FPN and using those features for contextual modulation of pooled features by an attention mechanism.\n\nStrengths:\n- Context-awareness is an important problem in object detection. Tackling such an important problem and trying to bring in some perspectives are important.\n- +2AP improvement achieved compared to the baseline.\n\nWeaknesses:\n- There are many studies trying to integrate into context awareness and frankly, it is not clear what the novelty is. There are many studies that extract global features and use them as attention maps for contextual modulation, e.g.\nhttps://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf\n\nhttps://openaccess.thecvf.com/content_ECCV_2018/papers/Zhe_Chen_Context_Refinement_for_ECCV_2018_paper.pdf\n\nhttps://arxiv.org/pdf/1807.00119.pdf\n\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9191309&tag=1\n\nIt is unfortunate that the paper just discards this wide literature by simply saying that existing methods just use context to perform feature calibration, which is very unclear.\n\n- Apart from positioning itself with the existing methods in terms of novelty, the paper fails to provide an extensive experimental analysis/comparison with respect to them.\n\n- The use of the proposed mechanism with one-stage detectors should have been experimentally investigated as well.\n\n- Comparison with state of the art is veyr poor and a comparison with stronger detectors is missing.\n\n- The hypothesis that features inside a proposal only belong to that object is incorrect. The features extracted via deep backbone CNNs carry information from other regions of the image as well.\n\n- \"However, using dense connection can integrate global context information at\ndifferent stages, so the model performance is better improved(outperforms FPN basline by 1.0% on COCO’s standard AP metric and by 2.0% on AP@IoU=0.5).\" => However, it does not improve over not using dense connection. Therefore, the claim that \"dense connection can integrate global context information at different stages\" is unjustified.\n\n- It is not clear what is mean by \"FPN baseline\". Please replace this with Faster R-CNN with FPN. FPN can be used with any detection method and therefore, FPN baseline is not clear.\n\n- The quality of the written text and the figures (see below) is very very poor and far from being acceptable to a conference like ICLR.\n\n\nMinor comments:\n- \"RoIPool/RoIAlign is an indispensable process for the typical two-stage object detection algorithm, it is used to rescale the object proposal cropped from the feature pyramid to generate a fixed size feature map. However, these cropped feature maps of local receptive fields will heavily lose global context information.\" => This process / problem is not unique to FPNs.\n- \"In FPN, it adaptively crop the regions of interest from the feature pyramid\" => Again, this is not limited to FPNs.\n- \"further leverage the attention mechanism to perform global context aware.\" => Please rephrase.\n- \"popular two-stage usually use\" => \"popular two-stage methods usually use\".\n- \"it adaptively crop the regions\" => \"it adaptively crops the regions\".\n- \"But is it reasonable to use only features of local receptive field like object proposal for classification and positioning?\" => Please rephrase.\n- \"the target object has a potential relationship with other objects\nin the background.\" => This is not limited to only background. A potential relationship can exist with other foreground objects as well.\n- \"section 4\" => Please capitalise the first character of Section, Figure, Table words.\n- \"we extent\" => \"we extend\".\n- \"RPN(Region\" => Please leave space before opening parenthesis.\n- \"take advantage of the nature of the anchor box can formed by keypoint and optimize\" => Please rephrase.\n- \"(Wu et al., 2020b) studies\" => \"Wu et al. (2020b) study\". Please read the author guidelines carefully.\n- \"2.2 Context Aware\" => \"2.2 Context Awareness\". Similar problem with Section 3.3.\n- \"the two stage object detection task\" => Object detection is the task, two-stage object detection is a method.\n- \"Fig. 2. Where Bottom-up\" => \"Fig. 2 where Bottom-up\".\n- \"fc2; c3; c4; c5g. Where Top-down\" => \"fc2; c3; c4; c5g where Top-down\".\n- \"fp2; p3; p4; p5g. Where\" => \"fp2; p3; p4; p5g where\".\n- Figure 3: The text in the figure is too small.\n- Eq 2 - D^i: Please use something other than i here as it is confused with i in Eq 1.\n- Figure 4: Regarding the ROI ALIGN text on one of the inputs: Does it mean that an additional ROI ALIGN is performed here? It is confusing.\n- \"22 epoches\" => \"22 epochs\".\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}