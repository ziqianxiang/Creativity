{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose two algorithms and their theoretical analysis for solving bilevel optimization problems where the inner objective is assumed to be strongly convex. The authors have greatly improved the paper to answer reviewer comments and three out of four reviewers have increased their scores. That said, given the large amount of new material added to this paper during the discussion phase, the program committee believes the paper requires a new round of reviews for a confident assessment. We encourage the authors to resubmit their work to a top conference such as ICML."
    },
    "Reviews": [
        {
            "title": "The paper propose algorithm for solving bilevel opt. with gradient descent (computed with back-propagation). It provides a clear complexity analysis. However, its novelty wrt previously introduced hypergradient computation is a bit fuzzy to me.",
            "review": "\n\nThe paper propose two algorithms for solving bilevel optimization problem where the inner objective is assumed to be strongly convex. The main idea is to use an iterative scheme to approximate the inner level solution, and apply to chain rule to compute the upper level gradient à la back-propagation. Similar techniques is used on a stochastic version in case where the objective function are written as an expectation. The authors also provide a nice complexity analysis (under relatively restrictive but common assumptions) and numerical experiments which confirm an improvement upon some others previous methods eg (Ghadimi and Wang, 2018). The paper is reasonably well written.\n\nI have only one concern on the novelty:\n\n- the paper (Franceschi etal, 2017, 2018) computes hypergradient by backpropagation (the reverse-mode) as well and the actual paper does not clearly describe the relation with these works (while claiming efficiency superiority). More there is no comparisons, neither theoretical nor practical. Note that its iteration complexity was recently provided in ICML (Grazzi etal, 2020). It should be interesting that the authors precisely comment on the differences and add this competitor in the numerical experiments. Specially clarification on the differences (if any) between hypergradient formula would be helpful, as well as the time and space complexity of the proposed algorithms.\n\nI am willing to raise my score after precise clarification of this point.\n\n- Also, it would be interesting to provide an easy to use open source implementation for the conference venue and futur comparisons.\n\nTypos:\n\"and and\" on page 1.\n\"fist\" on page 2.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Reviewer 1",
            "review": "The paper presents two algorithms - one for the deterministic and one for stochastic bilevel optimization. The paper claims the methods are lower cost in computational complexity for various terms and easy to implement. A finite-time convergence proof is provided for the algorithms.  Empirical results are presented for meta-learning, and (in the appendix) hyperparameter optimization.\n \n\nStrengths:\n \nThe deterministic bilevel algorithm is easy to implement and has a convergence proof given standard assumptions.\n\nThe stochastic bilevel algorithm has a convergence proof given standard assumptions.\n\nThey investigate in-depth how the complexity of their algorithms scales with various terms (GC, Hxy, Hyy, etc) showing benefits on some.\n \n \nWeaknesses:\n \nThe experiments presented in the paper are limited, and difficult to assess even after looking through the appendix.  What do the low-alpha parts of Figure 1 represent?  Why is CG used for the inverse with BA?  The algorithm you investigate (a Neumann Hessian inverse) was shown to work better in these kinds of problems by [1], [2], [3].\n\nAlgorithm 3 is not useful for practitioners to implement.  It seems like it does not use hessian-vector products, so I doubt the current incarnation would scale to any reasonably sized problem. See [1] or [2] for papers that provide memory-efficient Hessian Inverse estimators with the Neumann series and Hessian-vector products. Does Table 2 rely on storing the full Hessians in memory?  If so, this is infeasible in most applications and the complexity comparisons are not useful.\n \n[1] Lorraine, Jonathan, Paul Vicol, and David Duvenaud. \"Optimizing millions of hyperparameters by implicit differentiation.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n[2] Liao, Renjie, et al. \"Reviving and improving recurrent back-propagation.\" arXiv preprint arXiv:1803.06396 (2018).\n[3] Shaban, Amirreza, et al. \"Truncated back-propagation for bilevel optimization.\" The 22nd International Conference on Artificial Intelligence and Statistics. 2019.\n \n \nMy recommendation is to reject this work with a 4. The paper presents convergence proofs for variants of two common algorithms in bi-level optimization. If a version of algorithm 3 that uses Hessian-vector products is presented with similar convergence results I would change my score to a 6.  I would also increase the score if (quite a bit) stronger experimental results are presented -- the meta-learning results seem mediocre and all hyperparameter results are similar and in the appendix.  I am very familiar with this area, however, I worry I may have misunderstood some notation about the implementation, so I will put my confidence as lower than otherwise until author feedback.\n \n \nThe following points did not affect my score, but may help improve the paper:\n \nThe level of formality in the writing seems different than other submissions to ICLR.  For example, saying “etc.” or “and/or” in the abstract.\n \nI am not familiar with the term “order-level lower” complexity.\n \n“Inlcude” to “include” and “trails” to “trials”\n \nI think “hyperparameter” may be a more common spelling than “hyper-parameter.”\nAlso, “backpropagation” may be more common than “back-propagation”\n\nPassing the paper through a grammar checker might help catch a few other simple grammar typos.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "This is a paper that maybe cannot meet the acceptance criteria.",
            "review": "This paper proposes two novel algorithms, named deterBio and stocBio, for the nonconvex-strongly-convex bilevel optimization problems, and presents a comparison with several existing algorithms to demonstrate their superiority by experiments. Besides, propositions and theorems are well proved in the supplementary materials.\n \n+ The idea of the proposal for reliable bilevel optimizers with better gradient complexities is meaningful.\n+ The paper is well organized attached with sufficient supplementary materials for different propositions, and give analysis about the gradient complexities of proposed algorithms and related methods.\n+ It gives a detailed description of the connection between conventional bilevel optimization and meta-learning problems and applications of respective fields.\n \n\n\nConcerns: \n- Duplicate ‘and’ appears in the last line of the 1st Page.\n- The involved batch size and parameters are significant and sensitive to different problems, and the comparison between stocBio and other algorithms maybe not enough, perhaps more analyses are recommended.\n- Seems that the experiments are not enough to prove the flexible applications in relative deep learning fields, such as reinforcement learning.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes two algorithms to solve bilevel optimization problems, with applications to hyperparameter optimization and meta-learning.",
            "review": "Overall, I vote for rejecting. While most of the related literature focuses on computing the hypergradient, this paper tackles the full optimization problem, which is much harder and more interesting. In particular I really like the idea of taking advantage of the (finite sum) structure of the bilevel optimization problem (algorithm 2). The problem tackled is very important but I have a lot of concerns regarding the novelty of the 'proposed' algorithm 1. Moreover I do not understand the focus on avoiding to compute the inverse of the Hessian, since in such application the Hessian is never inverted, only linear systems are solved, which is much cheaper. \n\n##########################################################################\n\nAdvantages of the paper:\n- The paper is overall well written.\n- The idea of algorithm 2 is elegant.\n\n##########################################################################\n\nConcerns \n1- Authors may have missed an important reference [1], which is a seminal paper on bilevel optimization for hyperparameter optimization.\n\n2- Authors claim novelty on algorithm 1 'design a new bilevel optimization algorithm'. \nI may have missed something important, but it seems to me that algorithm 1 not new: it is exactly the algorithm in [1], Section 4.\n\n3- Moreover how do the authors chose the stepsize of the gradient descent $\\beta$? IMO this is a strong bottelneck in practice, that is why [2] uses LBFGS once the hypergradient is computed, and [3] uses a line-search procedure.\nDo authors have heuristics to choose the stepsizes $\\beta$ in practice?\nThis is paramount in practice, and the value of the stepsize is hidden in appendix, set to '0.001'. This seems very custom, and not general. How do authors know that the proposed algorithms converged for this choice of stepsize?\n\n4- 'proposition shows that the differentiation involves computations of second order derivatives'\nI do not understand. To my knowledge, the hypergradient $\\partial f$ can be computed in 3 differents ways: implicit[3], forward or backward [4].\nTo my knowledge there is not Hessian inversion, neither in the implicit nor in the backward: algorithm 1 in [3] solves a linear system to compute the gradient, and no Hessian are inverted in [1], Section 4. \nI am very confused with this Hessian inversion problem. Since I did not understand it, I lost a lot of the motivation for the introduction of algorithm 2.\n\n5- At the beginning of section 2.1 it seems that the end of an old sentence remains in the text.\n\n6- Part 4 is very technical and felt to me like an arid succession of theoretical results. IMO more insights and comments on the theoretical results would be very helpful for the reader.\n\n7- Part 5 proposes experiments using algorithm 1, which is already known (unless I missed something). Experiments on algorithm 2 are only in appendix.\nA comparison with  the approximated procedure of [3] would strength the paper: from my experience, approximated gradient like in [3] drastically improves the convergence time.\n\n\n[1]  Domke, J. Generic methods for optimization-based modeling, AISTAST 2012\n[2] Deledalle, C. A., Vaiter, S., Fadili, J., & Peyré, G. (2014). Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM Journal on Imaging Sciences\n[3] Pedregosa, F. (2016). Hyperparameter optimization with approximate gradient, ICML2016\n[4] Franceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017). Forward and reverse gradient-based hyperparameter optimization. ICML2017",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}