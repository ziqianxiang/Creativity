{
    "Decision": "",
    "Reviews": [
        {
            "title": "interesting idea, but questionable experiments and results",
            "review": "This paper studies the transferability of adversarial examples across different text classification models, varying different properties such as model architecture and size. The adversarial examples are created through existing methods that rely on synonym substitution. It's definitely interesting to identify \"universal\" adversaries, but the models here are mostly simple (larger and more performant models in the BERT family are left mostly unexplored) and the method for creating the adversaries is dubious (see more below). I am left wondering what the conclusion of this paper is, especially since the BERT-like models behave so differently than the others when confronted by adversaries. How do I make my models more robust given a set of universal adversaries? Overall, I think the paper should have gone farther than it did, and also it made a couple questionable experimental choices; thus, I cannot recommend its acceptance. \n\ncomments:\n- it's strange that the paper focuses heavily on network architectures that have been superseded by large-scale pretrained models such as BERT. as the paper notes, smaller models (e.g., LSTMs trained from scratch) are less robust to adversaries than BERT-like models. adversarial examples that fool simpler models do not often fool BERT (table 3).  the latter class of models seems much more practical to study these days.  the paper relegates RoBERTa and ALBERT to the appendix, and the results are similar to those on BERT: only a small fraction of adversarial examples from simple models transfer to these more performant and robust models. \n- in modern NLP, pretraining data is as important (if not more so) than model architecture and size. i do wonder how robust these \"universal\" adversarial examples are to models pretrained on different corpora. For example, BERT vs RoBERTa could be interesting to explore; the authors have done some experiments with these models but have not commented or analyzed the pretraining data factor. \n- i didn't find the main findings in 3.4 to be particularly surprising, especially in light of previous work. i'm also wondering why only two classification datasets were used, especially given other datasets such as MNLI which focus more on semantic understanding than sentiment / topic classification which are mainly keyword-based. \n- the word swapping method of creating adversaries is inherently limited. what does it really mean if a particular word replacement fools all models? is the new word likely to be infrequent in the training data? it would be more interesting to look at paraphrase-based adversarial examples that warp the syntactic structure as well as making lexical changes.\n- in table 6: \"funny\" > \"laughable\" is not a synonymous transformation, as the latter has negative sentiment. thus, this transformation is not an adversarial example; in fact, models that are \"fooled\" by it are actually behaving correctly. similarly, \"giddy\" > \"woozy\" is not a semantics preserving swap. this table makes me skeptical of these universal adversaries; how many of them are actually valid?",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "review": "This paper studies the adversarial transferability across models on NLP. The adversarial examples crafted for one model can fool a black-box model. Extensive experiments have been conducted to evaluate which factors can affect the transferability most. Based on the experiments of two attack methods, a few findings have been discovered, such as the input form has the largest influence on the transferability. Then, a universal black-box attack algorithm is proposed based on ensemble methods. \n\nGenerally, this paper is well written. The experiments are thorough for understanding the transferability on NLP, which could be a significant contribution to the field.\n\nSince I'm not an expert in NLP, I have the following questions (the authors can correct me if I'm wrong).\n\n1. Are the adopted task (text classification on two datasets) representative for evaluating the robustness of NLP models? Can the findings in this paper generalize to other NPL tasks (e.g., translation, comprehension, etc)?\n\n2. Are the adopted attacks (PWWS, GA) enough to draw the conclusions? As there are diverse attack algorithms for NLP, can the findings in this paper hold for other attack method?\n\n3. I do not totally understand the method presented in Section 4.1. What is the candidate v (is it an ensemble of models)? How does the proposed evolution algorithm cope with adversarial attacks?\n\nI would like to see those question addressed in the author response, which can be very helpful for me to evaluate the significance of this paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Universal adversarial attacks in NLP sometimes work, however, it's only a start now",
            "review": "##########################################################################\nSummary:\n\nThe paper considers the adversarial transferability of neural models.\nThe authors examine different models for their attacks as target and substitute models.\nIn addition, they propose an approach for universal attack generation based on an ensemble of substitute models. \n\n##########################################################################\nReasons for score: \nI vote for an OK but not good enough reject, as some findings are interesting, while the overall message hasn't been out yet:\n1. Experiments lack persuasiveness due to sometimes confusing results and leaky experimental design\n2. Overall universal examples in NLP data is questionable in a problem statement given, as it would be very easy to detect such examples by marking specific words or training a classifier that will easily detect such replacement pattern. Maybe, one can craft universal adversarial examples in an embedding space?\n3. Authors make some conclusions in e.g. 3.4, but they have little explanation via the nature of the models or other external factors. \n4. The whole ensemble approach seems a little too emphasized in the paper. From Table 9 we see, that the effect of usage of multiple models has a small dependence on the quality of the adversarial attack. Maybe it makes sense to improve the ensemble and design it to specifically create adversarial examples?\n\n#########################################################################\nMajor concerns: \n\n1. Figure 1: in my opinion, plots don't demonstrate, that humans are worse than the used algorithm, as the curves are pretty close to each other. Also, according to the text, you compare the quality of approach using remaining models, so if we use two different ensembles to make a decision, you test them for two diverse sets of remaining models. Thus, it is incorrect to compare them this way.\n2. The algorithm selects pairs based on fitness - what is the exact procedure? \n3. How many models are there in the ensemble selected by a senior researcher?\n4. Can you provide your code for experiments in this article? Some details are missing in the article, and the article will benefit from more reproducibility of the results\n5. In Table 5 you compare your model-based approach UAWR with unsupervised PMI. It seems, that to demonstrate the quality of your approach it is more interesting it to another model-based approaches [1-4]\n6. You don't consider the defenses against proposed attacks. For many attacks, it is pretty easy to construct a high-quality defense e.g. [4]. In my opinion, it is an important issue in crafting adversarial examples in NLP, as most of them are easy to detect (e.g. rule-based approach presented in Table 6)\n\n[1] Ebrahimi, J., Rao, A., Lowd, D., & Dou, D. (2017). Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751.\n[2] Ren, Y., Lin, J., Tang, S., Zhou, J., Yang, S., Qi, Y., & Ren, X. (2020). Generating Natural Language Adversarial Examples on a Large Scale with Generative Models. arXiv preprint arXiv:2003.10388.\n[3] Sato, M., Suzuki, J., Shindo, H., & Matsumoto, Y. (2018). Interpretable adversarial perturbation in input embedding space for text. arXiv preprint arXiv:1805.02917.\n[4] Fursov, I., Zaytsev, A., Kluchnikov, N., Kravchenko, A., & Burnaev, E. (2020). Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers. arXiv preprint arXiv:2006.11078.\n\n#########################################################################\nProposed minor improvements: \n1. Table 2: misprint, Adversarial transferability -> Transferability rate\n2. Table 3: what dataset is used in these experiments? What model is an attack model, and what model is a target model?\n3. Table 4: what dataset is used in these experiments? please, highlight the best values in bold\n4. Page 4, Since some models... - please write a formula for quality metrics you use to evaluate \n5. Page 5, two times: slightly transfer worse -> transfer slightly worse \n6. Figure 1, caption: all the possible pair -> all possible pairs of models\n7. Formula 1: what is x? Does the right side depend on x?\n8. Figure 2: line 1: how do you initialize h? line 12: remove i subscripts \n9. Table 5: provide WER. Why Word change % is smaller than 30%? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Generating universal language adversarial examples",
            "review": "This paper considers about adversarial examples in natural language processing. It contains three parts: (1) adversarial transferability evaluation based on numerical studies.  (2) genetic ensemble method for transferability enhancement. (3) adversarial replacement rules discussion. \n\nGenerally, this paper includes sufficient experiments and may contain interesting observations. However, I have doubts about the conclusion and its value for others' works. But first of all, I admit that although I have been working for adversarial attacks for serval years but only on image-related tasks and I have no experience on NLP, which makes me less confident to the following comments. \n\n1. For transferability evaluation: It is good to include exhaustive studies on the factors by varying one fact and fixing the rest.  However, I do not think without convincing discussions, the obtained results is very meaningful. For example, if two facts are coupled, varying only one fact does not work. More importantly, the conclusion seems valid with a specific attack. For example, as the paper itself shows, ensemble method can improve the transferability, then will it also affect the \"main findings\"?\n\n2. In image-tasks, different attack methods have different transferability and there have been many elaborate transferability enhancing methods. Indeed, the proposed ensemble method can improve the transferability. But it is not supervising and the readers would be more interesting to see the transferability enhancement  from method design and then it will be natural for them to use ensemble to get further improvement. \n\n3. Generally, the three parts, which are expected to be closed linked, are detached. If the main findings in the first part is meaningful, then it should have guidance for the other two parts. Also, the readers want to see, with different attacking with different replacement rules, how the main findings change (or not). For the later one, I think using only numerical study, without any theoretical discussion (this is the current situation of this paper), is impossible. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}