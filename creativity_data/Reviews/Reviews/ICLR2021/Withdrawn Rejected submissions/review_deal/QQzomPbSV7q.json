{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper is truly borderline. On one hand, the theoretical contribution seems novel and interesting, however, there appears to be somewhat of a gap between theory and practice. \n\nThere is unfortunately another problem. According to the authors, the main contribution of this publication is arguably the introduction of the nearest neighbor as the positive example in the triplet loss. However, the authors seem to be unaware of the history of the triplet loss. It was originally introduced by Schultz & Joachims 2004 as a loss over all triplets.  Weinberger et al. 2005 changed it and use the nearest neighbor as \"target neighbor\", which is called \"easy positives\" here, as the objective of LMNN. In 2009 Chechik et al. subsequently relaxed this positive neighbor formulation to any similarly labeled sample (going back to the Schultz & Joachims formulation) but sampling triplets. The re-introduction of the nearest neighbor as \"easy positive\" was then covered by Xuan et al. 2020.  \n\nUnfortunately all of this diminishes the novelty significantly and it is clear that the paper in its current form does not have a strong enough contribution. I do encourage the authors to take a close look at the original LMNN publication and Xuan et al and write an improved re-submission for the next conference that maybe focuses more on the theoretical contribution. \nGood luck,\n\nAC"
    },
    "Reviews": [
        {
            "title": "A simple sampling manner for diverse and distinct sub-classes in metric learning",
            "review": "The authors find that the popular triplet loss will force all same-class instances to a single center in a noisy scenario, which is not optimal to deal with the diverse and distinct sub-classes. After some analyses, the authors propose a simple sampling strategy, EPS, where anchors only pull the most similar instances. The method achieves good visualization results on MNIST and gets promising performance on benchmarks.\n\nAvoiding class collapse is meaningful and important in metric learning when dealing with some tasks. The analyses in the paper provide insights. Here are some possible issues of this paper.\n1. The authors should discuss when we need to avoid such class collapse. Maybe in some cases, pulling all similar instances to a single point leads to more discriminative embeddings. Even some methods are designed following that consideration. Some examples and demonstrations are required.\n2. It's better to write a sketch of the analysis on how to extend it to multi-class cases and analyze will the definition of the noise influence the final results.\n3. Maybe the authors need to find another real-world dataset with multiple meanings in one class and show the advantage of the proposed method. We can find the improvement of performance on the benchmarks, but the numbers are hard to illustrate the effect of the method.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Gap between theory and empirical",
            "review": "Motivated by the theoretical results on class collapse problems, this paper proposes a simple positive sampling mechanism called EPS for metric learning. The method is simple -- each sample selects its nearest same-class counterpart in a batch as the positive element. The authors provide both theoretical motivations and empirical studies on the proposed method. \n\nStrengths: \n+ Theoretical analysis on the existing class collapse problem for triplet and margin loss\n+ well-motivated and simple solutions that are proven to be effective in theory\n\nWeaknesses:\n- there is a gap between theoretical analysis and empirical studies. In the analysis, the paper shows in the noisy label setting, margin and triplet loss also induce the class collapse problem but in the empirical study, the paper only conducted analysis on dataset with clean labels. \n- the theoretical analysis based on the assumption that the function f can approximate any functions. However for any fixed deep nets, it does not satisfy the requirement \n\nOverall, the EPS method is simple and supported by theoretical analysis. I would find it more convincing if the paper can provide empirical analysis on noisy labeled data. In addition, I wonder how different architectures can affect the difference between previous methods and EPS.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple and effective sampling method, but is used before and its effectiveness is yet to be validated",
            "review": "This paper proposes/adopts a simple positive sampling scheme in metric learning: only sampling the easiest positive for each anchor. Authors give a theoretical analysis of how the proposed sampling scheme can reduce class collapse. Experiments on fine-grain retrieval datasets show the effectiveness of the sampling scheme. Using the sampled easiest positive, nearly all current metric learning methods got improved performance.\n\nPros:\n\n1. propose/adopt a simple easiest positive sampling scheme, and show its usefulness in fine-grain retrieval task;\n2. Extensive theoretical analysis of how the proposed sampling scheme can reduce class collapse.\n\nCons:\n\n1. I don't think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. Specifically,  Arandjelovic et al. (2016) used exactly the same easiest positive sampling scheme. Though authors explicitly show the difference between Arandjelovic et al. (2016) and the proposed method (Equation in section 4.3), I found no difference.\n\nIn the paper of Arandjelovic et al. (2016), they don't clean the positive set (only minor negatives could be included) for efficient training. This is good for practical usage.\n\n2. For section 4, it is good to analyze the Class-collapsing property. However, I would suggest using three classes to derive theorems, rather than using two classes. As a metric-learning problem usually has multiple classes, having two or three classes are usually different stories.\n\n3. While I trust the effectiveness of the easiest positive sampling scheme in the fine-grain image retrieval datasets, I strongly suspect its effectiveness in a broad image retrieval task.\n\nFor example, in the following paper:\n\nRadenović, Filip, Giorgos Tolias, and Ondřej Chum. \"CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.\" European conference on computer vision. Springer, Cham, 2016.\n\nThis easiest positive sampling scheme falls short in performance.\n\nCombining the conclusions from the above paper and the paper under review, I would say this easiest positive sampling scheme has limited contribution, as it is not broadly applicable.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "REDUCING CLASS C OLLAPSE IN M ETRIC L EARNING WITH EASY POSITIVE S AMPLING",
            "review": "Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4.\n------------------------------------------------------\nThis paper proposes an easy positive sampling method for deep metric learning which aims to reduce the class collapse problem which is found to harm the performance of existing DML methods.\n\nPros: \n1. This paper is well-written and easy to follow. \n2. The idea is simple but makes good sense. The author also provide solid theoretical analysis of the flaw of existing methods and the advantage of the proposed easy positive sampling strategy. \n\nCons:\n1. The idea of sampling easy positive for deep metric learning is actually not new. [1] already proposed an easy positive sampling method and the motivation is quite similar (to relax the constraints of intra-class variations). [1] should be cited in this paper.\n2. The authors only provide theoretical analysis on the binary case and claims it can be easily extended to the multi-label case, which I find not trivial.\n3. A concern is the limited batch size, which might cause the easy positive sample of one particular sample at different iterations to be different (and possibly from different subcluster). This might lead to inconsistent effect of pushing the same sample to different subclusters. \n4. Similar to the last one, a more general problem is the theoretical analysis only consider the optimal situation but neglects the nature of batch-based training, which might bring unexpected problems.\n5. For the experiments, the performance improvement using the proposed easy positive sampling is not strong. Specifically, the best performance on the Cars196 and CUB200 dataset is achieved with EPS + margin, but the authors did not report the performance of margin loss  with distance-weighted sampling. Comparisons with other sampling methods on the same loss should be provided.\n6. The authors should design an experiment to better demonstrate the class collapsing problem on a regular dataset like CUB. The toy experiment on the MNIST dataset is not convincing.\n\nIn summary, I think this paper is solid and well-motivated, but I find the idea not new and the experiments not satisfying. The latter weighs more in my decision. \n\n[1] Xuan H, Stylianou A, Pless R. Improved embeddings with easy positive triplet mining[C]//The IEEE Winter Conference on Applications of Computer Vision. 2020: 2474-2482.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}