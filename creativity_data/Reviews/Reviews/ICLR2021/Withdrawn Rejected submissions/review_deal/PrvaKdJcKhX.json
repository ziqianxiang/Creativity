{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers agreed that there were a few issues with the current version of this work, mainly:\n\n- Some missing baselines that are mentioned in the paper, but not sufficiently compared to\n\n- Problems with the presentation that did not make it easy to understand.\n\n- Not an optimal fit with the intended audience of this conference.\n\n\n"
    },
    "Reviews": [
        {
            "title": "An OK but not good submission",
            "review": "##########################################################################\nSummary:\n \nThis paper studies the (multi-)agent spatial coverage problem. They propose a framework that can approximate the general class of spatial coverage objectives and their gradients via spatial discretization methods.  \n\n##########################################################################\nReasons for score: \nThe contribution of this paper seems trivial, it is a refinement of training DeepFP [Kamra 2019] by deriving a differentiable loss function. The comparisons to existing baselines are missing. I therefore recommend for rejection.\n\n \n##########################################################################\nPros: \n \n1. This paper studies a specific problem of resource allocation problem in a continuous setting. I appreciate two detailed examples of problem formulation for both single-agent and multi-agent settings. \n\n2. The main contribution of this paper is the loss function derivative in Theorem 1, which mainly uses classical calculus.\n \n##########################################################################\nCons: \n \n1.\tThe author believes discretization the continuous domain is not a good way to solve resource allocation problem. However, the proposed method still have to adopte the discretization on computing the integral of r and \\partial r/u. Although the author emphasise that they do not need to discretize the action domain, but it seems to me that why it is significant is unclear. \n\n2. The author mentioned that model-free methods such as PSRO, has the issue of low sample efficiency and high variance issue for policy gradient methods, however, both of which has not been addressed by this work, and this proposed method is not compared to model-free method either. PSRO and its variants, for example, have shown many success on blotto type of game, so I think the advantages over PSRO [Lanctot 2017] type of methods need clarifying. I would consider improving the score if author can show comparative advantage over PSRO.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A weak accept",
            "review": "The main goal of this paper is to address the problem of multi resource spatial coverage.\nThe main challenges are the non differentiability of the utility function and the complexity of the action space making this problem hard to optimize.\nThe authors make a clear review of the state of the art approaches that often focus discretizing the action space. In the contrary, the authors propose to discretize the target space to construct a differentiable approximation of the utility function.\n\nI'm really not an expert on multi resource spatial coverage, so I am not able to judge if the presented results are original, nor significant.\n\nHowever, I found the paper clearly written, and the subject nicely presented with examples.\n\nI suggest a few points of improvement:\n- Part of the conclusion and the abstract mention that the paper proved that \"multiple-ressources are combinatorially hard to approximate with neural networks\". I didn't see proofs of that statement, I agree that the authors develop an approach that doesn't seem to suit NN and they outperform a few NN. This can be enough to show that the presented method is efficient but it is not enough to claim that the compared methods are not. \n- I read appendix A1, and I still do not understand why the NN curves, why do they have biases ?\n- In table 1 and 2, it would be great to indicate how confidence intervals are computed.\n\n\n----- Edit after rebutal ------\n\nThank you for the answers.\nThe authors only answered partly my concerns. I'm still not convinced by their explanation on the NN curves. I don't understand how their performance decreases with the number of iterations, it seems that they weren't well tuned.\n\nAnyway I've been convinced by R4 and R1 that a few baseline models are missing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The paper presents a viable method but overall does not meet the standard of ICLR.",
            "review": "##########################################################################\nSummary:\nThis paper studies the coverage game where agents allocate their resources to target spaces to maximize their coverage, and the goal of this paper is to (approximately) compute the Nash Equilibrium. The proposed method simulates the game by iteratively updating the best response, and the main contribution is an algorithm to approximate the gradient of the utility function with respect to the resource allocation (over the space). In particular, the paper proposes to decompose the gradient into two parts and estimate each part by discretization. \n\n##########################################################################\nWhile the paper presents a viable method, I vote for reject for several reasons. The paper does not successfully justify the merits of the proposed method, with many hidden parts that are hard to follow. The paper lacks a good organization, and the writing could have been more formal (see detailed comments). The experiments are not convincing.\n##########################################################################\n\nDetailed comments:\n\nFirst of all, I am slightly concerned that this paper's work does not really fit ICLR, as its goal is to approximate the Nash Equilibrium of a game, without any learning process involved. The only part I see relevant is that two learning-based methods were adopted as competitors.  \n\nThe presentation of this paper lacks clarity and a good structure. I could not figure out the problem this paper wants to solve after reading the first three pages. The abstract and the introduction claim that the paper wants to solve the resource allocation problem, but it turns out the specific target is to approximate the gradient. \n\nFrom my view, using the proposed decomposition is a doable method, but I cannot see the novelty in doing so. The paper mentions some hardness to be overcome: (a) the integration has no closed-form and (b) the coverage drops to zero outside the coverage area. However, the paper does not justify how the aimed hardness was addressed by the proposed method. \n\nThe paper mentions some existing methods (such as DeepPF in a published paper) but does not take them as competitors. I am also confused that the paper names the proposed method also as DeepPF, so how does it compare to the existing DeepPF?\n\nIt looks like the competitors in experiments are baselines without much calibration. The information in the appendix does not provide sufficient details. In addition, instead of providing code in the appendix, it could better to include source files and datasets in the supplementary materials. The currently provided information does not support reproducibility. \n\n\nThe organization and writing of this paper could be improved – e.g.,\n-\tPlease specify the domain when introducing a variable.\n-\tThe definition U=\\times_{p \\in P} {U_p} is problematic – {U_p} is wired since U_p is already a set.\n-\tThe definition cvg: q \\times u -> R is problematic – it should be cvg: Q \\times u -> R\n-\tInstead of saying the agents aim to achieve Nash equilibrium, it would be better to say the game will reach the Nash equilibrium. In addition, is the Nash equilibrium unique? \n-\t“for e.g.” is not correct.\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very hard to follow paper",
            "review": "This paper shows that spatial coverage objectives with multiple-resources are combinatorially hard to approximate with neural networks and proposes a spatial discretization based approximation framework to solve this problem. \n\nThe paper is very hard to follow due to all the notations the authors introduced and the details set up in the examples. Many details in the two examples look a little bit extra, which could be properly abstracted. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}