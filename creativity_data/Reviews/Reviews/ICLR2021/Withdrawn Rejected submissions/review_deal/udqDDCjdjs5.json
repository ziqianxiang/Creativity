{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review ",
            "review": "The paper provides a framework for writing analytical expressions of different neural networks. The paper does not sufficiently motivate why this is important or provide any particular insight/use-cases achieved using such a framework. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The idea may be promissing but the motivation is missing",
            "review": "The paper proposes to unify wavelet filter banks and convolutional neural networks.  They authors start from presenting a brief theory of M-Band Discrete Wavelet Packet Transform. Then, after a series of relaxations and transformations they connect the derived equations to the equations of convolutional neural networks.\n\nThe motivation for developing such an approach is not properly presented in the introduction. It has no experiments that could demonstrate some advantages of the developed framework either. At the current stage, the paper requires a clear motivation. It will allow one to position the developed framework properly. Without a clear positioning of the developed approach, it is very complicated to assess the pros and cons of the paper. \n\nThere are minor issues which can be fixed either way\n1. Introduction. Starting the paper with a very big letter is not according to the guidelines. It must be fixed\n2. The paper has some grammar issues. For example \"It can also helps\" in paragraph \"Insights\"\n3. I found the color map used in Figures 3, 4, 5, 6 very uncomfortable\n4. The references to equations are done incorrectly. Now it has the following form \"Eq. equation 11\". It should be either \"Eq.\" or \"equation\". Not both of them at the same time.\n\nTo sum up, the paper presents a connection between wavelet filter banks and convolutional neural network. However, the motivation of developing or using this connection is not presented in the paper. The proposed connection does not demonstrate any insights. No new models are derived from the proposed connection. No experimental evaluation is done. At the current stage, the idea is raw.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Motivation is unclear: a new formalism is interesting if it brings theoretical or empirical insights or a new point of view",
            "review": "Summary: \n\nThis paper revisits the signal processing formalism to describe CNNs through the scope of filter bank formalism. The paper is structured in a progressive manner, from a well understood formalism with guarantees through the CNN formalism. First, the authors describe the cascade algorithm, which is mainly linked to a unitary wavelet transform: a scale dilation is obtained by a cascade of the same conjugate mirror filters and down-sampling. A second relaxation is to only use a cascade of filters bank associate to different frames. Finally, a third relaxation is obtained by relaxing the frame property and incorporating non-linearities. Then, the authors note that the main difference with CNNs is the mixing of the different channels of a CNN: in the proposed framework, the channels are separated by the different filtering. This allows to describe the convolution along a CNNs as a multi-channel separations through different paths of filtering, which scatter signals in different bandpasses.\n\nPros:\n- Signal processing formalism is always insightful, because it's an overlapping but different field of ML, that leads to strong analysis.\n\nCons:\n- Novelty, motivation and use-cases are really unclear to the reviewer. It seems that this is simply a rephrasing of various signal processing ways of thinking, rather than a new framework.\n\nRemarks:\n- I believe the proposed formalism of the Sec 3.1 could already be observed through other works, like Sec 4 of https://arxiv.org/pdf/1403.1687.pdf . Sec 3.2 is also similar to https://www.di.ens.fr/~mallat/papiers/RSTA2015Published.pdf There, this formalism is mainly introduced to express a Scattering Transform as neural networks thanks to the filter bank formalism, which allows to obtain some theoretical guarantees on a specific class of Neural Networks. However, it is unclear to me what the formalism in this undereview paper brings: what do we learn thanks to this vocabulary about a standard CNNs?\n- The formalism of Sec. 4 seems very similar to the one developped in the Section 3 of https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf , but less deeply.\n- It is likely in deep learning that the non-linearity is at least as important (if no more) than the path of filtering. Some non-linearities are well understood (like modulus), but here a discussion about this non-linear effect seems completely avoided, whereas necessary.\n- Assuming that the layers have an orthogonality constraints, I can't think of theoretical result which is easier to derive thanks to this framework. \n- One issue I see in the framework of 3.1 is the possibility to build invariants to more complex group: without a recombination along the channel axis, it is more difficult to obtain roto-translation invariants (eg, Fig 1 of http://www.cmapx.polytechnique.fr/~sifre/research/cvpr_13_sifre_mallat_final.pdf ). How do you plan to deal with that?\n\nSuggestions for improvements:\n- In Sec 5, the authors state: \"among the prospective developments that can be based on the analytical expressions provided by the paper, one can cite ...\". I believe it would be great to do it, to motivate further this work, and this is precisely in the scope of this paper (Sec 2 could be widely shortened in order to incorporate the claimed new insights).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "What is the merit?",
            "review": "__Summary of contributions__\nThe paper provides a general mathematical framework which incorporates the convolutional neural networks and wavelet filter banks. The paper also gives some (unexplored) ideas how the proposed framework can be useful at.\n\n__Strength__\nThe proposed mathematical framework is sound. Also, the idea of semi-supervised architecture sounds interesting, while it was not the main objective of study in this paper.\n\n__Weakness__\nI could not find any convincing argument why one should even consider the proposed mathematical framework. More specifically, why is considering additionally the M-DWPT a useful thing to do? I am not particularly convinced by the arguments in Atto et al. (2020); could the authors provide or refer to any more empirical background that competing strategies (e.g., pruning and restoring weights) for convolutional networks are not effectively/efficiently accounting for the transfer learning scenarios?\n\nAlso, I am not sure whether the existing mathematical frameworks fall short of discussing the M-DWPT. For instance, existing mathematical representations in [Lee&Raginsky](https://arxiv.org/abs/1812.09658) or [Long&Sedghi](https://openreview.net/forum?id=r1e_FpNFDr) look quite similar to the form given in the present manuscript. If there is a critical difference, it would help the readers if the authors could explicitly outline the discrepancies.\n\n__Typo__\nIn the abstract, dowstream should be downstream, I guess?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}