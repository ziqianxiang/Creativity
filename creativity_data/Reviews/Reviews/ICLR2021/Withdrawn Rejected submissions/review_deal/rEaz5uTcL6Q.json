{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents an approach to tackle visual reasoning by combining MONET and transformers. All reviewers agree that there is some performance improvement shown. But there are several concerns including clarity/writing (multiple reviewers point it), experiments (baselines) and most importantly missing insights from experiments (why it works). While some of the concerns have been handled in rebuttal, the paper still falls short on primary concern of insights/why it works (which reviewers argue is critical for a paper on reasoning). AC agrees that the paper is not yet ready for publication."
    },
    "Reviews": [
        {
            "title": "Clear message and solid execution, but experiment is limited",
            "review": "Summary: the paper propose to tackle visual reasoning problem in videos. The proposed solution is to combine MONET (Burgess et al., 2019) with self-attention mechanism (Vaswani et al., 2017) to first encode images into object-centric encodings and aggregate the encodings using self-attention to make the final prediction. The method is shown to outperform neural-symbolic reasoning approaches such as MAC (Hudson & Manning, 2018) and NS-DR (Yi et al 2020.) on image QA and R3D (Girdhar & Ramanan, 2020) on CARTER, which is a video reasoning task / benchmark.\n\nPositives:\n- The overall goal, i.e., showing that neural network can solve reasoning problem without specialized supervisions and structures, is clear and well-motivated.\n- The solution chosen makes intuitive sense --- discover objects using MONET and encode object encodings to make the final predictions using transformers.\n\nComments:\n- The proposed method is intuitive and does prove the main hypothesis of the paper in terms of benchmark performance, but I wish the paper can provide more intuition on how exactly the attention mechanism is able to perform visual reasoning. Specifically, I'd like to see either qualitative examples of minimum toy examples comparing the baseline methods and proposed method and show, clearly, under what circumstances does the proposed method perform better than baselines and why. This would make the main message of the paper even more salient and bulletproof.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Difficult to understand the paper, the technical contributions are interesting",
            "review": "The paper is bad-written, very difficult to extract, and understand what the authors want to express. There are long sentences with wrong clauses and prepositions. The descriptions are very unclear, figures are not illustrative. I suggest the author polish the paper in a better form.\n\nTechnically, it combines the MONet and self-supervised learning via masking the objects and making the self-supervised learning as an auxiliary task. I agree with the motivation and observation that the high-level tasks that requires object-level understanding are similar to the BERT. The results also demonstrate the auxiliary task will benefit the self-supervised learning and improve the performance a lot.\n\nI am curious about the relations and methods of using the self-supervised learning as an auxiliary task. Usually the auxiliary task does not directly improve the original task with a huge margin since the model has additional objectives, and methods like BERT also used the self-supervised learning for pre-training, not as auxiliary task. The paper that referred to as using auxiliary loss actually used the self-supervised learning as pre-training. I wonder what is the performance if we pre-training first and fine-tune with supervised loss. \n\nOverall, I believe there are huge improvement space for the writing. The results look great, but I am still curious about where the significant improvements come from, since my previous experience suggest the self-supervised auxiliary task cannot usually bring huge improvement to the supervised learning tasks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studies the temporal and spatial reasoning in videos. Specifically, the authors propose to combine unsupervised object representation learning MONet with self-attention transformer and introduce self-supervised learning through masked representation prediction. Experiments are conducted on CLEVRER and CATER dataset and the performance improvements compared to baselines validate the proposed methods.\n\nOverall I think the idea in this paper is interesting, especially using unsupervised way to extract object-centric representation then using self-attention to learn the higher-level reasoning. The experiment results seem promising. However the idea of combining visual-linguistic features in transformer and pretraining with masked representation has been studied in previous works like VL-BERT: Pre-training of Generic Visual-Linguistic Representations and Learning Video Representations using Contrastive Bidirectional Transformer. So the novelty is a major concern. Also the effectiveness of temporal reasoning is degraded by the experimental results that \"masking one object per frame is the most effective\". \n\nAnother concern is some missing/vague technical details making the reading rather difficult, for example the section for different masking scheme.\nSome detailed questions like:\nIs representations learned from MONet learnable during masked representation prediction?\nIs there a pretraining phase for the masked object representation prediction? Or the auxiliary loss is applied together with the classification loss?\nAny reasons why not compare with other baselines in CATER like \"Learning Object Permanence from Video\" by Shamsian et al?\nWhat does it mean by \" MONet does not assign objects to slot indices in a well defined way\"?\nWhat's the attention design for \"hierarchical attention model\"? Is the representation input to transformer for the whole image rather than single object?\n\n--------\nAfter discussion:\n\nAfter reading the author's reply as well as the opinions from other reviewers, I will stick to my original rating since 1) the writing makes the paper hard to understand 2) current experiments cannot support the central claim of spatial-temporal reasoning. While the author resolve some of the concerns, they are encouraged to further polish the paper and use more evidence to support their claims.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Results on performance and data efficiency are impressive, but how should we interpret such results? I think the results presented here are too premature to support the claimed contribution.",
            "review": "This paper proposes to use a self-attention component in combination with unsupervised object representation learning (e.g., MONet) for CLEVRER and CATER. Results show considerable performance improvement and data efficiency compared with prior methods.\n\nPros:\n\n+ This paper is easy to follow.\n\n+ Results on performance and data efficiency are impressive.\n\nCons:\n\n- It is a common misconception that performance is still the first priority in artificial tasks like CLEVRER or CATER. It is not. Artificial tasks are designed to probe specific capabilities. Hence, it is vital to show that a model indeed learns these capabilities before drawing any firm conclusions; for instance, the model is capable of counterfactual and causal reasoning. The authors fail to justify this aspect in the current version.\n\n- Specifically, although it is natural and reasonable to follow prior evaluation metrics, it is essential to ask whether the prior setting is still meaningful in the new context. This paper is set to test whether self-attention-based neural networks are able to perform causal reasoning and physical understanding. The important context here is that such a family of neural networks has been very successful in more complex tasks, hence given an artificial task and smaller size of training data, it is predictive that the model would perform better. Note that this is not new to visual inputs; for instance, the Named Detection Transformer (DETR) also uses similar components for segmentation. Also note that whether such a family of model indeed possesses a certain level of reasoning is still debatable, as some researchers found in conversational reasoning (e.g., [1,2]) and in particular, advocated by Gary Marcus. Given these contexts, it is natural to ask whether the proposed method, in fact, possesses a certain level of reasoning capability instead of merely fitting the data.\n\n- To refute such a potential hypothesis, what could we do? Since the goal is to test whether such a model is indeed capable of causal reasoning or physical understanding, I think the authors need to present something beyond the prior evaluation metrics to really justify their claims. Otherwise, we are simply far more premature to draw such a conclusion that the proposed model, built on top of existing knowledge of neural networks, is capable of causal reasoning and physical understanding.\n\n- To verify a learned model is capable of physical understanding, can we train the model on one task domain and test its generalization in another task domain? Physical understanding should be very transferrable once learned, akin to a physical engine that simulates the virtual world; one does not need tens of engines, and one suffices. Similarly, cognition has presented tons of evidence [3], showing how humans/infants react to causal reasoning tasks. Can we augment the existing dataset to show such a capability? In this way, we can have sufficient pieces of evidence to support the claimed contributions.\n\n- In summary, adopting a powerful and newer model on existing artificial tasks, one needs to be careful to draw conclusions by presenting sufficient pieces of evidence beyond a single measurement of performance. The current experimental results, in my opinion, are not sufficient to support the claimed contributions.\n\n- Some sections of the writing are more than necessary. For instance, it is completely unnecessary to spend almost 2 pages on various sections talking about self-attention in Transformer. Most of these backgrounds and information should be assumed known to a reviewer at the top conference.\n\n[1] From Eliza to XiaoIce: challenges and opportunities with social chatbots\n[2] Augmenting end-to-end dialog systems with commonsense knowledge\n[3] A theory of causal learning in children: Causal maps and Bayes nets",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}