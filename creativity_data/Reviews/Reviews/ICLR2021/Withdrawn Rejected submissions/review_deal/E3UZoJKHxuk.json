{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work introduces a method for supervised learning that takes a data-generating process into account. While the paper proposes an interesting approach to learning a causally invariant model, the reviewers had several concerns about the proposed method. I thank the authors for having the paper revised, addressing the reviewers' comments. However, there are still legitimate issues unresolved about the specific theoretical results and assumptions made throughout the work.  I share similar concerns, and, therefore, recommend rejection. Still, I would like to encourage the authors to address the reviewers' problems in the paper's next iteration. \n"
    },
    "Reviews": [
        {
            "title": "Paper 729 Review",
            "review": "### Summary\n\nIn this manuscript, the authors introduce a method for supervised learning in an out-of-distribution (o.o.d.) setting.\n\nIn particular, the authors consider the case where labelled data pairs $(x, y) ~ p^{e}(x, y)$ are available for training sampled from multiple environments / interventions $e \\in \\epsilon_{\\mathrm{train}}$ and the goal is to learn a predictive model $\\hat{y} = f(x)$ that will generalize to data sampled from environments $e \\in \\epsilon_{\\mathrm{test}}$ not present in the training set.\n\nIn a nutshell, the authors propose to tackle this problem by fitting a deep generative model, in their case, a variational autoencoder, that models both features $x$ and targets $y$ as being jointly generated by a set of latent factors $(z, s)$. Crucially, their model removes the connection between the latent factors $z$ and the target $y$, encouraging disentanglement of causative (ideally, captured by $s$) and non-causative (ideally, captured by $z$) factors at the latent variable level. In their model, variation across environments is limited to the prior over the latent factors $(z, s)$, which is modelled conditional on the confounded $c$ (when known) or environment/domain indicator $d$ (when the true confounder is unknown) as $p(z, s | c) = p(z | c)p(s | c)$ (resp. for $d$).\n\nFrom a theoretical standpoint, the model builds heavily on recent advances on nonlinear independent component analysis (ICA), such as Hyvärinen et al. 2019 and Khemakhem et al. 2020. To frame the problem in the language of that line of work, the authors introduce additional assumptions, some of which include:\n(1) The (causal) mechanisms generating $x$ from $(z, s)$ and $y$ from $s$ must be invariant across training environments.\n(2) Noise in those mechanisms must be additive.\n(3) Additionally, assumptions analogous to those in e.g. Theorem 1 of Khemakhem et al. 2020.\n\nProvided all those assumptions apply, the authors are able to adapt results in that line of work to their proposed model and prove similar identifiability results for their new setting.\n\nFinally, the authors test their approach on a simulated toy dataset and three real-world datasets (a variation of the colored MNIST task introduced by Arjovsky et al. 2019, the NICO dataset from He et al. 2019 and a medical imaging classification task from the Alzheimer’s Disease Neuroimaging Initiative, ADNI). Results suggest that the proposed approach slightly outperforms basilines such as IRM (Arjovsky et al. 2019) or DANN (Ganin et al. 2016), among others.\n\n\n### High-level assessment\n\nOut-of-distribution generalization is, in my opinion, a topic of utmost relevance for the machine learning community.\n\nFrom a methodological perspective, the authors here propose a novel application of existing ideas, models and identifiability results (e.g. Hyvärinen et al. 2019 and Khemakhem et al. 2020, among others) which they adapted to disentangle latent factors that jointly affect $x$ and $y$ from those which only affect $x$ while being able to borrow most of the theoretical machinery from that line of work to provide similar identifiability results. Because of this, I believe the contribution to be a sound, sufficiently innovative step worthy of publication, albeit arguably leaning on the incremental side.\n\nAll in all, I lean slightly towards recommending acceptance of the manuscript. Nonetheless, I also believe the paper has substantial room for improvement in key areas such as (i) soundness and depth of the experimental results and (ii) clarity of exposition, description and writing style. Those shortcomings, coupled to the incremental nature of the contribution, prevent me, for the moment, from giving a more enthusiastic endorsement of the manuscript for publication.\n\n### Major points / suggestions for improvement\n\n1. To the best of my knowledge, the manuscript is currently lacking key information to allow a reader to reproduce the results. In particular, I could not find any clear description of the architectures and training procedures used for all baselines and how these differ from (1) their original publications and/or (2) the architecture and training procedures of the proposed approach.\n\n2. Related to the previous point, I believe it would be essential that the authors disentangle the effect of model architecture / model capacity from the actual methodological contribution. Because of the aforementioned issue, it is at this point not possible for me to assess whether the experiments were carried in such a way that all baselines have a similar capacity or whether a part of the performance improvements achieved by the proposed approach could be explained away by the model having more parameters and/or expliciting using for prediction additional variables (e.g. the domain indicators) than some of the baselines.\n\n3. I believe the methods in Isle et al. 2020 and Teshima et al. 2020 are sufficiently relevant and should be included as additional baselines to better assess the practical impact of the differences between the proposed approach and those methods.\n\n4. In my opinion, the manuscript is at the present moment not particularly clear. For example, in sections such as 4.3, I believe that the writing does not make it sufficiently clear which statements are supposed to be modelling choices / approximations / compromises for computational tractability and which statements follow mathematically without loss of generality. All in all, I would also recommend having the manuscript proof-read for English style and grammar issues.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Assumptions Risk Taking Away the Essence of the Problem",
            "review": "I would like to thank the authors for the interesting work they proposed. I tried to explain my concerns below and I am open to changing my score with their feedback. \n\n1) My main concern is whether the considered causal graph indeed captures the phenomenon that the authors are attempting to address. What I mean is that, yes, indeed C induces correlation between Z and Y. But only in the combined dataset where we do not fix C. For each realization of C=c, Z and Y become independent. This is in contrast with what we experience in practice, e.g., we see cows on pasture sceneries more often than not in a single dataset. So the correlation should exist even when we only look at a single dataset. Then this correlation may change in a different dataset where we see cows on the beach. In both datasets, there is spurious correlation which needs to be resolved. The only reason there is spurious correlation in data obtained from this graph is because we assume C is not conditioned on, i.e., we are given mixed data from multiple environments. One of the proposals of IRM was to make use of this environment information, rather than work with the combined data and still there are many challenges there. Theirs is one attempt to address this. In short, I am not certain if this causal graph is suitable to model the phenomenon we are facing in ML today, which the authors talk about in the introduction. \n\n2) Another concern is about the invertibility of f_x to obtain s,z. If s can be uniquely obtained from x, then how does C affect p(y|x) across different environments? Writing down Bayes rule, normally posterior is affected by p^e(s,z) but when p(x|s,z) is deterministic, this is not the case: For each x, one can uniquely go to s only to then go to y, without C playing any role in the process. Then I think this invertibility assumption might make the effect of C void. Please comment on this. \n\n3) When the new (test) environment is one of the environments that we trained on (c^e is known) then we can just use the training data p(y|x) in that environment. Why is this analysis needed then for that case? \n\n4) When the new environment is not known (paragraph before Section 5), the authors propose optimizing s and z and to maximize the data likelihood for the trained model and finding the most likely y given this s. This is connected to the second condition of Definition 4.2, correct? In other words, under the assumptions, this inference will give the correct distribution p(y|x) in ANY new environment. Please verify if I am understanding the result correctly here. But then again, based on 2) above, I am having a hard time seeing the value of the identifiability result under these assumptions.\n\n5) Assumptions are revealed sequentially one by one and too late in the paper. It would also be great if the authors could make an itemized list of assumptions (ANM, exponential family, invertibility of functions etc.).\n\nThe following are my detailed feedback and further questions:\n\nThe authors assume a simple causal graph where features (image) and labels do not cause each other but are caused by latent factors that are affected by the environment (different interventions). To get to p(y|x) in a way that does not depend on the environment to be able to do inference on new data, the authors attempt to learn the mechanism to generate S from X and Y, where S is not observed, for each environment. They show that under a number of assumptions, an identifiability result arises, i.e., there is a set of models which all give the same \\sum p(s|x)p(y|s), implying that we can predict Y from X for any new environment and this can be recovered from p^e(x,y) given enough number of diverse environments. \n\nThe narrative is a little overwhelming and unnecessarily confusing in that, very simply, authors propose using a specific causal graph, with specific modeling assumptions within the SCM framework. Their main contribution is the identifiability result which shows that under the given assumptions, we can identify an invariant mechanism p(y|x) that does not depend on the environment (if i am understanding the theorem correctly).\n\nExperimental sections are too briefly described and should be expanded on. Also please comment of Figure 2.\n\nPlease explain the variables corresponding to those in the considered causal graph in each real experiment. This is very important to assess the validity of the analyzed causal graph and the assumed missing edges between the variables. For example, what are the variables in CMNIST? I believe Color should cause Image but not the Label and Digit should cause both the Image and the Label. But the narrative is different than this. \n\nAdditive noise model, which is a huge assumption is only revealed in page 4, whereas claims about identifiability of the SCM are made starting abstract. This should be given much before. ANM assumption is made for the SCM of both X and Y.\n\n\"Information intersection property\" in first par. of Sec. 4.2\nf_y^{-1}(y')=[f^{-1}]_S(x') for all x',y' will not be true unless the exogenous variables are zero since y=f(x)+e. What am I missing here? Maybe the authors mean invertibility from f(x) back to x rather than from y to x.\n\nPlease explain the notation in Definition 4.2 in detail. What is [f_x^{-1}]_S(x)? Without these, it is very hard to parse the statement. I was able to decode these eventually but they need to be introduced for ease of reading. \n\nOther notation is also very confusing. Boldface capital T is a matrix but also a function that takes inputs. Again, I can decode what authors meant but a more rigorous and precise notation is needed. \n\n\"It is shown in supplement 7.2 that ∼p is an equivalence relation.\"\nCan you please elaborate in the main text? \n\nThere is some mismatch with the experiments and the theory. Additive noise models and the exponential families enable the identifiability results whereas experiments use a VAE formulation. Why not assume the additive noise models and the exponential families in the experiments as well? Then I am sure other inference methods could be used to discover the latent space from the exponential family and still leverage the proposed theory for identification. Did you try this and does it not perform as well? Please comment if possible.\n\nCan you give intuition on why the identifiability result requires a condition on the rank of \\Gamma matrix but not on the other parameters? For example, what if all T's are zeros? Then no condition on Gamma should be sufficient since it doesn't appear in the equations anymore. How is this and similar corner cases are covered by the theorem?\n\n\"For causal prediction, the old-school causal learning frameworks (Peters et al., 2016; Buhlmann, 2018) causally related the output label Y to the observed input X, which however is NOT conceptually reasonable in scenarios with sensory-level observed data (e.g. pixels in image classification).\"\nI understand what this implies but it is very implicit. Please consider elaborating: Modeling pixels as variables of a causal graph does not make much sense. \n\n\"For such applications, we rather adopt the manner in\"\nThis divergence is also confusing. The hinted difference is not a fundamental difference but a difference in how one models the causal structure. \n\n\"into a novel causal model\"\nI would consider changing this statement. The authors assume a specific causal graph, calling it a novel causal model does not seem accurate. \n\nFigure 1 is interesting because it makes the strong assumption that the features X do not cause the label Y. It also makes the strong assumption that S and Z cannot cause each other. Please comment on these non-causality restrictions imposed by the graph.\n\n\"Notably, far beyond the scope in existing literature (Khemakhem, Kingma and Hyvarinen, 2020), our results can implicitly, and are the first to disentangle the output-causative factors (a.k.a, S) from others (a.k.a, Z) for prediction, to ensure the isolation of undesired spurious correlation.\"\nI believe the related work for identifiability of causal factors should be much richer than this. Please consider including identifiability of SCMs in linear and related settings from the causal inference literature. \n\nI do not think there is a need for a new name \"Causal Invariant Mechanisms (CIMe)\" where what the authors mean is to learn the structural equations in the SCM alongside the distributions.\n\n\"From the perspective of causality, the confounder C blocks the back-door path from Z to Y , making the Z spuriously correlated with Y .\"\nMaybe splitting into two sentences will help with clarity here. The path induces the correlation, not blocking the path by C. \n\n\"brutal-force\"->\"brute-force\"\n\nIn (1), it looks like the joint distribution on S,Z is assumed to factorize as p(z)p(s), i.e., they are independent. But this is per environment, indicated by the index e. I believe it is better if the authors could write this as p(s,z|c) instead of p^{e}(s,z).\n\nThe arxiv identifier of Tan et al. is in boldface.\n\nI think the authors should discuss identifiability of SCMs especially in the ANM setting more explicitly. It would also help to discuss that once the causal graph is known, under the ANM assumption one can trivially obtain the SCMs. The challenge arises when there are latent factors, which is the setting considered here.\n\nWhat is assumption (4) mentioned in the proof? Is it the fullrank condition written in caps? Please cross-reference. \n\nCausal Markov condition is invoked before (3) to factorize interventional distribution, but this connection between different environments and them essentially representing different interventions is not made before. I think this will be very helpful if the authors enable the connection. \n\n%%% After the Author Responses and Paper Updates %%%\n\nI would like to thank the authors for very seriously considering my recommendations and genuinely attempting to implement many of them, even in their proofs. I do think their updates made the paper stronger in general. A minor general note is that many of the newly edited sections have typos and could benefit from proof-reading. Following are my final remarks:\n\nAfter going through the paper again, I realized a step in the proofs which indicates an extra assumption that is not mentioned in the main paper. The proof in lines (14,15) of Section 7 (supplementary material) seems to assume that the exogenous variable has a fixed distribution, i.e., despite two different models inducing the same observed distribution having two different functions f_x and \\tilde{f_x}, their exogenous noise \\epsilon_x must have identical distributions for the proof to go through. Similar steps are used for the exogenous noise of Y as well. These steps can only be explained by the very strong assumption that the exogenous noise term of every variable has a fixed distribution across different causal models. Such an assumption is not mentioned anywhere in the paper as far as I can see - this has to be definitely addressed. Moreover, I do not think the identifiability result is very useful when obtained under such a strong assumption. Unfortunately, I cannot recommend acceptance due to this. But I encourage the authors to pursue this direction and seek out ways to relax this condition.\n\nI had brought up the point that C is not used to induce correlation within a dataset, which takes away from the essence of the practical problem they are trying to address. In order to address this, i.e., to be able to handle the confounding within each dataset, the authors added an extra condition on the effect of confounding: They assume that once p_1(x,y)=p_2(x,y), this implies p_1(x,y|c)=p_2(x,y|c) for all c. This extra assumption allows the authors to use the machinery they developed as is with this additional argument.\n\nUnfortunately, this assumption, much like the others, is also presented in between the lines. I think it will be really helpful if the authors could explicitly write down their assumptions in a theorem environment (\n\\begin{assumption} ... \\end{assumption}\n) and make them very explicit rather than only within the theorems or in-text: Assumption 1: ANM Assumption 2: Exponential family Assumption 3: ... etc.\n\nThis relates to the implicitness of another key assumption made in the paper: \\Gamma matrix is assumed to be full rank. This intuitively suggests that the experimental conditions are sufficiently different. But this is an algebraic statement and is hard to interpret. I would recommend the authors to think about how to interpret this condition, i.e., assess how it impacts the conditional distributions - exponential assumption allows them to make an algebraic assumption here, rather than probabilistic; but a probabilistic interpretation would be more intuitive.\n\nSome of the typos that I can see: \"and functions the prior of distribution of C\"->\"and functions as the prior of distribution of C\"\n\ntypo in (62): \"R\"->\"r\"\n\n\"We generalize the identifiable result in theorem 4.3 \"->\"We generalize the identifiability result in theorem 4.3 \"\n\nMany of the arXiv citations are actually published in various venues, please go through the bibliography and update.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A thorough study on causally robust supervised learning",
            "review": "# Summary of the review\n- The authors make a strong case regarding their approach to causally robust supervised learning. Their contributions are thorough and carefully situated in the relevant subfields. I believe the authors' research will be of interest to the relevant research communities.\n\n# Summary of the paper\n- The authors suggest a causal latent variable model for high dimensional supervised data, such as images (input, $X$) with labels (output, $Y$), which can be used to obtain robust predictions as well as learn disentangled representations for high dimensional input given heterogenuous observational data, collected in different environments $e \\in \\mathcal{E}$. In their generative model, the authors distinguish two types of latent variables. Continuing the image classification example, a group of latent variables (called $S$ in the paper) correspond to the object level abstractions such as shape and contour, that causally determine the label $Y$ as well as the image $X$. The other group of latent variables (called $Z$ in the paper) correspond to contextual information such as pose or lighting, that causally only effect $X$ but also can be spuriously correlated with $Y$ (see below). Accordingly, the foundation for the authors' model is the robust / invariant / transferable functional relationships between the latent variables and observed variables, $f_x: (S, Z) \\to X$ and $f_y: S \\to Y$ that are constant across $e$, whereas $P^e(S, Z)$ is subject to change given the environment $e \\in \\mathcal{E}$. The authors name $f_x$ and $f_y$ Causal Invariant Mechanisms (CIMe). The model is completed by an observed confounder $C$ that causally affects both $S$ and $Z$, and corresponds to information determining the distribution of $P^e(S,Z)$. $C$ enables the backdoor path that spuriously relates $Z$ and $Y$. When $C$ is inaccessible, the authors assume that a binary index $D$ is accessible instead, which serves as a label for the environment which observations were made. This completes the Latent Causal Invariance Model (LaCIM).\n- The overall aim of the authors approach is to formulate the learning problem such that when a function mapping input to output $f: X \\to Y$ is to be learnt, this learned function is not biased by the spurious correlations between $X$ and $Y$ that likely would not generalize to new environments or tasks. \n- Based on their model LaCIM the authors:\n    - demonstrate the $\\sim_p$ identifiability of $f_x$ and $f_y$ \n    - propose variational inference methods to approximate these functions given $C$ or $D$\n    - conduct experiments demonstrating how their method performs on out-of-distribution (OOD) generalization tasks, interpretability tasks, and adversarial robustness tasks.\n\n# Strenghts of the paper\n- The authors model accommodates different types of high-level latent variables that might affect the observations causally or induce spurious relationships between them. The model also makes clear what relationships are expected to be invariant due to being causal mechanisms, an how spurious relationships can be blocked by conditioning on environment properties (if accessible), and. The authors also provide a variant of their model given only the label/index of the environments are accessible.\n- Building on the work by Khemakhem et al. (2020), the authors prove the identifiability of the inveriant causal relationships between latent and observed variables, given certain conditions on the number and diversity of environments observed. \n- They provide a variational-autoencoder algorithm for training for the case when confounders are observed and unobserved. \n- They provide extensive experiments to 1- verify their theoretical claims, 2- show that their model shows the desired robust prediction properties under different settings 3- demonstrate their method estimates meaningful latent variables.\n- The authors make extensive comparisons with the literature both methodologically and experimentally. Actually they provide a more thorough review in the supplementary material. They do not mention this additional review in the main text, which I think they should.\n- Though being dense, the paper is easy to follow (save for occasional typos, see below).\n \n# Questions and potential improvements\n- As the authors note, their identifiability conditions rely on the existence of a virtually unknowable number of different domains. How should we expect the performance of their algorithms to degrade when the number of domains are lower than this unknown number? An initial idea for this could be obtained through a simulation study, where the dimensions of the latent variables could be controlled.\n- In the experiments, the LaCIM-D variant of the algorithm seems to perform better in the cases where the back-door paths are less likely to be exhaustively blocked. Given finding such a set of variables should be even more difficult in realistic scenarios, do the authors see a realistic use for LaCIM-C? If so, how would they compare the two algorithms in terms of their applicability in different tasks? Do they see a case when the LaCIM-D would be disadvantageous?\n- The authors mention that given insufficient number of environments, the environments/datasets at hand could be e.g. clustered and treated as heterogenuous (as does e.g. Buhlmann 2018). How successful would the authors expect such an approach to be? Though such an approach could be helpful to some extent, it is unlikely that any homogenous data with $\\mid \\mathcal{E} \\mid = 1$ can be successfully treated as heterogenous after some processing.\n- The authors mention the diversity condition at Pg. 5 without having introduced it before. The authors might want to clarify this aspect of their exposition.\n\n## Minor comments, typos\n- Section 3: The authors' back-door path explanation is hard to understand, they might want to replace it with something to the effect of: \"an unblocked, undirected path between $V_a$ and $V_b$ that has edges going into $V_a$ and $V_b$\".\n- Section 3: $d^e \\in \\mathbb{R}^m$ could be replaced by $d^e \\in {0, 1}^m$.\n- Section 3 and later: The abbreviation for structural causal models could be capitalized: SCM\n- Section 4: \"and priori\", \"less and equal\"\n- Section 4.1: \"the confounder $C$ blocks the back-door path from $Z$ to $Y$, making $Z$ and $Y$ spuriously correlated\": the authors might want to restate this part, when the path is \"blocked\" ($C$ is observed/conditioned on) $Z$ and $Y$ are no longer spuriously correlated.\n- Section 4.1: By \"brute force data fitting\" the authors must mean an algorithm that tries to directly learn a functional relationship between $X$ and $Y$. They could express this part a little more clearly.\n- Section 5: title \"Experiment\"s\n- The word \"condition\" in Theorem 4.3 is capitalized, I am assuming by mistake.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "leveraging causality to learn invariant models across domains ",
            "review": "This paper proposes a VAE based model for learning latent causal factors given data from multiple domains. Similar to [Kingma and Hyv¨arinen, 2020], it utilizes additional labels as supervision signals and learns the model using a Bayesian optimization approach given a fixed hypothetical causal structure. The identifiability is obtained by assuming the casual mechanism to be domain invariant, which is partially supported by some empirical experiments.\nI have three concerns for the current version of the paper.\n1.\tDirectly mitigating the identifiability result of [Kingma and Hyv¨arinen, 2020] to this model seems to be inappropriate. The result of [Kingma and Hyv¨arinen, 2020] shows that the sufficient statistics of the latent code are recoverable. However, it does not mean the causal structure is unique: additional transformations may be allowed to be applied to the adjacency matrix. As a result, it is in question whether your causal mode learns the correct factors under given structure.\n2.\tThe symbols are a bit confusing. Some important concepts stay intuitive and lack rigorous mathematical definitions. For example, “output-causative” “cross-domain causal effect” stability measure etc should be defined. A clear table listing the symbols and its meaning would be helpful.\n3.\tThe empirical results of table 1 do not fully support your conclusion “true causal factors are learnable”. Simply computing a MCC to the original factors is not enough to me. Some experiments, like examining the vulnerability and performance of the system under the condition that the latent factors are controlled or intervened, for the claimed “invariant causal model” are better to be included to convince readers.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}