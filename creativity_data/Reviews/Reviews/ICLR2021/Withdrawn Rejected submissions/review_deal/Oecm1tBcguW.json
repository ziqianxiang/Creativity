{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper addresses the problem of prior selection in Bayesian neural networks by proposing a meta-learning framework based on PAC-Bayesian theory. The authors optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. The method does not rely on nested optimization schemes, instead, they directly minimize PAC bound via a variational algorithm called PACOH-NN which is based on SVGD and the reparameterization trick.  The method is evaluated on experiments with both synthetic and real-world data showing improvements in both predictive accuracy and uncertainty estimates.\n\nInitially many reviewers were positive about the paper. However, it was noticed by one reviewer that the submitted paper presents a very significant overlap with\n\nJonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. arXiv, 2020.\n\nAnother reviewer mentioned that they were actually reviewing for AISTATS the above manuscript by Rothfuss et al. The ICLR program chairs were contacted for a possible violation of the dual submission policy for ICLR: \n\n\"Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences or journals, are not allowed and violate our dual submission policy.\" \n\nThe ICLR program chairs decided that the similarities between the two papers are not enough to issue a desk-rejection. However, in the discussion period,  three reviewers out of 4 pointed out that, even though the authors did revise Sections 4 and 5 in the current version, these modifications do not seem to be strong enough to make up for the really strong overlaps between the two papers. The reviewers agreed on rejection and stated that this paper should either be merged with the Rothfuss et. al. one (assuming the authors are the same), or its content should be developed to the point of making both of them clearly distinct. "
    },
    "Reviews": [
        {
            "title": "Paper is well-written but its contribution is marginal",
            "review": "The paper addresses the problem of learning data-driven priors for Bayesian neural networks. Assuming zero-centered Gaussian priors for BNNs often results in poor generalization and uncertainty quantification, whereas choosing informative priors is challenging due to limited interpretability of network weights. To be able to overcome these issues, the authors propose a meta-learning framework based on PAC-Bayesian theory, in which they optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. Unlike the previous approaches in the literature, their approach doesn’t rely on nested optimization schemes, instead they directly minimize PAC bound via a variational algorithm called PACOH-NN which is based on SVGD and reparameterization trick.\n\nThe main paper and its comprehensive appendix are well written mostly, especially the PAC-Bayes review is very concise and elaborate for a conference paper. After the introduction of related concepts about PAC-Bayes bounds and meta-learning, their application to the derivation of PACOH bound for finding a hyper-posterior expressed so naturally. Proposed methodology sounds, and claims of the authors seem correct. Experimental results both on synthetic and real-world data seem fine, and there seems to be no fallacy in empirical evaluations. Also, experimental methodology is discussed thoroughly both in the main paper and appendix. Empirical results from various experimental setups are sufficient to show that their framework improves both predictive accuracy and the uncertainty estimates compared to several popular meta-learning approaches with additional computational advantages.\n\nAs a researcher with superficial knowledge of the field, I didn't have a major difficulty in following the paper, despite the use of heavy mathematics in methodology. Mathematical notation is clear and consistent (except the usage of $M(\\Theta)$ and $M(H)$ interchangeably). Derivations in both the main paper and the appendix are easy to follow. Also, relation to prior work is clearly addressed, relative advantages and disadvantages of the proposed methodology are discussed.\n\nIn my opinion, the problem of meta-learning informative priors for Bayesian neural networks will certainly be of interest to the ICLR community. On the other hand, paper’s contribution to the work of Rothfuss et al. seems limited. Authors state that the main contribution of the paper is their practical variational algorithm for PACOH bound. In order to tractably optimize PACOH for a BNN, they employ a Stein gradient variational descent algorithm by utilizing reparameterization trick. However, the specifics of their algorithm are presented very quickly, which makes it difficult to grasp the actual novelty of PACOH-NN. For instance, neither the advantages of this particular optimization scheme over the previous methods, nor the theoretical guarantees regarding convergence and approximation quality for PACOH-NN aren’t discussed.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Principled approach to meta-learning with strong empirical results",
            "review": "The paper proposes a method for learning BNN priors based on optimising a PAC-Bayes bound for meta-learning, which they call PACOH-NN. This extends previous work on optimising PAC-Bayes bounds (PACOH), and is attractive for being principled in construction and effective in practice. The main claims are that fewer tasks are needed to achieve good performance in terms of both utility and calibration, and that the algorithm comes with performance guarantees.\n\nIn the empirical analysis, they firstly show that the method is competitive in terms of utility (for both classification and regression) with baseline methods, but with superior calibration properties. Meanwhile the method is also superior in terms of computational scaling. They also demonstrate the effectiveness on a bandit learning task for vaccine development. Along with proofs, extensive details are given for the experimental setup and training algorithms.\n\n\nStrengths:\n- Principled approach to meta-learning for BNNs\n- Efficient algorithm with good scaling properties\n\nWeaknesses:\n- A little hard to read in places\n- Performance guarantees never shown\n\nSpecific comments:\n- One of the main motivations for the paper is that the method comes with performance guarantees. E.g. in the related work section\n- The use of Hoeffding’s lemma could be very loose, particularly in the cases where values close to 0 or 1 are unlikely\n- In the use of the Hoeffding’s lemma, it looks like there should be an extra $m^2$ term in the numerator. Where has this term gone? Similarly, I would expect a factor of $m$ for the sub-Gamma bound in the numerator. This would carry over to the derivations for Thm2 as well, and would also affect the convergence properties\n- It seems that you have hyper-priors for the means of the BNN priors only. Are the variances always set to a default value for new tasks? Did you consider putting a hyperprior on the variances/precisions as well? This would feel more in keeping with the Bayesian flavour, and might give the ability to selectively “freeze” (softly) any weights that are stable across all tasks.\n- In Figure 1, with only a single training example, I don’t understand how PACOH-NN is able to transfer the knowledge of sinusoidal functions. Since the transfer is only at the hyperprior level, it seems to me that the only possibility is that the tasks are similar enough that the hyperpriors are actually capturing the entire distribution over tasks, so that there is almost no learning required at all on the target task. Can you comment on this?\n- You cite the Bayesian MAML paper by Yin et al. However, in the experiments, you only use the regular MAML and first order version, both of which give point estimates. A comparison to BMAML seems to be essential here\n- The experiment in 6.2 is a nice addition, and the results are quite impressive on first glance. However, since none of the baselines are meta-learners, we can’t be sure here if the difference is due to the effect of transfer learning or the superior uncertainty quantification. Another meta-learner should be included. There are several meta-bandit algorithms that are available.\n- Figure S1 in the appendix is really helpful! If possible, I would prefer to see this in the main paper\n\n\nMinor comments/Typos:\n- P1 variation -> variational\n- P2 in the BNN description, the regression model have a noise parameter but the classification model does not. Wouldn’t that imply that the classes would have to be separable (i.e. no mislabelled examples)?\n- P3 $\\mathcal{H}$ is undefined on first use. Also, in the description the prior is defined as $P \\in \\mathcal{M}(\\mathcal{H})$, but in (1) you have $P \\in \\mathcal{\\Theta}(\\mathcal{H})$ and $\\mathbb{E}_{\\theta \\sim P}$. On page 4, you then have $P \\in \\mathcal{M}(\\mathcal{\\Theta})$\n- Where does Theorem 1 appear in Alquier et al 2016? Is this the correct reference?\n- For the sub-Gamma case, you reference Boucheron et al , which is a book. Please make the reference more specific\n- Tasks are defined as $\\tau_i := (\\mathcal{D}_i, S_i)$, but it would seem more accurate to define them as $\\tau_i := (\\mathcal{D}_i, m_i)$. Not that later you have an expectation where you draw $(\\mathcal{D}, m) \\sim \\tau$ which is also consistent with the latter\n- In (7) you use LSE - mention this is LogSumExp\n- P8 you say “nearly constant memory and compute”. I’m not sure what “nearly constant” means - is it really linear, but with better scaling constants? Or is it truly constant?\n- P12 $p$ undefined in lemma 1\n- P18 (53) second line should be $\\theta_l$ rather than $\\theta_L$\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of the PACOH-NN algorithm for learning BNN priors ",
            "review": "### Summary:\n\nOne of the main issues that BNNs have to face is the choice of good informative priors in order to provide precise information about the uncertainty of predictions. The present work connects BNNs and PAC theory to formulate a new system to obtain a general-purpose approach for obtaining significative priors for Bayesian NNs. This is done by employing the closed-form solution for the PAC-Bayesian meta-learning problem. The meta learner here learns a hyper-posterior over the priors of the parameters of the BNN by using the closed-form expression PACOH  (PAC Optimal Hyper-Posterior). This is applied in the context of NNs, where the priors are to be defined over the NN parameters. Extensive experiments are carried out to show the performance both in regression and classification datasets, as well as its scalability. In all of these regards, the system seems to be competitive, improving on the results of previous state-of-the-art methods and producing promising results in real-world problems. \n\n##### Pros:\n\n* The method does not employ nested optimization problems, therefore avoiding all the issues related to these approaches altogether.\n* The usage of Bayesian seems to point in the right direction since the Bayesian framework allows for an easy formulation of the different levels needed here to formulate the system. \n* The construction of the PACOH closed-form expression seems innovative and relevant \n* The final system improves on the previous state-of-the-art in most of the cases here shown, and in some experiments, the improvement is very clear. Thanks to the fact that it is agnostic to the inference method in use, it presents itself as a very general-purposed approach, able to improve the predictive qualities of previous methods. \n* The article is very complete and detailed, both in the main body and in the appendix. The appendix is particularly extensive, providing insight on many of the main points of the main text. \n* The experiments conducted are exhaustive and complete, providing a very wide scope of the capabilities of the method. Detailed results can be found for every experiment and task. \n* The text is well written and comprehensible\n\n##### Cons:\n\n* The choice in section 5 of using Gaussian priors with diagonal covariance matrices is motivated by the convenience in the computations. Moreover, the hyper-prior is also modeled by a zero-centered spherical Gaussian. How does choosing these distributions affect the final results? Are there been any experiments on which the parametric distributions chosen here are different from the ones presented? Please, describe how do you think these choices may affect the results and bias the final distributions obtained. \n* The calibration error is all the information provided to quantify the quality of the predictive intervals for regression. It would be helpful to include some other quantification of this, such as the usage of CRPS or other strictly proper scoring rules. In the same line, using metrics such as the Brier score for classification tasks would help getting a more complete picture. \n* How does the complexity of the BNNs employed affect the final results? Does using more layers and/or more units improve the results? There have been recent works (e.g. Functional Variational BNNs) regarding artifacts that arise when using large BNNs which make the system not able to properly learn the data. Could these problems be solved as well with this approach since the final prior is constructed using the data? \n\n###### Other comments: \n\n* As an optional suggestion, the article is well written but can be difficult to comprehend at some points. To that end, I would suggest trying to provide qualitative descriptions of some of the quantities in the expressions that later prove to be of importance. As an example, providing some intuition on $\\psi(\\sqrt{m})$ would help to understand the expression (1). In general, I think the article would benefit from extending a bit the explanations of some parts, especially section 4. \n* Typo: section 5, paragraph 2 - \"categorigal\" \n* To have a clearer explanation of figure 1 I would try to include the description of the sinusoidal functions that is already present in the appendix section C.1, since it seems relevant to the text in section 6.1 as well.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "PAC-Bayesian meta learning with Bayesian neural nets",
            "review": "This submission has significant overlap with the following reference: \n\nJonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes-Optimal Meta-Learning with\nPAC-Guarantees. arXiv, 2020.\n\nAssumably, the submission is from the same authors as theoretical analyses and experiment setups are almost identical. The main difference I can see is the Bayesian baseline model is Bayesian neural nets while the reference used Gaussian processes. The rating I gave is because of this main concern on overlapping content. \n\nThe analysis of PAC-Bayesian bounds led to the conclusion that Gibbs posterior can help minimize the bounds in the meta-learning settings. With Gibbs base learner, the bound expressed with the corresponding partition function can be done more efficiently compared to existing PAC-Bayesian meta-learners, including MLAP. \n\nMy other concerns are as follows: \n\n1. The authors may need to make sure about the main contributions of this submission with most of theoretical results already presented in the referred paper. If Bayesian neural nets are indeed showing much better empirical performances compared to GPs, the authors may need to emphasize that and discuss thoroughly with performance comparison. \n\n2. The final solution to derive Gibbs posterior is just deriving the model posterior given the hyper-prior using the data from all training tasks. If that is the case, the authors may present it clearly and discuss the connections as well as pros and cons to other existing meta-learners, for example, the existing metric-based methods that reweighting the loss across training tasks. \n\n3. The experiments may need to be done more consistently. For example, it is not clear why figure 1 only compares PACOHNN with a vanilla BNN using a simply prior instead of, for example, comparing with MLAP or PACOH with GPs? For classification examples, the authors did not compare with neural processes as claimed at the beginning of section 6.1. In section 6.2, why max regrets are smaller than average regrets in figure 3? How many training samples were used for the five tasks to get the reported results? \n\n4. The writing of theoretical analyses (main text, but especially appendix) can be improved. There are typos and confusing math notations. For example, in section 5,  above equation 7, \"\\epsilon_l j \", where j appears to be a typo? In A.1, \"to proof the theorem\" should be \"to prove the theorem...\"; in step 1 and 2 of the proof, X_k was used to denote the data point and then task, respectively, which is indeed confusing. It is not clear either what the subscript k was used for. Cleaner math notations, better organization, and careful rewriting may significantly help the readability of the paper. \n\n5. The authors may want to add complexity analysis of PACOH-NN. It is in fact quite confusing to me why it \"maintains a nearly constant memory and compute load as the number of tasks grow.\" as stated since the number of tasks does factor in based on Algorithm 1 in Appendix B. Clearer setup descriptions with complexity analysis and discussion are needed. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}