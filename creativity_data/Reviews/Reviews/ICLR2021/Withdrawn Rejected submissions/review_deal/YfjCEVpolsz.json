{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting topic, with an interesting question; however, conceptual issues outweigh the analysis",
            "review": "Summary of the paper:\n\nThe paper investigates how macro-learning techniques in reinforcement learning can extract macros that exhibit \"synergism\", i.e., the property that more macros lead to better performance. They relate learning macros to neural architecture search, in that they define an additional (outer) MDP that searches over macro-combinations for the (inner) domain MDP. The outer MDP is then optimized on the final reward of the inner. \nMore specifically, the authors have the outer MDP construct fixed-length macros (with a \"null\" action to allow arbitrary-width macros), where the actions within the MDP are outer-MDP actions corresponding to inner-MDP actions. The outer MDP uses a deep-Q network, while the inner is learned using PPO. They show that their proposed technique generally outperforms a baseline from existing literature, and that the \"synergism\" property holds for their approach, but not for the baseline.\n\nCommentary on the goal of the paper:\n\nI agree with the authors that macro-actions and how they can be learned is an important, and underinvestigated, area of reinforcement learning research. The goal of the paper - synergism - is indeed desireable to balance the increase action space size with the benefit of additional macros.\n\nStrengths:\n- The paper is mostly well-written, and easy to understand\n- The paper shows results on a fair number of domains, with a thorough analysis\n- The proposed approach shows consistent improvements over the baseline\n\nWeaknesses:\n- The paper does not define \"synergism\" exactly. The closest to a definition is in section 4.4.2. From what I gather there, I find \"synergism\" to be a bit of a misnomer, as I understand \"synergism\" as meaning that the total is greater than the some of its parts. The paper suggests that \"synergism\" means something like monotonicity instead, i.e., that each additional macro actually increases performance.\n- The paper lacks motivation for macros. That is, learning macros is not a goal in itself. Macros are desireable because they add \"shortcuts\" to an MDP, thereby helping with exploration (and potentially the complexity of the decision graph), making learning more efficient, either in terms of computation or (more likely) in terms of sample efficiency. That, in turn, means that the goal of experiments should be to show that learning macros either 1) helps with sample efficiency or 2) helps with generalization, which in turn helps with sample efficiency over multiple domains/variations of the same domain. Given the cost of the proposed approach to learning macros, and the fact that the authors do not include a competetive non-RL baseline, makes me doubtful that either of the two goals is actually achieved.\n- I am not entirely sure what the assumptions are that explain the use of a NN function approximator. In general, NNs are smooth function approximators. However, I would expect the space of macros to be somewhat non-smooth, so I am not sure why this approach works. Some analysis of this would be interesting.\n- I strongly dislike that the paper compares the baseline with the proposed method under the constraint of an \"identical amount of wall time\" (section 4.3). Wall-time always makes me suspicious: did the baseline get the same resources? The same optimizations? The same parallelism? Therefore, I will always argue to either measure computational effort in algorithmic complexity, or (in RL), in terms of training sample usage, which is generally the more constrained resource.\n\nIn total, I cannot recommend the paper for acceptance in its current state. The conceptual and systematic issues outweigh the benefits.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "paper needs to be more thoroughly compared to options",
            "review": "The paper presents a method for automatically learning macro actions that work well in synergy. The macro actions are created and evaluated in parallel processes.  The results show that the proposed method is more sample efficient than the IEB baseline – another macro action construction approach.\n\nThe experimental evaluation is thorough – 19 atari games with 5 seeds each. However, it’s a bit hard to decipher how many training steps your algo uses. The ensemble evaluation method is tucked away in the appendix. It introduces the hyperparameter H, which controls how many samples are used to evaluate the macro action. Even though this can be parallelized, this number is critical to determine the overall sample efficiency of the system. Technically, an off-policy algorithm could use these extra samples to improve its policy. I can’t seem to find what H was used. \n\nI find the omission of options from the main paper to be surprising - since this is one of the main ways to learn temporally extended actions. This is somewhat addressed in the appendix, however, the discussion in A2.3 about the comparison to options needs to be moved to the main paper and expanded upon. “Options are closed-loop temporal abstraction based approaches, which are irrelevant to the open-loop macro actions described in our work”. I would not say irrelevant, options are very relevant to the proposed macro action approach. Similarly, the experiments including the option critic baseline in table A3 needs to be aggregated or summarized and placed into the main paper. \n\nThe comparison to the “Primitive” algorithm in table A3 also needs to be moved to the main paper as well. As a community it is very important to know if the proposed method for learning macro actions is significantly better than the primitive approach. Significance testing should be used to compare the results from table 1. I assume the primitive algo is DQN? Or it could be PPO? On that note, why does the controller C use DQN and the workers use PPO. It would be cleaner to use the same RL algorithm.\n\nOverall, I would need to see the comparison between the primitive and option critic approaches moved to the main paper, and reformulated in terms of samples (including H in the budget for the proposed approach) – not just wall-clock time as reported in table A3. If the main argument for the paper is to have optimal wall-clock time, which could be a valid argument since the proposed algorithm can be easily parallelized, then the training plots should be reconstructed as such, wall clock time should be reported. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of \"Toward Synergism in Macro Action Ensembles\"",
            "review": "## Summary\n\nThe authors propose framing the problem of constructing viable macro-actions as a sequential decision making problem. A Controller sequentially constructs an entire macro vector, which is divided into sets of macro-actions and provided to traditional RL agents that use an action space augmented with these macro-actions to learn a policy on Atari games. The paper claims that learning the macro-actions jointly leads to discovery of \"ensembles\" of macro-actions that work synergistically. They compare to a method that builds the macro-actions iteratively based on frequency of use.\n\n## Positives\n+ Drawing the connection between constructing macro-actions and neural architecture search is interesting.\n+ Learning sets of macro-actions together so that they work well together is also a useful idea, and ablations that highlight this detail add value to the paper.\n+ Comparison to option-critic in the appendix is appreciated.\n\n## Drawbacks\n- The paper mentions several times that \"synergism\" is a property it seeks in the set of macro-actions it searches for. This term has not been defined anywhere in the paper. At best, we get a vague explanation of this term in Section 4.4 (on the last page of the paper). A clear precise definition of a term that is being used to highlight the technique presented in the paper would be very helpful in understanding the paper.\n- The paper claims that construction of macro-actions is an MDP, without defining the state space or transition function and without specifying whether the returns are discounted and/or episodic. The reward function used is defined in equation (1). Based on this reward function,  it seems to me that the Controller C constructs the entire vector of macro-actions before it is ever evaluated. Why then is the macro-construction a sequential decision making problem?\n- Connected to this question, why aren't partial macro-actions evaluated? Such an evaluation would make the problem more obviously sequential and would also make sure that actions are not added to a macro unless they are useful.\n- In related work, \"Strategic Attentive Writer for Learning Macro-Actions\" by Vezhnevets et al. is not discussed. This is another macro-action construction algorithm that is relevant to the discussion in this paper, and a possible baseline to compare against.\n- The constructed set of macro-actions are used with a PPO implementation to evaluate their efficacy. The baseline compared to (IEB, Durugkar et al.)  uses DQN augmented with the discovered macro-actions. I did not find any mention of whether IEB was implemented using DQN or PPO in the paper, nor what hyperparameters were used to train it.\n- Subsequently, is the improvement in performance of the technique in the paper because it uses PPO? A comparison with PPO without macro-actions is necessary to clarify. Since this technique uses significant interactions with the environment to first construct the macro-actions, this PPO without macro-actions baseline should be allowed to train for the total number of interactions given to the macro-action construction + training algorithm presented in the paper.\n- Additionally, IEB has been presented as an algorithm that constructs and uses macro-actions during training, whereas here the evaluation is done by first executing IEB to discover macro-actions and then use it. \n- The hyperparameters N and T used in the algorithm are not specified anywhere in the paper.\n\n## Suggestions\n- The authors defer background and problem setup to the appendix. It is important to note that the appendix is generally where readers go to get additional details that are not essential to the paper. The problem setup and background is an essential part of a paper, especially for readers not familiar with MDPs, Reinforcement Learning, as well as abstractions and their value in RL. This section should be brought into the main paper.\n- As mentioned above, the term \"synergism\" should be defined clearly.\n\n## Typos\n- Page 3, below Equation 1: ... supplymentary material.\n- Algorithm 1, line 2: ... an episodes T\n- Section 3.3, line 1: ... Eq. equation 1 ...\n\n## Summary\nThe idea of phrasing the construction of macro actions as sequential decision making problem is interesting. Further, constructing the entire set of macro-actions together to ensure that they work well together is also useful and clearly demonstrated.\nHowever, the paper needs to be presented so that the ideas are more clearly communicated. More clarity on the experimental details and possibly another baseline to compare against would add value.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An Interesting Idea That Needs More Polishing",
            "review": "I appreciate the work into the topic of macros. This simple technique is known to well work in practice, but in my opinion it is under-studied as we still do not have a good understanding of what makes a good macro and a principled way of obtaining them.\n\nThe paper is generally well-written, and easy to follow.\nUnfortunately, I believe there are several areas where this paper could have improved.\n\n- Definition: the paper emphasizes several times that \"synergism\" of macros is at the core of the research. However, there is no precise formal definition of what the author mean by synergism. The only description of what is meant by synergism is presented in section 4.4. The definition provided is:\n \"An ensemble of macro actions is said to possess the synergism property if the performance achievable by the macro ensemble is higher than the maximum performance of the individual macros within that macro ensemble.\"\nWhat is meant by the performance achievable by the macro ensemble? Is it the return an agent can get? If so, any return achievable through macro-actions is also achievable by primitives. So this definition would need to be more precise.\n\n- Baseline in experiment: the only baseline in the experiments is IEB. Some clear baselines are missing: what is the comparison with an agent only with primitive actions? And a simple scheme of macros where we just use action repetition for macros instead of learning them?\nWithout them, we cannot know whether this method learns macros that improve over these simple baselines.\n\n- Why is a \"null\" action needed? Is it used to denote the end of a macro?\n\n- The state of the controller, is it a complete history of all actions taken? How is this initialized, with a default value? If so, the value of T, episode length plays an important role in efficiently of learning the controller. The dimensionality of the state space could be huge. Am I missing something here?\n\n- In equation 1 we have the definition of the reward function, which is described as \"EvaluateEnsemble\". Based on the algorithm, this is an extremely expensive evaluation, as it requires learning a new policy with the macros. Why are the best and worst 10 episodes eliminated? This seems to be an arbitrary decision.\n\n- Finally, I would suggest having figure right before or after they are being discussed. For example, fig 2 is first discussed in page 6 but the figure is at the top of page 4. I would suggest playing around with the formatting a little bit to make the paper more readable.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}