{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output. \n\nPros:\n1. Handling less structured data is surely an important problem in machine learning and is much less explored. \n2. The paper is well written, easily understandable even with a fast browsing. \n3. The experimental results show some improvement. \n\nCons:\n1. The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4. \n2. By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4's comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental.\n\nAlthough the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper."
    },
    "Reviews": [
        {
            "title": "A simple yet useful technique for tabular data",
            "review": "The paper describes an MLP architectures for problems in which the features do not have a known structure (eg, tabular data). A \"differentiable routing matrix\" partitions the data into K blocks. Then, standard MLPs are applied to each block and the results are recursively aggregated by moving forward in the model.\n\nOn the plus side, the paper is well written, the topic is significant (as evidenced by any Kaggle competition on tabular data), and the model is simple enough to be understandable even on a quick reading.\n\nHowever, I do have a few concerns that put the paper on a borderline situation.\n[Addendum after review: most of the concerns have been addressed by the authors with a large set of additional experiments.]\n\n1. The paper is based on the assumption that a \"split-then-combine\" prior is sufficiently flexible to handle most real-world cases. However, this is kind of strongly limiting, eg, each feature can only contribute to a single \"high-level concept\" in the network. I don't feel that the paper does a strong job in justifying such an inductive bias.\n[Addendum after review: As the authors point out, this is only partially true, as each feature can be selected for multiple groups.]\n\n2. In the experimental part, I don't understand the motivation for using images. I can't imagine a single case in which one is given raw sensory data with absolutely no structure on it.\n\n3. Concerning the other datasets (the tabular ones), the authors report a decent improvement compared to an MLP, with a significant decrease in complexity. However, I do not understand the rationale for excluding random forests (or any non-neural baselines) from this comparison. A random forest would be the default algorithm in these situations (both in terms of accuracy and speed), and making MLPs competitive is one of the motivating factors in the paper itself.\n[Addendum after review: the authors have added a random forest to the experiments. However, this is the only method whose parameters are not fine-tuned (which might be a smaller problem for random forests). No time comparison is given, and some accuracy deviations appear well within the standard deviations. Still, this is a very good addition to the paper.]\n\n4. Is interpretability a concern here? Inspecting the routing matrix should reveal information on the feature grouping. I feel this can provide a strong boost to the paper.\n\nSummarizing, point (3) is the most important. Showing that the proposed Group MLP is superior to a classical alternative should be the critical aim of the comparisons.\n[Addendum after review: This point has been partially addressed.]",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "learning group features ",
            "review": "The authors proposed a neural network architecture, Group-Connected Multilayer Perceptron (GMLP), which automatically groups the input features and extracts high level representations according to the groups. This paper focuses on classification problems.\n\nThe architecture can be decomposed to three stages. The first stage is to automatically group the input features by multiplying soft-max of a routing matrix. At the second stage, a locally fully connected layer with the corresponding activation functions is used for each group, and a pooling layer merges two groups to a new group. At the final stage, all the groups would be concatenated and input to a fully connect layer to get the final output. \n\nThe experiments showed that GMLP outperforms vanilla MLP, SNN, SET, FGR on seven real-world classification datasets in different domains. GMLP ensures higher accuracy with lower complexity compared to vanilla MLPs.\n\nThe authors claimed that if we consider the groups as leaves, this method then becomes growing a binary tree from the leaf to the root. The extensive experiment results showed the effect of GMLP hyper-parameter choices, e.g. number of groups (number of nodes), width of each group (size of nodes) and type of pooling layer (way to built parent nodes). But in terms of a tree, it would be interesting to have some experiments to show the effect of the way combining feature groups. \n\nThe experiments on the simulated Bayesian network dataset supported the claim that this architecture can utilize the fact that some of the features are not related and do not need to interact with each other. However, the architecture the authors used corresponds to the model that generated data, which is almost impossible in many real life problems. It can be helpful to have some results on simulated data with mismatched architectures from the model to help better understand the performance. \n\nOne of the most important ideas in this paper is limiting the group-wise interactions. The size and number of groups the experiments chose would lead to many overlapped groups and many features chosen multiple times. It would be nice to have some analyses on the chosen groups and selected features, e.g. the existence of a set of features that always come into one group, and/or comparison of derived groups by GMLP and randomly chosen groups.\n\nA more detailed explanation of the dataset would be needed. For example, the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset.\n\nIf the complexity analysis of GMLP, equation (7), is only for inference, please also include the training complexity. Another concern is that the results suggest that the number of feature groups may need to be quite large (compared to the number of original features). In addition to figure 2, please provide the analysis of model size in addition to complexity analysis. \n\nGMLP selects features by using soft-max of a $km \\times d$ matrix. The authors may want to investigate reparametrization tricks to solve similar problems, including concrete relaxation in the following references: \n\nC. J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.\n\nMuhammed Fatih Balin, Abubakar Abid, and James Y. Zou. Concrete autoencoders: Differentiable feature selection and reconstruction. In ICML, 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "There are many domains where the data is tabular and does not contain local relationships as in the audiovisual domain. The use of Multilayer Perceptrons (MLP) discards any known prior, for instance, the structure between features. This paper proposes a method that groups features and applies operations (fully connected layers, ReLU, batch normalization) only to the same group member.",
            "review": "################################\nPro:\n$\\bullet$ It is an exciting idea to learn the relations in among feature dimensions, group them, and apply the operations within groups to train an MLP.  The experiments on various datasets, and important ablations such as the effect of number and size of groups, types of pooling were done.\n\n################################\nCons:\n$\\bullet$ $\\mathtt{Group-Pool}$ operation applies pooling inside feature group. Looking into Table 6 of the Appendix, mean and max-pooling were used in different datasets.  The ablation study is done on CIFAR-10 (Fig. 4). The assumption is always to have pairs of groups and apply pooling to consecutive group members. It contradicts the initial idea. Why didn't you apply a routing layer to learn which layers to pool together?\n\n$\\bullet$ *Ablation studies on image dataset:* Among many datasets experimented on; it is difficult to understand why all ablation studies are conducted on CIFAR-10. The paper's main motivation is to propose a method that works on domains beyond image, voice, and graphs. However, all ablations are done on an image dataset naturally that has spatial relations. \n\n$\\bullet$ *Incremental performance gain:* Looking into the performance comparison with vanilla MLP and other methods (SNN, SET, and FGR), the only considerable improvement from the MLP baseline is on the CIFAR-10 dataset. The results on all other datasets, accuracy, and area under curve scores are incremental. How many trials did you conduct the same experiments (the stdev values inside brackets of Table 2)? Did you run any statistical significance test between the trials of GMLP and MLP? \n\nMLP baselines can be reported from an architecture matching with the number of parameters in GMLP. In Table 2, the number of parameters should be written or standardized in each method.\n\n$\\bullet$ *What are \"MIT-BIH datasets\"?* They are not mentioned, referred to in the text, but given in Fig. 2 (b)\n\n$\\bullet$ *Finding optimal numbers of m and k:*  Finding the optimal number of groups and group sizes can introduce a large computational overhead. Fig. 5 and 6 show the performance margin could be 10-15%. Did you compare the optimal numbers of $m$ and $k$ between the datasets with a similar feature dimension? In other words, are the optimal values calculated in one dataset generalizable to other datasets to eliminate the necessity to train many models to find a good value.\n\n$\\bullet$ How does the group routing evolve during training? In a learned model, what is the relation between inter-group and intra-group feature correlations? (Are the members of the same group more relevant to each other in vice versa?)\n\n################################\nMinor:\n$\\bullet$ *As a term, the use of \"group\":* Use of group as a term causes confusion in the reader. In the introduction, it may be useful to include that the expression is not related to group ina mathematical sense, and only represents a subset of feature dimensions.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}