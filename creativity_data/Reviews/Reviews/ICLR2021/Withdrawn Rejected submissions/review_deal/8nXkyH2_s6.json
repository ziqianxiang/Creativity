{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper studies the behavior of the intermediate ReLU-like activations of trained neural networks and show empirically that the intermediate activation can be used as a hashing function for the examples with some key advantages, including almost no collisions and that there are desirable geometric properties (i.e. can use k-means, k-nn, logistic regression on these embeddings).\n\nPros:\n- The experimental analysis was solid and thorough investigating the effects of model size, training time, training set, regularization and label noise. \n\nCons:\n- An overall lack of novelty. It is already quite well-known throughout the ML community that in many cases, using the intermediate embeddings serve as useful features to apply more classical methods such as kNN and clustering.\n\nOverall, the reviewers appreciated the solid and thorough investigation into the hashing properties of neural network activation patterns, which convincingly confirms some intuitions about the behavior of activation patterns in neural networks. However, the reviewers also agreed that there was no significant new finding. There have already been many studies on clustering and kNN on the embeddings of a network.\nThus, the core novelty of the paper appears to be the finding that almost every linear region has at most one datapoint after training (which does not seem too surprising given Hanin & Rolnick (ICML 2019)); however, without further novel implications of this finding, the impact of the paper is limited."
    },
    "Reviews": [
        {
            "title": "MNIST alone is not enough to draw any decisive conclusions",
            "review": "This draft proposes to use the relu activation pattern of the neurons in the neural network as the hash code for the input. Essentially the input features are bucketized into small piecewise linear regions. The authors show empirically that the proposed hash code has small collision and high accuracy given certain conditions including\n1. The features are around the sample manifold \n2. the training time is long enough\n3. the network is wide enough\n4. the training sample size is large enough\nThe authors also found empirically the effect of regularization is relatively small on the encoding properties.\n\nI feel this is an interesting thought but I am not sure if this is the first work on it. Some other possible issues:\n1. Figure 4(a) somehow shows the redundancy is the smallest at epoch 0, then it goes high after 1 epoch and decreases slowly as the number epochs grows. Can authors provide some explanation on this? Does it suggest the random initialization of the neural network gives a good hash code in terms of the redundancy metric? (of course the accuracy will be bad)\n2. The authors used K-means as another benchmark to compare. To me k-means is an unsupervised clustering algorithm. How do you get the accuracy from k-means? How do you match the cluster id to the labels?\n3. My biggest complaint is that only the MNIST data set is investigated in the experiment. MNIST is too easy to show any conclusive results. You may need to work on other data sets such as image net, cifar 100, or NLP related data set to draw a convincing conclusion.\n4. All the conclusions are purely empirical. Can authors provide some explanation or intuition on why the redundancy ratio decreases as the training time grows? Is this related to the type of optimizer being used? Why can a larger sample size also help reduce the redundancy ratio?\n\n\nOverall I think this draft has some really good ideas but the empirical result is not quite conclusive due to the lack of extensive experimentation.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review",
            "review": "**Update after rebuttal:** I thank the authors for their detailed responses and the additional experiments. The responses addressed most of my concerns. I noticed that I had the wrong notion of redundancy ratio in my mind (I'm glad the authors now give a more formal definition of this concept as I think this would trip up many other readers). I'm also glad that the authors have clarified the difference between their results and those reported in Hanin and Rolnick (ICML, 2019). Given these, I'm happy to increase my score to a weak accept (a weak accept, because I'm still not quite sure about the significance of the results reported in this paper). \n\n--------------------------------------------------------------\nThis paper reports some observations about properties of linear regions in deep ReLU networks. Unfortunately, I have several major issues with the paper. \n\n(1) First of all, I am a bit confused about the motivation behind this work. The motivation is initially couched in terms of hash codes, so one gets the impression that the authors are going to propose a new hash coding scheme using neural networks. But this is clearly not the case. The proposed coding scheme is practically useless as a hash code because of the enormous dimensionality of the codes (I estimate this to be on the order of millions even for the toy MNIST case studied in the paper, but it would actually be very helpful for the reader if the authors explicitly mentioned the dimensionality of the proposed hash codes--at least the order of magnitude--). This is also why the authors are stuck with the toy MNIST dataset throughout the paper, because the proposed scheme is completely impractical for any reasonably large dataset and model.\n\n(2) This begs the question: what exactly is the significance of the observation that linear regions satisfy some properties of hash codes, if they’re not going to be used as hash codes? It’s not meaningful to just point out that something satisfies some properties of hash codes. One can point to a million different things that satisfy some properties of hash codes. What exactly is the significance of linear regions having these properties?\n\n(3) This brings me to the related works section. This section is written very shallowly, the authors do not do a good job of situating their contributions in the context of prior works. You cannot just say: “Other advances in linear region counting include …” (p. 3) and then cite a bunch of references. You have to tell us what each of these papers did and how what you’re doing in this paper differs from these earlier works and makes a meaningful and novel contribution to the prior literature.\n\n(4) The concept of redundancy ratio seems to play a central role in the paper, but it is not defined formally, just a verbal (potentially ambiguous) definition is given. Please define this concept formally to avoid any ambiguities. I’m also not sure this concept is the right one for quantifying the goodness of a code: consider a case where each point in a dataset shares its linear region with exactly one other point in the dataset vs. a case where all points belong to the same linear region. It seems that the redundancy ratio will be 100% in both cases, however intuitively the code in the first case should be much better than in the second case (for example for retrieval).\n\n(5) Relatedly, Figure 3c suggests that the redundancy ratio is close to zero for an untrained random network. Then, by the authors’ own definition, the encoding is actually very good before training. Please consider what this means (also see point 7 below).\n\n(6) Unfortunately, I don’t think classification results for MNIST only are very meaningful. Almost anything will get above 99% accuracy on MNIST. Moreover, no effort is made by the authors to understand what drives good test accuracy in these experiments. A very straightforward explanation is that accurate classification is primarily driven by higher layers, so one actually doesn’t need most of the dimensions in the hash code for good classification performance (similar “cache” models using high layer features have been proposed before: e.g. Dubey et al., CVPR 2019; Orhan, NeurIPS 2018; Khandelwal et al., ICLR 2020).\n \n(7) Most importantly, one of the main phenomena observed in this paper (the change of redundancy ratio over training) has already been reported in Hanin and Rolnick (ICML, 2019): they note that the number of linear regions in a deep ReLU net first decreases and then increases during training (please read their section 3 carefully). This would easily explain the trajectory of the redundancy ratio observed in Figure 3c (and in Figure 4a) in this paper. Moreover, the concept of diameter is also rigorously defined (and diameters of linear regions studied) in Hanin and Rolnick (ICML, 2019), but this is not acknowledged at all by the authors. This is a pretty serious omission. \n\n(8) Typos: should be: “Mapping induced by a relu network” (p. 1), “it is worth noting” (p. 2)\n“Geometric properties of linear regions” (p.3). “Another diameters of linear regions”? (p. 5)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting empirical observation that intuitively makes sense - but currently lacking some important controls.",
            "review": "**Update after authors' response**\nI am very happy to see the additional results on CIFAR-10, and the layer-wise ablations and other control experiments. To me, these results have shed a lot of light onto how the observed hashing effect can be explained. These explanations mostly confirm intuitions. Nonetheless I think it's worth reporting the empirical verification of these intuitions that tie in with earlier results reported in other papers. To me, the most interesting aspect of the work is how the hashing properties change over training (and across layers). The paper could still be improved by experiments on e.g. CIFAR-100, ImageNet and non-vision tasks, as well as more mathematically sophisticated definitions of some of the measures (e.g. average stochastic activation diameter). I personally think the results are now sufficient (and sufficiently backed up) for a publication and most of the criticism raised by the other reviewers has been addressed sufficiently (for me). I would now rate the paper as a 6.5 - but to facilitate the reviewer's discussion I will take a clear stance and have thus raised my score to 7.\n---\n\n**Summary**\nThe paper investigates activation patterns in ReLU networks, which are known to be piecewise linear. The main finding is that trained MNIST classifiers produce unique activation patterns for most points from the data-distribution (but not for random data). To be precise, an activation pattern is a binary matrix where each entry corresponds to one ReLU unit, and is 1 if the unit has non-zero activation and 0 otherwise. This means that trained MNIST classifiers can be viewed as “hashing” each data-point into a unique activation pattern. Importantly, simple clustering/classification algorithms (K-means, K-nearest-neighbors, and logistic regression) on these hashed patterns lead to good classification accuracy, implying that the hashed patterns follow some underlying geometric regularity (which is not trivially expected from an arbitrary hashing function).\n\n---------------------------------------------------------------------\n**Main contributions, Novelty, Impact**\n1) Systematic study of the “hashing” behavior of MNIST classifiers. To the best of my knowledge this particular type of analysis is novel. The findings are interesting, though perhaps not totally surprising. The impact of the results is currently limited by 2 important factors: (i) results are shown on MNIST only, and (ii) it is currently unclear whether the hashing effect is mostly explained by early layer activations, or not. To address (ii) layer-wise ablations would be crucial (see improvements for more details).\n\n2) Various ablations and control experiments to identify the impact of certain hyper-parameters and architectural variations. The ablations and control experiments are interesting to further characterize the observed hashing effect. Two important control experiments are currently missing: (i) results for untrained networks, and (ii) results when fitting and random labels (to determine whether the observed effects are mainly input-data-driven, or whether the hashed landscape is mainly shaped by label information).\n\n3) Definition of an “Activation Hash Phase Chart” with three qualitatively distinct reasons. The main idea behind this is interesting, but the paper never shows such a diagram, and (importantly!) how the chart is meant to be used (hyper-parameter tuning, or rather as a diagnostic tool, …?). It is thus currently unclear whether such a chart could have wider impact in the community.\n\n---------------------------------------------------------------------\n**Score and reasons for score**\nThe main observation in the paper is interesting and supported by a number of experiments. Unfortunately the generality of the findings is currently fairly limited since evaluation happened for MNIST classification only, and some control experiments are missing. I think the ingredients for a strong and interesting paper are there, but it does not quite come together yet. I therefore currently suggest a major revision of the work, which means resubmission at another conference. I personally want to strongly encourage the authors to spend a bit more time and work on the paper to turn it into the strongest version possible - in which case I think the paper could be quite impactful.  I am of course happy to reconsider my final verdict based on the other reviews and the authors' response.\n \nPerhaps the most important unaddressed point is how the observed hashing property can be reconciled with the good generalization performance: if each test-datapoint is mapped to a unique pattern, how does the classifier generalize well? I suspect that the hashing property is mostly a property of early layers that gets washed out towards the output of the classifier (but it might also be the case that the hashing property holds essentially until the final layer). I think the hashing property itself is not very surprising, but what’s surprising is that the “goodness-of-hash” changes quite a bit over training - not only for training points but also test data, but not random data. Shedding more light on this would make the paper much stronger in my opinion.\n\n---------------------------------------------------------------------\n**Strengths**\n * Observed hashing effect and hypothesis stated fits quite well with the wider literature (which is also well cited in the paper).\n * The experiments that are conducted are thorough (multiple repetitions per run, covering a broad range of hyperparameters).\n * To me Fig 3c is the most convincing result that the hashing effect is (at least partly) an caused by training and not simply a generic feature neural networks with a certain width.\n---------------------------------------------------------------------\n**Weaknesses**\n * Results are on MNIST only. Historically it’s often been the case that strong results on MNIST would not carry over to more complex data. Additionally, at least some core parts of the analysis does not require training networks (but could even be performed e.g. with pre-trained classifiers on ImageNet) - there is thus no severe computational bottleneck, which is often the case when going beyond MNIST.\n * The “Average stochastic activation diameter” is a quite crude measure and results must thus be taken with a (large) grain of salt. It would be good to perform some control experiments and sanity checks to make sure that the measure behaves as expected, particularly in high-dimensional spaces.\n * The current paper reports the hashing effect and starts relating it to what’s known in the literature, and has some experiments that try to understand the underlying *causes* for the hashing effect. However, while some factors are found to have an influence on the strength of the effect, some control experiments are still missing (training on random labels, results on untrained networks, and an analysis of how the results change when starting to leave out more and more of the early layers).\n\n---------------------------------------------------------------------\n**Correctness**\nOverall the methodology, results, and conclusions seem mostly fine (I’m currently not very convinced by the “stochastic activation diameter” and would not read too much into the corresponding results). Additionally some claims are not entirely supported (in fullest generality), based on the results shown, see comments for more on this.\n\n---------------------------------------------------------------------\n**Clarity**\nThe main idea is well presented and related literature is nicely cited. However, some of the writing is quite redundant (some parts of the intro appear as literal copies later in the text). Most importantly the writing in some parts of the manuscript seems quite rushed with quite a few typos and some sentences/passages that could be rephrased for more fluent reading.\n\n---------------------------------------------------------------------\n**Improvements (that would make me raise my score) / major issues (that need to be addressed)**\n1) Experiments on more complex datasets.\n\n2) One question that is currently unresolved is: is the hashing effect mostly attributed to early layer activations? Ultimately, a high-accuracy classifier will “lump together” all datapoints of a certain class when looking at the network output only. The question is whether this really happens at the very last layer or already earlier in the network. Similarly, when considering the input to the network (the raw data) the hashing effect holds since each data-point is unique. It is conceivable that the first layer activations only marginally transform the data in which case it would be somewhat trivially expected to see the hashing effect (when considering all activations simultaneously). However that might not explain e.g. the K-NN results.\nI think it would be very insightful to compute the redundancy ratio layer-wise and/or when leaving out more and more of the early layer activations (i.e. more and more rows of the activation pattern matrix). Additionally it would be great to see how this evolves over time, i.e. is the hashing effect initially mostly localized in early layers and does it gradually shape deeper activations over training? This would also shed some light on the very important issue of how a network that maps each (test-) data-point to a unique pattern generalize well?\n\n3) Another unresolved question is whether it’s mostly the structure of the input-data or the labels driving the organization of the hashed space? The random data experiments answers this partially. Additionally it would be interesting to see what happens when (i) training with random data, (ii) training with random labels - is the hashing effect still there, does the K-NN classification still work?\n\n4) Clarify: Does Fig 3c and 4a show results for untrained networks? I.e. is the redundancy ratio near 0 for training, test and random data in an untrained network? I would not be entirely surprised by that (a “reservoir effect”) but if that’s the case that should be commented/discussed in the paper, and improvement 3) mentioned above would become even more important. If the figures do not show results for untrained networks then please run the corresponding experiments and add them to the figures and Table 1.\n\n5) Clarify: Random data (Fig 3c). Was the network trained on random data, or do the dotted lines show networks trained on unaltered data, evaluated with random data?\n\n6) Clarify: Random data (Fig 3). Was the non-random data normalized or not (i.e. is the additional “unit-ball” noise small or large compared to the data). Ideally show some examples of the random data in the appendix.\n\n7) P3: “It is worth noting that the volume of boundaries between linear regions is zero” - is this still true for non-ReLU nonlinearities (e.g. sigmoids)? If not what are the consequences (can you still easily make the claims on P1: “This linear region partition can be extended to the neural networks containing smooth activations”)? Otherwise please rephrase the claims to refer to ReLU networks only.\n\n8) I disagree that model capacity is well measured by layer width. Please use the term ‘model-size’ instead of ‘model-capacity’ throughout the text. Model capacity is a more complex concept that is influenced by regularizers and other architectural properties (also note that the term capacity has e.g. a well-defined meaning in information theory, and when applied to neural networks it does not simply correspond to layer-width).\n\n9) Sec 5.4: I disagree that regularization “has very little impact” (as mentioned in the abstract and intro). Looking at the redundancy ratio for weight decay (unfortunately only shown in the appendix) one can clearly see a significant and systematic impact of the regularizer towards higher redundancy ratios (as theoretically expected) for some networks (I guess the impact is stronger for larger networks, unfortunately Fig 8 in the appendix does not allow to precisely answer which networks are which).\n\n---------------------------------------------------------------------\n**Minor comments**\nA) Formally define what “well-trained” means. The term is used quite often and it is unclear whether it simply means converged, or whether it refers to the trained classifier having to have a certain performance.\n\nB) There is quite an extensive body of literature (mainly 90s and early 2000s) on “reservoir effects” in randomly initialized, untrained networks (e.g. echo state networks and liquid state machines, however the latter use recurrent random nets). Perhaps it’s worth checking that literature for similar results.\n\nC) Remark 1: is really only the *training* distribution meant, i.e. without the *test* data, or is it the unaltered data generating distribution (i.e. without unit-ball noise)?\n\nD) Is the red histogram in Fig 3a and 3b the same (i.e. does Fig 3b use the network trained with 500 epochs)?\n\nE) P2 - Sufficiently-expressive regime: “This regime involves almost all common scenarios in the current practice of deep learning”. This is a bit of a strong claim which is not fully supported by the experiments - please tone it down a bit. It is for instance unclear whether the effect holds for non-classification tasks, and variational methods with strong entropy-based regularizers, or Dropout, ...\n\nF) P2- The Rosenblatt 1961 citation is not entirely accurate, MLP today typically only loosely refers to the original Perceptron (stacked into multiple-layers), most notably the latter is not trained via gradient backpropagation. I think it’s fine to use the term MLP without citation, or point out that MLP refers to a multi-layer feedforward network (trained via backprop).\n\nG) First paragraph in Sec. 4 is very redundant with the first two bullet points on P2 (parts of the text are literally copied). This is not a good writing style.\n\nH) P4 - first bullet point: “Generally, a larger redundancy ratio corresponds a worse encoding property.”. This is a quite hand-wavy statement - “worse” with respect to what? One could argue that for instance for good generalization high redundancy could be good.\n\nI) Fig 3: “10 epochs (red) and 500 epochs (blue),” does not match the figure legend where red and blue are swapped.\n\nJ) Fig 3: Panel b says “Rondom” data.\n\nK) Should the x-axis in Fig 3c be 10^x where x is what’s currently shown on the axis? (Similar to how 4a is labelled?)\n\nL) Some typos\nP2: It is worths noting\nP2:  By contrast, our the partition in activation hash phase chart characerizes goodnessof-hash.\nP3: For the brevity\nP3: activation statue\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Hash encoders of Neural networks",
            "review": "In this paper, the problem of linear partition in the linear spaces of neural networks with ReLU-like activations is studied. It demonstrates that such a partition exhibits two properties, including determinism and categorization, across a variety of deep learning models \n\n————————————————————————————————————————————\n\nThis paper presents an interesting work, however, there are a few issues/comments with the work:\n\n1.This paper mentions that “simple classification and clustering algorithms, such as K-NN, logistic regression, and K-means can achieve fairly good training accuracy and test accuracy on the neural code space”,  About “achieve fairly good training accuracy and test accuracy”, maybe some comparisons with existing methods will strength this statement.\n\n2.The paper concludes that model capacity, training time, and sample size play important roles in shaping the encoding properties, while several popular regularizers have little impact on the encoding properties.  As for the conclusion of the regularization technique, it is better to try other methods, such as drop-out, to verify gradient clipping and weight decay not only on MLP but also on RNN, before reaching such a conclusion. Moreover, only for MLP on MNIST, it seems unconvincing.  It will be better to try out some existing/well-known NN models on more datasets. \n3.I was confused about the sentence below Section 5.4 “we trained 345 MLPs on the MNIST dataset” and the last sentence in the caption of Figure6 “Totally, 115 models are involved in one scatter. “. If possible, could you please explain that?\n\n4.In Figure 2 (c) and (d), “Diameter“ on the horizontal axis correspondings to the “Layer width” in Figure 2 (a) and (b)\n\n5.What is the meaning of “Frequency” on the vertical axis?\n\n6.In Figure 6, the third and fourth subfigures mean that for some of MLPs with depths between 3 and 100, “ without BN” would be better than  “with BN”, BN sometimes harms the performance of these models? is that so?\n\n7.It needs some space between the second and the third paragraphs below Figure 2.\n\n8.In deep models, depth generally is more important than width. Why only analyze the model capacity by width?\n\n9.For the definitions of three regions “\tUnder-expressive regime”, “Critically-expressive regime”, and “Sufficiently-expressive regime”, may not need to be repeated on page 8, as similar definitions are explained on page 2. Also, is that possible to quantify/formalize them? If so, it would be more interesting and useful.\n\n10.The format of references is inconsistent, for example, references “Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al.. 2017” and “Li Yuan, Eng Hock Francis Tay, Ping Li, and Jiashi Feng. 2019”.\n\n==========================================================================================\n\n\"In addition, I am not sure the description would be enough to reproduce and no code seems to provide.\" In the beginning, I did not find the codes related to this paper, but later, the author(s) uploaded codes. Thanks.  For the codes,  if setting random seed program-wide in codes maybe it will be more helpful for reproducing. \n\n==========================================================================================\n\n————————————————————————————————————————————\n\nOverall, I think this is an interesting piece of work that may be of interest to people to explore/understand (deep) neural networks, but I think the results/conclusions require more careful/extended analysis.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}