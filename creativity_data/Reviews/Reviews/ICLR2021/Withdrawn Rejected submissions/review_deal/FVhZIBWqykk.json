{
    "Decision": "",
    "Reviews": [
        {
            "title": "A clean well-motivated approach to attention",
            "review": "This paper describes a novel attention method that makes use of submodular functions to effectively give diminishing attention on an input item over time.\n\nPositives: it's a clean, motivated approach, with a good empirical study showing gains across several tasks.\n\nNegatives: it doesn't feel like a very large step over previous work, although it is certainly a useful contribution.\n\nOverall this is a clearly written paper. One important point: it is a little unclear how the submodular functions are used in training and decoding of the model. Are they inserted into the computational graph in both training (calculation of gradients) and decoding? \n\nA few other questions:\n\n* how much does this approach add to efficiency/complexity of training and decoding? \n\n* Section 3.3 seems to be a little ad-hoc. Intuitively it would seem that given an appropriate definition of F_1 you could achieve the same effect. Is there some reason that can't be done?\n\n* After the modification in section 3.3, can the authors confirm that the resulting model is fully differentiable?\n\n* For translation models, in practice (I think) many heuristics are used to ensure that the output produced is of (roughly) the correct length. Could the authors explain what cost function they use in decoding, and how that interacts with coverage?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "diminishing return of attention",
            "review": "This paper aims at introducing submodularity into the attention mechanism, such that even with the same attention score, a source word that has been covered more in the past would receive less attention. To achieve this goal, this paper proposes a modification to the attention mechanism: the attention score is computed as the difference between a submodular function over accumulated attention scores and the same function over past accumulated attention scores (excluding the current time step), through which diminishing return is introduced. Experiments on a document summarization task, an image captioning task, and a machien translation task show that a model finetuned with the proposed submodular attention mechanism gets better performance and better coverage over source words.\n\nPros:\n1. The modification is simple and generally applicable.\n2. Experiments confirm the effectiveness of this approach.\n\nCons:\n1. This work is not well motivated . Numerous works on attention coverage regularization can also solve the attention coverage issue, why is this method better? To demonstrate its superiority, this work at least needs to compare to other related works, such as the \"constrained softmax attention\" proposed in https://www.aclweb.org/anthology/D17-1036.pdf, which can ensure that the attention coverage of any source word is at most one. The claim that some other works require to change loss or to introduce new parameters is not very convincing since the introduced parameters only account for a negligible fraction of total parameters.\n2. AFAIU, one problem of the proposed method is that the new attention scores might not sum up to 1, since if you use normalization then the submodularity might get lost. In fact, I doubt the sum of attention scores might decrease over time as the gradients are getting smaller, which seems to be undeseriable.\n3. I don't see the advantage of the dynamic variant over the main approach, the performance improvements seem marginal.\n\nQuestion:\n\"we fine-tune with our diminishing attentions\", does that mean that the proposed mechanism is applied only in the finetuning phase, but it needs a training phase using normal attention? If so, why don't you directly train with it?\n\nTypos:\n1. is represented as a one-hot vector of length -> it's not a one-hot vector since there might be multiple ones\n2. the derivative $fâ€²(\\theta)$ is non-increasing in $\\theta$. Since $\\theta$ is a vector, this is not very clear.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A strong model contribution, but missing a convincing explanation of why it works",
            "review": "\nOverview\n=============\n\nA relatively straightforward paper bringing back the \"classic\" submodular objectives for summarization to neural models.  The paper has strong results, including on human evaluations.\n\nPros: Good numerical results, I appreciate the idea of bringing back submodular functions to summarization\nCons: Lack of analysis of how submodularity helps, lack of analysis of which submodular functions work best, no qualitative analysis of results\n\nDisclaimer: I'm not an expert on summarization, so I'm not up to date on the latest research\n\nExperiments\n=============\n\nThe experiments and comparisons seem reasonable to me.  While I'm not up to date with the most modern neural summarization approaches, these citations and comparisons seem reasonable to me.  I like the use of multiple related tasks including the image-paragraph task.\n\nThe human evaluation seems reasonable, although I would have preferred a little more explanation in the prompt about what \"representativeness\" and \"readability\" mean.  I would not trust crowdworkers to disentangle / understand these concepts without a clear plain language definition.\n\nI would have also liked to have seen confidence intervals, although the gains seem large enough that this would not be an issue here.\n\nAblation / Exploration Studies\n=============\n\nI would have liked to have seen a more thorough study of the components with a more skeptical view of what is helping the summarization.  I would have liked a little more exploration of the role of the possible submodular functions (both for individual submodular functions and for the dynamic diminishing attention) and then how these translate into the effective coverage.\n\nBetter visualizations of these functions and how this results in better summarizations would make it clear that this is the right approach to improve summarization / generation.\n\nI also wonder how far down the stack these new candidates are: if you just added an appropriate penalty at decoding time, would this give similar results?\n\nModel\n=============\n\nApart from adding the modularity to attenuate modularity, the model changes are relatively slight.  This is a benefit, as it should be relatively easy to add to other models.\n\nLack of Qualitative Analysis\n=============\n\nThe biggest problem in the paper is a lack of qualitative analysis.  While I appreciate the examples in the appendix, I would like to see more of an analysis of *why* this approach seems to be working better.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Seems to be a new idea but poorly demonstrated or justified",
            "review": "For encoder-decoder neural sequence generation, this paper defines a new attention mechanism called diminishing attention based on submodular functions.\nExperiments on text summarization, machine translation, and paragraph-level caption generation show that the proposed attention mechanisms could outperform the base systems in terms of the automatic metrics.\n\nPros:\n+ The idea seems to be new to me\n+ Seems to have brought marginal gains over the state-of-the-art numbers of the automatic metrics such as BLEU, ROUGE, and CIDEr.\n\nCons:\n- Insufficient clarity & casual descriptions without sufficient justification\n- Lack of reproducibility as important details are missing\n- Lack of an important baseline for the most part of the experiments: adding a normal coverage mechanism over the base system\n- Lack of informative qualitative analysis to show how and why the proposed attention scheme improves over a normal coverage baseline\n- Difficult to interpret the source of performance gain in terms of the metrics\n\nThe clarity of this paper is not good enough. As a reader I do not think I have understood the logic of why submodularity should appear in hidden states or tokens.\nThe paper says \"Intuitively, the information gain from adding w to S' should be higher than adding it to S...\"\nHowever, language as a sequence of words is rarely treated as a set of tokens in the era of neural networks.\nThe claim of the diminishing return property on language does not even seem to be well-defined.\n\nTechnically speaking, many parts of the descriptions seem to be casual.\nSome of the claims need citations or justifications. For instance:\n\"it can be shown that many popular extractive summarization methods (Carbonell & Goldstein, 1998; Berg-Kirkpatrick et al., 2011) optimize a submodular objective.\"\nIf that MMR as a submodular objective has been proved by Lin and Bilmes (2010;2011), then what about the Berg-Kirkpatrick et al. (2011) formulation?\nAlso, the SotA claim at the bottom of the first page is almost meaningless for text generation tasks, since none of the metrics reported in this paper has been convincingly proved to be reliable for measuring the real generation quality in practice.\n\nThere are important details missing that makes it difficult to reproduce the results of this paper without a released implementation. For example:\nWhich non-decreasing concave function was finally used as g() to produce the reported performance and why is this function specifically being used instead of other alternatives?\nHow does the specific choice of g() affect the performance?\nHow does this modification affect the training process due to the increased complexity of computation?\n\nMeanwhile, more qualitative analysis over a few case studies will make the paper more informative and more convincing.\nCurrently, readers have no clear idea on why and how the generated output has changed under the new attention mechanism.\n\nOne big issue with the submission is the shortage of direct comparison with other coverage mechanisms.\nFrom the currently reported numbers, it is difficult to know whether the marginal performance gain is due to better coverage modeling or indeed from the consideration of the diminishing return property.\n\n\nOther comments:\n\nPunctuations should be added to the end of those mathematical equations whenever needed.\n\nThe addition operation (\"+\") is not defined between a set and a discrete element.\n\nAlso, a citation is needed for introducing this concept and the related terminologies or claims since these are not original concepts in this paper.\n\nThe necessity of the paragraph describing \"the discrete analogue of concave functions\" does not seem to be clear in the context of this work.\n\nFig 1 caption: \"attentions\" => \"attention weights\"\n\nThe term \"optimality of 0.63\" does not seem to be technically precise.\n\nThe paper currently does not discuss the relationship with this highly relevant paper:\n- Kiddon et al. Globally Coherent Text Generation with Neural Checklist Models. EMNLP 2016.\nIn-depth comparisons between the proposed diminishing attention mechanisms and previous coverage mechanisms are also missing.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}