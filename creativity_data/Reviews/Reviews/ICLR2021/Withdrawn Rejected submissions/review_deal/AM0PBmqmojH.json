{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose to approximate the kernel matrix used in the Sinkhorn algorithm by a combination of sparse + low rank approximation. To do so, the authors propose to compute a low rank approximation of a sparsified (thresholded below a certain value to be 0) kernel matrix using Nyström, and then correct it by adding back the true entries at non-sparse entries, after removing those obtained from the approximation. This results in a matrix whose application then results in sparse + low-rank.\n\nThe first version of the paper contained mostly experimental evidence, which was deemed a bit short by some reviewers.The authors have added theoretical material on the way. Although I believe these are worthy additions, as AC, I do not feel comfortable accepting the paper as of now, because I believe these additions were not properly reviewed. I understand this must be disappointing for the authors, who have sprinted to add new content during the rebuttal phase, but I hope they agree that the rebuttal process is not here to handle entirely new sections, but rather to improve existing parts. In particular, that section should be reviewed by authors knowledgeable on low rank kernel factorization, something I did not see in the pool of reviewers. I also believe the paper still has a few shortcomings. Taken together, I therefore recommend a re-submission.\n\nideas to improve the paper\n\n- the authors claim to use Nyström on a sparsified matrix (see eq. 4). The sparsified kernel is no longer positive definite. I would like the authors to comment on this. I understand Nyström could be used naively without any psd-ness guarantees, but I think a heads-up is needed.There are, furthermore, several local/global factorizations of kernel matrices available out there (e.g. MEKA, https://www.jmlr.org/papers/v18/15-025.html), the main difference here being that the product by such approximation must be guaranteed to be positive for it to work in the Sinkhorn algorithm. I would expect that bounds in expectation to break down sometimes, and therefore result in \"catastrophic\" failures (i.e. nan's). I think that an algorithm that claims to improve or replace another one, and which has such blind spots, needs such additional experiments (I have read the Limitations section in Appendix B, something more precise would improve the paper). I understand these were not part of the original Nyström paper for Sinkhorn, but since this is an increment over that previous work, therefore lacking a bit its originality, more knowledge needs to be contributed.\n\n- For instance, since the authors write an entire paragraph on this (Appendix B), I am surprised that there is not direct mention to the fact that a sparse sinkhorn may simply *not* converge, because it may not satisfy the fully indecomposable property required of matrices for Sinkhorn's algorithm to converge. \n\n- i dont think that users have the various identities (14,15) in mind when they think about \"backpropagating\" through Sinkhorn. What is typically needed is to compute the differentiable properties of the regularized OT matric and/or of the regularized OT cost w.r.t. *point locations* (i.e. x_i). The statement \"LCN-Sinkhorn works flawlessly with automatic backpropagation\" is misleading in the sense that it ignores that problem altogether. Since so many extensions of OT today relay on that differentiability, the section, as it is written now, is problematic.\n\n- several methods claim to be faster of more efficient than Sinkhorn to solve OT. Either these methods display faster theoretical convergence (e.g. by using acceleration) or display faster practical convergence (e.g. heavy ball variants) using synthetic, controlled datasets. Using synthetic data helps exhibit highlight relevant regimes for regularization parameters, including those where LSE Sinkhorn may converge but LCN does not work, or vice-versa. I understand that the authors' wanted to use real data, but it would be great to clarify whether that setup was used because LCN works better there (in which case this becomes more of a paper at the intersection of OT and word embeddings) or because this happened to be the first and only example the authors thought of."
    },
    "Reviews": [
        {
            "title": "A practical OT work that enhances performance on three application tasks",
            "review": "This paper studies how to approximate Sinkhorn computation using more efficient kernel matrix representations (low-rank approach + LSH based sparse approach). Neither of both ideas is completely new, the authors studies a combination of them that hasn't been explored in the literature, and use the proposed tech in three applications: ranking, embedding alignment, graph distance regression. \n\nPro:\n\n- Authors have attempted multiple applications to prove the effectiveness with quantitative metrics. The main contribution seems the empirical validations. \n\nCon:\n\n- I think the presentation is a bit out of focus. Some sections can be left for appendix (e.g., Backpropagation in Sec 4. and Sec 5.) since some are either very standard in the literature or not really solidly experimented. Since I consider the main contribution to be the empirical evidences, the space should left for more details on those empirical experiments (for reproducibility purpose).\n\n- the paper's idea is not particularly innovative. (yet it is not the sole reason for my scoring). \n\n- Some relevant works on low-rank ideas have not been compared/cited. e.g.  \n\nForrow, Aden, et al. \"Statistical optimal transport via factored couplings.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.\n\n- The writing can be improved. A lot citations/references are not particularly relevant to what this paper is about and make some parts not enough self-explained. \n\n----\nThe authors addressed my concerns and I raised my evaluation ratings. \n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review on Warpspeed Computation of Optimal Transport, Graph Distances, and Embedding Alignment",
            "review": "Proposal:\n- First log-linear time algorithms for entropy-regularized OT that work for complex \n  real-world tasks using high-dimensional spaces with little to no loss in accuracy\n  ... many claims in one statement\n- Locally Corrected Nyström\n  ... this would deserve a single paper - just to proof everything is still fine and valid\n  in particular to alternatives\n- In the way this paper is written I am positive it gets accepted \n  (because it fits the writing style of the more recent papers in the field)\n  and contains sufficient novelty\n  ... but I always wonder if good (published) science originates from clarity (or confusion)\n\ncomments:\n- sorry but the text is very hard to follow !\n- 'Optimal transport is concerned with the problem' (on p.3) - I think some introductive\n  work may not harm in the first page\n- the reader is thrown up by terms and references - in my view more confusing than enlighting\n  --> it may not harm to add some brief explainations of terms (from Cuturi:)\n  'A transport plan is a flow on that graph satisfying source (a i flowing out of each node i) \n   and sink (b j flowing into each node j 0 ) ... --- which is simple an optimal flow\n  in a graph ... I am not sure why we not simple can call it like this but need to come up with\n  new terms\n- The paper is written (following the very strange title\n   ... although Cuturi did the same it would be nice if we can stop having marketing titles\n  but focus a bit on science again ... in particular in 10 years many things proposed nowadays\n  are not lightspeed or warpspeed anymore) like providing an all issues solving theory \n  --> this does not improve the readability of the paper\n  For example Eq.2 what is the 'meaning' of (s) and (t) -- I have an idea but it is not written there\n- it is hard to proofread and verify a paper if it is written with the objective to confuse the\n  reviewer ;-)\n- widely incremental work by combing some known ideas (Nystreom, LSH, ...) - with a lot of addon\n  theory which is probably correct but not very clear in the presentation\n- 'Since the Nyström method is a low-rank approximation it only accounts for the\nglobal structure of the kernel matrix K and not for the local structure around each point x.'\n  - this is actually wrong (!) - if the landmarks are indepentent and the number of landmarks\n  aligns with the rank of the matrix - Nystroem will provide a perfect (!) reconstruction\n\t--> there is a lot of work on the approximation bound of Nystroem (and related methods) - see\n  e.g. work by Dhillon\n- where is the definition of 'sparse approximation K^sp' used in Eq 3? \n- how precisely does \\bar{P} (after Eq 3) link to the part around Eq 2? - is the Kernel K_{ij} used\n  here as well and / or where is the actually input data Kernel matrix \n- in Eq 1 what should be a cost function here and how do you obtain C_{ij}?\n- Ok Eq 4 is an actual proposal by balancing (and joining) sinkhorn and Nystroem in one distance formulation\n  and it would not harm to motivate why and where you need such a distance in advance \n  (problem statement --> solutions --> particular strategy --> outlined proposal --> evaluation + proofs)\n- 'Most modern ML models are trained using backpropagation' - lets rephrase it as: nowadays neural network\n  approaches are trained by backprop ... there are many other methods which are not at all trained by backprob\n  for good reasons \n- '... Usually we want to learn embeddings which act as point sets X p and X q and therefore need gradients' \n  - well yes, if we stick on neural networks we need vectorial inputs and hence are often looking for (costly)\n  embeddings - if we do not use NN we may not have this problem (but others)\n- 'We can either estimate these gradients via automatic differentiation' - this is in general the more costly\n  way to do things and I am happy to see that explicit derivations are given\n- regarding table 1 --> before you come with numbers (where you measured something) it would be good to specify\n  details of your scenario (which are omitted before) - in particular which data, which cost function, which parameters \n  a.s.o. -- and although I understand that you like your method most it would still be good to provide some\n  oldfashion baseline (and not - not from sinkhorn)\n- 'We propose the graph transport network (GTN) to evaluate approximate Sinkhorn and enhanced optimal transport and advance the state of the art on this task.' --- fantastic on page 6 you actually outline a more userfriendly motivation\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good practical approach, but not sufficiently clear for a general audiece",
            "review": "Summary:\nThe paper considers the problem of approximating Sinkhorn divergence and corresponding transportation plan by combining low-rank and sparse approximation for the Sinkhorn kernel and using Nystrom iterations as a substitute for Sinkhorn's iterations. The corresponding approach is amenable to differentiation and can be used as a building block in different architectures. Numerical experiments in several settings are performed to compare  the proposed approach with existing ones and demonstrate its scalability.\n\nEvaluation:\nI believe the proposed framework is a valuable contribution in terms of practical performance and wide list of applications where OT could not be used before because of the high computational cost. So, I would recommend accepting this paper. \n\nPros:\n1. High scalability of the proposed approach and linear up to log factors in dimension complexity.\n2. Flexibility of the framework due to a combination of sparse and low-rank approximations, which are complementary to each other.\n\nCons:\n1. Some parts of the paper seem to be not clear for a general audience.\n\na. First page. $n$ is undefined.\n\nb. First paragraph of Sect. 2. What is \"set of embeddings\"?\n\nc. Last but one paragraph on p.2. $d$ is not defined.\n\nd. In (1) $F$ stands for the Frobenius product, does it?\n\ne. Proposition 1. $N$ is not defined.\n\nf. First paragraph of Sect. 5. What is \"OT with multiple heads\"?\n\ng. What is meant as embedding?\n\nh. In the experiments, what is used as the cost function to define the Sinkhorn kernel $K$? If this is an $L_2$ distance, the convolutions can be used to accelerate the standard Sinkhorn and it would be nice to see the comparisons with convolutional Sinkhorn, which is also log linear.\n\ni. Appendix A. $B,r,b$ are not defined when they are first used.\n\nj. In (17), how was the last equality obtained?\n\nk. In (19), (20), how were the first equalities obtained?\n\nl. Appendix E, first paragraph. What is \"log-sum-exp trick\"?\n\nm. Appendix G. What is \"similarity matrix\"?\n\n2. As far as I understood, the proposed approach is not amenable to parallel computations on GPU as opposed to standard Sinkhorn.\n\n\nMinor comments\n1. Maybe it is too strong to state in the abstract that this is the first log-linear time algorithm given that when the Sinkhorn kernel corresponds to a convolution, the Sinkhorn's algorithm is log-linear by using the FFT.\n2. Bibliographical note. (Altschuler et al., 2017) did not show $1/\\varepsilon^2$ bound for the Greenkhorn. Their bound for Greenkhorn is the same $1/\\varepsilon^3$ as for the Sinkhorn. The bound for Sinkhorn was improved to $1/\\varepsilon^2$ in http://proceedings.mlr.press/v80/dvurechensky18a.html and the bound for Greenkhorn was improved to $1/\\varepsilon^2$ in http://proceedings.mlr.press/v97/lin19a.html.\n3. Bibliographical note. Quadratic regularization for OT was proposed in https://arxiv.org/abs/1704.08200.\n4. Appendix A. I believe that in this framework a general value of the regularization parameter $\\lambda$ is used. If it is the case, then the number of Sinkhorn iterations to find an $\\varepsilon$-solution to the regularized problem is $1/(\\varepsilon \\lambda)$. This follows from http://proceedings.mlr.press/v80/dvurechensky18a.html Theorem 1 and an estimate for $R$ in Lemma 1. The bound $1/\\varepsilon^2$ corresponds to finding an $\\varepsilon$-solution for the non-regularized problem. In this case one has to set $\\lambda=\\varepsilon/(4 \\ln n)$, which may be too small.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper and potentially significant.But theory is lacking, and experimental section can be improved",
            "review": "Overall, I found this paper interesting, and I think it does address a relevant problem for the community.\n\n\nI have been playing with Nystrom approximations myself and I know the results are a bit disappointing, but it is grounded on strong theory. Then, it is pretty much welcome that attempts to patch the problems of Nystroms are made, so Optimal Transport becomes more scalable.\n\nThe reason for why my judgement is below acceptance is that I believe both theory and results altogether are not strong enough so they live up to what is promised in the abstract. Typically when new methods are proposed with the promise of bridging a gap and solving a relevant problem, I hope they will have a thorough theoretical justification, or the results will be compelling. None of this convincing enough here.\n\n1)On the theoretical side, I missed a convergence analysis, as the one in the Nystrom method. The theory side focuses on some derivative calculations, but I would love to see how the interplay between sparsity, Nystrom method and LSH lead to better convergence. I acknowledge this can be hard to do, though. Without having the theory it is hard to understand whether any reported experimental result is a consequence of choosing particularly good example for their method.\n\n2)I found the experimental/methodological side was a bit disconnected from the rest; the paper contains  several vignettes about some applications/improvements in the practical side, but when reading that felt like belonging to other paper. I recommend authors work in creating a more coherent story\n2a) I found the discussion on Multi-Head OT unjustified and even a bit misleading. The Authors refer to some NLP papers, like arguing OT plays a role there. But none of these papers have any OT at all. The analogy of  \"softmax for rows\" is not convincing as this is simply a softmax applied many times. There is a world of difference between that and the result of sinkhorn algorithm, but the narrative seems to downplay the actual difference between them. I recommend the authors elaborate more on this connection, because otherwise it is hard to follow (and the subsequent results).\n2b)Results on translation seem impressive (Table 2), but raise a concern. Why would your method outperform Sinkhorn if it is only approximation? is it perhaps the result of randomness? since an explanation of this phenomenon is missing I am led to believing. Authors should improve the exposition of the baseline \"original\". Why does full Sinkhorn does better than original?. In summary, I think authors should improve the discussion about the validity/significance of their empirical results, highlighting the regimes when they are supposed to express and when they are not.\n2c)The main figure is Fig 2. I recommend authors build on this and expand those results so it is clear when their method is better and when it is not",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}