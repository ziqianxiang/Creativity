{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting weighted contrastive loss for transfer learning. Some minor improvements would be beneficial.",
            "review": "The papers proposes a smooth contrastive loss devised for transfer learning. In particular, a weight term is used instead of the class labels to represent the semantic similarity between images in the source feature space. Experiments on deep metric learning and self-supervised representation learning show the benefits of the proposed method.\n\nPros\n- The loss function does not seem to over-complicate the training, reasonably simple to implement and is widely applicable to many tasks.\n- The authors carry out a good analysis of the proposed loss, talking about the gradients and providing some additional motivations for their contributions.\n\nCons\n- This approach seems to be a different approach towards searching class prototypes, whereas in prototypical learning each class prototype can be represented by the average of the embeddings of all the images, in this approach each image weight in the transfer learning is weighted by the similarity to the source embeddings. Could the authors discuss this point?\n- The authors do not explicitly describe the steps required to apply this loss in a transfer learning experiment. For example my understanding is: train a source model, extract features for the target samples and their similarities w, then do transfer learning on target. This could help the reader to understand better the method.\n- In the experiments section 4.1.1, personally I find the description of the source target training not clear (\"Source and target embedding networks\" paragraph). Could the authors please clarify?\n\nQuestion\n- In all the experiments source and target belong to the same dataset? \n\nOverall I consider this paper well written and the proposed loss interesting. I would appreciate if the authors could clarify my concerns and fix some minor issues I pointed out.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official AnonReviewer5",
            "review": "This paper proposes a novel smooth contrastive loss for embedding transfer. Specifically, in contrast to original contrastive loss, this paper utilizes the pairwise similarities between samples as a weight to take place the equivalence indicator. Experiments on several benchmarks show the proposed approach improves the baseline. Overall, I vote for weakly reject for this time, considering the advantages and disadvantages as follows: \n\n####Pros:\n1) The methodology part of this paper is very clear and easy to follow.\n2) The experiments part combine the self-supervised learning approach is interesting, showing the potential application space of the proposed approach.\n\n####Cons:\n1) The novelty of this method is marginal to me, replacing the equivalence indicator with the pairwise similarities is not a big contribution. Besides, this similar idea has been explored in RKD. This also reflects in the motivation, pairwise similarities distillation has been explored by RKD.\n\n2) The experiment is not easy to follow. I appreciate the author provide many experiments, while these experiments make me confused which task the author focus on. I think the experiments part should be very related to motivation. However, it is not very clear to me. The author compared PKT, and RKD in Table1, and I assume the PKT and RKD is one of type knowledge distillation methods, which should be compared in the true distillation context, i.e, compared in CIFAR10, CIFAR100 in distillation context with other approaches including considering teacher network and student network.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper proposes a new loss, termed as the smooth contrastive loss for embedding transfer. This loss aims to pull together or push apart a pair of samples with the strength determined by their semantic similarity. However, the novelty of this paper is limited, since the authors only have a small modification on the original contrastive loss by using the pairwise similarities between samples instead of class labels. ",
            "review": "Strengths:\n(1) The writing is clear and easy to follow.\n(2) The addressed problem that previous approaches fail to utilize detailed inter-sample relations in the source embedding space is solved well.\n(3) The experiments show that the proposed approach can achieve good performance on standard benchmarks or reduce sizes and embedding dimensions of target models effectively.\n\nWeaknesses:\n(1) This paper's novelty is limited since the authors only have a small modification on the original contrastive loss for embedding transfer. \n(2)  There are some typos. (e.g. the third line in experiments )\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical Evaluation Cross Entropy vs Square Loss ",
            "review": "The paper describes an approach for embedding transfer learning. It follows the well known contrastive learning approach and modifies it to include embedding similarities in source task space. Instead of relying on class labels to push together or pull apart the embeddings in target space, the method relies on embedding similarity in the source embedding space. This ensures that distances in the sources embedding space explicitly impacts the embedding learning process in the target space. Empirically, the method is applied for metric learning, self-supervised representation learning and model compression. Results are presented on 3 different benchmark datasets. \n\n\n1. The main idea appears to be simple and intuitively makes sense. How sensitive is the learning w.r.t to the parameter delta ? In the experiments it has been fixed to 1 but I am curious if that is something which may require rigorous tuning \n\n\n2. I am not sure if I understood the motivation behind normalizing the distances by their mean (Eq 6). How does this address the issue of required l2 normalization ? \n\n\n3. Currently the method uses the Gaussian kernel to compute the weights for transfer. I am curious if other similarity/distance measures can be used. It would help if these were also used and evaluated to see the impact of similarity/distance measure. For example in one of the referred works (Park et. al. 2019) also distance and angle-wise distillations are considered. \n\n\n4. It is not clear to me what is the source task from which the embedding transfer is being done. What is the source task (dataset) for the results in Table 1? \n\n\n5. It seems that target models are pre-trained on Imagenet.  How important is it to pre-train the target models on Imagenet ? \n\n\n6. Results and analysis of the self-supervised task does not appear to be very strong. On both datasets, the results are closed to the one before embedding transfer. Moreover, the results with RKD also are comparable for CIFAR-10. I think this self-supervised task needs a  more thorough evaluation to get a better picture. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple but effective method. More variants can be considered.",
            "review": "This work presents a new contrastive loss for self-supervised representation learning. The basic idea is utilizing the embedding learned in the source space to smooth the contrastive loss function. Then, similar samples in the source embedding space will be pushed to be closer to each other in the target space. Motivated by recent works, the authors also involve multi-view augmentation strategy to boost the performance of the proposed method. Experimental results show that the proposed method outperforms other models.\n\nPros:\n- The proposed method is novel and easy to be understood. While the samples are close in the source space, it is natural to enforce them to be close in the target space. The reproducibility is also ensured.\n- Quantitative results demonstrate that the proposed method outperforms state-of-the-art models.\n\nCons:\n- While the authors use the Gaussian kernel to reweight the contrastive loss, it would be more complete to explore more variants. For example, using different distance metrics, or to train a learnable mapping. Or this work seems to be less complete.\n- It is highly suggested to provide parameter sensitivities of the method with respect to $sigma$.\n\nOverall, this work is well written and clear. Nevertheless, I think there is room to improve the proposed method by introducing more different types of distance metrics to instantiate more powerful weights.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary & Strengths\n- This paper proposes an embedding transfer method, smooth contrastive loss. Unlike the original contrastive loss, the proposed one uses the \"smooth\" similarity based on the source embedding space. This allows considering the relations between samples and their importance.\n- This paper demonstrates the effectiveness of the proposed method in two tasks, metric learning, and self-supervised representation learning, by comparing with existing approaches, PKT, RKD, and DarkRank.\n\n---\n\nConcern (or Weakness) \\#1: Baselines\n- It is worth noting that many distillation/transfer methods minimize distance (or divergence) between penultimate layers of source/target models. It means that they can be considered as embedding transfer baselines. I wonder why only PKT/RKD/DarkRank are compared with the proposed method.\n- Especially, Contrastive Representation Distillation (CRD, [Tian'20]) directly maximizes the mutual information between source and target embedding distributions. As reported in [Tian'20], CRD achieves higher performance than PKT and RKD in the distillation task. It means that CRD can be considered as a better embedding transfer method (i.e., a stronger baseline), and thus it should be compared with the proposed method.\n\nConcern (or Weakness) \\#2: Motivation & Verfication\n- The proposed method only transfers similarities ($w_{i,j}^s$), thus it transfers the embedding information ($f^s_i$) indirectly. In contrast, distribution matching (like CRD) transfers the embedding information directly, but the similarity information indirectly. I wonder why the proposed method should be superior to other transfer methods.\n- In this paper, there is no ablation study and analysis. Which component (e.g., Eq (4) or (6)) of the proposed method is most important? Can the main idea be applied to other contrastive objectives (e.g., N-pair loss [Sohn'16])? The authors should verify the proposed similarity $w_{ij}$ is effective, for example, by comparison with 0/1 similarity (using class labels).\n\nConcern (or Weakness) \\#3: Distillation Experiment\n- This paper does not experiment with the popular distillation task (e.g., image classification). I'm curious to know why this experiment is not conducted. If the proposed method is capable of transferring knowledge more effectively than existing methods, the superiority should appear in the distillation task.\n\n[Sohn'16] Improved Deep Metric Learning with Multi-class N-pair Loss Objective, NIPS 2016 \\\n[Tian'20] Contrastive Representation Distillation, ICLR 2020\n\n---\n\nConclusion: While the idea is interesting, motivation/verification is somewhat weak, and experiments seem to be not enough to accept.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}