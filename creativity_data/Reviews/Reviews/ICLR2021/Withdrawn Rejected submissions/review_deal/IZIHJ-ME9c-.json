{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting idea lacking evidence that it works for causal discovery",
            "review": "The paper describes a two-stage supervised learning approach to causal discovery based on training neural networks first to predict all values of random variables based on their masked versions, and second to predict the presence of an edge in the causal graph based on the confidence of previous predictions.\n\nThe proposed approach is a fresh and interesting way to consider the problem of causal discovery. Almost a thought experiment to address the following question: if we could train a model to predict causal edges directly, would we get a model that indeed can recover these edges in previously unseen data and graphs?\n\nWhile the first part of the process makes all sense and is highly practically relevant, the second part raises serious questions in terms of its relevance for the proposed use&#x2014;causal discovery&#x2014;applicability to which is absolutely not established or supported by current experiments.\n\nThe problem with it, the proposed schema cannot be used for scientific discovery as presented and suggested, because the trained model may be unable to generalize to settings that deviate even slightly from those used in the generation of synthetic training data for the model. Neural networks do benefit from pre-training, but performance drops, and sometimes dramatically when moving to new datasets. Fine-tuning usually helps to recover most of it, but the method proposed in the paper will not be tunable on any kind of real data for the reason pointed out in the paper: we do not know the ground truth for the supervised training. Furthermore, the only type of data generation used in experiments is the Gaussian linear model. Will the model trained in this setting still work with non-Gaussian noise? As it stands, without substantial and completely different experiments demonstrating that the approach can sustain out of distribution data and generalizes well, the approach is a curious toy. It is an interesting demonstration of the power of supervised learning, but not too surprising nevertheless. After some thinking, in the supervised settings, one may even be able to train a network to predict the complete graph structure from a set of observations with acceptable accuracy.\n\n# Other comments\n\nThe paper critiques the PC algorithm for reliance on the type of independence test used, but I did not readily find the kind of the test used in the experiments and whether a training (hyperparameter tuning) of the type of test to use was conducted for it. Arguably, PC algorithm has a flexibility in test choice and we could select the best one for the problem in a supervised manner to be fair to the supervised experiments proposed in the paper. The synthetic experiments would be fairer if the kind of the test used in PC was tuned as a hyperparameter.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach but not yet convincing ",
            "review": "This paper explores the idea of treating inference of causal relations as a classification problem and use comparisons of predictive errors with different variables as predictors as features. The idea is interesting and worth exploring, and the paper is overall well written and very readable. However, in its current form the paper is not very convincing, for two reasons. First, there is no suggestion on how the method proposed in the paper may be analyzed theoretically. Second, the experiments, though careful and commendable in other ways, are a little too limited by focusing on PC and GES as the benchmarks. It will be more convincing to include other algorithms for comparison, such as MMHC and NOTEARS, which have shown better performances than PC and GES in the relevant settings. Moreover, it seems advisable to show experimental results on more general data types than binary and Gaussian data, given that one of the motivations is to have a method that remains effective for a variety of data types. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting contribution with deficits",
            "review": "The paper introduces a novel approach to structure learning, based on prediction errors of generative models trained on the observational data. This avoids usual conditional independence tests and therefore renders assumptions on the dependency types unnecessary. The authors introduce their approach, leveraging a generative (dropout) model on the observational data. The implementation is compared to state-of-the art approaches, outperforming these.\n\nOverall I suggest to reject the paper\n\nPros:\n(1): The claim of outperforming state-of-the art structure learning approches justifies the publication, structure learning is a notoriusly\n\n(2): The choice of graphics is well-suited and provides a good additional value to the paper.\n\n(3): The ablation study is a good additional extension of the experimental results (even though it might be more suited for an appendix)\n\nCons:\n\n(1) The problem the paper focusses could be stated more clearly. The term \"causal discovery\" alone is used for a multitude of problems, where structure learning is only a subset of. \n\n(2) While the brevity of the first chapters help the fluidity and readability of the paper, a more granular structure might provide additional value. \n\n(3) This holds as well for stating theorems and definitions within the text, which might decrease the fluent structure of the text but serve to highlight important \n\n(4) A more thorough comparison of the introduced model against other, deep-learning based structure learning approaches (e.g. CGNN) would be useful\n\nMinor details\n\n(1) Baseline values for table 2 would be interesting to see\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A different idea but weak empirical evaluation and many issues are left unsolved",
            "review": "The paper proposes an interesting idea of random perturbation of inputs for causal discovery by observing the predictive errors. The method consist of 2 steps: first generation of multiple such pairs, and then fitting a supervised model for obtain causal graphs. \n\nStrength: \n- the idea is definitely different from existing causal discovery approaches. \n- quite a good set of empirical evaluations\n\nWeakness:\n- some related works, which contain very related ideas such as on masking (Ng et al, Masked Gradient-Based Causal Structure Learning) and input-output pairing (Lachapelle et al, Gran-DAG), are not cited. Comparison with these and other deep learning based neural methods are missing as a result. \n- PC and GES are not strong baselines, and more state art methods should be compared, such as Yu et al, DAG-GNN and Zheng et al NOTEAR (the nonparametric extension). \n- Some understanding on the number of pairs are needed or some sort of convergence guarantee, as indicated by authors in conclusion, would be helpful. Currently, the paper left many such understandings unanswered.\n- Efficiency of different methods are not reported and compared.\n\nOther comments:\n- for continuous variables, do you standardize all variables or just masked variables?\n- what would happen if one trains each dropout to convergence? Does this improve accuracy?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}