{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a new method of employing some existing techniques to improve robustness, which was verified through experiments. According to the reviewers’ comments and the authors’ responses to these comments, the reviewers generally appreciate the authors’ effort in properly improving and clarifying the proposed method. However, their major concerns still rely on the novelty of this paper, which is identified as a combination of some existing techniques. In addition, the proposed method at its current status still contains some un-convincing points. Hence, the paper is recommended rejected."
    },
    "Reviews": [
        {
            "title": "Good empirical results but there are concerns about the novelty and clarity of the provided justifications ",
            "review": "**Summary:**\nThe paper proposes a new way to combine existing techniques for improving adversarial robustness: adversarial training, Jacobian regularization and TRADES consistency loss. The obtained results on three datasets are better than the baselines which rely on adversarial training, Jacobian regularization or TRADES separately.\n\n**Pros:**\n- Good empirical results.\n- Extensive empirical evaluation on MNIST, CIFAR-10, CIFAR-100.\n- Ablation study for the components of the method.\n\n**Cons:**\n- My main concern is the novelty of the proposed approach. The paper proposes to use adversarial training together with the TRADES-loss (with a reversed KL-divergence) and a variant of approximate gradient penalization at an adversarial point. All these approaches existed before but now just all combined together.\n- Additional concern is that the method requires 2 additional hyperparameters alpha and beta compared to usual adversarial training that doesn’t lead to any additional hyperparameters. I think this concern is especially relevant since the method is proposed to be useful for fast adversarial training (i.e. with one-step adversarial examples).\n- Another concern is the clarity of the presentation. It was really hard to grasp what are the sets $P_x$, $P_x’$, $S_\\theta(x)$, $S_{\\theta^1}(x)$, $S_{\\theta^2}(x)$ and relations between them. Maybe it would be better to clarify it with a picture / diagram. Currently, the theoretical part doesn’t seem to be clear or convincing enough to me.\n- It would be more insightful if one can provide a clear discussion on how and why the proposed method mitigates the catastrophic overfitting problem (similarly to these recent papers [Li et al. (2020)](https://arxiv.org/pdf/2006.03089.pdf) and [Andriushchenko et al. (2020)](https://arxiv.org/pdf/2007.02617.pdf) which focus on overcoming this problem). There are some mentions \n\n**Minor suggestions**\n- For me, it seems to be a bit misleading to call a subset of the input space a patch given that in the literature, patches are mostly referred to perturbations on the image plane (e.g. as in [Adversarial Patch paper](https://arxiv.org/abs/1712.09665)).\n- Contributions 2 and 3 at the top of page 2 are nearly duplicates.\n- “we **take care** of local balls at various points” -- imprecise (what it means to take care in this context?) and a bit informal language.\n- Equation (6): in the denominator it should be f_c’ instead of f_c in the second term.\n- Page 5: “as we evaluate it at an adversary x’ instead of the image x” -- it’s not yet clear what “an adversary x’” means, in particular with which method this point is obtained, is it epsilon-bounded (I suppose yes but this becomes clear only much later in the text) or not. Would be good to clarify this.\n- Page 7: “validation set robust accuracy guided learning rate scheduler” -- consider splitting this phrase which is too complicated.\n- Page 7: “natural images will **prevail** when weak adversaries are used” -- meaning is unclear.\n\n**Score:**\n5/10 because of the concerns about the novelty and clarity of the provided justifications of the method.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Initial review",
            "review": "In this paper, the authors introduce a novel technique (called LEAP) that improves model robustness at local patches (around an adversarial example) and combines \"local patches\" to get global robustness. Their approach works better than other techniques when using a \"weak\" adversary.\n\nOverall, the paper is well structured. However, it is a bit confusing at points, and sometimes not clearly motivated. The experiments seem fair and demonstrate that the proposed approach is better than other state-of-the-art techniques.\n\n1) I find the notion of local patches difficult to understand, and the paper seems to go through many hoops to justify a rather simple idea (simple ideas are good). This paper reminds me a lot of [1] and the loss presented could be justified through other means.\n2) Talking of [1] (although this is not necessary due to short amount of time between NeurIPS acceptance and ICLR submission), I'd appreciate a discussion on the differences.\n3) When mentioning PGD, cite [2] (for BIM). When mentioning FGSM, cite [3].\n4) When W^x and b^x are initially introduced, it is unclear what they are representing. Is that a first order Taylor expansion around x?\n5) A few sentences seem useless and sometimes the language is hand-wavy: e.g., \"it is highly likely the volume of patch P_x is non-zero\".\n6) The whole paragraph before section 4.1 does not seem to be useful for the rest of the paper.\n7) It is unclear in Sec 4.1 whether proposition 1 is part of Hoffman et al. (2019).\n8) It took me a while to understand the A(J(x)) notation. A() seems to be a function that takes J(x) as input. Consider directly using the Frobenius norm |J(x')|_F here and mention that it can be approximated.\n9) Re-ordering the final loss a bit, we have L_leap = l(x') + \\beta * KL(...) + \\alpha * |J(x')|_F. The first two terms are reminiscent of TRADES (except that they are applied to x') and the last term acts as a regularization term (similar to one used in [1] or [4]). Could the experiments also include an alternative loss:  l(x) + \\beta * KL(...) + \\alpha * |J(x)|_F (combining TRADES and zero-step JAC).\n10) When mentioning Untargeted attack with margin loss, cite [5]. When mentioning Multi-targeted, cite [6].\n11) The authors seem to use beta = 4 for TRADES. However, TRADES seem to work better with beta = 6. In particular, training a WRN-28-10 using TRADES with 10 steps of PGD on CIFAR-10 should reach 51-52% robust accuracy, the authors get 49.66% for a WRN-28-8. Can the authors explain the gap?\n13) The authors should also compare with \"Adversarial training for free!\" [7]\n\nOther comments:\n\nA) Qin et al. [4] is lumped with techniques that requires a strong adversary. However, Qin et al. show that only 2 PGD steps are necessary.\nB) Table 1 could include the number of steps rather than the vague \"multi-step\" wording.\nC) The Discussion seems more like a Conclusion.\n\n[1] https://arxiv.org/pdf/2007.02617: Understanding and Improving Fast Adversarial Training\n[2] https://arxiv.org/pdf/1607.02533: Adversarial examples in the physical world\n[3] https://arxiv.org/pdf/1412.6572: Explaining and Harnessing Adversarial Examples\n[4] https://arxiv.org/pdf/1907.02610: Adversarial Robustness through Local Linearization\n[5] https://arxiv.org/pdf/1705.07263: Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods\n[6] https://arxiv.org/pdf/1910.09338: An Alternative Surrogate Loss for PGD-based Adversarial Testing\n[7] https://arxiv.org/pdf/1904.12843: Adversarial Training for Free!",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary:\nThe authors developed a novel robust training algorithm LEAP to focus on the effective use of adversaries. The proposed method improves the model robustness at each local patch and combines these patches through a global term, achieves overall robustness. The authors showed by maximizing the use of adversaries, they achieved high robust accuracy with weak adversaries. Furthermore, when trained with strong adversaries, the proposed method matches with the current state of the art on MNIST and outperforms them on CIFAR-10 and CIFAR-100.\n\n\nComments:\n\n1 . The authors’ main idea is to promote local patch robustness combined with global robustness. While the local patch robustness is further related to the Jacobian norm, the global robustness is exactly the same as TRADES’ regularization term. My major concern is that the proposed method still does not have a very convincing intuition that why the local patch robustness term or combining the two terms helps. In particular, the LEAP_g algorithm is actually very close to TRADES, why it could achieve better robustness. The authors might want to add more explanations/demonstrative experiments to show that.\n\n\n2 . In eq (5), what does it mean by decision boundary = 0? Also in eq (6) what is standard computation? I assume the authors refer to first-order Tylor expansion? The authors need to reorganize the presentation of this part to make everything clear. And equation (5)/(6) is actually useless since finally, the authors only rely on Proposition 1 to promote the local patch robustness term. For the approximation of the Jacobian norm, the authors might want to briefly introduce some details on how the approximation is done.\n\n3 . The following work also performs robust training directly on x’ instead of x,\n\n\"Improving adversarial robustness requires revisiting misclassified examples.\" ICLR (2019).\n\nThe authors might also want to comment on it.\n\n4 . Notice that even adversarial training based algorithms could cause obfuscated gradient problem, therefore, it might be a good idea to further evaluate model robustness via totally gradient-free methods, such as hard-label attacks \n\n“RayS: A Ray Searching Method for Hard-label Adversarial Attack” KDD (2020)\n\nIn order to make the experimental results more convincing.\n\n\n5 . In section 4.1, “Since l∞ norm is equivalent to l2 norm”. It is fine to only present L2 norm analysis but this statement is not appropriate and may cause confusion. In cases where the data dimension N is large, the difference between the two settings could also be drastically different.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited novelty and some concerns regarding the experimental results",
            "review": "The paper proposes a new adversarial training scheme, LEAP, to obtain models robust against $\\ell_\\infty$-bounded adversarial examples. The loss used as the objective minimized during training involves both local and global (wrt the input space) properties of the network. Experiments suggest improved performance compared to single and multi-step standard adversarial training and TRADES.\n\nPros\n1. The proposed method achieves in the reported experiments better results than existing methods, for both single and multi-step adversarial training.\n2. The authors provide detailed ablation studies in the appendix.\n\nCons\n1. The final loss used looks like a straightforward combination of TRADES and a regularization term on the norm of the Jacobian matrix at the adversarial points. As mentioned in Sec. C.1, as differences from TRADES, first LEAP uses the adversarial images rather than the clean ones when computing the cross entropy loss, and second it computes the adversarial examples maximizing the cross entropy rather than the KL distance. Both seem minor modifications: the first change should lead in principle to lower clean accuracy (in Table 1 LEAP models have 1-2% lower clean accuracy than TRADES ones on CIFAR-10 and CIFAR-100) and worse trade-off clean vs robust accuracy, while the second one is not discussed in the main paper. Moreover, the regularization of the Jacobian matrix is, as acknowledge in the text, not novel.\n2. Although the experimental results seem to favor LEAP, I have some concerns about the hyperparameters used in PGD at training and test time. For the multi-step adversarial training on MNIST, with $\\epsilon=0.3$, 20 steps of PGD with step size 0.01 are used (Sec. B), meaning that the $\\ell_\\infty$-ball cannot be fully explored by the attack. Conversely, on CIFAR-10 and CIFAR-100 a step size of 0.07 > 17/255 is used with and $\\ell_\\infty$-ball of radius 8/255, which is quite large with a budget of 10 steps. For testing the robustness of the models, the MultiTargeted attack is used: it is a strong attack, but it is given a budget of only 20 iterations, which might be insufficient for the attack to be effective. Increasing the budget for the attacks or using some standard evaluation like that from (Croce & Hein, 2020) would strengthen the results.\n3. When evaluating LEAP on cheap adversarial training, considering that the computational cost is roughly 3x that of one-step ADV, also the method from (Andriushchenko & Flammarion, 2020) should be included in the comparison, as it should have a runtime similar to the proposed scheme and it is shown to outperform one-step ADV.\n\nOverall, I think the novelty is very limited, which combined with some concerns about the experiments makes me lean towards giving a negative score.\n\nCroce & Hein, \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\"\\\nAndriushchenko & Flammarion, \"Understanding and Improving Fast Adversarial Training\"\n\n---\nUpdate after rebuttal\n\nI thank the authors for the response. I think that the revised version improved clarity. The overall impression I have of the paper is still similar to the initial one.\n\nThe final proposed loss is reasonable, but just the combination of two existing ones with minor modifications. About this, even in the revised version the authors seem not to discuss and justify (at least in the main part) how the adversarial point $x'$ in computed at training time, which is different from TRADES according to Section C.1.\n\nAbout the experimental part, I thank the authors for adding the new experiments in Section D. However, the baseline in Table 4 seems a bit weak, at least for 10 steps ADV. For reference, the baseline WRN-28-10 in (Gowal et al., 2020) has robustness under AutoAttack + MultiTargeted close to 51% (I see that here a WRN-28-8 is used, but I wouldn't expect such difference).\nMoreover, although a minor concern, the authors didn't address the question about the step size used on MNIST.\n\nThen, I keep my initial score.\n\nGowal et al., \"Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}