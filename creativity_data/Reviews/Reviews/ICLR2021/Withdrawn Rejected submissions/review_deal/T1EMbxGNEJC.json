{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Despite the performance gains of RankingMatch over the benchmarks used in the paper, the reviewers remained concerned about how the paper compares to state of the art in several respects."
    },
    "Reviews": [
        {
            "title": "Review of RankingMatch",
            "review": "This paper considers the problem of semi-supervised learning, where the training set contains both labeled images and unlabeled images. To address this task, the paper proposes a method called RankingMatch which exploits the idea that input images having the same label should have similar model outputs. To incorporate this idea into training, the paper develops a BatchMean Triplet loss to guide the model learning from the unlabeled images. Experiments are conducted on the CIFAR-10, CIFAR-100, SVHN, and STL-10 datasets. Results of RankingMatch show some improvements over competing methods in some cases.\n\nQuality: The presentation of this paper is clear. Figure 1 clearly illustrates the pipeline of the framework. This paper reads well and is easy to follow.\n\nClarify: This paper clearly presents the proposed method and the motivation of each design choice is clearly described. While some of the components are from existing papers, the paper clearly acknowledges the source. This allows me to understand which parts of the method are built upon existing approaches and helps me better identify the contributions and new components of this paper.\n\nOriginality: Generally, the idea of this paper makes sense. However, there are several parts similar (if not the same) to existing methods. The cross-entropy loss in Equation 2 is a standard loss function that is commonly adopted in classification tasks. The loss in Equation 3 is from FixMatch (mentioned in the paper as well). The triplet loss and contrastive loss are two loss functions that are widely applied in recognition and representation learning tasks. All of these loss functions to me are standard and common knowledge in the community. The idea of applying ranking loss on the model outputs is highly similar to a recent paper published in ECCV 2020 [a]. The idea of learning from the unlabeled data in [a] is to leverage class-wise similarity scores to form a representation and use this representation to describe an unlabeled data. For a pair of unlabeled images, [a] uses the class-wise similarity representation to assign positive/negative labels to that pair of images by comparing the distance between the two similarity representations. After assigning pseudo labels to that pair of images, [a] applies contrastive loss to further learn from the unlabeled data. The idea in this paper is very similar. The output of the model (classifier) can also be viewed as a form of class-wise similarity distributions. Namely, for each unlabeled image, the classifier outputs a distribution that indicates the similarity of that input image to each class of the labeled set. To me, the high-level idea is similar. However, the authors did not acknowledge the similarity with [a] in the submission. This will make readers feel like the idea of applying ranking loss on the model outputs is original in this paper.\n\n[a] Chen et al. Learning to learn in a semi-supervised fashion. In ECCV, 2020. https://arxiv.org/pdf/2008.11203.pdf\n\nSignificance: Given that some parts of this paper are highly similar to [a], the significance of this paper is downplayed. Also, there are several components in this paper are from existing/well-known papers. Based on these grounds, the contribution of this paper is limited.\n\nRequest for author response: I would like to see how the authors compare their paper with [a] in terms of the idea of class-wise similarity and representation learning. In particular, I would like to know the pros and cons of this paper compared to [a]. It would be great if the authors can talk about whether the semantics-oriented similarity representation in [a] could be used (and how to use it) to help improve the performance of the proposed method in the setting concerned by the paper.\n\nRating: Given that there are many parts similar to [a] and they are not acknowledged in the paper and many of the components in the paper are from existing approaches, I can only rate 4 for this submission at this point. I will reevaluate this paper after seeing the reviews from the other reviews as well as the author response.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but would require more in depth study",
            "review": "This paper proposes a novel method, called RankingMatch to the problem of semi-supervised learning (SSL). It extends known consistency regularization - based SSL methods by adding an additional loss that encourages similarity of outputs for samples of same class. In addition to, the paper proposes a new version of the triplet loss, called Batch Mean Triplet loss (BMT).\n\nIn general, there are some innovative and novel parts in this work, but in total it is somewhat incremental since big parts of the method and the results result from FixMatch, which serves as the basis for this method. The experimental results are promising, although not conclusive, for instance with larger amount of classes in cifar100, a different version of the method works better compared to a dataset with less classes, cifar10 (see Tables 1 and 2).\n\nPros:\n\nThe proposed idea to regularize the class output consistency between samples of the same class is interesting and should be studied further by the research community.\n\nAnother interesting idea is the BatchMean triplet loss that seems to be novel and could also be utilized in other settings.\n\n\nCons:\n\nThe clarity of the paper could be improved by clearly stating the differences and similarities to the closest method (I assume FixMatch in this case) in other parts than the intro as well. This applies to all variants of the proposed RankingMatch method.\n\nOne problem I see in this work is that the method is based on a relatively ad-hoc idea of class output similarity between samples of the same class. This assumption might hold for some datasets and not for others - this should have been studied in itself for various datasets. Definitely there may be correlation for many datasets, but one could imagine that for instance in tasks with big amount of classes, the correlation could sometimes be relatively low. For instance, in Imagenet, the output could include other objects that accidentally happen to be in the image together with the main object. Anyway it is interesting to see it works to some extent with the simple datasets, but it should be tested with Imagenet like was done with FixMatch.\n\nAnother problem is the batch-specific nature of the proposed loss variants that require also samples from the same class. This is not typically a problem in non-class-specific contrastive losses, but in this case, especially when the amount of classes increase (e.g., 1k or 10k), the proposed may start to perform worse when the same-class samples co-appear less often in the same batch. \n\nEdit: I have read the changes and the author responsed, but have decided the keep the score.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "No clear advantage over FixMatch",
            "review": "Summary:\nThe paper presents an SSL method extending FixMatch by introducing an auxiliary loss motivated from the metric learning literature. For example, the triplet loss is utilized to define the loss, where any triplet of anchor, positive and negative examples, either using ground-truth labels for labeled data or pseudo-labels for unlabeled data, are used to compute the loss. Variants using hard triplets of the mean as well as using the contrastive loss are proposed. The proposed methods are evaluated on standard SSL benchmarks.\n\nReview comments:\nWhile authors presented ablation study with various metric learning losses proposed in this work, the biggest concern is that none of these losses shows the clear performance improvement over FixMatch when combined with FixMatch losses. This begs the essential question on the effectiveness of the proposed losses on top of FixMatch. If RankingMatch is not clearly more performant than FixMatch, is there any scenario that FixMatch cannot be applied but RankingMatch can? \n\nIt is also unclear to me the motivation for introducing auxiliary metric learning losses -- why is it a good addition to the FixMatch loss? Which aspect of FixMatch loss is problematic and how does adding metric learning loss fix that issue?\n\n\"We argue that the images from the same class do not have to have similar representations strictly, but their model outputs should be as similar as possible\" --> There seems no supporting empirical evidence to this statement.\n\nReason for decision:\nGiven the fact that the proposed method is constructed incrementally to the previous method (i.e., additional loss to FixMatch loss), the initial decision criteria is its empirical effectiveness. Unfortunately, I am not convinced that the proposed method is clearly improving upon previous methods in any settings considered in the paper, so I would recommend for rejection.\n\nI have read author response and thank authors for their response. I have decided to keep my initial rating. I am not convinced by the response that the proposed method has a clear advantage over Fixmatch as some of their reported numbers for Fixmatch are very different (worse) from the ones from the original paper.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting work; more thorough experiments will be helpful",
            "review": "********Summary\nIn this paper, they introduced a new objective function, dubbed BatchMean Triplet loss, for Semi-supervised learning on image classification problems. In this framework both labeled and unlabeled data are used for training at the same time. The labeled data will be augmented weakly and cross entropy loss as well as ranking loss will be computed on their output. The unlabeled data will be augmented weakly and strongly, and the label based on the weakly augmented will be a pseudo label for the strongly augmented input, and similarly cross entropy and rank loss will be computed for the unlabeled data. For the rank loss, they used Contrastive loss and BatchMean Triplet loss, which is a variant of Triplet loss. They applied Triplet and Contrastive loss to the model output instead of image representation.\n\n********Positives\n- The paper is well-written and well-organized. Figure 1 is self-explanatory, showing the general framework introduced in this paper.\n\n- They covered background and related works to this paper when explaining their method, which makes the work clear.\n\n********Notes\n- My main concern about this work is lack of contribution. This could be negligible if I see a better result in their experiments section. Table 1, their performance is very similar to FixMatch(RA). I do not see much improvement of their method over baselines in Table 2 (they are on CIFAR-10 and CIFAR-100 only).\n\n- They have mentioned that \"Our BatchMean Triplet loss has the advantage of computational efficiency of existing BatchHard Triplet loss while taking into account all input samples.\" It would be interesting to quantitatively see how much their method is computational efficient.\n\n- It would be interesting to see the comparison of two ranking losses explained here (BatchMean Triplet loss vs Contrastive loss).\n\n- I would like to see the comparison of their method against baselines on all of the datasets explained here. I understand the limited computing resources issue, that is really unfortunate that some institutes cannot provide enough computational resources for their researchers experiments. But as a reviewer, I need to see how this method works on a wider range of datasets.\n\n********Reason to accept or reject\nThe paper is well-written and they explained their method clearly, which is important. My concern is lack of contribution. If they could provide more thorough experiments, this could be more helpful work for other researchers in this area. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}