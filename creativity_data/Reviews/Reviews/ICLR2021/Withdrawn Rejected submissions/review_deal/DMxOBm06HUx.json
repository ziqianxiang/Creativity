{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper suggests extending pre-trained contextual language models to use both fine-grained and coarse-grained tokenizations of the sentence. A sentence is tokenized twice and then each is passed through a transformer block, with shared parameters except the embeddings. Having 2 granularities shows gains.\n\nPros\n\n- Easy to read paper, straightforward method\n- Gets experimental gains from using word/phrase combo\n- Evaluates on a range of tasks\n\nCons\n\n- Novelty is limited, since other models like SpanBERT and ZEN already explore different tokenization granularities\n- Improvements may come as much from the ensemble of two models as the two tokenization granularities\n- Number of parameters or amount of computation are increased by method, though authors do significantly address this in their revised paper.\n- Some over claiming of results when there are modest incremental gains on small models (the abstract sentence \"outperforms the existing best performing models in almost all cases\" suggests that we are going to get results of a new model outperforming the state of the art models on tasks, but really we get improvements over baseline models at the BERT-base size. I believe this is fine for experiments to show the scientific value of ideas but it should not be described as it presently is in this abstract.\n- Gains are more for Chinese than English\n\nOn the better results for Chinese: Isn't the reason that the results are more impressive for Chinese because in Chinese the fine-grained version is just single characters, but this is more fine-grained than standard BERT word pieces in English, where the word pieces are already commonly words, most of which would be two or more character sequence in Chinse (whether for common words like, say \"fishing\" or \"vault\" or place names like \"Mississippi\"), so the fine-grained Chinese here is more fine-grained than the standard English wordpieces, and so not too surprisingly there are bigger gains from using the Chinese word segmenter granularity. But really this is sort of equivalent to how the original BERT authors showed that you could get gains by masking whole words not individual word pieces. And at any rate, the value of word segmentation for Chinese was already shown by Yiming Cui et al.'s paper on Chinese BERT, no?\n\nOverall the strong majority of reviewers were unconvinced that this paper was suitable for ICLR 2021. They mainly emphasized concerns of novelty, missed or unfair comparisons, concerns of extra parameters or computation, and the fact the paper is somewhat incremental. I would add to that that to the extent that this paper is primarily an examination of the value of different granularities, that feels much more like a linguistic question for an NLP conference than an ML question well suited in particular to a conference on learning representations like ICLR. That is, the choice of granularities is hand-specified, and/or the grouping is done by simple n-gram statistics, not by learning representations. As such, I do not think the paper should be accepted to ICLR at this time, and in general think that an NLP venue may be more appropriate for it. "
    },
    "Reviews": [
        {
            "title": "Require better comparison.",
            "review": "The paper combines fine and coarse-grained tokenizations to learn word and phrase-level representations. The authors introduce two variations on AMBERT: (1) using two separate encoders for fine and coarse, and (2) combine fine and coarse into a single encoder. This method is more expensive in terms of computations. The improvement is incremental, but it seems very effective in representing Chinese. However, the motivation for adding fine and coarse granularities is not well introduced. \n\nStrengths:\n- The performance is consistently good, although AMBERT has more parameters than baselines: BERT and ALBERT.\n\nWeaknesses:\n- The model is more complex and computationally more expensive (2-4x)\n- The proposed method seems not very effective for English, only to Chinese\n- Lack of intuition\n\nQuestions and Suggestions:\n- Can you train a model with the same parameters as the baselines? I think it would show the significance of the approach.\n- Please elaborate more about the intuition of the proposed method\n\n** Post-Rebuttal **\n\n> I want to thank the author for addressing my concerns. I will keep my score. The paper has merits, but the comparison is not fair since they have different parameters with the baselines unless they have smaller parameters like ALBERT. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting model proposal but not experimentally rigorous enough",
            "review": "\nThis paper introduces AMBERT, a general purpose pre-trained model that uses both fine-grained and coarse-grained tokenizations of the sentence. Given a sentence, AMBERT tokenizes it with both vocabs, then each sequence is passed independently through a shared transformer block. During pre-training, BERT’s masking function is applied at the coarse-level, and the corresponding tokens are also masked at the fine-grained level. During fine-tuning, each tokenization leads to a sentence representation that is used for a classification loss. This classification loss comprises an agreement term, incentivizing the classifications based on each representation to be close. The goal of this paper is to explore whether using multiple levels of granularity for tokenization can lead to better models.\nThe authors investigate the effectiveness of this method in the monolingual setting, proposing an English and a Chinese version of the model.\n\n### Pros:\n* The question of how to best tokenize sentences and whether multiple tokenizations of the same sentence are useful has been understudied in the large language models realm. This paper draws attention to this question and starts to shed some light onto it.\n* The authors have evaluated on a wide range of tasks and on both Chinese and English tasks. \n* The method seems to perform well overall and better than the two proposed baselines (Combo/Hybrid). However, some \ncomparisons to other systems are more unfair. (see cons) Results on CLUE are more compelling than their GLUE counterpart.\n\n### Cons:\n\n* It is not clear on what metric the authors purport this method to improve on other ways to improve accuracy in models. The authors highlight in the abstract that the model “outperforms the existing best performing models in almost all cases”, making it a key contribution of their paper. However: \n    * The proposed method adds significant overhead at all steps (pretraining/fine-tuning/inference) since the transformer block is run twice on every sentence. A fairer “Our BERT” baseline would be trained for twice longer or use an ensemble of two BERTs. All the more concerning is that this baseline is not far off from the performance of AMBERT on many English tasks.\n    * The authors highlight that their method does well among methods of less than 200m parameters, but this does not seem to be a very good basis for comparison:\n\n      * Most methods do not share parameters between two encoders like AMBERT, requiring significantly less compute. ALBERT has shown in the past that the # of parameters in a model could be significantly reduced at the cost of more compute. \n      * The authors do include ALBERT, but they do so inconsistently. For Chinese, ALBERT x-large is included while for English ALBERT base is included. This is a surprising choice, especially considering that ALBERT x-large/large do better than AMBERT. It would be better to include a consistent ALBERT model and highlight that ALBERT uses more computer/param due to layer sharing.\n      * The threshold of 200m parameters is arbitrary and AMBERT is significantly closer to it than most baselines.\n\n  * The numbers reported in the external baselines are often not ideal/misleading. Take the case of the ELECTRA model in table 6 for instance. The authors choose to use the ELECTRA-base number in the paper instead of the ELECTRA-base++ one just below (see Table 8 in the Electra paper). ELECTRA-base uses 1/16th of the steps \\* batch_size \\* seq_length that the AMBERT model shown uses.  ELECTRA-base++ still uses 1/4 the steps \\* batch_size \\* seq_length that AMBERT uses. ELECTRA++ performs 2.2 points better than ELECTRA on GLUE without WNLI, which puts it firmly above AMBERT on GLUE. On SQUAD (average of SQUAD 1.1 and 2.0 dev numbers), the authors report a performance of 74.8 for their own fine-tuning of ELECTRA compared to 84.775 on the paper. This makes it hard to put a lot of faith in the comparison with external systems, both for numbers copied from the papers and from numbers obtained through fine-tuning.\n  * For CLUE, the results are more impressive. It is not entirely clear to me whether the data augmentation mentioned for some tasks is used for all the models (including external ones) or only for AMBERT and its baselines. I am less familiar with this benchmark so cannot comment as precisely.\n\n* “Note that the number of parameters in AMBERT is comparable to that in BERT because of the parameter sharing.” -> This ignores the added ~50m+ parameters added due to the additional word embedding table, which represent ~50% of BERT Base’s total parameters.\n* Ablations: Though computational requirements are an obvious constraint that limit the number of experiments that can be done, key ablations that would better highlight the effectiveness of the proposed method are missing:\n  * The authors mention that many approaches have used coarse-grained approaches to masking and obtained improvements, particularly BERT-wwm and SpanBERT. They implicitly use a similar masking for AMBERT (since they use the masks derived from the coarse-grained for both the coarse-grained and fine-grained representations) but their BERT baseline uses the subpar wordpiece masking.\n  * Using the same masked tokens for both tokenizations of the sentence might be subpar for the AMBERT-Hybrid model, limiting how it can learn interactions between coarse and fine-grained representations.\n  * It would be good to ablate the impact of the agreement regularization term. This should be easier than other ablations as it is fine-tuning specific. The use of this additional hyperparameter with several values (1 or 0, see Table 8 and 9) also means the AMBERT/AMBERT-Combo models likely had more fine-tuning runs than their BERT baseline counterparts.\n* Given the high variances of fine-tuning on small data GLUE tasks such as RTE/COLA/MRPC (see for instance STILTs Phang et al, Revisiting Few-sample BERT Fine-tuning Zhang et al), it would have been good to have more than a single run for those.\n* There is little discussion of previous work that uses different levels of tokenization, such as models using both character and word-based inputs. There is also no discussion of works that allocate additional tokens/representations for entity-like multi token expressions (such as those obtained in a coarse-grained representation such as “New York”). Some examples of the later include KnowBERT, Entities as Experts (the authors do mention ERNIE, but mostly for its masking function, not the use of additional entity representations). This is especially relevant since many of the phrases shown in Appendix E are indeed entity-like.\n* The approach is quite brute force. In reality, the differences between the two tokenizations can be small (see for instance the examples shown in Appendix E), so re-running the entire model seems inefficient. It should be possible for instance to tokenize Sentence 1 Example 1 in Appendix E in an approach like Ernie or E-BERT (Poerner et al):\n    * What Star Trek episode has a nod to Doctor Who? Star_Trek Doctor_Who (Ernie style)\n    * What Star Trek / Start_Trek episode has a nod to Doctor Who / Doctor_Who ? (Ebert style)\n\nOther concerns:\n* I was not able to find anywhere whether English version of the model uses a cased or uncased vocabulary. This is important to mention in a paper that proposes to study tokenization. \n* It was not entirely clear to me whether the 70k vocab is a superset of the 30k one. If not, how can you ensure that the masked tokens are aligned for both sentences?\n\nMinor:\nSection 4.2: Mix-precision -> mixed precision\n\n### Recommendation:\n\nIn light of the above comments, **I believe this paper should be rejected**. There are important flaws in the presentation of the results and comparisons, both to external models and baselines. When correcting for those, the results are not as impressive and the claims of “SOTA” that are central to the paper are more dubious. \n\nI believe the paper could be made stronger by trying to assess what are the changes introduced by using multiple levels of tokenization. There are interesting observations of the differences in impact it has across tasks (seems to be more useful for RC), and across languages (seems to be much more useful for English), etc. There are also likely less brute-force ways of introducing multiple levels of tokenization that would offer better complexity trade-offs, as suggested in the last con.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel and effective multi-grained BERT",
            "review": "Summary:\n\nThis paper proposes a novel Transformer-based architecture AMBERT which uses the multi-grained characteristics of several natural languages. The novelty of the work relies on two encoders sharing parameters: one focusing on a fine-grained representation of the text (characters or words) and the other one on a coarse-grained representation of the text (words or n-grams). Both fine-grained and coarse-grained representations are used during pre-training and fine-tuning. The architecture, which combines fine-grained tokenized representations with coarse-grained tokenized representation, achieves very strong performance on benchmark datasets for Chinese and English, outperforming BERT and other Transformer-based models most of the time. The authors also provide ablation studies and analyses using variations of AMBERT: AMBERT-Combo (fine-grained and coarse-grained encoders but without parameter sharing) and AMBERT-Hybrid (one encoder using the concatenation of fine-grained and coarse-grained representations).\n\n\nPros: \n-\tThe architecture is novel and take advantage of the multi-grained components within the sentences (character, word, n-gram) while sharing parameters between the two encoders and thus improving efficiency.\n-\tThe architecture’s performance is validated empirically on many tasks and datasets. AMBERT outperforms most of the time recent Transformer models on both Chinese and English benchmark datasets.\n-\tAblation studies in the form of a comparison with similar architectures (Combo or Hybrid), analyses of attention maps and distance between representations are proposed and help understand the performance and differences between models\n\n\nCons: \n-\tEven though the performances are almost always significantly better than BERT or other Transformer models, the cost is also a significant increase in the number of parameters to train (108M for Google BERT vs 176M for AMBERT in Table 1 and 110M vs 194M parameters in Table 4)\n\n\nQuestions: \n-\tCould the authors provide more information regarding the coarse-grained tokenization algorithm used? \n-\tRegarding the last paragraph of section 4.5, could the estimated quantities be biased/inaccurate if the 10K sampling is performed only once?\n \n\n\nMinor Comments: \n-\tThere is a typo at the end of the second paragraph of section 1: \"tokeniztion\" instead of \"tokenization\"\n-\tIt would be interesting to see the study generalized to other scales of the input documents (sub-words, pairs of characters, pairs of phrases etc.)\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiments not Comprehensive",
            "review": "This paper proposed to use both fine and coarse grained tokenizations of text to train large language models like BERT. The method is relatively straightforward. The input is tokenized at different two granularities (words and phrases for English; characters and words for Chinese). Each type of tokenized text is passed through BERT layers with shared parameters to generate contextual representations. At the finetuning stage, both fine grained and coarse grained representations of the CLS token are jointly used to predict the target.\n\nI was not sure about what exactly was Our Bert (word). Is it not using the BERT WordPiece tokenizer ? If not, which tokenizer is it using ? Also, if it is not using BERT tokenizer, I am very surprised to see it perform better than Google BERT in all cases. Since Google BERT uses a more fine grained tokenization, I would expect it to perform equally well or better. Please clarify this part.\n\nThere are two baselines which I think are important for evaluation. First is a BPE tokenizer trained on the data you are using. This might resolve the problems with single granularity for both language considered here. Second is the \"whole-word masking\" approach of BERT. You can use fine grained tokenization but mask out whole words / phrases as the case may be, which might give you the best of both worlds. Without these baselines, I am not convinced that we should prefer Ambert to other approaches.     ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Initial Review",
            "review": "[General Review]  \nIn this paper, the authors propose a new pre-trained language model called AMBERT, which focuses on both fine-grained and coarse-grained tokenizations. \nThe idea of the model is straightforward. \nThe proposed AMBERT adopts two encoders: \n1) fine-grained: for encoding char-level (Chinese) or sub-word level (English) information.\n2) coarse-grained: for encoding n-gram information.\nThe final hidden representations of two encoders are concatenated and perform MLM+NSP pre-training tasks.\nThe authors also propose two alternatives AMBERT-Combo (without parameter sharing between two encoders) and AMBERT-Hybrid (using one encoder for both granularities).\n\nThey carried out experiments on both Chinese (CLUE) and English (GLUE, SQuAD 1.1, SQuAD 2.0, RACE) benchmarks. \nThe experimental results show the proposed AMBERT could achieve significant improvements over various baseline systems. \n\nOverall, the design of the model is straightforward, and the paper is easy to read. However, I also have several concerns about this paper.\n1) Using multi-granularities in a pre-trained language model is not novel, considering the SpanBERT (Joshi et al., 2020) and ZEN (Diao et al., 2019) has already been existing for some time. \n2) The source of the improvements is not clear. As the proposed model uses about 2x parameters of BERT-base, it is not clear whether the improvements are benefited from these additional parameters or the design of the multi-grained encoders. Note that they only perform the experiments on base-level models but not on large-level models, which indicates that the pre-trained models could still benefit a lot from increasing the parameter sizes.\n\nBased on these observations, I am leaning towards a weak rejection of the paper (but I'm open to discussing). \n\n\n[Strengths]\n1. Demonstrate that the combination of word-level and phrase-level information is helpful in pre-trained language models.\n2. Good experimental results on the existing Chinese and English benchmarks.\n\n[Weaknesses]\n1. The technical novelty is limited. There is nothing much exciting to know that the model could achieve better scores, considering its parameter size (about 2x of BERT-base) and the use of variants of whole word masking or n-gram masking.\n2. Some of the implementation details are missing, and the replication of the results remains uncertain (no supplementary codes or pseudo implementation provided).\n\n\n[Questions for Authors]\n1. I've checked Appendix C.1, but did not find if your model is pre-trained from scratch or starting from BERT checkpoint by Devlin et al. (2018)? I think the fine-grained encoder is the same with original Chinese BERT.\n2. For the Chinese word segmentation, the authors mention that 'a word segmentation tool based on an n-gram model'. I am not sure if the CWS tool is in-house implemented? If so, how was the word vocabulary (72,635) obtained? Was it a truncated list by frequency order?\n3. The proposed AMBERT on Chinese MRC got worse results on CMRC and C^3 on CLUE (Table 3). The authors indicate that this may be caused by the inaccurate CWS for these tasks. However, when it comes to English counterparts, AMBERT shows similar or better performance than state-of-the-art counterparts. I am quite confused about these results. Could you explain more about this? (maybe it is also related to Question 2)\n\n\n[Minor Comments]\n1. page 2, section 1: as extension -> as an extension\n2. page 4, section 3.2: (Next sentence prediction -> Next sentence prediction\n3. page 5, section 4.3.1: reading-comprehension -> reading comprehension\n4. It is better to mark as '[CLS]' instead of 'CLS' in Figure 1. Also for '[SEP]'.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}