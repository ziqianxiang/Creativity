{
    "Decision": "",
    "Reviews": [
        {
            "title": "Simple and effective; however, large improvement needs",
            "review": "This paper focuses on improving novel class generalization of few-shot learning. This paper follows the meta-baseline proposed by Chen et al. (2020), which firstly learns features extractor with based classes then adopts the classifiers with the support set of novel categories. While the meta-baseline fixed the pre-trained features, the proposed Adaptive Feature Distribution (AFD) learns the element-wise scale of pre-trained features using the support sets of novel classes. Because only a single scale vector is fine-tuned, the number of additional parameters are low while obtaining high generalization ability of few-shot learning. The proposed method is effective. However, the paper has a large room for improvement. \n\nPros\n- The proposed method is simple; it only adds a scale vector for improving the feature vector. \n\n- The model selection with DB-index improves performances largely. \n\n- The performance improvements by AFD on the CUB dataset are relatively high. \n\nCons\n- The proposed method is not compared with the case when whole features are fine-tuned. The update of only features scale would be justified when it outperforms this case.\n\n- How to apply DB-index measure for model clustering is not clear. Clustering is not conducted in the proposed method.\n \n- If DB-index measure selects the best epoch is unknown. The graph of accuracies in different epochs with the selected epoch with DB-index would help to understand. \n\n- Figure2 shows the distribution of the feature of novel classes is not separated. How AFD improves this distribution also should be visualized. \n\n- The performance improvements by AFD on min-ImageNet and tiered-ImageNet are low. The improvements in 1-shot are only -0.03\\% and 0.04\\%, respectively, for these datasets. \n\n- It is not clear what the dot-product means in Table3. AFD uses this component, but it seems not explained before. \n\n- The effect of the normalization layer in Eq.(2) could be conducted in an ablation study. \n\nMinor problem\n- P1. Abstract “reducing number of parameters”; however, the proposed method adds parameters compared with fixed pre-trained features.  \n- P3. “subtract” features seems to be “extract” features\n- $\\mu$ and $\\sigma$ seem to be vectors. They should be bold. \n- P.3 “feature are regularized by following the normal distribution” . The normalization of features distribution to be zero-mean and the unit standard deviation is “standardization,” not “regularization”. \n-\tP5: Eqs.(11),(13).  exp should be exp(  ).  What is $S$ in Eq.(11) ? \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Improve Novel Class Generalization By Adaptive Feature Distribution for Few-Shot Learning\"",
            "review": "### Summary\n\nThis paper propose a simple method for fine-tuning few-shot learning classification. The proposed method, AFD, is built on top of metric-based methods and consists of modulating the (normalized) features at test time using the support set. The authors report results on what they call \"in-domain\" (miniImageNet and tieredImageNet) and \"cross-domain dataset\" (evaluating on CUB dataset).\n\n### Strengths\nThe idea of adapting features on new categories on the fly is interesting and promising for few-shot learning (as well as standard supervised training). Instead of adapting all layers, the proposed approach only learn a scale vector (with, obviously, much less parameters) that modulates feature distribution of each novel class.\n\n### Weakness/ Comments\n\n- This paper is not very well written and difficult to follow. It contains more typos/grammatical errors one can state here. This make the paper difficult to understand. I would highly recommend the authors to fully rewrite the paper.\n\n- Moreover, many current methods (that outperform the proposed approach) are missing on the experimental section. For example, Dhillon el al. (ICLR20) propose a somehow similar idea and achieve better performances. Other methods like Hou et al. (NeurIPS19) and Rodriguez et al. (ECCV20) also outperform the proposed approach but are not mentioned on this manuscript.\n\n- I don't really understand why the authors consider experiments in CUB as cross-domain datasets. Is it because the model was trained on ImageNet and evaluated on CUB? still all the new images do come from the same domain. A much more interesting dataset to try the model would be Meta-Dataset (Triantafillou et al.. ICLR20). \n\n- Better ablation to understand why and when the proposed method work would be more helpful.\n\n### Rating\nBased on the above, I rate this paper as a Strong Reject.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Adaptive Feature Distribution for Few-Shot Learning\"",
            "review": "Summary\n-------------\nThe authors propose a method (AFD) to alleviate the generalization gap between base and novel classes in few-shot classification. The method consists in adapting the query embeddings to the novel classes by multiplying them element-wise by a vector (named s). This vector s is found episode-wise by fine-tuning on the support set. The intuition (as shown in Figure 2), is that this scalar will adapt the feature distribution of the novel classes to better align with the distribution learned by the feature extractor.  As an additional contribution, the authors propose to use a cluster quality metric (DB-Index) instead of validation loss for model selection. The authors report better results than previous works they compare with on mini-imagenet, tiered-imagenet, and cross-domain evaluation on CUB. \n\nOverall Review\n--------------------\nThe main strength of this work is that the proposed approach is simple yet improves the baseline performance of the models. However, the problem of feature adaptation for few-shot learning has just been addressed by multiple works [1,2,3,4,5], obtaining better performances than this submission, and more complete experimental set-ups (meta-dataset, transfer from multiple domains, etc.). Moreover, these works have not been referenced nor compared with AFD. This fact, and the other issues exposed in \"Weaknesses\" below, clearly incline the balance towards rejection. \n\n[1] Hou, Ruibing, et al. \"Cross attention network for few-shot classification.\" NeurIPS. 2019.\n\n[2] Ye, Han-Jia, et al. \"Few-shot learning via embedding adaptation with set-to-set functions.\" CVPR. 2020.\n\n[3] Tseng, Hung-Yu, et al. \"Cross-domain few-shot classification via learned feature-wise transformation.\" ICLR. 2020.\n\n[4] Liu, Jinlu, Liang Song, and Yongqiang Qin. \"Prototype Rectification for Few-Shot Learning.\" ECCV. 2020.\n\n[5] Dvornik, Nikita, Cordelia Schmid, and Julien Mairal. \"Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification.\" ECCV. 2020.\n\nStrengths\n-------------\n1. The proposed method is simple yet improves the baseline performance.\n\nWeaknesses\n-----------------\n1. The authors have missed comparing with highly relevant works in the literature [1,2,3,4,5].\n\n2. The clarity could be improved:\n\n    2.1. (Abstract and Section 1, last paragraph.): \"we provide a solution of reducing number of parameters\". What parameters are being reduced? You mean that since s is smaller than the network, the amount of parameters to fine-tune is smaller? In that case, methods that do not fine-tune on the novel classes use even less?\n\n    2.2. The message of \"finetune one scale vector\" is repeated three times in the abstract, becoming a bit redundant.\n\n    2.3. $l$ is not defined in the second paragraph of the introduction.\n\n    2.4. (third paragraph, Intro.) \"estimation of the distribution is hard to achieve\". Could the authors clarify what distribution they refer to?\n\n    2.5. (Last paragraph of the Introduction) \"all three evaluation datasets\". The datasets have not been introduced yet except in the abstract.\n\n    2.6. $x$ does not appear in equation (5) but it is referenced just above.\n\n    2.7. The main contribution is the element-wise multiplication by $s$. However, the submission includes 13 equations, which could mislead the reader into thinking that the method is overly complex. I would suggest the authors to simplify the text, reducing the number of equations, and using that space to either provide more insight on why their method works well, or to extend the comparison with previous state of the art.\n\n     2.8. Section 4.3. \"features as input to the evaluation metrics\". Do you refer to query features?\n\n     2.9. Introduction. \"h is expected to be highly invariant and this property empowers .... lights and etc.\". I understand you mean that invariance to noise is good, but I am not sure why you say it in this context. Could you please rephrase and possibly split in two sentences? \n\n3. Section 5.1. \"Baseline refers to the pre-trained feature extractor of the last epoch for training\". You could select the feature extractor from the validation loss instead of the last epoch. Why do you compare last epoch vs cluster index? \n\n4. Table 3. What is the performance of AFD with cosine? what is the performance of AFD without data-aug? There are two 50.99, is it right?\n\n5. There are many typos that hamper readability. For instance:\n\n  5.1. Introduction:\n\n    5.1.1. \"label generation set limits\" -> remove set\n\n    5.1.2. Citations in general. When the authors of the referenced paper are not part of the sentence, you should use \\citep{} which puts them between parenthesis. For instance: 'Moreover, Chen et al. proposed blabla'. (\\citet). 'Moreover, metric-learning approaches (Chen et al.) propose...' (\\citep).\n\n     5.1.3. \"to conducts\" -> \"to conduct\"\n\n     5.1.4. \"the adaptive feature distribution\" -> \"to adapt a feature ...\"\n\n  5.2. Section 3:\n\n    5.2.1. \"which correspondingly used\" -> \"which are....\"\n\n    5.2.2. \"to subtract features\" -> \"to extract features\"\n\n    5.2.3. \"by using this non-parametric metrics\" -> \"... metric\"\n\n    5.2.4. \"when we construct our the non-parametric\" -> \"...our non-parametric\"\n\n  5.3. Section 5.3:\n\n    5.3.1. \"which is serve\" -> ??\n \n\n ",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Proposes an interesting approach but suffers from clarity issues",
            "review": "Summary\n========\nThis paper proposes a simple approach to few-shot classification which first pre-trains a feature extractor via the classification loss on all base classes, as is usually done. Then, to solve each  test task, they propose to learn a task-specific scaling co-efficient for each (normalized) feature of the pre-trained embedding function. This learning is done via a few steps of gradient descent on the support set of the given test task, for the objective of correctly classifying the support examples using a simple readout layer that is set to the prototypes of the different classes. This approach is parameter efficient, as it only requires optimizing the scaling co-efficients in the inner loop.\n\nPros\n====\n[+] I really like the idea of isolating a small set of parameters to tune for the purpose of achieving task adaptation. This allows avoiding the expensive fine-tuning of the entire feature extractor, and may be more expressive than simply learning a per-task readout layer.\n[+] I also liked the intuitive explanation of the role of the scaling factor by inspecting the gradients of the task-specific loss with respect to the scaling parameters. It’s indeed interesting (though not unexpected) to see that this mechanism can amplify dimensions of the embedding space that are most discriminative for each task at hand.\n[+] I really like the proposed approach for model-selecting for the pre-trained model. I think this is a valuable contribution and the strong performance of this component alone is very interesting.\n\nCons\n====\n[-] A major weakness of this paper in my opinion is the lack of clarity. I found parts of the paper hard to follow, both relating to the proposed framework as well as the experimental setup. It would additionally be useful to proof-read the paper and correct typos, grammar errors and incorrect usage of words throughout. Specific clarity issues are outlined below:\n\nA. Clarity issues relating to the proposed framework.\na) The loss in Equation 2 is only summing over N points, where N is the number of examples per class. I was expecting it to sum over the entire support set (total of K*N examples). Is this a typo or is this meant to represent the loss of the examples of a given class only? If so, that should be indicated.\nb) In Equations 2 and 3, it would be clearer to rename z_j to z_{ij}, since it is the logit of example i belonging to class j. Similar minor notation issues apply in all following equations.\nc) In Equations 9 and 10, instead of \\nabla s, it would be clearer to write this as the partial derivative of the loss of the particular example i w.r.t s.\nd) I agree that for 1-shot P_{y_i} == 1 (because the single example is exactly the same as the prototype in that case) but I’m not sure why that quantity is approximately 1 more generally.\ne) In Equation 11, what is the symbol S between the \\exp and \\cos?\nf) In Equations 12 and 13, shouldn’t the normalized features and scale be used instead of the original features F?\n\nB. Clarity issues relating to the experiments.\na) What exactly are the models referred to as “Baseline” and “Baseline*” in the tables? I understand the difference between them is the model selection method from the pre-training round. But it’s not clear what is the algorithm they use to solve the downstream few-shot tasks. Are they simply learning a readout layer? If so, is that layer initialized from the prototypes as is done for AFD?\nb) I found the ablation section very hard to follow. First of all, the caption of Table 3 is “Ablation Studies on CUB”, but the results in that Table are on mini-ImageNet and tiered-ImageNet. Is this then a case of cross-domain study where the models were trained on CUB and evaluated on those two datasets? The description of the results of this table (e.g. in the paragraph “The Importance of Fine-tuning Features”) sometimes refers to a “mini-ImageNet trained feature extractor” (or analogously for tiered-ImageNet) and sometimes refers to “cross-domain cases” which makes it very hard to understand how the results in Table 3 are actually obtained.\nc) In Table 3, rows 4 and 5 don’t have the “Models” entry filled in. Are these additional AFD variants or “finetune-weight” variants? \nd) There is a paragraph titled “Effects of Different Feature Extractor” that refers to studying the effect of a feature extractor trained on a larger dataset. I couldn’t figure out which results this paragraph is referring to from the table. It would be useful to explain which entries of Table 3 these observations are describing.\ne) In the paragraph titled “The Importance of Fine-tuning Features”, again I was unsure which results some of the descriptions are referring to as the reported percentages in that paragraph don’t all correspond to numbers in Table 3. It would be useful to be clear about which entries in the table each of the findings refers to.\n\n[-] Another weakness of the paper is the limited discussion of related work. There have been various attempts at approaches that fine-tune only a cleverly-selected subset of the feature extractor parameters in each test task. Two examples that come to mind are: [1] and [2]. Additionally, this approach is related to FiLM-based models [3, 4] that learn a scaling and shifting transformation for each task (based on its support set). These are all different from the proposed approach in that they are meta-learning models (as opposed to algorithms that are applied at test-time only on top of pre-trained features), but they are otherwise similar in spirit, so they should be discussed and compared against. Further, [6] should also be cited alongside [7] in the context of conducting meta-training with a pre-trained feature extractor.\n\n[-] The proposed approach does not actually seem to outperform the baseline on the “in-domain” cases (Table 1) as the increase in performance over the baseline is very small and the confidence intervals overlap. It seems that the cross-domain experiments better showcase the ability of this method (since in that scenario adapting features is more important), so perhaps future experiments should further explore that scenario.\n\n[-] Finally, I felt that important baselines were missing, that would reflect alternative ways of fine-tuning subsets of the feature extractor less selectively than the proposed approach (see the section below for some suggestions).\n\nOverall\n======\nOverall, I vote for rejecting this paper due to the weaknesses discussed above. To reiterate, a major drawback in my opinion relates to the clarity of the presentation of this work. Other weak points include the lack of discussion and comparison to relevant literature and baselines and the weak performance of the proposed approach over the reported baseline.\n\nSuggestion for additional experiments\n=================================\nThe authors show experimentally that learning only a per-task readout layer is not expressive enough compared to their method that modifies the features as well via their proposed scaling mechanism. There are, however, other “baselines” that should be compared against that also modify the features in different ways. Specifically: while learning the readout layer (possibly initialized from the prototypes as is done in the proposed approach), we could also fine-tune the feature extractor. This baseline is used in the literature in some older work, e.g. the “Baseline-finetune” model in [5], as well as in more recent works in more challenging settings [6]. The drawback associated with fine-tuning the entire backbone is the potential overfitting to the support set for low shots, so it would also be useful to try variants that fine-tune the top X layers of the network. These explorations should happen on the features obtained by Baseline* for an apples-to-apples comparison with AFD.\n\nFurther, perhaps the performance of AFD can be improved by fine-tuning the output layer too (which is initialized from the prototypes) during the inner loop that tunes the scale parameters? My understanding is that the output layer is currently fixed to the prototypes and isn’t optimized at all which might be restrictive.\n\nFinally, given that the proposed model only appears to significantly outperform the baseline in the cross-domain evaluation scenario (Table 2), a good test-bed for additional experiments with this method might be the Meta-Dataset benchmark [6] that is comprised of diverse datasets and therefore really necessitates a flexible model that can adapt to different distributions.\n\nReferences\n=========\n[1] Fast Context Adaptation via Meta-Learning. Zintgraf et al. ICML 2019.\n[2] Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace. Lee et al. ICML 2018.\n[3] Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes. Requeima et al. NeurIPS 2019.\n[4] Improved Few-shot Visual Classification. Bateni et al. CVPR 2020.\n[5] Optimization as a Model for Few-shot Learning. Ravi and Larochelle. ICLR 2017.\n[6] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. Triantafillou et al. ICLR 2020.\n[7] A New Meta-Baseline for Few-shot Learning. Chen et al. 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}