{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper considers the question of identifying whether a model is bad from an OOD perspective or certifying that it is good. The reviews agree that there are interesting ideas in the paper, however, lack of sufficient experiments and presentation issues were pointed out which make the paper not ready for acceptance at this stage."
    },
    "Reviews": [
        {
            "title": "This work proposed a useful metric to decide when to apply IRM/tune IRM models - Why cant conditional mutual information based metrics be used ?",
            "review": "Summary:\n     This paper tries to understand the following question - a) Given a set of environments - when would we expect an ERM solution to be completely bad in the out of distribution (OOD) generalization sense. b) Given a trained model, how can we certify that the model is good in an OOD sense when we dont have access to a new environment. \n\nThis metric might be useful to tune IRM procedures which has been identified as a real issue in some recent work.\n\nIRM optimizes over representation $\\phi$ and a classifier on top of it $\\gamma \\approx E[Y| \\phi(X)]$ is remains the same across all environments. \n\n The metric proposed: Influence function which measures the change in $\\gamma, \\Delta \\gamma $ caused by upweighting an environment $e$'s loss by $\\delta$. The metric assumes that the IRM optimization is done using $\\sum_e L(\\gamma,\\phi, S^{e}) + \\lambda R (f, S^{e})$ where $S^{e}$ represents the dataset for environment $e$.\n\nThe limiting influence function when the weighing factor $\\delta$ tends to $0$ is given by a inverse of a Hessian matrix of the regularized loss over all data  (whose changes are measured with respect to the classifier $\\gamma$) and a gradient vector of the loss (without the regularizer) with respect to the $\\gamma$ on the data of the environment which is up weighted.\n\nNow, log of the spectral norm of the cross covariance matrix when changes are made to the different environments involving these influence measures is used as a metric.\n\nTo test a model: One has to check if this derived measure is low and test accuracy is high (test set made from training envrionments). If so then, that model can be expected to have a high OOD generalization to unseen environments.\n\nTo test environments if they need special OOD optimization frameworks: The authors propose to a) randomly create m environments, and b) use the pooled data and train an ERM model for both. If the metric proposed in b) is larger than the metric in a) across computed using the actual environments, the authors propose that one should check for an algorithm (like IRM) that is specifically tailored towards OOD solutions. \n\nFour experimental results :\n\na) For a simple Bayesian network X_1-> Y->X_2  with linear models and different environments change the Y|X_2 part but not the Y|X_1 part the authors compare REx, IRM and ERM and show that their metric predicts better performance with respect to the causal error and non causal error (measured by presence of just any coefficient on X_2).\n\nb) For Colored MNIST the authors show that when the environments have a varying color-label association , then their metric to test environments is highly correlated with the diversity parameter.\n\nc) Similarly, they compare IRM and REx with different lambda parameters to show that the models with the smallest metric does better in terms of OOD accuracy.\n\nd) For the simpler Linear bayesian network case, they show that for the optimal ERM classifier, only when the environments become identical their metric becomes small. While for IRM, when lambda (regularizer imposing invariance) is large their metric goes to - infty.\n\n\nStrength:\n\nI feel the metric proposed is novel for an important problem of determining the need for IRM and/or tuning IRM based frameworks where new envrionments have not been seen. \n\nThis has been very nicely demonstrated for both simple linear Bayesian networks (also proven for them) and Colored MNIST.\n\nWeaknesses:\n\n1) When authors says we test if the environments are OOD - they dont precisely define it. Only in the colored MNIST experiments this becomes clear. But one has to be very precise about what the authors mean by \"testing if environments are OOD\" - Does it simply mean one is checking if anything else would do better than ERM in terms of OOD accuracy? \n\nI feel this claim is a bit imprecise. However, due to the colored MNIST demonstration of variation spurious factors being correlated with their metric - I am not very inclined to penalize them in the scores.\n\n\n\n2) [My major concern] Another metric to test if a model captures invariance is simply to compute conditional mutual information (CMI) between $e$ ( the environment label) and $Y$ given $\\phi(X)$ (the learnt representation). A good place to get a code that computes a good lower bound to this is https://arxiv.org/abs/1906.01824. Their code is public. \n\nCan the authors plot their metric versus this for example for Figure 3.  Because I would like to understand why just directly computing conditional mutual information would be different from the proposed metric. \n\n\n3) Lots of typos \"..when inversing Hessian..\", \"...unstable inversing...\" in Page 8. \n\nI have conditionally given 7 - However some comments, comparison to conditional mutual information would be more useful (Refer to my point 2 in the weakness)\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Using influence functions to understand out-of-distribution generalization",
            "review": "\n\n=================================\n\n**Update after discussion period**\n\nMy feeling is still that the proposed method has a lot of interesting potential, but that the paper still needs some improvement. I've bulleted my main remaining concerns below:\n\n1. The clarity of the writing still needs to be improved in many places. Notably, the authors' discussion of their metric is mostly discussion-based (rather than providing concrete theoretical claims about their metric and its strengths). While this isn't necessarily a problem, such a presentation really needs precise language and phrasing so that the details of the claims and their supporting arguments can be completely understood. From what I can follow, I think many of the authors' arguments are going in good directions, but the writing should be improved to be sure.\n2. I still don't completely understand why $\\beta$ is being fixed here. The argument in Appendix A.3 seems to say that if $\\hat\\beta$ varies, then the meaning of $\\hat\\gamma$ changes, and so looking at the change in $\\gamma$ would not be meaningful. I agree, but that does not mean we cannot look at the combined change of $\\beta$ and $\\gamma$. It seems like this would be straightforward to experiment with on some smaller problems that only have parameters in the tens of thousands; I think it would be better to include such experiments rather than trying to argue verbally that such an approach will not work well.\n3. I am also not sure I see why we should not be interested in leave-one-domain-out CV. I appreciate the clarification that Figure 1 is actually showing CV's estimate of error, and definitely find this to be a compelling experiment. It is pretty surprising to me that the authors' metric could succeed in this case, given that the influence functions are an approximation to the parameter change as each domain is left out (this is true, whether or not this is the intent of the metric). I think further investigation of this point is needed. Would computing the proposed metric with the parameter changes under leave-one-domain-out CV not detect the OOD issues in Figure 1? Or is it just that feeding these parameter changes into the function measuring test error that is providing a poor assessment of OOD generalization in Figure 1?\n\nOn a different note, I appreciated the increased discussion of \"the shuffle;\" I think this is an important part of the paper that didn't come up much during the discussion period. As a side note, I think it is more common to just call this sort of thing a permutation test. And, along those lines, it would be good to actually perform a permutation test (i.e. run over many shuffles and examine the distribution) so that we know for sure the reported shuffles aren't just unluckily high/low.\n\n===================================\n\n**Original Review**\n\nThis paper is focused on understanding whether a model fitted model is going to have out-of-distribution issues. In particular, the authors consider models trained on multiple \"environments,\" which may fail to generalize to future environments. To detect whether a model will fail in this way, the authors propose to look at the change in model parameters as each environment in the training set is infinitesimally upweighted (i.e., they compute influence functions). If upweighting each environment the same amount changes the model by a similar amount, the authors reason that the model must not be overly fit to any one environment, and so will generalize to future environments well.\n\n\nI think a major issue is the derivation of Eq. (4), which claims to be the \"change of top model g caused by upweighting the environment.\" I'm interpreting this as the change in the parameters $\\gamma$, which are part of the overall parameter vector, $\\theta = (\\gamma, \\beta)$. My understanding of how to calculate influence functions is that the influence function for the overall parameter $\\theta$ would be:\n$$\n    -\n   \\begin{pmatrix}\n            H_{\\gamma\\gamma} & H_{\\beta\\gamma} \\\\\\\\\n            h_{\\gamma\\beta} & H_{\\beta\\beta}\n    \\end{pmatrix}^{-1}\n     \\begin{pmatrix}\n          \\nabla_\\gamma f \\\\\\\\\n          \\nabla_\\beta f\n     \\end{pmatrix}.\n$$\nIf $\\gamma \\in \\mathbb{R}^D$, I would say that the (infinitesimal) change in $\\gamma$ is really the first $D$ entries of this vector. But this is not what is written in Eq. (4), and the derivation of Eq. (4) in Appendix A.2 does not actually derive Eq. (4) (the derivation in the Appendix derives the equation written above and does not refer to $\\gamma$). I think this is especially important, as the reason that Eq. (4) is computable is that the matrix to be inverted is $D \\times D$, rather than the much larger $dim(\\theta) \\times dim(\\theta)$ in the equation I've written above. Can the authors clarify what's going on here?\n\nThe clarity of the writing is also an issue. Many sentences have grammatical issues (e.g. \"...according to detailed derivation in Appendix A.2, the change of top model g...\" is missing the word \"the\" twice). Most of these don't make the paper any harder to understand, but should still be fixed. I did have trouble following some of the more expositional parts of the paper. For example, the following sentences weren't clear to me:\n\n>First, not all OOD problems demand models to learn invariant\nfeatures, e.g. when varying features are always more strongly correlated to the prediction. But to\nour concern, we regard the OOD problem as a bridge to a truly causal model, so these minor ”OOD”\nproblems are out of our consideration. To a large extent, invariant features are still the major target\nand our proposal is still a good criterion to model’s OOD property.\n\nCould the authors provide more details about what their example in the first sentence refers to (is it a counterexample to a specific definition of an \"OOD problem\")? What is a \"truly causal model\" (one that only uses features that have a causal effect on the outcome)? Is this a different goal than building a model that uses the \"invariant features\" discussed in the third sentence?\n\nA final issue is that the authors' method seems close to cross-validation (CV), where each fold of CV leaves out an environment, but this connection isn't discussed. On the surface, it seems like an even better way to understand how much a model is depending on an environment is to actually remove the environment from the training data, rather than just changing the weight of the environment by an infinitesimal amount. Could the authors describe why their method is more desirable than CV in this context? If the issue is computation time, note that influence functions have been used to approximate CV [Giordano et. al 2019].\n\n----------------References---------------------\nR. Giordano, W. T. Stephenson, R. Liu, M. I. Jordan and T. Broderick. A Swiss army infinitesimal jackknife. AISTATS. 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper proposes to use influence functions to build a new metric to understand OoD performance from training data since accuracy is not a good metric. This issue was already resolved in the existing works. Many things incorrect with the paper including a major mistake in using the regularizer from the work of Arjovsky et al.",
            "review": "Summary\n\nThe authors study the problem of out-of-distribution (OoD) generalization. The key question authors seek to answer is when given access to data from multiple training environments, can one only rely on test accuracy? or does one have to rely on some new measures to estimate the out-of-distribution performance of the model. The authors develop a metric based on influence functions, which authors claim is a better reflection of OoD accuracy than test accuracy. The metric proposed by the authors measures the variance in the model when the data from each environment is upweighted. The authors show that the proposed metric empirically correlates to the OoD performance of the models.\n\nPros:\nI appreciate the problem that the authors consider. It is important to develop metrics that are good indicators of OoD performance since accuracy on train environments is not a good indicator. \n\nCons: \nI divide my concerns into different subsections below:\n\na)  IRM Arjovsky et al. (2019) already solve the problem that authors want to solve in this work: \nThe authors claim that they show that how test accuracy is not a good metric and highlight the need for a new metric to measure OoD performance. IRM (Arjovsky et al.) have already established this point. Recall the colored MNIST experiment in Arjovsky et al., which has precisely already shown accuracy over any training environment does not suffice in selecting a predictor with good OoD behavior.\n \nIf not accuracy, then what is the right metric? The authors in the current paper propose an influence function-based metric but completely ignore the metric that is already used in IRM to inform the learning of the OoD model. After all IRM is able to learn a model based on training environments itself, which means the IRM loss itself can be thought of as a proxy for the OoD performance. Recall that the loss used in IRMv1 in Arjovsky et al. balances the accuracy over the training environments and invariance achieved across all of them. \n\nLet us now precisely state a metric based on the work of Arjovsky et al.  We perfom the following steps:\n\ni)      Split the data in each environment into a train split and a validation split. Learn the model using the train split from each \nenvironment.\n\nii)     Use the trained model and compute the IRMv1’s penalty value on the validation split for each environment \n\niii)\tCompute the mean and the variance of the penalty value across environments. \n\nWe now discuss why the above metric can capture the OoD performance better than just training accuracy for instance.\nThe penalty in IRMv1 checks if the predictor learned satisfies invariance conditions across all the environments. The penalty by definition is greater than zero. If the penalty is close to zero for all the environments, then the model satisfies invariance across environments. \n\nConsider two models: one trained from ERM and other trained from IRM. Assume that they have a small average validation error across all the environments. Let us analyze the following scenarios:\n\ni)\tSuppose the mean of the penalty value across the environments is close to zero for the IRM model. Suppose the mean of the penalty value across the environments is much larger than zero for the ERM model. In such a case, the IRM model satisfies invariance and will perform better than ERM model on OoD data. \n\nii)\tSuppose the mean of the penalty value across the environments is close to zero for the IRM and the ERM model. In such a case, it is likely that both ERM and IRM models have a good performance on OoD data. \n\niii)\tSuppose the mean of the penalty value across the environments is similar for both the IRM and the ERM model but is greater than zero. In such a case we cannot rely on the mean, we should use the variance in the penalty value across environments. The model with a lower variance is likely to have a better OoD performance. \n\n\nb)\tIncorrect use of IRM loss to compute influence\n\nThe authors define in equation (3) a regularized loss function. Following equation (3) the authors define two types of regularizers from IRM and REx. The regularizer for IRM is incorrectly defined as \\sum_{e}(\\|\\nabla_{w}\\ell(g(w \\Phi), S_e)\\|)^2. The correct regularizer is\n\\sum_{e}(\\|\\nabla_{w}\\ell(w \\Phi, S_e)\\|)^2. There is no function g needed as the scalar w serves the purpose for the classifier that operates on top of the representation. Subsequently everything authors do with computing changes on g is incorrect as g itself is not needed.  \n\n\nc)\tConcern regarding influence computation in REx model\n\nThe authors divide the predictor into a representation and a classifier on top. However, it is not clear why would one do that for REx. REx uses the entire predictor as one. Hence, breaking down the predictor in the context of REx and analyzing the influence on the classifier does not make sense. There needs to be a better justification for doing what authors seem to be doing.  \n\n\n\nd)   Influence metric should be defined properly\n\nThe authors want to analyze influence of all the points in a given environment. The authors suggest upweighting the points but for reasons not very well explained the authors decide against upweighting the points inside the penalty. The whole principle of computing influence of a point is to upweight that  point in the entire loss and then determine how does the trained model change. The current approach decides only to upweight the part concerned with ERM loss and not the regularization. \nThe penalty is a crucial part of IRM and not upweighting it seems to against the spirit of understanding the impact of each environment on the model.\n\ne) What is the correct way of computing influence of a each environment? \n\n\nI would suggest the authors to follow the standard steps used in Koh et al. and upweight the points in an environment in both the ERM part of the loss and the regularizer. Instead of analyzing the impact on some model g, the authors should see the impact on the model phi itself. Analyzing the variance of change in the model phi may be correlated with OoD. However, this remains to be seen.\n\nf) Concern with using influence at all\n\nThe main reason to use influence functions is the computational advantage offered. When we do not want to retrain the model for every point we want to analyze, it makes sense to compute the influence. However, the authors are computing the influence per environment. In general, we do not expect too many environments, thus why not just retrain the model after upweighting the points in an environment. The only argument that can be made in favor of deriving influence in this case is if the authors subsample from the environments and determine the distribution of influence values. However, authors have not discussed any such justifications. \n\nRecall from Koh et al., the analysis of influence requires that the number of points being upweighted be much smaller than the total number of points.  An entire environment can constitute a significant proportion of the total data and changing all the points in an environment can lead to severe miscalculation of influence. The authors should have shown through a plot similar to Figure 2 in Koh et al. that computing influence of an environment vs. actually retraining the model lead to similar values. \n\ng) Concerns regarding experiments\n\nIf the authors believe that influence based metrics do lead to a better estimate of OoD performance, then they should have at least compared with metrics based on IRM penalty that I describe above and also REx penalty. All the IRM based methods rely on some type of signal from the training data to train a good OoD model. \n\nQuality: \nUnfortunately, the paper is of poor quality. The authors have not carefully analyzed how existing works already address this problem. \n\nClarity: \nThe writing of the paper is average. An explanation of how influence is computed for REx vs IRM vs ERM should have been clearly stated and if they used the same technique, then an explanation of why should have been there too. \n\nSignificance:\nThe problem authors are trying, i.e., building metrics that are better indicators of OoD performance  than training accuracy, is very important. However, the approach taken by the authors is incorrect and ignores many important comparisons. \n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting Idea - Experiments and Exposition Need Improvement",
            "review": "**OBJECTIVE**: Develop a method to reliably determine 1) if a model will generalize to out-of-distribution (OOD) samples 2) if it is advantageous to use algorithms that explicitly promote OOD generalization (in place of empirical risk minimization). \n\n**CLAIMS**: \n1)Test error can be a poor indication of OOD generalization. \n2)Model stability across training environments (in the sense of “cross validation” between training environments) is a good indicator of OOD generalization. \n3)The proposed index that uses influence functions to estimate how sensitive the model is to removing training environments does a good job in predicting how robust the model is against OOD samples. \n\n**MAIN CONTRIBUTION**: The authors propose an influence function-based index to determine how well the model will generalize to OOD samples and whether OOD methods are needed in the first place. \n\n(Note: PAIR stands for “please address in rebuttal”)\n\n**STRONG POINTS**:\n**Valuable line of inquiry**: The paper is on a very important topic that will be relevant to many in the ICLR community.\n**Valuable connection**: The idea of utilizing influence functions to analyze OOD properties of machine learning models is, I believe, novel and interesting. \n**More efficient than cross validation across training environments**: Assuming that influence functions are accurate enough to estimate the effect of removing training environments, the proposed method can significantly speed up the process of “running cross validation across different training environments”. Also, running the influence function computation on only the top model (i.e. the classifier) makes sense, and can significantly speed up computation – making the model feasible on even potentially large-scale tasks. \n\n\n**WEAK POINTS**:\n**Experiments**: Given that the main contribution is a heuristic measure of OOD generalization, I believe the experiments section is particularly important to prove that the proposed index is indeed useful for sufficiently complex, realistic problems (if not large scale). I believe the experiments are lacking in several ways: \n1.\t**Lack of different learning objectives**: There are many methods described in the related works section for learning models that generalize to OOD samples. However, only IRM and REX are used in the experiments.   I consider, in particular, the lack of “the robust learning objective” important. This seems particularly important to me, as I expect the proposed index to favor models trained with the robust learning objective that actually might not generalize to OOD samples. (PAIR) \n2.\t **Lack of benchmark OOD generalization diagnostics**: Are there other papers out there that try to quantify how well a model will generalize to OOD samples? How does the model compare against those? For example, the paper “In Search of Lost Domain Generalization” by Gulrajani and Lopez-Paz (2020)[1] (which is not cited), and its related works section, seems quite relevant to this paper and probably should be considered by the authors – both in terms of which benchmark datasets are suitable, and which model selection criteria to use. (PAIR, if you have the time and resources) It would also improve the paper if the authors could try actually running some kind of “cross validation across training environments” (which the proposed approach is, as far as I understand, aiming to more or less capture more efficiently) to see how well it compares with the proposed approach? (PAIR). The authors also note that the proposed approach can be used to see if different training environments are actually different enough to require OOD generalization approaches for training. Couldn’t one simply check this by training a classifier to predict which training environment each example belongs to? If the classifier achieves random accuracy, one probably doesn’t need anything more than ERM. Am I right by saying this? (PAIR)\n3.\t**Lack of more realistic datasets**: All of the experiments in the paper are run on highly synthetic and unrealistic datasets. Although these experiments are still extremely helpful, it would improve the paper drastically if there were experiments run on more realistic datasets. Again, In Search of Lost Domain Generalization” by Gulrajani and Lopez-Paz (2020) seems like a valuable reference.  (PAIR if you have the time and resources)\n\n**Faithfulness of influence functions**: Influence functions, in essence, try to estimate how much the weights would be perturbed if the training set didn’t have particular training examples (or environments in this case). The experiments reported in the paper are small-scale enough that one can likely afford to actually remove particular environments, retrain, then compare the weights with what’s predicted by the influence function. Did the authors run such experiments? (PAIR) I realize that the authors cite Koh (2019) that shows influence functions can still be useful to predict the effect of large number of training examples, but I believe the current setting is different enough that it would be worthwhile to repeat such experiments. \n\n**Lack of justification for the particular form of the index**: It’s not that clear from the paper why the authors picked the particular form of the proposed index. Did the authors consider other forms? For example, I wonder if one could alternatively use the gradient of the excluded training environment with respect to the perturbation (which can be efficiently computed using influence functions) to derive an alternative index to compete with the current one. \n\n**Is test accuracy really that problematic?**: I agree that without plenty of test environments, simply relying on test accuracy is problematic, as the authors point out with a concrete example. However, couldn't a sufficiently rich test set be seen as the gold standard to test OOD generalization, as also expressed in [1]?\n\n**Missing related work**: I believe [1] is important to cite in this paper. \n\n**DECISION**: Weak Reject. \nMain reason: While I find the paper interesting and the line of inquiry promising, I believe it is not yet ready for publication, especially due to the weakness of the experiments section. With a better selection of baselines and benchmarks (both in terms of model selection criteria, more OOD generalization approaches (like robust learning objective) and more justification as to why the current form of the index is selected, the paper, in my opinion, can be ready for publication. \n\n**QUESTIONS TO AUTHORS**: In addition to the sentences I’ve marked with the acronym “PAIR”, I have the additional questions:\n1.\tWhat do you mean by “same model change” in the last paragraph of page 4? Lets say I combine MNIST and CIFAR10 images to create a 20-way classification dataset, and I’m viewing MNIST and CIFAR10 as different training environments. Why should the classification model respond the same way to perturbations made to the different environments in this case?\n2.\tJust to clarify: does “2-norm for matrix” refer to the Frobenius norm, or actually the operator 2-norm?\n\n\n**MINOR POINTS**: (these didn’t play much role in my decision)\nUnpolished writing: I‘d recommend the authors to carefully go through the paper to remove as many grammar errors and typos as possible. That being said, the existence of these errors didn’t make it harder for me to read the paper, hence didn’t contribute to my final decision (at least consciously). \n1. You might also consider restructuring the paper to make the introduction less meaty, and dedicate a section to discuss why test accuracy is limited. \n2. You might want to describe what influence functions are verbally first, before giving a formula for it. \n3. In section 3.2, you use both IF and I to mention influence functions, as far as I can see. \n4. It would be nice if Table 1 refers to Section 5.1. \n\n[1] In Search of Lost Domain Generalization, Ishaan Gulrajani and David Lopez-Paz∗  2020 \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}