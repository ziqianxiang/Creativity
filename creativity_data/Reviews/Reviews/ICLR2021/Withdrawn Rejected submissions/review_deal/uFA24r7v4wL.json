{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper is concerned with improving the scalability of GCNs which is an important problem and relevant to the ICLR community. For this purpose, the authors propose a new distributed training method for GCNs which uses a boundary sampling strategy to reduce the number of boundary nodes. The paper is written well and, overall, good to follow. Reviewers highlight the promising improvements in throughput and memory footprint as well as and the possibility to avoid potential loss of information compared to other methods.\n\nHowever, currently there exist still concerns around the manuscript. Reviewers raised concerns with regard to the novelty of the method (straightforward combination of existing techniques) as well as the experimental evaluation (overhead of METIS, full-batch accuracy, edge boundary vs node boundary, etc.) The revised version addresses some concerns (comparison to DropEdge, p=0, etc.) and clearly improves the paper. However, given the aforementioned issues and the absence of strong support from reviewers, I agree to that the current version would require an additional revision to iron out these points. The presented results are indeed promising and I'd encourage the authors to revise and resubmit their work considering the reviewers' feedback."
    },
    "Reviews": [
        {
            "title": "This paper addresses the efficiency issue raised by GNN training on large graphs. The proposed solution falls in the category of data parallelism, which aims to partition graph into smaller parts while reduce the communication cost caused by extensive number of boundary nodes. The proposed idea is interesting and practical, and the experimental results have demonstrated the superiority of the proposed approach.",
            "review": "This paper addresses the efficiency issue raised by GNN training on large graphs. The proposed solution falls in the category of data parallelism, which aims to partition graph into smaller parts while reduce the communication cost caused by extensive number of boundary nodes. The proposed idea is interesting and practical, and the experimental results have demonstrated the superiority of the proposed approach.\n\nStrengths:\n1.\tThis paper successfully identifies the issues in the data parallelism-based GNN training algorithms â€“ each graph partition share too many nodes with other partitions, called boundary nodes, which significantly affect memory and communication cost.\n2.\tThe proposed solution is simple yet neat, which is to sample the boundary nodes to reduce memory and communication cost.\n3.\tThe experiments are convincing, boosting the throughput (meaning number of epochs per sec) by up-to 500% and reducing the memory usage by up-to 58%, while achieving the same or even better accuracy.\n\nWeaknesses:\n1.\tThe graph partition idea is interesting, but more details on how to apply METIS is expected.\n2.\tThe experiments can be further enriched.  \n\n\nDetailed comments:\n1.\tIn this paper, it has mentioned many related studies. But in the experiment section, only a small subset of the mentioned methods are treated as baselines. Also, for the data partition-based baselines, only throughput is compared. How about the accuracy comparison? Which method needs more epochs to converge, which will affect the run-time in addition to the throughput? For the sampling-based methods, more methods are expected to be compared to the proposed approach. In addition to accuracy, run-time comparison is also expected.\n\n2.\tFor the sampling-based approach, one more baseline is expected: \n\nZou et al., Layer-Dependent Importance Sampling for TrainingDeep and Large Graph Convolutional Networks, NeurIPS 2019.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some baselines are missing.",
            "review": "How to perform GCN on large-scale graphs efficiently is crucial in graph learning. Towards this goal, this paper proposes distributed GCN training, BDS-GCN. In particular, it first conducts graph partition and then allocate different partitions into different GPUs.  For each partition,  BDS-GCN samples a portion of boundary nodes (that are shared across partitions) and broacasts the features of the sampled boundary nodes over all partitions. After the message exchanging, BDS-GCN computes one-layer forward pass for each partition  in parallel. The backward pass is done similarly.  \n\nIn contrast to previous sampling-based methods, this paper can potentially avoid information loss by only sampling the boudery nodes while awalys keeping the inner nodes and their connections. \n\nIn contrast to typical distributed graph frameworks, this paper stores each partition locally in each gpu, and the communitation time can be reduced by boundery sampling.\n\nOverall, this paper is good written, the idea is valuable, and the exerimental results generally support the claims the authors have proposed.  Below are some concerns for the current version:\n\n1. It seems in Table 2 that BS-GCN can still obtain promising results when p is small. The reviewer wonders what will happen if p=0, when no boundery node is sampled. This is a crucial baseline that should be compared, as it can tell how important the message exchanging between partitions is, and thus what the boundery sampling stands for. \n\n2. The authors claim that DropEdge is not practical in distributed graph learning, which seems problematic. The communication cost indeed depends on the number of edges between two different partitions. In this sense, it will be interesting if we conduct boundery edge sampling like DropEdge to save communication time. Different from boundery node sampling, boundery edge sampling will keep all nodes (including the boundery nodes) and all connections (including the ones between inner nodes and boundery nodes) within each partition to preserve the information as much as possible, and only those edges connecting different partitions are randomly dropped. It is expected to explore this kind of baseline, and compare it with boundery node samling method. \n\nOther minor issues:\n\n3.   Are the weights stored globally? In a parameter server?\n\n4. What do U1, Um mean in Line 5 in Algorithm 1? Algorithm 1 is only under the per-partition view, and including a global view will better help readers to understand the whole framework. \n\n5.  Some previous results on Reddit by FastGCN, AS-GCN, etc are missing in Table 2.\n\n6. In figure 3, it seems increasing the number of partitions does not necessarily reduce the time cost, why? And how does the accuracy change if we use different number of partitions?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a simple idea for Full-Graph Training of Graph Convolutional Nets",
            "review": "This paper developed a simple method for the distributed training of GCNs. In particular,  the excessive amount of boundary nodes in each partitioned subgraph, which can easily explode the required memory and communications for distributed training of GCNs. To address this problem, this paper proposed BDS-GCN, a method that adopts unbiased boundary sampling strategy to enable efficient and scalable distributed GCN training while maintaining the full-graph accuracy. The experimental results show good performance on large graphs. \n\n1. The writing is good and the idea is clearly presented.\n\n2. The idea is straightforward. It just communicates a subset of neighboring nodes to reduce the communication cost. However, this simple sampling method may incur large variance, slowing down the convergence speed and degenerate the prediction performance. \n\n3. In table 1, why is the number of boundary nodes larger than that of inner nodes?\n\n4. What is the difference between this method and DropEdge (https://arxiv.org/abs/1907.10903)? It seems that this method is identical to DropEdge. In particular, this method drops the edges across different machines. Thus, it is critical to discuss the difference between these two methods, and conduct experiments to compare these two methods.  Otherwise, it is diffcult to see the novelty of this method. \n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simple and effective idea, lacks novelty",
            "review": "-------------\nSummary\n-------------\nThis paper presents a system design method to train GNNs on large graphs in a distributed manner. Due to the irregular edge connectivity and the limited GPU memory, intelligent graph partitioning is necessary to support full-batch computation. The authors identify the challenges in computation and communication due to inter-partition connections. Then with the objective of minimizing the edge-cut and improving load balance, the authors use the existing METIS partitioning algorithm to obtain node assignments. To further reduce the amount of communication across GPUs, the author further apply random sampling on the boundary (i.e., inter-partition) nodes. The sampling rate is considered as a hyperparameter to control the tradeoff between accuracy and efficiency. Experiments on two large benchmark graphs show that the proposed method achieves significantly higher hardware efficiency without loss of accuracy, compared with various baselines. \n\n------\nPros\n------\n+ Clearly defined the motivation and techniques. The overall paper is easy to follow.\n+ Strong experimental results (see also cons below for experiments)\n+ Distributed GPU training is an important topic\n\n-------\nCons\n-------\n- Limited novelty: the main algorithmic steps, partitioning and sampling, seems straight-forward. Specifically, the partitioning is based on the well-known METIS algorithm (However, METIS may not be scalable by itself). The \"boundary sampling\" simply performs IID sampling on the inter-partitioning nodes. This is a simple extension to the random layer sampler of the GraphSAGE minibatch method. \n- Motivation for full-graph training: Limited GPU memory is not the only reason for minibatch GNN training. Compared with full-graph training, it is also likely that minibatch training can improve the generalization of the model and thus achieves higher accuracy (as can be verified by the GraphSAGE vs. GraphSAINT rows in Table 2). Therefore, how to justify that full-graph training is important?\n- Insufficient experiments: The authors only evaluate the proposed method on two graphs, Reddit and ogbn-product. More importantly, it is not justified why we even need distributed training for Reddit in the first place. The training graph of Reddit can fit in the memory of a single GPU. What is the speedup obtained compared with single GPU training of Reddit? In addition, it seems that the accuracy is not significantly affected with very small p (Table 2). This implies that maybe we don't need boundary sampling at all, and can drop the boundary nodes entirely. \n- Unclear experiment setup: the GNN architecture details and the hyperparameter tuning procedures are completely missing. This makes the Table 2 results less convincing. \n- Where does the accuracy gain come from? Due to the lack of GNN architecture details, it is hard to judge where the accuracy gain comes from in Table 2. Supposedly, the \"GraphSAGE\" row corresponds to the case of full-batch training of the GraphSAGE architecture. According to the claim of the paper that dropping neighbors incurs information loss, the BDS-GCN accuracy should achieve at most the accuracy of GraphSAGE. Why the accuracies are higher?\n\n-----------------------------------\nRecommendation: Reject\n-----------------------------------\nIn summary, I think this paper lacks motivation and novelty. The experiments are not thorough enough. \n\n\n--------------\nQuestions\n--------------\n\n1. There is a claim in page 3 that all the sampling based methods have \"extra time cost for sampling batches\". How significant is such overhead? It seems that the sampling of GraphSAGE and GraphSAINT are pretty cheap -- they are just based on random neighbor selection and random walks. For Cluster-GCN, it seems that the overhead is the same as the proposed BDS-GCN, since both use METIS. \n\n2. Claim on unbiased sampling: Such claim should be more carefully phrased. Under what condition is the sampling unbiased? Only for GraphSAGE-mean? Ignoring the activations?\n\n3. In Figure 4, it seems that increasing the number of GPUs does not give any benefit in terms of training time. What about the time for a single GPU training without any partitioning?\n\n4. In Figure 5, the reduction in memory seems inconsistent with the measurement in Table 1. I would expect that reducing p from 1 to 0 would reduce the memory by multiple folds (since the number of boundary nodes is an order of magnitude larger than the number of inner nodes). However, in Figure 5, the memory is reduced only twice. Why is it the case?\n\n5. What is the GNN architecture and what is the hyperparameter tuning procedure for Table 2?\n\n6. Where does the accuracy gain come from compared with GraphSAGE row in Table 2 (see above \"Cons\" paragraph)?\n\n7. What is the accuracy if we completely drop the boundary nodes (i.e., setting p=0)?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}