{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non-target class. The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written.\n\nUnfortunately, it seems that some of the main claims, such as DNNs trained on clean data \"learn abstract shapes along with some texture\", resort to qualitative evaluation of the few examples shown in the paper. Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors' claims. In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated. \n\nAs these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point.  That said, the approach looks very promising and I hope the authors continue to pursue this idea."
    },
    "Reviews": [
        {
            "title": "A simple but reasonable method",
            "review": "This work studied what the model has learned from the data. It proposed a class-wise pattern searching method to demonstrate the existence of class-wise predictive patterns in the input space. It studied three training settings, including natural, backdoored and adversarial, and revealed different characteristics of these training settings. \n\nI think that the motivation is very clear,  and the writing is readable, though some minor typos or inconsistencies should be corrected. \n\nThe proposed class-wise pattern searching method is very simple but reasonable. My opinion is that it may get somewhat inspiration from the backdoor triggers, since the format of the mixed input in Eq. (1) is same with the poisoned sample with triggers in the backdoor learning. If a discussion about this connection is added, then the derivation will be more smooth. \n\nMore details about the minimization of $\\mathcal{L}$ in Eq. (2) should be added. Although one algorithm is presented in Appendix A, there is still no sufficient explanations and analysis (e.g., the convergence). And, there are many hyper-parameters in the algorithm. Their settings and adjustments should be more clear, though there are some brief descriptions at the beginning of Section 4. \n\nThe studies about three trainings settings appreciated. And the presented results indeed provide some insights of these training settings. \n\nThe proposed method reveals many interesting insights of what DNN has learned from the data under different training settings. However, if the insights could be used to improve the current training method or DNNs, the value of this work could be larger. I would like to see some discussions about this point in this manuscript. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a visualization method to reveal the class-specific discriminative patterns of DNNs in the input space. When added to images from another class, such patterns can lead the DNN to classify the images into the pattern's class. From the experimental results, the authors conjecture that images trained on natural data can have backdoors. It also claims that the method reveals the trigger patterns of backdoor attacks, that adversarially trained models learn simplified shape patterns, but an intentionally-perturbed robust dataset improves model robustness by sacrificing its ability to represent shapes. \n\nDespite feeling unsure about certain points, I think this paper is well-written. However, some of its main conclusions do not seem to be well-supported by the experimental results and analysis. \n\nStrengths:\n1. Proposing the interesting idea and method to find class-wise patterns that change the DNNs' prediction into a certain target image when added to images from another class. Such patterns are shown to reveal the abstract shapes and texture of the target class. \n\n2. Generalizing the analysis to various scenarios including backdoored models and adversarially-robust models.\n\nWeaknesses:\n1. The conjecture that \"DNNs trained on natural data can also have backdoors\" does not seem to be well-supported by the \"predictive power\" in Figure 3, due to the definition of predictive power. My understanding of a backdoor trigger for image classification is some pattern that generalizes well to images that are not used to craft such triggers. However, by definition of predictive power, it is evaluated on the examples that are used to generate such patterns, since $ACC(f(x_n+p_y), y)$ seems to be defined on the nontarget-class images. This is more like finding universal targeted adversarial attacks in the white-box setting, which may not be so surprising to succeed. I think it would be much more convincing if $ACC(f(x_n+p_y), y)$ is defined on a different set of nontarget-class images that are not used to craft the patterns.\n\n2. It is shown in Figure 6 (right) that the patterns from backdoored models do not transfer well to natural models, and it is suggested that such a method can be used to detect backdoored models. However, the results for transferability across natural models trained from different initializations seems to be missing. We do not know how the pattern's predictive power changes on another natural model that is not used to craft the pattern, and if it drops significantly, we cannot expect to use the method to detect backdoored models.\n\n3. Patterns shown in Figure 6 (left) do not seem to reveal the trigger at all. Only the pattern from the white canvas match the pattern, but its position is not correct. It says the position mismatch may be caused by data augmentation, but it is not convincing enough since no results on backdoored model trained without data augmentation is shown. It would be much better if such results can be included.\n\n4. The patterns from Figure 7 are also not conclusive enough. For example, patterns for the \"deer\" class seems to be in a worse shape when crafted from $\\hat{\\mathcal{D}}_R$ than ${\\mathcal{D}}$, but it is the opposite for the \"airplane\" class. Therefore, I do not think we can conclude that \"intentionally-perturbed dataset improves model robustness by sacrificing its ability to\nrepresent shapes\".\n\n\n### Updates after the rebuttal\nI appreciate the authors' timely response. The latest update explaining the regularization effect of using a fixed canvas $x_c$ and a learnable mask to avoid overfitting to spurious features, which seems to be inevitable if using a learnable $x_c$, does seem plausible and differentiates itself from the general idea of UAT. From the updated content in Appendix H, using a learnable $x_c$ does not seem to be able to find interpretable patterns. I feel contributing a new method with some improvements is worth an accept.\n\nHowever, I still feel the results are not conclusive enough. Regardless of the final decision, I hope to see more comparisons with simpler baselines in the future version. To highlight the novelty and effectiveness of the proposed method, the authors should try to compare with more baseline approaches that do not have presumptions on $x_c$ and initialize $x_c$ randomly. Currently, $x_c$ are selected as images having highest prediction scores from the model. Instead, we can learn $x_c$ from random initializations by using some approaches that enhance the transferability of adversarial examples. There are simple methods on improving the transferability of adversarial examples, e.g., adding Gaussian noise when crafting adversarial examples [1], or adding different random data augmentations [2]. These methods may also lead to interpretable patterns but the effectiveness of the resulting patterns could be stronger. I do not fully agree that a desirable canvas has to be a neutral or unbiased image. \n\n[1] Wu, Lei, Zhanxing Zhu, Cheng Tai, and Weinan E. \"Understanding and enhancing the transferability of adversarial examples.\" arXiv preprint arXiv:1802.09707 (2018).\n[2] Huang, Qian, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. \"Enhancing adversarial example transferability with an intermediate level attack.\"  CVPR (2019).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but needs a different focus",
            "review": "#### Summary\nThis work proposes a technique to compute sparse patterns in the input space that are highly predictive of a class, even when added to samples from a different class (non-target samples).\nPatterns for class $c$ are produced by optimizing a point-wise weighted average of two samples (a fixed sample called the canvas and one that changes after each optimization step referred to as $x_s \\in X_s$) s.t. the result is highly predictive (yields a low error) for $c$. Learned weights for the average (the mask) are thresholded and used as a binary mask for the canvas (which is either a sample that has been labeled/predicted as class $c$ or a white image).\nAnalysis of the resulting patterns focuses on explainability on various fronts: first, the consistency of the pattern is evaluated (via a custom metric called 'predictive power') which shows that resulting patterns do cause a CNN to predict the class that has been optimized for.\nA few remarks on the patterns are made and also some claims regarding the link to the model's learned features (like position, structure, and number of shapes).\nAttention maps confirm that the saliency of the patterns prevails over the original content of the canvas image. Finally, models trained adversarially and with backdoors are analyzed by computing the sparse-mask patterns.  \n\n\n#### Strong and weak points of the paper:\n##### Strengths:\n - The paper is easy to read and the main ideas can be followed. A wide array of experiments are conducted with a small and (at least a subset of) a large dataset.\n - The proposed method is straightforward and described in enough detail to be easily reproducible.\n - Comprehensive and relevant related work has been cited and compared.\n - Section 4.2: an interesting baseline showing that the resulting patterns are more effective at causing the model to predict a target class than highly responsive areas of a \"natural\" sample.\n - Section 4.3 and 4.4: the inclusion of an analysis of training regimes that are known to affect the feature space learned by a neural network (adversarially trained, trained with robust features, and trained with backdoors) provides relevant use-cases for a method that is proposed as an interpretability tool.\n - Section 4.4: interesting results that seem to agree with Ilyas et al. 2019 in that more robust models require a perturbation that looks more like the adversarial target.\n\n##### Weaknesses:\n - The main concern is that the goal of the paper is not aligned with the findings: I consider that the patterns shown are a valuable tool to analyze the behavior of a neural network, but from a completely different perspective. These patterns (and the algorithm to obtain them) suggest that there are global-targeted adversarial perturbations (a close analogous to the work of Moosavi et al.: global perturbation because it is one perturbation applied to a set of samples; targeted because each perturbation has been designed to cause attacked samples to predict one specific class). In turn, the patterns' appearance is as difficult to interpret as those of classical adversarial perturbations or global perturbations. Claims regarding shape, texture, position, etc, are not well supported by measurable evidence (beyond a few observational remarks supported by a handful of examples), and are more likely the result of the method by which these perturbations are obtained (e.g, the regularization w.r.t. the mask, clipping, binarization, point-wise multiplication). Instead of trying to interpret the appearance of such perturbations, a focus on the implications of global-targeted adversarial perturbations can provide valuable insights into the manifold's structure learned by modern neural networks as well as their shortcomings. \n - Most experiments are missing key details regarding their setup. In particular, the canvas image is often not shown (especially relevant for Figures 1, 2, and 7). From figures where it is shown (like Figure 4), it seems like regardless of the canvas, a binary pattern with a global-targeted perturbation is learned.\n - An analysis of the impact of the canvas is missing w.r.t. predictive power which could support/invalidate the idea of the learned pattern to be a global-targeted adversarial perturbation.\n - Section 4.1, Patterns Revealed by Positive Canvas: several claims regarding the properties of the patterns (e.g., structural consistency, number, and position of shapes) in the mask are only supported by a qualitative evaluation of a few samples. No metrics are proposed.\n - Section 4.1, Patterns Revealed by Positive Canvas: although a metric (predictive power) is used to measure the consistency of the pattern w.r.t. a target class, results are based on a few classes. Why not report it for all 10 classes on CIFAR10? Also, results on 5 classes of Imagenet is not representative of the variety found in that dataset. A simple average of the predictive power over each class would provide stronger support for the results found in the subset of selected classes.\n - Section 4.1, Patterns Revealed by Different Canvases: the absence of texture in the patterns is more likely caused by the sparsity term on equation 2 (together with the threshold and the point-wise multiplication with the canvas) rather than by an intrinsic property learned by the model.\n\n\n#### Recommendation:\n - In its current form and with the proposed focus, I consider that this paper cannot be accepted. The focus of the analysis can yield stronger, more conclusive results from the standpoint of global adversarial perturbations.\n\n#### Supporting arguments for recommendation:\n - Without the right context (i.e., not that of explainability but adversarial attacks) the analysis of what the patterns represent is poorly supported by experiments. There is a lot of work done in the direction of adversarial attacks that findings in this work could re-purpose. Meanwhile, the impact of proposing a \"global-targeted adversarial attack\" based on one sample seems promising, but it is out of the scope of this work's analysis. Refer to the list of weaknesses for more details.\n\n#### Questions to clarify:\n - How are the class-patterns related to universal perturbations?\n - How is this method fundamentally different (i.e., more advantageous) than a universal adversarial perturbation (Moosavi-Dezfooli 2017) generated for each class?\n - If the learned pattern says something about the (clean) feature space of the classifier, what do these patterns reveal about classes with a large intra-class variation? For example, the class \"clock\" contains digital and analog clock faces (on the extreme, digital clocks show only numbers, and analog ones only the hands of the clock). Apart from their semantic relationship, there is no visual correspondence between the two sub-categories. Would this scenario be a limitation of the proposed method or what does it reveal about the model/the data?\n - Section 3, Equation 2: what happens to the mask (therefore to the conclusions from the patterns in the mask) with other regularization metrics like Total Variation (TV)? TV is used in [2] to find masks with connected areas that are representative for each sample in a counterfactual manner (i.e., blurring the area of the mask, causes the model to predict something else). In fact, not using TV makes the mask converge to an adversarial perturbation. Comparing those findings to those on this work (in particular Equation 2), makes me think that the resulting mask is more of a targeted adversarial attack rather than a representative pattern of the class. Moreover similar work published in ICLR 2016 [3] shows how almost any source image can be 'anchored' to the prediction of a guide image (equivalent to the canvas image in this work). This paper is essentially computing a sparse version of that attack via a masked input and the output layer (in [3] they don't apply masks and the deep representation was not constrained to the output but an arbitrary layer). It would be interesting to show the reconstruction of samples that have been \"attacked\" with the patterns in this work (following [3]).\n - Section 3, sampling canvas and results in Figure 4: how is the predicted power affected by the choice of canvas (positive, negative, or white)? Evaluating on a few classes can be misleading and although the metric of predictive power (PW) seems adequate, I suggest presenting a global metric by taking the mean PW (mPW) to better support the generality of the corresponding results.\n - Experiments with Imagenet: can you specify which classes the chosen IDs map to? Are they all related somehow? What were the criteria to select those 5? Why only 5?\n - Figure 2: it would be important to see what the canvas image looks like in order to establish how much of the pattern is just coming from that canvas.\n - Section 4.1, Patterns revealed by Positive Canvas: how are the number of shapes measured? Where does one shape start and one ends? It seems this observation is rather empirical and only a few samples are shown as evidence (one may ask how often does this happen and whether it happens because the dataset also has multiple shapes -which requires no mask, just simply looking at the samples). The same issues arise with claims about the position where the patterns occur (how is this measured?).\n - Section 4.1, Patterns Revealed by Different Canvases: how is the consistency of patterns evaluated? Is there further evidence beyond the 4 samples shown in Figure 4 for CIFAR10? Those seem to share structural similarity but, is this always the case? How do you establish structural similarity on the samples from Imagenet?\n - Section 4.2: taking one test image per class (source) whose highest activation area (with GradCAM) is pasted into ...how many target images? In other words, are the results in figure 5 (right) based on how many samples overall? This experiment needs to be described in more detail to be reproducible.\n - Claims for backdoor images about the trigger being recoverable is rather weak for two reasons: (1) there is only one class being evaluated and more importantly (2) there is only one (simple) pattern that has been analyzed. From this experiment, there are some indications that backdoor patterns may be reliably retrieved but nothing conclusive (the full trigger pattern has not been reconstructed and other, more complex patterns have not been evaluated).\n\n\n#### Additional feedback (not necessarily part of the decision assessment):\n - The notion of 'backdoor' is not explained in the abstract and it is not clear from the context what it could be. Including a brief description about it on the abstract could improve readability.\n - In the second paragraph of Section 1: references for \"backdoor\" attacks and \"adversarial\" attacks are swapped.\n - Section 1, 4th paragraph: the notion of \"canvas image\" is used without it being defined.\n - Section 2, \"General Understandings of DNNs\": there is work that has also focused on modeling properties of the input space over the dataset (and not only sample-wise) [1].\n - Section 3 paragraph \"Canvas Sampling\": typo for \"reveal\".\n\n\n#### References:\n[1] Palacio, Sebastian, et al. \"What do deep networks like to see?.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\n[2] Fong, Ruth C., and Andrea Vedaldi. \"Interpretable explanations of black boxes by meaningful perturbation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n\n[3] Sabour, Sara, et al. \"Adversarial manipulation of deep representations.\" ICLR (2016).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Insightful and a pleasure to read",
            "review": "The papers proposes a simple method for visualizing the patterns learned by deep neural networks in the supervised classification setting. Informally, suppose you have an image x that is \"representative\" of the class y and let X be a set of images that belong to other classes. The authors propose an optimization problem that looks for a mask (i.e. set of pixels) along with values of those pixels such that when this pattern is added to any image in X, the model will predict the new image to have the label y. This optimization problem can be solved using iterative thresholding and one may control the level of sparsity as the authors studied. Despite its simplicity, it can reveal clear patterns, particularly on high resolution images, such as ImageNet. The authors, then, show how this method can be used to interpret neural networks, detect backdoor attacks during training, and verify robustness.\n\n\nOverall, it is a great paper, insightful, and a pleasure to read. \n\nSome comments:\n\n1- The authors insist throughout the paper that this is a method for discovering class-wise patterns. To me, it appears to be a sample-wise pattern since a single representative sample x is used. When multiple samples are used, multiple patterns are detected at different locations as shown in some of the figures. I don't understand why the authors make such an emphasis. The algorithm seems great for providing interpretability per sample. Perhaps, this is because other methods for sample-wise interpretability already exist, such as attention maps, but I don't think that is a serious issue since it does not hurt to have multiple approaches for the same problem. I would suggest that the authors mention, at least, in the paper that this can be used sample-wise as well for interpretability. \n\n2- I am curious to know if upsampling CIFAR10 to have 224x224 images with a reasonable interpolation method would generate better patterns (closer to the patterns observed in ImageNet).\n\n3- In Section 4.3, the authors speculate that augmentation is responsible for the fact that the method finds the trigger patterns at different locations. This is easy to verify by simply training without augmentation. Has this been done? \n\n4- It would be better for readability if all images are placed at the top of the pages, rather than in arbitrary places. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}