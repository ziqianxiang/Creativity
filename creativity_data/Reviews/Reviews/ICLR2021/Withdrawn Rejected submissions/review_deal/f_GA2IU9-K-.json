{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work proposes a non-decreasing quantile functional form for distributional RL, and secondly propose using the distributional error as a means of exploration. The experimental results are very exciting. The paper, however, needs further work before acceptance: the reviewers raised concerns about Theorem 1: a full proof is not included (nor written convincingly during discussion), and while several encouraging experiments were added during the discussion to the paper addressing the reviewers concerns, they fell short (understandably, given the time available).\n\nThus on this basis, I recommend rejection at this time, but think it likely that with these adjustments the paper will be accepted in future."
    },
    "Reviews": [
        {
            "title": "Interesting alg that extends DLTV to IQN  ",
            "review": "This paper studies distributional RL and proposed two extensions. One is a method to enforce a non-decreasing ordering of quantile functions by a linear and non-negative increments. The other is extends the idea of DLTV which adds exploration bonus in action selection by using the random network distillation method, which in particular, using a measure of inconsistency between target network and predictor networks as a frequency measure of sampled states. \n\nHow do you convince us enforcing a non-decreasing ordering of the learned quantile functions is helpful? \nI understand your arguments, but there is no evidence in the paper showing that doing so is helpful. \n\nComparison with DLTV is missing. \nThe paper argues that DLTV is not applicable to continuous quantiles. However, it would be to include this comparison especially they have results on Atari games as well. \n\nThe empirical results are not very strong, with 13 and 14 ties and losses with/to IQN. It appears the advantage of DLTV over QRDQN is larger than your advantage over IQN. \n\nThe technical quality and presentation of the paper can still be much improved. \n\nAbstract: \ntwo important problems still remain unsolved. \nthe other is how to design\nan efficient exploration strategy to fully utilize the distribution information\n--> Later you showed this is false argument by introducing DLTV (Mavrin et. al. 19)\n\nWe describe the implementation details of the two architectures with\nwhat are they?\nyou have two \"architectures\"?\n\n (b)(c) a simple incremental structure proposed in(3):\nthis sentence is confusing. \n\nWhat is the circle dot operator? (.)\n\nEq 4 is just interpolation to ensure positive increments. \nWhy do you need to show (3) since it's not good?\n\nTh 1: \nThe definition of the \\Pi operator isn't clear. \n\nBefore Sec 3.4:\n\nIs Relu is a good choice? What is your thought on other functions? How to choose g in practice?\n\n\nThe prediction error would be high\n->The prediction error would be higher\n\neq 11:\nThis is similar to Mavrin's idea: using exploration bonus -- UCT style. \n \n\n\n\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "Summary:\n\nThe paper provides a method to learn the quantile function in distributional reinforcement learning that guarantee its monotonicity. Moreover, an exploration strategy for distributional reinforcement learning is also presented.\n\nReasons for score:\n\nThe paper claims two contributions: a better way to model quantile functions and an exploration strategy for distributional reinforcement learning. The former is a trivial extension of previous work, and the latter is not properly explained. \n\nPros:\n\n1. The experiment results look good.\n\n2. The DPE exploration strategy seems to be effective in the examples.\n\n\nCons:\n\n1. The NDQFN is a trivial extension of (3) through piece-wise linear interpolation. The explanation of Figure 1 is misleading.\n\n2. There is another class of methods in distributional reinforcement learning which uses generative network to model the Z function, which doesn’t suffer from lack of monotonicity as in quantile function learning (see e.g. Y. Yue, Z. Wang, M. Zhou ``Implicit Distributional Reinforcement Learning”). These type of methods need to be properly discussed and also compared in the experiments.\n\n3. Theorem 1 could be problematic. The quantile projection defined in this paper doesn’t seem to be contractive. \n\n4. Section 3.4 is not well presented. \n\nQuestions:\n\nPlease address and clarify the cons above.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but not yet sufficiently proper experiments to understand the role of NDQFN and DPE separately, and fairer comparison with the baseline IQN is required ",
            "review": "###  Summary\nThis paper proposes to use a monotonic quantile function for distributional RL by: (i) estimating the quantile values at the supported quantiles via a cumulative sum of non-negative incremental value, and (ii) interpolate the quantile values at the unsupported quantiles via linear combination of the nearest supported quantiles. The paper then also proposes to estimate the exploration bonus for each state using random network distillation. It then conducts experiments in Atari games to verify some conclusions.  \n\n### Strong points \n-\tClarity: The paper is well structured. \n-\tTechnical novelty: immediate. The idea of piece-wise linear function to approximate quantile function in Eq. (4) seems novel though very natural. The idea of using random network distillation to measure the exploration bonus seems novel and useful. \n-\tEmpirical significance: The combination of DPE with NDQFN performing well in some hard-explored games (Fig. 3) looks very encouraging and promising. In my opinion, I think this is the most interesting and significant empirical result reported in this paper in the current form.\n\n###  Weak points\n-\tThe experimental comparison/conclusion with IQN in the present form is unfair. \nIn particular, after the Table 1, the authors conclude that “NDQFN with DPE significantly outperforms the IQN baseline”. I think this is an unfair conclusion as the result of IQN reported in Table 1 is merely the performance for distributional part, i.e., the IQN result in Table 1 does not include any orthogonal improvements such as n-updates, double networks and any advanced exploration method beyond epsilon-greedy while NDQFN incorporates DPE exploration method (and n-step updates  and double Q update if I infer correctly from the paper). To be fair, I think the paper should include to IQN any orthogonal improvements that NDQFN has.  Even though Figure 2 has a fair comparison between NDQFN and IQN where both use n-step updates and without DPE exploration, I think only 6 games are not sufficient to make a reliable conclusion that the proposed non-decreasing quantile function is more helpful than the original quantile function in IQN. \n-\tIt is unclear from the experimental results how helpful are each non-decreasing quantile function and DPE individually. \nExcept a nice result in Figure 3, I am not fully convinced if the proposed non-decreasing quantile function is actually more helpful than the original quantile function as a distributional component. For example, I think it is more useful to report the result of a mere NDQFN (i.e., without DPE or any exploration methods rather than epsilon-greedy, without any orthogonal improvements such as n-step updates and double Q value) in the full Atari games to see how much it improves over the original IQN (reported in Figure 1). Regarding DPE, since it is a new exploration method, I think it is more helpful and reliable to experimentally compare DPE with some other distributional exploration methods such as  DLTV (Mavrin et al. 2019). I think the current experimental results are hard to make any reliable conclusion about each NDQFN and DPE individually except a nice observation that a combination of NDQFN and DPE can help in some hard explored games.   \n\n###  Questions for the authors. \n-\tThough I believe that IQN indeed does not guarantee the monotonicity in quantile values it estimates, the illustrative figure for this (Fig 1) seems a hypothetical one instead of a real experimental result.  Can we show that empirically by a simulation? \n-\tDoes the NDQFN+DPE in Table 1 also have n-step update and double Q network update style? \n\n###  My initial recommendation\nFor the main reasons in the weak points section, I vote for rejecting for this current form.\n\n### My final recommendation \n\nThe authors have attempted to address some of my points but these points require more time to fully address as they require to run more experiments. For the current form, I remain my inital score and recommend rejection for this time. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Paper Summary: This paper mainly contributes in two parts: 1). A non-decreasing structure for quantile values in quantile-based distributional RL. 2). A curiosity-based intrinsic reward using distribution disagreement.\n\nClarity: \n- Some mathematical expressions, while correct, are hard to interpret, e.g. G_{i,\\delta}, H_{i,\\omega', \\omega^*}. It would be nice if the authors could provide a brief explanation in words. For example, G_{i,\\delta} is just linear interpolation given fixed support.\n\n- The paper stops at experiment results. There are no further discussions or conclusion section.\n\n- The relation between RND and proposed DPE is not as close as the authors suggest. In RND, the target network is a fixed network for the predictor network to mimic. In DPE, both the target network and the predictor is trained with the same objective, i.e. minimize TD error. The intrinsic reward is evaluated by the disagreement between the predictor and the target network. Such disagreement-based intrinsic reward had been studied in other lines of work, e.g. [1].\n\nPros: \n- Resolves existing crossing quantile issue in most quantile-based distributional RL algorithms.\n- Proposed method ensures monotonicity on all possible quantile sets. Using linear interpolation for distribution approximation feels natural and straightforward.\n- Distribution based curiosity is novel.\n\nCons:\n- The significance of DPE is not properly evaluated. In fact, the paper only provide detailed results for NDFQN+DPE. It is hard to tell exactly how much of the performance gain is credit to NDFQN or DPE. To evaluate the significance of DPE, I would expect the authors to compare across different curiosity-driven exploration methods on the same baseline, say IQN. At least, the authors should answer how much distribution-based disagreement outperforms value based disagreement.\n\n- Same reason as above, the performance gain of NDFQN itself cannot be inferred from just 6 games. Judging from section 4.1 and 4.2, I would presume that the performance gain is mainly credit to DPE instead of NDFQN.\n\n- The issue in figure 1(c) is not entirely resolved. Different from the issue in figure 1(b), this issue directly comes from using incrementals instead of values. The authors partially resolves it by using a fixed set of support, but if the support is changeable the issue remains.\n\n\nQuestions:\n- Is the embedding of p* really necessary? Or, is p* necessary for network input? As the support p* is fixed, I do not believe that they should be part of the input. From my own point of view, using modified QR-DQN's structure sounds more reasonable. If the authors find out that an additional, fixed input does impact the performance significantly, some explanations or insights are required.\n\nPost rebuttal:\n- The authors have addressed most of my main concerns and the additional experiment looks good to me. Therefore, I increase my score to 6. However, some experiment results are still missing and the paper still needs some editing before published, especially the experiment section. For example figure 6. is misleading since the comparison is not fair.\n\nReferences:\n[1] Pathak, Deepak, Dhiraj Gandhi, and Abhinav Gupta. \"Self-Supervised Exploration via Disagreement.\" International Conference on Machine Learning. 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}