{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nDescription: \nThe paper presents a generative model, SketchEmbedNet, for class-agnostic generation of sketch drawings from images. They leverage sequential data in hand-drawn sketches. Results shows this outperforms STOA on few-shot classification tasks, and the model can generate sketches from new classes after one shot.\n\nStrengths:\n- Detailed, technically sound, presentation \n- Shows that enforcing the decoder to output sequential data leads to a more informative internal representation, and thus generate better quality sketches \n- Improves over unsupervised STOA methods\n\nWeaknesses:\n- Experiments are done against methods that do not use the sequential aspect of sketches. Because ground-truth in this case contains much more data, comparison is not quite fair.\n- Will have been useful to see results against a baseline that uses it.\n- Quality of sketches generated from natural images is low"
    },
    "Reviews": [
        {
            "title": "High quality submission with impressive and promising results",
            "review": "Authors investigate the possibility to learn a generalized embedding function that captures salient and compositional features of sketches by directly imitating human sketches. The manuscript is written clearly and concisely. Methods have been presented with enough detail and seem accurate. Particularly, the results from the Quickdraw and Omniglot datasets showing generated sketches are rather impressive, and the ones for the natural images seem promising. Overall, I very much enjoyed reading the paper and suggest it for publication without any major changes.\n\nIn my view the results presented in Figure 5, and especially 5C, are the most impressive and interesting ones. These results deserve more space in the manuscript. I was curious to know whether there were also many unsuccessful conceptual composition examples. Are the examples shown in Figure 5C the best ones, or are they representative of performance in general? Does this approach also work with natural images to any extent? Could the authors elaborate on why or why not this may be the case?\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Initial review",
            "review": "A brief summary:\nThis paper shows that the model trained to restore sequential data from images in a supervised manner tends to capture more informative latent representations of the data.\n\nStrengths:\n+ Demonstrates that training a decoder model to reconstruct sequential sketches leads for an encoder to better represent the input image.\n+ Achieves the SOTA result in Omniglot recognition task, compared against existing unsupervised methods.\n\nWeaknesses:\n- Confuses readers with the ambiguous claim. The embedding function is said to produce `salient and compositional features' (p1), but no evaluations on compositions were included in the main paper. \n- Does not include any ablation studies to show the effectiveness of each components of the proposed model.\n- Does not include thorough explanations or analysis on each set of experiments.\n\nInitial recommendation:\nBorderline reject.\n\nReasons:\n- While this paper provides a lot of experimental results, which I very much appreciate, I still found most of them quite irrelevant to support the main claim by the authors, which discusses compositional embeddings. The main contribution of this paper, I believe, is mainly the idea to utilize the sequential sketch data during the supervised training time. This needs to be clearly stated, particularly inside the tables, as most baselines only use sketch images, not their sequential data. \nThat being said, this trick is shown to work well, and if these significant improvement on Omniglot is further verified, this trick will be found useful by the community of researchers who work with sequential data, such as sketches or handwriting.\n- However, I found most of the implementation details quite unclear, and experimental results were often misleading. For instance, the results from Table 4, if we look closely, the authors' results use ResNet12 trained with Sketchy data, which quite differs from other results in the list.\n\nFeedbacks:\n- Authors briefly mention in p.4 that a KL term did negatively effect the performance, but no detailed explanations/experiments were given.\n- What happens when you provide the class labels and train the model with classification errors as well? Will it improve the test result? If so, how much? \n- May have been much more interesting if this paper explores the unsupervised disentanglement in latent space, to support their claims for what they note as structured embeddings. Will other latent disentanglement methods (e.g. beta-, factor-, VQ-VAE, etc.) lead to better representations?\n- Conv-VAE alone may not be the best baseline.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice approach to learning image embeddings based on vector sketches but requires some additional clarification and discussion",
            "review": "### Summary\n\nThis paper proposes learning embeddings for sketch or natural images by training a network that takes in a raster image and outputs and collection of sketch strokes. The architecture consists of a standard CNN encoder followed by an RNN decoder. The authors evaluate their learned embeddings on few-shot classification tasks and explore the the quality of the latent space. They demonstrate that they outperform unsupervised few-shot classification approaches and seem to obtain a latent space that is more aware of long-range structure than those from methods that operate purely in raster space.\n\n---\n\n### Explanation of rating\n\nInterpreting and synthesizing sketches in a deep learning context is an promising research direction. While the idea of focusing on of sketch-aware embeddings of images is an interesting one, the main technical contribution simply involves taking a standard convolutional encoder with a recurrent decoder, which has already been used for sketch generation (sketch-rnn). In addition to this, some of the claims made in the paper require some clarification or additional experimentation, as I explain below. Thus, I believe that some additions and changes must be made for the paper to be accepted.\n\n---\n\n### Pros\n\n- I like the idea of using the vector structure of sketches to gain more insight into image content.\n- It is difficult to design deep networks that are able to operate on non-raster data. In some sense, this approach sidesteps this issue by allowing the relationship between raster and vector to be learned.\n- The experiments in Section 6 confirm the idea that the embeddings are aware of this high-level long-range relationships that aren't so obvious on the pixel level.\n\n---\n\n### Cons and questions\n\n- I am not sure I follow the intuition behind why the proposed model achieves better semantic awareness than a convolutional VAE. The authors use the example of a six legged turtle and state that the VAE would only retain information about a single \"leg-like feature but not how many legs are present.\" How is having an RNN stroke-based decoder different from a standard convolutional decoder in this sense? In both cases, the final reconstruction must contain the original number of legs, and so the latent vector is encouraged to retain this information.\n- The performance on natural images in Figure 4, especially on unseen classes, is not great. I would be interested to see the nearest neighbor images in the training set for the examples shown. Even in the case of unseen classes, the resulting sketches look like they might match a similar image from the training set.\n- The authors claim that balancing the stroke and and pixel losses via a curriculum mirrors how humans learn to draw (\"paint-by-numbers\"). However, I'm not sure how some of the experiments fit into this methodology. In particular, most of the experiments are done on datasets that have ground truth sketch strokes but not their ordering (e.g., SVG files). In this case, it seems like imposing an order on the strokes (and asking the decoder to replicate it) is a counterintuitive constraint for the model. On the other hand, in the natural image experiments, the pixel loss cannot be used at all. Some discussion of the consequences there would be interesting.\n- I'm not sure that it is fair to compare to fully unsupervised few-shot classification methods. While the proposed method indeed does not use class labels, the ground truth stroke information may provide considerably more information than just the raster image data. Perhaps it would help to have a baseline without stroke loss (i.e., $\\alpha=1$). Even the ablation study in Table 3 does not include this case.\n- How sensitive is the approach to quality of stroke decomposition? For instance, what happens if you subdivide each stroke in the ground truth SVGs?\n\n---\n\nThe authors have addressed many of my concerns in the rebuttal, and so I am increasing my rating.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "contributions are not very clear",
            "review": "This paper proposes a generalized sketch drawing model named SketchEmbedNet for producing sketches and visual summaries of open-domain natural images. The idea is interesting and the experimental results show SketchEmbedNet is able to do not only few-shot classification but also one-shot generation. \n\nOverall, I vote for rejecting. In my opinion, the main contributions of this paper are not very clear. The introduced model, SketchEmbedNet has limited novelty on neither the methodology nor the network structure. As stated in the title and introduction, the authors aim to capture and generalize the compositional information from sketch/natural images. Section 4 reports the latent variables organization performance, which is directly related to the key motivation I believe. But the authors only compared SketchEmbedNet with VAE, which is not enough to demonstrate their advantages. Moreover, it is not clear why few-shot classification and one-shot generation performance in Section 5 and 6 support their main idea. Thus, this paper needs further improvements.\n\nDetailed comments:\n(1)\tIn the first paragraph of section 2, the authors claimed that the CNN embeddings must preserve a compositional understanding of the input sketches/images to improve the performance in their pix2seq task. So how did you preserve the information? Many sketch synthesis methods, such as [8] in the reference, can reconstruct the sketch with a sketch image input. Do these methods preserve the ``compositional understanding’’?\n(2)\tStill in the same paragraph, I’m pretty sure a vanilla auto-encoder with a CNN encoder containing average pooling layers can reconstruct a six-legs-turtle well as input. Thus, the information about the positions and the turtle legs number must be transported to decoder by the latent embeddings. So why you confirm that a regular CNN embedding cannot preserve that information after an average pooling layer? \n(3)\tThe same CNN encoder is used for both natural images and sketches. As these two types of images are with totally different patterns, many recent studies, such as reference [55], used a two-branched encoder for natural images and sketches, respectively. In this paper, the CNN backbone is a 4-layer CNN or a ResNet12, which are very basic structures. Is it able to extract the image features well?\n(4)\tFigure 5 shows SketchEmbedNet outperforms VAE on latent space organization. In my opinion, the well-organized latent space of SketchEmbedNet is mainly due to getting rid of the KL loss term, which drives the latent distribution to be a uniform distribution. I would like to see the comparison with sketch-pix2seq, which is the reference [8], as both SketchEmbedNet and sketch-pix2seq do not use KL term in training.\n(5)\tAs this paper focuses on sketch drawing, why there is no comparison between SketchEmbedNet and other sketch generative models, such as [8, 11, 15, 26] in the references? \n\n\n\n\nAfter reading the response from the authors, we raise our score by +1.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}