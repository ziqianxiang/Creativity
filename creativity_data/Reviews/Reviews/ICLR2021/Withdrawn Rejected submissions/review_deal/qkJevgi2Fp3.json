{
    "Decision": "",
    "Reviews": [
        {
            "title": "Feed-forward variant of LMU model",
            "review": "**Summary**:\n\nThe paper proposes changes to the LMU model to enable using purely feed-forward computations. The approach is evaluated on a variety of tasks including sentiment classification, semantic similarity, natural language inference and image classification. The experiments are performed on the proposed approach and an LSTM variant, and when applicable corresponding results are extracted from published works. The results indicate that the proposed approach outperforms the LMU, the LSTM, and also the alternative RNN variants.\n\n**Concerns**:\n\nEven though the paper aims to bridge the gap between Transformers and RNN models (e.g. feed-forward like Transformers but low parameter count like RNNs), the comparisons are primarily with respect to RNNs, and this limits the potential impact of the paper.\n\nThe paper highlights that the proposed method sets new *state-of-the-art results for RNNs* on the IMDB, QQP and psMNIST datasets. However, this claim is problematic given that the proposed changes turn the model into a feed-forward one, more similar to other approaches (like the Transformer) which were excluded from the comparison.\n\nFurthermore, given that the paper claims to be a feed-forward version of the LMU, I think a more detailed parallel between the two would strengthen the paper. For instance, do the characteristics in the original paper (e.g. optimality and uniqueness) remain true given the proposed changes?\n\nOverall, building a feed-forward version of LMU is an interesting direction, but I think that the paper is not ready for publication in its current form.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting model and results but too many open questions",
            "review": "The paper introduces a new variant of the legendre memory unit (LMU), that i) removes the need for a recurrent hidden state that's separate from the memory, ii) uses parallel memory slots to store information about high-dimensional inputs and iii) shows that the proposed model, though recurrent, is parallelizable which leads to a significant speed-up. The efficacy of their method is demonstrated on a number of standard language benchmarks as well as on permuted MNIST classification, a synthetic task that aimed at benchmarking memorization capabilities of RNNs. On all these tasks, their model shows strong performance when compared to similar approaches and in particular to LSTMs.\n\nI believe the paper makes some significant contributions that can improve LMUs, especially in terms runtime. Given the simplicity of the model that is applied to all language tasks (models with d=1), there is an interesting story to be told, either about the tasks themselves as in how \"easy\" are they, or the all the other more complicated models that show poorer performance. However, I have quite a few questions and doubts about the paper and method overall, which makes me hestitant in suggesting acceptance at this point. What bothers me most is that most models work with d=1 which trivializes the model to using something like exponential moving average as \"memory\", which just doesn't seem like a model that can capture more structured information.\n\n\nQuestions:\n\n\n1) The authors say  in the introduction that \"transformer-like architectures have very large parameter counts\". Why should this be considered a problem per-se? The only thing that matters in my view is the computational cost you have to train a model, or their cost at inference. Saving parameters is not an argument for or against an architecture. I believe the paper doesn't have to make a point why transformers are bad as I think it is totally fine and even necessary to explore non-mainstream approaches in parallel, even if they are not yet competitive.\n\n2) What happens if i=j in Eq (4)?  Also there is a typo: \" We thus take ff-LMU to refer to equations (6)-(7), \" --> shouldn't it be 6-8?\n\n3) It would be useful for the reader to explain how (7) can be rewritten as (10), and how such an operation would be implemented efficiently. It's not obvious from the manuscript.\n\n4) It would also be helpful to explain better the motivation for some of the changes to the original LMU, such as making $u$ a vector. I guess it is because in the current formulation there is not recurrent notion of h anymore which incorporated more than just scalar information from the input in the original LMU. In general I am curious how LMUs work in the first place considering that they only store scalar information in the memory units which seems like a fairly strong bottleneck. Do you have any intuitions on that and how it relates to your design choices? I guess one could argue that this paper implements a kind of parallel LMU since inputs to LMUs are only scalars.\n\n5) Looking at the formulae it seems like setting d=1 would simply implement something like an (elementwise) exponential moving average over the inputs u_t. Is that correct? It would be interesting to see that this is enough to achieve such results on the given tasks but it k+ind of limits the appeal of the paper as it is written right now, because this fact trivializes the LMU unit. I am quite certain that this wouldn't work on more challenging language tasks that would require more structured representations, e.g., QA.\n\n6) I think the datasets the model is tested on are very toyish. Most of them are small scale and not very hard. They serve as a good proof of concept but for a new architecture I would like to see at least one realistic setting where they really sohw promising performance.\n\n7) Precomputation of the memory is nice, but it would require encoding the raw input signal rather than a learned representation. Could you elaborate a bit more on how this could work on more realistic data than mnist? Could this really be a practical thing to do?\n\n8) Why is LMU_o used for some tasks and LMU_m for others? This seems rather arbitrary to me. Why not have a single omdel applied to all tasks?\n\n9) How well does the model transform when d is greater than 1 for language, or when setting d=1 for pMNIST? \n\n10) In sec 3.1) how come that the LMU has only 101 parameters? If the input embeddings are set to 100 and d_u are at least 100, then W_u should contain at least 100*100=10k parameters, no?\n\n11) Why is o_t not directly incorporating information from x as well, like in the original LMU?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New proposed model ff-LMU, which needs further investigation to be useful for the community",
            "review": "This work presents a new type of recurrent network, the so-called \"feedforward Legendre memory unit\" (ff-LMU).\n\nThe motivation is to have a more efficient model, while still keeping or even surpassing performance of other common more expensive models.\n\nMultiple aspects of efficiency are discussed (training time, decoding time, memory consumption), although the focus is the training time. This is also presented in Table 1 (although it does not clearly say that this is for training).\n\nExperiments are done on sentiment classification, semantic similarity, natural language inference, and image classification.\n\n\nRandom comments:\n\n- In Table 1, \"ff-LMU (convolution - frequency)\" is somewhat unclear, what this refers to. The word \"frequency\" is never used elsewhere in the text? I assume this refers to the calculation of equation 10? This should be made more clear.\n- Table 7 has a typo: \"pre-comupted\"\n- It's not exactly clear to me how the pre-computation of ff-LMU works. In equation 6, the first computation is the matmul with W, which cannot be pre-computed, because W is being updated during training. So what else can be pre-computed?\n- Table 7 is supposed to compare training time. But actually (for the non pre-computed case; also when it would use multiple layers, I assume) it doesn't look really faster than LSTM (s/epoch time)? Why is that? However, convergence rate (total time until 95% accuracy) looks better. But this is not really discussed further.\n\nPros:\n\n- Performance looks good on the selected benchmarks.\n- It looks like the model can be competitive with 3 orders less parameters (although this might be just on these selected benchmarks, where other complex models might have been overkill in the first place).\n- Table 7 suggests that convergence rate is faster (on this specific benchmark).\n\nCons:\n\n- No source code is published. This should be a requirement for acceptance.\n- I think the complexity per layer of \"ff-LMU (convolution - frequency)\" is wrong. How do you end up with \"ln(n)\"?\n- I think the sequential operations of \"ff-LMU (convolution - frequency)\" is wrong. If this is using equation 10, this clearly also takes O(n) sequential operations?\n- I think the selected benchmarks are not well selected to demonstrate the performance of the model in general terms. Language modeling tasks like enwik8 or WikiText-103 etc would be much more useful. Without such experiments, this work is incomplete. This should be added before this can be accepted. Even better would be to add the performance for some standard benchmarks like speech recognition or machine translation.\n- The pre-computation of ff-LMU is either wrong or unclear (see above).\n- A more consistent comparison of training time is missing, on a standard benchmark (e.g. enwik8), with a reasonable model, comparing LSTM, Transformer and ff-LMU. Currently it looks like it is actually not really faster (from Table 7)?\n- Analysis of convergence rate, general model behavior, etc is missing.\n\nSummary:\n\nI think this needs further work, most importantly evaluation on more standard language modeling benchmarks, but also more analysis.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "More evidence is needed to justify ff-LMU",
            "review": "This paper proposes a modified version of the LMU, named feedforward LMU (ff-LMU), the core of which is a linear time-invariant (LTI) system. The authors claim that the ff-LMU can be trained in a purely feedforward manner and yet executed during inference in a recurrent fashion. Thus ff-LMU can be trained remarkably faster than LSTMs of the same size. Overall, the problem studied is interesting, but I have some questions about this paper.\n\n1. The authors should train very large scale RNNs and compare it with transformers to validate the claimed advantages. Just doing experiments on a few very small scale datasets cannot support the claim made in the abstract. For instance, at least the authors should test in the PTB dataset. \n\n2. In paragraph 1 of the introduction, the authors claim that transformers are purely feedforward, which is a big advantage of RNNs for training efficiency. The main drawback of such an approach is that during inference, transformer-like networks tend to have very large parameters counts, and so are very demanding of computational resources. I would like the authors to give some evidence to support this statement. As far as I know, to achieve comparable results on the PTB dataset, transformer-xl does not require much more number of parameters than LSTMs. See Table 5 in the paper ``Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, arXiv:1901.02860’’.\n\n3. One of my main concerns is that recurrent neural networks can learn long-term dependency in the time series data. I want to see that the model in equations (6)-(8) can also learn long-term dependency. I would suggest the authors compare ff-LMU with SOTA exponential-RNN and other models on copying and adding tasks.\n\n4. ff-LMU_m is a linear dynamical system that cannot really approximate an arbitrary dynamical system. I doubt the utility of this model. I would like to see the performance of ff-LMU on approximate some given no nonlinear dynamical systems, e.g. x_{k+1} = 10*sin(x_k).\n\n5. ff-LMU_o still has a recurrence relation in the model, how can you train that purely as a feedforward neural network? I suppose backpropagation should be used here. The experiments section is very toy, which cannot really indicate the advantage of ff-LMU over LSTMs. I would suggest the authors test on copying and adding tasks, TIMIT, and PTB datasets at least.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}