{
    "Decision": "",
    "Reviews": [
        {
            "title": "an interesting study on adversarial examples. ",
            "review": "This paper proposes a framework for removing malicious perturbations to mitigate the negative effect of widespread adversarial examples. The authors show the challenge of defending against multiple and even unknown attacks. To defend against widespread adversarial examples, they propose extracting the invariant information of adversarial examples which is called perturbation-invariant representation (PIR) by performing adversarial training on the perturbation-specific information so that the unknown attacks could be generalized well via a prior distribution. Then, they generate examples that have no malicious perturbations from the PIR.\n\nStrengths:\n1: The task of defending against widespread attacks studied in this paper is interesting. The authors point out that attackers can use multiple attacks and even create unknown attacks to interfere with machine learning algorithms. This observation is consistent with the situation in the real world. With the increase in research on malicious disturbances, a defense that can withstand potential threats is of practical significance.\n\n2: The proposed framework is clear and technical sound. Some adversarial examples with different perturbations (pixel-constrained perturbations and spatially-constrained perturbations) are used to evaluate the effectiveness of the framework. Moreover, the extensibility evaluations in Section 4.4 more fully demonstrated the effectiveness of removing malicious perturbations. In particular, the authors conducted a hardness inversion test and use Lid to reflect that their framework eliminates malicious perturbations.\n\nWeaknesses\n1: The writing could be further improved, e.g., “via being matched to” should be “via matching to” in Abstract.\n\n2: The “Def-adv” needs to be clarified.\n\n3: The accuracies of the target model using different defenses against the FGSM attack are not shown in Figure 1. Hence, it is unclear the difference between the known attacks and the unknown attacks.\n\n4: Even though authors compare their framework with an advanced defense APE-GAN, they can further compare the proposed framework with a method that is designed to defend against multiple attacks (maybe the research on defense against multiple attacks is relatively rare). The results would be more meaningful if the authors could present this comparison in their paper.\n\nOverall the paper presents an interesting study that would be useful for defending the threat of increasing malicious perturbations. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Work is directly based on a defense that has been shown to be ineffective",
            "review": "This paper proposes an autoencoder architecture that is trained to remove adversarial perturbations (dubbed ADD) and which is claimed to provide better robustness against unseen attacks.\n\nThe idea is a modification of the APE-GAN defense and is directly compared to it. Unfortunately, this paper fails to mention that APE-GAN was broken in [1] and its robustness is almost the same as vanilla networks. However, the evaluation in this paper would suggest that APE-GAN is robust, meaning that the robustness evaluation in this paper is insufficient as it can not reproduce known results. Given the similarity of APE-GAN and ADD, the same holds for ADD. In fact, ADD adds a random component into the autoencoder making it even less likely that standard non-adapted adversarial attacks can succeed, which would explain the higher robustness values of ADD compared to APE-GAN.\n\nI recommend the community guidelines [2] and the tutorial on adaptive attacks [3] as a starting point for designing strong adversarial attacks against the proposed defense in order to get a more reliable picture of its efficacy. In particular, the proposed defense has a lot of moving parts (six loss functions, multiple hyperparameters, several subnetworks, adversarial losses, etc.) and one has to analyse carefully how each component has to be taken into account to design an optimal attack against this defense. Also, the use of ensemble averaging is required to stabilise the gradients in the face of the random perturbations in the autoencoder. I'd suggest to look out for red flags as present in Table 1 where some untargeted attacks are less effective than targeted ones (which should never happen for a strong adversarial attack).\n\nAll in all, there is little evidence that the proposed modification of the (broken) APE-GAN can defend against strong adversaries.\n\n[1] MagNet and “Efficient Defenses Against Adversarial Attacks”are Not Robust to Adversarial Examples, https://arxiv.org/pdf/1711.08478.pdf\n[2] On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.06705\n[3] On adaptive attacks to adversarial example defenses, https://arxiv.org/abs/2002.08347 [3] Decision-based adversarial attacks: Reliable attacks against black-box machine learning models, https://arxiv.org/abs/1712.04248",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Unclear defense with unconvincing evaluation",
            "review": "This paper proposes a defense against adversarial examples by training a model to learn perturbation-invariant representations. The authors argue that this approach should better generalize to unknown attacks. The defense is evaluated across a variety of datasets, models and attacks and is shown to achieve very high robustness.\n\nThe idea of this paper seems to be to learn representations that \"denoise\" adversarial perturbations. The premise seems sound, but the actual approach proposed in this paper is not very clear. The MMD term in (1) seems to encourage the encoder to learn a single representations for different attacks. Then, an additional network tries to predict the attack label from the representation. Moreover, latent representations are somehow mapped to Gaussian distribution, and an additional classifier is trained on this latent space.\nOverall, this defense is very complicated and exhibits all the tell-tale signs one would expect of gradient masking.\n\nThere are clearly issues in the proposed evaluation:\n- Figure 1 compares the performance of adversarial training and ADD with FGSM attacks. These results cannot possibly be correct. Adversarial training with FGSM attacks is known to not confer any robustness to iterative attacks such as PGD or C&W.\n- The paper compares to APE-GAN, and shows that APE-GAN achieves >80% accuracy for most attacks. Yet, Carlini & Wagner showed that APE-GAN is ineffective (\"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Method\", 2017)\n\nThe authors do apply BPDA, but it isn't clear how this is done. What loss function is being optimized? Which components of the model are non-differentiable, and bypassed with BPDA? \nBPDA is not a magic bullet. It has to be applied carefully to be useful. Similarly, AutoAttack is known to have a harder time attacking randomized defenses (I presume the defense is randomized because of the mapping of the latent representation to a Gaussian distribution. But it isn't actually clear how this part of the defense is supposed to work).\n\nIt is also unclear what the authors exactly mean by \"unknown attacks\". E.g., in Figure 3, the known attacks are untargeted and targeted PGD (presumably in the l-inf norm?). The unknown attacks include other l-inf attacks, L2 attacks, etc.\nIt is known that adversarial training against one type of l-inf attack will generalize to others (see Madry et al. 2017). Generalization across perturbation types is more interesting so it should be clarified here which attacks use which perturbation type.\nA simple experiment to check for gradient masking would be the following: train a model against known PGD attacks, and then evaluate against the brute-force rotation-translation attack of Engstrom et al. 2018 (\"A rotation and a translation suffice\"). Since this latter attack just tries out all possible perturbations, it cannot suffer from gradient masking.\n\nTo conclude, I strongly encourage the authors to familiarize themselves with current guidelines for evaluating defenses against adversarial examples, e.g., \n\"On Evaluating Adversarial Robustness\", Carlini et al. 2019\n\"On Adaptive Attacks to Adversarial Example Defenses\", Tramer et al. 2020\n\nThe current paper simply lacks enough details about the proposed defense, and has some clear evaluation issues that make me very skeptical about the claimed robustness.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This article studies a practical and meaningful problem, and the authors make sufficient experimental evaluations for their proposed method.",
            "review": "This paper discusses the problem of defending widespread adversarial examples. Considering that the new attacks are constantly being proposed, defenses that are trained based on known attacks may have limited performance against widespread unknown attacks. To solve this problem, the authors utilize adversarial training, Gaussian prior distribution, and a classifier in latent space to obtain the domain-invariant representation which removes the perturbation information generated from both known and unknown attacks. \n\nThe strategy of extracting invariant information from adversarial examples inspired by cognitive science is meaningful for defending against multiple attacks. The proposed method could help defense the unknown attacks, which are not used to train defense models. The application can be more widely used in the real world. In addition, the evaluations are implemented not only on common attacks (such as the PGD attack and C&W attack) but also on the attacks that produce shape changes (spatially-constrained perturbations), which is important to verify the effectiveness of a defense.\n\nIn general, this article studies a practical and meaningful problem, and the authors make sufficient experimental evaluations for their proposed method.\n\nHowever, there are some problems in the paper as follows:\n1. Authors need to give more details about the symbols in Fig 1(b), like \"ORI\", \"Adv-1\" and \"Adv-2\". There is a wrong link (“The Fig 3(b) in Appendix C.1”) in the second paragraph of Section 4.1.\n2. Similar to the multiple comparisons in Section 4.2, it is better for authors to compare their method with more defenses in Section 4.1. It would be nice if the authors could demonstrate that their method is more effective than some defenses which use mechanisms other than generative models.\n3. In addition, in Section 4.1, the target model without defenses has higher accuracies under some attacks, but the target model using defenses shows lower accuracies compared with other attacks, such as the targeted PGD attack on Fashion-MNIST and SVHN, the untargeted LS attack on SVHN in Table 1. The authors need to explain this phenomenon.\n4. The differences of the adversarial patches generated by attacks 1-5 in Figure 4 (a) need to be presented (authors can show the images of adversarial patches in their paper).",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}