{
    "Decision": "",
    "Reviews": [
        {
            "title": "missing theoretical or empirical justification for the improvements",
            "review": "\nThe main topic of this paper is the enhancement of recurrent networks for improving the task performance.\n\nThe proposed method combines the idea of external meta-controllers with recursive architectures. As a result, recurrent networks can be controlled by themselves by selecting the gate functions.\nThe authors show a relationship between several existing recurrent networks.\nFrom their explanation, the proposed method becomes identical to one of those by changing the hyper-parameter setting.\nThis may tell that the proposed method can be a broader framework that covers many recurrent networks.\n\n\nOverall, the method itself is fascinating and might provide a new insight though it seems very complicated relation structures in it.\nHowever, this paper seems to significantly lack detailed explanations for reproducing the proposed method and experiments.\n\nThe following are questions and concerns of this paper that should be solved before publication.\n\n\n1,\n\nFirst of all, the tile is somewhat hard to parse.\nSimilarly, it would be much better to provide an intuitive rough sketch of the proposed method's overall architecture.\nCurrently, it seems very hard for readers to understand the aim of the proposed method correctly.\n\n\n\n2,\n\nIt seems that there are several notation errors in the equations.\nFor example, it is hard to interpret the meaning of Eq.4,\nh^n_t =o_t ⊙c_t and h^n_t =h_t +x_t (4)\nWhat is the relationship between “o_t ⊙c_t” and “h_t +x_t”? \nIs this “o_t ⊙c_t = h_t +x_t” since both terms are equal to “h^n_t”?\n\nThere is no explanation about “x_t”.\nSimilarly, there is no explanation about “n”.\nThe relationships between “f^n_t” and “f_t”, “o^n_t” and “o_t”, and “c^n_t” and “c_t”, are also missing.\n\n\nTypo:\n“where F∗ (xt) = W xt + b is a simple linear transformation layer applied to sequence X across the temporal dimension”\n“F^∗ (xt)” => “F^∗ (x_t)”\n\nPlease check and add an accurate explanation for the proposed method; otherwise, it is hard to accurately judge if the proposed method has some meaningful new features to be shared in the community.\n\n\n3\n\nThis paper provides many experimental results to show the effectiveness of the proposed method.\nThe results seem to suggest the effectiveness of the proposed method.\nHowever, the main drawback of them is not providing theoretical or empirical evidence that the proposed method works well.\nThis paper only show the observation of improved performance of each task, but instead, it would be much informative to show why the proposed method successfully improved the performance comparing with the baseline methods.\n\n\n4,\n\n“For our method, we replaced the multi-head aggregation layer in the Transformer networks (Vaswani et al., 2017) with a parallel version adaptation of X-RNN. The base models are all linear layers.”\n\nIt is hard for me to correctly catch the proposed method's actual settings in detail from this statement.\nRecently, the community requires high-reproducibility papers, I think.\nHowever, this paper lacks a sufficient amount of explanations for reproducing the experiments in this paper. \nPlease explain more accurately what the authors have done in the experiments.\n\nThere seem to be several ambiguous statements that may become a disadvantage of this paper in terms of reproducibility.\n\n\n5,\n\nThis paper does not argue for the computational cost.\nSince the proposed method seems to have very sophisticated dependency structures, it is hard to know the computational cost only from a brief explanation of the models.\nPlease provide the actual decoding speed (and memory footprint, if possible) to judge the proposed method's efficiency.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting recursive concept that might provide advantages for some tasks",
            "review": "Summary:\n\nThe authors propose a recurrent network model where the gates are controlled by the same network recursively. The authors propose and show to some extent that this provides a useful inductive bias for hierarchically structured data.\n\nOverall I vote for an accept since the extensive empirical evaluation shows that the XRNN does have an advantage in many tasks. But the paper does have some key weaknesses discussed below.\n\nStrengths:\n\n+ The model is clearly described and the paper is well written.\n+ The model is well motivated and is simple (in spite of being recursive).\n+ The experiments are thorough and extensive. \n+ The advantages in terms of reducing the number of parameters for some tasks are significant.\n\nWeaknesses:\n\n- The paper lacks a discussion of the computational properties of the XRNNs. I would expect they are computationally more expensive, and this needs to be quantified and discussed more extensively.\n- In the pixel-wise sequential image classification task, the results are misleading because none of the other methods use R-Adam. And without that, it is not clear XRNNs have a performance advantage.\n- The performance improvements with the recursive parametrisation are small. An analysis of what the network is doing for these tasks (tree-traversal, logical inference) would be quite interesting. I would have expected stronger performance for the tree traversal task. I suppose this could be because only the gates are recursively parametrised, but suggests a potential direction of recursively parametrising more than gates.\n- A further discussion of why the optimal maximum depth and base units are different for different tasks would be useful. (Table 9)\n\nMinor:\n* The plots in Fig. 5,6,7,8 are unreadable on paper.\n* $F_n^*$ is used in the beginning of section 3.1 but defined much later. A reference to the definition would improve clarity.\n* The authors seem to have missed an excellent opportunity to use a self-recursive acronym for their model. May I suggest: SSRNs -- **S**SRNs are **S**elf-controlling **R**ecurrent **N**etworks. I'm sure the list of even better acronyms are endless.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea with many flaws in experiments, notation, analysis",
            "review": "The paper proposed a new recurrent model, namely the X-RNN, which is inspired by previous work where neural networks control/calculate the weights of another neural network. Here this is done in a recursive way, for multiple meta level. The meta level / depth index is dynamically calculated (in that way, similar as ACT, as it is stated).\n\nSuch ideas are not new. This has been proposed multiple times, maybe the first time by Schmidhuber 1992 (also mentioned by the paper). This specific variation, and also the dynamic meta level is somewhat new. The novelty is not too much a problem though, as this is still interesting to study further, as there are not too many successful reports on this idea in the literature so far.\n\nThe main problem with this work is the quality of the paper, the experiments, and the analysis. It has many small errors/mistakes which make it hard to read and understand. But also, the analysis is way too shallow, and also the experiments don't really give much interesting insights.\n\nRandom comments and questions:\n\n- The paper looks a bit short, although there is lots of room for further analysis.\n- Where can we find the source code?\n- You define alpha/beta globally for one X-RNN layer, right? You could also have it separately per depth index $n$, i.e. like $\\alpha_t^n$.\n- $h^n_t$ is defined twice, once as $o_t \\cdot c_t$, another time as $h_t + x_t$. Which one is it?\n- I think the values of $h^n_t = o_t \\cdot c_t$ can only be within $[-1,1]$, right? Because $o_t$ is a sigmoid, $c_t$ is a weighted sum of sigmoid and tanh. As $h^n_t$ is used as the weights for depth index $n-1$, it means that the weights are always in such a limited range. Is this restriction a problem?\n- The max. depth is defined as $L$. But in \"Connections to other recurrent models\", max depth is $n$? $n$ was used as the depth index before. This is confusing. Don't reuse the same variable for multiple things.\n- In task 1/2, this is a encoder-decoder (seq2seq) model, without attention? Does this make sense? Without a mechanism like attention, you only have a fixed-sized vector to represent the whole input sequence. This obviously cannot generalize, and cannot work for longer sequences. This is well known.\n- In table 1, what is EM and P? I assume from the text EM = exact match accuracy, and P = perplexity? This should be made clear in the caption.\n- In table 1, what is $n$? This is not the max meta depth (earlier $n$, which was $L$ before), right? Is this what is described in the text under \"Task 2\" as $N$, for the max depth of the constructed tree? The text in \"Task 2\" also mentions both $N$ and $n$. Is this a different variable? This is very confusing. I assume the max tree depth $N$ is meant, but this should be made much clearer, and should use a different variable name. Or use a subscript $N_{\\text{tree}}$ or so.\n- In table 1, with increased max tree depth $N$, it becomes extremely bad (accuracy only ~3%).  As mentioned before, an encoder-decoder model without attention cannot handle this, as it becomes clear here. I'm not sure if you can interpret anything out of these numbers when they are so bad.\n- In table 8, the $N$ is what $L$ was before? This is again confusing.\n- How can the model get worse with higher max $L$? The model should automatically learn to use alpha/beta properly, or not?\n- Similarly, how can it be worse with a LSTM instead of linear base unit? Maybe this is overfitting? Or what is failing there? This should not happen.\n- In figure 5, what is the left/right gate? Is left=alpha, right=beta? Why so confusing? Also, what exactly does the value actually represent? Is this the average over a single sequence? Over the whole dataset? How much variance does it have? Is it actually used in a meaningful way by the model?\n- In figure 6-8, what is LL,LR,etc in the figure? This is unclear.\n- Figure 7 and 8 are not mentioned at all in the text.\n- Increasing $L$ means also increasing parameters, and somewhat similar could be to increase the number of layers instead. Maybe this is not really due to the increased meta depth, but just more parameters and thus more modelling power, and the same could be achieved by more layers?\n- The X-RNN model could in a way also be interpret as a variant of a multiplicative LSTM (mLSTM), because when you construct the weights, effectively this means that you do multiplication. This could be studied and compared in more depth.\n- There could be more systematic comparisons to other recurrent models. The connection is already shown to SRU or LSTM. There could be systematic experiments where you modify it step by step from a LSTM into a X-RNN, to understand and study the effect of each individual components and aspects.\n\nPros:\n\n- Interesting idea, interesting area to perform studies on.\n\nCons:\n\n- Many small mistakes and errors, which makes it difficult to understand.\n- The experiments don't provide much insights.\n- Overall, the analysis is way too short. The analysis doesn't really show whether the model uses the dynamic depth in a meaningful way. The analysis does not give too much insights into the model.\n\nSummary:\n\nThis needs further work, on the mentioned flaws. Most importantly, the analysis section need to go much deeper, and the experiments should be designed to deliver more insights.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Another complex but performant variation of recurrent cells",
            "review": "This paper presents a recurrent cell in which the gates are recursively controlled by other instances of the cell itself and by auxiliary (recurrent) networks. The authors show how it can be parameterised and how special parameterisations make it equivalent to existing recurrent cells. Experiments on logic tasks such as sorting or tree traversal and real world tasks like machine translation are presented and good results are achieved with less trainable parameters than other models. \n\nOverall, this paper is well written and easy to read. The idea is interesting although it seems like another variation of previously proposed methods. The related work section and relation to other recurrent cells are nice to read. \n\nThe cell is clearly presented in Sec. 3 but would greatly benefit from a diagram of the proposed cell for readability. Yet equation 4 is a bit confusing as one does not see how $h_t$ is computed, and there are two distinct formulas for the computation of $h^n_t$. Also, it looks like the final output is that of $X-RNN_0$ but it is not clear in the text.\n\nThe experiment section (Sec. 4) presents a list of experiments, which are numerous but contain little analysis. The design choices among all variations presented in Sec. 3 are not always clear, and not compared to each other, which would add a great value to the paper and maybe help understand the contribution and usefulness of the different choices, beyond the promising results. \n\nThe analysis section is a bit light and harder to understand. It is for example not clear in 4.6.2 what the left and right gates are and what this analysis is measuring to trying to show.\n\nWithout a stronger theoretical or empirical analysis, or a measure of the impact of the different choices, for example with an ablation study, and despite the interesting idea and good results, this paper is interesting but yet another complex variation of recurrent cells and slightly below ICLR standards.\n\n\nMinor comments:\n  - X-RNN in the abstract is not defined.\n  - the tables are sometimes inside the text, making the caption hard to read.\n  - in 3.1: $F_*(xt)$ should be $F_*(x_t)$\n  - EM, P in Table I are only defined and explained in the next page\n  - a runtime analysis of the proposed cell would be interesting\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}