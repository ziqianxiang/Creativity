{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper proposes two approaches of input recreation (image inpainting and image super-resolution) to increase the robustness over adversarial attack.  ",
            "review": "This paper presents leveraging two input recreation approaches, i.e., inpainting, and super-resolution, to enhance the model robustness against adversarial examples. Human perception systems inspire authors this idea, and they demonstrate its usefulness with a fair amount of experiments. However, the examplar content reconstruction methods are from previous works, and I feel questionable about the proposed input recreation methods' generability to real-world data. Meanwhile, there are several other content reconstruction approaches, like deblurring, denoising. Why only discuss SR and inpainting, and how do we know what kind of reconstruction approach is the best for a specific type of attack? \n\nIn addition to the choice of content reconstruction, I feel doubtful about your experimental setting. For instance, the training data for image super-resolution obtained by adding noise and bi-linear downsampling is trivial. Doing such ways will bias your reconstruction model to this simple type of degradation. Thus, I am afraid that feeding other types of data will make your model work or not, like different noise distribution or different levels of blurring or degradations. Simply put, I guess your final classification performance will rely on the input reconstruction result. Unfortunately, the input reconstruction methods will face difficulty when there is a gap between your training and testing datasets.  \n\nOverall, I think this novelty level of concatenating SR/Inpainting inside adversarial training is incremental. The generability of such kind of content reconstruction to real-world data, i.e., making SR works good for a different type of degraded input, is questionable. More importantly, the use case of different kinds of content reconstruction methods is unclear. And I feel this paper is more like a case study since the experimental settings are too narrow to specific data types; thus, the claim is not convincing.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results, while the big picture is unclear",
            "review": "The author proposed a type of network architectures that are designed for better defenses against adversarial attacks, called $\\varphi$DNN.\nThe idea is to insert a reconstruction layer to recover the corrupt input image as the new input image. They use super-resolution GANs and downscaling operations as one type of corruption/reconstruction, and auto-encoders for pinpointing input images with missing holes. These two techniques corresponds to two architecture augmentation, called FIR and NSR. They experimented on CIFAR/SVHN/ImageNet and show that such augmentation can improve the robustness of a deep classifier.\n\nThe positive factor is that the insertion of the corruption/reconstruction component indeed improves the robustness of a backbone network, as shown in Table 1. \n\nAfter reading this submission, this reviewer has several questions that remain unanswered.\n\n1. The hypothesis is that humans are less prone to adversarial examples because humans have blind spots and can reconstruct the image. This leads to the design of input re-creation. However, a more reasonable explanation is that this input recreation can serve as a filter to ignore the adversarial perturbations. For example, the ablation methods in Table 2 can be considered as input corruption (no re-creation) and they can also improve the robustness, but the reason is not human eyes also do JPEG compression.\n2. Human eyes may do input re-creation, but humans have no difficulties performing classification. But the test accuracies of NSR/FIR architectures are very low.\n3. After adding the corruption/reconstruction component, the test accuracy becomes worse. If we simply prune a DNN, it will also be more robust to adversarial attacks, meanwhile achieving a lower test accuracy. Does this mean pruning DNNs is comparable to the proposed method?\n4. Why view the input re-creation network and the task network as a whole? Similar to 1, these two parts can be completely decoupled and trained on different data distributions. Combining them all together and call them $\\varphi$DNN seems a bit weird.\n5. For NSR analysis, if we think from the perspective of robust training, it is natural to expect NSR outperforms SR. When we add noises, we essentially require the network to perform well in a small neighborhood, thus increasing the robustness.\n\nOverall, while this paper provides good results (Table 1), this reviewer has doubts about the motivation and the hypothesis. In my view, input re-creation is the same as all other methods that try to filter adversarial signals, such as the methods in Table 2. Meanwhile, the results in Table 1 and Table 2 show that their method can achieve better robustness compared to commonly used baseline methods.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good idea, interesting paper, but the experiment protocol could improve",
            "review": "### **Summary**:\n \nThis work proposes, perceptual DNNs, a new defense against adversarial attacks. The idea behind perceptual DNNs is to control the input to the network by letting the system to recreate (reconstruct) the input before feeding it to the classifier. The benefits of this concept are introduced via two implementations, one based on image inpainting, and a second one based on superresolution using a GAN. This approach can be combined with adversarial training, leading to additional robustness to both white-box and black-box attacks. The method achieves good results in CIFAR-10, SVHN, and Imagenette datasets, against FScattering and simple adversarial training.\n \n---\n \n### **Reasons for score**: \n \nI feel divided about this work in this current state, the reason why I am scoring it with a “**marginally above acceptance threshold**”, but I am trying to define it as “**borderline**”. The topic is clearly relevant, and the idea is well formulated, simple, and easy to reproduce. From the results, it seems to be effective, but I have concerns about the structure of section 4 and the experiments therein. I will elaborate on these concerns in the next sections.\n \n---\n### **Strengths**:\n \n* I think the idea presented in this paper is interesting, and it is worth exploring.\n* The proposed approach is quite generic and easy to reproduce.\n* The proposed approach significantly outperforms the baselines in CIFAR-10, SVHN, and Imagenette.\n* The paper is well written and to the point.\n---\n\n### **Weaknesses**:\n\nI see some issues in the structure of section 4 and the decisions made to evaluate the approach:\n* The experiment protocol is subsampling the test set. It seems that not all the images of the test set are used. Is this the case? If so, this makes a comparison with other approaches very complicated (for those papers coming after yours). I think the paper needs more clarity on this issue and probably a fix in order to keep experiment protocols as standard as possible.\n\n* I think it would help to see the results in terms of model accuracy instead of attack accuracy. How much does the classification accuracy go down in each case? This can’t be perceived using the attack accuracy metric. In the end, what the approach is trying to achieve is to improve resilience to attacks to keep the classifier operative, working within a high-accuracy regimen, so it makes sense to evaluate based on the accuracy of the final task.\n\n* I think Imagenette is typically intended to do quick testing. You already have results on CIFAR-10 and SVHN. I think the next logical step is to use ImageNet to test your approach, but I don’t see the need for Imagenette in this context. Presenting good results in ImageNet would make your “selling point” much stronger.\n\n* I believe this paper may benefit from the following baseline: [Adversarial Robustness through Local Linearization](http://papers.neurips.cc/paper/9534-adversarial-robustness-through-local-linearization.pdf). It is one of the top approaches for adversarial defenses, along with FScattering. In fact, it is hard to tell which one produces the best results. I think it would be a good addition to the baselines.\n\n\n---\n\n### **Questions to be discussed during the rebuttal period**:\n\nI would like the authors to discuss the concerns presented in the **Weaknesses section**.\n\n---\n\n### **Other considerations**:\n\nNone.\n\n---\n\n### **References**:\n\n1. [Adversarial Robustness through Local Linearization](http://papers.neurips.cc/paper/9534-adversarial-robustness-through-local-linearization.pdf)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}