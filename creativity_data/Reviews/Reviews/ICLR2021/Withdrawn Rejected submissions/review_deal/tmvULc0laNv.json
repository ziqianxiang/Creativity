{
    "Decision": "",
    "Reviews": [
        {
            "title": "Optimizing latent space directly using ODE solvers",
            "review": "This paper proposes a model that finds a latent space for data using \"just a decoder,\" using and ODE that optimizes the latent representation that decodes to a particular datapoint. They also propose a 2nd order \"accelerated\" gradient flow to solve this. I think the main idea is interesting, and potentially valuable, but the paper is poorly, and the results do not seem particularly convincing. \n\nFirst this work needs to be put in proper context. A \"decoder without an encoder\" is exactly what the GAN generator is. However it is penalized using a discriminator which effectively checks if the decoding is part of a distribution rather than trying to recreate each particular point. Furthermore, recent papers in the GAN world have tried to optimize the latent space of GANs for the purpose of improved generation. One example is:  Wu et al, \"Logan: Latent optimisation for generative adversarial networks,\" 2019.  \n\nHowever these works use gradient descent, whereas the authors use an ODE solver. They don't explain the intuition for this at all, but the idea is that each particular point in the data space needs to have an optimized embedded point, so each training point would need several gradient descent iterations. Thus finding the fixed point of the gradient as a differential equation may be a good idea.\n\nThe remainder of the paper seems to largely focus on improvements to the ODE solving paradigm. The results of this speed are shown in Figure 4. However, throughout the results section it is extremely confusing to me why there is a cross-entropy loss. If the purpose is reconstruction shouldn't it be a reconstruction loss?\n\nFigures 6 and 7 showing the tSNE embeddings of the latent space don't show any clear improvement of the latent space. What is better about the latent space? This is not discussed even in the motivation or introduction. For instance, VAEs attempt a latent space where points are embedded contiguously for interpolation. What is the feature of this latent space that would be better? \n\nA possibility here is to see if the latent space preserves the data manifold better. For this I would suggest visualization using PHATE (Moon et al. Nature Biotechnology 2019) and the associated DeMAP (denoised manifold-affinity preservation) metric. \n\nOther confusing parts of the result:\n\nWhy is z initialized to 0 instead of a Gaussian or more entropic distribution? \n\nThe authors state that,  \"For training with MNIST and FashionMNIST datasets we implement a sequential linear neural network.\" Why would they use a linear network?  Then the authors say, \"The decoder network architecture corresponds to four gradually increasing linear layers with ELU non-linearities in-between.\" This would make the network non-linear. \n\nAdditionally there are many typos and grammatical problems : \n“ The authors replace the auto-encoder network with an auto-decoder where, a similar  to here, latent” (missing word)\n“ this approximate inversion requires to be”  (ungrammatical) \n“ learning with fewer data” (plural/singular disagreement)\n“ signed Distance Function.” (capitalization)\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Gradient Flow Encoding with Distance Optimization Adaptive Step Size",
            "review": "###################################################################\n\nSummary:\n\nThis paper proposes a new generative model that only has the decoder and uses gradient flow to find the latent representation of data samples via solving an ODE equation. The new model is named the Gradient Flow Encoding (GFE) model.  Furthermore, in order to improve the convergence, the paper extends the GFE to use the 2nd-order ODE which corresponds to the momentum Nesterov’s accelerated gradient descent for inferring the latent representation of data samples. In the paper, the authors also develop an adaptive solver for the gradient flow, which prioritizes minimizing the loss at each integration step. Experiments on MNIST, SegmentedMNIST, and FashionMNIST are provided to validate the advantage of the proposed GFE model over the autoencoder (AE).\n\n###################################################################\n\nReason for the Score:\n\nOverall, I vote for rejecting. I have two main concerns. My first concern is the significant lack of experiments to justify the advantage of the proposed GFE model. My second concern is that the proposed GFE model shows very limited or even no advantage over the baseline AE model in terms of cross-entropy loss when training with all MNIST images. In the meantime, the GFE model is very inefficient and much slower than the AE model. \n\n###################################################################\n\nStrong points:\n\n1. The idea of replacing the encoder in AE by solving an ODE to find the latent representation of data samples is interesting. In addition, the use of a second-order differential equation that approximates Nesterov’s accelerated gradient method to speed up the encoding process is promising and should be explored more. \n\n2. The proposed adaptive minimize distance (AMD) solver is practical and helps speed up the solver when the solver is used as part of a deep neural network. \n\n3. The GFE yields better results than the AE when training with a small amount of MNIST training images.\n\n###################################################################\n\nWeak points:\n\n1. The paper only compares GFE with the baseline AE on simple MNIST-related tasks. This is not enough to validate the advantage of the GFE. Comparison with other popular generative models such as VAEs and flow-based models and on more complex datasets such as CIFAR are needed. \n\n2. The GFE model is much more inefficient compared to the baseline AE.  When training with all MNIST images, the GFE yields similar results as the AE but is much slower than the AE.\n\n3. Error bars are needed for the reported results.\n\n4. Image generation results are not provided. In the paper, the authors only show results for image reconstruction. The AE can generate images by sending an input latent code into the decoder. The generated images from the GFE are needed to show its promise as a generative model.\n  \t\t\n###################################################################\n\nAdditional Concerns and Questions for the Authors:\n\n1. Test cross-entropy loss for training with a small amount of training data as in Table 1 should also be shown for FashionMNIST and SegmentedMNIST. Given good-looking reconstructed images for the trained AE that only sees 1% of FashionMNIST in figure 5a, the test cross-entropy losses of the GFE might not be significantly better than those of the AE on this task.\n\n2. Test-set reconstruction for trained AE that only see 1% of MNIST data in figure 5a looks suspicious. I think better training hyperparamters such as changing the learning rate might help improve the reconstruction.\n\n###################################################################\n\nMinor Comments that did not Impact the Score:\n\n1. The curve in figure 3 (left) does not seem to converge yet. For such an easy task as MNIST, the small gap in cross-entropy loss between GFE-approximate and GFE-full adjoint might be important. \n\n2. Is the GFE-amd in figure 3 (right) for the 1st or 2nd-order method? If it is for the 1st order method, how about the results for the GFE-amd for the 2nd-order method?\n\n3. There are many typos in the paper. Below are some of them.\n\t\n- Near the end of page 3, “A second-order differential equation” instead of “A second differential equation”.\n- In section 3.3, “they can be used” instead of “they can be use”.\n- Right before figure 3, “this result is robust to O of the experiment”. What is O?\n- FashionMNIST and FMNIST are used inconsistently. The authors should use only FashionMNIST or FMNIST to name that dataset in the paper.\n- The acronym GFE is not defined in the abstract. \t\t",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Iterative optimization of latent representation is an old idea",
            "review": "Summary\n-------------\nThis paper proposes to get rid of the encoder of autoencoders by optimizing the latent representation directly. To find the optimal latent state the authors test gradient descent (\"GFE\"), Nesterov momentum (\"GFE-2nd order\"), and gradient descent with line search (\"GFE-amd\"). Empirical results are reported on a linear autoencoder trained on MNIST, FashionMNIST, and SegmentedMNIST.\n\nOriginality (2/10)\n------------------------\nOptimizing the input to a decoder directly/iteratively is not a new idea. E.g., see Kim et al. (2018) or Yang et al. (2020) for some recent examples combining encoders with iterative inference in variational autoencoders (VAEs). In fact, the whole idea of VAEs was to get rid of iterative inference (see also _\"amortized inference\"_). Before VAEs, inference by iterative optimization was the norm. E.g., Olshausen & Field (1996) directly optimized the latent representation of a linear model/decoder.\n\nQuality (2/10)\n------------------\nThe quality of this paper appears low to me mainly due to a lack of awareness of related work. A discussion of related work is virtually absent from the paper.\n\nCouldn't the approximation in Eq. 6 be motivated by noting that _dL/dz = 0_ when evaluated at a local optimum _z*_? What's the motivation for using ODEs?\n\nClarity (3/10)\n-------------------\nThe authors spend several pages explaining the three different approaches using ordinary differential equations. It seems to me that this could have been condensed to a paragraph or two using simpler language while not losing any insights (e.g., see summary above). As the authors observe themselves, _\"the exact path of the gradient flow may be less important than the final point of convergence.\"_ So why not just treat the problem as an optimization problem instead of using ODEs?\n\nThe paper is riddled with typos and its grammar could be improved.\n\nIn Equation 8, should it be _D(z + av, theta)_ instead of _D(z, theta)_ for some _a_? Otherwise, isn't it just _momentum_ instead of _Nesterov momentum_?\n\nThe experimental section evaluates the autoencoder in terms of cross-entropy but it isn't clear to me which cross-entropy was used (I am assuming MSE).\n\nSignificance (2/10)\n-------------------------\nThe significance of this paper is limited by a lack of conceptual contributions and empirical insights. The experiments are limited to a toy model on toy datasets. The only interesting observation is that iterative optimization performs significantly better when the training set is small. Perhaps if this effect had been explored much more thoroughly (e.g., differently sized encoders) the paper would have been interesting.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Decoder only model for learning latent space of data",
            "review": "**Paper summary**\nThis paper presents a method for learning a latent space for data using only a decoder (and no encoder as would typically be the case with autoencoders). In order to achieve this, the authors propose a method where, for a fixed decoder D (mapping latents z to data x), the representation z* of the datapoint x is the one that minimizes $||D(z) - x||_2$. They encode this minimization problem as an ODE and let the representation z* be the solution of an ODE at some time tau.\n\nTo train the model, the authors optimize the weights of the decoder by minimizing the difference $||D(z*) - x||_2$ over a set of datapoints (e.g. MNIST images). As z* is the solution of an ODE, the authors use the adjoint method (as presented in the neural ODEs paper) to compute the gradients with respect to the weights of the decoder. The authors then claim that some of the terms in the adjoint method can be ignored for the purposes of their algorithm.\n\nThey then investigate using an equivalent of momentum based gradient descent (again encoded as an ODE) to more efficiently train the model. Finally they propose using another solver with an adaptive step size to train the model, which they find works better in practice.\n\nThe authors test their model on MNIST+FashionMNIST reconstruction tasks and find that the model can better decode MNIST+FashionMNIST test images with fewer datapoints than a regular autoencoder.\n\n**Positives**\n- The idea of using a decoder only framework for learning latent spaces of data is interesting.\n- The data efficiency experiments are quite nice.\n\n**Negatives**\n- The presented method and model is extremely confusing and not well motivated. There are also several justifications for the methods that are questionable. Firstly, it is not clear why you would need to solve an ODE instead of just performing a fixed number of gradient descent steps to find an approximate z*. Further, it is not at all clear that the solution of the given ODE corresponds to solving the minimization problem. For example where does the $\\alpha(t)$ term come from? The authors say that they want the ODE to reach a steady state (indeed if $\\frac{dz}{dt}=0$, then $\\nabla_z D$ is zero). However, it is not clear why solving the ODE to a time $\\tau$ would lead to a steady state solution. Further, even if it does reach a steady state solution, it doesn’t find the z* as mentioned in the paper but rather some local minimum of D. It is not clear why the authors don’t just perform some steps of gradient descent instead of using this fairly convoluted method without justification. In addition, when using the adjoint method, the authors then ignore the actual adjoint function and justify this as ignoring “higher order terms”. However, these terms are not higher order, they are first order terms that come from computing the total derivative of a function that depends on two variables. It does not make sense to call these higher order terms (and it does not make sense to ignore them). The authors argue for using a momentum based ODE as well as it converges faster. However again in this case so many steps are confusing and not well motivated. The authors choose the function $\\alpha(t)$ out of nowhere and then say that using a regular adaptive or fixed step solver does not work, leading them to use a logarithmic series for the step size. However, it is not clear what logarithmic series means in this case and the whole approach feels a little hacky and not well motivated. Finally the authors use a method they term adaptive minimizing distance (AMD) solver which ignores the ODE and instead focuses on the final step of the ODE only. However, this basically seems like the authors are saying that an ODE is not needed which puts into question large parts of the method. The AMD method description is also very confusing. In general the method seems to be composed of a lot of “hacks” which are not well motivated and are very confusing for the reader.\n- The experimental results are weak. The method is only tested on MNIST and FashionMNIST (and even then only single run results without means and standard deviations). Further, on these datasets, the proposed algorithm learns slower in wall clock time than a regular MLP autoencoder. On the test set (when training on the full training set) the results for autoencoder and the proposed method are the same within statistical error. The only advantage the model has is when using fewer training points. However, this is not a fair comparison, as the model requires optimization at test time to perform inference (whereas autoencoders have an encoder for this purpose), which makes it easier for the model to adapt to new samples.\n- The entire paper seems very rushed. The script is riddled with typos (e.g. repeated words, missing punctuation, misspellings etc). I would recommend reading over the paper again to fix this.\n- The paper misses huge amounts of related work. While the authors mention the DeepSDF there are very many follow up papers that use a similar decoder only framework. Further, the gradient origin networks paper also uses the gradient as an encoding and is not mentioned either. The auto decoder framework was also proposed before (e.g. in the variational auto decoder paper). These should all be cited.\n\n**Recommendation**\nWhile learning a latent space for data with a decoder only framework is interesting, the paper has severe methodological and clarity issues. Further the experimental section and results are weak. I therefore recommend rejecting this paper.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}