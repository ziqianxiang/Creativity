{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper argues that a successful backdoor attack on classifiers is connected with further fundamental security issues. In particular they demonstrate and not only an original backdoor trigger but also other triggers can be inserted by anyone with access to the classifiers. Furthermore, the alternative triggers may appear very different from the original triggers, which confirms the claim in the paper's title that such classifiers are \"fundamentally broken\".\n\nThe paper offers an interesting insight into the features of poisoned classifiers. However, such insight is diminished by the fact that the proposed attack requires a substantial manual interaction. The user must manually analyze the adversarial examples generated for robustified classifiers in order to determine the key parameters of alternative triggers. While manual intervention as such does not undermine the main observation of the paper, this makes an automatic exploitation of this idea hardly feasible and hence decreases the significance of the paper's main result. "
    },
    "Reviews": [
        {
            "title": "introducing backdoor triggers to classifiers makes them vulnerable to alternative attacks, not just those intended via the original trigger. ",
            "review": "## Summary ##\nSuppose that we wish to have a classifier that works well under normal circumstances, but fails when we want it to. One way to do so is via data poisoning attacks: introduce a special datapoint to the training data, such that if the classifier observes it, all of its labels become completely broken. \n\nThe basic premise of the paper is that poisoned classifiers are broken in a fundamental way - not only are they vulnerable to attacks based on the original trigger image, they are also vulnerable to attacks by adversaries who do not know the original trigger. This is interesting for both the learning and the privacy communities, as it shows that backdoor attacks introduce a host of vulnerabilities far beyond that which was assumed. \n\nTo test this hypothesis, the authors show that one can design novel triggers on poisoned classifiers, that, in some cases, outperform the attacks based upon the original trigger. \n\nSimilar avenues do not work as effectively on “clean” classifiers, where no triggers have been introduced.\n\nThe authors show that their methodology is effective on a variety of datasets; in fact, the authors show that users are better able to distinguish poisoned classifiers even without the backdoor present. \n\nOverall I liked the authors’ approach, and the paper was a pleasure to read. The authors make clear, concise and refutable claims, and proceed to analyze them in a rigorous manner.  \n\n## Pros ##\n1. The paper tackles an interesting and well-motivated problem, and shows that a single line of attack is much more worrisome than previously thought.\n2. The paper is well written, all claims are easy to follow.\n3. I like the fact that the authors incorporated user studies into the evaluation.\n\n## Cons ## \n1. It is not fully clear whether the approach extends beyond theoretical interest and curiosity. Is there a clear and immediate implication for how we train or protect our ML models?\n2. The paper provides a heuristic approach that may or may not be generalized beyond the datasets that were tested. There is no analysis showing why the approach makes sense.\n\n## Questions to the Authors ##\n1. How does this line of work relate to formal privacy notions? The authors mention a few similar modes of attack, but am I correct in assuming that differentially private training methods are immune to such attacks?\n2. If we use robust training methods, does this issue go away?\n\n## After Rebuttal ##\n\nI thank the authors for their response to my question. I think that the comment regarding differential privacy being ineffective is particularly interesting. It would be nice to actually demonstrate this empirically - construct a DP classifier with a poisoned backdoor, and show that the authors' method is still effective. \n\nMy support for the paper remains unchanged. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting study,  some concerns regarding the method and experiments ",
            "review": "Summary:\nThis is an interesting study on the analysis of poisoned classifiers and backdoor attacks. The authors showed that with some post-processing analysis on a poisoned classifier, it is possible to construct effective alternative triggers against a backdoor classifier. In particular, after creating several poisoned classifiers, and smoothing them using a Denoised smoothing technique, one can generate adversarial examples. Using these examples, it is possible to extract color or cropped patch as new triggers to break the poisoned classifier.\n\nReason for score:\nThe process of generating alternative triggers and finding effective triggers for each backdoor attacker is mainly manual and needs human intervention. This makes the proposed solution very challenging. Also, the experimental results do not clearly show the generalizability of the model, especially on more difficult backdoor attacks. I think more experiments need to be done to evaluate the consistency of the results and to study the generalizability of this work across various datasets.\n\n\nMore detailed comments:\n-         How the adversarial examples of robustified poisoned classifiers look like when we have invisible backdoor attacks? This is important since the trigger generation is directly related to the backdoor pattern of the generated adversarial examples. Did the authors investigate this matter?\n-         Does changing the location or appearance of the original trigger affect the backdoor patterns in adversarial examples?\n-         In the experiment section, it would be better if the authors showed both color patch and cropped patch results on fixed images and shows which technique has a higher success rate.\n-         In the experiment section, the authors only showed two samples to prove that clean classifiers are not easily broken. Please show the overall success rate for clean classifiers on both datasets as well.\n-         It would be better if the authors also showed the results on larger datasets with more number of classes.\n-         I would recommend to open source the code.\n\nMinor comments:\n-         In section 3, please briefly explain how the poisoned classifier is trained using the two triggers (it was not clear before reading the experiment section)\n-         For table 1, please mention the number of samples used to calculate the success rate. The authors mentioned the highest success rate is picked for different triggers. It would be great to see how each trigger performs separately regarding the success rate.\n-         Some references are missing, please add more recent papers. For example:\nRethinking the Trigger of Backdoor Attack\nInvisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization\netc.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "It is an interesting finding that backdoor-poisoned models are also vulnerable to alternative \"triggers\", although the reason behind it is unknown",
            "review": "Summary:\nThis paper demonstrates that backdoor-poisoned machine learning models can also be vulnerable to alternative triggers. Specifically, adversarial samples that are generated against models robustified with Denoised Smoothing often show backdoor patterns. Therefore, these adversarial samples can be used to create new triggers either by (1) choosing a representative colour from a backdoor pattern, or (2) cropping an image that contains a backdoor pattern. Experimental results suggest that alternative triggers can be equally or even more effective than the original trigger.\n\nPros:\n1. This paper studies an important question of the vulnerability of backdoor-poisoned machine learning models.\n2. Two methods are proposed to generate alternative triggers that can cause the poisoned machine learning model to misbehave.\n\nCons:\nWhile it is a novel finding that poisoned machine learning models are not only vulnerable to the initial triggers, I have the following questions on the proposed method:\n\n1. The biggest concern is that as pointed out in Section 3.3, both the two proposed strategies for generating alternative triggers require human inspection. Can these triggers be generated automatically? How easy is it for an algorithm to identify the backdoor pattern, and then to crop a patch image that contains the pattern?\n2. What could be the potential reason why the proposed approaches are effective for generating alternative triggers? What could be the relation between Denoised Smoothing and the backdoor pattern? Could there be other ways to create triggers? There is a lack of discussion on this issue.\n3. In terms of parameters, the perturbation size \\epsilon is set to 20 and 60 (in l2 norm) in the experiments. What is the impact of \\epsilon on the attack success rate? Specifically, with a smaller value of \\epsilon, can the attack still achieve such high success rate?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A expermental report but not a research paper",
            "review": "This submission just describes some phenomenons in a strange setting but not proposes any valuable questions.  The authors claim that \"anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time.\" However, this paper creates the so-called trigger from the poisoned calssifier. Such a choice is unsual in the adversarial machine larening problem. In most cases, this model is not occupied by the adversary. The authors did not provide some convicing reasons to verify the rationality of this setting. Moreover, the proposed method constructs the alternative triggers by first generating adversarial examples for a smoothed version of the poisoned classifier and then extracting colors or cropped portions of adversarial images. But the motivations of building the  adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}