{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper clearly has great ideas and reviewers appreciated that. However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue. We don't know if the reported accuracy is a respectable one (in the public domain).   Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility. \nWe recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non-trivial.  "
    },
    "Reviews": [
        {
            "title": "Summary: This paper presents a technique to generate formulas given table context in spreadsheet (even though I am not very clear about motivation and usecase). Authors propose a BERT based model to learn dependencies  between table rows and columns.",
            "review": "The motivation for the work is not very clear to me. The paper is difficult to follow especially the motivation, problem setup and architecture part. The dataset is not publicly available for everyone to use to the idea is not reproducible. Authors write in the paper that dataset is publicly available in their organization, I don’t understand what do they mean by that. Comparison with state of the art neural symbolic machine/ function generators is missing. I didn’t find the baselines to be strong enough. \n\nOverall:  I found this paper to be hard to follow. The motivation for the task of predicting formulas given table is not very clear. It would have been better if authors could explain a real world example application for their task. Figure-1 is not clear, it needs some more intuition and discussion. A running example would have really helped. I found introduction to be a bit disconnected.\n\nQuestion: why authors have not reported results for other related tasks like taskfill using their technique? Can you please explain motivation/real world application for the task and technique in details? How different their taks is from semantic parsing for table question answering?\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel problem formulation",
            "review": "**Summary:**\n\nThis paper presents an interesting formulation for spreadsheet formula synthesis. Instead of taking the input output pairs as input, as is done in the programming by example (PBE) approaches, the proposed approach takes the semi-structured tabular context as input for predicting a formula for the target cell. A neural network architecture is presented which uses a BERT-based encoder to leverage the natural language meta-data.\n\n**Strengths:**\n\n* The paper is clear and well written. Problem formulation, model architecture and evaluation setup are presented in sufficient details.\n* I found the program formulation quite interesting. For synthesizing a spreadsheet formula, instead of taking the input output pairs as input, the paper proposes to take the tabular context as input (without an explicit specification) . This is much closer to the real world application where spreadsheets are typically semi-structured and contain rich metadata. \n* Supports the full  “Google Sheets Language” with rectangular ranges. This is much more complex than Flashfill and brings many challenges which the paper does well to address.\n* The proposed approach out-performs the neural network approaches for programming by examples. It achieves reasonable accuracy on a large-scale benchmark to be usable in practice.\n* Evaluation and ablation studies are quite thorough. In-depth analysis of results gives insight into the contribution of various architectural choices towards the overall result. The results are inline with the design rationale.\n* The work is well situated and relation to the related work is discussed in sufficient details.\n\n**Weaknesses:**\n\n* I find the problem formulation interesting but would rate the overall approach as medium on novelty. The problem formulation is very specific to the task at hand and has a limited scope of being applicable to outside it (To be fair, the paper doesn’t claim so.) Nothing to take away, however, from the flawless execution. \n* I couldn’t find any glaring weakness but here are a few questions/suggestions.\n* Although the formulation takes into account the rich metadata of the semi-structured spreadsheets, it misses out on certain inputs that would have been easier for the user to provide. For example, users can provide the ranges as input. I believe this would improve the accuracy further since the sketch prediction accuracy is already higher. Have the authors tried something similar?\n* It would help to add a few qualitative examples of synthesized formulas in the main paper to get the feel of the results.\n* It is not clear to me if the output of the model/beam search always results in a valid program. It would also like to know the wall-clock time required for synthesis of top-5 formulas.\n* Does the model come up with formulas that are semantically equivalent but syntactically different from the ground truth? How do you handle this in the evaluation?\n* It is not clear to me if you handle formulas involving absolute as well as relative ranges. Are the absolute ranges part of the sketch in such cases?\n\n**Overall Remark**\n\nI believe that overall problem formulation and the architecture would be valuable to the community considering that the proposed system supports the full “Google Sheet Language” and performs reasonably well to be used in practice. The spreadsheet formula synthesis is an important domain with a wide audience. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "### Paper summary\nThis paper addresses the problem of inferring spreadsheet formula from surrounding cell values and headers (i.e. brief description of each column). Compared to the standard programming-by-example line of work, which concerns synthesizing a program from a set of independent input/output examples, the problem setup proposed in this paper is more realistic and allows a model to synthesize a program by incorporating the contextual information. To this end, the paper proposes a framework that is specially designed to encode the header and the data from multiple rows and columns. To alleviate the difficulty of decoding a program from scratch, it proposes to first produce a program sketch that consists of formula operators and literals, and then predict the relative range which specifies where the formula operators should be applied. The experiments compare the performance of baselines as well as the proposed framework and its variations. The experimental results show that the proposed framework outperforms the rest and justify many design choices (e.g. leveraging a pre-trained BERT model, predicting a program sketch first, encoding rows and columns independently, etc.) I believe this work proposes an interesting and promising research problem as well as a framework that can achieve reasonable performance. Therefore, I vote for acceptance.\n\n### Paper strengths\n**motivation & problem setup**\nThe motivation for incorporating contextual information for programming by example is convincing. I believe this will make the neural program synthesis line of work more applicable to real-world problems.\n\n**novelty & technical contribution**\n- As far as I am concerned, the problem setup is novel, since it includes header information, does not consider different rows independently, predicts a formula/program in a specified target cell, and allows to reference multiple cells.\n- Leveraging a pre-trained BERT model seems crucial (~10% performance gap).\n- Encoding row and column information separately and merging them with convolutions is compelling.\n- Predicting a program sketch first to alleviate the difficulty of sequentially producing each token of a program is convincing. This paper presents an effective way to implement this idea.\n\n**clarity**\nThe writing is very clear and the figures illustrate the ideas well. Also, the organization of the paper is easy to follow.\n\n**ablation study**\nAblation studies are comprehensive. The proposed framework consists of multiple components. The provided ablation studies help analyze the effectiveness of each of them, including\n- using a row BERT encoder only / using a column BERT encoder only / removing convolution layers that merge BERT's output\n- decoding a program sketch and its range altogether (without the proposed two-stage decoding)\n- using a BERT model that is not pre-trained on text corpora\n\n**experimental result**\nThe presentation of the experimental results is very clear. The authors present insightful information, which includes:\n- top-n accuracy\n- performance vs. different program sketch lengths\n- sketch only accuracy and range only accuracy\nThe analysis of the experiment results is detailed and insightful.\n\n### Paper weaknesses\n**formula accuracy**\nIf I understand correctly, it is possible to synthesize a formula differently from how the ground truth formula is written. If this is the case, I wonder how formula accuracy is computed as one would need to enumerate all the possible ways of writing a particular formula to do so.\n\n**pre-trained BERT for headers only**\nGiven the sampled shown in the paper, I am not sure if it is good to employ a pre-trained BERT to encode the content of the spreadsheet except for the header. Therefore, I would like to know if the authors have tried to use a pre-trained BERT for encoding headers only while using a BERT learning from scratch to encode the rest of the spreadsheet. \n\n**noisy data/header**\nI wonder how the proposed framework can deal with headers that do not properly/correctly indicate the meaning of the data.\n\n**dataset availability & more samples**\nIt would be great if the dataset was provided so that the readers can better judge the difficulty of the problem and the performance of the models. Even if it is not possible to make the dataset publicly available, it would be better if a set of randomly selected data points (spreadsheet + target formula) were included. Also, providing some statistics of the dataset would be helpful, such as the length distribution of the target formula, the most commonly used operators, etc.\n\n**reproducibility**\nGiven the clear description in the main paper and the details provided in the appendix, I believe implementing the proposed framework is possible. Yet, without access to the dataset, it is still impossible to reproduce the results.\n\n**related work**\nWhile the related work sections in the main paper and the appendix sufficiently cover most of the relevant works, I believe this paper can still be benefit from including the following papers\n- Improving Neural Program Synthesis with Inferred Execution Traces\n- Execution-Guided Neural Program Synthesis\n- Neural Scene De-rendering\n- Learning to Describe Scenes with Programs\n- Neural Program Synthesis from Diverse Demonstration Videos \n- NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}