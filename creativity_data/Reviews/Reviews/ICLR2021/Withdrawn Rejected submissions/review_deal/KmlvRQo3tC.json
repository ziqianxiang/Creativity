{
    "Decision": "",
    "Reviews": [
        {
            "title": "The idea is not new, and the formulation of its method is wrong.",
            "review": "1- About the idea. The proposed feature propagating idea is not new at all. TPN[1] is the first paper utilizing this embedding propagation for few-shot learning. A small difference is that TPN used Gaussian similarity to calculate the weights. Besides, [2] has another similar embedding propagation. It is hence not clear what is the difference between this submission to [1][2] regarding its main contribution (embedding propagation). More interesting is that these two papers are so related but not discussed in this submission (note that [1] is cited but simply for claiming there is a transductive setting in previous work [1]).\n\n2- About the method/formulation of the method. In my understanding, Eq (1) and (5) are the key formulations of the method. However, they are wrong. The weighted x_i[t] just derives the original x_i[t] (as the weights are summed up to 1), which means the iteration produces no change on the feature embedding of x_i. Maybe it should be x_j[t] (on the right side of the equation), but not sure what is the critical difference to the embedding propagation used in related works (as I mentioned in the above comment).\n\n3- About the performance. Two important issues here are (1) no significant improvement over those simple (easy) baseline methods (so far the few-shot learning methods are far more advanced than those baselines); and (2) no comparison to the cases of plugging the related methods of embedding propagation such as [1][2] in baseline architectures (such as protonet and maml). Note that Table 4 last block is not for this but for another transductive setting and it plugs the proposed method only.\n\n[1] Liu Y, Lee J, Park M, et al. Learning to propagate labels: Transductive propagation network for few-shot learning. ICLR 2019\n\n[2] Rodríguez P, Laradji I, Drouin A, et al. Embedding Propagation: Smoother Manifold for Few-Shot Classification. ECCV 2020\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The idea is simple but seems effective",
            "review": "Summary:\nThe paper proposes a simple idea to improve the performance of the meta-learning. In particular, a sample propagation layer is proposed to learn to propagate sample representation within a task, via dot product attention. Extension to transductive setting is discussed\n\nStrength:\nThe idea is easy to implement and provide some improvements \n\nConcerns:\n-The novelty of the idea needs to be better discussed, e.g. compare with previous work: \"Learning to Propagate for Graph Meta-Learning\"\n-More justification is needed for the 1-shot scenario. Considering (1), when the task adaptation is being performed, there is only one sample for each class, so the propagation is solely based on the samples from different classes which is supposed to be discriminated from. The point is, after preprocessing using the proposed method, the features from different classes become more compact which does not seem to be good for classification. In particular, when the 1-shot query sample of a class for training is an outlier, this preprocessing can decrease the inter-class distance, which could degrade the classification accuracy. The authors may like to clarify this.\n-The performance of current meta-learning methods is saturated on Omniglot. Also considering the distribution and the centralized characters, it is not quite a challenging dataset for current algorithms. The authors may want to provide some results using datasets like miniImagenet, CUB or others.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and appears to be effective, but needs better experimental validation",
            "review": "This paper aims to improve feature representation for FSL purposes to alleviate the effect of outlier-like support samples in learning a new task. For this purpose, the paper proposes to transform features via a sample propagation mechanism such that the features become more class center oriented. The resulting transformed features can then be used in existing FSL methods, such as prototypical networks & MAML. The proposed mechanism is lightweight and \"plug-and-play\", which makes it easy to use in combination with most existing FSL approaches.\n\nThe core idea in the proposed sample propagation layer is to update each training sample by accumulating feature vectors from similar support examples , where similarity is measured by a learnt attention estimator. In my interpretation, I assume that there is a typo in Eq 1, where x_i on the RHS should instead be x_j, because otherwise x_i would not change at all from t to t+1.\n\nIn sample propagation, same-class samples' attention weight is enhanced through a multiplicative constant (a hyperparameter) on the attention logits. The proposed method is also extended to the transductive setting. The proposed training approach requires having a pre-trained feature extractor, which can be seen as a set-back in the ease-of-use of the approach.\n\nThe paper reports promising results on few-shot classification and few-shot graph meta-learning.\n\nA major shortcoming of the paper is its insufficient experimental validation for the few-shot classification tasks, at least in two ways. First, many contemporary few-shot learning works (including Snell et al. and Finn et al.) focus on datasets more challenging than Omniglot, such as MiniImageNet, CUB or CIFAR100. Results on Omniglot are quite saturated, where it is hard to make any conclusive observations. More challenging benchmarks, such as MetaDataset (Triantafillou et al.) can also be considered.\n\nThe second shortcoming is the fact that despite making a heavy emphasis on 'plug-and-play' like nature of the method, the paper evaluates the approach only on Prototypical Networks (from 2017) and MAML (again from 2017). It could have been a lot more interesting to see its effects on more recent approaches, such as MAML++ and others. \n\nAdditional questions & remarks:\n- can you please give a bit more information about the networks h1(.) and h2(.), and, explain why are they different?\n- t-sne plots are indeed informative, yet it would have been even better to include support set points after sample propagation.\n- it could have been good to see a discussion on the potential risk of losing intra-class variation as a result of the proposed technique in the paper.\n\nOverall the paper has an simple and effective technique to improve feature representation in FSL. However, the experimental validation is too weak in its current form to warrant an ICLR publication. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for \"MASP: Model-Agnostic Sample Propagation for Few-shot learning\"",
            "review": "Summary\n-------------\nFew-shot classification methods learn to classify a set of classes from a small number of support samples per class. Thus, these methods are highly sensitive to outliers in the set of support samples. In this submission, the authors propose to alleviate this problem by refining the class prototypes by propagating information in a graph. In order to do so, they compute attention values between prototypes and perform a weighted sum. In order to evaluate the robustness to outliers, they provide experiments on WebVision, and they use Omniglot, tieredImagenet-Close, and tieredImagenet-Far to provide results in standard settings. For most experiments, the proposed method (MASP) improves the baseline performance.\n\nOverall Review\n--------------------\nThe topic of the effect of noise and outliers in few-shot learning is under-explored and interesting for the research community, and the proposed model seems to work well in terms of accuracy. However, the idea of propagating or correcting information between few-shot embeddings has already been explored multiple times in the literature [1,2,3,4,5], and the authors did not compare MASP to other similar approaches, either in standard setups (miniImagenet, tieredImagenet, and/or meta-dataset) or noise reduction setups. Another concern is on technical correctness, since the authors claim multiple times that by just adding their method they obtain an improvement of 4%  and they do not specify what is the increase in parameters of their method with respect to the backbone feature extractor. Finally, the authors put most emphasis on their model's performance and  it would be more interesting if instead they provided more insight on the effect of noise on MASP and other SotA. Overall, given these concerns and the ones in \"Weaknesses\" I do not recommend this paper for acceptance but I encourage the authors to continue improving their work.  \n\nStrengths\n--------------\n* The topic is interesting and under-explored\n* MASP improves baseline performances.\n\nWeaknesses\n-----------------\n* Although the main motivation of the work is to \"improve robustness to outliers\", the authors did not provide enough analytic or empirical evidence of this claim. Thus, I suggest the authors to add more evidence of these claims. A thorough study on the effects of outliers and noise in few-shot learning would be of high impact to the community.\n* There already exist multiple embedding refinement approaches that were not properly compared with MASP [1,2,3,4,5]\n* The authors claim multiple times that MASP achieves up to 4% accuracy improvement. However, the neural networks used by the authors are small and the amount of parameters introduced by MASP is not negligible (particularly if they apply it more than one time). I suggest that the authors show the improvement with respect to a few-shot Wide ResNet architecture [6].\n* The authors introduce multiple hyperparameters such as $\\alpha$, $S$, $\\lambda$, and the hidden size. However, there is no analysis in how they affect MASP. It would be interesting if the authors included an analysis of the effect of these hyperparameters.\n* Clarity. Since the authors provide the code, I could solve many doubts just by looking at it (which is a positive point). However I had to look at it many times to clarify points that were not clear from the text. For instance, where is the keep ratio defined? I could not find it in equation (1). What is $S'$? and how is it different from $S$?\n\nTypos\n--------\n* \"We propose model-agnostic sample propagation (MASP) calibrating\" -> for calibrating\n* \"of our MASP To be reproducible\" -> period missing\n* (MASP)\"that -> (MASP)\" that\n\n[1] Gidaris, S., Komodakis, N.: Generating classification weights with gnn denoising autoencoders for few-shot learning. CVPR 2019.\n\n[2] Rodríguez, P., et al. \"Embedding Propagation: Smoother Manifold for Few-Shot Classification.\" ECCV 2020.\n\n[3] Ye, Han-Jia, et al. \"Few-shot learning via embedding adaptation with set-to-set functions.\" CVPR. 2020.\n\n[4] Tseng, Hung-Yu, et al. \"Cross-domain few-shot classification via learned feature-wise transformation.\" ICLR. 2020.\n\n[5] Liu, Jinlu, Liang Song, and Yongqiang Qin. \"Prototype Rectification for Few-Shot Learning.\" ECCV. 2020.\n\n[6] Rusu, Andrei A., et al. \"Meta-Learning with Latent Embedding Optimization.\" ICLR. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}