{
    "Decision": "",
    "Reviews": [
        {
            "title": "Highly relevant work with some drawbacks",
            "review": "##########################################################################\n\nSummary:\n\nThe authors provide an interesting direction for learning hybrid quantum-classical stochastic networks. In this type of models, the activation functions of each layer are random variables. The density/mass function of these random variables are parametrized by the output of a neural network which gets the activation of the previous layer as input. This setting is prevalent in various state-of-the-art models, including, e.g., variational autoencoder. The authors focus on models which consist of Gaussian layers and Quantum Boltzmann Machines (QBMs). They contribute a new method for sampling from QBMs to generate generic MonteCarlo estimates for the ELBO and Q-ELBO gradients. The method is evaluated on synthetic as well as psychiatric genomics data.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I like the idea of using quantum computing to improve machine learning.\nMy major concern is about the applicability of the proposed method on real quantum hardware and that the authors do not relate their proposed method to competing state-of-the-art approaches (see cons below). Since both points represent major issues, I'm afraid that the authors can not address my concern in the rebuttal period. \n\n \n##########################################################################\n\nSTRONG \n\n1. Improving machine learning models by means of quantum computation is one of the key challenges in the next decade. Probabilistic models are computationally highly demanding, which renders them as an ideal candidate for finding quantum advantage. \n\n \n2. The Q-ELBO bound used during QVAE training does not permit direct optimization of the transverse weights in the embedded QBM. Tackling this problem via restricted world-lines and auxiliary energies is novel.\n\n \n3. Comprehensive experiments, including both qualitative analysis and quantitative results, on synthetic and real-world data show the practical relevance of the proposed framework. Moreover, interpretability of the quantum models is discussed. \n\n \n##########################################################################\n\nWEAK\n \n1. The problem of learning QBMs with arbitrary Hamiltonian (including transverse weights) has already been addressed. See \"Variational Quantum Boltzmann Machines\" (https://arxiv.org/pdf/2006.06004.pdf) The authors neither compare their method to the existing work, nor do they cite this highly relevant work.\n \n2. Moreover, the existing work is compatible with actually existing quantum hardware and has proven to deliver reasonable results on that hardware. In contrast, the proposed approach might not be compatible with near term quantum hardware (see below). As a consequence, only simulation results are provided and no experiments on actual quantum hardware. A couple of years ago, this was indeed a neglectable drawback. However, today, various real quantum systems are available. Most of them even with free access for researchers.\n\n3. Basic terms and notation are provided in Section 2. Therein, concepts which are fairly common in machine learning (like the ELBO) are introduced. On the other hand, not-so-common but highly relevant and important terms and concepts from quantum computing (e.g., \"Hamiltonian\", \"world-line\", \"quantum annealing\") and sampling (e.g., \"Continuous Time Quantum Monte Carlo (CT-QMC)\" and \"Population Annealing (PA)\" are neither defined nor explained. \n\n(4.) Some important open questions in the field like the existence of Barren plateaus are not discussed. \n \n##########################################################################\n\n\nRegarding WEAK point 2:\n\nThe authors shall discuss the limitations of actually / near-term quantum hardware in the context of their proposed method. As an example, the authors mention on page 5 that parts of their model may be carried out by a physical annealer, e.g, the D-Wave’s 2000Q. In that discussion, it is pointed out that \"for large N and T the number of logical qubits required will exceed the capacity of current physical annealers (∼2000 physical qubits on D-Wave’s 2000Q system)\". However, the number of qubits in the D-Wave’s 2000Q is only 2000 if the underlying logical (model) topology matches the physical D-Wave Chimera topology exactly, which is very unlikely. For a densely connected logical topology, one shall not expect more than 64 qubits in the 2000Q system. This is important, because it reduces the applicability of the proposed approach not only \"for large N and T\" but for \"most N and T\". Indeed, the authors use the above issue to motivate their \"compressed discrete representation\". \nHowever, it seems that the main reason for the space reduction is \"In the clamped case, we assume that at most two break-points occur per world-line\". Unfortunately, the authors do neither discuss the implications of this assumption, nor the intuition behind it. Does this assumption exclude some (maybe important) data sets or applications? What effect would this assumption have on a classical Boltzmann machine? Why is this assumption reasonable? \n\nRegarding WEAK point 3:\n\nIn the first part of the paper (up to Sec 2.1), the authors focus on models which consist of Quantum Boltzmann Layers and Gaussian layers. In the main part of the paper (starting from Sec 2.1), the Gaussian layers do not appear again. It is unclear why the authors focus on Gaussian layers in the first place. Moreover, in the introduction, the authors spend a lot of text and Figure 1, to explain to the reader the various possible combinations of Gaussian layers and Boltzmann layers. However, this presentation is not relevant for the rest of the paper. Since the main contribution of this work is a sampling strategy, it would be much better if the authors use this space to explain terms like \"Continuous Time Quantum Monte Carlo (CT-QMC)\" and \"Population Annealing (PA)\" or some concepts which are not-so-common in the ML community like the inherent limitations of quantum annealers. \n\nMinor\n=====\n\"boolean\" -> \"Boolean\"",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Algorithm for training a new class of parameterized quantum models.",
            "review": "The paper addresses stochastic networks with Boltzmann layers, where some of the distributions in the intermediate Boltzmann layers can be quantum, ie. they arise as Gibbs samples induced by a parameterized Hamiltonian operator. Building on research into quantum parameterized models analoguous to neural networks in other areas, the authors posit that such a quantum-classical hybrid network could have greater capacity for representation compared to a similar classical architecture.\n\nTo this end, the paper presents an algorithm for training the parameters of these hybrid stochastic networks by optimizing the Quantum Evidence Lower Bound (an analogue to ELBO obtained using the Golden-Thompson inequality for matrix traces. A Monte-Carlo procedure is outlined for sampling the gradients of the Q-ELBO.\n\nPros:\n\nThe system presented seems sound and efficient to implement on a suitable quantum annealer. The empirical results seem to indicate that the hybrid quantum models do have greater representational power than a corresponding classical system.\n\nCons:\n\nIt is not completely clear to me that the advantage obtained from the hybrid quantum models is due to an intrinsic 'quantum advantage' or a consequence of the relatively greater complexity of the quantum distributions used. Given the cost of constructing a quantum computer, it will almost always be more cost and resource efficient to train a classical model over a slightly smaller quantum model. \n\nThe current presentation of the paper is ill suited to readers without much background in quantum physics/computing. Several ideas from quantum computing are presented without context or intuition, and as such it may be hard for a classical machine learning researcher to glean the central ideas in this study.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Hybrid Quantum-Classical Stochastic Networks with Boltzmann Layers",
            "review": "**Summary**\nThis paper extends previous work on Quantum Boltzmann machines (QBM) and Quantum VAEs to models with multiple stochastic layers where each layer is either a (classical) layer with a Gaussian output distribution or a Quantum Boltzman machine. The QBM differs from the classical Boltzmann machine by the presence of a transverse field potential that acts on each qubit, and which makes the corresponding density matrix non-diagonal. Following [1] the parameters of the QBM layers are optimized by deriving a quantum ELBO (Q-ELBO), which is a lower bound to the usual ELBO and thus can also serve as a lower bound to the log likelihood. However, as noted by [1] the transverse field parameters cannot be optimized by gradient based methods that act on the Q-ELBO. The authors circumvent this by using quantum annealing-based sampling techniques.\nSeveral hybrid models with up to 2 QBM layers and one Gaussian layer and an entirely classical model are evaluated on 2 datasets/tasks: 1) a synthetic dataset with 100 datapoints for the train/validation/test sets respectively, where each datapoint is a binary vector of size 8, with density estimation as the task; 2) a subset of a genomics dataset where gene expressions are either binarized or kept as continuous numbers and the task is density estimation or classification using latent representations of the learned density estimators.\nBased on the experiments the authors claim that the hybrid models achieve better accuracy compared to the classical model, and that they find evidence that the transverse field term can be interpreted as introducing tunable higher-order interactions by connecting genes that belong to common biological pathways.\n\n\n**Pros**\n* The overall topic of quantum machine learning is quite interesting and many interesting directions are still open to research. Extending Quantum Boltzmann Machines and Quantum VAEs is definitely a valid research topic that will interest researchers that work on the intersection between quantum computing and machine learning.\n* The solution to optimize the transverse field terms certainly seems nontrivial.\n\n**Cons**\n* My main concern is whether ICLR is the appropriate venue for this paper. The target audience that is able to understand this paper is fairly narrow, and the paper is not written in a sufficiently self-contained way such that people who have basic knowledge of both quantum computing and machine learning can easily jump in. For instance, a lot of the concepts/derivations used in this paper are explained more pedagogically in [1]. I found myself looking at [1] for several parts of the paper to get a clearer understanding of what is going on. I think that in its current form it is more suitable for a physics oriented journal than ICLR.\n* The datasets that are used to evaluate the models on are really small, and the motivation for the choice of the genomics dataset is missing. \n* The claim that the transverse field term can be interpreted as introducing higher-order interactions between genes is backed up by very weak evidence. The results in Fig 2 on the magnitudes of the transverse field parameters for different layers and genes are very indirect hints that this could be the case.\n* Some derivations in the paper are unclear. For instance, the factorization of the approximate posterior Q(z|x) is unclear. Eq 5 implies a factorization of the approximate posterior $Q(z|x)=Q(z_0, z_1, …, z_{L-1}|x)$ that is very peculiar, something like: $Q(z_0, z_1, …, z_{L-1}|x) \\stackrel{?}{=} Q(z_{L-1}|x) Q(z_{L-1}, z_{L-2} | x)Q(z_{L-2}, z_{L-3} | x) ... Q(z_1, z_0 | x) Q(z_0 | x)$, which is not a proper factorization. What is going on here? Do you perhaps mean something like $Q(z_0, z_1, …, z_{L-1}|x) = Q(z_{L-1}|x) Q(z_{L-2}| z_{L-1}, x)Q(z_{L-3}| z_{L-2}, x) ... Q(z_0 |z_1, x) $ ?\n\n**Minor comments**\n* Please use boldface gamma in the paragraph above eq. 2.\n* Why is the model with P_0 = Gaussian and P_1 = QBM called a QBM with structured parameter space (QBM-p)? What makes this structured?\n*  First line on page 4 should refer to eq 1 and not eq 2?\n* First line of eq 5: H should be the same H as the one used for the Shannon entropy ($\\mathbb H$).\n* Last sign in front of the entropy of $Q(z|x)$ in eq 5 is wrong (it should be positive, just like in the first line of eq. 5).\n* Eq. 6, should $\\Lambda_x$ be $\\Lambda_{z_l}$ ? \n* Please increase font sizes of figure 2.\n\n\n[1] Quantum Boltzmann Machine, Amin et al. https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.021050",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Lacking baselines",
            "review": "This is an interesting paper that develops hybrid quantum and classical probabilistic models. The models are trained using variational inference, and a scheme for drawing samples in a classical simulation (via Trotter expansion) or with a quantum annealing device is described. The evaluation is conducted using synthetic data.\n\nIn its current state, it is difficult to evaluate the work - especially the claim that \"we show that hybrid [quantum and classical] models are able to achieve better perdictive accuracy compared to classical models of matching architecture\" (from the abstract). This is a strongly-worded statement and requires strong evidence. \n\nSpecifically: the datasets studied are not standard. There is a synthethic dataset and a genomics dataset with 200 datapoints. To have broader impact, I suggest a comparison of hybrid quantum-classical models trained on MNIST. Then it would be possible to compare the models developed in this work to the existing literature. If the MNIST dataset is too large (e.g. due to the hyperparameter T in the Trotter expansion), then subsampling and re-training multiple times is OK, to get a number for average-nats-per-datapoint. This would be comparable to existing work, and allow for a straightforward understanding of when a quantum model might outperform a classical counterpart (however, it seems difficult to make a comparison as the quantum models in this paper are simulated classically, so are essentially RBMs, which are not widely used anymore).\n\nSecond, there is little discussion of scalability: is it because the trace is difficult to evaluate for large systems, for calculating the density of a quantum layer in the method? Or is it because the Trotter expansion requires a large number of slices? Clear descriptions of when the method is applicable in its current state would help a practitioner understand when this might be useful and worth considering. Score function gradients are notoriously difficult to fit, and I am surprised they work here. How many samples were used? Were variance reduction techniques (e.g. https://dataspace.princeton.edu/handle/88435/dsp01pr76f608w) tested? A plot of gradient signal-to-noise ratio would be helpful to somebody thinking of implementing this method, as a function of # samples.\n\nFor additional baselines, a sigmoid belief network with a single layer, or multiple layers, needs to be included. These models differ from and usually outperform restricted boltzmann machines. For example, this paper has numbers for MNIST: https://arxiv.org/abs/1402.0030 and it would be very surprising if there was an improvement if an undirected graphical model such as a restricted boltzmann machine was used instead.\n\nThe paper is relatively well-written, and the authors do a good job of describing prior work and how this approach fits in.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}