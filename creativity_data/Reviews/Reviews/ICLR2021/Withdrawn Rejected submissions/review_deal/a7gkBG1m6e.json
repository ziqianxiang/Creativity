{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Thank you for your submission to ICLR.  Overall the reviewers and I think that this paper presents some nice contributions to the adversarial attacks literature, demonstrating a low-sample-complexity, \"physically-realizable\" attack in a domain of clear importance and interest in machine learning.  The move to considering more \"in the loop\" adversarial examples is particularly compelling, and the threat model and improvement over BO methods are both compelling here.\n\nThe main downside of this paper, of course, is the fact that the \"physical adversarial examples\" are of course nothing of the sort: they are simulated.  Rather, they are just simulated in a manner that may plausibly be slightly more amenable to real-world deployment. The authors claim that they don't carry out an evaluation on a real system because it is \"dangerous\" is a bit overly dramatic: the tests could easily be carried out in a controlled environment, and demonstration on an actual physical system (even, e.g., and RC car) would vastly improve the impact of this work.  As it is, the paper is borderline, but ultimately slightly below the high bar set by ICLR publications.  I would strongly encourage the authors to reconsider the inclusion of the word \"physical\" in the title, as it honestly sets expectations high for a promise that the paper cannot deliver on, or (even better) to run real experiments on even a small physical system, demonstrating the transferability there.  The paper ultimately has the potential for a high impact in this field, if these issues are addressed."
    },
    "Reviews": [
        {
            "title": "motivation not strong enough",
            "review": "This paper presents an approach to design physical adversaries to attack end-to-end autonomous driving systems. The proposed approach maps adversarial patterns onto video frames recorded from real world to generate adversarial examples for deviating the control of a vehicle. \n\nThe problem considered by this work is significant as physical adversarial attacks pose serious threats to the safety of autonomous driving. The paper is well written and most of the technical details are clearly articulated. The experiments demonstrates the efficacy of the proposed solution with results reported under different physical conditions . \n\nOne main concern I have about this work is that its motivation does not seem strong enough. Since both the training and evaluation are performed based on simulation, how real video streams help in developing a more robust approach remains unclear to me. The authors may need to more clearly argue why simulated images are less advantageous in this case. \n\nThe geometric and color parameters are obtained from manual calibration in this work. While this makes learning easier, designing an automatic matching method to learn these parameters from data would be more interesting, and potentially leads to more robust attacks against realistic environments. \n\nSome other comments:\n1) Itâ€™s a little bit difficult to understand why a few painted boxes on the road could fool an autonomous system. It would be helpful if the authors could provide some explanations of how these physical modifications affect the vehicle's controller.\n2) How are M and V determined in Eq. (1)?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A scalable framework for generating effective adversarial examples for driving scene",
            "review": "This paper proposes a scalable and efficient approach for finding adversarial physical modification to the video inputs of autonomous driving. Assuming the perturbations are in form of a collection of several rectangles, the model parameterizes the physical modifications. By simply ignoring the closed-loop of viewpoint sequence and frames, the model directly creating adversarial frames with compositing methods. Some approximated algorithms are used to ensure the model can be optimized by the gradient-based method. With the improvement above, the iteration speed of the model is greatly improved.\n\nStrengths:\n+ This paper proposes a highly scalable framework for designing physically realizable adversarial examples against end-to-end autonomous driving architectures, which makes the much stronger attack results.\n+ In the simulated climate settings, the proposed method demonstrates robustness against unforeseen variations in weather and visibility.\n+ With a small number of initial calibration running, the search for optimal parameters can be carried out with end-to-end gradient-based optimization, instead of relatively slower Bayesian Optimization.\n\nWeaknesses:\n+ It lacks of a convincing explanation about why ignoring the closed-loop of viewpoint sequence and frames the model can still work (in Section 2.2).\n+ The mask function is still not differentiable in most cases, thus approximation should be adopted. The authors should clarify how much such approximation affects performance.\n+ There are too many long sentences makes some parts of the article difficult to understand. Please try to improve the presentation\n\nI hope to hear the authors response regarding the weakness listed above during the rebuttal period.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting setup while less practical for real scenarios",
            "review": "To summarize, the authors propose a road-painting attack with rectangles to deceive a controller network such that the car will deviate from the correct trajectory. The simulation is done on CARLA.\n\n**The threat model**\nPainting roads with rectangles is very interesting. The closest one I saw is patching stop signs with rectangle markers [Eykholt 2018] as cited in the paper. Meanwhile, this setup brings many questions.\n\nFirst, the attack space is incredibly small (parameterized color rectangles on the road). With such a small space, I would expect the space allowed for changing the controller network output is also small. On the contrary, in traditional pixelwise adversarial attack, the attack vector is high-dimensional so the vulnerabilities can stack up to change the network output to arbitrary values. In the current setup, such stacking-up is unlikely to happen. Therefore, even the attack is successful, I believe tuning with gradient-based or gradient-free optimizers does no help much; the baseline attack success rate could be already high.\nIn other words, I doubt that even random rectangles may already cause deviations and infractions. Searching with BayesOpt or GradOpt may not help much; there could be a wide range of parameters that can cause reasonable deviations already.\n\nThis setup is also perceivable by humans and thus not stealth. In traditional adversarial attacks, the perturbations are small enough to be ignored by humans but will cause a deep network to fail; the current setup is not the case; therefore it can be easily defended by humans.\n\n**GradOpt v.s. BO**\n- BO is a black-box optimizer that has no access to the inner structure of the controller. GradOpt in this paper is a white-box model and it is unfair to compare BO with GradOpt.\n- GradOpt is a very standard gradient-based optimization pipeline for adversarial attacks involved with 3D projection and rendering.  Judging from the images, I believe the rectangles are not shaded, so an affine transformation already suffices. This limits the novelty of the method.\n\n**Constraints for the attack vector**\nI could not find the description of the constraints for the attack vectors. If there is no constraint; then there exists a trivial solution: just paint the road to a constant color using an infinitely large rectangle and there are no lanes anymore. It will be very interesting to see the evaluation metrics in this case just for ablation purposes.\n\n**Transferability**\nWith a small attacking space, I am curious whether GradOpt learns to find the vulnerabilities in the network, or learns to cover the important regions on the road. Also it is unclear whether it will succeed for real images and videos. The submission lacks transferability experiments to study those scenarios.\n\n[Update after reading authors' comments]\nThe critical issue is resolved: it seems their method is indeed better than plain BO, which in turn is better than random parameters. Though I still have a little doubt about how practical it can be in real scenarios, I increase the score to marginally accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A more efficient approach for finding physical adversarial examples in autonomous driving simulation",
            "review": "The paper proposes an end-to-end differentiable method for finding adversarial patterns to be added to the environment for autonomous driving. It utilizes image composition with homography thus it can compose the adversarial pattern into the image frames of all image frames of a driving sequence. Combined with a neural-network based controller which outputs the steering angle, the proposed method can find adversarial examples more efficiently comparing to a Bayesian optimization(BO) based baseline while resulting in trajectories with greater deviation.\n\nThe proposed method relies on an approximation of the image frames of the trajectories by adding random noise to the controller outputs and use those trajectories for learning. This dramatically reduces the number of calls to the simulator comparing to the baseline BO method. The paper states that \"Given the fact that actual control is closed-loop, it is not evident that this simple approach would work; however, our experiments below show that it is remarkably effective, despite its simplicity.\" Is it possible the reason for this approximation to work is that the scenarios are not complicated enough? If for more complicated scenarios where more calls to the simulator are needed, the benefit of the proposed method over the baseline would be much smaller. \n- Related question: are the trajectories with the noisy control resulting in infraction or not?\n\nOther questions / comments:\n1. The choice of using four trajectories as an approximation seems random. Any ablation study on the number of trajectories?\n2. The results in the three tables only contain average values without standard deviation.\n3. In sec 2.3 \"we use the absolute value of the sum of the differences between the angles as our divergence metric.\" It seems to me that the sum of the absolute values of the differences between the angles makes more sense because the positive and negative difference values would cancel out without taking absolute values before the summation. Similarly in Sec 3.1.\n4. Related to 3, I'm also curious about the design choice of using the controller deviation as the optimization objective function while using the trajectory deviation to measure the effectiveness of the adversarial attack. Those two may not necessarily correlate (An example will be if the car is supposed to go straight, while zero steering and both a left and a following correcting right steering will keep the car straight thus resulting in very similar trajectory, the controls are more different. On the other hand, a single left or right steering will result in large trajectory error as it accumulates.)\n5. All experiment results are in simulation so it's hard to draw conclusion regarding real driving scenarios.\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}