{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose a novel approach to a dialog-based  automated medical diagnosis, and present promising empirical results. The focus of this work is on robustness and reliability besides just the accuracy of diagnosis, which appears to be an important aspect in medical applications. The paper is clearly written and well-motivated. However, in there are still several concerns raised by the reviewers, and the paper may require a bit of extra work to be ready for publication."
    },
    "Reviews": [
        {
            "title": "Review of Towards a Reliable and Robust Dialogue System for Medical Automatic Diagnosis",
            "review": "The paper proposes a method for automatic medical diagnosis in a dialog system which is comprised of two modules: one which proposes symptoms to inquire about, and another which decides whether to go ahead with the inquiry or inform a disease. The second module makes the decision by looking ahead and checking whether the symptom wold case differences in determining the disease. The authors argue that existing systems have only been evaluated with respect to diagnosis accuracy, and introduce two additional metrics to evaluate the reliability and robustness of the method. They evaluate their method and compare with existing state-of-the-art methods on two datasets. Furthermore, they do a small human evaluation and report results on how evaluators perceive the diagnosis validity, symptom rationality, and topic transition smoothness of different methods. Overall, the paper is well motivated and written, and the authors show equal or better results compared to other state-of-the-art methods.\n\nI have the following questions/comments. Given clarifications in an author response, I would be willing to increase the score. \n- Can you add more information about the datasets/augmented test sets used (e.g., train/dev/test size). What data is the disease classifier trained on? What do the diseases in Table 4 represent?\n- Some details about the training are missing. For example, how many seeds were used? How long does it take for the training to converge using each method?\n- For the results of the DX dataset, is the same NLU model used for all three models? If not, how does that impact the results?\n- What do the numbers in Table 2 indicate? Are they accuracy results on the noisy test-sets?\n- The section \"Accuracy of diagnosis\" is not well written. Sequicity is introduced without any discussion on what it is. Additionally, new methods are introduced at the end of the paragraph. Why are these methods not used in Table 1? What is the intuition behind different results for different diseases? I suggest rewriting this section to first introduce the methods that are used, and then discussing the results. It might make sense to put this section before the \"Trusts for reliability\" section on page 6.\n- In the same section, the sentence \"And the results of each disease of Basic DQN is missing because the results are from the paper\", it is not clear that refers to table 4.\n- Since you discuss Figure 2 in detail in section \"Qualitative Analysis\", I suggest removing it from section \"Accuracy of diagnosis\" as mentioning it just briefly in this section is confusing.\n- There is no discussion of the errors that the proposed approach is making. Are the errors the same as the ones made with the other methods?\n\nThe text has spelling errors:\n - Page 3, P2: measures DSMAD -> measuring\n - Page 3, P3: dataset is includes -> dataset includes\n - Page 3, P4: paper is the same -> paper are the same\n - Page 4, P1: record -> records, produce -> produces, Remove And from the beginning of the sentence\n - Page 4, P2: imformed -> informed\n - Page 8, P1: an reckless -> a reckless (what does a reckless diagnosis refer to?)\n - Page 8, P2: ask -> asked. Remove And from the beginning of the sentence\n - Page 8, P3: mimics -> mimic, could -> can, therefore insensitive -> therefore is insensitive\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, need more clarity on some aspects",
            "review": "This paper proposes an interesting medical diagnosis dialogue system. It considers cooperation between two separate modules: an inquiry module that manages symptom inquiries and an introspective module to decide when to inform the diagnosis based on available symptoms. The paper also proposes two evaluation metrics based on reliability and robustness of the models. Results show the usefulness of the proposed model in terms of these evaluation metrics along with a human evaluation study.\n\nStrengths:\n\n- The idea of considering the symptom inquiries and the diagnosis decision making as a separate cooperative process in a deep reinforcement learning setting is interesting.\n\n- The proposal of how to meaningfully evaluate the proposed models using two new metrics is very useful. \n\n- Experimental results appear to be solid and the comparisons with related work are also meaningful.\n\nWeaknesses:\n\nThis paper appears to be an incremental piece of work based on the prior work on the same task. In my opinion, the main limitation of the paper is its novelty in terms of the core idea as it claims to mimic the decision making process of the human in defining the introspective system - however, this idea and the framework presented in Figure 2 is not new (see Ling et al. \"Learning to Diagnose: Assimilating Clinical Narratives using Deep Reinforcement Learning\"). Furthermore, the core modules of the paper are essentially same as Xu et al. 2019 as also acknowledged by the authors in Section 3.1.\n\nFew other comments: \n\n- More clarity needed for the notion of contradictory diagnosis and correct diagnosis as they appear to be ambiguous.\n\n- Not sure about the rationale behind discriminating the symptoms between explicit and implicit - why was it necessary to consider them differently for modeling?\n\nOverall, the proposed methodology is solid and the work would be beneficial to the community.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose an interactive RL based medical dialog system with a novel introspection mechanism",
            "review": "##########################################################################\n\nSummary:\n\nThe authors propose a new method for an RL agent-based , interactive medical dialogue. The method has two core components: a symptom inquiry module which selects a new symptom to inquire about a time step t, and an introspection module which looks ahead one time step to evaluate *if* knowing the outcome of this inquiry will change the predicted disease of the patient at time step t + 1. If the predicted disease at t + 1 would not change on the basis of issuing the symptom query at time t, the system returns the diagnosis, else it issues the query and repeats this process until convergence. The authors argue that this mimics the human decision making process and leads to a more principled diagnostic procedure. \n\nThe authors also propose two new metrics by which to evaluate medical dialogue systems. Reliability, which measures how sensitive the model is to random fluctuations during training (internal) and how well the selected symptoms align with human doctors (external). Robustness, which measures how well the model performs under an application-specific adversarial attack.\n\n##########################################################################\n\nRationale for score:\n\nI proposed a recommendation of reject due mainly to the deep concerns I have about the experimental setup. The two example vignettes the authors have provided are both seriously flawed both with respect to the accuracy of the ground truth (symptoms and diagnoses) and with respect to the example dialogues (see below). These issues will substantially impact at least one of novel metrics proposed by the authors (external reliability), which arguably is the most important metric. Due the experimental issues, I am unable to trust this metric and, unfortunately, the authors' method is no better (and in some cases substantially worse) on the second most important metric (in my assessment), diagnostic accuracy. \n\n##########################################################################\n\nPros:\n\n1. The proposed approach is novel and interesting and addresses an important problem. I particularly like the introspection aspect. I do however wonder if authors could provide some theoretical insight into situations where their one-step look ahead procedure will and will not converge? e.g. is the proposed method always guaranteed to terminate after a finite number of steps or are there situations when it might never return a diagnosis without a hard, pre-defined step cutoff?\n\n2. The figure is clear and extremely helpful.\n\n3. The introduction of new metrics other than diagnostic accuracy is new and somewhat well motivated.\n\n##########################################################################\n\nCons:\n\n1. There seems to be very little theoretical connection between internal and external reliability, as described. It might be better to break these out into two different metrics.\n\n2. Internal reliability is not fully defined. From equation (3), it appears to be the average probability from k=100 bootstrapped models. That, to me, seems to be a way to *reduce* aleatoric uncertainty but does not measure model sensitivity to perturbations. Something like the standard deviation of this bootstrapped probability would measure that however. \n\n3. Even though the authors are using a pre-existing dataset, I have deep concerns about how realistic the simulated datasets are, and how it may specifically impact their proposed metrics. For example, I showed the conversation about infantile diarrhea to my clinical colleague who is a practicing neonatologist and board certified pediatrician. Several things that were indicated to be the correct symptoms (orange boxes) were completely nonsensical in her assessment. In her opinion, no physician would ever describe a baby who has trouble feeding as \"anorexic\" and a \"stomach murmur\" is not a term she has ever heard. Likewise in the second example her opinion is that none of the systems collected sufficient information to render a diagnosis of pneumonia, and that actually upper respiratory infection (agent 1) provides the most sensible diagnosis, and the symptom requested by INS-DS (egg pattern stool) makes little sense.  Unfortunately, if these examples are representative of the rest of the data, it casts doubts on the strength of the authors claims. Their external reliability metric rests on how well the ground truth symptoms are captured by a system, and it seems that there could be systemic issues with the ground truth symptoms and diagnoses. The authors report some human assessment with three medical students, but it is hard to understand how the evaluation was done due to insufficient detail, especially in light of the two examples provided in the paper. \n\n4. Unfortunately, the authors proposed approach is not as accurate as other methods in terms diagnostic accuracy, and sometimes by a substantial margin. As the authors argue, other factors are certainly important (reliability and robustness), but all are secondary to diagnostic accuracy in my opinion. If the system you are interacting with is robust to noise that's great, but if it performs worse on average, then you will do worse in the long run as a potential user. \n\nMinor typos/confusing sentences:\n- \"Dialogue system for medical automatic diagnosis (DSMAD) aims at learning an agent to collect patient’s information and make preliminary diagnosis\"\n\n- \"but ignore the importance of robustness and reliability for practical medical application\"\n\n- \"Despite the contradictory diagnoses, the agent will still get a positive reward from the upper one since the diagnosis is correct.\" (what is the upper one?)\n\n- \"resulting in the system vulnerable to the noise happened during the interactions.\"\n\n- \"As for evaluation, aware of the insufficient validity of final classification accuracy and the number of query steps, current methods measure the performance of the diagnosing process by computing the hitting rate of the inquired symptoms.\"\n\n- \"Therefore, measures DSMAD according to the recorded symptoms in each user goal could not reflect\nthe actual performance of DSMAD in general.\"\n\n- \" Different from the most popular diagnosis systems which laving equal emphasis on symptom-inquiry and disease-diagnosis\"\n\n- \". And only when the most valuable inquiry proposed by the symptom-inquiry module makes no difference to the future diagnosis will the predicted disease be imformed\"\n\n- \"Emperically, the size of the replay buffer is 1e6,\"\n\n- \"Trusts for reliability. \" (Section header, should be tests?)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "INS-DS shows improvement over existing DSMADs, but clarifications are needed regarding experimental settings",
            "review": "In this manuscript, authors proposed a novel dialogue system for medical automatic diagnosis (DSMAD) called INS-DS. There are three components in the general DSMADs, which are NLU, DM and NLG. In this paper, NLU and NLG components are adopted from Xu et al., 2019. Authors focused on designing decision-making parts in the dialogue management (DM) component. INS-DS includes two modules: an inquiry module and an introspective module which is inspired by the introspective process when humans make decisions. Authors also introduced two metrics evaluating the reliability and robustness of general DSMAD agents.\n\nAuthors demonstrated INS-DS achieved state-of-the-art performance compared with other DSMAD agents and performed better with respect to robustness and reliability. My concerns are mainly related to the experimental settings and the detailed comments are listed as follows.\n\n#### Major Comments:\n1. (Table 4) Evaluating and improving the robustness and reliability of DSMAD agents is one of the major contributions in this manuscript. Authors have evaluated the robustness and reliability of DSMAD agents based on proposed metrics. One of the further experiments authors can conduct is to check the performance of DSMAD agents trained on MZ or DX and test on the other. This is a general and widely-accepted approach to demonstrate the agents' robustness and reliability.\n\n Both DX (527 conversations) and MZ (710 conversations) are relatively small datasets and there are only two diseases (I.D. and U.R.I) that appeared in both of the datasets. The performances regarding I.D. are pretty consistent but this is not true for U.R.I. The symptoms related to U.R.I. may be different across these two sets but there should be a reasonable overlap. Could you train the model on DX and test it on MZ focusing on I.D. and U.R.I? This would be more convincing to demonstrate the agent's robustness and reliability and this helps understand the inconsistency of diagnosis accuracies regarding U.R.I.\n\n2. (Table 1) For Basic DQN and KR-DS, their performances in Ext. (External trust) across MZ and DX are dramatically different. Is there any reason that can explain this difference especially considering that DX dataset was proposed along with KR-DS?\n\n3. (Table 2) It's better to include the baseline performances (test dataset without noise) for each of the agents. Otherwise it is hard to see whether agents are robust to noise or not under different settings (NS.1, NS.2 and NS.3 ).\n4. (Table 3). If more related symptoms are appended, the tasks should be relatively easier for humans. But NS.3 is consistently worse than NS.2 across all agents. Could you elaborate on this observation?\n\n5. (Figure 3) How does the diagnosis validity correlate with the ground-truth diagnostics for the test set? The diagnosis validity score ranges from 2.8 to 3.2 which is far from the best score 5. Is this due to the inconsistency between the ground-truth diagnosis and the diagnosis made by students?\n\n6. (Page 7, robustness analysis) In the robustness analysis, authors made use of noise test sets to demonstrate the agents' robustness. For humans, the diagnosis should be invariant with respect to the orders of the explicit symptoms or implicit symptoms. Based on this assumption, it is natural to augment the train set by permuting/sampling symptoms and this should further improve the performances of all models. Have you applied this augmentation strategy in the training phase?\n\n\n#### Minor Comments:\n1. (Figure 3) It is helpful to include the error bars (or all three data points) to show the variance of scores assigned by students.\n\n2. (Table 2) Robusteness -> Robustness\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}