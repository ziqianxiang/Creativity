{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes the Skill-Action (SA) architecture, based on the insight that semi-MDPs in the option framework can be posed as an equivalent MDP. The paper presents interesting theoretical results and very promising empirical results.\nWe thank the reviewers for their revisions, which provided more insights into the method. Of particular interest was the discussion on the \"dominant skill problem\".\nWe still feel that the paper would benefit from additional experiments, as discussed in detail in all of the reviews. I believe with this inclusion this will be an impactful paper."
    },
    "Reviews": [
        {
            "title": "Well-motivated paper, but concerned about experiments & prior work",
            "review": "This paper shows that the semi-MDP option framework has an MDP equivalence, by adding extra dependencies into the master policy. The proposed method (“Skill-Action” architecture) trains a skill policy which marginalizes those dependencies, allowing the master policy to be updated at each time step. SA encodes primary actions to skill context vectors, and applies the attention mechanism to these vectors to temporally extend the skills. \n\nPros:\na. The paper is well-written and well-motivated.\nb. The proposed method outperforms baselines on infinite-horizon mujoco locomotion tasks, and achieves comparable performance as baselines on finite-horizon mujoco tasks.\nc. I think the empirical analyses of the learned skills, especially Figure 4, is interesting and valuable to the RL community.\n\nCons / Questions:\n\n1. It is mentioned that SA has faster learning speed because it only has one action policy decoder, unlike DAC and AHP. Do all of the methods have the same number of learnable parameters for fair comparison?\n\n2. SA outperforms baselines on infinite-horizon tasks, but achieves similar performance on finite-horizon tasks. The authors “conjecture that finite horizon games contain less significant temporal relationships than infinite horizon games”, but I would have expected the opposite (?). I think more experiments on finite vs. infinite horizon tasks would be useful to the research community, e.g., compare SA vs. baselines on the same task as you increase the time horizon.\n\n3. Insufficient related work: There is a plethora of prior work on skill context vectors for RL (e.g., [1,2,3]) but they are not discussed or compared to:\n[1] PEARL - Rakelly et al. 2019 https://arxiv.org/abs/1903.08254\n[2] InfoGAIL - Li et al. 2017 https://arxiv.org/pdf/1703.08840.pdf\n[3] Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning - Gupta et al. 2019 https://relay-policy-learning.github.io/\n\n4. Since this is a work on hierarchical RL, the paper could be made stronger with experiments on a more difficult long-horizon continuous control task with distinct skills, like robotic manipulation, or Ant locomotion around a small maze.\n\nMinor typos:\np.2: “while improves their functionalities” -> “while improving their functionalities”\np.2: “Unlike the option framework requires M action policies” -> “Unlike the option framework, which requires M action policies”\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "I think this paper needs more work, both experimentally and in its discussion of related works.",
            "review": "This paper introduces the skill-action (SA) architecture, an instantiation of the options framework that can be optimized with standard MDP algorithms (such as PPO). The SA architecture uses a latent one-hot vector to specify individual skills. The action decoder is conditioned on learned embedding vectors corresponding to the skills. Skills can be temporally extended through a skill policy with an attention mechanism. In the empirical evaluation the SA architecture trained with PPO outperforms baselines from the DAC codebase in infinite horizon environments and performs comparable to the baselines on finite horizon tasks.\n\nThe paper is generally clearly written and, as far as I can tell, technically correct. The results on the OpenAI Gym are encouraging and improvements in HRL would certainly be significant and of interest to the ICLR community. \nHowever, I have some concerns about the paper in its current form:\n- Firstly, I think that the formulation is close to works in the broader HRL literature that use latent variables to learn skills without the options interpretation. Some (not particularly representative) examples include (but are certainly not limited to) Hausman et al. (2018), Tirumala et al. (2019), Wulfmeier et al. (2020a). This literature should be discussed in the related works section.\n- I think HO2 (Wulfmeier et al., 2020b) is a quite related work that appears to outperform SA. I recognise that it is a recent paper but it should at least be discussed in the paper.\n- I think the paper would be stronger if there were more environments in slightly different domains (perhaps in a manipulation setting).\n- Similarly, learning skills/options seem particularly promising in a transfer setting. I think transfer experiments would strengthen the paper.\n- The SA architecture as presented seems unlikely to learn multiple useful skills since there is nothing in the objective that encourages using different skills or diversity among skills. Indeed it seems that the method typically collapses to using one skill almost exclusively (figure 3). I am willing to change my mind on this point if the authors exhibited a scenario where the SA architecture reliably discovers and uses several skills.\n- The multi-head attention aspect seems like an architectural choice, rather than something required by the framework. I would like to see an ablation of this architecture.\n\nFor these reasons, I think the paper should be rejected in its current form.\nMinor comments:\nI would recommend changing 'games' to 'environments' throughout the paper.\nReferences:\n- Hausman et al., Learning an Embedding Space for Transferable Robot Skills, ICLR 2018, https://openreview.net/forum?id=rk07ZXZRb\n- Tirumala et al., Exploiting Hierarchy for Learning and Transfer in KL-regularized RL, https://arxiv.org/abs/1903.07438\n- Wulfmeier et al., Compositional Transfer in Hierarchical Reinforcement Learning,  RSS 2020,  https://arxiv.org/abs/1906.11228\n- Wulfmeier et al., Data-efficient Hindsight Off-policy Option Learning, https://arxiv.org/abs/2007.15588",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A single (master) policy algorithm for learning a discrete set of skills",
            "review": "### Summary\nThe paper proposes the \"Skill-Action\" architecture with an MDP formulation, inspired by their novel discovery of an equivalence of Semi-MDPs and MDPs under some assumptions. The benefit of the MDP formulation is that a single skill-policy (i.e. policy that selects what skill to follow at a time step) and a single action-policy (for decoding primitive actions given skills) is learnt as opposed to learning an individual policy for each skill (as is the case in prior work on options framework, option-critic, etc) and no explicit termination function is required. The paper defines a new value and Q function to accomodate the skill context vector at each time step and derives the recursive bellman equations and policy-gradient update steps for the proposed formulation, stating that the new value function has lower variance than the conventional value function. Empirical evidence shows better convergence and overall performance on infinite horizon tasks but not for the finite horizon case. Interpretation of skill context vectors shows that the skill embeddings correspond to distinct behaviors in the HalfCheetah environment.\n\n### Strengths\n- The paper is the first to propose a concrete formulation of MDP-based skill learning with a single {skill policy, action policy} pair and with policy-gradient updates. The equivalence between semi-MDPs and MDPs is also novel.\n- The new value function having lower variance while being an unbiased estimate of the traditional value function is a useful result (I have not verified the proof given in the appendix).\n- Performance on three infinite horizon MuJoCo environments is particularly strong (HalfCheetah, Swimmer, HumanoidStandup), though the choice of comparing results at 1 million steps as opposed to the 2 millions steps used in prior works hides the overall picture of whether the baselines eventually reach the same performance or not. Nevertheless, convergence (sample efficiency) of the proposed method is higher on these environments.\n\n### Weaknesses\n- The choice of experiments seem to be slightly cherry picked. Only 4 infinite horizon MuJoCo envs are chosen out of which 3 show good performance whereas all of the finite horizon environments do not show any improvements. Given that no significant gains were seen in the finite-horizon setting, I am surprised to not see more experiments demonstrating the efficacy of the proposed method on more infinite-horizon environments.\n- The skill activation pattern in Figure 3 seems to suggest that only one or two skills are being learned effectively by the proposed algorithm among the 4 possible skills to be learnt (the authors interpret this as the forward movement skill in HalfCheetah and the recovery skill for balancing itself). Firstly, the low number of skills to start out with seems quite limiting -- why not try any larger values? Secondly, given that just one skill is mainly used, this setting effectively reduces to the PPO baseline with a single policy, yet the proposed SA+PPO algorithm performs significantly better than all baselines. I suspect that the reason for increased performance may not necessarily come from having multiple skills and this can be verified by adding another baseline to all plots in Figure 2 -- the proposes SA + PPO algorithm with just 1 skill instead of 4 skills. If this baseline is same as PPO, then the conclusion can be drawn that SA + PPO effectively coordinates the skills learnt and this coordination is the reason for improved performance.\n- The explanation provided for lack of improvement on finite horizon tasks seems weak. I would argue that a good skill-learning algorithm should be able to pick up temporally abstracted skills for completing episodic tasks. For environments like HalfCheetah, if one skill is responsible for making the cheetah move forward, then it is basically equivalent to learning a single policy (PPO baseline for example) to move forward, which is the only way to achieve positive reward. Rather than showing improved performance on environments such as HalfCheetah or Swimmer, some more carefully chosen environments would have been better to demonstrate a diverse set of skills which are essential for solving the task (e.g. HumanoidStandup or a non-MuJoCo environment with a discrete state space).\n\n### Other issues/comments\n- Footnote 4 mentions the Reacher environment and states that SA performs 38% better than the second best on this env. However, the footnote link in the main text talks about finite horizon envs (HumanoidStandup in particular) and not Reacher. Further, the plot in Fugure 2 does not show the 38% better performance on Reacher. This may be typo, but correct me if I am wrong.\n\n### Feedback to authors\n- $Q_{O}[\\hat{o}_{t},s_t]$ are $Q_{O}[s_t,\\hat{o}_{t}]$ used interchangeably in Eqns 5, 6. It would be good to have a consistent notation for position of state and context vector.\n- Section 3.1 para 2, last line: This sentence does not make sense.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}