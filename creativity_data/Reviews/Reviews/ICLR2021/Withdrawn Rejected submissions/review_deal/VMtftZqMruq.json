{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper begins to formalize a connection between value decomposition and difference rewards. Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper.\n\nGiven the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results. As these are representative of the potential audience at ICLR, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community.\n\nThe empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper \"Qplex: Duplex dueling multi-agent q-learning.\" Given the theoretical connection to difference rewards (e.g. COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications."
    },
    "Reviews": [
        {
            "title": "A hard look at the latest MARL value function factorization methods",
            "review": "The paper takes a step back to examine some of the latest MARL methods in value function factorization by employing a classical framework, FQI. With some assumptions, the authors provide their analysis of the examined methods in the Bellman error minimization context and aligns them through a coherent unifying perspective.\n\n\nStrengths:\n\n+ The paper captures an important essence of lasting and meaningful research to deconstruct and provide some understanding of some of the latest MARL methods. In that aspect, the reviewer sees the need for more papers like this.\n\n+ As with successful papers with a similar theoretical approach, this paper carries a well-targeted array of relevant works and aptly incorporates the study of each in the analysis.\n\n\nMajor Concerns:\n\n- Even as a \"first-step\" analysis of related works, the two assumptions presented are, to my knowledge, unprecedentedly strong in the context of related works. It reads as though the assumptions are excessively strong to the point that the theoretical results that follow are natural corollaries. For example, if the transitions do not carry any stochasticity (as in Assumption 1), what need would there be for any non-linear value decomposition? In fact, if Assumption 2, also, further holds, then would there be a need for any value decomposition in the first place? The reviewer would very much like to be presented with explanation beyond \"decomposition for the sake of decomposition\". What kinds of insight can we gather from making a rather strong assumption and further deciding to carry out value decomposition?\n\n- The connection made between FQI and the more recent works should be explained in more detail. In other words, Section 5 could be better written with comparative analyses of VDN, QMIX, QTRAN, etc. in the eyes of linear value decomposition. When the two assumptions hold, how do those works compare against each other? What do the comparative analysis results mean?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work",
            "review": "The theoretical understanding of MARL with linear value decomposition is limited at the moment. Due to the limited representation of LVD (linear value decomposition), the standard Bellman update is not a closed operator in the joint action-value function class with LVD.\n\nThis paper is inspired by the current limit of the understanding and it provides a series of theoretical characterizations. The first result is that the updated Q_i(s, a_i) can be expressed by the decomposed target (equation (8)) up to some term in s with assumptions on discrete state and action spaces. The analysis is mostly based on weighted linear regression. This theorem has a moderate implication that the decomposed target is a counterfactual credit assignment. The second result is that the update (7) is closed, even with a sufficiently small eps for eps-greedy. Without eps this result is obvious, but with an eps it requires some tweak to analyze it. The third result is that if the linear decomposition is replaced with consistency on argmax (likewise in QMIX). This result is more or less intuitive and obvious.\n\nThe greatest contribution of this paper lies in that it proposes a closed-form solution to the Bellman error minimization derived in FQI-LVD up to some term in s. Implication 1 is rather obvious, and Implication 2 is an interesting explanation of the current limitation of linear decomposition. As the paper does not propose a new algorithmic approach, these theoretical implications can be limited. I believe that the information included in this paper might not be very sufficient.\n\nPros:\n\n1. this paper is of clear logic. For example, a). the research problems are well explained with strong motivation. b). The results and conclusions are solid verified by comparing the performance of different algorithms.\n2. the findings are meaningful. The closed-form solution can serve as a powerful mathematical toolkit, which encourages follow-up profound theories and explores potential insights from different perspectives.\n3. this paper is theoretically and mathematically rigorous. It gave more than 20 pages of proofs for nearly the lemmas and theorems\n\nQuestions:\n\n1. The author didn't interpret the credit assignment part very well when discussing the closed-form solution to Bellman error minimization. For example, the paper states that one of the contributions is that it implicitly implements a classical multi-agent credit assignment. However, in the literature review part, VDN also realized a multi-agent credit assignment. Is there an overlap? Or, is there a connection or difference between the credit assignment of the two solutions? Also, this paper gave examples of how the credit assignment was done when all joint actions generate the same reward signals. What if joint actions generate different rewards (and obviously this case is more close to reality)? How would the closed-form solution assign credits intuitively? I think the author should make more elaborations here.\n2. On page 7, the table shows a cooperative game. From table(a), we can see that the optimal policy is both agents take A1 simultaneously. However, in Table (b) and (c), both FQI-LVD and VDN algorithm choose the optimal policy of A2. I wonder why the two algorithms fail to find the optimal policy here?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting paper analyzing the success and failures of current value decomposition schemes ",
            "review": "This paper is largely a theoretical undertaking and focuses on bringing new insights into the currently popular value decomposition schemes like VDN, QMIX etc. for multi-agent reinforcement learning. They find two major implications: 1) linear value decomposition leads to implicit difference based credit assignment, and 2) richer Q function classes like those of QTRAN, QPLEX can improve convergence because otherwise there is a risk of unbounded divergence.\n\nThe paper was an enjoyable read (although there are a few grammar errors here and there).\nInstead of the theoretically more complicated online RL problem, it uses the offline RL setting to analyze various properties of value decomposition schemes, especially the possibility of them not being a proper $\\gamma$-contraction.\n\nThere are some minor issues in the presentation. In fig 1b, the $\\epsilon$ term hasn't been defined. One can guess that it's likely the $\\epsilon$-greedy policy for data collection, but greek variables should be clarified before they are used in general.\n\nGiven QTRAN should have a harder time enforcing IGM constraint, it's a bit surprising in Fig 2, that it works so much better than QPLEX on the harder problems. Any speculations into why?\n\nIt would be useful to also draw connections to coordination graphs and their pairwise and higher order interactions of value functions (like that of DCG for example [1]).\n\n[1] https://arxiv.org/abs/1910.00091\n\nPost discussion: I'll defer to R2 and R4 for judging the theoretical contributions and am convinced that they are not as significant.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper but has issues. ",
            "review": "This paper analyzes the behavior of Bellman update in cooperative multi-agent setting, when the value function has the form of a linear value decomposition of individual Q function per agent. The paper assumes no partial observability (i.e., all agents can see all states) and shows that under deterministic dynamics and factorizable Dataset, after one update, the individual Q function has close form, building connection with COMA. The paper then shows with this function class, the Bellman update might diverge in some MMDP. By extending to a broader function class, the Bellman update is close again. \n\nThe theory proposed in the paper looks interesting. I appreciate that the authors make an attempt to create a theory to analyze how the value decomposition fits to the Bellman equation, which is a fundamental question to ask. It can be a good contribution to the research community. However, there are several questions: \n\nQuestions:\n\nThe assumption of factorizable data and no partial observability seems to be super strong. What is the reason behind the two assumptions? If we remove them, to how much extent you conclusion still holds? \n\nI don’t see very strong connections between the theoretical analysis and the empirical studies. Why “|Q_tot^{FQI-LVD} - Q_tot^{VDN}| = 0.22 strongly illustrates the accuracy of Theorem 1”? Shouldn’t we compare the two terms with relative error? Note that FQI-IGM basically removes the constraint of linear value decomposition. It looks like QMix is in FQI-IGM (the Q combination is nonlinear) and it should work according to the theory, but why it doesn’t work in Matrix Game? \n\nThe StarCraft experiment is kind of confusing. How is it connected to the theories? Basically you just show methods with more rich neural network models work better, which usually holds without the theory. Note that the dataset is constructed with a pre-trained policy (by VDN), I am not sure whether the factorizable assumption holds here? Did you check? It looks like a lot of ablation studies are needed to make the connection clear. \n\n=========\n\nAfter rebuttal I will still keep the score. \n\nThe Assumption 1 is still quite strong. While it is true that $p(\\mathbf{a}|s) = \\prod_i p(a_i|s)$ holds for any policy with decentralized execution, the assumption of full observability is really strong and it doesn't seem that it can be got rid of easily. With this assumption, $p(\\mathbf{a}|s) = \\prod_i p(a_i|s)$ is actually a trivial assumption, since all information needed to make a decision of action $a_i$ for agent $i$ is already contained in the full state $s$ and can be determined independent of each other. In the revised paper, the authors suggest that communication can solve it but this would require thorough communication over the entire MDP, which can be hard to achieve (and if that's easily achievable then there is no need to study Dec-POMDP anymore). Without relaxing this assumption, I have concerns that the theory is not substantial (which are also concerns from other reviewers like R3). One baby step is to at least assume each agent may receive a noisy version of the full state $s$, and see what's going on. \n\nI thank the authors for additional experiments. Note that in addition to the proposed theory, there are many possible explanations of the empirical results presented by the authors. E.g., as a general rule of thumb in RL training, using offline data is often worse than using online data. Without detailed analysis, it is hard to tell. I would rather use an environment that is more complicated than the matrix game, but much more simpler than SMAC, which is partial observable and has too many moving parts. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}