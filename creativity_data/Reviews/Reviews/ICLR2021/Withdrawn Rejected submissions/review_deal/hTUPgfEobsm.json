{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Most of the reviewers had serious problems with clarity to start out. \nThe authors have addressed some, but not all of these problems. \n\nMore importantly, there were issues of significance and experimental evaluation.\nI concur with r4 on the experimental evaluation. \nI think if you're going to explicitly specialize toward disentangling affine transform parameters, \nthat's fine, but then you're in application-paper land, and I think there needs to be more of an attempt to show\nthat it will work \"in the wild\". \nFor this reason, and for the general reason that reviewers unanimously voted to reject, I am recommending rejection."
    },
    "Reviews": [
        {
            "title": "Description of method incomplete",
            "review": "This manuscript presents ADIS-GAN (affine disentangled GAN), a method for learning disentangled affine transformations of images such as rotation, zoom, and translation. The authors enforce this disentanglement using a regularizer that penalizes the difference between affine transformations on latent representations. The method is difficult to follow with many critical details left out. Without a complete description, it’s impossible to fully understand and evaluate this work. Some specific comments and questions follow below.\n1.\tHow is the latent vector, C, used? How does it interact with W_basis and W_real? Must it be 5-d to match with rotation, zoom, and translation? How are additional factors of variability encoded?\n2.\tHow is W_basis learned?\n3.\tWhat is Norm_AL\n4.\tWhat are the encoder/generator/adversary model architectures and how were they trained? For that matter, what is the encoder listed in algorithm 1? I can’t find any reference to it in the text.\n5.\tHow do you ensure that affine transformations of the latent vector actual produce affine transformations of the image? In Figure 5, it looks like the faces aren’t simply being transformed by zoom, because other attributes clear change. In column 5 from the right, the top face has short hair, an exposed ear, and visible teeth. The bottom face, in contrast, has long hair, covered ear, and no visible teeth. This does not appear to be a simple zoom transformation.\n\nThings that would improve my score:\n1.\tFully explain the method, neural network architectures, and training parameters.\n2.\tExplain how affine transformations of the latent vector are forced to produce matching affine transformations of the image.\n",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "review",
            "review": "Paper summary:\n\nThis paper seems to be about aligning latent variables in a GAN with certain affine transformations of the image. This is done by adding an additional \"affine regularization\" on top of the InfoGAN formulation. This affine regularization seems to be the L2 distance between the randomly sampled c (interpretable latent variables?) and some transformed c'. How the transformed c' is computed was not understood by the reviewer, but it seems to depend on a set of explicitly chosen affine transformations. The affine transformations specified are rotation, horizontal and vertical zoom, horizontal and vertical translation. The authors experiment on MNIST, dSprites, four shapes, and CelebA data. Quantitative metrics are reported for dSprites.\n\nStrong points:\n\nNot many papers tackling disentanglement in GANs.\n\nThe paper seems to show nice visual results and seems to be working at recovering the selected affine transformations of images.\n\nWeak points:\n\n(My overall concerns are about the additional supervision required by the method and the clarity of the writing, in particular the method itself.)\n\nThe paper mentions this is a self-supervised approach, but it seems closer to a supervised approach where the affine loss can only help recover specific latent factors that must be known beforehand. Would ADIS-GAN be capable of discovering unknown latent factors?\n\nThe paper repeatedly claims in the text and in Table 1 that previous works do not have axis-alignment or scalability. Does ADIS-GAN guarantee alignment in some way that is more reliable than in previous works? Isn't the scalability aspect (modeling horizontal and vertical zoom) explicitly encoded by ADIS-GAN, whereas previous works attempt to learn this from data?\n\nI could not follow the exposition in the paper. For instance:\n  - There seems to be a complete change in notation from Sections 1-3 to Section 4. No mention of the previously defined x_real, x_fake, z, or c are used in Section 4, where the core contribution is described. \n  - Many sentences end in \"see Appendix\", but I have looked at the Appendix and still come away confused..\n  - Why is M 3x3?\n  - Where do the A_{ij}'s come from in equation 9? I guessed it might be elements of c, but there are 5 elements in c and 6 A_{ij}'s.\n  - There seems to be derivations of a maximum likelihood estimate for constructing c', but these depend on knowing what the A_{ij}'s are.\n  - The purpose of Norm_{AL/LA} is never explicitly mentioned, though I imagine it's for scaling the interval [-1, 1] to specific bounds so that M can be applied?\n  - What is Encoder in Algorithm 1 and why does it return a linear operator? (Is this different from the encoder in InfoGAN which outputs a distribution over the latent variables?)\n\nAdditional feedback:\n\n - There is mention that ADIS-GAN used categorical codes as well. I assume these do not contribute to the affine loss?\n\n - There are multiple rows in Table 2 that reference models not explained in the text. e.g. references for HFVAE and ChyVAE, and what is \"InfoGAN(modified)\"?\n\n - Please use \\citep to include brackets around your citations, when not explicitly using them in a sentence.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Some interesting results, but the benefits of the explicit parameterization are not clear ",
            "review": "This paper proposes a method that can explicitly learn the affine transformations via disentangled representations in a self-supervised manner. The proposed ADIS-GAN learns to extract affine parameters by adding an affine regularizer on the top of InfoGAN. It seems to me that the main contribution of the paper is to learn the explicit parameterization of the affine transformation, such as rotation, zoom, and translation, by maximum likelihood estimation.  Experiments show that the proposed method is successful in estimating individual parameters of the affine transformation. \n\n**Strengths**\n+ The paper represents the first method that can learn explicit parameters of the affine transformation in a self-supervised manner.\n+ The experiments show that the proposed method can learn a disentangled representation with explicit affine parameters. \n\n**Weakness** \n- Experiments are not convincing enough. The paper only contains experiments on simple/synthetic datasets. The benefits of the proposed method are not completely clear. For the dSprites dataset, the proposed method does not outperform InfoGAN-CR, while InfoGAN-CR can perform disentanglement beyond affine transformations. For scalability, the paper only presents one example for zoom. It is also unclear how to merge zoom parameters into one. It is also helpful to give real applications that can benefit from the explicit disentanglement. \n- Writing of the paper can be improved. For example, Section 4.1 needs improvements. The notations are not completely clear, and their roles in the whole algorithm are not clear. It would be better to strengthen the connection of Equation (7) to the affine regularizer loss defined at the end of Section 4. \n- The factorization of the affine matrix and derivation of the maximum likelihood estimator are straightforward, making the paper's technical contribution less significant. \n\n**Minor issues**\n- The paper emphasizes scalability, but it is not completely clear what scalability the proposed method achieves and how it is achieved. For example, is it possible to merge the translation parameters into a single one? If so, how to do it, and how well the combined parameter performs?\n- It is unclear whether the proposed method can factor out skews. Figure 1 shows examples for skews, but the paper does not present how the skew parameters are estimated. \n- In Appendix A, the paper assumes the noise statistics for all parameters A_{ij}. It seems unrealistic. \n- The citations are correctly formatted. The parentheses are missing for citations.\n\n**Post-rebuttal**\n\nThank the authors for the rebuttal. It addresses parts of the raised issues. However, my rating keeps the same after reading the rebuttal and other reviews because\n1. the contribution and utility of the proposed method are not significant;\n2. the writing needs improvement; and\n3. the experiments are not convincing enough, and its advantages over previous methods are not clear. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}