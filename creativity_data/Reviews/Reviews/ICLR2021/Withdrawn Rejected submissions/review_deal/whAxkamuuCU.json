{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed a  new type of models that are invariant to entities by exploring the symbolic property of entities. This problem is important in language modeling since it gives intrinsically more proper representation of sentences, which can better generalize to new entities.  However I still suggest to reject this paper for the following reasons \n1. The description of model is not clear enough which can certainly use a serious round of revision.\n2. The experiments on bAbi is not convincing enough since it is an overly simple and toyish data-set with many ways to hack\n3. Similar entity-invariant idea has been explored long time ago by  (https://arxiv.org/pdf/1508.05508.pdf) which attempted to represent entities as “variables”\n\n"
    },
    "Reviews": [
        {
            "title": "Equivariant Networks for NLP",
            "review": "The authors propose a network that is equivariant to entity permutations without requiring the pre-specification of the set of entities. To this end, the authors propose a hybrid semantic-symbolic embedding which they integrate into two QA models. Finally, the authors show significant gains on the bAbi tasks, with especially impressive gains in the 1K setting. \n\nThe problem is quite interesting and challenging in the setting where entities are not prespecified.\nHowever, given the model description it is not clear at all how the model is able to learn a symbol-shift equivariant embedding. \nI don't understand how the model is able to determine that \"apple\" and \"orange\" have the same embedding while \"apple\" and \"John\" have different embeddings. \nWhat is the loss/model architecture/data augmentation guiding this? How is the model able to figure out that \"John\" and \"Sasha\" share embedding?\n\nApart from the high level details, I don't understand the following notations and operations:\n* In Section 4, what is $n$? Is it total number of words in the sequence?\n* If $B_\\varphi \\in R^{m \\times n}$  and $e^m_j \\in R^m$, the multiplication $B_\\varphi e^m_j$ doesn't make sense.\n* How exactly is $\\alpha_x e_{\\varphi(x)} \\in R^m$?  What exactly is $\\alpha_x$ and what is it's shape? \n\nThe notation and the working of the model is not clear to me, hence, I am giving a low rating for now.\nApart from this I also doubt the proposed method's generalizability beyond toy settings.\n\n\n\n\n \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "not very well motivated",
            "review": "This work proposes to improve the generalizability of bAbi models through [entity permutations].\nMore specifically the approach assumes domain knowledge of word/entity type equivalences, which helps restricting possible permutations between word POS (e.g., “John” vs “why”) or gender (e.g., “John” vs “Mary”). Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation. Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models (especially TPR) to handle artificial data sets with large number of entity names. \n\nOverall I find the proposed research not very well motivated. Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach. Commonly used strategy is to replace named entities in sentences with their word type tokens . e.g., from [how old is Obama] to [how old is PERSON]\nhttps://arxiv.org/abs/1601.01280\nhttps://arxiv.org/abs/1611.00020\nThe proposed approach seems to achieve a similar effect, but is a lot more complex.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Confusing",
            "review": "**Summary**.\nThis paper proposes a new type of models that are equivariant to entity permutations, which is an important criterion to build language models that can easily generalize to new entities. The authors modified a Memory-Network and a Third-order tensor product RNN to make them symbolic-shit invariant. The new models were evaluated and compared on the 20 bAbi tasks. Results show that the symbolic versions of the models yield better performance than the original ones.\n\n**Positives**.\nThe topic is of great interest and it is indeed crutial that neural language models become symbol-shift invariant to allow them to better generalize. This work is clearly motivated.\n\n**Confusions**.\nThe beginning of Section4 mentions that the main idea of this work is to concatenate a regular \"semantic\" word vector with a \"symbolic\" representation essentially corresponding to a one-hot vector of the token order of appearance.\nIn the following paragraphs, the work presented lacks clarity and seems to over-complicate concepts with hard-to-follow math notations. For instance, the “*Mapping words into and from symbolic representations*” paragraph introduces tedious math notations to describes something simple that was clear before, namely, the mapping from tokens to their respective symbolic vector, which is simply defined as the one-hot vector position appearance of this token in the context.\nSimilarly, the \"*Hybrid semantic-symbolic embeddings*\" paragraph uses again tedious math notations to describe how semantic and symbolic embedding are concatenated.\n\nGiven the confusion presented in Section4, it is currently not clear how adding a one-hot vector to the input embedding can make a neural model symbol-shift equivariant.\nIn particular, below are the two things I could not understand:\n1) The paper mentions that \"*all parameters are differentiable*\". It is not clear if that also includes the symbolic representation or not? If so, then the initial one-hot vector may not be a one-hot vector after the gradient updates performed during training, which would result in a non-symbolic representation? if it is kept fix during training, then it is not clear how it is used by the network.\n2) In addition, assuming that the symbolic representation of all tokens stays the same during training, I don't see how \"_permuted symbols share the same latent representations_\" if the latent representations are made of both on-hot vectors **and** regular word vectors. I understand that the symbolic representation does not change for a permuted word since it will appear at the same place as the original word. But the semantic representation will be different. For instance, the semantic word vector of “banana” is similar but still different than the word vector of “apple”.\n\n**Conclusion**.\nI would suggest the authors to simplify their mathematical notation and make their paper easier to read. As of now, I could not fully understand the paper and unfortunately for that reason could only put a score of 4 with a low confidence of 2.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}