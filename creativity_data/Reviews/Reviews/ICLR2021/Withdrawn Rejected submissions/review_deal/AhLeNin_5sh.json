{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an architecture of a model for multiple correlated time series that has application in anomaly detection.  The main idea is to use attention both along time to capture trends, seasonality, etc and along series types to capture their correlation.  Training loss attempts to reconstruct masked ranges of values, and thus the discrepancy between predicted and actual values can be used to flag anomalies. \nWhile the ideas proposed in the paper are useful and lead to a well-engineered model while combining current architectural elements to best suit their task, the paper fails to impress as a research contribution.   The writing requires improvement, and the experiments on two datasets are not entirely convincing.  The authors have realized these limitations, as can be seen in the author feedback.  However, the changes entailed are too extensive to  revise the ratings across all reviewers.\nI hope these will help the authors submit a better next version to another conference."
    },
    "Reviews": [
        {
            "title": "Model description is not clear enough and does not enable reproduction",
            "review": "The authors present an architecture to detect anomalies in multi-variate time series. To represent this complex input, they rely on a multi-correlation attention. Their approach compute correlation while taking into account periodicity of the signal through FFT.\nAfter describing their approach, the authors experiment sucessfully on 2 datasets.\n\nSection 3 should be clarified. The authors have to clarified their architecture and the computation that are made before proposing proofs. For me, the architecture is not clear.\nf1 -> f4 are not formalized, the link between function f anf FFT should be clarified.\n\nI think that notation on x and X are not consistent in equation (1) to (8)\n\nThe variable ranking on precision and recall depending on the model would push to an AUC metrics. Do the authors carry out some experiments in that direction?\n\nExperiments on latency are very interesting",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A new architecture for multivariate time-series anomaly detection. Proposal of two attention mechanisms to capture patterns within and between time series.  Unclear evaluation/advancement of the state of the art",
            "review": "The paper presents GenAD, a DNN architecture for multivariate time-series anomaly detection. The key differences in comparison to existing approaches are two attention mechanisms tailored to capture patterns within time series and correlations between time series. Experimental results on two types of datasets show the effectiveness of the proposed architecture. In addition, the paper studies a pre-training method to transfer models to new applications. \n\nPros\n\n- Tackles a timely problem as multi-variate time series are becoming prevalent\n- The architecture is simple yet effective\n- The pre-trained methodology can improve the practicality of such architectures\n\nCons\n\n- Technical novelty is limited with attention mechanisms proposed to be extensions of current approaches\n- The writting needs substantial improvement. Math formulas have issues\n- Evaluation is weak - essentially one of the two datasets show improvement - in the other dataset the difference is marginally better\n\nDetails:\n\n- The novelty is low. It seems that a big benefit from the attention mechanisms comes from working in the frequency domain. However, this is something that all other methods can take advantage of. The FFT-filter essentially smooths the data, which might be the sole reason for the improvement. The integration of such smoothing in the remaining methods is necessary to understand the true effect of the proposed attention mechanisms.\n\n-  Similarly as above, experiments are required to understand how the method performs with and without transformation to the frequency domain. \n\n- Many formulas have issues. Most references need spacing between previous work. Writting is not clear, terms are mentioned (ripple effect) before introduced etc.\n\n- The evaluation seems preliminary. In the MSL dataset there is no improvement. In the SMD datasets, which is split into 3 categories, there is improvement, but as mentioned, it's not clear which part of the method is responsible for that. is the Fourier-smoothing ? the attention mechanism?\n\n- Previous methods can take advantage of current attention mechanisms. Even though they might not have been proposed with them, an integration is needed to understand if the newly proposed attention mechanisms are substantially better than existing approaches",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes an algorithm that represents multiple correlations in the data and uses time series attention to better represents relationships within multivariate time series and use these for anomaly detection.",
            "review": "This paper proposes to better represent relationships between different variables within time series and use the result to perform anomaly detection. The experiments show reasonable results. However, the algorithm needs to be presented in a cohesive way to be able to understand what is actually being done. Various steps are presented, but they are not tied together in a way that a reader can implement the algorithm. A detailed algorithm explanation and pseudocode would give all the details needed, including some of what are referenced below.\n\n1. The introduction has a sentence \"In practice, the overall status of an entity is more concerned about than each constituent metric...\" I don't understand the meaning of this sentence.\n2. Section 2. Note that, with OCSVM, time series or sequence-oriented kernels can be used to enable it to be used with time series. E.g., Das, et. al., \"Multiple Kernel Learning for Heterogeneous Anomaly Detection: Algorithm and Aviation Safety Case Study,\" KDD 2010.\n3. In section 3, it says that GenAD sets the duration of the training portion of the time series to be four times the duration of the testing portion. How was this chosen? What different splits were tested?\n4. Section 3.2.1: The proposition is not stated in a rigorous enough way to be able to tell what is being proven in the proof that is below it.\n5. In section 3.3, it says that GenAD selects a random 20% of the dimensions and masks them with fixed random series. Is a different 20% chosen with every time series or for a given problem? How do the results vary depending on the dimensions chosen?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}