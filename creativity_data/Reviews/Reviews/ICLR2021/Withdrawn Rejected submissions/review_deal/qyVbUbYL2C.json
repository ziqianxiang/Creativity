{
    "Decision": "",
    "Reviews": [
        {
            "title": "This is a paper that maybe cannot meet the acceptance criteria.",
            "review": "Summary:\nThis paper provides an insightful viewpoint to investigate the relationship between mimic biological visual development and CNN training. This paper explores the effects of training CNNs on images with different levels of applied blur, including training regimens with progressively less\nblurry training sets. Extensive experiments demonstrate the hypothesis of this paper and support the utility of learning from sequences of blurry images for more robust image recognition.\n+ves: \n+ The idea of discussing the relationship between the mimic biological visual development and CNN training is novel.\n+ The comprehensive experiments also show the defectiveness of this paper, the exploration of training strategies with different types of blurring kernel is compelling.\n+ This paper has clear motivation and well writing skills, the conclusion part has a clear logical flow.\n\nConcerns: \n\nFirstly, the main concern comes from the motivation part. The body construction of human being and CNN-based methods are very different, why the experience about the development of the human being can guide the training strategy for objection classification? What the intrinsic rule about this motivation. Secondly, the reason for using the blur kernel from the small sizes to the large sizes is not very clear. The hypothesis is contradictory since the vision of human beings can be clear progressively. The statement ‘we may start to bridge the large gap that still remains between human and computer vision’ is a little exaggerated.\n\n+ The experimental results are not very sufficiency. In my opinion, it is not so sufficient to convince me with only two classical CNNs and one high-level vision task through the hard effort had made by the authors. It would be better to show more experimental results, such as the activation map of CNNs to demonstrate the viewpoint.\n\n+The experimental result shows that the Alexnet cannot change significantly according to whether it was trained on the blurry image. I wonder to know the reason for this phenomenon.\n\nMinor comments: \n The last paragraph of subsection 3.1 has some composing errors.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation to Reject",
            "review": "The paper addresses the problem of learning robust models by using blurry images. By carefully designing experiments on two datasets, they verified a familiar fact that training with blurry images or image sequences can help enhancing the robustness of a model. Meanwhile, blurry training data have negligible effects on training convergence time and false positive classification certainty.\n\npros:\n1) The paper is well organized and very easy to follow. \n2) The experiments are carefully designed. The sizes of blur kernel are carefully chosen to follow the average childhood visual acuities at increasing ages.\n\ncons:\n1) Similar conclusions have been drawn in the previous literature. The paper just verified a well familiar conclusion in a series of carefully designed experiments. So I think the significance of this paper is limited.\n2) The paper proposed two kinds of training methods, i.e., single blur and sequential blur training. The two types of training methods yielded quite different results on the same dataset for the same model. It is not clear to what extent the training methods cause on the classification accuracy than training using the blurry images.\n3) During sequential blur training, each network is trained on varying sequences of images with different levels of blur. It is not clear why the described training mechanism in the paper is used. That is, first train the network on images with one pixel blur and then train on blur of three pixels followed by blur of one pixel. Is the order of blur sequence important for the training result?\n4) Other than Gaussian blur, there are many types of blur, for example, motion blur, blur caused by fog or rain. Does these types of blur help the model training?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Blurring might help, but more analysis is needed",
            "review": "Summary\n-------\n\nInspired by the way that Human vision develops from near blindness to near normal visual acuity in the first few months after birth, this paper explores training CNNs on blurry images. The paper investigates two differing strategies for training - one in which images are trained on a single blur level, and the other in which the amount of blur is monotonically decreased during training. In both cases the models are tested for classification accuracy at different amounts of blur. Experiments are performed on two small subsets of ImageNet; the key results are: (a) as one might expect, networks trained on a single blur work best when the test set has the same blur; and, (b) sequential training (decreasing the amount of blur over time) leads to models that are more robust overall the entire range of blurs, but slightly less performant with small blurs.\n\nPositives\n---------\n\n- The experimental methodology (experimenting with both single-blur training, as well as sequential schedules of blur) is good, and the results, although rather preliminary, are interesting (particularly the sequential ones).\n- The sensitivity analysis (testing trained models on different blur levels) is a good idea.\n\nConcerns\n--------\n\n- Important technical details regarding the blurring are omitted: what is the variance (or S.D.) of the differing Gaussian kernels? Are the integer kernel template approximations sufficiently large (i.e. +/- ~4 S.D. or otherwise) to ensure that aliasing is not occuring?\n- Whilst AlexNet and SqueezeNet provide some baselines, I'd like to see a wider investigation of model architecture; in particular I would be interested to know if there is any interaction between the effective receptive field size of the models and their performance under the differing settings. (This might even be part of an explaination of the differences in performance between AlexNet and Squeezenet, but this requires analysis.)\n- The SqueezeNet loss plots for the mild blurs are concerning - some noise is expected, but this appears to be a larger problem worthy of debugging.\n- There is no analysis of the failure cases in the sequential experiments; the best sequential model is slightly worse at classifying images with small blurs - why is this? what sorts of errors are being made?\n- I'd like to know what the effect of the blurring is on a larger dataset (i.e. full imagenet) where there is a mix of subsets of classes with high visual similarity as well as groups of significantly different classes.\n- Blurring is inherently linked to object scale; have the authors considered this? To what extent is the change in performance observed linked to the size of the object being classified in the images? Going further, it's not clear that the authors considered their results in the context of other approaches that utilise Gaussian Scale Spaces in the training of networks, or even approaches that attempt to induce scale equivariance (e.g. https://papers.nips.cc/paper/8956-deep-scale-spaces-equivariance-over-scale.pdf) or invariance.\n\n\nRationale for score\n-------------------\n\nOverall, I think there are some interesting ideas emerging from this work, but in its present state it doesn't feel mature enough for ICLR. As outlined above I have a number of concerns that need to be addressed in order for the results of the study to be fully understood.\n\n \nMinor points\n------------\n\n- Figure 2 could be better presented (labels repositioned to make the axes clearer; clearer titles; etc)\n- \"classify and image\" -> \"classify an image\"; p6\n- a few ? instead of ' on p6\n- \"millions parameters\" -> \"millon parameters\"; S2.2\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}