{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper extends previous work on intrinsic reward design based on curiosity or surprise toward multiple intrinsic rewards based multiple model predictions and fuse the reward using meta-gradient optimization.  While most reviewers find the paper clearly written, several reviewers do bring up the concern on limited contribution of the work on top of existing ones. Reviewers also would like to see experiments conducted in environment with sparse reward rather than the delayed reward setting constructed from dense reward environments. More ablation studies on the different design choices will also be helpful. "
    },
    "Reviews": [
        {
            "title": "Contribution of the paper might be insufficient",
            "review": "Summary\nThe papers looks at the problem of using intrinsic rewards to help agent explore in sparse reward settings. The paper proposes combining multiple intrinsic rewards and proposes a meta-gradient based method to learning the fusion of these intrinsic rewards.\n\n\nStrengths\n1. Learning in a sparse rewards setting is a challenging and relevant problem to the community.\n2. The paper is well written \n3. The idea of learning to use multiple intrinsic rewards and using different combinations of them at different times seems to be helpful in the experiments\n\nWeaknesses/Comments/Questions\n1. My main concern is that the contribution of the paper might not be sufficient. The main contribution claimed by the paper to me seems like the idea of combining different intrinsic rewards and also learning the hyper-parameter that controls the fusion. In practice different intrinsic rewards are indeed used together and varied at different times. Also learning RL related hyperparameters using meta-gradient (Meta-gradient RL) is not new.\n2. I don't think Zheng et al is really meant for sparse reward settings. So not sure if that's the right SOTA baseline to compare with. Zheng et al is not expected to learn intrinsic rewards that guide initial exploration as it needs extrinsic feedback to learn these intrinsic rewards, else they are just random.\n3. It would be good if the paper evaluates on some domains that are naturally sparse reward setting instead of converting a dense reward setting to a sparse reward setting by accumulating and delaying rewards.\n4. If the main contribution of the paper is on how to fuse, then simple methods of fusing like learning a lenear combination or just a single layer neural network over the different model prediction errors to output the intrinsic reward to use all seem reasonable. Comparison with those would be good.\n5. The conditions that lead to the particular choice of fusion should be verified using ablation study to see if they indeed lead to the performance boost expected or if the performance comes from something else.\n6. Clarification: Is it that all the experiments only fuse between two intrinsic rewards (prediction errors)? If the paper is about the importance of combining different prediction errors, it would be nice to have more than 2 and also some interestingly different model predictions and also analysis and comparison on what happens if we just use them separately. \n\nQuestions to authors.\n1. Please respond to the comments above. Thanks.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposed a novel method that fuses multiple transition models to generate intrinsic rewards for sparse reward reinforcement learning",
            "review": "In this paper, the authors explore a model based intrinsic reward generation mechanism, in environment settings where the reward assignment is sparse. The authors used an ensemble of models,  and computed the alpha-mean value of their KL divergences with respect to the \"true transitions\". The alpha-mean serves as the intrinsic reward, with the parameter alpha being co-optimized during the training.  The authors demonstrated that their proposed approach yields top performance in six augmented MuJuCo continuous control tasks. \n\n\nPros:\n\n(1) A clearly written paper and easy to read and understand. \n\n(2) The approach demonstrated consistent top performance in all benchmarks.  \n\nCons:\n\n(1) Although the author provides a few ablation studies. I propose to add at least one more study that compares the effectiveness of their method under different \"sparsity\" settings. In all experiments, the authors used a fixed interval of 40, at which extrinsic rewards are computed. I am interested to see if their conclusion can still hold when this interval is set at, for example, 10, 20, 60, etc.\n\n(2) The alpha-mean method is one type of scale invariant transformation. I wonder if the authors have studied other transformation functions, or even scale invariant kernel functions?\n\n(3) There are other sparse-reward tasks such as pushing/sliding/pick-and-place, which are under a different category than MuJuCo continuous control tasks. I am interested in seeing if the proposed approach can still perform in these settings. This will also help make the author's statements more convincing. \n\nConclusion:\nPlease address my concerns raised in the \"cons\" section. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes a new method to fuse predictions from distinct models in the sparse-reward reinforcement learning scenario.",
            "review": "In this paper, the authors present a generalization of the model-prediction-error-based intrinsic reward method by fusing predictions from multiple models.\nThe authors considered the sparse reward scenario in reinforcement learning.\n\nIn related work, the authors mentioned that previous works on image spaces are not directly related to theirs. However, I did not understand what are the limitations or the caveats of the proposed method that leads to this conclusion.\n\nAlso in related work, I did not understand the purpose of detailing not related work in Sec. 2.2., I believe that the authors could use this space to discuss, for instance,\napplications of fusion methods in reinforcement learning scenarios, such as: “Data fusion using Bayesian theory and reinforcement learning method”.\nAnother option, even better, is discussing approaches proposed in the line of investigation using ensembles\n(e.g. “Model Ensemble-Based Intrinsic Reward for Sparse Reward Reinforcement Learning”).\n\nI would like to understand the computational costs involved in using such a fusion approach., both in terms of individual methods and alpha optimization.\n\nIn Fig. 2, it is hard to conclude that the gain over Module baseline is significant only considering the figure. I don’t know if the authors are considering some\nstatistical test when they mention significance, if this is the case, they should properly present the test and premisses.\n\nIn Sec.4.2.4, it is hard to see how the performance improves as K increases. First, only four values for K are a limitation to conclude this. \nAdditionally, for instance, when considering the Walker2d dataset, the performance for K=2 is better than K=3.\n\nReferences should point to the published work rather than the arxiv entries, when the former is available (e..g. Exploration by random network distillation, ICLR’19).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good paper with a missing baseline",
            "review": "This paper proposes an intrinsic reward formulation to address the challenge of sparse reward in reinforcement learning. The key idea is to learn multiple models. The prediction error of each model is used as a component of the intrinsic reward. These components are fused using an alpha-mean function. The parameter alpha can be tuned automatically throughout the training process using meta-gradient methods. The method is evaluated on 6 OpenAI continuous control benchmarks (with delayed reward), and demonstrates better performance than several state-of-the-art prior works on intrinsic reward.\n\nOverall, the paper is tackling an important challenge of reinforcement learning. Thus its potential impact is large. The paper is also well written and the results are promising. My main concern is that one important baseline is missing: [Pathak 2019]: \"Self-supervised exploration via disagreement.\" This prior work is very relevant. It also used multiple models and the implementation was so much simpler than the proposed method in this paper. In addition, this paper also claims to be a generalization of [Pathak 2019]. Thus, adding [Pathak 2019] into the baselines for comparisons seems necessary.\n\nI am also curious why not use MSE to measure the model-prediction error? The KL Divergence used in this paper introduces a lot of complexities, both in derivation and implementation (e.g. eq. 9, 10, 11). If MSE is not compatible with the proposed method, more discussions in the paper is welcome. Otherwise, adding an ablation to compare MSE with KLD would help, especially if KLD yields better results than MSE. This would justify the extra complexity introduced by using KLD.\n\nThe need of Condition 2 in Section 3.3 is not sufficiently motivated. How does this condition affect derivation, implementation and final results? It would be great to have a few sentences to clarity this.\n\nIn summary, this is a good paper. However, missing an important baseline [Pathak 2019] seems a significant oversight. I hope that this baseline will be added in the future version of this paper.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}