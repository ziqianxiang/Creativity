{
    "Decision": "",
    "Reviews": [
        {
            "title": "Some nice work; some lack of clarity (which may or may not be due to my own lack of background)",
            "review": "Summary: This paper gives theoretical guarantees on the “accuracy parity gap” – difference between accuracies on two sensitive groups – under a missing data condition. They give an upper + lower bound with analysis of convergence, and also provide some experimental validation of their results.\n\nRecommendation: Due to my lack of clarity on the communication of the problem, I lean reject for this paper. However, my confidence is low due to my lack of in-depth theoretical knowledge (i.e. not an expert in generalization bounds) in this area.\n\nStrengths:\n-\tTheoretical work seems sound (although I am not an expert in the theory-heavy side of this)\n-\tThis type of fairness metric is interesting, and doesn’t receive much attention I find\n-\tEmpirical explorations of sample imbalance and model misspecification are useful\n\nWeaknesses + Clarifications:\n-\tI am a little confused about the definition of “missing data”. The authors seem to define it through D_S, which is the set of examples where no component is missing. If this is the case, then why even bother with missing components at all in the problem model? We could equivalently consider it a form of dataset shift – either a point is present, or it is not\n-\tIn general, the motivation needs work – not quite clear how this should map onto real scenarios. For instance, top of 2.3 “fairness in the complete data domain is of primary interest” – it isn’t quite clear why this is true\n-\tMore analysis of the missingness mechanisms would be nice as they are discussed in detail at the top \n-\tWould like to see more clarification of the difference between this paper and Martinez-Plumed (2019) – you don’t really touch on it in 1.2\n-\tIn Theorem 2, I’m not sure how to interpret “with probability at least 7 / 1440” – this seems very low. Is this a useful result in this case? My interpretation is that this result relies on there existing potentially a very odd sample, which doesn’t seem like such a useful thing to know. However I might be misunderstanding results of this type\n-\tAdditionally, my confusion propagates in Fig 1a – is there some intuition for why this bound might work despite this possibly very low probability of holding? Would like some intuition\n\nOther feedback:\n-\tThe motivation section in the intro needs some work – would be good to focus more on the specific examples in the second paragraph.\n-\tTop of 2.1 – z_(0)I definition is confusing. Does this mean a certain number of “missing” tokens? The elements which are actually unobserved? The indices which have missing data?\n-\tWhy is D_T called a domain if it is a distribution? \n-\tTop of 3.2 – how can the bound on the variance be shown? Not obvious to me\n-\tIn COMPAS experiment design – why does it matter which feature you throw out? My understanding is that if any feature is missing, the whole data point is removed from the complete case domain and as such is a “missing” data point\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Important Topic that Merits Further Study. Rigorous Theoretical Analysis with Unclear Value in Practice.",
            "review": "# Summary\n\nThis paper presents a theoretical analysis of the estimation error of group fairness metrics in settings with missing data. The authors consider standard mechanisms that result in missing data and derive an upper bound and a lower bound on the estimation error for classification and regression problems. The paper includes numerical experiments on synthetic and real datasets to illustrate the tightness of each bound.\n\n# Pros\n\n- Paper focuses on an understudied topic that is relevant to many practical applications of machine learning.\n\n- Versatile theoretical analysis that applies to different fairness metrics as well as classification and regression problems. The paper includes a lower bound, which seems noteworthy, and could guide future theoretical work in this setting.\n\n- Paper includes experiments, which is appreciated in a theory paper. In particular, the experiments on the ADNI dataset seem well-suited for this setting and should be a focus of any future on this topics.\n\n# Cons / Comments\n\n- Paper does not illustrate or motivate practical issues that may arise as a result of incomplete data. This seems like a missed opportunity as it is one of the only works focusing on this topic. The paper would be far stronger if the authors could clearly describe some of the failure modes due to missingness. \n\n- The practical value of the upper and lower bounds is unclear. Both bounds depend on quantities that are difficult to estimate from data (e.g., propensity scores or the total variation distance between distributions).  The upper bound is likely to be vacuous in most settings where we would care about fairness. \n\n- The experimental results provide minimal added value. In particular, the \"quality\" of the bounds (i.e., the main result that would justify the bounds) are only assessed through experiments on synthetic data. The results of this experiment are extremely difficult to parse. The experiment focuses on a regression task, does not normalize the estimation error,  and reports the bais on a log-scale.\n\n- Ideally, the experiments on synthetic data should focus on a classification task, which is far more relevant to the fairness literature. The paper would be *far* stronger if the authors could report estimation bias in a way that would be accessible to a general audience. \n\n# Rating\n\nOverall, the paper considers an important problem but does not address it in a way that will have a long-term impact. My main concerns are that: (1) the paper does not describe the issues related to missingness in a way that is accessible to a general audience; (2) the main contributions of the paper (i.e., the upper and lower bound) appear to have limited practical value. \n\nGiven these reasons, I have currently awarded the paper a 4. However, I am willing to *dramatically* raise my score if the authors were to report results for a synthetic dataset for a classification problem in a way that would be accessible to a general audience (as this could address both of my concerns).  To this end, the results that I am looking for is a statement like the following:\n\n\"We consider a synthetic classification dataset where the true performance disparity between groups A and B is 10%. In this problem, we find that our estimates of the disparity gap vary between 8% to 12% when we sample n = 5000 points from each group with no missingness. These estimates vary between  -5% to 15% when there is missingness (suggesting that we can incorrectly evaluate fairness X% of the time). Using the bounds provided in this work, we know that – with probability at least 95% – the estimates of the disparity gap will range between -10% to 20%. This is useful because...\"\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper talks about convergence bounds for incomplete data case but lacks novelty and insight. The method also seems impractical. ",
            "review": "The paper presents an IPS weighted estimator for fairness using the accuracy parity gap. The contribution of the paper lies in proposing the IPS estimator that operates on the complete rows of the data with weights that can be estimated by fitting a propensity model. The paper then provides convergence guarantees for the estimator in addition to experiments on simulations show how the method is effective.\n\nAlthough the proven bounds in the paper are rigorous, I still do not consider the contributions of the paper to be significantly novel to merit an acceptance for the following reasons: \n1. The paper extends the already well-studied theory of importance sampling for unbiased estimation (and learning which this paper doesn't touch) from Cortes et al. 2010, and more recently Swaminathan & Joachims 2015. Just like the authors argue that the convergence bounds \"can similarly be derived for other fairness metrics\", the bounds for accuracy parity gap proposed in this paper can itself be derived from the references above. \n2. I believe that it is important to study the case of missing data in the context of fairness in machine learning but using a method that ignores data rows (with missing values) is far from desirable. Given that the paper proposes a method that only uses the rows with complete data for fitting the model and the learnt model is only usable for prediction a complete row. From an application perspective, even though through a certifiable method one can verify that the model is fair according to a particular metric, the model may already have suffered from underfitting because of all the data that is dropped. Even though this doesn't show up in the convergence rates, it is indeed an effect when working with finite data. \n3. The choice of propensity models in the experiments section seems to be crafted to the needs of the model: If we were to tune the propensity model in the experiments section to have an order of magnitude higher propensity of missingness, only a very few data points will have all the columns available. This is indeed a problem as it makes the method unusable.\n4. There are a plethora of other--more prevalent--methods that are used to handle missing data e.g. imputation-based methods, that are not studied in the paper. \n5. Minor: In the experiments section, It is not clear what \"missing values are generated using the propensity model...\" means. Perhaps it should mean the missingness of data (or the random value for observation) is drawn using the propensity model?\n6. One of the drawbacks of IPS methods that rely on predicting IPS scores is model misspecification (as also mentioned in the paper). The results in the case of using predicted propensities are not explained enough. It is not clear why everything except Random Forest would behave like the uniform propensity model. As far as I understand, the more expressive the propensity model class is, the more likely it is to be able to model the true propensity. So XGBoost if used correctly should at least be as good as RF. Also, some works show that it is not even necessary for the propensity model to be very accurate for the debiasing using IPS to work very well (for example Schnable et al. 2016 (Recommendations as Treatments...)). ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Application of a classic method on a metric motivated by fairness",
            "review": "The paper studies a fairness metric that captures the difference between the average accuracy of a predictor among the two classes of a protected binary attribute. This is a natural metric of group fairness, and the paper is devoted to the study of the following question: how can we estimate this metric when the distribution of the data points we have is different from the real distribution (perhaps through a mechanism that removes some of the data points in a way that potentially correlates with the protected attribute). \n\nThe solution is to simply apply the generic propensity score matching technique: when the distribution of available data is different from the real distribution, to estimate an estimand with respect to the real distribution simply weigh each data point by the inverse of its propensity score. The paper proves bounds on the accuracy of this method. The bound is composed of two parts, the first capturing how different the weights are from true inverse propensity scores, and the second is the sampling error, which is roughly between O(1/sqrt(n)) and O(sqrt(log(n)/n)), where n is the size of the smaller class. This estimation method is then evaluated empirically on a number of data sets.\n\nOn the positive side, the paper is well-written and easy to read, the model, the fairness metric, and the methods are all reasonable, and the results make sense. On the negative side, there's really not much technical novelty. The paper simply applies a well-known method on an estimand that happens to be defined to capture fairness, and the results are simple applications of classic bounds on this estimand. For this reason, I don't think the paper meets the acceptance bar for ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and well motivated paper on the transferability of fairness between train and test data when the distribution shift is due to missing values in the training set. ",
            "review": "#### Summary:\n\nThis paper addressed the problem of transferability of fairness between train and test data that have different distributions. They specifically focus on the case where the difference is due to the presence of missing values in the training data while the test data contains full observations. The authors make the distinction between “complete case domain” which represents the training data where observations with missing values are discarded versus “complete data domain” in which there are no missing values and represents the test / real data. \n\nThey propose to evaluate, on the complete data case, the fairness of a model trained to achieve some group fairness constraints on a training data with missing values (complete case domain). They focus on the “accuracy parity gap” (\\delta) measure they introduce as the difference in prediction error for two different demographic groups. The objective is to correctly estimate \\delta_T on the complete data domain (realistic test set, subscript T) from \\delta_S in the complete case domain (training set with extracted fully observed examples, subscript S). \n\nThe proposed approach is based on weighting the fully observed examples in the training set in computing the prediction error. They provide a lower and upper bound on |\\delta_T  -  \\delta_S^hat| and analyse them for the different possible missingness mechanisms on the training set.\n\n#### Strengths:\n\n- The paper addresses an important problem that is not yet fully explored, which can be useful to the algorithmic fairness literature\n- The authors provide an in-depth theoretical analysis of the proposed fairness estimator which extends upon prior work on transferability of fairness between different domains. However, I have not checked the provided proofs in detail.\n- The paper is well written and theoretical results are discussed and analysed through experiments on both synthetic ans real data. \n\n#### Weaknesses:\n\nThe definition of Z_obs and Z_mis are unclear. The complete case domain is defined as the subsampling fully observed examples for training to circumvent missing data in the training set. The work assesses how the fairness differs in the complete data domain  for a model trained on the complete case domain. Section 2.2 defines the training set for the predictor as Z_obs. However, the definition of Z_obs section 2.1 corresponds to selecting the observed features for each example in the training.\n\n#### Recommendation:\n\nI recommend acceptance for this paper. I believe this paper addresses an interesting problem which is not extensively explored in the literature, as far as I know. The work is theoretically well motivated and the authors do a good job of clearly discussing the different implications of the missingness mechanism on the results. \n\n#### Questions:\n\nClarify the definition of Z_obs and Z_miss in terms of D_S and D_T\n\n#### Additional feedback:\n1) The nomenclature “complete case” vs “complete data” is not very distinctive. I would recommend finding better names that clearly distinguishes the two notions.\n2) Typos: \n- Last paragraph in section 1.1: cross → across\n- Line 4 in section 2.1: ‘define’ → ‘we define’\n- Line 4 in section 2.1: 1{z_{ij} is observed} → 1_{z_{ij} is observed} \n- Line 5 in section 2.1: ‘Denote” → ‘We denote’\n- Last line, paragraph 2, section 2.1: ‘fairness guarantee’ → ‘fairness guarantees’ or ‘a fairness guarantee’\n- Line 2, paragraph 3, section 2.1: 1{z_i is fully observed} → 1_{z_i is fully observed}\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}