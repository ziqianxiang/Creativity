{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "One referee supports acceptance, whereas three referees lean towards rejection. All referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened. The rebuttal addresses R1's concerns about novelty and unfair comparisons, R2's concerns about computational efficiency of the methods, R3's concerns about motivation of the proposed approach and some missing baselines, and R4's concerns about motivation. However, the rebuttal does not address the reviewers' concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with SOTA. I agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring. However, after discussion, the referees agree that further work should be devoted to strengthen the contribution. I agree with their assessment and hence must reject. In particular, I would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following OGB guidelines to report the std of the results and including a proper comparison with the state of the art. "
    },
    "Reviews": [
        {
            "title": "Elegant solution but need more detailed experimental analysis",
            "review": "—Summary:\n\t\n\tThe authors propose a generalized neighborhood message aggregation function for GNNs. The proposed choice of generalized aggregation functions is SoftMax and PowerMean, which generalizes Max and Mean functions and interpolates them. Additionally, they propose a variant of these two methods, which can also encompass the Sum function. By making the components of these generalized aggregator functions differentiable, the GNNs can choose an approximate instantiation of the aggregation function that best optimizes the task. \n——\n\nPros:\n\n\tA straightforward and elegant solution \n\tThe Paper is well written and is easy to follow. \n\n——\nConcerns:\n\n\t(i) It is not clear how generalized aggregation functions to aid training deeper GCNs \n\t\tAlso, from the experiments, the baseline, ResGCN model’s performance also increases with layers and is falling short of the proposed model only by a small margin. \n\n\t(ii) Missing model wise results on OGB benchmark (Table 2)\n\t\tA detailed study is done only on one dataset, and in all other datasets, the best of the models alone is reported.\n\n\t\tFor all the datasets, results for the following compared models from Table1 is required to understand the achieved performance improvement. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. \n \t\t- baselines: ResGCN and ResGCN+ with mean, max, sum, \n \t\t- Fixed simple variant: Softmax, PowerMean, Softmax-sum, PowerMean-sum\n\t\t- Learnt variants: Softmax-sum and PowerMean-sum\n\n\t(iii) Benefits of the proposed model is inconclusive:\n\t\tComparison with ResGCN+ is reported only for two datasets, and out of which, one dataset, the arXiv dataset, has results reported only for one of the aggregation functions. From the other protein dataset, the avg score for Max-ResGCN+ is 85.09, Learned PowerMean, and SoftMax based models scores are 84.56 and 85.22. There is both a drop and gain in performance with the adoption of the generalized aggregation functions. Hence, it is not clear whether both the generalized functions aid in improving performance without seeing the results on other datasets for all the four or two versions of the learned generalized aggregation functions (as in Table 1.d) \n\n\t(iv) Need a statistical significance test report. \n\tThe results are very similar among models and are not clear whether there is any significant difference in choosing one over the other. \n\n——\nQuestions during rebuttal:\n\n\tKindly address the concerns raised above.\n--- Post rebuttal:\nI've read the author's response and there is no change in my scores. \n\n- The given argument regarding better-generalized aggregation function aid in training deep GCNs is not clear and convincing. \n- I agree that doing a detailed ablation study on a large dataset is expensive. In which case experiments on either synthetic or other smaller real-world datasets would be helpful.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting direction but limited novelty of the proposed method. ",
            "review": "This paper studies how to train deeper graph convolutional networks by using different aggregation function. Convolutional networks typically show better performance when getting deeper. However, this is not true in graph convolutional networks -- deeper graph neural networks usually show degenerate performance. So designing a deeper model for graph is both necessary and valuable.  This paper proposes a general aggregation function which summarizes sum, max, and softmax operations etc. I list the pros and cons as following. \n\nPros:\n1. This paper studies an important problem for graph convolutional networks. Also, this paper proposes an interesting perspective to study this problem.\n2. This paper conducts extensive experiments and analysis of different aggregation functions and their combinations. \n3. The paper is well written. \n\nCons:\n1. The greatest weakness of this paper is its novelty. Although studying aggregation function is interesting, this paper doesn't propose a new aggregation function beyond analyzing the combinations of existing ones. It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax. \n2. The proposed general aggregation function doesn't show significant improvement over softmax with learned temperature. Actually, the proposed powermean and softmaxsum are even worse than max. \n3. The comparison to other methods (GraphSAGE and GCN) is not fair because they are shallow models while the model used in this paper is quite deep. I'd like to see more fair comparisons with the same model size. At least the authors should present the number of parameters and the size of their model. \n4. This paper is based on DeepGCNs. DeepGCNs conducts experiments on point cloud recognition tasks and compare to DGCNN etc. I'd like to know why this paper doesn't consider point cloud recognition tasks as DeepGCNs tested on. Is it possible to compare to DGCNN/DeepGCNs on S3DIS?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #4",
            "review": "The authors propose new and, in particular, parameterized aggregation functions for GNNs in order to especially support the construction of deeper GNNs. The paper is fairly understandable and the \"deeper GNN\" topic has gained more attention recently. However, in my opinion, the paper is missing the theoretical justification of its proposals and I have concerns about the results (details below). Hence, I do not vote for acceptance.\n\n(+) The dynamic aggregation considered in Section 5.2 seems really interesting to me.\n\n(-) Approach: The authors define several parameterized aggregation functions but the motivation is left unclear. Indeed, it seems that the main motivation for splitting the definitions in this way seems to be to generalize as many of the existing functions as possible. The paper is missing an explanation why these different aggregation functions are supposed to specifically support deeper GNNs.\n\n(-) Experiments: \n* The main table contains very many similar results but is missing standard deviation. This is especially strange given that the OGB code supports several folds.\n* Table 2 is supposed to show the comparison with SOTA, but I claim that it is lacking. The initial OGB leaderboard contains only the most basic GNNs. However, when proposing an approach for creating deeper GNNs, the paper has to compare to this kind of models too. The row in which the authors compare to GCNII, JKNet, DAGNN, etc. actually seems to show that these models are as good/better. \n----------------------------------------------\n\nSmaller Comments:\n- p.1 \"The power of deep models become more evident with the introduction of more challenging and large-scale graph datasets\" this claim should be supported by references. Why are they especially helpful for this kind of data?\n- Table 1. The gray color is hardly readable and does not really allow for a \"convenient comparison\". I suggest to use bold face or anything else.\n\n----------------------------------------------\n\nUpdate after Rebuttal:\nI have read the authors' response but do no change my scores.\n\nThe experimental results alone do not offer a theoretical justification. The paper does not have to contain the latter but then the evaluation would have to be exhaustive.\n\nI see that the authors have done a huge number of experiments. However, if basic standards like standard deviation are neglected in order to run just more experiments, the results of the entire evaluation remain questionable. Similarly, related approaches have to be considered adequately to have an appropriate comparison to SOTA. The authors have not made them available yet.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation to weak accept",
            "review": "#####Summary#####\n\nThis work proposes a generalized aggregation function for graph neural networks. This generalized aggregation function can cover commonly used aggregation functions (i.e., mean, max, and sum) by particular setting of hyperparameters. Also, these hyperparameters can be learned with model in an end-to-end fashion instead of being predefined manually. The experimental results on OGB is impressive and can demonstrate the effectiveness of the proposed approaches. \n\n#####Pros#####\n\n(1) The proposed generalized aggregation functions are intuitively and empirically effective.\n\n(2) The included experiments are comprehensive and can demonstrate the effectiveness of the proposed method.\n\n(3) This paper is well organized and easy to follow.\n\n#####Cons#####\n\n(1) Although the provided experiments are comprehensive, some key experiments are missed. A. The proposed aggregation functions seem need more computations than simple mean/max/sum. A comparison of efficiency between proposed aggregation functions and existing simple functions should be included. B. The effectiveness of the whole model has been shown strongly. However, an ablation study to show the respective improvement brought by the increasing depth and proposed aggregation should be considered.\n\n(2) It would be better to provide some theoretical support if possible.\n\n\n\n#####Update######\n\nThank the authors for the response. I keep my score as 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}