{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting paper with some promising results",
            "review": "Summary:\n\nThis paper addresses two major issues in previous works of long-tailed recognition: 1) models are over-confident on “head” classes. 2) the domain shift (dataset bias) is ignored in two-stage models. Accordingly, they propose two novel solutions including label-aware smoothing and shift learning on batch normalization. They also discover that mixup can improve the representation learning on long-tailed recognition. The experiments verify the effectiveness of their approaches and show the significant improvements on ImageNet-LT, iNaturalist, and Places-LT.\n\nPros:\n- Overall, the presentation of this paper is clear and the writing is good.\n- The authors address the issues in previous works and conduct various ablation studies for verification.\n- The experimental results on benchmark datasets seem very promising and outperform baselines by a large margin.\n\nCons: \n- In experiments, I wonder if the authors apply mixup alone or with other basic augmentations, such as rotation, flipping. It would be interesting to see the performance combining with some basic augmentations since training without data augmentations usually gives bad performance.\n- It would be nice to add a plot to indicate the difference among the three functions for eqn (3), e.g. move the left plot of figure 11 to the main text and explain their behaviors.\n\nOverall, I find this paper interesting and novel. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper is aimed at long-tailed recognition. It finds that cRT and LWS in Decoupling suffer over-confidence and are prone to miscralibrated. It uses mixup to remedy this problem and proposes a label-aware smoothing method to improve classifier. Besides these, it is the first to note the dataset bias between different sampling strategies in 2 stages and proposes BN shifting to solve it.",
            "review": "The proposed method has large limitation as it fully based on cRT and LWS. The novelty and original work are unadequate since the most contribution is adding mixup to stage 1 and re-weighting to stage 2. The performance improvement is small.\nPros:\n1. The motivation and solution are clear and easy to unstand.\n2. The view of over-confidence is interesting and novel.\n3. The experiment and analysis are adequate\n3. The paper finds that different sampling strategies make the data distribution different and proposes to use BN shifting to solve this problem.\nCons:\n1. In 3.1, the paper judges that mixup damages the classifier but data in tabel 1 doesn’t reflect this problem.\n2. The proposed label-aware smoothing is in fact a re-weighting technique which gives bigger loss to tail class and smaller loss to head class. Howerver, in the part of analyzing the reliability diagrams, the paper only covers cRT and LWS which are not re-weighting methods.\n3. The proposed method is fully based on Decoupling cRT and LWS, which has high limitation.\n4. The performance improvement on Imagenet_LT is small, which is only 1% higher than cRT+mixup and 0.7% higher than LWS+mixup.\n5. The novelty and originality are not enough. It is virtually adding mixup to stage 1 and re-weighting to stage 2.\n6. An error: the cRT retrain the classifier from scratch rather than finetuning.\n7. How to get the equivalent (2) is unclear.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper studies the problem of image classification models trained on dataset of long-tailed distribution that they are prone to be mis-calibrated and over-confident even with the state-of-the-art two-stage methods. The proposed label-aware smoothing and shifted batch normalization are very interesting ideas, and their efficacy in improving calibration and classification performance on long-tailed datasets has been proved in the extensive experimentation.",
            "review": "Strengths\n- The paper is well-written and the story is clear. \n- It gives a nice overview on related work especially on recently published methods for training classifiers on imbalanced datasets, calibration and regularization methods. \n- It explored the effect of mixup in the two-stage decoupling framework. The proposed label aware smoothing is an effective method for handling the different degrees of over-confidence for different classes. It’s also the first work to study the dataset bias or domain shift in two-stage resampling methods for long-tailed recognition. The findings provide good insights in building deep models for long-tailed image classification. The experiments are mostly thorough and detailed. \n\nOther comments:\n- Mixup seems to contribute the most to the overall performance improvement, as we can observe from Table 2. However the efficacy of mixup has been studied in previous work, e.g. Thulasidasan et al. (2019), which found that networks trained with mixup are better calibrated. Table 4 also shows that mixup can improve other state-of-the-art methods which achieve performance close to the proposed MiSLAS.\n- Does the proposed method use the same hyperparameters as the other two-stage methods like cRT or LWS?\n- When first referring to cRT and LWS, it’s better to include their full names as well.\n- Please define N_j before using it in Figure 2’s caption firstly.\n- The paper did experiments on different IFs, but doesn’t have much discussion on how the proposed method’s performance is related to IF. Adding more detailed analysis on this will also be useful.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}