{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents an inference-softmax cross entropy (I-SCE) loss, a modification to the widely adopted \"Softmax Cross Entropy\" (SCE) loss, to achieve better robustness against adversarial attacks. The original submission had critical issues on motivation, theoretical analysis and experiments. Although the authors provided a revised version, it needs another round of thorough examination before publishing. "
    },
    "Reviews": [
        {
            "title": "Looks to me a degraded version of the adversarial training",
            "review": "The authors propose a loss function that is robust to the adversarial samples and claims the training with this loss function makes the model achieve better generalization ability.\nHowever, there are a number of problems in this claim.\n\nThe method does not ground on a solid theoretical analysis\\\nI tried to interpret the analysis, but failed.\n1. The right hand side of Eq(1) does not satisfy the rule of the probability. i.e., it does not satisfy $\\sum_y P(y|x) = 1$ when $\\sum_y P(y’|x) = 1$ and $m$ is a non-zero constant.\nThus the difference between two KLs around Eq.(5) does not make sense because $KL[P|Q]$ works as a valid divergence only when $P$ and $Q$ are probability distributions.\n2. We cannot derive Eq.(3) from Eq.(1)\n3.  If we add $m$ inside the exponential at the numerator for all the class $i$, it is equivalent not to add anything because it vanishes after normalization.\n4. There is a lack of the explanation why the adversarial training is not sufficient. It seems for me adversarial training will be a natural choice if we want to make the model robust to the adversarial training. \n\nStrange statement\\\nThe authors wrote “Since the ground truth distribution of training data is **known**, $H(P(y))$ is a constant and hence,”\nUsually we assume the ground truth distribution is **unknown**, if we know the ground truth distribution, there is no need to train the model. We can get immediately the minimizer of the loss. This may be a typo, but even if it is the mistake of “unknown”, still the statement is strange because it does not relate to whether $H(P(y))$ is a constant or not.\n\nExperiment is not convincing\\\nThe adversarial training works well for this experiment. Although the authors claim the adversarial training has a noticeable sacrifice of accuracy on clean examples. The mechanism is not well studied.\nAlso the authors tested their method only on MNIST and CIFAR10. It is not so appealing.\n\nReference\\\nThe intuition of the authors claim has a relationship with the distributional robust learning. The authors should discuss the relationship with these methods.\\\nA. Sinha, et al., “Certifiable distributional robustness with principled adversarial training,” in ICLR 2018\\\nAmir et al., “Robustness to Adversarial Perturbations in Learning from Incomplete Data” in NeurIPS 2019.\n\n-----------------------------------------------\nI appreciate the authors to improve the clarity and consistency of the method.\nI understand it has a practical value, easy to use and yet benefit from the method.\nHowever, if the authors want to stress its practical value, we need more convincing experimental results beyond MNIST and CIFAR10.\nThus I concluded it is still below the acceptance threshold.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review of \"Improving robustness of softmax corss-entropy loss via inference information\"",
            "review": "Paper proposes a modification to the widely adopted \"Softmax Cross Entropy\" (SCE) loss function that they refer to as I-SCE, with I standing for \"Inference\", that is designed for making the loss function more robust to adversarial examples. \n\nThe presentation of the idea is major mathematical flaws which make the idea authors are trying to present hard to grasp. \n\nThe main idea of the paper is to stretch the logit for the \"true\" class. This idea is presented in Eq. (7), where the logit term is stretched using the affine function x -> sx + m\n\nThis idea is introduced in the paper higher up, at Eq. (1) with adding a constant term to a probability! I had never seen a probability that is obtained from another one by adding a constant to the former. The reason being that adding a constant will break the \"sum to one\" property of a probability density function.  This is the first misleading / confusing math formula, which is used again in Eq. (3) & (4)\n\nAnother mathematical flaw in the paper is the text following Eq. (5): \"This equation indicates that DKLI (P (y)∥Q(y)) − DKL(P (y)∥Q(y)) < 0, showing how inference KL divergence avoids the overfitting scenario.\". I do not see why the difference of KL divergences being negative implies that overfitting is avoided. Perhaps authors can explain?\n\nFigure 1 is not related to the text: why don't we have m but delta instead here? I don't get what this figure means\n\nFigure 2 is misleading as well: which one is x, which one is y which is z axis, and what is presented here? can you add text to the axes so that I could understand what is going on there? how does this represent an adversarial attack anyway? \n\n\n\n \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The proposed method is very similar to label smoothing but evaluates at different benchmarks. ",
            "review": "Summary:\nThe paper claims that the vulnerability of DNN w.r.t adversarial samples comes from the overfitting issue when applying softmax cross-entropy loss. They propose I-SCE to mitigate the overfitting issue and show good performance in several adversarial attacks benchmarks. \n\nThe main concern from my point is the originality of this paper.  The author claims that avoiding overfitting can help improve the robustness under adaptive attacks. So the proposed method can be regarded as a regularization method to prevent overfitting issues. However, I think the proposed method shares a lot of similarity with label/model smoothing[1,2]. Also, the objective function eq8 basically is the same as Cosface loss function [3]. \n\n1) For eq. 1 and section 3.2, the authors state that the proposed method tends to produce the model prediction Q(y) close to P(y)-m instead of P(y). I think this is also the claim of label smoothing which avoids the model to output overconfident predictions w.r.t ground truth. \n2) For eq.8, I would say the objective function is almost exactly the same as cosface loss ([3] eq.4) where s is the scale factor, m is the margin.  The cosface loss improves the model inter-class separability by setting a margin. I am not sure whether the proposed method benefits from inter-class separability or claimed label smoothing. \n\nOverall, though the paper achieves good performance in several adaptive attacks benchmarks, I think the novelty of this paper is limited. Specifically, it shares a similar motivation and objective function to label smoothing and cosface. I hope the authors can elaborate on the difference between your work and label smoothing.\n\n[1] Regularizing Neural Networks by Penalizing Confident Output Distributions, Gabriel Pereyra et al.\n[2] Confidence Regularized Self-Training, Yang Zou et al.\n[3] CosFace: Large Margin Cosine Loss for Deep Face Recognition, hao et al.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "The authors present a new inference-SCE loss in order achieve a higher robustness to adversarial attacks. \nThey evaluate their model based on 2 datasets and compare their approach to several well known baselines. \n\nThe works seems convincing, even though I would have liked to see more extensive experiments on more complex dataset than CIFAR10 and MNIST.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}