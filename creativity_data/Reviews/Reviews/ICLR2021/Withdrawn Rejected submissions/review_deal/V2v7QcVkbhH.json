{
    "Decision": "",
    "Reviews": [
        {
            "title": "Important topic is addressed, but the paper needs more work",
            "review": "Summary:\nThe paper studies data augmentation for few-shot image classification. The authors establish 4 possible modes for data augmentation that act on the level of support set, query set, or the level of tasks and analyze what contribution to the validation performance each mode has. Finally, the authors propose a way to combine existing data augmentation modes into a Meta-MaxUp data augmentation strategy, which is a direct extension of MaxUp to the level of tasks.\n\nStrengths:\n- The paper addresses a very important problem. Data augmentation is a simple way to increase the performance of vision systems almost for free and the fact that we still did not know how to do it properly for few-shot tasks is embarrassing. This paper is proposing a systematic way to address this problem.\n- I like the careful classification of existing data augmentation modes and studying each of them in order to determine what actually brings gains.\n- The results of analyzing the 4 augmentation modes are useful and encouraging. The fact that one needs to apply data augmentation on the level of queries is not an apparent finding and is useful to the community. Also, the fact that one can increase the performance of the learning systems by 3-4% just by doing data augmentation carefully is very important.\n\nWeaknesses:\n- The Meta-MaxUp contribution is weak. Comparing to using standard data augmentation techniques for query set and data augmentation on the task level, the relative gain of Meta-MaxUp in Table 4 seems insignificant at most. Because of that, it seems like the main contribution of the paper fails to increase the accuracy on top of standard data augmentation.\n- For that reason, the novelty is limited. The authors use existing data augmentation techniques and show how to improve meta-learning performance by stacking them in a correct way. This is not a bad point per se, however, if the paper is aiming to provide an ultimate way to perform data augmentation for few-shot learning, it is not convincing.\n- Related work is incomplete. It must cover meta-learning methods, the classical ones, and the ones used in the experiments, standard data augmentation, and data augmentation for meta-learning.\n- The opening of Section 3 gives an overview of meta-learning. This part is in my opinion not clear for readers not already familiar with meta-learning. My suggestion for the authors would be to re-write this part in a more traditional way and specify that all the tasks come from the same distribution (in your particular case they are simply subsampled from the same dataset), specify what image set consists of what images.\n- The last paragraph in Section 3.1 you say “Existing meta-learning algorithms for few-shot image classification typically use horizontal flips, random crops, and color jitter on both support and query images”. Citations of the existing works are missing.\n\n\nSuggestions:\n- Enrich related work and improve explanations in the paper.\n- In my opinion, the paper would be more compelling as an analysis-paper, rather than a method-paper. Going away from selling Meta-MaxUp as the ultimate augmentation strategy for meta-learning and spending more time analyzing different modes of existing augmentation may be a better idea in the case of this paper. Of course, Meta-MixUp can still be used to maximize the performance and should be present as an additional experiment in the paper as a sort-of \"Combining our knowledge together\" section.\n- As additional points for analysis, it may be interesting to see why exactly certain augmentation modes and why some others don't. \n- Add a larger additional dataset, like MetaDataset.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good experimental paper, but lacks in-depth analysis.",
            "review": "The authors study the way of data augmentation for meta-learning. Four modes of DA are considered: support augmentation, query augmentation, task augmentation, and shot augmentation. For each individual mode, query augmentation achieves the best improvement over baseline without any DAs. Combining augmentations may degrade the performance. Later the authors propose Meta-MaxUp which is generalized from MaxUp for non-few-shot classification task, and empirically show that it narrows the generalization gap.\n\nPros:\n+ The authors give a comprehensive study of data augmentation methods for meta-learning.\n+ The proposed Meta-MaxUp further improve the performance after adapting to existed four meta-learners.\n+ This work may shed some light on future meta-learning methods in the choices of data augmentations.\n\nCons:\n- For experiments in Sec 4.1 to 4.4, can consistent empirical results be found for mini-Imagenet?\n- The authors show empirical results and conclude some empirical findings/conclusions. But it seems to lack some in-depth analysis of why certain DA ways work and why some don't work. \n\nOverall, I think this paper did a good job of summarizing the experiments. But it seems to lack some in-depth discussion on analyzing the reason of the experiment results. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Data augmentation adapted to few-shot learning significantly improves performance",
            "review": "This paper shows how data augmentation can be used in different ways for few-shot learning and that the classical way of using it, it is not optimal. Additionally, this work shows that Max-Up (a recently proposed data augmentation approach) improves also few-shot learning. Results show that the combination of the two contributions boosts the performance of most of the recent few-shot approaches on CIFAR-FS and mini-Imagenet.\n\nQuality and Clarity:\nThe paper is well written and presented. There are some minor issues with the presentation of the results. \n\nOriginality and Significance:\nThis is an incremental work which shows that a better use of data augmentation can improve results on few-shot detection and shows that for best accuracy, the right data augmentation plays an important role.\n\nPros:\n- Good and clear presentation \n- Results comparable with the state-of-the-art\n\nCons:\n- As only some combinations of data augmentation are presented (e.g. tab 2), the presentation of the results can be biased. Specifically, it is not clear if Query augmentation is always better  than Support+Query augmentation as usually done (only the case with CutMix is shown). Alsi, in Table 4, authors present results with baseline without DA, their improved DA and Meta-MaxUp. However, it is not clear what is the performance of the methods for standard DA. This would give a better idea about the real amount of improvement obtained by the paper.\n\n\nAdditional comments:\n- From the last sentence in sec. 4.2 it seems that using support + query data augmentation might be detrimental to few-shot learning, but in 4.3 this is showed only for the case of CutMix.\n- In Tab. 2, the improvement of shot augmentation is limited.\n- Last sentence pag.5, the authors says that baseline augmentation do not prevent over-fitting in Fig.1 (a). I would better say that the different regularization techniques have different levels of overfitting. Even if MaxUp highly reduces, the model is still overfitting as training accuracy is higher than validation accuracy.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting but rushed paper",
            "review": "This paper applies the data augmentation technique to the field of meta-learning. The authors propose four modes of data augmentation for meta-learning in terms of the various stages, i.e. support augmentation, query augmentation, task augmentation, shot augmentation, by using different existing data augmentation techniques.\n\n\nStrengths of the paper:\n\n1. Exploring the data augmentation technique for meta-learning, which is an important interesting direction.\n\n2. The experimental result seems to be promising.\n\n\nWeaknesses of the paper:\n\n1. The writing of the paper needs to be significantly improved. A number of grammar and typos can be easily found, which lower the quality of this submission. And it is often hard to follow the paper, since of the lack of reasonable organization and structure and unclear notations. \n\n2. And some augments are incorrect. For example, in the abstract, \"Conventional image classifiers are trained by randomly sampling mini-batches of images.\" is not true, since \"conventional classifiers\" are ambiguous, not all of them needs randomly sampling mini-batches, e.g. SVM.\n\n3. The paper should be self-contained. Some key notations and descriptions are unclear. For example:\n\n   1). Section 3 line 1, \"archetypal meta-learning algorithm \" that was first mentioned in the paper, but a proper citation is missing.\n   2). \"Adopting common terminology from the literature, \" but no reference is cited for referring. \n   3). Since the lack of good organization, missing references, and unclear notations, the general readers are hard to follow the key component of the proposed model.\n\n4. According to the paper, \"the model is evaluated on *query* data\", it is really hard to make sense of whether it is meaningful to make a comparison of the query augmentation with other augmentations, as shown in Table 6 of the paper. Although the authors conclude that \"query augmentation is critical\", did they evaluate in a fair manner?\n\n5. The technical contribution is limited. All the data augmentation methods, including the MaxUp method, are all simply applications of these models.\n\n6. More existing data augmentation baseline of meta-learning should be compared.\n\n   - e.g. Mai, Zhijun, et al. \"Metamixup: Learning adaptive interpolation policy of mixup with meta-learning.\" *arXiv preprint arXiv:1908.10059* (2019).\n\n  \nOverall, it is a good exploratory work, but due to the lack of presentation and limited technical contribution, it is still far below the acceptance threshold.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}