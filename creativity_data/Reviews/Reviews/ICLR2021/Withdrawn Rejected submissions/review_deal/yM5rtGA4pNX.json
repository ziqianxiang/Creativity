{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Review #3",
            "review": "The authors propose a method to generate singing voices in high resolution e.g. 48kHz. The paper provides a recipe to do so given. the music score input. The work proposes a novel sub-frequency GAN for capturing a wider range of frequencies for a Mel-spectrogram generation. Specifically, the authors propose a three-discriminator set up to capture the predicted mel-spectrogram with high-frequency detail. The authors proposed to use three different sized windows for their neural vocoder motivated by and experimentally verified to capture both low and high-frequency patterns present in the raw audio. The long receptive field also allows authors to synthesize long vowel sounds without artifacts. Additionally, authors use pitch and voiced/unvoiced flag to condition the neural vocoder since they are more suited for singing synthesis. The authors conduct an ablation study showing the importance (and consequent change in the quantitative metric) for the key design choices they make in the paper.\n\nPros:\n1. Ablation study is helpful in understanding the contribution of key choices made for the paper.\n2. The results from the paper show improvements over the chosen baseline.\n\nCons:\n1. It is difficult to understand the novelty of the proposed approaches since their relationship with relevant prior work has not been discussed. (See below for clarification questions)\n\nHow does ML-GAN approach related to GAN-TTS (https://arxiv.org/abs/1909.11646) which has discriminators operating on random windows of different sizes? How does it relate to multi-scale discriminator in MelGAN (https://arxiv.org/abs/1910.06711)?\n\nHow is mel-frequency computed exactly? What are the cut-off frequencies used?\n\nHow does the design choice to use F0/VUV-flag relate to the previous TTS papers which used pitch in the conditioning of the neural vocoder e.g. in Deep Voice 2 (https://arxiv.org/abs/1705.08947) and Char2wav (https://openreview.net/forum?id=B1VWyySKx)?\n\n\nOverall, I think the paper is marginally below the acceptance threshold in the current form. However, satisfactory answers to the questions above and consequent re-writing would make me vote for acceptance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Needs more description about motivation to have fast synthesis for SVS.",
            "review": "This paper applied non-autoregressive neural TTS architecture \"FastSpeech\" to singing synthesis and introduced changes to make it work well for HiFi audio (i.e., 48kHz sampling).  The changes include the use of F0 and voiced/unvoiced decision as output, GAN discriminator to subband mel-spectrogram, and multi-length GAN in time-domain audio signals.  The proposed HiFiSinger in 48kHz got MOS=3.76, which was significantly better than the baseline in 48kHz (MOS=3.44).  Technical details and model architecture are available in the body of the paper, then more details are provided in the appendix.  Audio samples are provided in the demo page.\n\nThere is a fundamental question where the reviewer would like to ask; the authors emphasizes \"fast\" generation but also emphasizes \"high quality\".  Why do you need both?  What kind of application are you thinking about?  For creative application like singing synthesis, the quality is more important than speed, isn't it?  If so, isn't it better to use a different type of neural vocoder than the current one?  It is great if the authors can make the motivation clear.  Currently the motivation behind \"fast generation for singing synthesis\" is unclear.\n\nComments:\n- As the authors noted in the paper, neither using 48kHz sampling nor F0 as acoustic feature is novel. \n- It is unclear what kind of loss was used for pitch and V/UV.  pitch is typically discontinuous (continuous value in voiced frames but 0 in unvoiced frames), so did you interpolate unvoiced frames? \n- Table 6 is interesting.  Tacotron TTS models often use 50ms/12.5ms, whereas parametric TTS uses 25ms/5ms.  50ms can give higher frequency resolution but less time resolution.  The higher frequency resolution is helpful to capture pitch information in spectrogram.  It is interesting that 20ms/5ms gave better results than 50ms/12.5ms.  I'm wondering if this is thanks to the use of F0; F0 can give the information about harmonics thus high frequency resolution is not required.  So it is great if you can include the analysis without F0.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #1",
            "review": "Summary:\n- The authors proposed HiFiSinger, a Singing Voice Synthesis (SVS) system that can produce 48kHz raw audio. Both the acoustic model and vocoder consists of a parallel, GAN-based model, which enables fast training/inference and high-fidelity synthesis. The authors first construct the baseline with SVS version of FastSpeech and Parallel WaveGAN, and then further improve the quality with careful design choices.\n\nKey strength of the paper:\n- Intensive amount of ablation study (Table 2~7) justifies the design choices of HiFiSinger.\n- Rather than simply adopting the TTS models for SVS, the authors pointed out the domain gap between TTS and SVS, and then use their insight to design HiFiSinger.\n\nMain weakness of the paper:\n- The paper should be an important milestone for SVS research, however, it lacks contribution and discussion from a machine learning perspective. When compared with previous works on SVS and TTS, only SF-GAN and ML-GAN was new. Furthermore, the multi-scale adversarial training was already proposed by several works of literature across domains [1, 2].\n- The paper is missing some important details for reproducibility. Please refer to the \"Questions\" section for details.\n\n\nQuestions:\n- Mel-spectrogram computation:\n - What was the frequency range of mel filterbanks?\n - Was it a log mel-spectrogram (as Tacotron2 used) or not?\n- Receptive field:\n - The authors mentioned the use of a larger receptive field; does it only applies to the vocoder part?\n - What previous model is being compared with HiFiSinger by saying \"larger kernel size\"?\n- Forced aligner\n - Was HMM-based forced aligner reliable enough to extract alignment from singing dataset? It should be noted if some audio segments were discarded in the case of failure of the forced aligner.\n- Audio upsampling:\n - In Table 1, the reviewer cannot clearly understand the difference between \"Baseline (24kHz)\", \"Baseline (24kHz upsample)\", and \"Baseline (48kHz)\". What kind of upsampling algorithm was used to upsample the results from \"Baseline (24kHz)\" to 48kHz?\n- SF-GAN ablation study:\n - What kind of loss function is applied when no discriminator is used (0 SF-GAN)?\n - How can the authors conclude that the diverse frequency details could not be modeled from increasing the number of mel bins, by showing the insignificant CMOS gain (Table 3)? It would be better if the corresponding mel-spectrogram comparison is added to the appendix.\n- Details on the loss function:\n - To the reviewer's understanding, the SF-GAN was trained with both adversarial loss and F0 regression + V/UV classification loss. How were they compounded? In other words, what weights were used for the weighted sum of loss terms?\n - What about ML-GAN? How were the adversarial loss functions from ML-GAN and Parallel WaveGAN compounded?\n- Mel-spectrogram comparisons on the appendix:\n - In Appendix A.5 (Figure 5), the reviewer could not spot the differences between the ground-truth, HiFiSinger, and F0 & V/UV ablation. Please elaborate on them, as the authors did in Figure 4.\n - In appendix A.6 & A.7 (Figure 6 & 7), the y-axis on the right side looks strange. How can the frequency grow linearly while the corresponding mel filterbanks grow on a log scale?\n\nInitial Rating: 3 (Clear rejection)\n\nExplanation of Rating:\n- The reviewer would like to appreciate the amount of effort done in this work. However, at this point, the writing is not clear enough to ensure reproducibility, and the contribution/discussion on the machine learning perspective is too low to meet the demand of the general ICLR audience.\n\nMinor comments that did not impact the score:\n- If available, it is generally recommended to cite the conference version of the paper instead of its arXiv preprint. The reference entry for Wang et al. (Tacotron) might be replaced with its Interspeech 2017 version.\n- It would be better if the authors could add the term SF-GAN, ML-GAN to the caption of Figure 1.\n- The abstract is too long to attract readers with the author's main point and can be made shorter. (But it's up to the author's decision to revise the abstract or not)\n\nUnfortunately, the reviewer's evaluation may miss the important points of the paper in the following perspectives:\n- The reviewer is not a Chinese Mandarin speaker; thus the reviewer was not able to correctly comprehend the audio samples.\n\nReferences:\n[1] Kumar, Kundan, et al. \"Melgan: Generative adversarial networks for conditional waveform synthesis.\" Advances in Neural Information Processing Systems. 2019.\n[2] Shaham, Tamar Rott, Tali Dekel, and Tomer Michaeli. \"Singan: Learning a generative model from a single natural image.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results, but limited scientific significance",
            "review": "Summary: This paper describes the design of HiFiSinger, a singing voice synthesis (SVS) system built on FastSpeech and Parallel WaveGAN, with several modifications that empirically improve the system. The modifications include multiple mel spectrogram discriminators (\"SF-GAN\"), multiple time-domain discriminators (\"ML-GAN\"), and extra features (F0, V/UV flag). Audio samples are provided in 24 kHz and 48kHz, achieving notable improvements over benchmark audio samples.\n\nPros:\n\n1. The system created in the paper works relatively well, judging by the audio samples; results are pleasant with some but limited audio artifacts.\n\n2. Several important dataset and task-specific modifications are made, including multiple frequency-domain and time-domain discriminators and F0/V/UV extra features.\n\n3. MOS evaluations are done to demonstrate that the modifications are helpful over the baseline.\n\nCons:\n\n1. The novelty of the contributions is limited. Specifically:\n\n(a) F0 and voiced/unvoiced input features are common and have been used often in neural TTS applications, from older models such as Char2Wav [1] and Deep Voice [2] to recent publications such as Mellotron [3].\n\n[1] https://openreview.net/forum?id=B1VWyySKx\n[2] https://arxiv.org/pdf/1702.07825.pdf\n[3] https://arxiv.org/pdf/1910.11997.pdf\n\n(b) Multiple time-domain discriminators are also common practice for voice synthesis with GANs, such as MelGan [4], GAN-TTS [5], or HiFi-GAN [6]. \n\n[4] https://arxiv.org/abs/1910.06711\n[5] https://arxiv.org/abs/1909.11646\n[6] https://arxiv.org/pdf/2006.05694.pdf\n\n(c) Multiple frequency-domain discriminators are also common practice, such as HiFi-GAN [6] or this CycleGAN for VC [7]. (Other prior art also exists.)\n\n[7] https://arxiv.org/abs/1804.00522\n\n2. The comparison to existing models is weak. A baseline is provided, but this baseline is not a well-described model which exists in the literature, and nothing but this baseline and HiFi-Singer is presented. Additionally, only a single dataset is used, which is the dataset that HiFi Singer was created for; no open commonly used dataset is analyzed. The dataset is internal and not released.\n\nRecommendation: Reject. Although results are good, this is clearly a tech report about the creation of a system on a given dataset using well-known techniques, rather than a research paper with novel contributions to the field.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}