{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "First, I'd like to thank both the authors and the reviewers for extensive and constructive discussion. The paper proposes a generalization of SAC, which considers the entropy of both the current policy and the action samples in the replay pool. The method is motivated by better sample complexity, as it avoids retaking actions that already appear in the pool. The paper formulates a theoretical algorithm and proves its convergence, as well as a practical algorithm that is compared to SAC and SAC-Div in continuous sparse-reward tasks.\n\nGenerally, the reviewers found the method interesting. After rounds of discussion and revisions, the reviewers identified two remaining issues. Theoretical analysis still requires improvement and the positioning of the paper is not clear. Particularly, the method is motivated as an exploration method, and it should be evaluated as such, for example, by comparing to a more representative set of baseline methods. Therefore, I'm recommending rejection, but encourage the authors to improve the work bases on the reviews, and submit to a future conference."
    },
    "Reviews": [
        {
            "title": "Novel idea with some clarity and technical issues",
            "review": "This paper considers the exploration efficiency issues in off-policy deep reinforcement learning (DRL). The authors identify a sample efficiency limitation in the classical entropy regularization, which does not take into account the existing samples in the replay buffer. To avoid repeated sampling of previously seen scenarios/actions, the authors propose to replace the current policy in the entropy term with a mixture of the empirical policy estimation from the replay buffer and the current policy, and term this approach as sample-aware entropy regularization. The authors then propose a theoretical algorithm called sample-aware entropy regularized policy iteration, which is a generalization of the soft policy iteration (SPI) algorithm, and show that it converges assuming that the empirical policy estimation is fixed. A practical algorithm based on the sample-aware entropy regularized policy iteration, called Diversity Actor-Critic (DAC), is then proposed. This algorithm is a generalization of the well-known soft actor-critic (SAC) algorithm. Finally, numerical experiments show that DAC outperforms SAC and other SOTA RL algorithms, and some ablation studies are also provided to demonstrate the effect of hyper-parameter choices in DAC.\n\nIn general, the approach is novel to my knowledge and the high level idea of using mixed policies in the entropy regularization to avoid repeated sampling and encourage unseen scenarios/actions is also interesting and reasonable. However, there are some clarity and technical issues that should be addressed and improved, as listed below:\n1. The authors study finite horizon MDPs, for which the optimal policy should be non-stationary in general. However, the authors only consider stationary policies. Instead, the authors should either change the underlying setting to infinite horizon MDPs or consider non-stationary policies.  \n2. In (2), $s_t$ should be replaced by an arbitrary $s$ in the state space. Otherwise there may be contradicting definitions of the policy $q$ if $s_t$ and $s_{t’}$ are equal for some two different timestamps $t$ and $t’$. And in (3), it is better to write the $q_{\\rm target}^{\\pi,\\alpha}$ in the entropy term as $q_{\\rm target}^{\\pi,\\alpha}(\\cdot|s_t)$, to be consistent with (1). \n3. It’s not very clear why the authors propose to estimate $R^{\\pi,\\alpha}$ with some (neural network) parametrized $R^{\\alpha}$. The authors mention that one can only estimate $R^{\\pi_{\\rm old},\\alpha}$ for the previous policy $\\pi_{\\rm old}$ in practice. However, since in $R^{\\pi,\\alpha}$, all the quantities including $\\pi$, $q$ and $\\alpha$ are known, I’m confused why one cannot evaluate it directly. On a related point, it’s not very clear why the estimation procedure for $\\eta$ (the parameter of $R^{\\alpha}$) using hat $J_{R^{\\alpha}}(\\eta)$ makes sense. The form of hat $J_{R^{\\alpha}}(\\eta)$ looks like an entropy term extracted from the $J_{\\pi_{\\rm old}}$ function, but it’s unclear why maximizing it gives a good estimation of $R^{\\pi,\\alpha}$. Some more explanations are needed. \n4. There seem to be several errors (at least inaccuracies) in the proof of Theorem 1 (in the Appendix). Firstly, in the proof of Lemma 1, the term “correctly estimates” is not very accurate, and should be simply stated as something like “equals”. Also, it’s not very clear when the assumption $R^{\\alpha}\\in(0,1)$ can be guaranteed (e.g., using Gaussian/soft-max policies?). Secondly, in the main proof of Theorem 1, convergence of $Q^{\\pi_i}$ to some $Q^{\\star}$ is correct, but this does not immediately imply convergence of $J_{\\pi_i}$, let alone the convergence of $\\pi_i$ to some policy $\\pi^\\star$. On a related point, the proof for the optimality of $\\pi^\\star$ in terms of $J$ is not clear. In particular, it is not clear why (7) and Lemma 2 implies the chained inequality $J_{\\pi_{\\rm new}}(\\pi_{\\rm new})\\geq J_{\\pi_{\\rm old}}(\\pi_{\\rm new})\\geq J_{\\pi_{\\rm old}}(\\pi_{\\rm old})$. I understand that the authors may feel that the proofs are similar to that of SPI, but indeed there are several significant differences (e.g., the definitions of $\\pi_{\\rm new}$ and $J_{\\pi}$). More rigorous proofs are needed for these claims. \n5. In Section 5, it is unclear why the authors need to include the parameter $c$, how to choose it and what it serves for. Some additional explanations are needed. \n6. On a high level, the eventual goal of the paper is not clearly stated. From the experiments, it seems that the average episode reward is the actual goal of concern. However, the problem setting and the theoretical results (Theorem 1) seem to indicate that the problem of concern is the discounted entropy regularized reward. Some discussion about this is needed. \n\nFinally, here are some more minor comments and suggestions:\n1. In the analysis of the sample-aware entropy regularized policy iteration, the authors assume that $q$ is fixed. However, in practice, especially in the long run (as concerned in the analysis), such an assumption will not hold (even in just an approximate sense). Can you still obtain some sort of convergence when taking into account the $q$ changes?\n2. Why do you need to divide the reward and entropy regularization term in $Q^{\\pi}$ by $\\beta$? \n3. It’s better to write out the “binary entropy function $H$\" explicitly for clarity. \n4. At the beginning of Section 4.3, “propoed” should be “proposed”, and In Section 5, “a function $s_t$” should be “a function of $s_t$”. \n5. Some high level explanations on why the $(1-\\alpha)$ term can also be dropped in (8) will be helpful.\n6. The theoretical results only show that the algorithm converges, which is already guaranteed by SPI. Is there any possibility to show that there is also some theoretical improvement?\n\nSo in short, the paper proposes an interesting modification of the max-entropy regularization framework, but contains several technical and clarity issues. Hence I think it is not yet ready for publication in its current form. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration",
            "review": "This paper proposes diversity actor-critic (DAC) for exploration in reinforcement learning. The main idea of the proposed algorithm is to take advantage of the previous sample distribution from the replay buffer for sample-efficient exploration. The authors provide convergence analysis of DAC and conduct empirical investigations on several benchmarks. \n\nPros \n\nThe idea of using previous sample distribution from the replay buffer for better exploration seems interesting. The proposed exploration bonus $\\mathcal{H}(q^{\\pi, \\alpha}_{\\text{target}})$ can be decomposed into three terms as shown in (4). Since the last term does not depend on $\\pi$, intuitively this exploration bonus encourages the exploration of $\\pi$ (first term), and tries to make $\\pi$ different with previous policies approximated by the replay buffer (second term). The authors provide a reasonable method to optimized the proposed objective, which can be naturally combined with state-of-the-art algorithms like SAC. \n\nCons\n\n1. Theorem 1 seems misleading. The diverse policy iteration can only guarantee the converge to the optimal policy with respect to the regularized value function, not the optimal policy of the original problem. The authors should make the definition of $\\pi^*$ clear. \n\n2. It’s hard to see the motivation of using a mixture of $q$ and $\\pi$. Could you explain more about this choice?  \n\n3. It’s worth to provide the results of SAC-div with JS divergence as it’s more similar to the proposed objective (4).\n\n4. The experiment results are not convincing enough as some important baselines are missing. For example, [1] also uses a mixture of previous polices to encourage exploration with strong theoretical guarantees. I believe this is closely related to the proposed algorithms. \nAlso, the experiment results are not very promising compared with the baseline algorithms based on SAC. \n\n[1] Hazan, E., Kakade, S., Singh, K. and Van Soest, A., 2019, May. Provably efficient maximum entropy exploration. In International Conference on Machine Learning (pp. 2681-2691). \n\nOther suggestions\n\nThe main idea of the proposed method is to make the current policy different with previous policies. The paper uses a nonparametric method  (2) to approximate the previous policies. I think it’s also worth to try parametric $q$. For example, $q$ could be learned by fitting the replay buffer, or use a moving average of previous policies. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Efficient exploration for offline policy learning",
            "review": "Summary\n\nThis paper proposes a novel exploration method in off-policy learning. Compared to previous methods which do not take care into account the distribution of the samples in the replay buffer, the proposed method maximizes the entropy of the mixture of the policy distribution and the distribution of the samples in the replay buffer, hereby making exploration efficient.\n\nReasons for score\n\nI vote for accepting the paper. The paper proposes an intuitive and efficient exploration method that generalizes existing methods, including them as special cases. The authors provide a theoretical guarantee (Theorem 1) that the policy obtained from the iteration of evaluation and improvement under this new regime converges to the optimal policy.  The presentation is clear and concrete, and the experiments are convincing.\n\nPros\n\nThe experiment results are not limited to just showing that the proposed method achieves higher reward than state of the art methods, but they also address important questions such as \n(i) the pure exploration when rewards are assumed to be 0\n(i) the necessity of the adaptation of alpha, the parameter that controls the ratio of the current policy to the sample distribution in the target distribution.\n(ii) the effect of controlling alpha, the entropy weighting factor beta, and the control coefficient c (required for adapting alpha), and also, the robustness of the proposed method to these parameters.\n\nThe authors have stated the experiment details clearly and the results are convincing.\n\nCons\n\nThe methodology part in Section 3 and 4 could be improved. Some notations are confusing.\n(a) In Section 3, the policy \\pi is defined as a function from S to A. It looks like it is a  fixed function over time.\n(b) An explanation on the definition of J_{pi 1}(pi 2) would be helpful,e.g.,  J_{pi 1}(pi 2) is value of J(pi_2) computed under pi_1.\n\nMinor Comments\n\nIt would be good to add the line of SAC and SAC-Div in Figure 5 (c ) to show that the performance of DAC with adaptive alpha is robust to control coefficient c. For now, one has to go back to Figure 4 (b) to check that most of the case (when c is not 0), DAC with adaptive alpha performs better than SAC and SAC-Div.  \n\nIn Section 6 in the 5th line, J(\\pi) should be specified as “J(\\pi) in (1)”. It is done in the next sentence, but I prefer that it is done when it first appears. It was confusing\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting, but poorly motivated and positioned",
            "review": "### Summary\n\nThe paper proposes DAC, an actor-critic method exploiting the replay buffer to do policy entropy regularisation. The main idea of DAC is to use the data from the replay buffer to induce a distribution  $q(\\cdot, s_t)$ and replace the entropy part of the Soft Actor-Critic objective with a convex combination of $q$ and $\\pi$. This results positively on exploration properties and leads to sample-efficiency gains on some of the considered MuJoCo benchmarks.\n\n### Pros\n- Formulating the diversity using the entropy of the replay buffer frequences is an interesting idea.\n- Using the convex combination of $q$ and $\\pi$ for entropy regularisation is a nice way of generalising SAC for the considered purpose.\n- The paper shows the convergence of their method to an optimal policy and derives a surrogate objective whose gradient direction coincides with the original one, but which can be practically used. (However, I have not checked the proofs which are in the appendix).\n\n### Cons\n- It is not clear, what is the problem the paper tackles. Is it exploration? Is it a generic RL setup? What kind of problems is DAC good for?\n- If DAC is for improving exploration, then it should be compared with other exploration methods, not with vanilla SAC. Comparison with RND should not be in the appendix and there should be more details on this. Related work in this case should have a paragraph on exploration methods in RL.\n- The paper is based on assumptions not challenged/tested by the authors, e.g. policy entropy regularisation is inefficient, because it does not take the distribution of the samples into account.\n- The paper focuses more on the technical details of the solution rather than justifying the assumptions and making the research question clear.\n\n### Reasoning behind the score\n\nI believe, the paper has a great potential. However, at the moment I vote for rejection. The paper has to have a clear research question and its motivation. This should define the experimental part of the work. Lack of a clear positioning makes it unclear if the baselines of the experimental sections are the right ones and whether the claims have been properly supported by the results.\n\n### Questions to the authors\n- Can you formulate the exact problem you are solving?\n- How can you justify the claim that 'entropy regularization is sample inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account.\n- \"it is preferable that the old sample distribution in the replay buffer is uniformly distributed\". Why is it true? Doesn't prioritized experience replay refute this claim?\n- You define $\\beta$ in Equation 1 in $(0, \\infty)$, can it really be infinite?\n- \"The rationale behind this is that it is preferable to have as diverse actions stored in the replay buffer as possible for better Q estimation in off-policy learning.\" What are the assumptions for this? Do you care more about better Q estimates or finding an better policy faster? How can you support your rationale?\n- In section 4.1. you define the target distribution as a convex combination of $\\pi$ and $q$. You assume that the buffer is generated by $q$. Does such a policy always exist? What are the assumptions for this?\n- You prove the convergence of your algorithm (I did not check the proof in the appendix), what are the assumptions for which the convergence is guaranteed?\n- Why do you use sparse/delayed MuJoCo benchmarks, but not the original ones?\n- The variance across different seeds seems to be huge for your method (as well as for the others). What do you think is the reason behind this? This also happens for the pure exploration task in 6.1, why do you think it happens?\n- For the adaptive $\\alpha$ case, you restrict the range of possible values, what is the reasoning behind the left boundary?\n- I think your paper can find an important application in Imitation Learning or off-line RL. Have you considered this? Are you aware of works which do something similar in those subfields?\n\n### Additional feedback not affecting the score\n- \"Reinforcement learning aims to maximize the discounted sum of rewards...'. Should be 'expected discounted sum'.\n- There should be a distribution over initial states under the expectation sign in 3.1.\n- 'A is the continuous action space'. This is not true for the general MDP definition, specify that this is specific for your paper.\n- Section 3.1, a policy is a mapping from states to distribution over actions, not to actions.\n- In off-policy, we can learn from any other samples, not only from 'previous samples' from our policy.\n- typo \"propoed\" at the bottom of page 4.\n- Equation 9 does not have a left hand side.\n- DAC acronym has been used in RL. I would choose a different one to avoid confusion.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}