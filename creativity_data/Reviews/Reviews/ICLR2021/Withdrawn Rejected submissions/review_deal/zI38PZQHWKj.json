{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Motivated by (1) the problem of scaling up optimal transport to high-dimensional problems and (2) being able to tolerate noisy features, this paper introduces a new optimization problem that they call feature-robust optimal transport where they find a transport plan with discriminative features. They show that the min-max optimization problem admits a convex formulation and solve it using a Frank-Wolfe method. Finally they apply it to the layer selection problem and show that it achieves state-of-the-art performance for semantic correspondence datasets. \n\nThe reviews were mixed for this paper. The main negative, which was brought up in all the reviews, is the lack of novelty compared to earlier methods like SRW which already combine dimensionality reduction and optimal transport. The new method in this paper still does have value since it can scale up to larger dimensional problems. It would have been nice to have a wider range of experiments, which would present a more compelling case for its applicability. Another reviewer brought up a correctness issue, however it is not clear if this is actually a bug or merely a misunderstanding about how the pieces in the overall proof fit together. In any case, the reviewers pointed out various places where the writing could be improved. "
    },
    "Reviews": [
        {
            "title": "Seems a bit incremental in terms of novelty and contribution, but well-written overall.",
            "review": "This work proposes variants of robust OT/p-wasserstein-dist (3)/(4), where the ground cost is in some sense the maximum over costs with (prefixed) groups of features. The motivation is similar to that for feature selection: where perhaps only few of these groups of features are critical/sufficient for OT purposes. So it can also be understood as joint feature-group selection with OT. The resulting convex problem is proposed to be solved using FW, whose details are presented (including convergence).\n\nPros:\n1. Though similar in spirit to SRW, the proposed formulation has few advantages: a) allows any cost, b) convex c) FW leads to scalable solver etc.\n2. Overall, the paper is very well-written, with nice organization and sufficient details.\n\nCons:\n1. Pre-fixed groups, more importantly, non-overlapping groups seems restrictive, especially because feature selection with overlapping groups is well-studied. (e.g., https://hal.inria.fr/inria-00628498/document , https://papers.nips.cc/paper/4275-efficient-methods-for-overlapping-group-lasso.pdf ) among others.\n\nMajor Comments:\n1. Given SRW and other robust/min-max OT works, and multitude of feature-selection/group-lasso works, the novelty seems restricted. Even in terms of optimization, it seems a straight-forward application of FW. This seems to restrict the technical contribution. \n2. In section 5.2, I am assuming for FROT, all layers were used as input; whereas for SRW, only few are used. Is this the case? If so, perhaps a case of FROT which uses exactly same input as SRW must be included for a fair comparison (along with the FROT with all layers). The authors do seem to agree that the improvement is more because of this skew in inputs. It is will nice to clarify this.\n3. Why is that T is set in an adhoc manner? for example T=10 in synthetic and T=3 in real-world? why not fix or validate ? Also, convergence plots showing obj vs T as well as accuracy vs T might be insightful when included.\n4. It may also be insightful to visually see some critical examples of image/pairs that highlight why FROT may work better than SRW etc. (more like fig5 in appendix)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "FROT -- feature-robust optimal transport",
            "review": "The proposed framework FROT - feature-robust optimal transport - seeks to select feature groups to both speed up OT computation for high-dimensional data and make it more robust to noise. The exposition is generally clear. My main concerns are limited novelty and lack of extensive experiments.\n\nThe paper draws the contrast between prior work SRW that yields a discriminative subspace via dimensionality reduction, and offers a dual perspective to use feature selection instead. A thorough discussion on the pros and cons of feature selection vs feature dimensionality reduction would add insight. \n\nTraditional entropy-regularized OT regularizes using the entropy of the transport plan $\\Pi$, whereas FROT regularizes using the probability distribution $\\alpha$. One expects a discussion on the effect of this choice.\n \nCurrently, the optimization for the group selection is done independently of optimization that produces the features, for instance by the choice of a pretrained network in the semantic correspondence application. It's worthwhile to explore joint optimization of feature generation and selection for downstream tasks.\n\nFor the claim of robustness to noise, experiments on data of dimension higher than 10 would be desirable.\n\nThere should be more extensive experiments applying FROT to more tasks and compared with additional baselines. Figure 3 compares the objective scores of FW-EMD, FW-Sinkhorn with that of exact OT, there are many other OT algorithms, such as tree-based methods, as referenced in the paper, that can be compared with. These plots should also include variations of the metric across trials. Similarly for the semantic correspondence results in Table 1. \n\nThank you authors for your response.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "FROT is interesting but the analysis is suspicious",
            "review": "*Summary*:\nThe authors try to solve a special kind of high-dimensional optimal transport problem. Specifically, they consider the cases when features are grouped and the grouping is known a-priori. The authors formulate the problem into the feature-robust optimal transport (FROT) problem.\nThe authors propose two solving algorithms, one based on the Frank-Wolfe method, and one based on linear programming.\n\n*Pros*:\nThe connection to the feature group sounds interesting to me, as it has a natural connection to the structure of deep learning models.\nThe presentation (other than the introduction) is easy to follow.\n\n*Cons*:\nNote that the first point is the main contributing factor for my rating.\n\n1. Section 3.1 is very confusing, and it seems to me that the authors fail to establish the correct convergence guarantee.\nAs in page 4, the target is $min_{\\pi} max_{\\alpha} J(\\Pi, \\alpha)$ \nIf we fix $\\pi$, we can solve for the optimal $\\alpha$. Plug this optimal $\\alpha$ back in and we obtain $G(\\Pi)$.\nIntuitively one may choose to solve for $\\alpha$ and $\\pi$ alternatingly.\nHowever the convergence of $G(\\Pi)$ says nothing more than, in a fixed iteration, one can solve exactly for the optimal $\\alpha$ and up to $\\epsilon$ accuracy for $\\Pi$.\nWe still don't know if the solution of the algorithm indeed minimizes the said loss.\nI checked the proof of proposition 4. It just invokes the standard FW-convergence analysis from Jaggi 2013, and argue nothing about the alternative part. Note that, even though the two subproblems (for $\\alpha$ and for $\\pi$) can be solved almost exactly, it could be non-trivial to set up the convergence of the entire alternating algorithm.\nAlternatively, maybe the authors want to argue that solving $min_{\\pi} max_{\\alpha} J(\\Pi, \\alpha)$ is equivalent to solving $\\max_{\\pi} G(\\Pi)$. However, this is also not obviously true for me.\n\n2.  What are the other potential applications of FROT? While Semantic Correpondance is an interesting application, I find it hard to convince myself that FROT is better than Liu's 2020-CVPR work (requiring validation dataset is not a big problem - you can always to train-val split). With its similarity to group lasso, FROT might have more interesting applications.\n\n3. Presentation of the introduction can be improved. I find it hard to parse the introduction until I almost finished reading the entire paper. Putting figure 1 to page 2 only creates more questions in my head instead of offering intuitions. Also, it would be helpful if the author can list their contributions in a more organized way.\n\n4. I didn't quite get the high dimensional part. While 'high-dimensional' appears in the abstract, introduction, and conclusion section, I didn't find the correspondence in the main text.\n\n5. I didn't get the robust part, other than the empirical performance in the evaluation section.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}