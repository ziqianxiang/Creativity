{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies different properties of the top eigenspace of the Hessian of a deep neural network and their overlap. It raised quite a lot of discussion, which finally went in not very constructive way. The reviewers generally agree that the paper has potential, but the actual contribution is limited.\n\nPros: \n- The idea that top eigenspaces between different models have high overlap is interesting\n- The explanation that these structures can be explained by Kronecker-product approximation of the Hessian.\n\nCons: \n- The connection to PAC-Bayes is unclear and seems artificial.\n- Many of the related work is missing\n- The models and datasets are too simple, and general conclusions can not be made on such kind of models. Much more testing is needed to verify the claims, including state-of-the art architectures and datasets."
    },
    "Reviews": [
        {
            "title": "Review for \"Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks\"",
            "review": "## Summary\nThe paper studies the empirical properties of the Kronecker Factorization based approximation to the Hessian of the loss. The paper makes the following two empirical observations: \n1. Top eigenspace of the layer-wise hessian has a high overlap for neural networks trained with different initializations and hyper-parameter setting. \n2. Top eigenvectors of the layerwise hessian as a matrix is approximately rank 1. \nPaper makes the decoupling conjecture which states that $\\nabla^2_{z^{(p)}}\\ell(z, y)$, which is Hessian wrt loss evaluated at the output of layer p, is independent of $x_{(p)}x_{(p)}^T$, correlation of the input to the layer $p$. Under this assumption, the paper gives an explanation of why the above-stated phenomenon holds. Finally, the paper uses this approximation to optimize the PAC-Bayes bounds for the stochastic classifier where the eigenbasis of the proposed layer-wise Hessian is used for the covariance of the posterior. \n\n## Evaluation\n\nOverall, the paper is not clearly written. It really needs to improve on the notation and needs to explain the ideas more clearly. Starting from Eq(3) in the paper, we are computing a gradient wrt $v$ for the hessian, but it’s really not clear from the notation how the output of $f_\\theta(x)$ is dependent on $v.$ Writing this precisely would ease the burden on the reader. \n\nHaving said that authors do make new empirical observations with regards to the layer-wise hessian approximation but they don’t do a good job in explaining why these observations are relevant in a more general context. I have some questions for the authors to help me evaluate this work.\n\n1. How close is the layer-wise Hessian approximation to the true Hessian? There is no discussion in the paper about the approximation. \n2. A majority of the paper hinges on the decoupling conjecture. How close are the matrices in Eq(6) in terms of the actual norm on matrices? In the paper, we’re only given visual proof for the closeness in terms of the eigenvector correspondence matrices. Honestly,  I had a hard time following how it was computed.\n3. For the PAC-Bayes bound, how the posterior variance is parameterized? It’ll be useful to give a mathematical characterization of it. Is the prior variance wrt standard basis still? If so, how is the KL computed? I am happy to just see the expression because couldn't find it in the Appendix. \n\nMinor comments: \nPlease review the citations added in references. For many of the papers, only Arxiv posts are cited whereas the papers are published at other venues. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Attractive Narrative. Needs to better reflect prior research contributions",
            "review": "1.\tI want to congratulate the authors on producing an attractive narrative, with a very nice flow to it and a very clear heuristic content.  I believe if I were seeing these ideas for the first time, I would definitely find this presentation friendlier and clearer than if I learned it from some other presentations.\n2.\tThe topic is important. The analysis of the spectrum of the Hessian of the loss function gives us significant insight, and it is rather amazing and beautiful that the spectrum has definite mathematical properties which also have mathematical explanation. The narrative excels in making us feel this as something clear and natural.\n3.\tThis is my first time ever reviewing for conferences of this nature; I am a bit unclear about the level of contribution we are looking for. My understanding is that researchers in this field are writing numerous very short papers each year and a paper doesn’t need to represent more than an incremental bump over previous work.\n4.\tOne reason I bid to review this Mss. is that the topic is not new to me. In the last few years three separate researchers at my institution have written papers on this general topic; and so I feel more conversant with this topic than some other material I might have been assigned. So …\n5.\tI feel that this work may not know about or may not have fully assimilated the contributions by other authors. I can mention these papers:\n\narXiv:1901.10159 \nAn Investigation into Neural Net Optimization via Hessian Eigenvalue Density\n\tAuthors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao\nAbstract: To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments.\n\n\nSo arXiv:1901.10159 clearly calls out the phenomenon of batch norm changing the hessian. In comparison, the paper under review gives a heuristically very clear explanation of why such an effect should be present, i.e. why batch normalization should be effective at making the objective easier to optimize.  However, the paper under review does not fully discuss the above research contribution of arXiv:1901.10159,  and it would be very easy for a reader of this paper to imagine that the phenomenon being presented here originates with this paper.   I should point out that the authors of arXiv:1901.10159 have made company-wide presentations about that work. Since the company is Google, that means that their work is fairly well known.\n\n\narXiv:1811.07062  \nThe Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size\nAuthors: Vardan Papyan\nAbstract: We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits \"spiked\" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually\n\nTo my knowledge arXiv:1811.07062  is the first paper to show the empirical deep learning community that one can compute the eigenvalue density spectra of modern deepnet classifiers eg those of the kind that are in use on large datasets like imagenet and in real applications. The result of having the technology in arXiv:1811.07062  is that the author is able to show that some of the key features of eigenvalue spectra that were seen in small scale situations are also present at full scale. The examples shown in the paper under review, in contrast, are of quite limited scale, and to my understanding do not represent the current state of the art. Consequently, although tools re available to test out the authors’ ideas on realistic problems, we are left wondering what seen in these small examples might generalize. \n\narXiv:1901.08244 Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians\nAuthors: Vardan Papyan\nAbstract: We consider deep classifying neural networks. We expose a structure in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain \"averaging\" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes\n\nTo my knowledge arXiv:1901.08244 goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:1901.08244  shows that not only do the C class means influence the eigenvalues, but actually there are C(C-1) eigenvalues that have structure deriving from means. The examples shown in the paper under review, in contrast, are of quite limited scale, and in contrast don’t seem able to show the full structure which we now know to be present in deepnet spectra across a very wide range of networks and datasets at full scale.  However, the paper under review does not discuss the research contribution of arXiv:1901.08244,  and in consequence a reader could get the misleading impression that this paper’s heuristic explanations of the mean structure are the only knowledge we have about this phenomenon, when we have actually dramatically more information. \n\n\narXiv:2008.11865  Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra\nAuthors: Vardan Papyan\nAbstract: Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, \"spikes\", and small but distinct continuous distributions, \"bumps\", often seen beyond the edge of a \"main bulk\". The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets.\n\nTo my knowledge arXiv:2008.11865  again goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. arXiv:2008.11865  in particular discusses the KFAC approximation of Martens and Grosse and in fact claims, with substantial evidence gleaned from many realistic examples, that KFAC is not a good approximation. The paper under review doesn’t cite arXiv:2008.11865, but does make heuristic claims about the adequacy of the KFAC approximation.  However, the evidence presented is much weaker and the implications much more sketchy than what is discussed at greater length  in arXiv:2008.11865.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A review",
            "review": "### 1. Content summary\nThe authors study the layer-wise Hessian of several well-known vision networks on several vision classification datasets. They, to me be very surprisingly, observe that the subspaces formed by top eigenvectors of those layer-wise Hessians have non-trivial overlap between independently initialized and trained models. They explain this by proposing the usage of Kronecker factorization, whose validity they experimentally show. They trace the surprising top eigenspace overlap to the structure of the data and show that it weakens with the application of batch norm. They also show that this overlap can tighten PAC-Bayes bounds.\n\n### 2. Strong points\n* The paper is well written and motivated. I went from the intro to the conclusion in one go and felt I didn't have to jump around to fish out definitions etc.\n* The question is well motivated and interesting.\n* The proposed simplifications (Kronecker factorization) are validated on real networks.\n* The networks and data used are non-trivial and close to practice, which makes this paper better than many others in bridging the gap between theory and experiments.\n* The story fits nicely with existing observations about the effect of batch norm on class-specific structure of the Hessian.\n* The obvious application to PAC-Bayes is nice!\n\n### 3. Weaker points\n* The paper would be strengthened by a broader suite of experiments: add ResNets, WideResNets, let's see what skip connections do, try CIFAR-100 to see the effect of >10 classes. I am not suggesting you work on ImageNet, but more experiments of even the easier variety would make the paper even more convincing. Perhaps FASHION MNIST and SVHN might be quick enough to run?\n* I am missing a summary table or plot showing how well the approximations proposed match the reality for different architectures and datasets. This is a small detail, but I kept wondering how valid many things were. I was convinced by your detailed figures (e.g. Figure 3) and I am sure a figure like  that for each combination of net x architecture would clutter the paper, but a summary plot showing the match between the assumptions and reality would be very nice and convincing.\n\n### 4. Points to clarify\n* In Figure 4 the scales for the E[M] and H_L are different, the difference presumable coming from the scale of E[xx^T]. Is that the case?\n* Am I understanding it correctly that the top eigenvector overlap is driven mainly by the structure of the data that through E[xx^T] \\approx E[x]E[x]^T becomes almost rank 1?\n\n### 5. Potential papers to look at\nStiffness: A New Perspective on Generalization in Neural Networks by Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, Srini Narayanan (https://arxiv.org/abs/1901.09491) looks at the gradient tensorproduct gradient structure which is a part of the Hessian and its class dependence and training time dependence. It might be relevant.\n\n### 6. Summary\nThis is a good, well-written and motivated paper with a (for me) surprising observation and well-formed explanation for it. More experiments would make it even stronger, but I already like it as it is! ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting observations, but the paper lacks experiment interpretation and conclusions.",
            "review": "Summary:\nThe paper studies the structure of the Hessian matrix of loss functions by approximating Hessians using Kronecker factorizations. Combining the Kronecker factorization with PAC-Bayes, the authors provide a tighter bound on classification error. \n\nPros:\n- The paper contains many experiments.\n- Interesting observations on the models trained with batch normalization. \n- The improvement of classification error bound.\n\nConcerns:\n- I suggest adding the Conclusion section. This section should highlight the contributions and summarize the study, and the paper will certainly benefit from it. \n- The main paper contribution, optimized PAC-Bayes bound, is not emphasized enough; many important details are moved into the Appendix section, while most part of the paper is devoted to eigenspace overlap. While eigenspace overlap looks quite interesting, it does not give many insights. One of the most interesting properties of the Hessian structure is the gap in eigenvalues distribution around the number of classes, but this is prior work.\n- The figure captions are not detailed enough; many figures lack axis labeling. For example, it is not apparent to me what is illustrated in Figure 3. What do the axes depict? Similarly, for Figure 4.\n- The key concern is an incremental contribution comparing to Martens & Grosse (2015). While Martens & Grosse (2015) use the Kronecker factorization for training acceleration, your paper focuses more on eigenspace overlap, which is supported by many rigorous empirical results and illustrations, but not on the possible interpretations and insights.\n\nUPD:\nThank you for your answer and for addressing the points raised in the review. Still, I agree with the concerns raised by AnonReviewer4 on the small scale of the networks used in the experiments (e.g., LeNet, fc). I am still not convinced by the experiments and the excess of hardly understandable plots given to support the main bulk of the paper's claims. Therefore, I leave my rating the same.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}