{
    "Decision": "",
    "Reviews": [
        {
            "title": "From vocabulary entropy to NMT quality: many issues along the way",
            "review": "### Summary\n\nThis paper aims to find an automatic BPE vocabulary optimizer to replace hyperparameter search. It is formalized as an optimal transport problem between characters and tokens. The proposed solution, merging a large existing BPE vocabulary bottom-up, uses an information-theoretic motivated criterion (AMD) to determine a halting point. Furthermore, AMD is used as an indicator for best translation quality when search for the best segmentation. In experiments on multiple translation tasks with shared source and target vocabularies, they compare the new algorithm and tuning criterion against fixed BPE sizes and a random search.\n\nThe main contribution is the interpretation of segmentation as an optimal transport problem which allows to utilize an efficient solver in place of hyperparameter search.\nHowever, the results are not convincing, and the paper is overselling the results while oversimplifying the actual problem (details below). In combination with the math being not clear, and not fully motivated, I recommend to reject the paper.\n\n### Weaknesses\n- *Scope & Generalization*: Vocabulary generation is crucial for many NLP tasks (as described in the motivation), but only NMT is studied. Therefore, the scope of the paper appears more limited than it alludes to. Furthermore, it should be clearly stated that the approach is built on top of the BPE segmentation algorithm, and thereby restricted to whitespace-tokenized languages. In addition, only joint source-target vocabularies are studied, so it is not clear how to apply the approach to distinct vocabularies nor whether it would generalize.\n- *Related Work*: Relevant related work is not cited, and general claims about the state of tuning BPE sizes (\"rarely tried to search for [it]\") disregard the existance of previous studies (see below).\n- *Interpretation of Results*: The relation between AMD and BLEU is not as simple as stated. In fact, one could take a look at the same plots (Fig. 1) and reason that 2k BPE is the magic solution that in most cases yields the best quality and therefore should be generally the best. There is no clear correlation between AMD and BLEU, and arguing for AMD just because a high value usually yields high BLEU is not sufficiently supported by the results, since large vocabulary sizes in Fig 1 show almost no reduction in AMD but a steep decrease in BLEU. As shown in the rightmost plot in Fig 1, AMD is not necessary for high BLEU either (1k vocabulary size). In Table 1, Info-VOT outperforms the baseline in roughly half of the cases and cannot therefore not be claimed to be \"much better\" (Sec. 6). It is also unclear if differences in BLEU are significant.\n- *Clarity*: It is not clear from the Introduction that the proposed solution is tied to BPE segmentation (only appears in Sec. 3), and not a self-standing solution. The dependence on heuristic post-processing is only mentioned in the end and is not clear from the description of the algorithm. Some open questions regarding the math (see below). There is a lack of precision in the math (see details below).\n- *Reasoning and Assumptions*: It is assumed that a high BPC, and thereby maximum entropy in the vocabulary distribution, is desirable, but this assumption is not well supported nor motivated. To relate it to MacKay (2003) that is cited to ground the work in information theory, the optimal input distribution for compression is dependent on the type of the channel, and the input with maximum entropy is therefore not always the best, see Chapter 9 in MacKay (2003). Furthermore, source and target language distributions are merged into one, and there is no consideration for interference or conditional dependence in a parallel corpus.\n- *Evaluation*: Confounding factors like architecture depth (which has been shown to be highly relevant in related work (see below)), vocabulary overlap for multiple languages (which affects multilingual NMT quality), regularization (which might be more needed for large models with large vocabularies), corpus size (shown to matter in related work) are neither discussed nor empirically studied. In addition, linear search rather than random search should be employed as a baseline since relations between vocabulary size and translation quality appear to be mostly monotonic.\n- *Reproducibility*: The code is not published (nor promised), and implementation details for OT are missing to replicate the results.\n\n### Strengths\n- *Novelty and originality* of the approach. So far, there has been no work relating segmentation to optimal transport.\n- *Potential impact*: Eliminating hyperparameter search for optimal BPE vocabularies could save the field a lot of wasted compute. The reported speed gains over random search are promising.\n- *Multilingual*: The experiments are conducted on multiple language pairs and a multilingual setting as well. \n\n### Missing Related Work\n- Cherry et al. 2018 (https://www.aclweb.org/anthology/D18-1461.pdf) already found indicators that segmentation close to character-level are advantageous for deep translation models, and proposed an algorithm to optimize segmentation granularity for downstream quality. The also show that the advantage of character-level modeling diminishes with increased training data.\n- Similary, Kreutzer & Sokolov 2018 (https://arxiv.org/abs/1810.01480) proposed to learn the input segmentation end-to-end, and also found a qualitative advantage of character-level segmentation with deeper models.\n\n### Details\n- Please provide a pseudo-code to illustrate the interplay of OT and BPE segmentation and the hyperparameters involved in the process.\n- The length normalization disappears from Eq 8 to Eq 9 on to the final objective (not numbered, p6). Please explain or correct.\n- Figure 1: The plots are too small to be able to read the axes' labels and titles.\n- With AMD being a dynamic feature defined on differences between merge steps, how can it be reported for a static BPE size like in Fig. 1?\n- \"AMD-BPC, short for AMD\" (p3) assumes a prior definition of the acronym, and probably should be reversed since \"AMD\" is shorter than \"AMD-BPC\".\n- Duplicated \"can\" on p4.\n- Eq. 8 should not have the min as far as I understand.\n- Eq. 8 should hold only under certain assumptions of l (that the average length decreases if tokens get more frequent), and therefore does not hold generally. For the case of BPE it is reasonable to assume that it holds.\n- What's the influence of the post-processing hyperparameter of 10% for comparing char and token frequencies? An ablation would be helpful to understand the implications of the post-processing.\n- The multilingual TED talk data set needs a citation because there are different versions on the web, I'd assume it's http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/ (Duh 2018)?\n- Table 1 boldfaces results that are not the best in each column, which is confusing for the reader and misleading for the interpretation. \n- Table 1 misses BPE-2k which seemed the consitently strongest setting for BPE in Fig 1. Is there any gain of Info-VOT over BPE-2k (vocabulary sizes are similar)?\n- How does it compare against character-level models? Those have been reported to perform well in related work, and need no tuning.\n\n-----\n\nUpdate addressing author response:\n\nI believe the quality of the paper has notably improved, in particular the clarity of the proposed method, and the wider space of experiments generally supports the method. However, some critical questions remain.\n\nQ1: \"For simplification, we use BPE segmentation to initialize the token candidate distribution. It does not mean that our approach can only be built on top of BPE.\" (also Q5) It remains unclear how important the initial choice is (why not pure characters or a random selection of merges?) and if it requires hyperparameter tuning (making the proposed solution less \"green\" than advertised). Same holds for the impact of post-processing heuristics.\n\nBPE as proposed by Sennrich et al. 2016 assumes the notion of white-space separated words or tokens. SentencePiece, however, does not. This blogpost illustrates the differences quite well: https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html\n\nQ7: The multilingual experiments confirm the effectiveness in some cases (significance tests would be great), but the effect of overlapping vocabularies is still not taken into consideration or analyzed.\n\nQ12: Several of these gains might not be significant due to very small absolute BLEU differences.\n\nDetail in the added references: the newly added reference Kreutzer & Sokolov was published at IWSLT 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting problem, good results, but the objective function may be flawed",
            "review": "In this paper, the authors optimize vocabularies for neural machine translation according to an information-theoretic framework. They first observe a correlation between BLEU and amortized marginal difference over bits per character. From there, they propose an objective function and an efficient procedure to construct vocabularies.\n\nStrengths:\n\nOn multiple datasets, the optimized vocabularies lead to comparable or better results than models trained with byte-pair encoding, while being much smaller.\n\nThe vocabularies may be constructed efficiently by solving an optimal transport problem.\n\nThis work may encourage further research in designing well-principled objectives for vocabulary construction.\n\nWeaknesses:\n\nThe bold-faced numbers in Table 1 are misleading. The authors only highlight their results, instead of the best ones.\n\nMy biggest issue with the paper is the step from equation (2, 3) to (4). All other things held constants, Eq. (2) and (3) would be maximized with a low value of $B(v_{k+m})$ and $B(v(t))$ respectively. However, in (4), we instead look for a high $B(v(t))$. (4) may be a lower bound of (3), but there is no guarantee about how tight the bound is. Hopefully, this is just a misunderstanding, and I may increase my score if this issue is clarified by the authors.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Unfortunately, the claims are not supported",
            "review": "The paper tackles the problem of building a vocabulary of tokens for a corpus of data, as well as the choice of the vocabulary size. It claims to empirically find the connection between the amortized marginal difference over bits per character (AMD-BPC) and the quality of an NMT system. This measure is then used to build a vocabulary, and the objective is formulated as the optimal transport problem. In the experiments, the authors compare quality and vocabulary sizes for the proposed method (info-vot) and BPE vocabularies with 10k, 20k and 30k merge operations. In this highly suboptimal for the BPE setting (more details on this below), the proposed approach improves in 12 out of 26 datasets with an average improvement of 0.4 BLEU and approximately the same drop for the rest of the datasets.\n\n------------------------------------------------------------------------------------------------------\nStrengths\n\nAn idea to use information theory for building a vocabulary if nice, especially considering that BPE itself is a compression algorithm.\n\n------------------------------------------------------------------------------------------------------\nWeaknesses\n\nUnfortunately, the paper does not support any of its claims. Namely, \n1) the proposed measure AMD-BPC does *not* indicate model performance\n2) empirical results, quality: the proposed approach does *not* lead to better quality even in a very suboptimal setup for the baselines\n3) empirical results, vocabulary size: since the current common knowledge that BPE vocabulary size for the small datasets used in the paper has to be about 0.5k-2k, the claim about reducing the vocabulary size does *not* hold.\n\nMore details on each are below.\n\n1) The authors claim that AMD-BPC indicates model performance based on empirical observations for several NMT models trained with vocabularies of 1k, 2k, 10k, 30k, 50k merge operations. However, Figure 1 shows that AMD-BPC and BLEU are not in agreement, and the two functions behave very differently. Moreover, in table 1 from supplementary materials, we see that vocabulary with the highest AMD-BPC coincides with the highest-BLEU model only in 4/14 datasets. Therefore, I believe that these results show that AMD-BPC does not indicate model performance and there is no reason to use it as a measure for building a vocabulary.\n\n2, 3) All the experiments (except for one with WMT14 En-De) are conducted on very small datasets (e.g. TED talks). For such small datasets, the current common wisdom is to use small BPE vocabulary with 0.5k-2k merge operations. This was shown in several papers (among the ones cited by the authors, Ding et al 2019 and BPE-dropout which adopts this for the baseline models; among not cited is Sennrich & Zhang, ACL 2019 “Revisiting Low-Resource Neural Machine Translation: A Case Study”).\nSince instead of 0.5k-2k vocabularies for BPE the authors consider only 10k, 20k, 30k (and claim vocabulary reduction compared to 30k), the results are not valid. First, even in this highly suboptimal for BPE setting the improvement is very small and for only 12/24 datasets. Second, the learned vocabularies for these small datasets are of the size 15k-2k, which is not smaller than the standard for these datasets BPE setting.\n\n------------------------------------------------------------------------------------------------------\nRelated work\n\nMentions of related work have a lot of inaccuracies. \n1) Starting from the abstract and further in the text, the authors say that 30k BPE vocabulary is the standard choice. This is not true: it is now well-known that the vocabulary size depends on the dataset size, and has to be very small for small datasets. See the links above.\n2) Related work, paragraph 1: “Initially, most neural models are built upon word-level vocabularies (Vaswani et al., 2017;...). ... these models fail on handling rare words under limited vocabulary size.” - note that Transformer uses BPE (i.e. subword-level vocabulary) and does not have problems with rare words.\n3) Related work, paragraph 2: “Costa-jussa & Fonollosa (2016) propose a character-level vocabulary .... Following this work, Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies.” - this is not true, BPE could not be inspired by this work. At the very least, because BPE first appeared in August 2015, while the former paper in March 2016.\n\n------------------------------------------------------------------------------------------------------\nTypos and language\n\nThe paper has many typos.\n\nSome examples from the first page:\n1) abstract: “search for best token dictionary” -> “search for the best token dictionary”\n2) introduction, paragraph 1, the last sentence: the same paper Sennrich et al 2016 is listed twice\n3) introduction, paragraph 3: “we find an exciting experiment phenomenon” - experimentally find? experimental phenomenon? Not sure what you meant\n4) introduction, paragraph 3: “AMD-BPE, short for AMD,” - maybe “AMD, short for AMD-BPE”? Also, probably you meant AMD-BPC (and not BPE)\n\n------------------------------------------------------------------------------------------------------\nPresentation\n\nTable 1: marking all results of the proposed method is misleading. Usually, in bold are shown the best results, of something else indicated in the caption. Here, it can be misleading, especially considering that in most of the cases, these bold results are not the best.\n\n------------------------------------------------------------------------------------------------------\nOverall recommendation\n\nOverall, since the claims are not supported, I can not recommend accepting this paper.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting problem, concerns about the thoroughness",
            "review": "# Paper summary\nThe paper studies the problem of creating a subword vocabulary for NLP tasks. It starts by presenting a relation between an information-theoretical measure (AMD) and downstream performance. It then proposes an optimal transport formulation of the problem of identifying the best vocabulary. The paper presents empirically studies the effectiveness of the new vocabulary construction method.\n\n# Strong points\n\n* The paper studies a critical and overlooked problem.\n* The approach (optimal transportation) is an intriguing idea.\n\n# Weaker points\n* Info-VOT is based on an unproven assumption, namely that AMD works. As I point out below, (1) it is not clear where AMD comes from; and (2) the experiments meant to show AMD's effectiveness are unsuitable.\n* Essential elements of the proposed method are unclear.\n* There are several unfounded claims in the paper.\n\n## Detailed comments\n* It is not clear whether the Amortized Marginal Difference over BPC is a pre-existing measure or one derived in the paper. The paper contains no references to prior work defining AMD. Page 3 (just before equation 2) makes it seem like AMD is first introduced in this paper, yet the article provides no argumentation for AMD.\n* I find the experiments in Figure 1 problematic:\n    1. The figures are difficult to read even when zooming in and using a 26-inch computer monitor.\n    2. \"As we can see, vocabularies with the highest AMD usually bring higher BLEU scores in most cases.\" –– We cannot actually see that due to the size and style of the figures. A closer look at the figures reveals that in none of the four cases is the best AMD point also the point of best BLEU. I have checked with Appendix A, and it confirms my observation.\n    3. It seems flawed that the paper analyses the correlation between AMD and BLEU by picking examples. There are many established statistical methods for analyzing and quantifying correlation.\n* I find Section 4 confusing:\n    - The definition of S does not make sense when considering that \"1 + (t - 1) * i is the upper bound of vocabulary size at t-th timestep\". Given that everything is based on BPE, doesn't the vocabulary start with all the alphabet letters? If so, wouldn't the vocabulary at step 1 be equal to the vocabulary size and then increase in increments of i as shown? (Figure 2 seems to agree that the vocabulary starts with the alphabet.) \n    - Furthermore, is that an upper bound on the vocabulary size, or is it the exact vocabulary size? I think it is the actual vocabulary size. The beginning of Section 4.3 seems to indicate the same.\n    - Who is **M** in the explanation of Equation 3?\n    - Why is it intractable to solve the problem in Equation 3? Provide proof.\n    - \"enumerate all timesteps and output the vocabulary satisfying Eq. 4.\" This is unclear as there are two vocabularies in Equation 4. I guess the text should read \"output the vocabulary corresponding to the time step statsifying Eq. 4\".\n    - Section 4.3 says, \"let $T \\in V_S$ be the vocabulary containing top S most frequent tokens,\", but the same section begins with \"Let\n$V_S$ be the set containing all vocabularies with size S.\". How does that make sense? Since $V_S$ already contains size S vocabularies, picking a T of S tokens means that T is the same size as $V_S$. How can one decide on the topmost frequent tokens from a collection of vocabularies whose cumulative frequency is equal?\n    - It is not clear to me why Equation 8 holds.\n    - Equation 9 is confusing and maybe does not hold. It is not clear what P(i,j) is. The text says, \"Let P(i, j) be the joint probability distribution of the tokens and chars that we want to learn.\" First off, it is unclear if *we* want to learn the joint probability or the tokens and characters. Secondly, if P(i,j) models the joint probability of token *i* and character *j*, then won't this probability be 1 for all *j* that *i* is made up of and 0 for the others? I.e., if *i* = ab then p(ab,a) = 1, p(ab,b) = 1 and for all other characters c, p(ab,c) = 0? If that is the case, then Equation 9 no longer holds as P(i) (which is defined earlier on the page as $P(i) = \\frac{Token(i)}{\\sum_{i \\in v} Token(i)}$) would be estimated to its length in characters rather than the probability defined earlier on the page.\n* Table 1 is confusing. \n    1. It is unclear from the caption, or the table structure, that the last four rows represent vocabulary sizes.\n    2. It is unclear why the size of BPE-XK vocabularies does not follow the strict formula $size = alphabet\\_size + num\\_merges$. Section 6 clearly states that the XK in BPE-XK represents the number of mergers. How can the alphabet for Romanian, for example, be 300 characters, then 200, then 0? This holds for all languages except for German, where it is consistent.\n    3. I cannot see in Table 1 how many times each experiment was run and the variance of the results. The text does not describe it either.\n* The subsection on Info-VOT vs. Info-VOT-based rules is confusing:\n    1. Why does the name change from Info-VOT-based rules to AMD-search?\n    2. How can AMD-Search both \"randomly search several vocabularies as candidates\" and \"AMD-Search randomly samples 10 BPE-variants\"? Isn't BPE a deterministic process? Is something else than BPE used here?\n    3. \"Despite the better performance of Info-VOT, we argue that AMD-Search is also a good information-theoretic solution, which outperforms widely-used vocabularies.\" –– There is no evidence for this in the paper. Table 3 shows experiments on TED (although that is not made clear), which are not compared with any other \"widely-used vocabularies\".\n* \"By contrast, the learning direction has a higher performance ceiling.\" -- Unsubstantiated claim. Please expand and explain, cite supporting work, or rephrase. \n* Following equation 2, the paper states, \"m is the increased operation size\". This is confusing. The previous paragraph has established that $v_T$ is a vocabulary of size T. It would be clearer to define *m* as the increase in vocabulary size. There is no need to refer back to BPE operations since AMD-BPE is a measure of vocabulary difference and could work on any two vocabularies of different sizes, regardless of whether the vocabularies resulted from BPE. By the way, equation 2 should require $m>0$.\n* The article does not specify what corpus constituted the basis of BPE and vocabulary selection for the experiments. As the paper points out both BPE and Info-VOT rely on corpus estimations of frequency and probabilities, respectively. It is important to understand from what corpus the vocabularies were derived, i.e. what size and domain.\n* The paper is difficult to read due to language issues (wrong prepositions, typos, and complicated sentence structure). I recommend the authors use grammar and spelling checkers and rewrite some parts of the paper to clarify. I include some examples to illustrate my point:\n    - Complicated sentence structure \"We argue that BPC is a\nmore fair evaluation feature than BPT, which avoids the effects of token lengths.\" -- Who avoids the effects of token lengths?\n    - \"Entropy is a common feature evaluating\" -- entropy does not evaluate, it measures.\n    - Wrong prepositions and confusing formulation \"feature, AMD-BPC, short for AMD\". How can the longer abbreviation be the shortening of the shorter abbreviation?\n    - Broken plurality: \"Imagine a vocabulary search policy that\nincrementally increases the number of merging operation\" –– operations not operation.\n    - Confusing and imprecise language: \"Given two sets of chars and tokens, we can define a transport matrix with each item (i; j) deciding how much chars are transported from char i to token j.\" – Why is there a need for two sets of characters and tokens when the matrix only uses one set of characters and tokens?\n * Peyre and Cuturi (2020) lacks venue and pages.\n\n## Minor issues\n* The first paragraph cites Sennrich et al. (2016) twice in the same parenthesis.\n* It is unnecessary to introduce the AVS abbreviation in the second paragraph. It is not used anywhere else.\n* << In this work, we\ntake a first step in the learning direction to explore \"how far can an information-theoretic learning approach can reach\" >> -- Is the last part of the sentence a quote? Where from (reference)?\n* \"Moreover, there is an efficient algorithm, the Sinkhorn algorithm\" –– please cite the work introducing the Sinkhorm algorithm\n* The text for Equation 9 states that the proof is in Appendix B, but Appendix B claims to prove Equation 10. There is no Equation 10 in the paper. Appendix B clearly refers to Equation 9. This is a typo.\n* It is okay to use commas to separate thousands, as in the US system. It is also okay to use space to separate thousands, as in the French / European system. It is confusing to separate using both commas and spaces at the same time; see Section 5.1.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}