{
    "Decision": "",
    "Reviews": [
        {
            "title": "A good paper but not very clear",
            "review": "This paper studied a problem to present information leaking from GNN embeddings. To achieve goal, they proposed a minimax game learning framework between desired GNN encoder and the attacker. The proposed method seems sound and the experimental results are provided to demonstrate the performance. \n\nPros: \n1. The problem studied here seems new. There are very few papers studying this setting. \n2. The proposed solution has theoretical guarantee. \n3. The extensive set of experiments are provided to show the effectiveness. \n\nCons: \n1. I think the paper title is very misleading. It looks like it studied a generic problem on graph adversarial examples. However it is actually focusing on the differential privacy problem for GNNs.   \n2. The proposed algorithm has little details so it is hard to follow why this algorithm is presented to solve the problem and how to exactly solve the problem. \n3. The experimental results are not very clear. What's the measurements for different T and A? How does it measure the performance? The smaller the better? \n4. There is no computational analysis about the proposed algorithm. How efficient is the algorithm? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Comparison with related work missing",
            "review": "Summary: The paper proposes a defense against attribute inference attacks on graphs. The defense is formulated as a minimax game where the defender aims to learn node representations that do not contain information about a given sensitive attribute, while the attacker aims to maximize the contained information. The authors prove a lower bound for the inference error (w.r.t. the sensitive attribute) that any attacker has to incur. They also discuss the loss in accuracy if the adversarial advantage is minimized and the conclusions depend on the correlation between the target and the sensitive attribute.\n\nReasons for score: The score of 5 is mainly due to a lack of comparison with related work in the evaluation and the similarity of the theoretical contributions with previous work (see weak points). If the authors include a more comprehensive empirical evaluation and clearly delineate the relation to previous work I am willing to increase my score.\n\nStrong points:\n* The theoretical characterization of the inference error and the loss in accuracy is useful (despite strong assumptions)\n* The detailed evaluation in terms of \\lambda and n-hop, as well as the comparison between node and neighbor attacks in insightful.\n* Studying attribute inference attacks on graphs is important and has not been tackled so far.\n\nWeak points:\n* Similar (or equivalent) theoretical contributions have been presented in [1] (accepted to NeurIPS 2020, initially submitted on ArXiv on 19.06.2019), compare e.g. Theorem 3 here to Theorem 3.1 in [1]. The difference to this prior work should be clearly stated. Moreover, the analysis does not exploit the fact that the encoder is a GNN. \n* There is no comparison to other approaches for (adversarial) removal of sensitive/protected attributes or approaches for learning fair representations (e.g. [2, 3, 4, 5] to mention just a few), except the single comparison to (Bose & Hamilton, 2019). See also questions.\n* There is no evaluation for the popular task of node classification, only link prediction and graph regression. \n\nQuestion for the authors:\n1. How does the proposed approach compare to other approaches for (adversarial) removal of sensitive/protected attributes (see weak points)?\n2. Why is the comparison to (Bose & Hamilton, 2019) only presented for ML-1M and not for the other datasets? Note that (Bose & Hamilton, 2019) also show results on FB15K-237 in their work.\n3. Often the sensitive attribute has a skewed distribution (e.g. majority vs. minority) in practice. How does the proposed approach perform in such settings? In particular, does the performance for the minority class suffer more than the majority class?\n4. How does your approach perform for other (general) architectures and tasks? (since the method is agnostic to the encoder function, see also weak points)\n5. Does the theory apply to the setting of multiple sensitive attributes? (I acknowledge the empirical results on FB15K-237)\n\nAdditional feedback that did not affect the decision:\n* The encoder g can be held fixed (e.g. after pretraining along with h). In the current experiments pretraining is done, but g is not fixed. How does the proposed approach perform if g is fixed and only h and f are optimized? What does this setup imply for the derived guarantees? \n* Can the requirement that \"the task classifier and the adversary have unlimited capacity\" be relaxed? In particular, can we take into account the set of functions representable by GNNs with a small number of layers as used in pratice?\n* It would be interesting to evaluate whether the proposed approach is effective at defending against edge attribute inference (including the existence of an edge).\n* It would be interesting to study how does a homophily (or heterophily) assumption change the problem in theory and practice. For example, experiments on data generated from an SBM model with different parameters might be insightful. \n\nTypos:\n* supresum \n* transformation to removing information\n\n##  After Rebuttal\nAfter reading the rest of the reviews and the authors' response I have decided to keep the score.\n\nIt is not clear why the techniques in [2-5] (or the techniques in the references given by Reviewer 4) cannot be applied on graphs since at least some of these formulations appear to be model/task agnostic. I would suggest either including a comparison or making a stronger case why a comparison is not possible.\n\nReferences:\n1. Zhao, Han, Jianfeng Chi, Yuan Tian, and Geoffrey J. Gordon. \"Trade-offs and Guarantees of Adversarial Representation Learning for Information Obfuscation\"\n2. Elazar, Yanai, and Yoav Goldberg. \"Adversarial removal of demographic attributes from text data.\"\n3. Roy, Proteek Chandan, and Vishnu Naresh Boddeti. \"Mitigating information leakage in image representations: A maximum entropy approach.\"\n4. Xie, Qizhe, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. \"Controllable invariance through adversarial feature learning.\"\n5. Louizos, Christos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. \"The variational fair autoencoder.\"\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting attack on graph data but misses related work on data privacy",
            "review": "This paper studies the problem of attribute inference attack on graphs with GNNs and proposes an adversarial training framework (minmax game) to maximize accuracy while minimizing information leakage about sensitive attributes.  \n\nPros: \nThe paper is very well written and studies an interesting attack problem on graph data. \n\nCons: \n-\tAttack setup is not clear. It is assumed that the attacker has access to the GNN’s representation before the last layer and wants to guess the sensitive attribute. This particular attack model needs to be motivated as it’s not clear from the paper what real-world scenario it targets.\n-\tSimilar attack setting has been indeed considered in many recent papers in context of data privacy [1-8], with most considering the use-case of private inference with network partitioned between a client and a server. Many papers have adopted adversarial training framework, and some have presented information theoretic analysis as well. \n-\tSo, while attribute inference attacks on graph data is an important problem, the novelty of the proposed method is limited as it seems previous approaches can be directly used on graph data and with GNNs as well. In fact, for the most part, it seems the proposed method in the paper is not specific to graph data and GNNs either. So, the paper must discuss in what aspects it is different from the prior art and how the proposed method compares with existing methods. \n\nReferences: \n\n[1] Edwards and Storkey, “Censoring Representations with an Adversary”\n\n[2] Jihun Hamm, “Minimax Filter: Learning to Preserve Privacy from Inference Attacks”\n\n[3] Li et al., “PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training”\n\n[4] Xie, et al., “Controllable Invariance through Adversarial Feature Learning”\n\n[5] Song and Shmatikov, “Overlearning Reveals Sensitive Attributes”\n\nAlso on text data:\n\n[6] Elazar and Goldberg, “Adversarial Removal of Demographic Attributes from Text Data”\n\n[7] Coavoux, et al., “Privacy-preserving Neural Representations of Text Maximin”\n\n[8] Li et al., “Towards Robust and Privacy-preserving Text Representations”\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Graph Adversarial Networks\"",
            "review": "****Summary****\n\nThis paper proposed to defend against the information leaking in the graph node embedding. The ‘adversarial attack’ in the title is not with the commonly used sense, but is talking about identifying certain sensitive attributes from the embedding representation. The authors came up with a minimax formulation, where the ‘primal’ GNN tries to learn a good embedding for the downstream classification task, while also avoiding leaking sensitive attribute information. The ‘dual’ model tries to tell the sensitive attribute from the embedding. The author shows some claims on the tradeoff between these two objectives, and has carried out extensive experiments on graph datasets to verify the idea.\n\n****Pros****\n\n- The paper is well written and easy to follow. \n- The theoretical claims on the tradeoff between predictive performance and ‘safety’ is interesting. \n- The experiments are relatively comprehensive.\n\n****Cons****\n\n- The problem might not be very practical, and the usage of adversarial attack might be confusing.\n- There’s little technical contribution or innovation.\n- There might be flaws in the experimental design.\n\n\n****Overall I lean towards rejection****. Please see my detailed comments below:\n\n- The major issue with this paper is the validness of its problem definition. This problem assumes that one can get access to the latent embedding representation of some deployed system, which is not quite practical. If the hacker is able to obtain such embedding, then it would be more direct to hack the database to get the sensitive information. Note that this is different from the classical adversarial attack, where one can make tweaks on the images, or in the graph domain where one can deliberately create/delete friendship to modify the graphs. The adversarial attack is an issue, as one can easily modify the inputs to try to fool the blackbox system. However I’m not fully convinced that the problem studied in this paper is valuable. \n\n- The technical contribution is mainly coming from the Eq (4), which is a standard objective in many adversarial learning works. The theoretical results are interesting, which show the trade-off between the two objectives mathematically. However, this is also limited to this specific problem proposed, and it didn’t give a practical solution on how to choose the best \\lambda (or maybe we don’t have a definition of the ‘best’). \n\n- I would suggest to avoid using the phrase ‘adversarial attack’, as it has its well recognized definition. The problem studied here is closer to the ‘privacy’ or ‘information leaking’. \n\n- One detail that might be missing is: whether the ‘attacker’ performance is obtained by the attacker that is directly trained in the minimax objective from Eq (4), or is it performed after Eq (4) is fully trained, and train a new attacker that uses embeddings from GNN and attribute labels from the training data? The concern with the former one is that the Eq (4) might reach some trivial equilibrium. \n\n- Regarding the neighborhood attack, what will happen if the attacker in Eq (4) is trained only with the node embedding information (without neighborhood gathering), and after that we train a separate attacker that leverages neighborhood information to infer the sensitive attribute? \n\n- Similarly to above, can we have a flexible form of adversarial f in Eq (4), such that after training, it would be robust to different parameterizations of the attacker that is trained with/without neighborhood information? As it is also hard to assume the specific model the attacker would use, the Eq (4) should be general enough to capture the worst f in the family. \n\n\n****Questions****\n\nI’d like to hear the answers to all my questions above. \n\n\n****How to improve****\n\nActually I feel the problem in machine learning fairness might be more relevant. Instead of formulating the problem as protecting from adversarial attackers, it would be better and more natural/useful to formulate it as a ‘fair’ classifier that shouldn’t make decisions on top of the sensitive attributes. There are several definitions in the fairness domain, like equalized odds, that might be relevant to Eq (2). \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}