{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Most of the reviewers pointed out a lack of rigor of this submission, unclear contributions, not too convincing claims and empirical gains. I thank the authors for the effort put in revising the paper and responding to the reviewer concerns. However, the reviewers did not deem them convincing enough."
    },
    "Reviews": [
        {
            "title": "Review: Convex Regularization in Monte-Carlo Tree Search",
            "review": "-Summary \n\nThe authors consider planning for Markov Decision Process. Precisely they study the benefit of convex regularization in Monte-Carlo Tree Search (MCTS). They generalize the E2W by xiao et al., 2019 by considering any strictly convex function as regularizer instead of the intial negative entropy. They provide a regret analysis of this algorithm named E3W and prove that EW3 converges at an exponential rate to the solution of the regularized objective function. Then they consider three particular instances MENTS with the Shannon entropy as a regularizer, RENTS with relative entropy to the previous policy as regularizer, and TENTS with the Tsallis entropy. They compare empirically these algorithms with PUCT as policy search in Alpha-go style MCTS on CartPol, Acrobot, and Atari games.\n\n\n-Score justification/Main comments:\nThe setting is not clearly written and some key definitions are not introduced properly (see specific comments below). Given that it is hard to understand the main results.\n\n\n-Detailed comments \n\nP2: It is V^\\pi(s) = \\sum_{a} \\pi(a|s) Q^\\pi(s,a). Is the number of actions finite?\n\nP3, (2): You mean \\max_{\\pi_s} \\sum_{a}Â \\pi(a|s) Q^\\pi(s,a) -\\tau \\Omega(\\pi_s) ? Because T_{\\pi}Q is a function of (s,a) and I do not understand the notation T_{\\pi_s}Q_s (and Proposition 1 will be wrong since in (4) the bound on the rewards should appear, e.g. take Q_s = 0).\n\nP3, (4): absolute value for |\\Omega^*(Q_1) - ....|\n\nP4, (7): what is the link with (6)? Here, by the choice of \\lambda you force the exploration as with \\epsilon-greedy with is what UCT is trying to avoid.\n\nP4, Theorem 1: Could you define the estimated value V_\\Omage(s). But at the end we would like to be close to the true optimal value V^*, can you deduce a bound for |V_\\Omega -V^*|.\n\nP4, Theorem 2: I assume that a^* is the optimal value for the regularizd objective? \n\nP4: In TRPO, Schulman et al, 2015, it is rather the reverse relative entropy than the relative entropy which is used.\n\nP5, Table 1: there should be the temperature parameter \\tau here.\n\nP5: V_i is not defined, a_i either, it is a sum over i in which set? I do not understand your definition of the regret. \n\nP5, Theorem 3: There is a sign issue in 13 since le left hand is greater than the right-hand. So the regret of E3W is linear?\n\nP7, figure 1: Is it UCT or PUCT used for the experiments because in the main test you say it is PUCT? And I would not say that UCT is \"clearly the worst\" approach in Figure 1.\n\nP9, appendix B: According to your setting the reward is deterministic. I do not understand. And it is an assumption no? Which leaf node?\n\nP10: Could you define properly r(a) and \\hat{r}(a)? Why there is a sum over k in the inequality below \"therefore\", is it a sum over a? And there is an issue with the parenthesis. I do not understand how you can control the term in exp(-1/(\\log(2+...))^3) by  exp(-t/(\\log(2+...))^3) in the last inequality.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Official Blind Review #1",
            "review": "This paper generalizes and build on top of the MENTS/E2W, and shows that the entropy regularization can be replaced with any convex regularization. It uses the tools of convex conjugates and duality to derive the theoretical results and the algorithm/updates. Empirical results on Atari games confirm the value of policy regularization in MCTS.\n\nStrong points:\n - Theory generalizes MENTS/E2W\n\n- Experiments further support the value of regularized polices, showing that entropy is not the only thing that \"works\".\n\n- Paper brings important and interesting insights into MCTS, which is potentially very impactfull.\n\n- Previous work is (to my understanding) well cited.\n\n- While the paper relies on non-trivial operations, it's well written.\n\n- The resulting algorithms/updates are \"easy\" to implement.\n\nWeak points:\n- This paper is very much incremental to MENTS/E2W, one could say it \"just generalizes\" MENTS.\n\n- Missing connection to previous results of policy/values dualities (please see additional feedback)\n\n- The empirical results are not very exciting.\n\nReasons for score: \nWhile the empirical results don't bring anything exciting (especially when contrasted to MENTS), they still bring interesting insights. It almost seems that any regularization is relevant. Furthermore, the presented theory/connection coming from the duality is important - I do not think this connection of duality was presented in the MENTS paper. Thus it's satisfying to know that this is where the \"magic\" of softmax and entropy as used in MENTS is coming from.  The reason I really like this paper is that it helps to build more intuition about the regularized policies in MCTS, all the while generalizing the underlying theory.\n\nAdditional feedback:\nOn high level, this paper essentially explores the duality of policies and corresponding (regularized) values. This idea/result/notion of duality between policy and value appears quite often (a quick example that comes to my mind being extensive form games but surely others), and I think few sentences on this would help the reader to feel \"less surprised\" about some of the presented derivations and techniques, and overall better place it in the context of previous relevant work. Reading this paper, one could think that the duality of policies/values is novel observation, while in general it's not.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid theoretical results, limited experimental results",
            "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides theoretical analysis of the regularized backup for MCTS. The paper carries out detailed analysis (regret, errors analysis) on three instantiations of the regularizations. Finally, the paper provides some empirical gains on certain toy domains and some atari games.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I am not very familiar with the theoretical results of the MCTS literature - however, it seems that the idea of adding convex regularization is not new in rl literature overall. I cannot be a good judge for the theoretical contribution and I will focus more on the empirical side of the paper.\n \n##########################################################################Pros: \n\n \n1. Related work. The paper seems to miss some highly related literature, in particular:\n\n[1] Buesing et al, Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions, AISTATS 2020\n\n[2] Grill et al, Monte Carlo Tree Search as Regularized Policy Optimization, ICML 2020\n\n[1] uses entropy-regularized MCTS backup; [2] relates MCTS to policy optimization with convex regularization. In particular, [2] proposes that the conventional MCTS backup used in Alpha-Zero (which is different from the max-MCTS backup), is carrying out approximate regularizations. I hope that the author could clarify on the connection between this work and these two pieces of prior work.\n \n2. Regarding results in Table 2, I wonder how many seeds do the authors run per game per algorithmic baseline. Does each number correspond to a mean value across a few seeds, or it is just a single run? Could the author also clarify how the t-test is done to denote significant differences? I would expect such tests to be run on averages over a collection of seeds.\n \n##########################################################################\n\nPlease address and clarify the concerns above.\n\n \n#########################################################################",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice results, but suspicious-looking details",
            "review": "The paper proposes a general framework for regularized Monte Carlo tree search, thereby generalizing the maximum entropy Monte Carlo planning of Xiao et al. The main result(s) looks very interesting, but some of the definitions and results don't seem right.\n\nThe main issue\n=============\n\nThe main problem is Proposition 1 and the definition of the convex conjugate:\n1) The definition of the convex conjugate in (2) differs significantly from the standard one (e.g., the definition in the two cited papers) due to the multiplier \\tau. In fact, this \\tau, unless set to 1, strongly questions the validity of (3) - and basically all the usual properties that are true for the standard definition of convex conjugates.\n2) The contraction property in Proposition 1 also seems strange. It is well known that the Bellman operators are contractions, but the property claimed here is not true for the standard definition of the convex conjugate: if \\Omega^* is a contraction, then \\Omega should also be a contraction which, in turn, would imply \\Omega^*=0 due to \\Omega***=\\Omega*. Finally, in case \\Omega* is indeed a contraction, the contraction parameter should depend on \\tau as well.\n\nAs Proposition 1 is essential to the main result, the above issues make the the correctness of the main result questionable.\n\n\nAdditional remarks\n================\n\nFirst of all, it would be nice to have a brief summary of how the analysis works and what the main idea is.\n\nSection 3.2: the part of the trajectory discussed above (6) presumably corresponds to the selection part, not the simulation. Additionally, it is not clear how equation (7) was obtained. Please discuss it in more detail.\n\nRegarding (10) and (11): please prove these equations or add a citation to some work where they are proved.\n\nEquation (13): what is V_i? (Presumably, i_n stands for the action taken in step n.)\n\nRegarding the experiments, they do show the superiority of proposed method on some benchmark tasks, such as CartPole, Acrobot and a couple of Atari games. However, it is not clear whether these tasks are the best to test UCT as the strength of UCT is the doing in-depth exploration on search trees with large branching factors, where the actions could have significant consequences in a very delayed fashion (in terms of reward).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}