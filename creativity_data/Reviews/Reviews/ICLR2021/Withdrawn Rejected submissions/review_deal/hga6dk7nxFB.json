{
    "Decision": "",
    "Reviews": [
        {
            "title": "ICLR 2021 Review of paper 3451: Deep Learning Proteins using a Triplet-BERT network",
            "review": "Deep Learning Proteins Using Atriplet-bert Network\n\n##### 1. Summary\nThis paper presents a method to fine-tune a BERT (Bidirectional Encoder Representations from Transformers) model. The BERT model is pre-trained on proteins. The fine-tuning is done with a triplet network, thus the proposed method is named triplet-BERT. The method is tested on different downstream tasks. The fine-tuning consists in modifying the embeddings, i.e., the representation of the protein, and improving the embeddings to include information about the protein that is relevant for the downstream task. The selected tasks analyzed by the authors are: i) peak absorption wavelength, ii) enantioselectivity, iii)  plasma membrane localisation, and iv) thermostability. Triplet-BERT is assessed against a set of comparison partners on the four above-mentioned tasks. The authors show that the proposed method outperforms the comparison partners.\n\n##### 2. Rationale for the score\nIn my opinion, the work is framed as an application paper. No methodological improvements are proposed for the BERT model or the triplet network. Having said that, for having the tenor of an application paper, the experiments are rather simplistic. The datasets used for the four tasks chosen by the authors are quite small for protein prediction standards or cannot be considered benchmark datasets that can be used to compare tiplet-BERT against other published methods. The choice of comparison partners is also questionable. For each of the four tasks, tiplet-BERT should have been compared against state-of-the-art methods for that task.\n\n##### 3. Positive aspects\n- The paper tackles a very relevant topic in an application domain like protein modelling where there is a very large amount of unlabeled data.\n\n##### 4. Negative aspects\n- This work is another iteration of papers that show the advantages of pretraining a model on a large corpus of (unlabeled) text data followed by fine-tuning on a specific classification task (BERT-cited in the manuscript-, RoBERTa [RRef5], XLNet [RRef6] and ELMo [RRef7]). There are also many recent publications that apply these models to proteins such as Bepler & Berger, 2019-normally referred to as P-ELMo, cited in the manuscript but ignored as a comparison partner-, SeqVec [RRef8] and UniRep-Alley et al, 2019, cited in the manuscript but ignored as comparison partner- and PLUS-RNN -Min, et al. 2019, cited in the manuscript but ignored as comparison partner.\n- As hinted before, the main weakness of this work, in my opinion, is the lack of comprehensive experiments to show that triplet-BERT outperforms state-of-the-art methods in protein prediction tasks. The methods mentioned above were not used as comparison partners\n- In addition to a lack of comparison partners, the tasks used to assess the performance of triplet-BERT are not standard benchmark tasks like the broadly used TAPE tasks. I recommend the authors to use published datasets that have been designed as benchmark datasets in protein prediction. Of importance is that the data splits must have been created by the respective publications, to guarantee fair comparison between methods, e.g., data associated with the DeepLoc method to predict subcellular localization [RRef9].\n- The authors show the loss function in Equation 3 but it is unclear if/how it can be differentiated to propagate the gradients. No analysis with respect to this topic is provided in the manuscript.\n- The authors point to Figure 3, that shows cluster maps,  as evidence that the fine-tuned encodings give more distinct clusters. This is not immediately obvious from the figure. If actual clusters are obtained, I would suggest using another standard metric to compare clustering results, such as silhouette coefficients or normalized mutual information (NMI). \n- Figure 4 is meant to highlight the performance of the attention mechanism in two proteins (at different layers). It is not clear how these results are later used, if at all. I would recommend adding more context around this figure, especially if it helps drive an important point, which at the present does not seem to be the case. Otherwise, it should be removed.\n\n##### 5. Problems with presentation of the text\nPortions of the text were extracted verbatim from other sources without the proper acknowledgement. This is a serious problem and can be construed as a case of plagiarism. One of the unnamed sources is another submission to ICLR 2021. Some examples are listed below:\n\n- The next to last paragraph in page 3 is a match, with minor rephrasing, to the last part of the first paragraph in Section 3 (page 4) of another manuscript under review at ICLR 2021 (indicated as [RRef2] in Section 7 of this review. Example text:\n> **In this submission**\n> In extension to this work, Vig et al. (2020) explored how this BERT model (Rao et al., 2019)\n> was capable of discerning structural and functional properties about the protein. Vig et al.\n> (2020) proved that the model was able to model long-range dependencies within the\n> sequence of amino acids, it was also able to deduce information about the protein based\n> on the folding structure, target binding sites,and additional complex biophysical properties.\n> They concluded that the specific heads within the model attended to individual amino acids,\n> as the attention similarity matrix was positively correlated to the expected substitution\n> scores (i.e. BLOSUM62) for each amino acid. Vig et al. (2020) noted that the deeper layers\n> of the BERT model focused relatively more attention on binding sites and contacts (i.e.\n> high-level concepts). In contrast, information about the secondary structure (i.e. low-to\n> mid-level concepts) within the protein was targeted evenly across each of the layers\n>\n> **In [RRef2]**\n> Vig et al. (2020) explored how this particular model was capable of discerning structural\n> and functional properties about proteins. As the BERT transformer was able to model\n> long-range dependencies within the protein, it was able to deduce information about the\n> protein based on the folding structure, target binding sites, and additional complex\n> biophysical properties. Vig et al. (2020) concluded that the specific heads within the model\n> attended to individual amino acids, as the attention similarity matrix was highly correlated\n> to the expected substitution scores (i.e. BLOSUM62) for each amino acid. They also noted\n> that the deeper layers of the BERT model focused relatively more attention on binding\n> sites and contacts (i.e. high-level concepts). In contrast, information about the secondary\n> structure (i.e. low- to mid-level concepts) within the protein was targeted evenly across\n> each of the layers.\n\n- There are many instances of sentences copied verbatim or with minor rephrasing between the current manuscript and RRef2]. Example text:\n> **In this submission**\n> However, such methods have yet to be rigorously tested within the context of protein\n> modelling.\n>\n> **In [RRef2]**\n> However, such methods have yet to be rigorously tested within the context of drug-protein\n> modelling.\n\n- Another example:\n> **In this submission**\n> These proteins were collected from the recently curated Pfam database (El-Gebali et al.,\n> 2018), which holds approximately thirty-one million protein domains, and\n> forms the corpus used to train large sequence models as featured in TAPE (Rao et al.,\n> 2019). The architects of the Pfam database have organised the proteins into clusters that\n> share evolutionary-related groups, also known as families\n>\n> **In [RRef2]**\n> “... protein sequences collected from the recently curated Pfam database (ElGebali et al.,\n> 2018), which has the proteins organised into clusters that share evolutionary-related\n> groups (i.e. families). This corpus holds approximately thirty-one million protein domains\n> that have been extensively used in bioinformatics. It now forms the corpus used to train\n> large sequence models, such as TAPE (Rao et al., 2019)\n\n- Text extracted mostly verbatim from the abstract of another publication [RRef3]. Although there is a reference to [RRef3] at the beginning of the sentence, following citation etiquette, the text extracted verbatim should be highlighted as “... text …”. Example:\n> **In this submission**\n> “... they introduced the Tasks Assessing Protein Embeddings (TAPE). They benchmarked\n> the current state-of-the-art models to a set of five biologically relevant semi-supervised\n> learning tasks spread across different domains of protein biology.\n>\n> **In [RRef3]**\n> “... we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically\n> relevant semi-supervised learning tasks spread across different domains of protein biology.\n\n- Text extracted mostly verbatim from another publication [RRef4]. Similar issue to the one described above. Example:\n> **In this submission**\n> Vig et al. (2020) noted that the deeper layers of the BERT model focused relatively more\n> attention on binding sites and contacts (i.e. high-level concepts). \n>\n> **In [RRef4]**\n> As shown in Figure 8, deeper layers focus relatively more attention on binding sites and\n> contacts (high-level concept)...\n\n##### 6. Questions to be addressed during rebuttal period\n1. Can the authors enhance the experiments by:\n- Adding relevant comparison partners for each of the 4 tasks?\n- Using standard benchmark datasets?\n\n2. How was triplet-BERT implemented? Please, provide details (libraries used, hardware on which it was executed, etc.)\n\n3. Can you please expand on how the loss function is used to optimize the parameters of the method?\n\n4. What can the authors say the problems with the presentation of the text, i.e., text extracted from other sources without proper acknowledgement?\n\n##### 7. Additional remarks: typos found in the text\nprotein sequences analysing -> analyses\na triplet of proteins our organised -> are organised\nbefore embeddings a sentence -> not clear, rephrase (before embedding a sentence?)\nare encoded a BERT model -> not clear, rephrase\nThe BERT model will then outputs the following -> output\ncomprises of the Gloeobacter violaceus ... -> comprises the ...\nFigure 3b is the final encodings -> encoding\nFigures 3a-3b presents -> present\nthe cluster map based on the triplet tuned encodings provide a more -> provides\ncan we ascertain the critical parts -> we can ascertain\n\n##### 8. References\n[RRef1] Esmaeil Nourani, Ehsaneddin Asgari, Alice C. McHardy, Mohammad R.K. Mofrad. “TripletProt: Deep Representation Learning of Proteins based on Siamese Networks”. bioRxiv 2020.05.11.088237; doi: https://doi.org/10.1101/2020.05.11.088237\n\n[RRef2] Modelling Drug-target Binding Affinity Using A Bert Based Graph Neural Network. Under review at ICLR 2021. URL: https://openreview.net/forum?id=Zqf6RGp5lqf\n\n[RRef3] Rao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. \"Evaluating protein transfer learning with TAPE.\" In Advances in Neural Information Processing Systems, pp. 9689-9701. 2019.\n\n[RRef4] Vig, Jesse, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. \"Bertology meets biology: Interpreting attention in protein language models.\" arXiv preprint arXiv:2006.15222 (2020).\n\n[RRef5] Joshi, Naman Goyal Jingfei Du Mandar, Danqi Chen Omer Levy Mike Lewis, Luke Zettlemoyer Veselin Stoyanov Yinhan Liu, and Myle Ott. \"RoBERTa: A Robustly Optimized BERT Pretraining Approach.\" In Submitted to International Conference on Learning Representations. https://openreview. net/forum. 2020.\n\n[RRef6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.\n\n[RRef7] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL)\n\n[RRef8] Heinzinger, Michael, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, and Burkhard Rost. \"Modeling aspects of the language of life through transfer-learning protein sequences.\" BMC bioinformatics 20, no. 1 (2019): 723.\n\n[RRef9] Almagro Armenteros, José Juan, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen, and Ole Winther. \"DeepLoc: prediction of protein subcellular localization using deep learning.\" Bioinformatics 33, no. 21 (2017): 3387-3395.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Predicting mutation effects with BERT, but lacking proper validation and interpretation",
            "review": "The authors adopt a triplet loss to predict mutation effects of proteins using a pretrained BERT model.\n\n80/20 test splits are not rigorous enough. Are they randomly throughout the protein? If so, this is the weakest form of train-test. More challenging split would be to leave out entire sites, or even more difficult, to leave out entire domains of the protein.\n\nI would like a better description and characterizations of the mechanisms for supervised learning in previous work, and potentially compare simple baselines here. How does a simple MSE loss on rank-normalized protein measurements perform? Is this what other papers use?\n\nIt would be good if the “prior work” section was cut down a bit. It is too long as of now, and needs to be more refined.\n\nThe results in Table 1 are hard to interpret. I think Spearman r is a more appropriate statistic, because it is comparable across different proteins. Moreover, it would be great to potentially do k-fold CV to get error bars on the accuracy.\n\nIn Figure 3, is there any statistic that can be formalized to determine if the clusters are actually capturing any meaningful biology? Potentially some permutation tests? \n\nI’ve seen that in Figure 5, the attention weights were mapped onto the protein sequence. How do you know these are significant? How does this look any different to just mapping random values onto the sequence? Is there some meaningful biological validation that could be use to show the significance of these values? This holds true for Figure 4: How is this biologically meaningful, and can it be validated? My bet is that the attentions from a a randomly-initialized, untrained model would be indistinguishable.\n\nIn Figure 6a-6b, why is it significant if no protein heads attend to the same region of the protein? Would it be bad if they did, or didn’t? Is it biologically meaningful?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Preliminary work; missing key baselines ",
            "review": "This paper proposes to fine-tune a BERT model using a triplet loss. \n\nStrengths\n- This is an interesting question to investigate; it wasn't used in the recent high profile protein language model results.\n\nWeaknesses\n- The authors are missing key baselines based on traditional computational biology, such as MSA-based methods.\n- The authors are missing key neural baselines including Heinzinger, et al. 2019 and Rives, et al. 2019. Those models perform better than the transformer from Rao, et al. 2019 - it would be interesting to see how the method described here performs on those models.\n- The authors do not check their models on the same tasks as Rao, et al. 2019.\nRao, et al. 2019, Alley, et al. 2019, and Rives et al. 2019 all included extensive discussion to motivate their downstream tasks. That motivation is missing here.\n- It is difficult to learn much from the t-SNE and attention plots.\n- A few concerns in the writing (easily addressable; listed below)\n\nWriting \n- The introduction could be improved by incorporating more recent works. Most of the papers mentioned are from 2015-2018.\n- The introduction states that \"pretraining...such method have yet to be rigorously tested within the context of protein modeling.\" This is not true, see Alley, et al. 2019; Rives, et al. 2019; Heinzinger, et al. 2019.\n- TAPE did not benchmark the current state-of-the-art models. Rather, they used their own reimplementation.\n- \"A drawback to Rao et al. (2019) work was that they only tested their transformer network using a character based encoding.\" It's unclear if using a BPE-like model would improve beyond this. Two papers I've seen submitted to this conference suggest otherwise.\n\nThis paper does not include the degree of empirical analysis that is expected at a venue like ICLR. This is a good preliminary exploration, but the authors would need to include additional baselines, tasks, and analysis for this paper to be accepted to ICLR.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1 ",
            "review": "##########################################################################\n\nSummary:\nThe authors proposed the Triple-BERT approach for regression tasks in protein biology: plasma membrane localization, thermostability, peak absorption, and enantioselectivity. They adopted triplet loss to fine-tune pre-trained BERT representations on each downstream dataset. They claimed that the proposed approach significantly outperforms the previous state-of-the-art. Furthermore, as an interpretation effort, the authors provide several visualization results of self-attention patterns and t-SNE of protein representations. \n\n##########################################################################\n\nMajor comments:\nWhile the paper has its own merits, unfortunately, it has several issues that need to be addressed.\n-\tWhile the authors claimed that Rao et al. has shown that the Transformer is the best candidate for modeling a protein sequence, this is not true. Instead, they have shown that each model shows different performance for different tasks. To be exact, they stated, “Our Transformer, which performs worst of the three models in secondary structure prediction, performs best on the fluorescence and stability tasks.” \n-\tIn my view, the authors left out too much information in terms of the compared methods. I could not tell how the pre-trained BERT baseline (Non-triplet) was evaluated for the downstream tasks. If the authors simply used the pre-trained representations without any fine-tuning, this would not be a fair comparison. In Rao et al, although the BERT model was frozen, they used an additional dense layer to be fine-tuned for the downstream tasks.\n-\tAssuming that the BERT baseline was only pre-trained without fine-tuning (page 6, line 6), it is difficult to say whether the fine-tuning itself was important or the combination with the triplet-loss was vital for the improvement. Please clarify the experiment setups and provide more ablation studies to see which component of the proposed work provided the performance improvement. Note that the fine-tuning itself is not novel in the protein biology tasks; Rao et al fine-tuned an additional dense layer and Min et al. fine-tuned the entire bidirectional LSTM model for the downstream tasks.\n-\tWhile the authors provided several visualization results as an effort to understand the learned representations, in my opinion, they did not provide many insightful observations. I think it’s more important to relate the visualization results with our domain knowledge and show their biological meanings, rather than simply listing how they looked like. While the authors claimed that triplet loss provided more distinct clusters in Figure 3, I could not tell significant differences between them. \n-\tDo you have a reason for focusing on the subword encodings throughout the paper? The proposed method does not exploit any subword encodings. Although the authors only compared the proposed work with those using subword encodings, I think it is necessary to compare with other methods cited by the authors, e.g., Yang et al., Rao et al., Min et. al.\n-\tCurrently, the authors do not provide detailed information or codes to reproduce the experimental results.\n\n##########################################################################\n\nMinor comments:\n-\tIn section 3.1, the authors stated that the features are pooled to form the vector representation of each protein. What kind of pooling was used here? Note that Rao et al. adopted an additional layer to compute attention-weighted mean protein embeddings.\n-\tI understand that the authors omitted detailed explanations of each task and data due to space limitations. However, to be self-contained, I would like to recommend to include more information in the appendix.\n-\tHow the cluster maps in Figure3 were generated? Please provide more detailed explanations for the figure generation. Although Figure3 is a vector image, it is difficult to view the Figure in a printed version. You may want to consider rescaling Figure 3. \n\n##########################################################################\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}