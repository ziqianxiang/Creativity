{
    "Decision": "",
    "Reviews": [
        {
            "title": "How do you compute with compressed parameters",
            "review": "The authors proposed a post-training model compression technique that, on top of sparsification and quantization, avoid storage of parameter values that are exactly the same.  \nIf I understand correctly, this is not weight-sharing that constrains model capacity during training, nor is it in a format that allows efficient computation at inference-time, but a compression method to reduce storage space of model parameters.  In this case, how does it compare to general purpose compression algorithms?  Don't they do much better?  ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some Open Questions",
            "review": "The paper identifies that after pruning and quantizing networks, there are some common pruned patterns found in the pruned network. These patterns could be further indexed and compressed using encoding approaches. \n\nPros:\n1. This is a very interesting observation of finding redundancy of compression formats. \n2. The approach is well explained and easy to understand. Also, the paper is well written.\n3. Experimentally, better compression rates are shown and observed.\n\nCons:\n1. While the paper addresses a good observation of pruning patterns, there are some open questions that is important to understand:\na. Is this an after effect of the current weight pruning approach only? In other words, does these patterns exist irrespective of what weight pruning approach is used? Can the authors show the results using different pruning approaches?\nb. Does these patterns exist without using quantization ? In other words, is the pattern introduced because of pruning or quantization?\n\n2. The experimental protocol and details are lacking completely - especially in what datasets are these models trained?\nAlso, the experimental protocols and all the hyperparemeters used during model training/fine-tuning/redundancy identification is unknown. This makes the paper highly non-reproducible.\n\n3. A suggestion - the paper title is not explanatory enough. I would suggest using something like, \"pruned pattern redundancy\" instead of \"weight redundancy\"\n\n4. In Fig 8 (a), why is there a totally unexpected pattern for Conv Layer 2 only? While Conv 3, 4, 5, follow an expected patterns.\n\nIn summary, if the authors could address some open questions in point1 and provide additional details for point2, I might be in a better position to evaluate the acceptance of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "More analysis required",
            "review": "This paper present a new way to compress pruned and quantized neural networks further using a loseless compression technique. Given a pre-trained full precision neural network, the method first iteratively prunes the network based on the methods suggested in previous works. Next, the method quantizes the weights into 16-bit. Finally, the method checks if there are repeated patterns in the granularity of specific tensor blocks (rows of kernels in conv layers, etc.), and replace redundant patterns with a flag and an index of the stored pattern. This paper presents a new technique based on the observation of redundancy, and shows that the method shows high compression capability.\n\nI find that the paper is interesting. However, I think the method must be described more precisely and the experiments must be designed more carefully to show that the proposed method is actually needed. If the questions are answered well, I'd be willing to change my evaluation.\n\n1. In section 4.2, you said that the non-zero weights are quantized to 16-bit. Is it 16-bit floating point, or fixed point? Please make it clear, as 16-bit is can be used for FP16.\n2. The method should be compared with other pruning and quantization methods. According to Table 1, propsed method seems to be able to reduce the size of weights by ~78% in case of AlexNet. While, for example, PACT (Choi, Jungwook, et al., 2018) reports that ResNet18 can be quantized into 4/4 bit (activation/weight) and AlexNet can even be quantized into 2/2 bit with minimal accuracy loss. This would lead to the reduction of weight footprint by 93.75% (32bit to 2bit) in case of AlexNet with quantization only. Is your method orthogonal to the quantization methods? Or can your method exceed the compression ratio of such papers?\n3. The decompression overhead must be described. Compared to the BSR format, proposed SBSR format requires additional iterative flag determination and index accesses to decompress the weight matrices. It seems like the additional index accesses must be done in a serial manner since the memory address of the flags are not determined prior to the memory read, but must be determined on-the-fly with some counter. Would not this harm the SIMD aspect of the BSR format and fairly increase the latency of the computation?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "limited novelty",
            "review": "This paper shows that there is still weight redundancy after pruning and quantization, and further proposes a method to reduce this redundancy by sharing same-value blocks. However, this paper has limited novelty and poor experimental support, making its contribution very limited to the research community.\n\nPros:\n1. This paper provides initial exploration on the redundancy after pruning and quantization. The study shows that along the direction of sharing same-value blocks may result in 1x-3x improvement. However the experiments conducted are not enough to validate.\n\nCons:\n1. Novelty is very limited. In the proposed compaction method, pruning, quantization, BSR are not novel techniques. Only the sharing blocks in BSR representation is a novel technique. This novelty is too little for a ICLR quality paper.\n2. Experimental results do not show support for usefulness of this technique. This new sharing block technique is applied after pruning and quantization to reduce number of parameters actually stored. In experiment results, only pure computation about how many parameters/blocks can be saved (compaction ratio) are shown. However, to show this technique is useful for research/production scenarios, many more are needed. E.g.\n              (a) Actual Memory Consumption. I suppose it may help with memory consumption. However, there is no actual CPU/GPU metrics provided for the actual memory usage.\n              (b) Computing Speed. This sharing block method may affect computing speed however no experiments related to it is provided. E.g. the cross-referencing blocks may decrease the actual computation speed thus a negative effect on efficiency. Some study should be conducted to investigate this.\n3. From 1) and 2), the contribution of this paper is not significant.\n4. Related work only includes papers before 2017? However, lots of new works in pruning and quantization after 2017 exist.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review comments #3",
            "review": "Summary:\n The goal of this work is to improve compaction ratios of the conventional BSR format. The main observation is that pruning and quantization introduce high redundancy in the weight tensors. \nIn the proposed method, the authors apply prior works' pruning and quantization methods, detect repeated weight blocks, and replace those blocks with references. The result of the proposed method outperforms the conventional BSR format. The authors also claim that vector-wise Huffman encoding works better than element-wise Huffman encoding in the pruned and quantized circumstances. \n\nPros :\n1. Well written paper. The approach is intuitive and easy to understand.\n2. Compaction ratio is higher compared to the conventional BSR format.\n3. Main observation that pruning and quantization increase redundancy in weight tensors is clever. \n\nCons:\n1. For different quantization schemes(e.g., binary-coded quantization), the proposed format may not be applicable. \n2. Resulting compaction ratio depends on the redundancy of weight tensors of the original model. However, the authors only measured the compaction ratios using a limited number of models and some of those neural network models are relatively old. \n3. The main disadvantage of the proposed format is that it breaks down the overall parallelism of the system. CSR and BSR are efficient in multi-threaded or multi-PE systems because they can be parallelized in rows. With CSR or BSR, each row can be executed on different PEs. However, SBSR breaks the independence of each row by \"referencing\" prior blocks. Reference introduces a new dependency on data access. Although SBSR may increase compaction ratios, it may not be effective in terms of latency and system throughput if all data(value) accesses are required to be sequential. Parallel execution is important in running CNN models because of the huge computational demand.\n4. SBSR uses indirect addressing mode which requires more memory accesses to access the value. \n\nQuestions:\n1. For quantization, why choose 16 bits for the quantization? It seems SBSR formats can benefit even more with lower precision.  \n2. It is not clear why vector-wise Huffman is discussed. Is it supposed to be one of the contributions in the manuscript? In the evaluation(Fig.13 and Fig.15), it outperforms SBSR in terms of compaction ratio. \n3. Do you have specific hardware in mind that is appropriate for the proposed SBSR format? Because of [Cons-3] and [Cons-4], common off-the-shelf manycore processors (e.g., GPUs) may not benefit from SBSR. In the case of recently proposed CNN accelerators, they assume special dataflow that can leverage reuse patterns (e.g., Output stationary ---  ShiDianNao, Row stationary -- Eyeriss accelerator). For such a case, they copy and store the same weights in many different PEs (which is the exact opposite of your approach). It would be helpful if you can provide a more detailed explanation about the system that benefits from SBSR.  \n4. Do you have any system-level performance evaluation using SBSR? This reviewer is wondering the relationship between the compaction ratio and the system performance (i.e., latency, overall runtime, overall throughput, memory bandwidth, and so on)\n\nMiscell: \nsome typos: indidual--> individual,",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}