{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper if fine, but it lacks a bit of rigor and novelty",
            "review": "### Summary:\n\nThe paper formulates the training of sparse neural networks as an optimization with respect to weights and weight mask parameters. Constraining the optimization by an upper bound on the number of connections is a complex problem, and the authors suggest a relaxation of this optimization problem where the mask parameters are replaced by the probability of masking the parameter.\n\nThis relaxation makes it possible to simplify the constraint on the $L_0$ norm of $w$, to a linear constraint on the probability $s$. The authors finally apply a projected gradient algorithm on this loss function: they use the gumbel softmax trick to derive differentiable gradients through the relationship between s and m, and derive rigorously the projection operator adapted to the constraint on $s$.\n\nThe authors apply their methods on CIFAR 10, CIFAR 100 and ImageNet with large VGG and ResNet architectures. They achieve better results than contemporary methods for 99% and 99.5% sparsity level. Out of the few analyses the more interesting in my opinion is to show that the probability converges empirically to a sparse binary distribution with an appropriate annealing schedule on gumbel softmax temperature.\n\n### Critical review:\n\nThe mathematical formulation in section 3 is not ideal and it seems to lack rigor. How is the equation (1) compatible with the definition $ K = k || m ||_0$ given inline afterwards ?\n\nI think it is misleading to call (2) a \"reformulation\" of problem (1) and it should be called a \"relaxation\" or something like that since the two problems are not equivalent: the constraint is softer in (2) since it applies on the probability only.\n\nAfter the intro it sounds like the method is training sparse networks throughout the training. But according to compute equation (4) and (5) the gumbel softmax gradient need the computation where m = sigmoid (...), and the mask takes values strictly in ]0,1[, therefore, the training seems to be performed with a dense network, and only at test time the network is made sparse. This should be stated clearly somewhere.\n\nRegarding the novelty, the essential mathematical innovations do not seem particularly novel: the relaxation from (1) to (2) is similar to the one in Srinivas et al. 2017 and something similar to this gumbel softmax trick was used in that Louizos et al. 2017.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Elegant and effective method for sparse training, but very similar to AutoPrune",
            "review": "This paper proposes an elegant solution for training a sparse network from scratch, i.e. identifying an unstructured mask to prune the weights at the same time as learning the weights. The method introduces masking probabilities for each individual weight, which are trained with gumbel trick to make the problem differentiable. The global sparsity is then easily controlled by the sum of probabilities. This leads to a very elegant and easy to understand (and implement) formulation. The method is compared against SOTA sparse training methods - especially Soft Threshold Weight Reparametrization (STR) method, and is shown to beat it in terms of final accuracy for given sparsity targets.\n\nThe trouble I have with this paper is that it is extremely similar to another paper that is not cited: \"AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters\" by Xiao et al - NeurIPS 2018. This paper essentially has the same solution, but in logit space rather than probability space - meaning that instead of using probabilities s_i directly, it uses sigmoid(h_i). The paper uses straight though estimator to update the h_i variables instead of using the gumbel trick, and controls the global sparsity in exactly the same way as proposed here. It is a *very* similar method, so I think this paper must cite AutoPrune and also compare itself against it and discuss the differences.\n\nI like this paper, and I think the results are significant, but the biggest issue is the similarity of it to AutoPrune. It's not clear to me whether this method is better than AutoPrune in any way - and if it is, why. Unfortunately a proper quantitative comparison may require reimplementation of that work, but it may be very easy to do given the similarity of the two works. But until there is a satisfactory discussion / comparison with AutoPrune, I think this paper shouldn't be accepted.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "initial review",
            "review": "The paper proposes to jointly learn differentiable masks with model parameters, so as to train sparse neural networks from scratch. The paper tackles the difficulty of identifying feasible layer-wise sparsity ratios through a global sparsity constraint.\n\n### pros\n* the paper conducts extensive experiments for VGG19 and ResNet32 on CIFAR-10/100, and ResNet-50 on ImageNet-1k.\n* the paper provides the ablation study to visualize the layerwise sparsity after using the probmask.\n\n### cons\n1. The idea of jointly learning differentiable masks with model parameters (and with the Gumbel trick) is not novel; it is better to cite the existing works, e.g. [1, 2, 3]. Without making a clear distinguishment/comparison, it is hard to identify the contribution of the paper.\n2. The related work section 2.1 should include more recent works (i.e. after the year 2018). Besides, even though the methods from the category of the 'sparse training from scratch' are the main competitors of the proposed method, it is still better to compare with other recent progress (e.g. pruning after the training [4, 5]).\n3. The existing results evaluate the accuracy in terms of the same pruning rate; however, due to the different layerwise sparsity ratios, right now it is hard to evaluate the performance of different methods in a more realistic setup (e.g. similar FLOPs).\n4. In ImageNet results, some results are taken from the prior work while the others are trained by authors; due to the potential different implementation details of the dense baseline, the current comparison may not be meaningful and it is suggested to also include the quality drop score compared to the dense baseline (corresponding to the same experimental setup/paper). \n5. The ablation study w.r.t. the hyper-parameter tuning overhead should be provided, as the current one introduces both Adam and SGD optimizer, with different learning rates, and with a cosine learning rate schedule (different from the conventional stage-wise learning rate schedule).  It is worth to check the sensitivity of these hyper-parameters. It is also unclear to me if the other competitors are using the same learning rate schedule or have the same amount of hyper-parameter tuning or not; based on the ImageNet description in the appendix, a list of non-standard hyper-parameters are being used, while some of the results are from the existing papers.\n6. As the paper proposes to jointly learn masks and weights, it will double the total number of trainable parameters. Thus, a study in terms of the computation overhead should be included.\n\n### reference\n1. Operation-aware soft channel pruning using differentiable masks, ICML 2020.\n2. Structured Pruning of Large Language Models, 2019.\n3. Learning Sparse Neural Networks through $L_0$ Regularization, ICLR 2018\n4. Comparing Rewinding and Fine-tuning in Neural Network Pruning, ICLR 2020.\n5. Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection, ICML 2020",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Discussion of and comparison with variational dropout missing. Otherwise good contribution",
            "review": "Summary: The authors propose a sparse training algorithm where the mask is encoded as a Bernoulli variable trained through reparameterization and Gumbel softmax. The network is initialized densely and gradually pruned until the desired sparsity is reached. The work's main goal is to have a sparsity algorithm that performs specifications globally rather than layer-wise.\n\nStrong points:\nThe work is technically sound, easy to read, and understand. Since sparsification during training is still under-explored, this is a very valuable contribution.\n\nWeak points:\n- A weak point is the technically unsound categorization of the algorithm. The authors state that their algorithm \"train[s] sparse neural networks from scratch,\" but Algorithm 1 indicates that the network is initialized densely and sparsified over time. As such, the presented algorithm is not a \"sparse training\" algorithm per se but more akin to compression algorithms, which start with a dense network. I view the work most similar to Kusupati et al., 2020 and Molchanov et al. 2017 (citation missing).\n- Molchanov et al., 2017 is the most similar work in that they start from a dense network and sparsity the network via the reparameterization trick of a binary variable over time. In fact, variational dropout is characterized by the very same parameterization that was introduced by the authors. The only difference is the global parameter capacity constraint.\n- Discussion of related work is incomplete or inaccurate. The related work discusses Wortsman et al. 2019 and Kusupati et al. as sparse training algorithms, but these start with dense weights or have gradient updates for all weights, and such do not constitute sparse training algorithms. A citation of Dettmers & Zettlemoyer, 2019, is missing. A discussion between the difference of sparse training and sparsification from dense weights is missing. Since the former problem is much more difficult it is an important distinction.\n- Table 1 shows results on CIFAR-10 but only compares to two methods and is thus not very comprehensive. It is unclear why these two methods were chosen. A comparison with variational dropout is missing.\n- Table 3: There is a handful of other methods compared at 10% of weights on ImageNet, and a comparison between the author's method and other methods is missing on this level. A comparison with variational dropout is missing. A comparison with variational dropout would show how beneficial it is to constraint the number of parameters during optimizations rather than after training and would be a great contribution. This is particularly critical in these experiments since variational dropout is quite stable at very high sparsity values. \n\n\nRecommendation (short):\nIn its current form, I recommend rejecting this work. The main reason is that the method is very similar to variational dropout, which is not cited or discussed. From the experiments, it is unclear what advantage a capacity constrain has over variational dropout where the constraint is applied after training.\n\nRecommendation (long):\nThe main point for rejection is similarity and failure to discuss and compare against variational dropout. Beyond that, the classification of the algorithm and discussion of related work is a bit sloppy: There is a clear difference between sparse training and network compression. This work is closer to network compression than sparse training. Blurring the line between these two areas makes working in either subfield confusing for both authors and readers. I would not like to see a paper accepted that fuels that confusion.\n\nComments for authors:\nThank you for your contributions. I think this is good work, but it needs a bit more polishing. I see the comparison with variational dropout as critical to making your work solid. Beyond this, I only see minor details that need to be reworked. If you decide to do the variational dropout experiments, I do not require your algorithm to be better or more favorable. If you can show that your global constraint during optimization has an advantage, that is great, but it is equally interesting if it has no advantage. If you include these experiments and fix the issues above, I am happy to accept your work for publication. Here some further comments and questions to improve your draft:\n- Why did you choose the exact methods to compare against in Table 1?\n- Table 1 is difficult to read. I know it isn't easy to improve it for sparse network papers since there are so many variables to display. Can you come up with something more readable?\n- Can you say something about the variance of weights across layers with you re-run your algorithm with different seeds. Is it always like in Figure 2?\n- Why did you choose those particular methods to compare to in Table 3? Why those sparsity ratios? There are a lot of methods that compare at 10% and 20% weights. It might be useful to include these data to give a complete picture.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}