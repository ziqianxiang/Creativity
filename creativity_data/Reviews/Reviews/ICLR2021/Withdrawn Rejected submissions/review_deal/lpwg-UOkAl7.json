{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper is hard to read and the evaluation process is not satisfactory",
            "review": "## Summary\n\nThis paper pstudy anomaly detection in hyperspectral images and propose an architecture based on ideas from the GAN and the sparse coding literature. The architecture is used in a weakly supervised framework using DBSCAN to identify background pixels that the network learns to reconstruct with compression. The proposed method is then tested on four classical image used to evaluate anomaly detection methods.\n\n\n## Overall assessment\n\n- The paper is very hard to read due to formatting (it is hard to differentiate citation from the text, use `\\citep`) and writting (some sentences are syntatically incorrect).\n- The description of the method is insufficient to understand exactly how it works. Functions $G_E, G_d, E, D_S$ are never defined. The `neural network optimization structure (GAN)` is never explicitely described and I don't understand what it refers to. Overall, the description is very confusing.\n- The evaluation process is not satisfactory either. The training loss described in (4) introduces 2 parameters but the setting of there value is not discussed. The value of the parameters used for DBSCAN ($\\epsilon$ and MinPts) are not disclosed and the process to select them is not described either. It seems that the parameter are selected without validation set but directly on to maximize the reported score, which seems problematic.\n\n\n## Minor comments, nitpicks and typos\n\n- p.4: `we search for sample of category \"1\"`: I guess the category are ordered by frequency and the category \"1\" refers to the most frequent category but I am ensure if I understood correctly.\n- p.4: `The reconstructed encoder` -> `reconstruction encoder`?\n- p.5: `the need to precompute $\\alpha$`: There is no need to precompute alpha for SC methods. It is just that computing it can be expensive but I don't see what the authors refers to here.\n- p.5: `well-learned network` -> not sure what the authors are referring to. I guess this is a `trained network`.\n- Eq.(8): why not introduce a third parameter to scale L_S relatively to L_A? The setting of this parameter to 1 seems arbitrary.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unconvincing experiments",
            "review": "This paper studies the hyperspectral image anomaly detection problem. The authors propose a sparse coding-based method for anomaly detection. They also involve adversarial training into the pipeline to better model the background distribution. \n\nMy main concern lies in the experimental part. The authors mainly verify the accuracy of the method in a qualitative approach, where some of the pixels, say, the airplanes, are defined as the target of interests, i.e., \"abnormal\" pixels, while others are backgrounds. In my opinion, such a setting will bring statistical bias to the qualitative results. The author may use the airplane pixels as a reference to evaluate the recall, but it cannot truly reflect the precisions since other background pixels may also contain \"abnormal\" pixels. Besides, the dataset used for experiments has a very limited number of samples. The authors only test on 4 hyperspectral images collected by AVIRIS, where each of them is only with 100x100 pixels. To my best knowledge, AVIRS is free of access and has thousands of imageries captured worldwide with different resolutions. The author should further verify the effectiveness of the method on sufficient data.\n\nAlso, the term “weakly supervised learning” is a little be confusing since anomaly detection should be naturally classified as a kind of unsupervised method. The “weakly supervised” signal is all from pseudo-labels, in which I believe the effectiveness is hard to be guaranteed under the proposed framework.\n\nFinally, I want to say hyperspectral image anomaly detection may not be the best fit for the ICLR venue. Although ICLR encourages research papers on diverse topics, the research may receive wilder attention on a remote sensing journal/conference such as IEEE TGRS or IGARSS.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok but not good enough - rejection",
            "review": "This work proposes a weakly-supervised HAD framework. It learns a discriminative latent reconstruction with small errors for background samples and large errors for anomaly samples, which means the latent differential image (i.e., latent reconstruction errors) can highlight the anomaly instances and restrain the background instances.\nSome major comments are as follows:\n1. In the background-category searching part, the authors use DBSCAN to prepare for weakly supervised learning and tries to predict coarse labels of the background samples. In other words, this step is mainly construct a relatively pure background dictionary. Using DBSCAN clustering is similar to use other traditional methods to construct background dictionary. Thus, it is actually lack of innovation in background samples selecting.\n2. The following network architecture using a GAN structure and a SparseNet which use an encoder to reconstruct the latent space. Overall, the network architecture is relatively simple. It mainly utilizes latent reconstruction errors to detect anomalies.\n3. All of the datasets using in the experiment part are AVIRIS sensors, considering the proposed framework’s generalization, please use the different sensors in the experiments. Thus the validity and generalization of the proposed framework are better proved.\n4. In the experiment part, to make the proposed method SparseHAD more convinced, the authors are suggested that using GAN-based or AE-based anomaly detection methods and sparse repressentation based method as the compared methods.\n5. This paper has some grammar and spelling mistakes, for example, in the page 2, “weakly supervised learning (WSL) are not always ground-ture…”, the authors should use ‘ground-turth’ to instead of ‘ground-ture’. Please carefully check the whole paper and revise them.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}