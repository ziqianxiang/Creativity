{
    "Decision": "",
    "Reviews": [
        {
            "title": "Fewmatch: Dynamic Prototype Refinement for Semi-Supervised Few-Shot Learning ",
            "review": "##########################################################################\n\nSummary: \n\n \nThe paper presents FewMatch as a 3-stage semisupervised few-shot learning approach. (1) Prototype Initial Inference (PII), via imprinted weights (2) Explicit prototype refinement and (3) Implicit prototype refinement using consistency regularization.\nTheir approach iterate between steps 2 and 3. The authors performed their experiments on mini-imagenet and tiered imageNet and showed that their method outperforms  the SOTA methods\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, my rating is pretty much borderline for this paper. I like the idea of dynamic prototypical refinement and that the results are better in chosen datasets.   My main concern is about some additional ablation models. I look forward to seeing the authors can address my concern in the rebuttal period. \n\n \n##########################################################################Pros: \n\n \n1. The paper addresses an important problem, which is leveraging unlabeled data to improve few-shot learning, also known as semi-supervised few-shot learning (SS-FSL). \n\n \n2. The authors propose an SS-FSL approach that has some novelty by proposing iterative Dynamic Prototype Refinement (DPR) as a training strategy for a few-shot model adaptation to unseen classes. The approach integrates ideas from meta-learning based Few Shot Learning (FSL) and also transfer learning-based FSL. \n\n \n3. This paper provides experiments on miniImageNet and tieredImageNet, showing that their DPR approach outperforms existing SS-FSL methods on these datasets. \n\n \n##########################################################################\n\nCons: \n\n \n1. The system is complicated, requiring >3 stages of training as the second and the third step alternate multiple times before training is complete. It will be good to have some time analysis comparing DPR to runner-up methods. \n\n \n2. More ablations are needed to study the effect of the number of alternate iterations between steps 2 and 3 (denoted as M in the paper). Also the effect of \\lambda in Line 218 and \\lamba_ft in Equation 269. \n\n\n3. I would suggest performing experiments on additional datasets as CUB-200-2011 dataset (in table  5 and 6 in TransMatch (Yu et al., 2020)) .\n\n4. why results in [R1, Li etal, NeurIPS 2019] are higher than the results in this paper. The results of this paper are not covered and it will give more context to comment on the differences. \n\n[R1] Li, X., Sun, Q., Liu, Y., Zhou, Q., Zheng, S., Chua, T. S., & Schiele, B. (2019). Learning to self-train for semi-supervised few-shot classification. In Advances in Neural Information Processing Systems (pp. 10276-10286)\n\n\n\nminor\n---------\nfor clarity, UD and LD are better to spell out as unlabeled and labeled data. \nLine 210: missing, before \"we.\"\n\n \n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper is clearly not ready for publication.",
            "review": "The paper proposes a semi-supervised method to handle the few-shot learning problem. \nIt consists on three steps: 1. training a classifier (it is the one proposed in [Qi et al., 2018]) using support set, 2. using unlabeled data sets to fine-tune the learned classifier, which are then used to select confident unlabeled samples to be augmented to the support set; 3. using a teacher-student networks obtain a refined model (it is the one proposed in [Tarvainen & Valpola, 2017]). \n\nThe paper is quite hard to read. Many sentences are broken (even those in the abstract). Grammar issues and typos abound. The authors use way too much abbreviations, which are not necessary and hurt the reading experience. \nEven the name of the proposed method is not consistent: Fewmatch or FewMatch?\nStrict proofreading is required.\n\nApart from the fact that the paper is not carefully scrutinized, the novelty is also very limited. \nAs summarized above, the proposed method mainly base on [Qi et al., 2018] and [Tarvainen & Valpola, 2017]. The paper merges these two and adds a data augmentation module (to select confident samples from unlabeled data sets). However, the latter is used in semi-supervised learning or active learning quite often. See [1-3].\n\nThe paper also lacks discussion and empirical comparison with a series of existing semi-supervised learning methods which are also used to solve FSL problem [1-3].\n\nOverall, in terms of novelty, presentation, and evaluation, the paper is not ready to be accepted. \n\n[1] Learning Algorithms for Active Learning, ICML, 2017.\n[2] Transductive Relation-Propagation Network for Few-shot Learning, IJCAI, 2020.\n[3] Few-shot learning with graph neural networks, ICLR, 2018.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "In this paper, authors propose a dynamic prototype refinement framework to address the few-shot learning under a semi-supervised setting. ",
            "review": "Summary: \nIn this paper, authors propose a dynamic prototype refinement framework to address the few-shot learning under a semi-supervised setting. More specifically, the weight imprinting is used as the fundamental structure for pre-training and further fine-tuning. In the pre-training stage, the additional unlabeled data is utilized in a teacher-student semi-supervised learning framework with consistency regularization, while during fine-tuning, the pseudo-labelling is adopted to select most confident unlabeled samples for refining the class prototypes with the consistency regularization loss.\n\nStrength: \n- Overall, the paper is well-written with clear motivations. In particular, authors identify the main issues of two popular semi-supervised learning approaches under the few -shot setting, i.e.,  iterative pseudo-labelling often yields models which are susceptible to error propagation and sensitive to initialization, while the consistency regularization is struggle with the limited training data which yields the unreliable predictions during the early iterations;\n- The proposed Dynamic Prototype Refinement is interesting, which combines the advantages of metric and meta-gradient based few-shot learning methods; especially it now affords CR convergence by enhancing the initialization through both implicit and explicit prototype refinement mechanisms;\n- Detailed descriptions of the framework as well as extensive experiments  on two standard benchmarks are demonstrated, experimental results against a series of prior CR and self-supervised methods with clear accuracy gains;\n\nWeakness: \n- The novelty of contributions are still considered marginal. Specifically, the proposed framework is composed of existing methods which are widely adopted in few-shot learning and semi-supervised learning, such as imprinted weights formulation and teacher-student semi-supervised learning framework. \n- The key difference against conventional CR approach that authors have highlighted in their setting is that the CR loss is directly depends on prototype instantiations. However, this is considered naturally when employing the imprinting framework rather a novel design;\n- The selection of top-k confident unlabeled sample process can be somewhat viewed as the pseudo labelling and it is iteratively adopted when step2 and step3 are performed iteratively.\n- In the implicit refinement stage, the prototypes are further updated with the gradients from top K confident samples selected from the teacher model, I'm curious about the performance if the samples are selected from the student model. Are the strong augmentations same as the ones applied in the explicitly refinement stage?\n- Also lack of some experimental comparisons against state-of-the-art transductive based methods, such as [1]Transductive information maximization for few-shot learning, NeuroIPS20; [2]Dpgn: Distribution propagation graph network for few-shot learning, CVPR20.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable method, but bears similarity with methods from recent SSL literature. Experiments need clarification.",
            "review": "Summary:\nThe paper presents a training algorithm for few-shot learning under semi-supervised settings. Unlike typical FSL setting, the paper assumes additional availability of unlabeled data for both base and novel classes. The algorithm is based on imprinted weights (IW), where the weights are imprinted not only using the labeled data, but also using confident unlabeled data (top-K) with pseudo-label (explicit classifier refinement). Once refined, the classifier and feature extractor are end-to-end trained using consistency regularization as in mean teacher method, but using weak and strong data augmentations (implicit refinement). Dynamic refinement alternates between explicit and implicit refinement steps. Experiments are conducted on mini and tiered ImageNet, showing strong performance.\n\nPros:\n1. The paper presents strong results on SS-FSL benchmarks. \n\n2. The presentation of the algorithm sounds reasonable.\n\nCons/comments:\n1. In table 2, while the effectiveness of explicit refinement has been demonstrated when comparing against consistency regularization at pre-training step only, its effectiveness in the entire framework is not clear. Moreover, it is unclear whether two rows without or with DR (or DPR) are trained for the same epochs. From Figure 2, it seems a single step of DPR, which I assume is the result for PCR + ER + IR, seems not converged. Therefore, leave-one-out analysis (e.g., PCR + IR while excluding ER but with longer training) is encouraged.\n\n2. One concern is the novelty. Many recent literature has shown the effectiveness of consistency regularization using weak and strong augmentations for semi-supervised learning, so it is not a surprise that the proposed approach could work much better than methods without using such data augmentations, such as LST. This begs a question, related to comment 1, that how important the explicit classifier refinement step.\n\n3. How correlated the unlabeled data accuracy in Figure 2 and 1-shot/5-shot performance in Table 2? For example, the performance of Remixmatch in Table 2 (IW + remixmatch) is 62 and PCR is 61, while unlabeled data accuracy at 0-th epoch of the former (red curve) is around 58 and the latter (blue) is around 68. Given strong SSL performance of Remixmatch (e.g., the performance of Remixmatch using 4 labeled examples per class is > 80% on CIFAR-10), it is a bit hard to believe that there is such a huge performance gap between the proposed method and Remixmatch. Overall, it will be instructive to provide how Remixmatch is trained and evaluated. \n\n4. FixMatch (Sohn et al., 2020, uploaded on Arxiv Jan 2020) has shown good performance on semi-supervised few-shot learning without labeled or unlabeled data from base classes (e.g., 4 labeled data per class, i.e., 4-shot), thus in more challenging setting. Given the similarity of the algorithm, it is encouraged to make a comparison to it.\n\n5. EMA decay rate of 0.5 is used in this paper, whereas mean teacher paper found 0.99 to be optimal and showed significant performance drop if below 0.9. As there is a huge inconsistency in findings from previous work, the performance w.r.t. different EMA decay should have been studied.\n\nReason for rating:\n- The efficacy of individual components (ER, IR, DPR) is not convincingly studied (see 1, 2 of Cons). \n- Given the fact that consistency regularization with weak and strong augmentation has been extensively studied in SSL, the novelty of IR or novelty of applying it to SS-FSL is limited.\n- I would consider increasing my rating if the effectiveness of ER and DPR are convincingly addressed in the rebuttal.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}