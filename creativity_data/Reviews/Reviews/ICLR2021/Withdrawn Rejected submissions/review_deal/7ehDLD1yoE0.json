{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper gives a gradient-free method for generating adversarial examples for the code2seq model of source code.\n\nWhile the reviewers found the high-level objectives interesting, the experimental evaluation leaves quite a bit to be desired. (Please see the reviews for more details.) As a result, the paper cannot be accepted in the current form. We urge the authors to improve the paper along the lines that the reviews suggest and resubmit to a different venue."
    },
    "Reviews": [
        {
            "title": "An Adversarial attack strategy for code2seq model.",
            "review": "The paper proposed an Adversarial attack strategy for the code2seq model.\nThe authors observed that L2 distance between pre- and post-training embedding of a token varies significantly based on the token’s frequency. Based on that they devised strategies (black and white box) to perturb the sub-token. Here, they change the local sub-token names using three heuristics. \n\nI like the simple approach to launch the adversarial attack, which shows a promising result. However, I have the following concern:\n\n-\tThe paper lacks motivation. Are there any security implications of such attacks (e.g., malware classification, etc.)? \n-\tOnly for one model (Code2Seq) the authors have tested their scheme.\n-\tDid not compare with other attacks that work on discrete domains (especially in NLP there are many attacks)\n-\tThe three strategies proposed in the paper for perturbation. However, these strategies are quite ad-hoc (e.g., why suddenly choose 5 sub-tokens).   \n- The paper said a black-box attack (even in the title), but they reply on white box information showing that L2 distance of high-frequent tokens are more.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for STRATA: Building Robustness with a Simple Method for Generating Black-box Adversarial Attacks for Models of Code",
            "review": "The paper proposes a gradient-free method to craft adversarial examples for Code2Seq model. Code2Seq model generates textual summary of code snippets.\n\nOverall I think authors proposed an interesting idea of an attack, however evaluation should be improved. Thus I recommend to reject paper at this point, but encourage authors to improve the paper and resubmit.\n\nStrong points:\n* Proposed novel interesting idea of gradient-free attack on source code\n* Paper is well written and easy to understand\n\nWeak points:\n* My main concern is evaluation. While authors claim that they compared to Ramakrishnan et al, in practice it looks like this comparison was performed using different baselines. Which makes results questionable.\n* The whole paper is about attacking one very specific model (Code2Seq) and it’s not clear what is the motivation of attacking this specific model (instead of some other). It’s also not clear how easy proposed method could be adapted to other tasks (which use code as an input). \n* Adversarial training is done by generating adversarial examples only one time (before the last epoch). This may lead for the model to overfit to specific adversarial examples.\n* No comparison of adversarial training with any cheaper mechanism. For example, adding random noise to embeddings during training.\n\nRecommendations on how to improve the paper:\n* Improve evaluation, add fair comparison (i.e. on the same baseline model) with other methods.\n* Show how this method could be adapted to different tasks which use code as an input and add evaluation on these tasks.\n* Update adversarial training to be consistent with what is typically done (i.e. adversarial examples generated on each training step for current state of the model) or provide motivation for the current way adversarial training is implemented.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, limited results",
            "review": "Summary:\n\nThis paper proposes STRATA, a novel adversarial attack against source code models, more precisely against code2seq. The attack strategy can be applied under black- or white-box threat models, targeted or untargeted. Adversarial training is based on STRATA adversarial examples is proposed to render the models robust. Experiments are performed on Java code datasets of variable sizes.\n\nStrong points:\n- STRATA is based on the interesting observation that token frequencies and their $L_2$ norms are strongly correlated. This information provides an elegant and effective strategy for choosing attack tokens.\n- The proposed attack seems computationally inexpensive.\n- The paper covers what one would expect when proposing a new attack, including performance evaluation, study of transferability and adversarial training results.\n- The paper is clear and well-written.\n\nConcerns:\n- STRATA seems limited in some respects: it requires access to a similar code base, can only be applied for functions with local variables, only replaces variable names.\n- The proposed strategies for generating replacement tokens are trivial and easily detectable (e.g., concatenating the same token five times). The more subtle strategies are significantly less effective.\n- The proposed adversarial training strategy does not seem enough for producing a robust model: only one epoch of fine-tuning on adversarial samples is performed at the end of natural training. This is coupled with unconvincing experimental evaluations of adversarial training. As such, the paper does not seem to prove that a robust model can or has been trained using STRATA samples.\n- The notion of adversarial examples against code in the sense used in the paper seems vague. Appendix Fig. 3 shows a qualitative example of attack, where \"product\" is replaced with \"identity\" in all variable names of a function. The model (incorrectly) predicts the name of the function being related to \"identity\", not \"product\". However, that prediction seems reasonable; a human would most likely predict the same, considering that nothing in the code of the function is related to \"product\". Can this indeed be considered an adversarial example?\n- It is unclear why the comparison to previous attacks is done under different metrics depending on the attack. Moreover, why is STRATA trained on the large Java dataset, but evaluated on the small one (Appendix, Tab. 7)?\n\nReasons for score:\n\nOverall, I lean towards rejection. I like the idea behind choosing attack tokens, but the experimental results and derivation of the adversarial training strategy do not seem thorough enough. Moreover, an attack designed specifically for models against code, tested on one architecture and one programming language only seems relevant to a small part of the community.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple adversarial attack against code2seq",
            "review": "This paper proposes STRATA, a simple adversarial attack against the code2seq model. The key idea is to replace local variable names in the input code with other randomly chosen sub-tokens with embedding vectors of relatively high L2 norms. Meanwhile, they observe that such tokens often appear frequently in the training set, thus alternatively they can simply use frequently appeared tokens as the target to perform the attacks. In this way, they can attack the model in the black-box scenario, without the knowledge of the model parameters and the training data, as long as they can roughly approximate the frequency distribution of different code sub-tokens in the training set. They evaluate their approach on code2seq models trained for the Java code, and compare with existing attacks against code2seq models. They first show that the 5-same attack, i.e., repeating a sub-token 5 times and concatenating them as the new local variable name, is the most effective attack. This attack decreases the F1 scores more compared to the baseline attack from prior work. In addition, they show that by adding STRATA adversarial examples for adversarial training, the new model becomes more robust to their proposed attacks.\n\nRobustness of models of code is an interesting topic, and the authors show that a simple and computationally inexpensive attack could degrade the performance of the code2seq model. However, this paper suffers from both the approach design and the evaluation, and I discuss the details below.\n\n1. A very straightforward defense is to anonymize the variable names. In particular, for 5-same attacks, although each sub-token itself is common, the concatenation of them doesn't seem natural, and thus a simple input preprocessing could already make the attack ineffective.\n\n2. The authors mention that they select ~40,000 test samples from the original test set to perform the attacks. In this case, when computing the F1 score, is it computed among the 40,000 test samples, or on the entire test set? For results of attacks proposed in prior work, have you evaluated them yourself, or did you quote the numbers from previous papers? I see that the baseline results are the same as shown in Ramakrishnan et al., but I don't think they limit their attacks to code with local variables, thus I would like to double-check.\n\n3. When evaluating the adversarial training and the transferability of the previous defense, they show that the previous defense does not show better robustness against STRATA. On the other hand, training with STRATA may also not improve the robustness against previous attacks. Have the authors evaluated the transferability of the model trained with STRATA adversarial examples? Meanwhile, did you train the same model to conduct attacks using STRATA and the baseline? I noticed that the F1 scores of models without perturbations are different for STRATA and the baseline attack.\n\n4. The scope of experiments is pretty limited. The authors only focus on a single model (code2seq) for one task (Java). The authors should extend the experiments to more models and/or more code-related tasks, or at least different programming languages, such as those evaluated in the code2seq paper or previous papers on attacks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}