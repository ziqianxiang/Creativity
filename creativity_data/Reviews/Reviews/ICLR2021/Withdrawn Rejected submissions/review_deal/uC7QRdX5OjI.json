{
    "Decision": "",
    "Reviews": [
        {
            "title": "Dialog systems paper is somewhat unsurprising from representation learning standpoint",
            "review": "The paper intends to contribute three items: an extended dataset, an approach to applying reinforcement learning with that dataset, and evaluation highlighting the role of combined reinforcement learning and pre-training (representation learning) in the new tasks. Considering this paper as an ICLR submission, the primary contribution seems to be the training scheme with the new dataset and tasks as supporting contributions. The key representation being learned is the shared representation that fused setting, identity, motivations, and the context of the ongoing conversations (with motivations being the novel addition).\n\nThe paper is well-written and uses sound technical methodologies throughout. The ablation study makes a strong case for the utility of combining interaction-based training with other kinds of pre-training (particularly when the evaluation metrics consider only interaction-specific performance).\n\nThe main weakness of the paper is in how it (fails to) frame itself a representation learning project specifically relevant to ICLR. The key representation is not named, nor is the word “representation” used in the body text of the paper. The paper reads like an excellent submission for a dialog systems conference that happens to have some representation learning at the center.\n\nA secondary weakness of the paper is the logic by which the evaluation tests the hypothesis. By comparing systems in an inherently interactive setting (where they earn points for achieving goals), it is not surprising that RL-based training contributes the most to the outcome. Instead of seeing how much it helps to pre-train on Reddit and LIGHT-Original when evaluated on the interactions of LIGHT-Quests, the hypothesis might have better been tested by how much interaction-based training on LIGHT-Quests improves performance on non-interactive evaluation based on Reddit or Light-Original. Does practice using language to achieve goals give an agent new insight into static texts? This reviewer can’t recall another paper using RL-based training as a pretext task to learn representations for non-interactive use -- that would be interesting to see.\n\nRecommendation: (4) Okay but not good enough - rejection. Considering this specifically as a submission to ICLR and not a dialog systems conference, the representation learning experiments aren’t particularly exciting. Using a bit of non-interactive pre-training on a static corpus to improve the representations exploited in an interactive setting is a familiar pattern. Using interaction-based training to pre-train representations for use in another task -- that would be an exciting new representation learning method -- but that’s not what is done here (and if it were done, it might improve the rhetoric of the paper even for a dialog systems venue).\n\nQuestions for the authors:\n- Given that agents can (sometimes?) satisfy their own goals by directly taking actions, it seems like an upper bound for the Act Goals metric could be constructed by measuring the performance of an agent that is constructed to trivially optimize this score (while doing terribly with respect to Speech Goals). If such an upper bound were available, it might help interpreting scores like 0.420 on the Act Goals metric. Is such an upper bound easy to construct.\n\nSection-by-section reactions:\n\nAbstract:\n- As representation learning is the focus of the conference, a stronger abstract might make the connection to representation learning more explicit. Certainly, representations are learned, but it’s happening as part of a larger application. What do the authors want us to see as the key representation learning contribution? At least identify what the key representations are intended to represent. Is it something like “states in an interactive dialog setting where the speaker is trying to achieve a goal”?\n\nIntroduction:\n- Good motivation for focusing on interactive worlds over static data.\n- Even with the explicit list of contributions (thanks for including it), it’s hard to decide the representation learning contribution. It looks like the word “representation” doesn’t appear in the body text of this paper. In reinforcement learning terms, should we be particularly focused on the state or observation representation?\n- The introduction should highlight the challenge of building the key representation and the potential value of training representations like it for future applications.\n\nRelated Work:\n- Excellent throughout!\n\nLIGHT-quests and ATOMIC-LIGHT:\n- Great!\n\nAgents that Act and Speak:\n- Ah, Figure 4 finally makes clear what key representation is being learned. The data on the black wire flowing through the switch needs a good name. This is indeed a representation learning paper, but the abstract and introduction set it up as if it were intended for a different venue on natural language dialog systems.\n\nEvaluation:\n- Reasonable.\n\nConclusion:\n- This section suggests “interactivity is the key to language learning” as a hypothesis of the work, but the evaluation revolves around performance in an interactive setting. To test this hypothesis, we should be comparing in a setting where non-interactive training was directly comparable (e.g. next token prediction on a held-out dialog script dataset). That just interactive training (Scratch) provides most of the benefit for the Act Goals and Speech Goals metrics that were defined in terms of those interactions is not surprising. Further, that the Adaptive model performs best suggests that, at least in some sense, interactivity is not key to language learning or that it is only part of the story.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "This paper presents new datasets, LIGHT-Quests and ATOMIC-LIGHT, for large scale crowd-sourced fantasy text-game. Then authors introduce a reinforcement learning system that incorporates the large-scale language model to the RL agents. In experiments, the authors compare the results between reinforcement learning and behavior cloning with various pre-training models.\n\nOverall, this paper provides a new RL environment for crowd-sourced fantasy text-game, but it’s unclear what the main contributions and what information and insight are intended to be conveyed in this paper. If the main contribution of this paper is to propose a new environment, then possible research topics and issues should be included with simple baselines. Moreover, the characteristics of the task should be described in more detail in the paper. Otherwise, comparison and analysis with clear baseline algorithms should be included. However, it seems that the contribution is vaguely described in this paper.\n\nBesides, for the evaluation section, I’m not sure what the authors want to show and claim through the experimental results, and I think that the analysis of the results seems to be insufficient. For example, in the case of Table 1, it shows a result (fine-tuning with RL after pre-training is less effective than RL from scratch) contrary to common sense, but there is no analysis or intuitive explanation in the paper. Additional experiments or explanations for this result are required. \n\nI acknowledge the considerable work of data collection and engineering, but it is unclear what the importance of the environment suggested in this paper and what the author is trying to convey through the experimental results.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting New Task Dataset + Agent, but Ablations Bring Motivations into Question",
            "review": "Summary and review decision: In the interest of developing agents that can communicate in pursuit of a goal, the authors introduce a dataset of quests in a fantasy text-adventure game called LIGHT and develop an agent that can navigate these quests with a partner agent. Their training method combines reinforcement learning, language pretraining, and language \"naturalness\" losses. Because it is unclear how the proposed dataset addresses the motivation (see weaknesses bullet 1) and the performance of the proposed agent is difficult to calibrate this submission is not ready for acceptance.\n\nOverall Quality: The stated purpose of this paper is to develop an agent that can act and communicate with another agent in pursuit of a goal. But, table 2 seems to show that the collected dataset is better solved by ignoring dialogue. Other contributions--adapting ATOMIC to enable commonsense reasoning in LIGHT, the RL switch agent architecture and training scheme, the comparison of different agent pretraining methods--are interesting but from the experiments it is unclear how these aid in the authors' goal.\n\nClarity: The main contributions of the paper could have been stated more clearly throughout.\n- For example, the method section describes using ATOMIC-LIGHT as a pre-training corpus, but this is ultimately combined with a different dataset from Reddit and used in an adaptive training scheme that is not described in much detail. An adaptive training scheme without atomic-light is not shown. Is ATOMIC-LIGHT also a main contribution?\nIt was difficult to understand how the dialogue enabled agents to pursue their goals (which is the stated motivation of the paper). Some concrete examples of dialogue with other agents being necessary to achieve a goal would make the motivation clearer.\n\nOriginality: Many of the training methods employed are straightforward applications of existing methods, but the setting is novel. The authors set up an adaptive pre-training scheme, but it is difficult to understand the originality without more detail.\n\nStrengths:\n- The authors provide a dataset that could help researchers interested in the role of dialogue in accomplishing goals.\n- The authors make an interesting, yet counterintuitive observation: pretraining on commonsense/language data hurts performance relative to an RL agent trained from scratch.\n\nWeaknesses:\n- Table 2 seems to indicate that dialogue doesn't contribute to successful goal completion in the collection of the dataset, which was its stated purpose.\n- The described RL agent does not include many novel components and its hard to calibrate its performance without more baselines.\n\nQuestion:\nIn the example in figure 3, why does the human need to use dialogue to achieve its goals? could it still accomplish its goals by using the actions available to them? e.g., by hitting the knight and grabbing the egg?\n\nTypos:\nintro, para2: objectsa -> objects",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "a good dataset paper",
            "review": "Summary:\n \nThis work aims to create agents that both act and communicate with other agents in a fantasy text-game. To achieve this goal, the authors introduced LIGHT-Quests, a collection of quests for LIGHT and each quest consists of environment description, quest motivations and human demonstration. In addition, authors constructed the themed commonsense knowledge graph ATOMIC-LIGHT adapted from ATOMIC and proposed a commonsense reasoning-based pre-training strategy that leverages ATOMIC-LIGHT for zero-shot goal completion.\n \nPros:\n \n1. The proposed dataset LIGHT-Quests provides novel environments for training text based RL-agents, and it is beneficial for future text-game research.\n \n2. The proposed adaptive pre-training strategy significantly improves the baseline model that trained from scratch.\n \nCons:\n\n1. Authors hypothesized that commonsense knowledge is an important prior to ensure successful interactions. However, the experiments didn’t provide the ablation study to compare the performance between Adaptive model and Adaptive model without pre-trained on ATOMIC-LIGHT (Table 1).\n \nQuestions:\n\nIt is not clear to me that pre-trained on General and Light hurt the model performance in RL setting. Could authors explain?\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}