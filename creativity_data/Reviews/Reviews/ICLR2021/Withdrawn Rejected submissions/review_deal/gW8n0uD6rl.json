{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an interesting approach that leverages shared dynamics across causal systems for improved joint causal discovery.  The reviewers and AC all agree that the approach is interesting, promising and that the paper is well written. \n\nWhile theoretical validation would be an exciting thing to have, it is perfectly acceptable for the paper to focus on an empirical study. But in this case, it is very important to provide convincing evaluation experiments.  As several reviewers have pointed out, in order to convincingly demonstrate the value of the approach, it would be very important for the experiments to go beyond noiseless systems.  We strongly encourage the authors to address this point as this will significantly strengthen the significance of their contributions."
    },
    "Reviews": [
        {
            "title": "Causal discovery based on shared dynamics is exciting but does this method work on real(istic) data in the presence of noise?",
            "review": "The paper makes an observation that signal dynamics common to a class of causal systems may contain strong information to enable the use of the encoder of the Neural Relational Inference (2018) for extracting (Granger) causal graphs. It minimally extends the NRI model with an empty edge type and demonstrates that the observation holds in a few cases of dynamical systems. Additionally, to handle the unobserved common causes the paper shows improvements in ROCAUC when the encoder is modified to model them directly.\n\nThe paper discusses a potentially interesting use of the Neural Relational Inference for (Granger) causal discovery in multivariate time series. Although lacking in detail, it is very clearly written. The proposed method of causal discovery for Markov order one system does demonstrate impressive performance on smooth dynamical systems.\n\nPossibly, the greatest weakness of the NRI approach in this formulation is the seeming reliance on relatively smooth noiseless dynamics. While for the NRI in the original paper (2018) it was within the proposed scope, the claimed use of NRI in this paper is not fully supported by experiments. All of them are from a smooth dynamical system. The Particles experiment is no different from NRI (2018) with the same reported AUC result. Even the most realistic dynamical system from Smith et al. 2011, which is also quite simplistic by the way, still model hemodynamic lag and thus a relatively smooth signal. Notably, this last less smooth of all presented signal is where the NRI model performs the worst.\n\n-   How would NRI in this proposed application behave if we use a still simplistic but potentially less smooth VAR model? Would be good to test it with different SNR.\n-   In general, even in dynamical systems used for method demonstration, what happens at various noise levels?\n-   How would it behave when the assumption of the model is slightly violated and the signal is generated by an SVAR model?\n\nGiven the above limitations, it is unclear if the paper presents something substantially novel/interesting. Technically it is still the NRI model of 2018, so not much contribution there. That would not be a problem if the paper would manage to indeed demonstrate that the model is useful for causal discovery in multivariate time-series. The experiments are limited to very specific kinds of signals and thus not convincing of the approach generality.\n\nIt is not immediately clear from preliminaries, whether the proposed model is capable to recover relations of a Markov order higher than one. The described \"summary graph\" as a compressed representation only holds true to the Markov order one underlying dynamic Bayesian network-like structure (see D. Danks and S. Plis. Learning causal structure from undersampled time series. In NIPS 2013 Workshop on Causality, 2013.), yet the description of the model is a bit vague and may indicate that the method works with the full unrolled graph and then folds it into Markov order one. This part, despite the extensive use of notation, is not clear. It would be simpler to say that indeed an unrolled representation of a graph of limited Markov order is used. It is not clear though if there are any restrictions on the Markov order as discussed since the encoded graph holds just Markov order one.\n\nWhile the problem of unobserved common causes is discussed, only a single type is considered - fully missing random variables (or particles). Yet, a simple mismatch in measurement and causal time scales will violate some of the assumptions (e.g. generate samples from the VAR model and drop every other time sample for all variables and the model can be fit with SVAR and not VAR). See:\n\n-   M. Gong, K. Zhang, B. Schoelkopf, D. Tao, and P. Geiger. Discovering temporal causal relations from subsampled data. In Proc. ICML, volume 37 of JMLR W&CP, pages 1898–1906. JMLR.org, 2015\n-   S. Plis, D. Danks, C. Freeman, and V. Calhoun. Rate-agnostic (causal) structure learning. In Proc. NIPS, pages 3285–3293. Curran Associates, Inc., 2015\n\nFrom the paper it is unclear what encoder and decoder models were used and can be used to perform the experiments or use the approach. How does the graph enter the decoder together with a variable-length input? If the Decoder is MLP the input layer has a fixed size. Possibly, the MLP is fed one sample at a time with a vector representation of the encoded graph, but that contradicts the paper description. Generally unclear. Supplement mentions that the models are exactly the same as for the NRI paper, but this is still not enough information and does not make the paper self-contained.\n\nFrom the description of experiments it is unclear how many random variables were used and how many can the NRI handle.\n\nWhat was the experiment in Figure 3? How many particles/random variables? Was the training done on a single causal generative graph while testing performed on a variety of different unseen graphs? Was it a random sample of a large number of different graphs for both testing and training? Is NRI even capable of learning transferable information from data generated from a single graph?\n\n-   Note, reference P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. Causation, prediction, and search. MIT Press, 2000. - Heckerman is *not* a co-author of [this book](https://mitpress.mit.edu/books/causation-prediction-and-search-second-edition)\n-   Last paragraph of page 3 $x\\in \\mathbb{X}$, the $\\mathbb{X}$ is not defined.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting experimental work but lacking some implementation details and theoretical supports",
            "review": "The authors proposed a framework called Amortized Causal Discovery (ACD) for recovering causal relationships in time series where samples are generated from models with different underlying causal graphs but shared dynamics. This framework is applicable in settings such as modeling neural spiking trains where the dynamics of how neurons react to the activities of other neurons remain the same. In the proposed framework, they considered a causal discovery encoder $f_{\\theta}$, which tries to extract causal relationships and map it to a latent space. Moreover, there is a dynamic decoder $f_{\\phi}$, which provides one-step predictions. The proposed architecture is similar to a variational auto-encoder and the encoder part is based on graph neural networks.\n\nAlthough experimental results show that a considerable improvement in some dataset (even in the presence of a latent variable), there are some concerns about the contributions of this work and also its presentation:\n\n1- About the two main assumptions: The authors considered two assumptions on the causal model of time series (no instantaneous effect and invariant causal graph in time). We know that both assumptions are not satisfied in many practical applications. For instance, we have instantaneous effects if the sampling rate is low. Moreover, most causal mechanisms are changing in time.  As an example, in (Pfister et al., 2019), this problem has been addressed by detecting change points. I am not sure whether that approach is promising or not but it is expected from an experimental work to handle a more general setting.\n\n2- About the shared dynamic: Is it possible to check whether the assumption of shared dynamic is valid in the observed data? What if there are some small changes in the dynamics across different training samples.\n\n3- In Eq. (12-14), it seems that we are using the values of other time series at only time $t$ to give a one-step prediction of a particular time series. Why do not we consider the history of previous values from time $1$ to $t$?\n\n4- About handling latent confounders: As noted by the authors, recovering causal relationships is challenging in the presence of latent variables. Although experimental results show that the proposed method has better performance than baselines, it is still unclear whether there are some theoretical guarantees in recovering the correct causal graph using the proposed method.\n\n5- The presentation of the paper could be improved:\n*Please provide the exact definition of $\\hat{\\mathcal{G}}_x$, $\\mathbb{G}$, $\\mathbb{X}$, $r$,....\n*Please explain the two methods (the amortized encoder and TTA) in more detail on page 4.\n\n(Pfister et al., 2019) Pfister, Niklas, Peter Bühlmann, and Jonas Peters. \"Invariant causal prediction for sequential data.\" Journal of the American Statistical Association 114.527 (2019): 1264-1276.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but lacking investigation into identifiablity and consistency",
            "review": "This paper studies the problem of learning causal graphs from time-series data. A new framework, called Amortized Causal Discovery (ACD), is proposed based on the observation that samples often share identical or similar dynamics. ACD is then implemented with a variational model, mostly following existing works. Overall, I think it is a very interesting idea to consider shared dynamics and, in some sense, to 'learn a score function' from data, compared with existing score-based methods that usually need to put assumptions on data generations to make the score function sound. However, there is also a key problem, identifibility and consistency, to ACD, for which I do not recommend an acceptance for now. I would be happy to increase my score if authors can address my concerns. \n \nIdentifiability and Consistency: (a) identifiability is an important problem to causal discovery from observational data. I find that only Peters et al, 2013 is cited for such an issue. However, in that work, the identifibility result requires a much stronger condition than the assumption used in this paper. It would be good to discuss this issue in more details. (b) as stated in Section, 4, the proposed approach can be seen a score based method with learned score functions. However, it is unclear when this score function could be 'consistent', i.e., maximizing the score function would lead to a consistent estimate of the true graph (possibly under some model assumptions). (c) Another part, related to this learned score function, is whether the overall approach is consistent. In particular, Eq. (5) has a regularization term (which is often a sparsity term, as stated in the paper). The question is how to pick this penalty term, including its weight, in practice? This is crucial for causal discovery and I do not find that this issue is explained in the current version. \n \nWriting: the overall writing is very good, but could be improved for some places. (a) Eq (12) uses $z_{ij, 0}$ which was not introduced until the next paragraph; (b) it seems that the problem size (how many variables of the problem) is small. It would be good to introduce the problem size explicitly in the main content. (c) as mentioned above, the identifiably and regularization term are not clearly discussed; they are however important to score based causal discovery.\n \nOther questions: (a) why the BIC score used by Chickering 2012 is called 'heuristic score'? (b) Gumbel technique may not give exactly zero or one, so how was the 'no edge' type processed? This part was somewhat omitted. (c) what if there are instantaneous effects? I do not see why ACD does not apply to this case.\n\n*********after reading rebuttal *******\nI have increased my rating, but may still have some concerns. See below.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors leverage the potential information shared across samples to illicit causal discovery.",
            "review": "# Introduction\n\nYou should have praise for your first sentence in your second paragraph. It is excellent. A neat summary indeed. Emphasise it if you can.\n\nRefer to figure one in paragraph one to allow the novice reader to get a quick understanding of what you are trying to do. Better yet, move figure one into the first page and put it at the bottom, it is a great eyecatcher.\n\n# Background\n\nN should be italicised in definition 2.1.\n\n# Amortized causal discovery\n\nPresumably like all work of this kind you assume, there are no cycles or self-cycles in the SCM. That is pretty common fare for causal work (and a huge limitation) but it should still be noted at the very top of this section or indeed expanded upon in the preliminaries. I take it thus that you cannot handle SCMs with self-cycles and cycles in the endogenous variables? A far harder problem of course, but just to be clear from this reviewer's point of view.\n\nWhat your second point is saying in the preliminaries is that you assume that your data is sampled from a stationary process. You may want to add that key-word as that says a lot to people from outside this field, and again, emphasises the limitations of this causal discovery (and most in fact) method. It also gives you plenty of scope for future work! Indeed you go on to talk about the summary graph which is by definition only defined for stationary processes.\n\nA common theme throughout this paper is that you just insert figures without referring to them beforehand. This is very confusing. Please stop doing this and re-write the paper so that your figures follow the discussion of them. You introduce an unnecessary break in the flow of the reading otherwise, like between pages 1 and 2 and 3 and 4. \n\nPlease explain what you mean by this: \"Note, that we do not apply a supervised loss on the graph prediction\"\n\nUse \\text{} in your subscripts in your equations, it reads better i.e. $\\hat{\\mathcal{G}}_{\\text{TTA}}$ is preferable.\n\nPlease explain what you mean by \"expresses that there is a directed edge of type e from time-series i to j\" -- what is type e?\n\n## Hidden confounding\n\nThis section could do with a diagram. I find it very confusing. I think you are saying that you allow the decoder to predict the additional variable (in the summary graph?) which confounds _all_ endogenous variables in the summary? Or some? or one? Which is it? What if the confounding variable does not affect all endogenous variables?\n\n# Related work\n\nSolid section. Well done.\n\n# Experiments\n\n## 5.1\n\nIt would be helpful if you could show the inferred adjacency matrix of the particle dataset, alongside the GT adjacency matrix.\n\nThese experiments feel very selective. Your goal is to demonstrate ACD's superiority over the other methods, granted, that's the process and we all know it. However, this does seem like an odd comparison. Take Khanna and Tan, 2020 (not 2019 like you've quoted it): on a Lorenz-96 simulation, they reach an average AUROC of 0.95. Yet here they get 0.607, and this ought to be because you have generated asymmetric connectivity matrices which renders a structure which eSRU cannot handle. But how we do know? You haven't run ACD on say the Lorenz-96 system which is pretty common fare in causal discovery. If your model does not outperform SOTA methods in that space you may have to relax some of your claims. What is more, Tank et al do very well on Lorenz-96 too. They are not particularly good at the DREAM-3IN SILICO NETWORK INFERENCE CHALLENGE, and that's where ACD could shine through, rather than taking these models 'out of context' as it were. Point being, please demonstrate ACD on the _simpler_ operating space where the likes of NGC and eSRU live. You appear to have started that process with Table 2 but that is a complex setting, perhaps too complex to understand where ACD is failing. Again, I advise starting simple with the standard Lorenz-96 model just to ensure that your method performs well in that easy space.\n\n# 5.1\n\nIt is not okay for you to bold ACD's performance value in table 2. Remove it. MI is clearly higher and putting anything else is bold is confusing.\n\n## 5.2.2\n\nThis experiment is very interesting. The figures are good too. My advice would be to make the axis text larger, and set alpha=0.75 or something when you plot the prediction -- that way the reader can better see how the prediction tracks the GT.\n\nHowever, I am a bit confused about the deterministic chaos that you briefly mention at the start of 5.2.1 -- the parameter setting you use in this section (5.2.2) presumably does not give rise to deterministic chaos?\n\n---\n\nNone of your experiments contains fully identifiable noise models from what I can tell? You mention \"noise\" twice in the manuscript. None is in the experimental section. Now since you are only using simulations, I find it odd that you do not discuss this nor ACD's robustness against noise. As you are fully aware of course, most of these models break-down in the presence of a low SNR. And for this to be useful at all in the 'real' world, how does ACD fare?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}