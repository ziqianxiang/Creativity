{
    "Decision": "",
    "Reviews": [
        {
            "title": "Implicit bootstrap training for large scale neural nets",
            "review": "This paper proposed NeuBoots: an algorithm for training an implicit bootstrap ensemble within large neural networks.\nThe authors claim this allows us to effectively train bootstrapped ensembles at lower computational costs.\nThey demonstrate their efficacy through evaluation on simple supervised learning tasks as well as an active learning framing of CIFAR, where this outperforms a selection of baselines.\n\nThe core of the algorithm is to separate a large network into (feature_extractor, fully_connected).\nAt the fully_connected layer a \"context\" vector is appended to the feature, and a representation of the bootstrap weights.\nThe remainder of the network is trained using this context as an input, to fit the bootstrapped data.\n\nThere are several things to like about this paper:\n- The problem of effective and cheap uncertainty quantification is an important one. Bootstrapping is a core tool in classical statistics but rarely used in deep learning... NeuBoots may help to bridge the gap.\n- The core algorithm is mostly reasonable, even if it does seem somewhat similar to previous work (e.g. Scalable Uncertainty Quantification via GenerativeBootstrap Sampler [cited]  and Deep Exploration via Bootstrapped DQN [not cited]).\n- The progression of experiments is reasonable and demonstrates some efficacy of the method.\n\nHowever, there are a few places where the paper falls short:\n- On a core level I'm not sure the algorithm is quite right... why would the neural net not simply learn to \"ignore\" this bootstrap indicator... since there is no prior effect/regularization, why would it not simply learn to *always* fit all of the data? Incorprating something like a \"prior function\" (see https://arxiv.org/abs/1806.03335 might help).\n- I think there is not enough discussion/comparison with other similar methods that use bootstrapping beyond a shared base (see BootDQN above)... this feels like an implicit-ensemble after the Conv-layer, and I don't think there is a clear ablation/discussion of the benefits versus that approach?\n- Parts of the paper are quite notation-heavy and dense to follow... the core ideas are there, but I think that breaking the mathematical paragraphs into more linear description line by line would help.\n\n\nOverall, I think there are interesting ideas here, but I don't see enough comparison/clear discussion of the strengths/weaknesses of this approach versus the prior work.\nAt it's core, I'm worried that there is no necessary reason (apart from random init) that the network should learn to pay any attention to the implicit bootstrap indicator... and why would it not simply learn to fit all the data perfectly?\nFor this reason I have to say it falls below the acceptance threshold in its current form, but answering these questions could change that.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising method, but needs more work.",
            "review": "# Summary\nThe work explores the use of bootstrapping in neural networks for improved uncertainty quantification. The proposed method takes a non-parametric bootstrap approach. To reduce the computational and memory cost a last-layer approach is used for more efficient Monte Carlo sampling.  The work proposes to use a  loss-conditional model rather than retain multiple models for each bootstrap sample. As far as I can tell, the combination of these ideas is novel and certainly interesting. \n\n# Related work.\nHowever, the work does not seem to acknowledge the existence of such ideas outside the bootstrap literature. No loss-conditional and last-layer literature is acknowledged or compared with, e.g. \nOvadia, Yaniv, et al. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift.\" Advances in Neural Information Processing Systems. 2019.\nDosovitskiy, Alexey, and Josip Djolonga. \"You Only Train Once: Loss-Conditional Training of Deep Networks.\" International Conference on Learning Representations. 2019.\n\n\n# Empirical analysis.\nThe lack of connection to similar ideas in other uncertainty work results in the experiment section lacking appropriate last-layer and loss-conditional competing methods for a fair comparison. Last-layer variants of MC-Dropout and deep ensembles should be included in the experiments. \n\nMoreover, the work does not explore the tradeoffs arising from the decision to use last-layer and loss-conditional methods. An ablation study and comparison with a full non-parametric bootstrap baseline is essential in a work like this. The work does not answer the question if the performance boost truly arises from the bootstrap approach, or the extra parameters and stochastic evaluation. It remains unclear which of the variants (original, feature adaptive, and babysitting) is most appropriate. \n\nThe most important result, presented in table 2, is not convincing. It's well known that calibration metrics can improve when accuracy drops, and vice versa. The methods presented in table one are not optimal in both, except for R-110/CIFAR-10, where the performance is overall lower than the R-34 results. A more appropriate metric would be among the lines of an AUROC metric that captures this tradeoff. \nMoreover, it's unclear if these results are significant due to the lack of confidence intervals here. The text however does not acknowledge this, and figure 4.1 cherrypicks a single configuration from this table that does not reflect the overall average performance presented in table 2.\n\nThe Out-of-Distribution result is limited to just one dataset pair. It's well known that relative performance between methods differs among pairs. The work should thus include multiple in/out-distribution pairs to provide sufficient evidence for the robustness claim.\n\nThe Active learning results are promising, but the experimental setup is unclear which undermines this result. Many questions remain: How many samples are taking for MC dropout v/s NeuBoots? How is the training procedure done? What do the confidence intervals of the curves over the five runs look like? What value for S was chosen for the non-FA variant of NeuBoots and why does it perform worse? Why was the babysitting variant not used?\n\n# Clarity and writing\nThe authors take a formal approach to introducing their method, but the method section is lacking in providing intuition. The ideas presented are relatively straightforward, but appear overly complicated due to the choice of presentation. The writing is generally clear, but could benefit from grammar/spell checking, e.g.:\n- \"we train a model by setting w_alpha as one vector\" (what does this mean?)\n- \"Note that \\Lambda can receives ...\"\n- \"Despite its success in statistics field\"\n- \"Figure 4.2: Actice learning\"\n\n# Conclusion\nThe method shows promise, but the method is not sufficiently evaluated and benchmarked against existing work. Ideas that have been explored by previous work need to be acknowledged. I argue for rejecting the work in its current form.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A modern use of bootstrap to quantify the uncertainty in NN  ",
            "review": "This paper provides a novel use of bootstrapping confidence interval to quantify the uncertainty of neural nets. To overcome the bottleneck of computations, the authors propose a fresh generative model perspective that accelerates the algorithm. Extensive experiments are made and several tasks are evaluated that I greatly appreciate. \n\nMy biggest confusion is that how the procedure compares to [1] aka. GBS: \"Scalable Uncertainty Quantification via Generative Bootstrap Sampler\"? It looks like the intuition and algorithm formulation are the same as that paper. If we compare (3.2) and (2) in [1], I feel they are the same. The authors mention GBS can only work for the classical statistics model. Well, this is not quite convincing. And I do not find anywhere else to compare with GBS. Since the author assumes the unknown function lies in a class of NN, I feel it is still sort of parametric model but over-parametrized. I think the authors should pay a bit more time to explain what the novelty goes beyond GBS and why it is hard to use GBS to NN. Otherwise, the novelty may be limited to provide some empirical evaluations of GBS on NN. \n\nIt is not quite safe to argue that the NeuBoots provides a \"valid\" confidence band of a regression function unless the authors could provide a rigorous theorem of bootstrap validity. Statisticians spent quite a lot of time on this and so far even for very simple model, the guarantee is still limited to asymptotics. The NN presents multiple additional difficulties such as the uncertainty induced by the training process, the highly non-convexity.   \n\nIt is great to see the application of proposed methods on different tasks and shows some advantages. It is better to explain why NeuBoots can outperform other competitors? So far I just see the consequence but does not know why. The competitors are based on very different principles to construct uncertainties. Without some principle understanding of bootstrap, it might be hard to see if the improvement is convincing.  \n\nI am wondering why a large body of stochastic gradient MCMC methods are not taken into consideration. I feel this is a popular choice to quantify uncertainty nowsdays and there are a large body of developments on this. \n\nSection 3.4: nonparametric regression is a bit confusing. What is the underlying true regression function? If it is a NN, it is a bit unfair to compare with Gaussian process. And why you compare with GP? They are based on even different point estimation. I think this paper provides a way for interval estimation. But if the point estimation is different, I am not clear if they are comparable.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "good idea but paper needs polishing",
            "review": "**Summary**\nThis paper proposes NeuBoots, a procedure to efficiently perform bagging of neural networks. The key idea is to learn a single model where the last layer $M$ is modified to take bootstrap weights $w$ as inputs. That model is trained to output the prediction corresponding to any input $w$. Block bootstrapping is used to contain the complexity of the model:  samples are partitioned in $S$ bloks and every sample in a block has the same weight. Theoretical results are provided for this approach. Two additional variants are also proposed. NeuBoots-FA (feature adaptative) uses $S$ equal to the size of the inputs to $M$ (last layer of the network). In that case the bootstrap weights $w$ are not added as an input to $M$ but element-wise multiplication is performed between $w$ and the input values of $M$. NeuBoots-BS (baby sitting) is the same as NeuBoots-FA, but $w$ is kept constant for some time at the beginning of the training. The proposed approaches are also evaluated on three problems: out-of-distribution estimation (OOD), confidence estimation (CE) and online learning (OL). \n\n---\n**Strong points**\n\n1) The problem considered is interesting. Bagging has been very useful. An efficient procedure to apply to to deep learning is great. \n2) Theoretical results are provided. I have not checked the proofs. \n3) The method has been evaluated in three different settings. Based on what I could understand, comparison to existing approaches show that the proposed method is as good or better than less efficient approaches.\n4) A solution to select the hyperparameter $S$ is provided. That's rare enough to be stressed.\n5) As far as I could tell, the method is new.\n\n**Weak points**\n\n1) The paper is well organized but writing should be improved. To give a few examples: there are some unclear statements, confusing mathematical notations, some figures lack axis labels and are sometimes very small. The paper should also be carefully proofread.\n2) Although the results reported in the paper look good, but I think the experiments should be more rigourous in some aspects.\n    21) One strong selling point of NeuBoots is its efficiency. Yet, there is no run-time or complexity comparison to existing methods besides one in the method description for MCDrop and inference. Run times for all experiments would be a good addition. It would also be good to compare the training time.\n    22) All proposed methods are not used in all experiments. OOD and OL use NeuBoots and NeuBoots-FA, CE uses NeuBoots-FA and NeuBoots-BS. I would suggest using all variants in all experiments or having a specific experiment setting or an ablation study to compare the variants. It would also be good to show the impact of $S$ on accuracy and training time.\n    23) Experimental settings ar described quickly. I am not sure I could reproduce all experiments.\n3) The selection of the weights for inference is not discussed. I do not expect the paper to study that particular point, but I think it should specify how these weights are obtained in the experiments, even if it is simply by sampling the distribution.\n4) No theretical results are provided for NeuBoots-FA and NeuBoots-BS.\n5) Evaluation is limited to CNNs, other types of networks would have been nice but not necessary in my opinion.\n\n---\n**Recommendation**\n\nI lean towards rejecting the paper at the moment. The proposed approach is interesting, the results look good at first glance but in its current state I think the paper is not ready to be published.\n\n---\n**Details**\n\nIn section 3.2: \"learned $G$ can generate bootstrap samples by plugging $w$ into $G_\\phi(X,.)$ without repetitive forward-propagation.\" Isn't there a repetitive forward propagation in $M$? \n\nIn section 3.3: \"we plug mini-batch size of $w_{\\alpha,k} (...) instead of full-batch size of $w_\\alpha$. I am not sure I understood this. Does this mean only the part of the loss related to the mini batch is updated? \n\nFor me, the definition of Feature-Adaptive NeuBoots was not clear. The paper says that $S=|F_\\theta|$, and $G_\\phi(X,w_\\alpha)=M_\\beta(F_\\theta(X)\\bigodot w_\\alpha)$. So $|\\w_\\alpha| = S|$. However $|w_\\alpha| = n|$ (block bootstrapping paragraph), which is necessary for the loss to be well defined. So at at the moment I am not sure what size $w_\\alpha$ is and how the loss is defined.\n\nOOD experiment: I am not sure I could reproduce the experiment. In particular it is not obvious to me how to \"use a logistic regression based detector which\noutputs a confidence score for given test sample\". Is it a new classifier trained using both in and out of distribution samples? I am not familiar with out-of-distribution experiments though so this did not weight much in my decision.\n\nIn Table 1, for AUROC, both methods with calibration have the same score, yet only the proposed method is displayed in bold.\n\nCE experiment: I could not identify what method the \"baseline\" refers to.\n\nThe legend and axis of most figures are too small and they are not in vector format. I have no problem with having to zoom a little bit, but I think that in figures 4.1 and 4.2 in particular the fonts is too small. Figure 4.2 has no axis label. \n\nThat being said, I am surprised that NeuBoots-FA has 20% better accuracy at step 0 than the other methods, where I assume the samples are identical for all models. It would be interesting to have some measure of the distributions of the results of the methods. It would also be interesting to report two results for each method: one where a model without uncertainty (the same for all methods) is learned on the samples chosen by the method, one where the model is the one learned by the method. As far as I understand, at the moment it is not possible to know whether the gain is due to the quality of the samples selected or because the proposed method build a better model. Considering the 20% at step 0, I assume the latter is reported at the moment.\n\n---\n**Questions**\n\n1) Could you please comment on my understanding of Feature-Adaptive NeuBoots?\n2) Why has NeuBoots-FA 20% better accuracy at step 0 in Figure 4.2? \n3) Are the models in Figure 4.2 learned by the same algorithm, or is each algorithm used to train the model?\n4) Is there any problem to apply your method to other architecture than CNNs?\n\n---\n**Minor details**\nSection 3.1: nonparmaetric\n\nSection 3.2: provided the uniqueness of the solution of equation \n\n3.1\nfrom the scratch\n\nthe are under the receiver\n\nin almost metrics",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}