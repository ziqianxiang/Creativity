{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper describes a framework for multi-agent reinforcement learning that uses Markov Random Fields. Unfortunately, the paper is not clearly written and would benefit from significant revisions that improve its structure and make the model and approximations more explicit.\n\nIn particular, the paper says a graph says which agents $i,j$ communicate. This is typically called the \"coordination graph\" in this setting, see\n\"Collaborative Multiagent Reinforcement Learning by Payoff Propagation\", Kok and Vlassis, 2006. Note that within that paper they provide Q-function decomposition, which can only serve to approximate the optimal policy.\n\nThe authors of this submission claim that an MRF is sufficient for optimal policies. I fail to see how this is true. In particular, Proposition 1 has to be checked more carefully. I tried to go through it, but it did not seem to make sense to me. Why is there an exp() term in the definitoin of the optimal trajectory probability? Why would minimising the KL divergence be enough to obtain an optimal policy? Perhaps it gives an optimal policy within the class of MRF policies, but that's not the same thing as the globally optimal policy.\n\nOverall, I find the lack of clarity and in depth discussion of early related work disturbing, particularly with respect to the theoretical claims in the paper.\n"
    },
    "Reviews": [
        {
            "title": "Review ",
            "review": "The paper proposes a multi-hop communication method for multi-agent reinforcement learning. This method is based on the loosely coupled reward structures among agents, which, as far as I am concerned, are generally held in complex multi-agent settings. The authors use experiments on CityFlow, MPE, and MAgent to demonstrate that their method can outperform the SoTA methods and is scalable. The empirical results is impressive. However, it is some concerns regarding methods that lead to my overall negative rating.\n\nFirstly, also the most importantly. Although the authors emphasize that they are communicating the intentions of agents, I think their method is quite similar to those communicating local observations, like NDQ (https://arxiv.org/abs/1910.05366), DGN, or CollaQ (https://arxiv.org/abs/2010.08531). One way to interpret the proposed communicating network structure is a normal multi-hop communication mechanism, but only with a softmax activation function. \n\nCompared to previous works studying communications of local observations, the proposed work (1) needs to address the problems induced by the joint policy, like sampling from it. The author use a variational influence approach to conduct sampling. However, this approach may hurt the scalability. And it (2) requires agents have access to the global states. For partial observable environments, the proposed methods needs to reply on DGN. \n\nSome other points: (1) I was expecting ablation studies where DGN is ablated on partial observable environments. (2) Some parts in the method section are hard to follow.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising approach using mean-field estimates to learn joint policy with suitable set of experiments",
            "review": "The paper proposes a scalable approach via intention propagation to learn a multi-agent RL algorithm using communication in a structured environment. An agent encodes its policy and sends the “intention” to the neighboring agents with the assumption that only the closest agents would be the affected by it. The approach involves using techniques from the embedded probabilistic inference literature using mean-field variational inference. The joint-policy is estimated using the mean-field approximation that is obtained via propagating intents in an iterative manner. So this approach helps in avoiding the need to factorize the value function explicitly.\n\nThe related works section does a nice survey of related approaches and the paper shows conceptual differences to an earlier proposed MFG that has stricter requirements. \n\nThe experiments shown cover many important baselines that are shown to be good baselines in respective environments. IP outperforms all the baselines in three competitive benchmarks.\n\nI have a few questions about the clarity of the presentation.\n\n- How important is the graph structure defined by k-means? A comparison with a randomized graph and ablation with different reset time (n) intervals would be interesting.\n\n- In the experiments, it would be interesting to check if intention only helps the nearby agents. How does adding/removing agents to the set of neighbors affect learning? A comparison with a fully connected graph should be sufficient. The plot in the Appendix shows results on the CityFlow task which has very structured observation with the set of immediate neighbors always set of 4. Doing such an analysis on a more dynamic environment like MPE would be helpful.\n\n- What is the computational cost of a densely connected graph as compared to method without using a fixed topology?\n\n- Fig 4c does not show plots until convergence.\n\nOverall I feel some restructuring of the paper would benefit the reader explaining some missing portions of the algorithm. For eg, taking out the environment images from the main text.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical insight, limited algorithmic novelty",
            "review": "Paper Summary\nThe paper considers the cooperative multiagent MARL setting where each agent’s reward depends on the state and the actions of itself and its neighbors The paper has a theoretical claim that, for such reward structure, the optimal maximum entropy joint policy in the form that can be factored into potential functions, one for each agent. In particular, if the sum of all agents’ rewards is a function on pairwise actions, those potential functions are one for each agent and one for each pair of actions (i.e. the equation after Proposition 1).\nThen, the paper proposes to use mean-field approximation to approximate the optimal joint policy (Equation (3)), which leads to a concrete algorithm that relies on passing the embedding of each agent’s local policy around to neighbors. The paper then empirically shows that the algorithm is particularly effective for domains with a large number of agents.\n \n \nMajor Comments/Questions\n1. Although the motivation has an interpretation of intention propagation, the resulting architecture (Figure 1b) and loss functions (Section 4.2) seems to be a standard messaging passing architecture with SAC loss functions that loses the *intention* semantics. I do not see too much algorithmic novelty here.\n \n2. For the baselines used in the experiments, it seems that only IP and DGN allow communication/message passing *during execution*, which makes it unsurprising that the two methods outperform other baselines. \n \nMinor Comments/Questions\n1. The beginning of Section 3 says the paper considers maximum entropy as the optimization objective, while eta(pi) at the beginning of Section 4 says the objective is long-term reward (no entropy). This seems to be an inconsistency here.\n \n2. For the assumptions on rewards, Proposition 1 assumes that each agent’s reward depends on its neighbors, while the derivation of Equation (3) (and thus the following algorithm) further assumes that the reward depends on pairwise actions. It is a little bit unclear what assumptions are required for all the theoretical and experimental claims of this paper. \n \n3. Is there reason to believe that the multi-round message passing will converge to the fixed-point of Equation (2)?\n\n4. What is the \"overgeneralization issue\"?\n \n \nOverall (weak accept)\nThe paper has a clear introduction and motivation of the proposed algorithm. The insight that optimal maximum entropy joint policy takes the format of Markov Random Field might be of some value and interest. However, I don’t think the resulting method has much algorithmic novelty.\n\n\n--------------\nThanks for the response and I've increased my score. I am satisfied with the response but still not convinced about the algorithmic novelty on the intention semantics built into the method, even after reading B.1.  In particular, it seems that the loss functions do not drive mu's represented by NNs to the fixed point solution of Eq (3); psi shows up in Eq (3) but does not play a role in the following development of the method. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper proposes a method for generating policies in cooperative games, using a neighbourhood-based factorisation of reward, and an iterative algorithm which independently updates policies based on neighbour policies and then propagates the policy to neighbours using function space embedding.\n\nThe experimental results looked promising, so there seems to be an idea here worth communicating.\n\nThe paper was very hard for me to follow. I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc.). Instead the main body of the paper felt like a collection of pieces that were used when developing the algorithm. I would suggest it might be easier to follow if written from the top down, instead: present a high-level overview of the idea, give a (detailed!) description of the algorithm, the experiments, and leave the derivation to the appendix.\n\nDespite being in the appendix, the algorithm is less than half a page, and doesn't explain the variables. eta and kappa might be described elsewhere, but it would be helpful to reference where. J is a loss: which one?\n\nOne of the claimed contributions is this is *principled* method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statemen\nOne of the claimed contributions is this is *principled* method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in?\nt: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in?\n\nAnother claimed contribution is computational efficiency. How does the computational cost compare to the baselines in the experiments?\n\n\nProposition 1: \"The optimal policy has the form ... 1/Z exp(...)\"\nI found the use of optimal slightly hard to follow throughout this. The usual definition of optimal policy would be a value maximising policy, which would be an argmax rather than a softmax. Following that definition, this proposition wouldn't be true, so it seems like it needs more explanation, or more careful wording.\nThe cited PRL article (Levine 2018) seems to retain this standard use of optimal: it uses a distribution over trajectories with an equation similar to here (a softmax over accumulated trajectory rewards), and makes use of the property that trajectories corresponding to an optimal policy have maximum probability in that distribution.\nCan the authors clarify this use of optimal?\n\nProposition 1:\nFor clarity, explain the intention of psi. Is this the future accumulated reward given the current state and selected action?\n\n=-= comments after author discussion\nThe authors were quite active in editing the submission, and addressing the concerns I had. I still find the paper a bit hard to follow, but none of my original concerns remain.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}