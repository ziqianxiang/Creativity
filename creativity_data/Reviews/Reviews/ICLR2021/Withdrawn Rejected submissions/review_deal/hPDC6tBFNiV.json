{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper investigates methods for producing and evaluating interval forecasts rather than point forecasts. The authors focus on spatio-temporal forecasts whose interval accuracy is measured with the Mean Interval Score.\n\nPros:\n\nUncertainty quantification is an important topic that is often ignored in the ML literature where the focus remains on point predictions. The COVID-19 example the authors use is a clear example of the need for such methods: the CDC-sponsored COVID forecast hub mandates interval forecasts.\n\nCons:\n\nIn the words of one reviewer \"This paper is almost a review paper\", a statement with which I fully concur. Neither the evaluation metric (MIS) nor the methods for generating intervals are novel. There is no particular effort to argue \"why MIS and not weighted interval score\" or \"why these methods\". If it were a review paper with more comprehensive coverage of the relevant state-of-the-art (a deficiency mentioned by multiple reviewers), it may make a more positive impression. As is, the story is mainly incomplete. To resubmit at another venue, I would suggest the authors either (a) state clearly what is new here in this paper or (b) make it a review paper that more comprehensively evaluates and discusses current state of the art. The statements in the last paragraph of page 1 read more like the authors are shooting for (b) than (a): \"we conduct a systematic study\", \"we investigate\". Finally, the evaluations undertaken here do not really make use of (or correct for) the spatio-temporal task. Presumably some locations/time-periods are more difficult than others. So simply averaging over everything (as in Table 1) doesn't take the structure into account. It is likely better to examine relative accuracy to strawman forecaster or perhaps use a random effects model."
    },
    "Reviews": [
        {
            "title": "Official Review",
            "review": "This paper aims to provide a systematic study on the problem of uncertainty quantification for spatial-temporal forecasting. The study reveals several interesting conclusions regarding different types of methods on the task.\n\nCould the authors elaborate on how MIS can be used as a loss function during training? Is it by jointly optimizing both the NN parameters theta and the lower and upper confidence bound of the predictions?\n\nIt is a bit vague in Sec. 3.3 and therefore not clear what is the proposed frequentist UQ method specifically. It would be helpful to write down the equations to be precise.\n\nThe general guidelines in Sec. 3.5 make sense and might be of interest to practitioners. The main conclusion here is that Bayesian methods perform better in terms of mean predictions and frequentist methods perform better in terms of confidence intervals, which is no surprise I guess.\n\nA large number of works on probabilistic NN and/or Bayesian deep learning for uncertainty estimation are missing [1,2,3].\n\nIt is interesting to see MC dropout actually harms the prediction accuracy and the only one method with significant improvement margin is the SG-MCMC.\n\nFor the COVID experiments, how is the graph constructed exactly? The description in Sec. 5.2.1 misses a lot of details.\n\nIt is a bit strange that Table 2 reports the prediction/forecast for the residual between the ground-truth death incident numbers and GLEAM’s predictions. Such a setting seems problematic. Why not directly predict the death incident numbers? Is it because the prediction is highly inaccurate? Either way, if this result is in the appendix, it would be better to move it to the main text. To me, even the results in the appendix (as a figure) are not useful enough. It is difficult to evaluate DeepGLEAM’s improvement.\n\nAnother question is: If one adds the predicted residual from DeepGLEAM to the GLEAM prediction, would GLEAM’s prediction accuracy be improved?\n\nAlthough some results are interesting. it is unclear whether this paper is a pure study or there are actually proposed new methods.\n\nMinor:\nPage 3: Is the adjacency matrix of size P * P?\nPage 6: Start of Sec. 5.1.2: Table 2 -> Table 1\n\n[1] Natural-Parameter Networks: A Class of Probabilistic Neural Networks, NIPS 2016\n[2] Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers, ICLR 2018\n[3] Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation, CVPR 2019\n\n----------\nAfter Rebutal\n\nI appreciate the authors’ response to my questions and they addressed some of my minor concerns on the clarity and presentation. The authors confirm that that their work is a pure study on UQ for forecasting. In this case, the methods evaluated is not very specific to the forecasting tasks, and the conclusions are not very surprising either. Therefore it seems there is limited technical merit from this paper. The setting of the COVID forecasting task is also a bit strange and the authors’ response does not fully address my concern. \n\nI would like to keep my ratings unchanged.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clear and interesting paper, even if it is not proposing something new. ",
            "review": "This paper proposes a comparison of deep learning (DL) uncertainty quantification (UQ) methods in spatiotemporal forecasting problems (specifically, in graph-based problems). The paper define first a metric that is used in statistics and econometrics for interval forecasts, mean interval score Then it reviews the main DL and UQ methods for temporal and spatio-temporal methods. Then it compares 6 methods, 4 from Frequentist framework and 2 from Bayesian framework, on 2 forecasting graphical problems.\n\nI found the paper clear and interesting. This paper is almost a review paper, yet I think it has a great value to the community. The comparisons are showing interesting findings, the state-of-the-art is curated and the use of the 'new' metric is also explained. While no new method or even metric is proposed, I think this paper is worth publishing. My main concern is about the spatial part of the 'spatio-temporal forecasting'. It looks like only graph-based problems are investigated, while a grid-version problem, which are very frequent, would have been interesting.\n\nQuestions/remarks:\n\n- In the second experiment (covid), what exactly is the point in using the encoder-decoder sequence? Why the DCRNN does not work as it is in this case? \n\n- Also, why using the flight mobility as graph edges? isn't the 'standard' propagation, by 'road' and in between neighboring states, neglected?\n\n- From Figure 3, it looks like the confidence bounds often do not contain the ground truth. Do you think it is because of lack of data or because the model is here too simplistic (some key parameters are not used as input)?\n\nsmall remarks:\n- part 2.1: equation of MIS: define rho\n- eq. 5: define A (adjacency matrix?)\n- Table 2 --> Table 1 (change ref. in the text)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Rejection: objectives not clear, lack of rigor and unfair comparisons",
            "review": "## Summary\n\nThe paper considers the problem of uncertainty quantification in spatio-temporal forecasting with deep neural networks. Both frequentist and Bayesian methods are compared on two datasets (traffic and COVID-19). Recipe for practitioners is given based on data size and computational budget.\n\n## Reasons for score\n\nUnfortunately, I vote for rejection. The work is not clearly motivated with respect to the state-of-the-art. There is a clear lack of rigor in the terminology used, with misleading claims. There are multiple issues in the experiments. After reading the paper, many questions and motivations related to design choices remain unanswered. More details are given below.\n\n## Strengths\n\n- Overall, the paper is easy to read.\n- Uncertainty quantification in Deep forecasting models is an important problem for optimal decision making. Spatio-temporal data makes the problem harder.\n- Multiple uncertainty quantification methods are compared (both frequentist and bayesian).\n\n## Weaknesses\n\n- The objectives are not clearly stated. The authors never really define uncertainty quantiﬁcation, which has different meanings depending on the context. In fact, there are multiple sources of uncertainties (aleatoric, epistemic, etc). Which one are you trying to capture? Furthermore, some of the methods considered do not capture the same \"uncertainties\" (e.g. quantile regression vs bootstrap).\n\n- There is a clear lack of rigor in the terminology used. Certain claims are too strong or misleading.\n\n1. The description of the Mean Interval Score is misleading. You say: \"It rewards narrower credible intervals and penalizes intervals that do not include the observations\". It does not have to be a credible interval. Furthermore, note that the \"optimal\" interval at level $\\alpha$ should have $1 - \\alpha$ of the observations outside the interval. The MIS tries to find the interval with the right coverage. You should also mention the Winkler score.\n\n2. \"We ﬁrst unify uncertainty quantiﬁcation from Frequentist and Bayesian perspectives into a single framework\". I find this statement a bit too strong. Although expression (1) and (2) might be useful to understand the two philosophies, they do not really \"unify\" them.\n\n3. \"Point prediction\" has a strong meaning in statistics/ML. We often contrast point prediction and probabilistic prediction (or forecasts). Why do you mean by \"prior works for deep neural network uncertainty estimation have mostly focused on point prediction\"?  Also, in the following sentence, you contrast point prediction and sequence prediction: \"Unfortunately, most existing UQ methods deal with point predictions instead of sequence predictions.\"\n\nOther notions that need clarification/definition:\n- Deterministic models -> What do you mean by \"deterministic\"?\n- high-quality uncertainty -> This is never defined.\n- Accurate predictions vs better generalization\n- Confidence intervals are not prediction intervals\n- \"Frequentist methods generally outperform Bayesian methods in conﬁdence interval.\" -> What does that mean? Are you referring to coverage?\n\n\n4. The paragraph \"MIS and quantile regression\" is poorly written\n- \"One-sided quantile loss function\" -> you did not define it. You can at least refer to part of the MIS expression.\n- Quantile crossing and remedies are poorly explained. Why an ensemble model would solve the quantile crossing problem? Why does it combine the frequentist and the Bayesian philosophies? \n- What do you mean by \"CRPS functions\"? Also, Gasthaus et al (2019) did not approximate the CRPS (a closed-form expression was derived).\n- What is Spline Quantile regression (SQ) ? I guess you are referring to Gasthaus et al (2019). You should make this clear, and say more about SQ.\n\n\n5. In Section 3.5, you say \"Through both theoretical analysis and experiments, ...\", where is this \"theoretical analysis\"? \n\n6.  The related work on \"Uncertainty quantification\" is weak. Given that this is the main topic of the paper, this section should be extended with more references. See some references below. Also, how this work fits in the literature is not clear.\n\n- Danijel Kivaranovic, Kory D. Johnson, Hannes Leeb. Adaptive, Distribution-Free Prediction Intervals for Deep Networks. Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, PMLR 108:4346-4356, 2020.\n- Tagasovska, Natasa, and David Lopez-Paz. \"Single-model uncertainties for deep learning.\" Advances in Neural Information Processing Systems. 2019.\n- Pearce, Tim, et al. \"High-quality prediction intervals for deep learning: A distribution-free, ensembled approach.\" International Conference on Machine Learning. 2018.\n\n7. There are multiple issues in the experiments.\n\nIn Table 1, the comparison between forecasting methods is unfair. In fact, optimizing MIS and evaluating on MIS will tend to win. This is also observed in Table 2 with the quantile loss. Instead, you should also report interval length and coverage, i.e. split the MIS into two components. This might give you more insights into the differences between the methods. You should also report MAE (since you are using it with MIS).\n\nAlso, evaluating a single confidence level might not be enough. It is standard practice to evaluate on multiple confidence levels to evaluate the entire distribution (see below).\n\n\"SQ method uses CRPS as the loss function, which could be unfair for comparison using MIS metric\". It can be shown that the CRPS is the integral of the MIS metric for all confidence levels. So, it is fair to compare them if you compute MIS for all levels.\n\nFor the COVD-19 dataset, you have less than four months of data, and you try to forecast the following 4 weeks. I find it too ambitious. Is COVID-19, the right dataset to use? With very few data points and significant non-stationarity, I think there are better datasets for a fair comparison between the methods. See the papers you have cited in the \"related work\" section.\n\n\n## Questions during the rebuttal period\n\nPlease address and clarify the issues raised above including the following additional questions:\n\n- Why do you focus on 95% confidence level?\n\n- What do you mean by \"MIS is non-parametric\"? I can optimize MIS for a parametric model.\n\n- \"Strong assumptions on the likelihood function\".  what are these assumptions?\n\n- What does DCRNN mean? Did you apply any hyperparameter optimization? (How did identify the values you used in DCRNN?)\n\n- Why did you combine the MAE with the MIS? Does that change your analysis in Proposition 1 and 2?\n\n\n# Other minor comments\n\n- When defining MIS in section 2.1, you do not introduce \\rho (the confidence level).\n- \"It is difficult for DNN as they often do not generate explicit likelihood outputs.\" -> I suggest you cite some normalizing flows papers.\n- Last paragraph before Section 3.2: You have already introduced MIS in Section 2.1. This sentence is out of place.\n- Section 3.5. \"Uncertainty qualification\" should be \"Uncertainty quantification\".\n- Time granularity of COVD-19 dataset is not clear (daily? weekly?)\n- You did not mention that the spline quantile function was a linear spline.\n\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}