{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting idea, but more experiments and discussion needed",
            "review": "This paper proposes a method to enhance the ability to represent hierarchical knowledge in Euclidean geometries. While there have been several attempts at representing such knowledge via a _specialized_ geometry (hyperbolic geometries, Posets induced by order embeddings etc), there is little work that directly looks at doing so in Euclidean space. The motivation of the paper is sound, since progress in encoding such hierarchical structure in specialized geometries is hard to translate to downstream tasks (as shown by Du et al. 2018).\n\n#### Pros\nThe *motivation* behind this work is sound, and the writing is to the point. \n\n#### Cons\nIn my opinion, this paper lacks along the following dimensions:\n1. The main hypothesis is that the proposed method is likely to be more useful since it operates directly over Euclidean geometry. However, integrating this with “standard” approaches may still be tricky since polar embeddings assume a very specific spherical structure for embeddings. Is this any evidence (empirical / theoretical) that suggests that polar embeddings can be used alongside off-the-shelf end-to-end models? \n\n2. Experiments:\n  * I would like to see some uncertainty estimates in Table-1. Perhaps means and std dev. for atleast 3-5 runs so we know that the results are not due to randomness.\n  * Details: How is Poincare/Order/Simple implemented? How were the hyperparameter choices made for these models? Did the authors use available code or reproduce results with their own implementation?\n  * Did the authors try huberising the L2 loss instead to ensure that gradient values are small for large error? \n\n#### Misc Nitpick\n1. The bolded numbers in Table-1 aren’t the best performing models. I understand that polar performs best among Euclidean approaches, but would be better to separate out the 2 tables.\n2. The fact that r is not learnt from data and is heuristically estimated, could make this approach not broadly applicable. Would like to see some discussion on that.\n3. Broken citation in the first para (Talmor et al.)\n4. What is $\\phi$ in Eq-5? This could be mentioned in the paragraph above it.\n5. How does this approach compare with box embeddings?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Polar Embeddings",
            "review": "This paper proposes a representation for Euclidean embeddings based on polar coordinates. Technically, they use a novel loss function for learning word angles (the Welsch loss function). Experiments were performed on the standard WordNet hierarchy for link prediction, against a number of alternate parameterizations based on both Euclidean and hyperbolic space.\n\nThe main weakness of this submission is that the usefulness is not clear, and there is an important baseline that is missing.\nFirst, a crucial element of the paper's pitch is that the representations may be more useful in downstream tasks compared to hyperbolic space, since they can double as Euclidean vectors. However, hyperbolic embeddings can also be trained with a particular representation (e.g. the Poincare sphere) and then used directly as Euclidean vectors as well. If the argument is that this is not semantically meaningful, then I don't see how the proposed representation is different in this regard: Euclidean operations on the learned polar embeddings might not have semantic meaning, since the embeddings were trained with a different representation and specialized loss function that might not compatible with the Euclidean distance.\n\nSecond, the core problem of this paper is learning embeddings on a sphere of fixed radius; it introduces a particular parameterization (spherical coordinates), and the technical contribution -- the Welsch loss -- is introduced to overcome problems with this parameterization. But why not just use the natural parameterization of the sphere (embedded in Euclidean space)? That is, optimize over Euclidean vectors with the natural parameterization constrained to have fixed radius. This approach has been done before [1], and is absent from the comparisons. In fact this would align with the paper's main goals more, since such embeddings would be more directly translatable to downstream Euclidean operations.\n\nIn fact, polar coordinates store the radius and angle separately, hence essentially learn an embedding in $[0, \\infty) \\times S$, where S is the sphere. Such product embeddings have been studied in the literature before [1]. In fact, you could learn the radius and the spherical coordinates jointly, overcoming the approach's weakness of requiring hand-crafted r values.\n\nOverall, a crucial piece that is necessary to change my score is a discussion and empirical results of why the proposed parameterization + training procedure is superior to simply training on the spherical manifold and/or a simple product space. There are other areas that can be improved, such as a limited empirical evaluation, but these are my main concerns about the core contributions and novelty of this paper.\n\n[1] Gu et al. Learning Mixed-Curvature Representations in Product Spaces\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough novelty ",
            "review": "\n#### Summary:\n\nThe authors present a parametrization of embeddings that explicitly separates out the radial and angular coordinates of an embedding, with an aim towards encoding generality in the radial component, and similarity in the angular coordinates. They learn the angular coordinates of the embeddings and set the radii deterministically and show that this can reasonably predict the transitive closure based on the transitive reduction of the WordNet noun hierarchy.\n\n#### Reasons for score:\n\nThis paper does not have sufficient novelty or experimental evidence for ICLR.\n\n#### Positives:\n\n- The optimization tricks, both the SVGD and Welsch loss are interesting tools.\n- The paper is well written, and has good references.\n\n#### Comments/Concerns:\n\n- This paper has insufficient novelty, as it corresponds to simply learning unit-norm embeddings, which has been done in plenty of previous work, usually by interacting with unconstrained embeddings directly through cosine similarity, or projecting to unit length during training.\n\n- It is unclear why the authors choose to parametrize the embeddings in polar coordinates, with all of the attendant optimization difficulties, rather than simply parametrizing them normally, and comparing with cosine similarity, since the radial coordinates are set deterministically after learning is done. There is insufficient experimentation to conclude that there is a benefit to training directly in polar coordinates.\n\n- The deterministic setting of radial coordinates based on the WordNet graph does not seem general enough to provide guidance on how to train or use these embeddings in other contexts.\n\n- More specifically, when using the WordNet noun hierarchy from Ganea et al 2018, since this consists of the transitive reduction, a simple deterministic procedure (transitive closure) can rediscover all the missing edges, so the results with r^g using global information seem very artificial.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Novel idea but rather \"hacky\" and use case is not made",
            "review": "Summary: Word embeddings are typically developed in Cartesian co-ordinates. The paper explores the possibility of developing embeddings in polar co-ordinates given that objects often fall into hierarchical structures, which can intuitively fit a spherical model \"fanning out\" from the origin. This seems analogous to previous work showing that tree networks/hierarchies can be embedded more naturally in hyperbolic space (e.g. Poincare Ball), e.g. \"Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane\"; Sakar, R; 2012.\n\nQuality: The paper lacks aspects expected of a \"high quality\" paper, e.g. the abstract (alone) contains unclear phrases (e.g. \"essential meanings\", \"makes distance apart\", \"iterative uniformization\") and unsubstantiated/non-rigorous claims (e.g. \"most real-world objects are arranged in hierarchies\"). More importantly, key concepts are not motivated/explained, e.g. the difference between \\theta and \\phi parameters, and there is insufficient empirical justification for the approach, given it lacks theoretical rationale.\n\nClarity: The paper is readable with some helpful diagrams, but many phrases are unclear.\n\nOriginality: To my knowledge, the use of polar co-ordinates for embeddings is novel.\n\nSignificance: Not demonstrated by the paper.\n\nPros: Considering the potential benefit of polar co-ordinates and how they might be learned seems a reasonable.\n\nCons: Overall, the method proposed is very \"hacky\" (a collection of sub methods whose combination does not seem \"natural\" or \"meaningful\") and its benefit is unsubstantiated.  The main optimisation process appears entirely about the angle - analogous to cosine similarity, and the radius \"r\" is either not learned or learned quite separately, which is not well justified. There is no `natural optimum to the optimisation process (as, for example, PMI for W2V), which seems to be why the Welsch and SVGD methods are required to prevent embeddings all \"bunching up\". The approach does not seem sufficiently principled - whilst that is not essential, as a proposed alternative method of embedding it seems a step back in this respect.\n - It is not clear that embeddings in spherical/polar co-ordinates can't be achieved - if so desired - by re-parameterising hyperbolic embeddings from the Poincare ball into polar co-ordinates (or possibly even w2v-type embeddings). \n - The experiment performed is not a standard way to evaluate unsupervised embeddings and their use is therefore unclear. Word embeddings are typically compared on tasks such as similarity and analogy, and/or more complex downstream tasks. Knowledge graph representations represent many types of relation, not just heirarchical ones, and are not given \"closure\" information. The proposed embeddings seem to be somewhere between these but are not shown to be as useful as either of them.\n - The score function seems an odd combination of Euclidean and polar aspects.\n - To be empirically validated, this work should show benefit of these embeddings over more well-known methods on several tasks/datasets.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}