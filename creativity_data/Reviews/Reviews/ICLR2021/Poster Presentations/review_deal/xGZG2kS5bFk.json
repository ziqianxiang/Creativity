{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviews for this paper were quite mixed (7744), and none were exactly borderline. All reviews were detailed and informative, as was the rebuttal. The main criticisms were (1) lack of detail in the experiments, and some missing evaluation (2) missing related work, (3) overall lack of polish (mentioned among positive reviews too), and (4) some unsubstantiated claims. Positively, reviewers praise the novelty, dataset, the demo, and some reviewers found the experiments mostly convincing.\n\nUltimately this is still a borderline decision. The rebuttal does appear to address many of the claims about missing evaluation, and the complaints about polish can be easily addressed. I think the unsubstantiated claims are reasonably rebutted too. Related work doesn't seem to be addressed in the rebuttal."
    },
    "Reviews": [
        {
            "title": "An improved system for generating dances",
            "review": "**Update**: Revised score from 4 to a 6, mainly because the authors rightfully pointed out that they did have an ablation study in their paper which demonstrates the efficacy of their proposed methods.\n\n**Summary**: This paper presents a method for generating dances from audio in an end-to-end fashion. Specifically, they pose the problem as a sequence-to-sequence learning task from acoustic features to pose information. They demonstrate that humans prefer dances generated by their method more often than those from the prior state-of-the-art dance generation system.\n\nBased on the human evaluation and my own observations of the qualitative results in the supplementary material, the authors' claim that this system is an improvement over the previous state-of-the-art seems reasonable. However, the technical contributions (novel Transformer architecture, a new \"curriculum learning strategy) are _not_ justified experimentally (e.g., by an ablation study). Hence, the work feels lacking in substance; at best, the paper simply demonstrates performance improvements to an existing task using existing metrics.\n\n#### Technical novelty is unsubstantiated\n\nThe biggest problem with this work is a lack of ablation study for the novel aspects of the proposed system. Specifically, the modifications to the Transformer architecture (Section 3.2) and the curriculum learning (Section 3.3). These constitute the _only_ technically novel aspects of this work (the latter is even included in the title of the paper), but neither is justified experimentally. Hence, I can only treat these ideas as implementation details rather than contributions.\n\n#### One song -> one dance?\n\nBased on my understanding of the proposed approach, it should only be possible to produce a single dance for a given musical input. But the \"Multimodality\" metric (incidentally, a strange and misleading name choice) is defined as the variation among the generated dances for the same piece of music. The multimodality score for the proposed model indicates that it _can_ generate multiple dances per song. How is this possible? The decoder doesn't model a distribution of pose information given audio, and I can't find an explanation anywhere in the paper.\n\nFurthermore, the multimodality score for the proposed model is worse than that of the previous state of the art. For downstream applications, is it better to have a system which generates a single excellent dance for a given song, or one which can generate many lower-quality dances? I think the human evaluation is a little bit unfair in this regard, as it does not take the variety into account. But at the very least, there should be some justification from an explanation of downstream use cases.\n\n#### Human element?\n\nOne high level question I have about this work is why generate _dances_ rather than _choreography_? Presumably it would be quite challenging to teach dances to humans from this pose information as opposed to typical choreography instruction dancers might receive. Perhaps teaching these dances to humans is not an intended downstream application, but it seems like it would be easier and more useful to generate choreography rather than 3D pose information. Can the authors comment on this?\n\n#### Unusual methodological decisions\n\nIt is a strange decision to use a Transformers for the encoder and an RNN for the decoder of the proposed seq2seq model. The stated justification for using an RNN as the decoder is that the decoder needs to be autoregressive. But most (all?) Transformer-based decoders are also autoregressive... this justification \"smells funny\". I'm guessing that Transformers just didn't work as well for whatever reason; why not just say that? Or better yet compare the two experimentally?\n\n#### Low-level comments\n\nMissing many citations (probably many more related to dance / choreography generation):\n- Citations on page 1 to Fan et al. 2011 and Lee et al. 2013 are broken (not hyperlinked) and missing (from the bibliography)\n- Dance Dance Convolution (Donahue et al. 2017)\n- Music-driven dance generation (Qi et al. 2019)\n- Dance beat tracking from visual information alone (Pedersoli and Goto 2020)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice work, needs some polish",
            "review": "This paper describes a method for generating dance movements (pose sequences) from musical audio inputs.\nThe proposed method combines an attention-based encoder with a recurrent decoder, and uses a curriculum learning strategy to gradually transition from teacher-forcing to autoregressive training.\n\nI generally found this paper to be well written, thoroughly executed, and the proposed method performs well compared to prior work.  Nice job!\n\nI do have a few suggestions for improvements, primarily to the presentation of the method and results.\n\n1. The appendix is quite short, but includes vital details to understand what the data is.  For example, the fact that openpose is used to extract pose from the video data.  These details should be in the main text of the paper.\n\n2. I didn't understand the \"beat coverage\" evaluation.  Is \"standard deviation of motion\"  the euclidean distance between successive poses?  Or something different?  What constitutes a \"hit\" here -- beat tracking evaluation usually involves a tolerance window to account for differences in analysis parameters when comparing systems (eg, in mir_eval [1]).  It would be helpful to have a bit more detail (eg an equation) here.\n\n3. The data includes genre information, but this doesn't seem to be explicitly reported on except by way of the \"style match\" evaluation.  Are there differences in performance across styles, or do they all perform comparably?  I mainly ask because some of the input features (eg onset strength) will work better on some genres than other (hip hop vs ballet), and it would be good to have a sense of sensitivity to style in general.\n\n\nMinor comments:\n\n- On the topic of beat tracking, I'm a little unclear on what exactly is being done here on the audio side.  The main text refers to (Ellis, 2007) for beat tracking, but the appendix refers to (Boeck and Widmer, 2013), which uses a similarly defined (but practically quite different) onset strength function.  It'd be great to check the consistency here and report exactly what's being used in each place.\n\n- Related suggestion, many dance styles depend on the downbeat (bar lines) in addition to beats (usually quarter notes).  It may be worth including downbeat estimations (eg from madmom [2]) as an input feature at some point.\n\n- I think there's a typo in the appendix on audio preprocessing: are features really extracted at 15400 frames per second?\n\n- If you continue this line of work, you might want to check out the recently published (2019) AIST database [3].\n\n\n[1] https://craffel.github.io/mir_eval/\n\n[2] https://madmom.readthedocs.io/en/latest/\n\n[3] Tsuchida, Shuhei, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. \"AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing.\" In ISMIR, pp. 501-510. 2019.\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Transformer-LSTM model trained using annealed teacher forcing generates dance sequences for music",
            "review": "Summary:\nThe authors present a seq2seq model with a sparse transformer encoder and an LSTM decoder. They utilize a learning curriculum wherein the autoregressive decoder is initially trained using teacher forcing and is gradually fed its past predictions as training progresses. The authors introduce a new dataset for long term dance generation. They utilize both subjective and objective metrics to evaluate their method. The proposed method outperforms other baselines for dance generation. Finally they conduct ablation studies demonstrating the benefits of using a transformer encoder over other architectures, and the benefits of the proposed curriculum learning scheme.\n\nComments:\n1. The authors claim to introduce the local self-attention mechanism, however, it is very similar to an already proposed sparse transformer architecture [1].\n2. In the music encoder section, the authors claim that they can afford to look at a small locality in the music to generate the dance sequence. This is not necessarily true. The structure of music is arguably an important feature in the choreography of a dance sequence.\n3. Section 4: The experiment setup lacks important details. There is no mention of the length of the length of the music clip input to the model. Furthermore, in the appendix detailing the audio pre-processing steps, the sampling rate, window size, and hop size are not mentioned without which reproducibility greatly suffers. The mention that the audio frames are aligned with video frames. This is not ideal to extract chromagrams or onset strength. Assuming an audio sample rate of 44100Hz, the equivalent frame length will be 2940 samples (for 15 fps). 2940 samples is a long enough time for several onsets to occur within the frame. Regardless, these details are necessary within the main text of the paper and should not be relegated to the appendix.\n4. There are a few minor issues: \n    - Lee et al., 2013 is cited in the text but no reference exists in the bibliography. \n    - genration -> generation\n    - grammatical issues here and there\n    - the paper ends abruptly. A conclusion section summarizing the key findings and discussing future direction would be nice.\n    - there also exists another large dance database [2] which may be worth mentioning in the paper.\n\nOverall the paper utilizes modern deep learning techniques well to solve an interesting problem. However, there is a lack of depth in terms of how these techniques are adapted for the particular task. Hence, I rate the paper as a 4/10.\n\nReferences:\n[1] Child, Rewon, et al. \"Generating long sequences with sparse transformers.\" arXiv preprint arXiv:1904.10509 (2019).\n[2] Tsuchida, Shuhei, et al. \"AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing.\" ISMIR. 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel audio-conditioned dance generation model with extensive performance analysis",
            "review": "**Summary**\n\nThis paper proposes a system for generating long sequences of dance movements conditioned on audio. Through extensive analysis, the proposed system is shown to outperform previous methods across many metrics.\n\n**Strengths**\n\nExtensive analysis across many metrics, both qualitative and quantitative, give solid evidence of this system outperforming others. The included demo video is also a great example.\n\nLong-term generation analysis in section 5.2 clearly demonstrates this model handles long term sequences and shows important differences compared with other models that struggle with that. I thought showing this breakdown by time in addition to the overall FID score in Table 1 was very convincing.\n\nPlanned dataset and code release is a good community contribution and will ensure reproducibility.\n\nA novel architecture was created to handle difficulties of long-term sequence generation of dance moves.\n\nThis work expands the field of cross-modal learning.\n\n**Concerns**\n\nThe paper mentions a new dataset and codebase will be released, but few details are given about the contents of the dataset (e.g., how many clips of each genre? how were they collected? what license will be used?), the codebase (e.g., what framework was used?), or how they will eventually be accessed.\n\nBased on the description in section 3.3, it sounds like the model never trains without a sequence $p$ of ground truth teacher forced in the output. Is my understanding correct? If this is the case, does the model exhibit any signs of struggling to generate sequences longer than the maximum length of $q$ during training? Have you tried changing the training schedule such that $p$ eventually disappears? I would like to see some more discussion/clarification around these questions.\n\nI would like to see more information about the balance of the dataset by genre or other important attributes and then also see some of the evaluation statistics broken out by those attributes. Does the model perform better on some genres than others? If so, is this because of training set imbalance, audio feature differences, or other issues?\n\nMore information should be provided about the human evaluation procedure. For example: how many raters were involved, how many questions per rater, were they dance experts or not, did they view wire renderings or 3d models, etc.\n\nIn section 5.1, under “Beat Coverage and Hit Rate”, you mention for the first time that features about music beat were incorporated into the model. Prior to this, model input has just been described as audio features. I finally realized that there was an overview of the features hidden in the appendix. I definitely think that at least an overview of what features are used as input should be included in the main body of the paper.\n\nIt would be nice to have a brief conclusion at the end, including a discussion of future research directions.\n\nIn the appendix, I think you should include much more detail about the audio features used as input. For example, what were the parameters for computing the spectral features? Did you try different sets of features in your investigations? Are both CQT and MFCC really needed?\n\n**Additional minor feedback**\n\nSection 1, first paragraph: dance creation assistant -> dance creation assistance\nSection 1, final paragraph: four-folds -> fourfold\n\nI found the wording of the second paragraph in section 3.3 confusing. There’s a particularly awkward split between the first two sentences. I would recommend reworking this paragraph to be more clear.\n\nI would recommend rearranging the tables and figures that start on page 6 to occur in the order they are referenced in the text. If reasonable, it would also be nice if the table/figure is on the same page as the text describing it. For example, section 5.1 starts off by talking about human evaluation, but that figure isn’t until the next page. Figure 2 is about beat tracking, but that text isn’t until the next page.\n\nFor beat detection evaluation and as feature input to the model, it might be interesting to use a more recent model such as “MULTI-TASK LEARNING OF TEMPO AND BEAT: LEARNING ONE TO IMPROVE THE OTHER” by Bock et al. (http://archives.ismir.net/ismir2019/paper/000058.pdf). There’s an open source implementation that would be easy to incorporate here: https://github.com/CPJKU/madmom\n\n**Questions for the rebuttal period**\n\nDid you find that limiting the model to $k=100$ caused any limitations related to long-term coherence in the dances that were generated? For example, was the model unable to repeat dance motifs over a period of time longer than 100 events?\n\nIn section 3.3, you mention that generating motion as a real-valued vector causes more problems with error accumulation than sampling from a discrete probability distribution. Did you consider using a Mixture Density Network to allow sampling from continuous outputs?\n\n**Recommendation**\n\nMy recommendation is to accept this paper. It proposes novel techniques for music-conditioned dance generation and extensive analysis to show the success of those techniques.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}