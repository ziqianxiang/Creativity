{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a method to solve regression without correspondence.  The problem is well-motivated, and the proposed method is technically sound. The motivation, organization, and presentation of the paper are very clear.  Reviewersâ€™ suggestions to further improve the paper (e.g., clarifications on initialization, comparison and discussion with with EM, AD, etc) were adequately incorporated to the revised manuscript. "
    },
    "Reviews": [
        {
            "title": "The paper present a novel method for regression which involves continuous parameters as well as finding a permutation between two sets of observations. Compared with baselines, the methods performs  well.",
            "review": "Strengths:\n- the problem formulation is clean and clearly explained\n- the method presentation is well written\n- the techniques in used in the optimization steps, after  the relaxation step, are well motivated\n\nWeaknesses:\n- it is not clear whether the significance of  Regression without Correspondence, is high. The Multi-object tracking experiment seems contrived, at least a very good approximation of the permutation matrix can be obtained using descriptors and motion continuity\n- the issue of initialization, is not fully resolved. In the reported experiment in the discussion section, ROBOT succeed 30% of the time. The percentage may vary and it is not clear how a user of the method should find a good initialization regime.\n- in the experimental set-up that most resemble realistic applications (GFC for the cytometry), the improvement over AM, say a \"trivial baseline\" is not significant.\n\nNote: calling a method ROBOT will make it very difficult to google ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Would like to see more consideration to EM, RANSAC and AD",
            "review": "\n\nThe paper presents a method for robust regression with no correspondences. The association matrix is replaced by a matrix with continuous positive values. Two practical and relevant problems are investigated.\n\nThis seems to be a very interesting investigation, and the authors have done many commendable things such as studiyng relevant problems with real data, comparing to multiple alternatives, and appear to bring an interesting perspective to the problems.\n\nThe exposition of the method seems to lack a few details that might perhaps have escaped the attention of the authors. The recommendation unfortunately should tend towards not accepting. The authors should be urged to consider their work inside a wider perspective.\n\nThe main method that is repeatedly compared to the proposal was only AM. The proposal is in many ways actually more similar to Expectation-Maximization. The Birkhoff polytope here basically appears to represent likelihoods of data assignment according to Gaussian likelihoods.\n\nIt does not seem clear if the authors are exploring some further constraint in their proposal. It would be paramount to contrast the method with EM, though. The paper, as it is, looks like it could be basically proposing a form of EM without recognizing this. And for sure EM must bring many of the advantages over AM as the paper promises.\n\nSome further smaller points:\n\nIn section one, on the second point, ending with \"...they get stuck in local optima easily.\" What exactly is the argument here? If we are talking about a local, hill-climbing style of algorithm, there is no hope to escape local optima unless the initialization is improved, or the landscape is improved, or the algorithm is modified in such a way that it is not hill-climbing anymore, becoming some form of global optimization. In what way does the proposal differ?\n\nEM, compared to AM, would imply in an enhanced landscape, as also seems to be the case with the proposal. Another benefit is merely to utilize slower although more robust first-order optimization methods.\n\nIt's important to make explicit what exactly are the proposed benefits. The way this sentence is written may give an impression the algorithm is proposing something that goes above a local search style of algoritm.\n\nAnother important point is about initialization which is certainly crytical to all such algoritms. In the experiments the authors suggest there's an improvement there, although the improvement must be more related only to a better landscape. Although what was the initialization afterall? This must be evaluated in the context of how good the available initializations are.\n\nStill on section one: \"Efficient first-order optimization algorithms\" - Efficient relative to what? How would second-order algorithms be classified?\n\nRegarding the experiments, RANSAC, by virtue of being a global optimization algoritm unlike the proposal and other alternatives, would be expected to be able to reach very high levels of accuracy, even if associated with a great computational cost. When comparing such a method with RANSAC one would expect to see a discussion about time/accuracy compromises. The paper is only presenting RANSAC as a method that could not reach a satisfactory accuracy. It should present at least an accuracy at a setting that was deemed to be equiparable computationally.\n\nOne final remark about AD. It is in general expected that AD techniques can match explicit derivative formulas, unless the problem is not well suited to the specific AD technique used. Or sometimes the user is required to make some extra tuning or configuration. It would be nice to review exactly how AD was not suitable, and whether there isn't an AD based solution for that (e.g. forward mode versus backwards mode). \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review --- A Hypergradient Approach to Robust Regression without Correspondence ",
            "review": "The authors proposed a novel method for regression problems with outliers. The main idea is to first propose a mixed-integer optimization problem for the regression problem and then and the optimization procedure of finding the solutiuon of the problem differentiable, and the objective function of the problem are also be rephrased as a differentiable function. Based on this, an end-to-end learning approach can be established.\n\nPros:\n\n1. The motivation of the paper is very clearly stated in the text, and the sketch of the theorems make the paper easy to understand. \n2. The experimental part is good and it proved the efficiency of the proposed method.\n3. The idea of converting a mixed-integer programming to a differentiable function is elegent. \n\nCons:\n1. The authors says that they are going to somehow relax the one-to-one matching constraints, however, in the main text we can see that the model is still based on strict one-to-one matching constraints. In the experiments, for synthetic data, every generated data is in fact an one-to-one matching. For other datasets, through they are not strictly one-to-one, but they are close to one-to-one.\n\n2. Theorem 1 is a trival result due to total uni-modular, and it is proved many years ago, maybe it would be better to simple give a citation there. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very clear presentation and interesting applications",
            "review": "In this submission, the authors propose a bilevel optimization based solution to the problem of Regression Without Correspondence (RWOC). \n\nStrong points:\n1. The paper writing is very clear. I didn't know the RWOC problem before reading this submission; but after reading the introduction, I can clearly understand the problem setting, its applications (the two provided examples are great!), its challenges and what is the high-level idea of the proposed solution. Also, the organization and presentation of experiment part are very clear and easy to follow.\n\n2. Besides the normal case of RWOC, the authors also consider and solve the case of \"partial one-to-one correspondence\". They also demonstrate its application using multiple-object tracking.\n\n\nThere are some issues that can be addressed to further make the submission strong:\n1. For the experiments about multi-object tracking, could the authors include some (at least one) strong baseline? The current experiment setting for this part is not so convincing and it is a bit difficult to justify the improvement.\n\n2. Besides the application of multi-object tracking, could the authors discuss more potential applications of RWOC? This can enlarge the application scope and make the studied problem more important and practical.\n\n\nMinor places:\n1. Page 2, the paragraph above \"Related Works\": rRWOC -> RWOC\n\n2. Fig 2 and Fig 3: For the printed paper version (black-white), these figures are hard to read. The authors can use different markers or textual to differentiate methods.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}