{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is proposing a multi-task learning approach extending existing weighting approaches. An important and novel contribution of the paper is separating the magnitude and direction information in gradient based information. The joint gradient direction is searched by using angle bisectors of task gradients and magnitude is searched by simply finding a scaling which results in uniform loss scales. This approach solves issues like small gradient norm bias of MGDA, etc. The proposed method works well and authors show that it is conceptually relevant to most of the existing algorithms. These conceptual unification is a strong contribution of the paper. The paper is reviewed by three reviewers and received both accept and reject scores. Specifically,\n\n- R#2: Championed the paper and argued for its acceptance\n- R#3: Argues that the novelty is limited and SOTA claim is problematic.\n- R#4: Argues that the gap between the empirical performance of the proposed method and existing algorithms is small. \n\nArguments on the empirical performance and the SOTA are irrelevant to the decision since ICLR does not require algorithms to be SOTA or performed significantly better. Hence, the remaining issues are: claim of the SOTA being true or not, and lack of novelty. I read the paper in detail and decided to accept it with the following comments about the reviews:\n- The paper is clearly novel. Direction and magnitude are first time treated separately. Moreover, resulting unification of the existing approaches and theoretical derivations of the important connections of existing methods are also significant.\n- The SOTA claim of the paper is technically correct but little misleading. I would recommend authors to simply rephrase it \"proposed method outperforms existing methods loss weighting methods under the same experimental settings\". The reason for this is the fact that; in principle, \"art\" includes every possible solution for that problem. Hence, claiming SOTA in a fair and limited evaluation is rather misleading.\n\nIn addition to the reviewer comments, here are additional issues which should be addressed by the camera-ready deadline:\n- I think the discussion about MGDA is a bit problematic since removing $\\alpha \\geq 0$ assumption simply removes the Pareto stationarity guarantee of the method. The resulting direction can increase some loss function and this disagrees with the main point of the Pareto optimality. Hence, I would recommend authors to clarify this while making the connection. Frank-Wolfe algorithm is also not really inefficient and unstable since the problem is quadratic with linear constraints and the stability as well as extremely quick convergence can trivially be proved.\n- In addition to the previous point, the proposed method can actually increase some loss functions as there is no consistency constraint enforced. This is an interesting observation and empirical results suggest that increasing loss of some objectives might actually be valuable. I think this observation deserves some discussion even in the introduction.\n\n"
    },
    "Reviews": [
        {
            "title": "A principled and practical solution to training multiple task in a fair way",
            "review": "Summary:\n\nThis paper presents a satisfying solution to the open problem of how to train all tasks at approximately the same rate in multi-task learning. There has been a bunch of work on this problem in the last few years. This paper characterizes existing work w.r.t. the fairness of training across tasks in order to motivate two new methods, one applied to shared parameters and the other to task-specific parameters, which overcome the shortcomings of previous methods. The two new methods can be naturally combined to yield a complete method for fair training. Experiments on common MTL benchmarks show the new method compares quite favorably to previous approaches.\n\nThe methods are developed in a theoretical manner from a reasonable set of assumptions. Despite the theoretical derivation, the resulting methods are straightforward to implement, especially compared to more complex iterative or probabilistic methods. Due to its high performance and simplicity of implementation, my expectation is that this new method could quickly become a standard tool for multi-task learning applications.\n\n\nStrong points:\n\nThe paper is overall well-written and well-motivated in the literature.\n\nThe paper addresses an important open problem in multi-task learning.\n\nThe theoretical derivations are well-motivated based on fairness desiderata.\n\nThe resulting methods are effective and simple-to-implement.\n\nThe theoretical and conceptual comparisons to existing methods make it clear how the paper unifies and extends the existing literature.\n\nThe experiments are convincing, and the value of each aspect of the method is independently confirmed.\n\n\nWeak points:\n\nThe paper could benefit from a discussion of the limitations of using fairness as a proxy for MTL performance. Fairness is an intuitive requirement, but one can imagine cases where some tasks should be trained more than others, e.g., if some tasks begin to overfit at higher losses than others.\n\nThe motivation is framed as addressing the problem of some tasks being sacrificed/undertrained. However, there is a common and closely-related issue of some tasks overfitting before others are satisfactorily trained. So, the more general question is how to synchronize the state of training so that all tasks reach peak performance at the same time. Does the approach in the paper address this more general question? Or is it only effective in the setting where overfitting is not an issue because the datasets are so large?\n\nFor example, in the paper the comment “NYUv2 is a rather small dataset, so uniform scaling can also obtain reasonable results” emphasizes that underfitting is the focus, but if the dataset is so small, could uniform scaling lead to overfitting that IMTL could address?\n\nIn the experiments, are all the key details of the existing methods reimplemented, or are some of them taken from official implementations? This should be made clear in the paper and in the released code.\n\n\nMinor comments:\n\nIn the first paragraph of the introduction, are Zamir et al. 2018 and 2020 really the key references for MTL overall? There is no evidence from the introduction that MTL was around before 2018.\n\nIn the first full sentence after Eq. 8: “corresponds to maximize the” -> “corresponds to maximizing the”.\n\nIn “Results on Cityscapes”: “Surprisingly, we find our IMTL can beat the single-task baseline where each task is trained on a separate model.” I don’t think this is surprising. By your definition of MTL in the introduction, this is what you expect if you do MTL right. However, it is somewhat surprising that none of the other MTL methods exceed the single-task baseline on all three tasks; this highlights the fact that they are missing something.\n\nIn Appendix D: “The output size is initially 8x down-sampled and later up-sampled…” Why?\n\n---------------------\n\nUpdate: The authors have adequately addressed my questions, and I am happy to maintain the rating of 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work but not that significant",
            "review": "This paper introduces an impartial multi-task learning approach to balance gradient and loss of multiple tasks. Since biased learning can degrade learning efficiency and this work tries to make a balance in a principled way. The approach is simple but looks effective.\nHowever, the gradient balance adopts the last shared feature instead of weights for faster training. From this, I wonder whether this kind of approximation decreases the capability of the weight-level balancing strategy. Theoretical or empirical analyses on this make the proposed method stronger.\n\nThe proposed method gives comparable performance compared to other strong competitors but the gap is not remarkable. I wonder if this performance gap can deserve the attention of practitioners. It would be great for more rigorous analyses to show qualitative results and how this approach differentiates it from others. The proposed method needs to show its potential further.\n\nIn addition, if this approach applies to multiple datasets (as multiple tasks, for example, visual decathlon challenge), the proposed approach may not work well due to the different characteristics when we train them using a hard parameter sharing approach. For a general approach, it would be great to analyze a variety of scenarios.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but lacks correctness",
            "review": "The authors propose to balance multi-task training using IMTL-G on the shared backbone and IMTL-L on the task-specific branches. IMTL-G enforces equal gradient projections between tasks with a close-form formulation to calculate the desired gradient weightings $\\alpha$. IMTL-L learns the loss weightings $e^s$ with a regularization term $-s$. Additional constraint by making all loss weightings sum to one is used. The paper compares the effectiveness of the proposed IMTLs with their counterparts on Cityscapes, NYUv2, and CelebA and claims state-of-the-art performance.\n\nFirst of all, the authors start out by claiming multi-task learning architecture improvement will lead to high inference cost (Sec 2 first paragraph), so they decided to investigate loss weighting balancing. However, methods have been developed to explore better task hierarchies to design better task grouping in multi-task network architectures in recent publications [R1, R2]. They show SOTA results without high inference cost. In fact, these works also show that better task grouping in network design can already alleviate the need to use sophisticated loss balancing tricks. Although multi-task network design and loss balancing can be orthogonal efforts, it is important to have reasonable motivation and fair discussion.\n\nThe authors claim state-of-the-art performance on all tasks. However, the proposed IMTL method obtained 91.12% accuracy on CelebA where method in [R2] obtained 91.62% accuracy. Therefore, it is incorrect to claim SOTA in the submission. \n\nEquation (2) requires expensive matrix inverse operation for every iteration during training. How much additional computational overhead is needed compared to other straightforward gradient balancing approaches? It would be great to have a clear time complexity comparison with real world runtimes measured.\n\nThe authors claim the proposed IMTL-L does not require any distribution assumption, resulting in the obtained loss weighting $e^s$ with a regularization term $-s$. The proposed formulation is actually quite similar to the original uncertainty weighting approach (Kendall et al. v1 eq.11) with the Gaussian distribution assumption. The only difference is the 0.5 scale for the first term. This similarity also reflected in the experimental results that in Table 1 we can see IMTL-L performs quite similarly to uncertainty weighting (Kendall et al). Uncertainty weighting has even better performance on instance segmentation and disparity compared to the IMTL-L. Can the authors discuss in more details the differences between the two approaches? \n\nBesides, I found Algorithm 1 on page 4 is quite straightforward. It would be great to use the space to further analysis the difference between the proposed IMTL-L and the uncertainty weighting approach (Kendall et al).\n\n[R1] Standley et al. Which Tasks Should Be Learned Together in Multi-task Learning? In ICML 2020. \\\n[R2] Guo et al. Learning to Branch for Multi-Task Learning. In ICML 2020.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}