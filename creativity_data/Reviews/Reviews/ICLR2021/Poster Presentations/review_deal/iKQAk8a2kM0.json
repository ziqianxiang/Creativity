{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The major concerns about this paper are that (1) There are too many hyper-parameters, such as those needed for ADMM. I'd point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper-parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it’s unclear why flipping fewer bits is an important objective - an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought-out approach that improves the state of the art by some metrics.\n"
    },
    "Reviews": [
        {
            "title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits",
            "review": "The paper proposes an optimization-based algorithm for bit-flipping a limited number\nof bits in a quantized / binarized deep-learning model, so that the prediction on a\ntarget input example is flipped while the prediction on the other examples is as\nuntouched as possible. The problem is formulated as a binary integer programming (BIP)\nproblem, which is then solved using a recent ADMM-based technique. Experiments CIFAR-10\nand ImageNet show that the proposed method outperforms the SOTA.\n\n**Weak points:**\n- The main shortcoming of this paper is the limitedness of the technical contributions.\n\n- The hyper-parameter tuning is not clearly outlined / explained. This is problematic\n  since there are quite a number of hyper-parameters. For example, I can count 4\n  hyper-parameters in equation (9) alone (including ADMM stepsizes rho_i).\n\n- It would be nice to have a back-of-envelop estimation of the complexity (running time,\n  number of flops, etc.) of the proposed method, as a function of the maximum number of\n  bits to flip (say).\n\n**Small issues:**\n- S. Boyd and co-workers have done a great job in popularizing ADMM. However, this method\n  has been around at least since the 70s. Key papers to reference when talking about ADMM\n  include:\n  * Glowinski and Marroco (1975) \"Sur l'approximation, par éléments finis d'ordre un, et\n    la résolution par pénalisation-dualité d'une classe de problèmes de Dirichlet non\n    linéaires dualite d'une classe de problemes de Dirichlet non linéaires\"\n  * Gabay and Mercier (1976) \"A dual algorithm for the solution of nonlinear variational\n    problems via finite element approximation\"\n\n**Strong points:**\n- The strongest point in favor of this paper is that unlike the SOTA methods, the proposed\n  method only flips to a very limited number of bits in the binarized DNN model, while\n  achieving the same or higher accuracy.\n- The experiments are very detailed and well-presented.\n\n**Errors:**\n\n- The equivalence in (6) doesn't seem to make sense. In the definition of $S_p$, $\\hat{b}$ is an element of what ?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper describes a bit-flipping white-box attack on deployed neural network classifiers: given a model with quantized parameters, find a perturbation of the parameters bits such that the model with misclassify one specific example, while maintaining high accuracy on other examples.\n\nThe attack is formulated as a binary programmig problem where the parameter bits are the optimization variables and the objective function is an additive tradeoff between an effectiveness term (misclassification loss on the selected example) and a stealthness loss (classification loss on a batch of training examples), a constraint on the number of bit flips is also included. The optimization problem is solved by continuous relaxation using the Lp-box ADMM solver.\n\nThe paper reports experiments on various standard classifiers trained on CIFAR-10 or ImageNet, with different level of quantization. The proposed attack is compared to other weight attacks in the literature, and it achieves comparable or better attack success rate (a measure of effectiveness) and post-attack accuracy (a measure of stealthness).\nThere are also experiments on different values of hyperparameters and on more robust models (obtained either by a defense technique or by making the model bigger).\n\nOverall I find this a valid contribution.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "This paper proposes an ADMM based optimization method to conduct adversarial weight attack, and achieves superior or at least comparable performance compared with previous heuristic methods.\n\nPros:\n1. Adversarial weight attack is an interesting research direction with important practical importance and deserve more studies. \n2. The proposed method is mathematically sound. And it empirically outperforms or at least is comparable with previous state-of-the-art methods on undefended models, and consistently outperforms previous methods on defended models.\n\nCons:\nI think this paper as an necessary step towards stronger adversarial weight attacks, which could be used as an evaluation method to benchmark future defense methods.  \n\nSome comments:\n1. Table 1 and 2 may not be the best way to present the results. Considering there are three evaluation dimensions (PA-ACC, ASR and Nflip), I suggest the authors to add some pareto frontier figures. For example, fixing PA-ACC, plot the tradeoff curves between ASR and Nflip of different methods.\n2. What are the time costs of different attacking methods?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unclear motivation and improvement over prior works",
            "review": "The paper proposes a bit-flip attack where model parameters weights are altered such that a certain sample is misclassified to a target class.  While the utilized optimization strategy and the combination of techniques seems interesting, a major concern is the motivation behind the proposed method and why it stands out against prior works. \n- Comparisons in the experiment section show that prior methods are performing on par with the proposed method in terms of the attack success rate and benign accuracy. The major difference seems to be the number of flipped bits. However, the paper in its current form does not state \"why\" the number of flipped bits seems to matter. \n- The method is also performing on par with prior works in terms of the resiliency against defense mechanisms. Therefore, it is not really clear why this work is preferred over prior methods, i.e., does reducing the number of flipped bits matter in the attack?\n- In addition to motivating the number of flipped bits, the authors need to also clarify why the current \"single image\" attack is preferred over other works where all images from a certain class are mapped to the attack target class. It seems like the proposed approach is in fact a special case of the latter scenario which is studied in prior works.\n- The evaluated defense strategies are all passive, i.e., they are performed before the attack and therefore are not aware of the attack strategy. For a comprehensive examination, the authors should also compare with the defense in [1] which applied the defense at the inference phase, assuming a bit-flip attack may have occurred on the model parameters.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}