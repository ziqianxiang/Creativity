{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper investigates the interesting question whether increasing the width or the number of parameters is \nresponsible for improved test accuracy. The paper is very well written and the question is novel and innovative.\nFrom a methodological point of view, the experiments are well conducted, too.\nThe theoretical part of the paper is somewhat detached from the experimental part and constitutes more of\na heuristic conjecture. In addition, more experiments on a variety of other data sets would have been great.\nIdeally, the theoretical section would thus be replaced by such additional experiments, but this is of course not \nan option for a conference reviewing system.\nGiven the innovative question and well-conducted experiments I think that the pros outweighs the cons,\nand for this reason I recommend to accept the paper. Reviewer concerns have been well addressed by the authors in their rebuttal and updated version of the paper."
    },
    "Reviews": [
        {
            "title": "Intriguing discussion which could have been more self-contained",
            "review": "This manuscript provides an intriguing discussion on the different roles that the width and parameter size could play in a neural network. While these two aspects are traditionally treated as -if not identical- correlated, the authors managed to develop a couple of configurations to decouple and analyze both separately. Especially the wide and sparse approach could be a new way to design neural networks that are supposed to be small and expressive at the same time. \nI'm only concerned about two aspects: i) The section 3 could have been perhaps easier to follow if the work by Jacot et al. 2018 had been briefly recapped. ii) The experiments are all conducted on image dataset. One might wonder whether we could draw the same conclusion for datasets of more general nature. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "I provide my personal opinions on the analyses about the effect brought by widening nets from both experimental and theoretical perspectives.",
            "review": "In this paper, the authors analyze the enhancements brought by widening networks with the number of parameters fixed. From the experimental side, they conduct various experiments to compare the methods of widening the networks and demonstrate different ratios of widening different networks on diverse datasets. From the theoretical side, the authors relate the training dynamics of neural networks to kernel-based learning, in  the infinite-width limit. As a consequence, the authors claim that wider networks indeed improve the performance of algorithms under certain conditions.\n\nMy personal comments are listed below.\nPros.\n1. The idea of widening the neural networks with fixed number of parameters is novel and the authors conducts abundant experiments to support their assertions.\n2. The expressions and grammars are excellent.\n\nCons.\nI. Clarity Part.\n1. The authors spend much time and space to introduce linear and non-linear bottleneck methods to widening the neural networks with fixing the number of parameters, and, subsequently, prove the 2 methods are not qualified. From my opinion, since the authors have asserted that they try to avoid big adjustments on structure of the networks, the bottleneck-like methods are inherently excluded from the list. Meanwhile, the explanations for sparsity methods seems to be lack.\n\n2. While the authors provide meaningful and comprehensive plots to demonstrate the results of widening neural networks, the explanations and instructions of the plots and pictures are sometimes missing. For example, In figure 5, the trends of test accurarcy to widening factor are not always positive, some go down when the widening factors are big, others seems to be static. How do they happen? I think further explorations can be made.\n\n3. As for the theoretical side, the authors reference the NTK techniques (Jacot et al., 2018) to explain the relationships between the finite-width neual network and the kernel-based learning. However, NTK techniques are designed for infinite-width neural networks, and have constraints when applied on finite-width networks. As the authors assert, the synchronization of optimizations of 2 methods is only conjecture, which means the prove provide here can only be used to support the NTK theorems. Besides, the relationships between NTK and widening neural networks are not detailedly explained.\n\nII. Quality Part\n1. Figure 3, (b) has no horizontal baselines for traditional methods.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but a bit vaguely-defined problem",
            "review": "Summary:\n\nThis paper analyzed the influence of neural network width on the network performances while fixing the total number of parameters. Specifically, the authors introduced several ways to change the model width without increasing the number of parameters, and showed experimentally that for widened networks with a random static mask on weights to keep the number of parameters, increasing the width can improve the performances of the models until the network become very sparse and hard to train. They author theoretically showed that for a not so sparse one-hidden-layer neural network, increasing width decreases the distance to the Gaussian Process kernel corresponding to the infinith-width limit, which partially supports their experimental findings.\n\nPros:\n\n1. Understanding the reason behind the generalization performance of neural networks is a very important problem, and the idea of decoupling the effects of network width and the number of parameters on the generalization performance is interesting.\n\n2. This paper is clearly written, well organized, and generally easy to understand. The figures in this paper are clear, and each figure was explained in detail. The methods/experimental settings are explained in detail, and the reasons why the authors use these models/methods are also discussed in the paper. The relationship to prior works has been clearly discussed.\n\n3. The experimental methodologies and theoretical computations appear to be correct.\n\nCons:\n\n1. The justification for this specific problem analyzed in this paper, i.e., fixing the number of parameters and changing the width, is a bit unclear. The number of parameters can be interpreted as a measure for the expressive power of the model, but this is not a very direct relationship because the expressive power is also related to other factors, e.g., the depth, the width, the choice and ordering of the layers. In the methods proposed in this paper, the authors were either changing the explicit number of parameters (e.g., linear bottlenetwork) or changing the expressive power of the network, which included other (perhaps unwanted) factors in this framework and could potentially influence the results. Details will be discussed in the next point.\n\n2. The definition of \"number of parameters\" is a bit unclear in this paper, making the problem that the authors want to analyze a bit vague. The authors didn't formally define it, but it is actually one of the core concept in this paper. The authors used three similar terms, i.e., \"number of parameters\"/\"number of tunable parameters\", \"effective number of parameters\", and \"expressive power\", in this paper. However, these three terms have slightly different meanings, and none of them is defined in this paper. From my understanding, \"expressive power\" is the set of all functions that can be reprensented by a model, and \"number of parameters\"/\"number of tunable parameters\" is a simple parameter count and can be used as a complexity measure of the expressive power, but \"effective number of parameters\" is kind of confusing. For instance, putting a low-rank constraint in Linear Bottleneck or using a sparse model actually changes the expressive power of the neural network but (claimed by this paper) does not change the \"effective number of parameters\". Due to the lack of formal definition of these three terms, I think the problem analyzed by this paper is not clear enough, making the points made in this paper unclear.\n\n3. The authors are explaining some of the experimental results in a very intuitive way without enough supporting evidence. For example, the authors said that Linear Bottleneck suffers from some \"implicit bias\", deleting weights from a sparse layer make optimization \"more difficult\", and small connectivity can hurt the \"trainability\" of the model. These terms are not formally defined or explained in detail, making the points made by the authors a little vague.\n\n4. The connection between the theory and experiments in this paper is a bit weak. People have observed that the performances of commonly-used neural networks always outperform their infinith-width limit, i.e., the corresponding infinith-width NTK. Therefore, in practical neural network models, simply decreasing the distance to the infinith-width limit may hurt the performance. Thus, I suspect that the explanation provided by the authors only works for some particular examples like small networks on MNIST.\n\nRecommendation:\n\nOverall, I vote for rejecting this paper. Although the idea of decoupling the influence of width and number of parameters is interesting, the problem analyzed in this paper is vague and a lot of terms used in this paper is not properly defined, making the arguments in this paper unclear.\n\nSupporting arguments for recommendation:\n\nSee \"Cons\".\n\nQuestions for the authors:\n\n1. Please address the cons mentioned above.\n\n2. For the experiments, I wonder whether you have done enough fine-tuning for the hyperparameters of each model and used the best test accuracy. Also, have you run each experiment multiple times or only once?\n\n3. I didn't understand why you do not consider the linear/non-linear bottleneck as an effective method. I think the goal is to analyze the effect of width but not to get a significant improvement on the test accuracy.\n\nMinor comments:\n\n1. Typo: Page 5, \"MLP\", paragraph 3, \"This results\" -> \"This result\"\n\n2. Page 5, paragraph 1, which model is \"the model with baseline width 8\"? Is it the model with the number of parameters 1.8e+5?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}