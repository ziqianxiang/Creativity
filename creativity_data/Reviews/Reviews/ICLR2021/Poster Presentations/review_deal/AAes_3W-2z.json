{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes  a novel and interesting embedding of graphs emulating the Wasserstein distance. The experiments are good and the authors did a detailed answer taking into account the comments of the reviewer. The responses were appreciated and the AC recommends the paper to be accepted."
    },
    "Reviews": [
        {
            "title": "The paper has merits, but I have a few concerns about the experimental analysis.",
            "review": "\nThe paper presents an embedding method for graphs, denoted as WEGL. The method operates by first representing each graph as a set of vector representations of its nodes, secondly, an optimal transport map is computed with respect to a finite reference set of points and, finally, a fixed-size vector representation is computed from that map.\nThe result is an embedding into a Hilbert space, where the distance between two graph representations approximates the 2-Wasserstein distance between their respective node distributions.\n\nThe paper is partly inspired by WWL (Togninalli et al. 2019) and provides an explicit embedding map, hence improving in terms of computational performance. I have some comments and concerns about the experimental analysis, which does not seem to directly validate the paper claims. Overall, it is well written and clear in almost all its parts.\n\nComments.\n1. The paper considers only the smallest of the datasets in OGB (Hu et al., 2020) for graph-level prediction, specifically ogbg-molhiv. I wonder if the proposed method, which is claimed to be computationally efficient, can handle also the other three.\n2. Choice of the baseline methods. I expected comparisons with other (explicit) embedding methods (eg, Hu* et al. 2020, Kriege et al. 2014) both computationally and at the task. Since WEGL is somehow a variation of WWL, I would like to see its performance compared also in Section 5.1.\n3. How performance has been assessed. The paper mention in the captions of Tables 1 and 2 that some of the results have been reported from already published papers. However, I couldn't find any details about the experimental settings of the other methods. Is the architecture of the GIN in Figure 3 the same as that in Tables 1 and 2? Are all the baseline methods operating on graphs with the \"virtual node\" variant (which is important for a fair comparison of the embedding capability)?\n\nTo further improve the paper, I suggest spending a few more words to\n- clarify the use of the Jacobian and the approximation in points 1 and 3, Section 3.1;\n- define the barycentric project and clarify why it is not invertible.\n\n(Kriege et al. 2014) Kriege, Nils, et al. \"Explicit versus implicit graph feature maps: A computational phase transition for walk kernels.\" 2014 IEEE international conference on data mining. IEEE, 2014.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": " A simple but interesting combination of 'linear' OT and graph embeddings, with less than convincing evaluation",
            "review": "Strengths:\n* **Computational advantage**. The quadratic-to-linear complexity reduction for pairwise computation (each of them an LP) of LOT is clearly the main selling point of this paper, which makes it an appealing approach for comparing multiple graphs. \n* **Writing**. The paper is overall very clear, well-written, and engaging. \n\nWeaknesses:\n* **Novelty**. The paper proposes a farily trivial combination of Wang et al's 'linear' OT with simple message passing on graphs. The main aspect where there could have been an interesting novel contribution, the computation of the reference embedding, is solved somewhat unsatisfactorily with a k-means embedding\n* **Unexplained design choices**. The use of the 'virtual node' is not well motivated, nor its contribution evaluatued experimentally. In addition, it's not clear why it was decided to not implement/comparse the entropy-regularized version of the proposed method, given that this could hold the key to truly scalable graph comparison. In addition, entropy regularization has consistently shown to even *improve* performance compared to exact OT in many applications. Most OT libraries already include this variant, so there really seems to be no excuse not to incoporate it. \n* **Some important trade-offs brushed under the rug**. Equation (13) seems to suggest that a NxNxN tensor has to be stored in memory, which could be prohibitive in many settings. Is it fair to say that LOT is implicitly trading time by space complexity? In fact, one could argue that this not even entirely correct, as the LP solved in (13) is now cubic rather than quadratic on N, so it seems there is a trade-off in computation too: reducing the number of pairwise comparisons but making each individual LP problem larger. \n* **Experimental results raise questions**. There are many moving parts in the evaluation (e.g., the addition of the virtual node, different types of classifiers), so it's hard to disentangle the core effect of the proposed method. \n    * The results in Table 1 seem to indicate that the classifier at the end of the pipeline has a much stronger effect on performance than the proposed method. \n    * What kind of classifiers are used in the other GNN methods? Where these also optimized for performance? Why not using the same type of classifier on all methods for fair comparison?\n    * All the baseline methods have a very strong degradation in performance between validation and test sets. For the proposed method, the degradation is much less signigicant, especially for the +AutoML classifier. How do the authors explain this? This seems to indicate that there is significant overfitting in all methods, and that proper regularization of the classifiers (e.g., via AutoML) is mitigating this for WEGL. This casts significant doubts on the results of this section in my opinion.\n    * Again, for the results in Section 2, the classifier seems to have an important effect in the performance of the proposed methods, yet there is no discussion on the classifiers used in all the baselines. In addition, the standard errors are so large that pretty much all methods have overlapping confiedence intervals, so it's hard to draw a statistically significant conclusion from these results. \n    * The runtime comparison is also limited. First, it's not clear whether the runtimes shown for WEGL include the full pipeline (e.g., including the reference embedding computation). Second, one of the methods is run on GPU while the other two are run on CPU, so it's hard to put those two sets of results in direct comparison. \n\n* **Misses importart related work**. E.g., \n    * [1] attempts to realize Characteristic (4) mentioned in Section 3\n    * [2] also leverages generalized geodesic for comparison of multiple measures\n    * [3] combines OT and WL Kernels and applies it to settings very similar to the ones tackled here. \n\nOther comments questions:\n* It would be informative to see a discussion on how well LOT enfoces Characteristic 4, that is, what guarantees does one have on how close the l2 embedded distances is to the true wasserstein distance?\n\nMissing references:\n* [1] Courty et al., \"Learning Wasserstein Embeddings\", 2017\n* [2] Seguy and Cuturi, \"Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric\", NeurIPS 2015. \n* [3] BÃ©cigneul et al., \"Optimal Transport Graph Neural Networks\", 2020",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising but can be stenghten",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposes to use a Wasserstein embedding to compare graph using also node embedding methods to convert graphs into vectors. While the elements are not new, the proposed method is new and is competitive with other classical methods (graph kernel and graph neural networks). \n\n##########################################################################\n\nReasons for score:\n\nOverall, I vote for mild acceptation. The method is clearly new, but the ingredients are already new and the results are not godd but not exiting.\n\n##########################################################################\n\nPros:\n\n1. The combination of node embedding with Wasserstein distance is new and show pretty good results.\n\n2. The method itself seems very flexible and effective compare to other state of art methods.\n\n3. The presentation of the methods is rather clear and most of the technical details are available. As such the paper is self-contained.\n\n##########################################################################\n\nCons:\n\n1. The framework relies on a node embedding method. Seems the choice of the method could be crucial, we need some discussion on this point.\n\n2. The proposed node embedding formula (equation 9) assumes that the information on the edge is a scalar. However previously in the paper the edge attributes are a vector, please give some slues on how to deal with such edges.\n\n3. The section 4.2 is not clear for me. Is the zero distribution compute on the whole database? Or is it compute class by class? Both ways seems possible please clarify?\n \n##########################################################################\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A smart, innovative embedding algorithm based on optimal transport",
            "review": "# Synopsis of the paper\n\nThis paper presents WEGL (\"Wasserstein Embedding for Graph Learning\"),\na technique for embedding graphs for graph classification. The primary\nnovelty of the paper lies in their smart choice of embedding, which\ncombines the expressivity of optimal transport paradigms (i.e. the\nWasserstein distance, in this case) with improved computational\nperformance; more precisely, the embeddings permit the use of effective\nclassification algorithms, as opposed to scaling quadratically with the\nnumber of samples.\n\n# Summary of the review\n\nThis is a well-written paper with a powerful algorithm and a strong\nexperimental section. It will make an excellent contribution to the\nconference, and I am excited to endorse it!\n\nThere are some issues with the current version of the write-up, though,\nwhich, if fixed, will make this an even stronger publication. My primary\nconcerns at present are:\n\n1. The paper spends a lot of space in outlining background information\n   that is not pertinent to the method. I appreciate the amount of\n   details that are being packed into Section 2.1, but as a reader of\n   this paper, I would prefer a more in-depth discussion of the method\n   rather than a discussion of GNNs, which are used as mere comparison\n   partners here. My suggestion would be to put some of these\n   information into the appendix.\n\n2. The section on Wasserstein distances could be streamlined. At\n   present, too many different concepts are introduced; I feel that\n   a non-expert reader will just be deterred, even though the remainder\n   of the paper is very hands-on. Maybe some additional intuition could\n   be provided here?\n\n3. On the other hand, Section 3 is *missing* important information; in\n   this section, I would be very much interested in knowing more about\n   the theoretical properties of the method (and its empirical\n   performance). The characteristics mentioned on p. 4 are intriguing,\n   but it would improve the paper if a more detailed write-up would be\n   provided; a reference with proofs for these properties would be\n   equally helpful. I am fully aware that it is hard to satisfy both\n   theory and experiments in a paper; I have some more detailed comments\n   about what could be added (potentially in the supplementary materials,\n   if the authors think that it detracts from the flow).\n\n# Detailed Comments\n\n- The point about the 'true metric' in the introduction is slightly\n  ambiguous. My understanding is that one obtains a metric between the\n  embedded feature vectors. This does *not* constitute a metric on the\n  graphs, though, unless the embedding is injective. I think the\n  sentence means to say that one obtains a metric in an embedding space\n  and this metric serves as a proxy for the Wasserstein distance.\n\n- In the related work section, I would suggest citing other variants of\n  the WL algorithm that have recently emerged:\n\n    - Morris et al.: *Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks* (https://aaai.org/ojs/index.php/AAAI/article/view/4384) \n    - Morris et al.: *Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings* (https://arxiv.org/abs/1904.01543)\n    - Rieck et al.: *A Persistent Weisfeiler-Lehman Procedure for Graph Classification* (http://proceedings.mlr.press/v97/rieck19a.html)\n\n- The way a graph is defined in this paper could be misconstrued as\n  *directed* upon first reading. Why not define edges in both directions\n  by using a subset notation instead of a tuple notation?\n\n- How are categorical attributes treated? In the supp. mat., the usual\n  one-hot encoding is mentioned. This could be discussed earlier (i.e.\n  on p. 2).\n\n- When discussing the embedding, I would suggest briefly mentioning the\n  concept of reproducing kernel Hilbert spaces (RKHS). Such a discussion\n  will provide the relevant backdrop for this publication.\n\n- As suggested above, I would shorten Section 2.1 somewhat (or put some\n  of the text into the appendix). While it is good to provide some more\n  details about the inner workings of GNNS, this is not required for the\n  paper.\n\n- Section 2.2 would benefit from more intuition; a lot of the\n  terminology is introduced too tersely and not re-used. I fully\n  appreciate that the paper is mathematically precise here, but at the\n  same time, I would suggest a more streamlined introduction of the\n  concepts that are necessary to define the embedding in the end.\n\n- Section 3.1 and Figure 1 differ slightly in terms of their notation.\n  I would suggest to harmonise the description here in order to be more\n  consistent.\n\n- For property 4 on p. 4, is it possible to quantify the strength of the\n  approximation? How good is this approximation in practice, and what\n  are the factors influencing it? I would suggest adding some more\n  details here (or citations, if appropriate).\n\n- To add to the previous point, I would in general like to know more\n  about the stability properties of the full embedding. Can this easily\n  be quantified? For example, what happens if I add some noise to the\n  attributes? I would assume that stability is a function of the\n  selected aggregation function *and* the Wasserstein embedding itself.\n  Assuming that the former is fixed to be the function described by\n  Togninalli et al., does the latter satisfy, for example, Lipschitz\n  continuity? (I am asking this out of professional curiosity;\n  understanding the inherent properties of the embedding seems key to me\n  for us to understand better ways to generate such embeddings; the\n  empirical performance of the proposed method speaks for itself, of\n  course!)\n\n- Section 3.2 is rather technical at present, making use of\n  hitherto-undefined concepts. I would suggest improving this section\n  by providing more intuition, relegating some of the more technical\n  results to the appendix.\n\n- Does the pseudo-invertibility of $\\phi$ cause any problems in\n  practice? It is my understanding that the embedding will not be\n  injective in general anyway, or am I mistaken?\n\n- How stable is the reference embedding? In Section 4.2, it is my\n  understanding that the reference embedding is by default obtained from\n  $k$-means, with $k$ being the average number of nodes. The appendix\n  depicts the results for another data-independent reference embedding\n  and states that there are no differences. Would this not suggest that\n  the way the reference distribution is obtained is irrelevant? If so,\n  why not use a normal distribution (which I would expect to be simpler\n  to calculate than a $k$-means embedding) for all experiments?\n\n  This point should be addressed somewhat better.\n\n- Why is the virtual node inclusion necessary? Only for the\n  simplification of the message passing itself? I am aware of this\n  standard modification, but I do not see why the proposed algorithm\n  could equally well work with the original graph.\n\n- Why is the variance of the proposed method high for some of the data\n  sets of the 'TUD' repository? Is this a consequence of the model that\n  was picked for working with the embeddings?\n\n# Style\n\nThis paper is well written; it was a pleasure to read and review. Here\nare some minor suggestions to improve style/clarity:\n\n- I would suggest so sort citations by some criterion (alphabetically,\n  for example) when citing multiple authors.\n\n- \"See the recent survey\" --> \"see the recent survey\"\n\n- \"such transport plan\" --> \"such a transport plan\"",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}