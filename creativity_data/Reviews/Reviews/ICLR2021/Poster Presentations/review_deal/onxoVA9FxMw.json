{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "With the advent of non-recurrent sequence-processing models, it has become costumary to augment input tokens with positional embeddings providing implicit positional information. Despite their potentially crucial role in modern architectures, such positional embeddings are rarely addressed in analytical studies. The current paper provides a systematic investigation of positional embeddings, characterized in terms of properties such as monotonicity, translation invariance, and symmetry. These properties are studies for different positional embeddings using language models fine-tuned on two separated benchmarks, with an emphasis on visual analysis.\n\nThe authors provided an impressive rebuttal, adding many of the experiments required by the reviewers. The latter are still somewhat split about the paper. I lean towards the positive side. I find that some of the criticism, while valid, is not really granting a rejection, especially after the authors' clarifications. In particular, one reviewer assumed that the authors claim that symmetry should be a property of an ideal positional embedding, whereas the authors are rather studying whether it is an important property of them, in light of the previous literature. Some claims about the results being \"interesting\" or \"surprising\" enough might depend on what the reader is looking for in the paper. I think that many readers in the \"black box NLP\" community will find the methods and analyses presented in this paper interesting and useful.\n"
    },
    "Reviews": [
        {
            "title": "Interesting Take on Comparing Position Embeddings",
            "review": "This paper proposes a formal framework to compare position embeddings (PEs) and presents an empirical study comparing variants of absolute position embeddings (APEs) and relative position embeddings (RPEs) on three properties: 1) monotonicity, 2) translation invariance, and 3) symmetry and evaluates their performance on classification (GLUE) and span prediction tasks (SQUAD). The authors also report results on learnable sinusoidal APEs and learnable sinusoidal RPEs, PE variants which had not been previously proposed. \n\nThe first three properties seem well-motivated (monotonicity and translation invariance), but it is not obvious that symmetry should be a property of an ideal PE, or at least the paper is not convincing on this front. In a sentence (ABCD), doesn’t the word A typically have a different relationship to B than B does to A?\n\nThe identical word probing test was a clever way to disentangle the impact of the word from that of the PEs. \n\nWhile it does seem valuable to more rigorously compare PEs as they are critical components of SOTA language models, the experimental results were not particularly convincing (e.g. although it’s a very appealing story, it didn’t seem so clear from the tables that APEs did better at classification and that RPEs did better at span prediction.)\n\nThe writing quality was borderline and there were a number of small errors:\n- fully-learnable APEs nearly meet all properties even under no constrains” -> “constraints”\n- Under the equation at the beginning of Section 3.1, “word-word correspondence” is repeated four times, which I am sure was not the intention. \n- nit: “since relative distance with the same offset will be embedded as a same embedding.” -> “the same embedding”\n- “compared to far-way” -> “faraway”\n- “attends more on forwarding tokens than backward tokens” -> “forward tokens”?\n- nit: “In Transformer, where attention calculation does not…” -> “where the attention calculation”\n- “allows PEs o better perceive word order” -> “to”",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review - AnonReviewer1",
            "review": "The paper presents a systematic analysis of approaches used to encode position information in transformers and in particular BERT-based models. The paper investigates absolute and relative position embedding strategies that use either fixed/learnable sinusoidal or fully learnable position embeddings. These embeddings are characterized based on different properties that are either inherent from their formulation or observed empirically such as monotonicity, translation invariance, and symmetry. Interestingly these properties appear to emerge naturally when having learnable parameters in APEs and RPEs.\n\nDifferent PE strategies are empirically validated by pre-training BERT with different PEs (including a combination of absolute and relative position representations) and fine-tuning on GLUE and SQuAD (1.1 and 2.0). Visualizations of the dot products between position vectors for different PE strategies are presented as well that demonstrate the monotonicity (or local monotonicity), symmetry, and translation invariance.\n\nOverall, the paper is well written, motivated, and systematically studies an important design decision in transformers. The overall methodology is sound and should prove useful to the community when studying position embeddings in transformers.\n\nStrengths\n\nThe paper is well written, well-motivated, and is systematic in its claims, experiments, and methodology.\nIt takes a good step forward in characterizing desirable properties of position embeddings and studying if they emerge directly from their parameterization or via training.\nIt studies a variety of position embedding strategies and their conjunction and experiments with fairly realistic models and benchmarks.\n\nWeaknesses\n\nWhile the analyses are well carried out, they are still specific to BERT-like masked language modeling training and it isn’t clear if PEs behave similarly in encoder-decoder like models for summarization/translation or decoder only language models.\nIt does not control for the choice of (pre)training objective, for example, if trained from scratch, purely as a retrieval model or on supervised text classification, would different learned PE patterns emerge?\n\nIt provides certain properties one can hope PEs to exhibit but doesn’t talk too much about circumstances under which these would be good inductive biases to have.\n\nQuestions & Comments:\n\nUnder what circumstances do properties like monotonicity in learnable PEs arise? Would PEs for an autoregressive variant also display similar properties? (would be interesting to see what happens with say a transformer LM trained from scratch on wikitext-103)\n\nIt isn’t quite clear to me at first glance why the preservation of the order of distances is necessary for position embeddings when used with models that build nonlinear functions of these embeddings.\n\nWould absolute position embeddings that are randomly initialized or all initialized orthogonal to one another (something like 1-hot vectors of dimension L) *and fixed*  work? This would be interesting to see (especially the latter) because it does not satisfy monotonicity or translation invariance and violates the desiderata in eq (1)\n\nIs Figure 7 based on BERT fine-tuned for NER on the CONLL/Ontonotes dataset?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting study of PEs",
            "review": "Updates after author responses and revisions:\n\nI was positive about this paper previously and am glad to see that the authors have done a great deal to try to respond to our concerns and strengthen the paper. I am more positive about the paper now and have increased my score to an 8. I think this paper is going to be useful for the community and I know I will reference it later and direct others to it who are interested in learning more about position embeddings in transformers (whether or not it actually gets published). \n\n--------------\n\nThis paper studies position embeddings (PEs) in transformers, suggesting a few reasonable formal properties of PEs and determining whether these properties are captured by various choices for defining PEs. The paper considers both absolute and relative PEs, and both fully-learnable and sinusoidal. A new variation is to learn frequencies in the sinusoidal PEs. Experiments are conducted by training BERT with various choices for PEs and evaluating on GLUE tasks and SQuAD. Fully-learnable absolute PEs, the default in BERT, works quite well overall, but adding learnable sinusoidal relative PEs as well can improve performance on average. There are also visualizations of PE dot products for the various methods, showing that learning PEs in the context of BERT training can yield PEs that satisfy the formal properties (for the most part) laid out by the authors. \n\nI really enjoyed reading this paper. PEs in BERT have been the subject of a lot of informal discussion over the past couple of years, but I don't think I've read a paper that studies the topic with such breadth and depth. With some doable improvements and clarifications, I think the paper can become an excellent resource for others interested in representing position and distance in transformer-like models. \n\nI like the idea of learned sinusoidal embeddings and think that idea can be potentially useful for other researchers. The paper notes that with the fixed sinusoidal embeddings, distances beyond about 50 are not distinguished, but with learning they can be. The experiments show that learning frequencies in sinusoidal PEs works better than using fixed sinusoidal PEs. I'm curious what the learned frequencies look like and how similar they are to the original frequencies chosen by the transformer authors. \n\nSome other thoughts I had while reading: \n\nAt least for language tasks with a given window, could we just re-use the learned frequencies and use fixed sinusoidal PEs in the future with those same learned frequencies? I guess a similar choice can be made with fully learned PEs. Do we really need to learn PEs from scratch every time? What aspects of the data or task would influence this? Maybe given their experience, the authors could hand-design useful general-purpose PEs? \n\nBased on the discussion in Sec. 5.3, maybe a good recommendation for BERT-like models would be to use a single learnable PE only for [CLS] and something else (e.g., learned sinusoidal APEs or RPEs) for other positions? This seems easy enough to do in practice and may bring the best of both worlds. \n\n\nBelow are some questions and points of confusion I had:\n\nIn the fully-learnable APE experiments described in Sec 4.2, for the first 10 epochs, were the unseen PEs randomly initialized and finetuned in the downstream SQuAD experiments? \n\nI'm confused as to why some of the visualizations in Fig 3 show white bands along the diagonal (d, f, and g). I would have expected all to have dark bands along the diagonal (as in b, c, d, and h). \n\nI was super confused by the identical word probing parts of the paper. Sec. 3.1 includes a sentence beginning with \"This shows that the selection of words\". The sentence is awkward and confusing to me. I don't know what the \"This\" is referring to. It seems like there should have been a result reported there for the \"This\" to refer to. The following sentence, starting with \"Namely\", is also confusing to me. The description of identical word probing then points to Sec 5.2, and the Sec 5.2 section title is \"IDENTICAL WORD PROBING\", but it seems to be focused on describing Fig 3, which has as its caption \"Dot products between relative position vectors\". Where are the identical word probing results actually reported? \n\nIn the \"Pre-training\" paragraph of Section 4, it is mentioned that a pretrained BERT checkpoint is used, but also that other BERT models with other position embeddings were trained. How was the pretrained model actually used? Was it used to initialize the other models or were all models trained from scratch?\n\nMinor things: \n\nIn the first equation in Sec 3.2, there are a couple of instances of K/2 which I think should be D/2 instead. Also, the vectors start with \\omega_1 but the summation at the end of the equation starts with \\omega_0. \n\nI'm confused by the bolding in Table 2. The best number in each column is not always in boldface (QNLI, WNLI, etc.). Also, sometimes when there is a tie for the best result, multiple numbers are in bold, while other times only one result is in bold. \n\n\nTypos:\n\np. 1: \"constrains\" --> \"constraints\"\n\nfootnote 4: \"seeing\" --> \"See\"\n\nSec. 4.1: \"outperform notably\" --> \"notably outperform\"\n\nSec. 5.3: \"PEs o\" --> \"PEs to\"\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An empirical study work that lacks convincing quantitative analysis",
            "review": "This paper studies the position embeddings of transform-based models, and proposed a unified position embedding evaluation method in three aspects, i.e., Monotonicity, Translation invariance and Symmetry, which can well summarise the properties of the existing position embedding methods.\n\n### Strengths of the paper\n\n1. It is good that the authors summarise three property for position embedding models, and discuss four related position embedding models under the three properties.\n2. The three properties proposed by this paper are suitable for most position embedding models.\n3. The experimental details are complete and easy to reproduce. Extensive experiment details are provided in the appendix.\n\n### Weaknesses of the paper\n\n1. The presentation and organization of this paper should be improved. Typos and language issues can be easily found, see the minor comments below. The contributions are not well highlighted in both the abstract and introduction section. \n\n2. The authors take many efforts to conduct experiments for the position embedding of BERTs, and provide an empirical study of the four existing position embedding models (fully learnable APEs (Gehring et al., 2017), (2) fixed sinusoidal APEs (Vaswani et al., 2017), (3) fully learnable RPEs (Shaw et al., 2018), and (4) fixed sinusoidal RPEs (Wei et al., 2019).). The author tested these models and their combinations over three benchmarking datasets, however, as an empirical study paper, the analysis is too weak.\n\n(1) For the qualitative result shown in Table 2 and Table 3, neither insight nor connection to three properties is provided for the result. In fact, the result shown in Section 4 is intuitive and not something new, since these are the basic motivations of the RPEs and learnable PEs. Moreover, the effectiveness of using both RPE and APE has already been validated in \"Self-Attention with Structural Position Representations, arXiv:1909.00383. 2019\".\n\n(2)  I would like to see, in the experiment part, some new experimental results and conclusions in terms of the four summarised properties, which are key contributions the authors claimed. However, what I see is just some position vector embedding similarities (i.e. Fig. 2 and Fig.3) in terms of the four position embedding models, where the results are also somehow expected and intuitive. The conclusions of the experimental results are too subjective. For example, from the analysis of Fig (2), the conclusion that : “Lastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.”  is a bit farfetched. In this figure the white part is as narrow as (-5,+5),  a further quantitative evidence in other forms for this conclusion will be more preferable. The same issue also exists in the conclusion of Figure (3): “This may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.”\n\n3. This paper lacks the discussion with the latest position embedding models such as,\n\n   [1] \"Learning to Encode Position for Transformer with Continuous Dynamical Model\". ICML. 2020\n\n   [2] \"Encoding Word Order in Complex Embeddings\",  ICLR. 2019, where the 'Translation' property is also discussed.\n\nOther minor comments:\n\n1. In the introduction section, \"distances in $\\mathbb{N}$ and $\\mathbb{R}^{D}$ \", $\\mathbb{N}$ and $\\mathbb{R}^{D}$ should be explained when they appear in the first place.\n\n2. The references should be formatted in a unified manner.\n\n3. Table 2, the bold indicators for the best performances are put on the wrong numbers, e.g. in QNLI, the bold should be 89.5,  in WNLI task it should be 51.3, and in STS-B it should be 87.5.\n\n   \nOverall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper.\n\n\n------\n### Comments after the discussion\n\nThank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}