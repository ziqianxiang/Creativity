{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Three reviewers recommended an acceptance (rating 7) while R1 deviated much from them (rating 3). After reading R1's concerns carefully and the authors' rebuttal, I found some of the criticisms to be invalid. The authors provided a satisfactory response, addressing concerns and clarifying potential misunderstandings. Because R1 did not update the review after the rebuttal period, I am assuming the concerns have been adequately addressed. The three other reviewers all unanimously agreed that this paper tackles a timely topic, proposes a simple and effective approach, and shows convincing empirical results. I concur with the reviewers' recommendations."
    },
    "Reviews": [
        {
            "title": "Nice problem to tackle, but questionable results",
            "review": "# Introduction\nFirst off, thank you (authors) for taking the time to put this together.\n\nThe problem the authors are trying to tackle is meaningful (+1) and relevant. Contrastive learning is in dire need of removing the dependency of using transforms. This paper goes one step and even tries to generalize it across domains which is desperately needed in the field.\n\n## Paper summary\nThe authors have taken an idea from supervised learning (MixUp) and derived the matching application for contrastive learning in hopes of removing the need for domain-specific augmentations.\n\nThey show how to apply their i-Mix to SimCLR, Moco and BYOL.\n\nFinally, authors show results for vision, tabular and speech data using standard datasets for each.\n\nResults have a few sections:\n1. Evaluate cifar-10, cifar-100, Commands and CovType using Moco and BYOL in addition to i-Mix.\n2. Evaluate larger datasets, imagenet and Higgs.\n3. Evaluate how depth of a resnet and length of training are affected with i-Mix.\n4. They show ablations with models trained without their standard data augmentations and i-Mix.\n5. They evaluate transfer learning on vision (cifar-10 <=> cifar-100) and (Imagenet -> VOC detection)\n\n## Strong points.\n\n1. The overall goal they are looking at is impactful and critical.\n2. They show results on small and large datasets.\n3. They are attempting 3 domains with the same method.\n4. Their most promising result is Table 3 which shows a nice gap between models trained without data augmentation but with i-Mix instead.\n\n## Weak points\nTheir general results are not significant enough. Most results look like this (82.5 vs 82.7).\nThis hints at these results being achieved largely via hyperparameter tuning, instead of meaningfully providing an advantage.\n\n## Suggestions for improvement\n1. Show distributions for results instead of a single number. (ie: not 82.5 vs 82.7, but instead run 50-100 times and plot the histogram)\n2. Many of the ideas mentioned here are also mentioned in non-cited relevant work (for example: [YADIM](https://arxiv.org/abs/2009.00104)).\n3. Provide code. Since some of these claims come down to implementation details, it's important to see the code as well.\n\n## Recommendation\nReject",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Timely topic, simple and effective method, would benefit from a more few baselines",
            "review": "Summary:\n\nThe key idea of this paper is to apply MixUp-style regularization to self-supervised contrastive learning techniques (SimCLR, MoCo-v2, BYOL). This is combined with another form of MixUp that involves only the images (not the labels), but the precise nature of this component is unclear. For large networks trained on small datasets, the proposed method improves downstream classification performance by reducing overfitting (Table 1, Figure 2). The results are mixed for large-scale datasets (Table 2, Figure 3). The proposed method is also investigated as a potential domain-agnostic augmentation. Though much better than not using any augmentations at all, the results are not typically better than using standard augmentations (Table 3).\n\nStrengths:\n\nThe paper is generally clear and the figures are well-made, though the presentation could be improved here and there (see minor comments below). The question of regularizing self-supervised representation learning is an interesting one - most contrastive learning papers use such large datasets that overfitting isn't really an issue, so that topic is novel as far as I know. The proposed method is simple and effective, clearly improving the performance of several contrastive learning techniques in the regime where overfitting is an issue.\n\nWeaknesses:\n\nThe paper proposes a regularization method for contrastive learning and demonstrates that it can help to reduce overfitting. However, it has not been demonstrated that the proposed method is unusual in this regard. Perhaps simpler, more traditional regularization methods (e.g. dropout, weight decay) would have the same effect.\n\nAs far as I can tell, the proposed method actually has two components - i-Mix and InputMix - whose effects have not been separated by an ablation study.\n\nRegarding the observation that \"i-Mix + linear classifier\" can outperform supervised baselines (Section 4.2) - it would be interesting to know whether this is still true when those supervised baselines also get to benefit from a regularization technique like MixUp. It seems like that might be a more fair comparison.\n\nOverall:\n\nThe role of overfitting in contrastive self-supervised learning is an interesting and timely topic, and the proposed method is simple, effective, and general. However, additional baselines would make the importance of this particular regularization technique more apparent.\n\nMinor comments:\n\nSection 3.2: Perhaps $v_i \\in [0,1]^N$ should be $v_i \\in \\{0,1\\}^N$.\n\nSection 3.3: \n\n\"...we propose to apply InputMix together with i-Mix, which mixes input data but not their labels\" - this a somewhat confusing phrase, since the description could be read as applying to either InputMix or i-Mix. \n\nWhat does \"with the largest mixing coefficient $\\lambda$\" mean? Do we sample $\\lambda$ first, then sample another mixing coefficient on $[0, \\lambda]$?\n\nThis section is the only one where InputMix is mentioned - are we to assume that it is used in all cases where i-Mix is used?\n\nSection 4.0: It might be helpful to point out that $\\mathrm{Beta}(1,1)$ is the uniform distribution (i.e. the standard setting for MixUp). Is there any intuition behind changing the distribution of $\\lambda$ for tabular data?\n\nSection 4.1: \n\nAre the 32x32 images resized for input to the network? Or is the network modified at all for the small images?\n\nSection 4.3: \n\nIt would be nice to see experiments of this kind on ImageNet, which would allow the enormous models and longer training times to be balanced out by a suitably sized pretraining dataset.\n\nIn Figure 3, the caption says \"MoCo\" but the legend says \"N-pair\" - not sure which it should be. Also, consider including \"10% of ImageNet\" information in the caption. \n\nSection 4.3: How is this 10% ImageNet dataset constructed?\n\nSection 4.4:\n\nIt would also be helpful to explicitly compare the \"no augmentation + i-Mix\" performance against supervised numbers so the reader can easily tell whether i-Mix by itself results in a strong pretraining strategy or not.\n\nAll sections:\n\nI recommend replacing $\\beta(\\alpha, \\alpha)$ with $\\mathrm{Beta}(\\alpha, \\alpha)$ wherever it occurs in the paper.\n\nThe experiments can be a bit \"patchy\" - different self-supervised methods used for different experiments on different datasets. If possible, consider fleshing out e.g. Table 2 to cover more of the dataset/method combinations.  \n\nI would write \"MoCov2\" instead of \"MoCo\" so people don't get the wrong impression if they're just glancing through the paper. \n\n**Update: As noted elsewhere in the discussion, the authors have addressed my primary concern. I will therefore increase my score from a 6 to a 7.**",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong empirical results, but a few clarifications are needed.",
            "review": "In essence, the premise and contribution of the paper is rather simple -- contrastive learningÂ and mixup are both very effective methods in their respective domains, and this paper proposes the use of mixup to improve contrastive learning, and the results seem to indicate this.\n\nPositives:\n- Strong empirical results, validated on a rather wide range of settings, including dataset type (image, audio, or tabular) as well as various contrastive techniques (SimCLR, BYOL, MoCO), and ablations (dataset size and model capacity).\n\nSuggestions / negatives:\n\n- (1) I see that you experimented with CutMix [3] and that you reported some results in the supplementary material. Can you comment further on this? (Also, see (3)). I am also concerned with the potential of vanilla mixup to 'underfit' (see [1]), though perhaps this would be mitigated via careful tuning of the mixing distribution parameters 'alpha' (again, see (3)).\n- (2) Please include measures of uncertainty in your results. This is lacking in all of your tables. You mention the results are an average of up to 5 runs, but the variance should still be stated.\n- (3) Did you do any HP tuning on the alpha for Beta(alpha, alpha)? Section 4 intro says that it was fixed to 1.0 for image datasets and 2.0 for tabular. Was this the result of HP tuning? And if so, can you comment on if the choice of alpha made a big difference?\n\nExtra suggestions / comments:\n- (4) It would be nice to test the effect of mixup without modifying the contrastive learning algorithm, that way you can examine its effects purely as a data augmentation scheme. It turns out that if you use Beta(alpha, alpha+1) as your mixing distribution, it is equivalent in expectation to the mixup loss where labels are *not* mixed. This applies to the categorical cross-entropy and binary cross-entropy loss functions. See [2] for the proof of this. This means that you could simply take the algorithm you have already, make the sampling distribution Beta(alpha, alpha+1) (rather than Beta(alpha,alpha)), and then simply use CCE(L, Y) as the loss, where you are *not* mixing Y's. I'm curious to know how this works, since you would be able to test mixup in isolation without having to modify the contrastive algorithm itself. Also, note that this particular parameterisation of the mixing distribution is Beta(alpha,alpha+1) rather than Beta(alpha, alpha), and maybe it is worth seeing if it either one performs better than the other.\n\nIn summary, I think what you have here is good, but you should add measures of uncertainty to your results and also clarify how you performed the hyperparameter tuning. I'd be very interested if you address (4) as well. My rating is a 6, though I open to raising the score if my concerns have been addressed.\n\nReferences:\n\n- [1]:Â Guo, H., Mao, Y., & Zhang, R. (2019, July). Mixup as locally linear out-of-manifold regularization. InÂ Proceedings of the AAAI Conference on Artificial IntelligenceÂ (Vol. 33, pp. 3714-3722).\n- [2]:Â HuszÃ¡r, F. (2017). Mixup: data-dependent data augmentation. Blog post:Â https://www.inference.vc/mixup-data-dependent-data-augmentation/\n- [3]: Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., & Yoo, Y. (2019). Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE International Conference on Computer Vision (pp. 6023-6032).",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Mixup + contrastive learning yields great results",
            "review": "Summary: The authors present a way of framing contrastive self-supervised learning techniques that enables them to use MixUp. Results indicate this helps especially on small datasets, and in domains where the right set of data augmentations is unknown.\n\nPros:\n+ The proposed approach for adapting the effective Mix-Up strategy to contrastive learning is simple. At the same time it is elegant in its generality: it improves all self-supervised learning techniques. There have been very few such general-purpose innovations in self-supervised learning since the original instance discrimination work that started the current trend.\n+ The fact that iMix helps a lot when domain-specific data augmentations are unknown is a very significant result, since it opens up self-supervised learning to a lot of new domains.\n+ I find the experimental evaluation quite careful and detailed. I especially like that the authors went beyond images as a domain; the self-supervised learning community has been overfitting to imagenet. \n\nCons: TBH I can't think of any. This clearly seems to be a worthwhile extension to the world of self-supervised techniques, especially relevant for new domains with limited data.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}