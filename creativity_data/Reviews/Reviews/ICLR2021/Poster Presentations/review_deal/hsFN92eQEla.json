{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper questions the use of cross-entropy loss for classification tasks and shows that using squared error loss can work just as well for deep neural networks. The authors conduct extensive experiments across ASR, NLP, and CV tasks. Comparing cross-entropy to squared error loss is certainly not novel, but the conclusions of the paper, backed by a lot of experimental evidence, are certainly thought-provoking. \n\nI would have liked to see a bit more analysis into the results of the paper, and perhaps a bit more theoretical justification. That said, the paper will be of interest to the community, given the ubiquity of classification tasks.\n"
    },
    "Reviews": [
        {
            "title": "The paper shows that the square loss is usually as effective as the cross-entropy loss across classification tasks in a variety of domains and model architectures.",
            "review": "This paper questions the omnipresence of cross-entropy loss for classification tasks while showing that square loss also yields competitive results. The experimental section spans a wide variety of tasks and architectures covering NLP, ASR, and Vision. The authors find that the model performance is more stable w.r.t. the random initialization of parameters when trained using the square loss. For NLP and ASR datasets, the authors find that the square loss yields slightly better results. The authors also report that the square loss usually takes more time to converge and requires rescaling with a larger number of classes. \nThe authors do not dwell on the fundamental reasons behind the observations made in this paper. However, I believe that these observations are indeed useful to advance further research for better loss functions. \n\nStrong Points:\n1. Diverse experiments and insightful results\n\nWeak Points:\n1. The paper does not make an effort to provide well-grounded explanations for the experimental observations. \n2. It is unclear how the CTC-based architectures for ASR were modified for square-loss. An explanation in the paper would be helpful. \n\nQuestions:\n 1. How were the CTC-based architectures for ASR were modified for square-loss. ?\n 2. What is the opinion of authors about the trade-off between the hyperparameter associated with the loss-scaling Vs. the simplicity of the cross-entropy loss for a larger number of classes?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Important work towards understanding loss function choice",
            "review": "Summary: This paper compares the more popular cross entropy loss function to the squared loss function for classification tasks. The authors look at NLP, ASR and and CV tasks, keeping the architecture the same (as much as possible) and varying the loss functions. The authors demonstrate that although cross entropy is more commonly used in state-of-the-art architectures and is standard across tutorial code, square loss functions are often times advantageous. \n\nStrengths: \nThis paper highlights an ongoing issue in deep learning -- we often take for granted what 'best practices' are and do not sufficiently investigate the best loss function, etc. when starting a new problem. \n\nThe study covers a reasonable amount of types of problems and architectures and conducts 5 random initializations which is rare in many ML papers. \n\nWeaknesses:\nWhile this work is highly important and the study was well-done, the novelty is a little low. Many papers have done variations of this same type of work to answer this question. \n\nWhile I understand there are only so many experiments that a group can do, there are some limitations to this study -- including not performing hyper-parameterization for architectures comparing the two loss functions which may reveal some important distinctions and use-cases. \n\nA minor comment is that the average accuracies are reported but not standard deviation to get a sense of the variation across performance for both loss functions. Statistical significance calculations would also be helpful to interpret results. \n\nQuestion for authors:\nWhat other areas do you find people are using the wrong hyper-parameter choice? This could be an interesting discussion write-up for researchers to re-consider how they select their training paradigm. \n\nI recommend accepting this work. The study was well-done for answering this question and more thorough than related work. My reason for not giving a higher score is that the work is not highly novel. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical Evaluation of Cross Entropy vs Square Loss ",
            "review": "The paper compares cross-entropy and squared losses on a wide range of tasks. The focus primarily is on thorough empirical evaluation of these two losses on NLP, Vision and Speech tasks. The paper shows that squared loss is better or competitive with cross-entropy loss in most cases. Most of the experiments and comparisons seem to be well done; parameters, setups etc. are well explained. \n\nI believe a few additional tasks and settings would have helped put a better picture of the comparison. \nFor speech, the application of the two losses in tasks beyond ASR might give more insights. Similar for vision, classification tasks other than image classification (on MNIST/CIFAR and Imagenet) might be useful. \n\nAdditional learning settings might also be useful. For example, there is considerable amount of work (e.g R1 and R2 below) on noisy label learning, where the loss function (often cross entropy losses) is modified for noisy label condition. What do we expect for these two losses  in this noisy label setting ? Essentially what can we expect in learning settings beyond the supervised training in vanilla form. \n\nFor square loss, scaling is done for a large number of classes. The motivation behind it is not very clear. Also, it is perhaps worth showing results for squared loss with softmax outputs. Especially for the large number of classes. Can softmax be advantageous in this case for performance, even though computationally it will be worse than the scaling mechanism applied. \n\nR1. Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\nR2. Normalized Loss Functions for Deep Learning with Noisy Labels\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper that challenges the conventional wisdom of CE loss being superior to MSE loss in clasffication tasks. ",
            "review": "\nI think this a very good contribution to ICLR given the topic and the quality of the submission (originality, contribution to the stare of the art, experimental evidence, etc) \n\n Some of the strong points of the submission are summarized as follows:\n\n1.\tThe paper tackles a very interesting subject, questioning the conventional wisdom the CE is superior to MSE loss in a wide range of machine learning problems. \n2.\tVery good introduction and motivations sections. The hypothesis, as well as the main motivations are discussed succinctly but in a very logical manner including historical aspects leading to the current state of affairs (in terms of the manner in which models are trained) that might be helpful for interested readers not sufficiently familiar with the aspects discussed in the article.\n3.\tThe state of the art (despite the previous comment) contextualizes the subject matter in a succinct but comprehensive manner. \n4.\tI have read people making similar claims in other forums and articles, but the authors here provide a very thorough and careful experimental design, which helps to validate their hypothesis. However, it would be interesting to see how these experiments generalize to problems (in particular in computer vision) where datasets are noisier or where the image quality/resolution are lower.\n5.\tThe experimental design is good, showing a careful analysis to validate the proposal and several ablation studies to confirm that the hypothesis holds for various machine learning domains, as well as several datasets.\n6.\tThe foundations for the method are presented in great detail in a formalized manner and provides sufficient to assess the validity of the proposed experimental design.\n\nHowever, there are certain things that in my opinion could be improved:\n\n1.\tFuture work could be further elaborated and discussion in specific domains (medical imaging, for instance) could be further discussed.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}