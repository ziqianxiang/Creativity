{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "In this paper, a method to solve the segmentation problem by continuous optimization is proposed by using a soft differentiable warping function. The proposed method is theoretically sound, and interesting experiments such as the data analysis of covid19 are also presented. This is a good paper in terms of both theory and application."
    },
    "Reviews": [
        {
            "title": "Novel method, interesting experiments, easy to read",
            "review": "The proposed paper introduces a novel approach for the segmentation of sequences. The proposed method is based on two-sided power distributions (TSP) that are mathematically well-define and enable differentiability. The main goal of the method is to jointly optimize model parameters, including the segmentation function. The method is validated through experiments on modeling the spread of COVID-19 based on Poisson regression, change point detection, a classification model with concept drift (insect stream benchmark), and discrete representation learning (speech signal). \n\nOverall, I have the impression that this is in interesting paper that has most things going for it. Replacing a hard segmentation function with a soft differentiable warping function (two-sided power distributions) seems technically-sound and is an interesting novel solution to a difficult problem. The paper is mostly well-written and easy to follow. Furthermore, the experiments are well-defined and novel datasets (COVID, insect stream benchmark) were used to validate the effectiveness of the model. Therefore, I am leaning toward accepting this work to ICLR 2021. \n\nAdditional comments: \n\n- The abstract is somewhat difficult to comprehend and appears more cryptic than necessary. \n- In Section 3.2. it is not clear what is meant by levels 0 and 1. \n- Section 4: It is not clear what 'bogus mode' refers to.\n- To better understand the model in Figure 3 (and given the white space around the equations) it would be helpful to provide  labels for the introduced variables. \n- Please add details on whether the RKI data is publicly available. \n- Currently the paper does not discuss any limitations. To further understand the introduced model it would be helpful to highlight corner cases across the experiments in which the model does not perform well. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "[summary]\nThe paper proposes a relaxed way to solve the segmentation of sequence that can directly leverage the deep learning architectures. The relaxed model allows each segmentation parameter to be a linear interpolations between two consecutive parameters depends on a continuous warping function. The paper then proposes to use mixture of TSP distribution to simulate a step-like warping function and perform a thorough empirical comparison results with different methods and different warping functions. It turns out that TSP-based methods consistently achieve good performance.\n\n[novelty]\nTo be honest I am not familiar with this area. It seems that this is a very novel method and may have impact across the area.\n\n[significance]\nThe algorithm is very easy to understand and simple to implement, which may be very easy to reproduce by the community. The segmentation problem itself, especially the COVID19 case, show the importance of this area and may attract further attention from even outside the community.\n\n[clarity]\nI enjoy reading the paper and find it very easy to follow. The experimental results are clear and detailed.\n\n[some further questions]\nI'm curious for the relaxed models in equation (4). I didn't see why (4) should be very general form of relaxation. Why is it important to only interpolate consecutive two parameters? Is is possible to rewrite (4) into a weighted average $\\sum_k w_{k,t} \\theta_k$ and we hope so that $w_{k,t}$ depend on a continuous function, similar to $\\zeta_t$, and index $k$?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Elegant differential segmentation representation; Limited evaluation",
            "review": "The paper describes the use of two-sided power functions for differentiable approximation of segmentation in discrete sequences with monotonic segmentation (each event in the sequence is defined by one continuous interval). The authors show that their particular parametrization allows to control exactly the length, starting, and ending point of the interval, and the slope of change (Fig. 2). \nThe main result of the paper is the ability to pose segmentation and segment estimation problem jointly and differentiably without assumptions on the model that produced the segment data, which is an improvement over several recent works with could also pose the problem jointly, but made such assumptions, such as van den Burg & Williams, 2020, and Arlot et al., 2019. \n\nThe authors show results on several tasks: modelling of COVID-19, change point detection on synthetic dataset from Arlot et al, 2019, and concept drift dataset. The results on change point detection and concept drift dataset are convincing, showing better quality of segmentation than previous approaches. The results on COVID-19 are, though, rather qualitative and serve more as an ablation study as there aren't many different results on this datasets.\n\nMy biggest issue is the absence of proper evaluation on harder tasks, where the estimation problem is solved with the help of a neural network, as opposed to something else, as in the previous 3 datasets.\nAuthors show results on speech segmentation without any comparison to other approaches except for random segmentation (with a comment that \"random segmentation is worse in 9932 out of 10000 trials (99.32%)\") in the \"eval-timit colab\", which is hardly a fair comparison). There is a number of baseline techniques and datasets for segmentation that the authors could compare to. For example, one way of extracting segmentation in OCR / Speech is by looking at logits for \"empty class\" coming from the decoder and it would be interesting to compare such segmentation with a task of segmentation + character recognition formulated in this framework. Another group of approaches, identified by the authors themselves, uses discrete latent space representation for segmentation, in the domain of Speech. Finally, in the domain of action segmentation  there are similar in nature approaches also predicting a differentiable segmentation, but without explicit parametrization, such as in this work (eg. [Fast Weakly Supervised Action Segmentation Using Mutual Consistency])\n\nOne other question is regarding section 5.3 and Fig. 6. It seems that increasing the number of segments up to 32 still produces improvements in accuracy, so I would be interested in seeing how the model behaves with even larger K. (I expect that starting at some value of K overfitting to small segments should start reducing the accuracy?)\n\nI believe paper could be strengthened by evaluation on harder tasks and comparison with other types of methods. That being said, I find the described approach valuable in and of itself, hence the rating.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}