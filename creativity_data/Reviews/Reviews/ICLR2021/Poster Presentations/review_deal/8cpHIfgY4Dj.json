{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Meta-learning for offline RL is an understudied topic with lots of potential impact in the research community. This paper takes the first stab against that challenging problem by proposing a solution similar based on PEARL and distance metric learning. The results look good and it seems like the authors have addressed some of the concerns raised by the reviewers. As a result, I suggest to accept this paper.\n\nHowever, this paper still has some shortcomings as reviewers suggested more baselines with more experiments on standardized benchmarks such as D4RL or RL Unplugged could make the paper stronger."
    },
    "Reviews": [
        {
            "title": "Proposes novel algorithm for offline meta-reinforcement learning",
            "review": "Summary : \nThe paper studies meta-reinforcement learning in the fully offline setting, and proposes a novel algorithm 'FOCAL'. Given offline datasets for tasks sampled from some prior, the algorithm learns a context encoder using distance-based metrics. The encoder is used for inferring the task-latent $z$, which is used to condition the policy rollout $\\pi(a | s, z)$. They demonstrate experiments where FOCAL outperforms baselines like PEARL.\n\nReasons for score :\nWhile I concur with the motivations of the paper, I vote for rejecting the paper (marginally below acceptance threshold). My main concern is the limited novelty of the proposed solution and some missing ablations (c.f weaknesses). I encourage the authors to incorporate feedback for the reviewers and work towards a stronger submission.\n\nStrengths:\n+ The paper is well written and quite easy to follow. The problem is sufficiently well motivated, and the algorithm builds on previous approaches, particularly PEARL/BRAC.\n+ The proposed algorithm seems to outperform reasonable baselines on few navigation/Mujoco tasks in the offline setting.\n\nWeaknesses:\n- As the authors identify, the assumption of task-identifiability from any transition (s, a) seems infeasible in a practical setting. This significantly reduces the scope of the algorithm. The novelty of the proposed algorithm is also quite limited it builds on the framework of PEARL by using (i) distance-based metrics for task inference, which have been studied in the multi-task literature (ii) offline policy learning, where it uses off-the-shelf baselines from the offlineRL literature.\n- The authors include few ablations, I believe adding one/more of the following would improve the quality of discussion:\n       (i) Evaluating how well the context-encoder performs in inferring the meta-test tasks? \n       (ii) Ablating the choice of encoder : Comparing the performance of BatchPEARL, where the encoder is still learned with Bellman gradients, while the policy is behaviorally regularized. I consider this to be a stronger baseline than the existing version of BatchPEARL.\n- Experiments pertaining to 5.2.2 in the ablations are not well-motivated. Given the contrastive loss definition in equation(13), it follows that the “inverse-power” losses learn reasonable task embeddings. In my understanding, n>0 here invalidates the choice of “linear”/”square” objectives. I’d be happy to be corrected if I misunderstood the setup. \n- The authors are encouraged to add details sufficient to reproduce the results. In particular, the setup of the baseline “ContextualBCQ” is scant on details.\n\nMiscellaneous:\n- The paper has several minor grammatical errors which could be corrected in the final draft.\n- I would encourage the authors to add more discussion to the experiments section. The current version reflects the empirical results, but provides little insight into them. Last half of section 5.1 has some mention on the importance of choice of datasets, but doesn’t provide details (until in the Appendix).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Thanks for the feedback. However, I think most of the performance gain came from behavior regularization, which is an already existing technique, rather than the proposed metric learning method (Figure3 and Figure 5). I’ll keep my ratings unchanged.\n\n============\n\nSummary: \nThe paper presents a mode-free, end-to-end offline meta-reinforcement learning algorithm.\nThe authors proposed the context encoder to encode the task from the history of transitions (called the “context”) using metric-based approaches. Specifically, they embedded the context to a latent vector z by clustering similar data points (context) taken from the same task while pushing away dissimilar data points taken from different tasks. The latent context variable z is used to condition the actor and the critic.\nExperiments are performed on the MuJoCo simulator and the 2D navigation problem with a sparse reward called Sparse-Point-Robot, using different reward functions depending on the task. The method proposed, FOCAL (Fully-Offline Context-based Actor-critic meta-rL algorithm), outperforms other offline meta-RL algorithms.\nThe main contribution of the paper comes from combining the context-based approach with metric learning.\n \nPros: \n- Overall, the paper is well written. The method of using metric learning has been well-motivated by toy experiments on the context embedding. \n- The paper tackles offline reinforcement learning and meta-learning, which are emerging topics in RL/ML.\n \nCons:\n- Format: The paper violated the paper length limit of 8 pages for the main context. \n- Missing experiments: The paper directly followed the algorithm and the experiment setting of “Efficient Off-Policy Meta-RL” (Rakelly et al., 2019) while omitting experiments on the MuJoCo walker and humanoid environments. Also, for the walker-2d experiment, only the toy experiment on context embedding is available with no return curve.\n- Ablations: The authors compare their method (FOCAL) to Batch PEARL while describing the Batch PEARL as “a vanilla version of FOCAL without behavior regularization or distance metric learning.” It would be interesting to compare FOCAL and Batch PEARL both with the behavior regularization to observe the effect of metric learning in isolation. \n- Novelty: The algorithm in Appendix A seems incremental to the algorithm in “Efficient Off-Policy Meta-RL” (Rakelly et al., 2019). The only difference is that the setting is offline and using metric learning to encode the context instead of KL-divergence. Although the combination of context encoding and metric learning is interesting, the proposed is method is still incremental. \n\n\nMinor comments:\n- There was no source code when I followed the Github link provided in the paper.\n- In the return curves on Half-Cheetah-Vel and Ant-Fwd-Back in Figure 3, there is no value near 1e6 sample steps, where the deterioration of performance begins.\n- I understood that all tasks share the same state and action space, with different transition and reward functions. But all the experiments in the paper use the same transition. I wonder if there is an experiment with a different transition depending on the task.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper tackles the problem of offline meta reinforcement learning, where an agent aims to learn a policy which can adapt to an unseen task (dynamics/reward), but from entirely offline data. As a result of being fully offline, the agent can no longer explore in the new task at test time, but instead receives randomly sampled transitions from the new task, from which it must infer the task. They then propose a method for learning task inference from fully offline data, as well as a policy conditioned on this task encoding from offline data built off behavior regularized actor critic (BRAC). Results indicate that in this outperforms PEARL as well as multi-task offline RL with BCQ.  \n\nPros:\nThe problem this paper tackles is important, and is formulated in a practical way. While in general meta-RL is often closely tied to online exploration to infer what the task is, in practice it seems like having an agent be able to adapt to a new task given just a few transitions from a potentially different policy would be a valuable instantiation of meta-RL. It also integrates nicely with progress in batch RL. \n\nSecond, the idea of decoupling the task inference objective using the metric loss does nicely integrate with the setting of using an offline dataset, and seems to work well as shown by the qualitative examples. While it does depend heavily on having access to the task labels during training, this assumption is used in a lot of meta-RL work. Additionally, the proposed negative power variant of the metric loss does seem to better encourage separation of tasks with different task labels, and seems like a good tool for metric learning in general.\n\nLastly, the experiments do compare against the relevant baselines to the best of my knowledge, and on tasks similar to those used in prior works. The ablations are also interesting, and make a compelling case for why in the offline case deterministic context encoders may be more fitting (when the tasks satisfy Assumption 1).\n\nCons:\nIn general I think the key assumption in this work, that tasks can be inferred from an unordered set of transitions is limiting, especially for tasks with sparse reward, or which involve adapting to changing dynamics. However, this does seem like it is the best that one can do while being totally offline, any more complex tasks would require some form of online exploration.\n\nI'm not convinced of the argument in Appendix C, that the reason why using the control policy to learn the context encoder works poorly is due to neural networks inability to tell apart small differences in the embedding vectors. Rather I think the decoupled training allows for an easier optimization of the task embedding, and then the policy can be learned using good (and stationary) task embeddings throughout all training. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}