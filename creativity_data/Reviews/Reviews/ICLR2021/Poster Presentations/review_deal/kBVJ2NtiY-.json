{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "First as a procedural point, the paper got 7, 7, 5, 5. AnonReviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score. They did not do so, but I must interpret their last messages as indicating they now support the paper. AnonReviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal. They did not update their score, but were happy to leave their certainty low and defer to other reviewers' recommendation. As such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews.\n\nThe paper adapts a method from tabular RL to Deep RL, allowing (as the title aptly says), agents to learn What to do by simulating the past. Reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method. It is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, I am happy to go with the consensus and recommend acceptance."
    },
    "Reviews": [
        {
            "title": "An original setting, and an interesting and promising approach.",
            "review": "This paper introduces an imitation learning algorithm that can take a single state from an expert's trajectory in order to infer the goal of the task. The idea is to train a reward function that explains both the past trajectory and the futur trajectory from that state, assuming that the very goal was that state (hence, the assumption of larger rewards in the past than in the future induced by the gradient). \n\nThe idea makes sens and is well explained. The results are encouraging and support the method.\n\nI also believe this work could be impactful in the explanation domain, as it provides an answer to the question \"why this state?\". Therefore, outside simple imitation, it could even apply to self-imitation or credit-assigment in direct RL settings. The only weakness would be the required heaviness of the implementation: one need to implement and train all the models (direct and inverse dynamics and policies, auto-encoders for features etc).\n\nHowever, I would ask for some clarifications:\n- What idea that could help the reader to understanding equation (2) (without going through the appendix of the cited paper)?\n- In equation (3), is \\tau' independent of \\tau_-T:-1, so the right-hand term (in red) can be moved outside the blue expectation? (so the simple difference in the last line of the algorithm would nicely appear?) \n- What is encoded and decoded by VAE? (I guess the states, but this is not explicitly told)\n\nAlso, I would have expected a discussion regarding the environmental limitation of this approach, for example, when a state can be completely misleading, or at least not providing any information (for example, the initial state of an environment).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studies the question of learning rewards given only certain preferred or terminal states. I found the setting of the problem to be interesting but not clearly motivated or explained. For this reason, I am unable to recommend acceptance at this stage, due to major clarity issues. I am open to revisiting the recommendation based on author feedback.\n\n(1) Can you explain the assumptions and setting a bit better? Do we only have access to terminal states from execution traces of an expert/human policy? I assume additional interactions with the environment are allowed?\n\n(2) The derivation for equation (1) is unclear to me. I understand that $P(s_0 | \\theta) = \\sum_{\\tau \\in \\Omega} P(\\tau | \\theta)$ where $\\Omega$ are the subset of trajectories that terminate in state $s_0$. Since the probability is a sum of probabilities (instead of product), why does the gradient of log probability decompose into a linear combination of the trajectory gradients?\n\n(3) The writing and notations are quite confusing and hard to follow. In particular, the negative time indexing makes a number of expressions counter intuitive. For example, let us take the expression in section 2.2:\n- What are the contents in $\\ldots$? Can you expand the equation further?\n- Why is $P(a_t | s_{t+1}, a_{t+1}, \\ldots s_0, \\theta) = P(a_t | s_{t+1}, \\theta)$? From a causality viewpoint, this seems counter intuitive since a future state should not influence current action. Is it actually a typo? If this instead has a filtering style interpretation, then shouldn't the decomposition be $P(a_t | s_t, s_{t+1}, \\theta)$ and $P(s_t | s_{t+1}, \\theta)$?\n- It might be easier for parsing and understanding to have the product be $\\prod_{t=-T}^{t=-1}$ instead of $\\prod_{t=-1}^{t=-T}$.\n\n(4) Based on an educated guess of the problem setting (per my understanding), an intuitive and perhaps simpler algorithm would be to learn a goal classifier using the provided {s_0} data, i.e. $P(s=\\text{goal})$, as well as a forward dynamics model $P(s_{t+1}|s_t,a_t)$. Then, one can use the goal classifier as a reward for planning. Would this be an applicable algorithm for this setting? Is this related to the GAIL baseline?\n\nOverall, I was unable to understand the problem formulation and setup, which makes it hard to appreciate the experiments. If the authors can better motivate the setting and improve writing clarity and notations during the rebuttal revision, I am happy to revise my score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "The paper considers an approach for reward learning from a single frame which was developed for tabular environments and explicit dynamic programming, and extends it to more complex environments through deep RL. \n\nMethodologically, the paper stays close to the ideas of RLSP, but instead of computing relevant quantities (optimal policy, forward and inverse dynamics) through explicit derivations, it suggests most exact steps can be replaced by leveraging deep learning, reinforcement learning and self-supervised learning. There are some minor degeneracy issues stemming from the extension (in particular in gridworld environment) which the authors can mostly solve.\n\nWhile the paper does not have strong methodological novelty, it is well written, the approach is sensible and combines well with state of the art deep RL, and the results are certainly interesting. The authors do a good job providing ablations (some ablations working surprisingly well maybe suggests the methods is working for slightly different reasons than we may assume). \n\n\n\nQuestions: \n- How is SAC(theta) computed? Is it a policy with its own parameters?\n- The inverse dynamics model is a function of the current policy, which is changing over time. This usually causes 'tracking/lagging' issues, and here especially so because the inverse dynamics is presumably a very nonlinear function of the policy. Did you observe any such issues?\n\n\n\n\n\n[1] Reward Learning by Simulating the Past (RLSP)",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Cool idea but a bit limited in a number of ways. I did not find the empirical results fully convincing. ",
            "review": "This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. An example of this described in the paper is an environment with a vase: if demonstration data shows an intact vase in the presence of an embodied agent then breaking the vase is unlikely to be the intended behavior. Otherwise the vase would already be broken in the demo.\n\nTo achieve this, the paper assumes a Boltzmann distribution on the demonstration policy and a reward function that is linear in some pre-trained state features. The paper then derives a gradient of the log probability of a demonstration state. The gradient estimator involves simulating a possible past from a demonstration state (using a learned inverse policy and inverse transition function) and then simulating forward from the possible past (using the policy and a simulator) The gradient is then the difference between features counts from the backward and forward simulations. \n\nThe paper is generally clearly written and works on a crucial problem in reinforcement learning, namely how to specify a human preference without resorting to tedious reward engineering. Novel, scalable approaches to this problem would certainly be of interest to the ICLR community. The primary technical contribution of the paper is the derivation of the gradient estimator which is correct. \n\nI find the idea of the paper very interesting and the results showing meaningful behavior emerge from a single demonstration are quite nice. However I think the paper is limited in a number of ways:\n- It requires access to a pretrained state representation\n- It requires access to a simulator of the environment which requires being able to reset the environment to arbitrary states. This seems quite limiting for real world applications. Worryingly, appendix D states that learning a dynamics model was attempted by the authors but failed to yield good results.\n- I think the choice of evaluation environments is a little odd and simplistic. I think environments more aligned with the eventual application areas for a method such as Deep RLSP would make the paper much more compelling. Given the motivation of the paper, I think perhaps manipulation environments where a robot arm interacts with multiple objects could be an interesting choice.\n- From the empirical results, it is not clear that Deep RLSP works substantially better than the simple average features baseline.\n\nOverall I think the paper has the potential to be a good paper but could still be substantially improved and I'm leaning towards rejection. \n\nMinor comments and questions for the authors:\n- I'm curious how you choose the gradient magnitude threshold? Does Deep RLSP fail without the curriculum? Could you provide an ablation that shows the effect of using a curriculum? \n- I would also be interested in an ablation of the cosine-similarity weighting heuristic.\n- I think the phrase recent work in the abstract could use a reference.\n- I'm a bit confused by the description of the environment suite by Shah et al. in appendix A, in particular the different rewards. Could you clarify and expand the description a bit?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}