{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Thank you for your submission to ICLR.  The reviewers and I are in agreement that the work presents some interesting connections between closed-loop control and stabilization of activations to an observed manifold.  Specifically, the idea of using optimal control dynamic programming techniques to compute optimal adjustments to ensure control on this manifold is an interesting one and may have other implications within deep networks.\n\nAlthough the reviewers were convinced by the experiments on robustness, I remain a bit skeptical here.  The results show that while the method marginally improves robustness to small-epsilon perturbations, the models are still quite non-robust against the size perturbations frequently used in assessing adversarial robustness (e.g., to eps=8 perturbations on CIFAR10, where the best approach gets ~11% accuracy against PGD attacks).  It doesn't really matter how well a defense works against sub-optimal attacks: if PGD is able to decrease its accuracy this much, clearly the model is not very robust (and it seems upon reading that only 20-step PGD, with no restarts, was used as an attack, which is a fairly weak variant of PGD).  Furthermore, the approach didn't improve much upon PGD-based adversarial training when combined with it, either, overall suggesting that the impact on robustness is somewhat minor, and needs to be evaluated quite a bit more.\n\nWhile I don't believe these concerns are substantive enough to override the beliefs of all reviewers, I think that the authors could do a much better job of evaluating the actual robustness of these models (following the advice of https://arxiv.org/abs/1902.06705).  And if the resulting metrics are not as strong as hoped for, then it would be good to evaluate other possible benefits of the approach (perhaps to random distribution shift? it seems a much more likely situation for there to be real gains?).  Thus, while I believe the paper has some interesting ideas, I think the authors should probably tone down some of the current claims of improving adversarial robustness unless they can provide a much more thorough evaluation."
    },
    "Reviews": [
        {
            "title": "Increasing robustness via optimal control",
            "review": "Keeping the performance of deep neural networks against data perturbations is an important and open problem. The authors propose an optimal control-based approach by taking dynamical systems perspective. The proposed method sounds intuitive and efficient. Authors supply theoretical analysis and (small) experimental evaluation. Overall, I believe paper is a good. However, I would like to get some points clarified:\na)\tAuthors used manifold assumption (which is a reasonable assumption for many problems) to define running loss (eq 3). (If I am not mistaken) They choose a quadratic loss to have a tractable optimization problem. However, under these assumptions, one may choose many different losses. Would you please comment on the form of the loss and its impact to the method?\nb)\tLet’s assume dynamical systems perspective is a right perspective for analysing deep neural networks (to be honest I don’t have any criticism about this). To use control theoretic tools, one needs to comment on controllability and observability of the controlled system. I suspect these mentioned properties are a function of the neural network architecture or do authors think the proposed method (as shown in Figure 1) makes each and every deep neural network architecture controllable and/or observable? I would like to hear authors perspective on these issues.\nc)\tAs I mentioned before, the empirical study is quite small, and I didn’t see any baseline (do I miss something here). Do authors consider extending their empirical study and compare their method with some baselines.\n\nI would like to emphasize one more time that, I am positive about the paper. However, I would like to note that I am not expert in the field and I am open to change my view in both direction.\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technically sound study and convincing empirical results",
            "review": "### Summary\nThis study develops a closed-loop control strategy to improve robustness of neural networks to adversarial attacks. The study is technically sound and the empirical results on classification tasks are convincing.\n\n\n### Quality\n\nThe paper is technically sound and the claims are appropriately backed by empirical evaluation. However, I would recommend the authors to discuss a bit more the additional computational cost of running the closed-loop method.\n\n\n### Clarity\n\nThe manuscript is clearly written and provides enough information for an expert reader to understand all the steps to reproduce the results.\n\n\n### Originality\n\nThe novelty of the study resides in the development of a closed-loop control method for increasing the robustness of neural networks. The strategy is devised to scale to the typical high-dimensional nature of neural network activations.\n\n\n### Significance of the work\n\nThe results suggest that the developed approach is a solid step towards developing robust neural networks.\n\n\n### Some typos:\n\n-instead of \"cause different data distribution deviating\", \"cause data distributions to deviate\";\n\n-instead of \"The resulting control policy [...] make it\", \"The resulting control policy [...] makes it\";\n\n-instead of \"the embedding are effective\", \"the embeddings are effective\";\n\n-instead of \"the perturbed states in Fig.2 [...] has\", \"the perturbed states in Fig.2 [...] have\";\n\n-instead of \"to obtain all the intermediate hidden states [...] and accumulates\", \"to obtain all the intermediate hidden states [...] and to accumulate\";\n\n-issue with reference \"E. 2017\".",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach .. ",
            "review": "The paper builds on recent revival in control-theoretic approaches to deep neural networks by proposing an adaptive controller that projects intermediate representations in the network to their \"manifolds\" and consequently makes the neural network robust to input perturbations. \n\nPros:\n\n+ Active controller-based projection of intermediate features is an interesting idea and the utility of Pontryagin's maximum principle to address the challenge of high dimensionality of the state (features) is a good observation.\n\nCons:\n\n- The manifold-based defense has been shown to be broken previously. For e.g. please see section 4 of https://arxiv.org/pdf/1712.09196.pdf . Manifold/GAN/VAE based defenses can be easily broken by just attacking the projection network and the original network together. The paper considers manifold based defense at all layers using an active controller doing the projection. While comparison with pixeldefense is useful, it would be good to launch an attack similar to the reference above and then observe the effectiveness of the approach. The arguments in the paper are not sufficient to convince the reviewer that this defense is practical. The paper is severely lacking in comparison with existing defense approaches. The state of the art for the used dataset in the paper is significantly better than the effectiveness of the presented approach. For e.g. see https://github.com/MadryLab/robustness \n\nIn summary, the paper is a good effort to exploit the use of active controllers in deep learning. But firstly, the use of manifold-based projection as a cost function is itself a non-robust defense against adversarial examples. Second, the experimental evaluation in the paper is significantly lacking and does not meet the standards of a venue such as ICLR. The reviewer will strongly recommend reviewing the advices in https://arxiv.org/abs/1902.06705 on this topic. At this point, the paper is interesting but it needs significant development and is not yet ready for publication. \n\nQuestions for the author:\n\n1. How critical is Pontryagin’s Maximum Principle to the presented approach? What prevents one from using projection to a lower dimensional embedding space followed by a state space control method? In particular, if one is building on the manifold assumption, then isn't it reasonable to not worry about high dimensionality for designing the controller too?\n\n2. Is it realistic to assume the \"input perturbation to be a random vector\" in Section 5 for theoretical analysis when we are considering adversarial attacks such as PGD, CW? If not, then isn't theorem 1 not relevant to the primary topic of the paper?\n\n------ After author's response:\n\n* The response of authors identifies the problem of using running loss in projected space. While one can try to get around it by projecting the loss function as well but that would be a convoluted way to solve the problem, and in any case, not a strong criticism of the presented approach. \n\n* The updated document has generalized the derivation to take general perturbations into account. \n\n* Updates Tables 1-3 resolve empirical analysis concerns of the reviewer. \n\nWith these improvements, the reviewer is happy to recommend acceptance of the paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper argues to proposed a closed-loop control in the robustness training to achieve good results on different types of attack. To me, this statement is ambitious to be called closed-loop control, but the overall structure is meaningful and interesting.",
            "review": "Strength:\n1. This paper first introduced layer-wised projection from the poisoned data to the clean data.\n2. The results show improvement of the robustness over the baseline on different types of attacks.\n\nWeakness:\n1. The statement of the closed-loop control is a little bit ambitious. The overall methods are the layer-wise projection from the poisoned data to the clean data manifold. Normally, in the closed-loop, we will use the control signal $u$ to control the original data instead of the next layer data. For example, the final balance stage should be $u=g(x+f(x+u))=0$. So closed-loop should be at least multi-steps within one layer. For different layers, the closed-loop control will have different control signals, since the dimension/ distribution between layers is much different. So this method is only a one-step layer-wise projection. The $x$ between layers cannot be viewed as the same sample to be controlled. I would recommend the author to change the statement from closed-loop into the layer-wise projection for a better suit. Also, this method is still a feed-forward network, not a \"loop\" control. I do think this method is interested, just a little ambitious. This could be a useful extension of the resnet-based network, since the control $u$ can be viewed as a complicated version of residuals.\n2. The experiment is weak for only comparing with one baseline. Also, can the author provide which baseline model that the author is comparing with? I cannot find it in the text. I would appreciate it.\n3. It's unclear to me about the training of $\\mathcal{E}(x)$, will this requires extra data to train? What's the running speed of this \"closed-loop\" method compared with others?\n\nSome tiny comments:\n1. A trivial comparison would be to train an autoencoder for each layer of $x$ and only use the decoded results to pass through the network. This in principle learns the data manifold and provides the projection to this manifold. \n2. Table 2, the dataset name is not aligned in the center.\n3. Table 3 should be more self-explainable. It's a little confusing in the current form.\n4. Have the author tried multi-steps in a single layer or constraint $x_t$ to be in the same space? \n\n\n\n----- post rebuttal -----\n\nThe authors addressed most of my concerns and the revision is better than before. \nI would like to increase my score and would recommend an acceptance.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}