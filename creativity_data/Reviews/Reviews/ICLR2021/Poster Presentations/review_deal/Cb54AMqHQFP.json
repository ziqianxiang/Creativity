{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper follows the observations of Renda et al. (2020) that the learning rate in the fine-tuning or retraining phase of neural network pruning is an under-considered component of the pruning process. Renda et al. (2020) argue for a technique that uses the learning rate schedule of the original training regime for fine-tuning. However, their work does not offer a hypothesis or an explanation for why this works.\n\nThis work instead offers more insight into why reusing the original learning rate is productive. Specifically, it shows that using high learning rates is the key component. To demonstrate this, the paper includes a study of using the original step-wise learning from the original training regimen, except accelerated for a given number of fine-tuning epochs. The paper also demonstrates that Cyclic Learning Rate Restarting (CLR) also provides an effective, if not better, learning rate schedule for fine-tuning.\n\nAs noted by the reviewers, the core observations and contributions of this work are modest, but are still a valuable addition to the literature in the pruning community.  \n\nHaving said that, there are some confounding issues with CLR. Specifically, that CLR itself may simply be a more effective learning rate schedule for training neural networks, independent of the particular application to fine-tuning (Reviewer 1). The revision includes an additional appendix that dispels some of this concern. However, indeed, the CLR does improve the base network performance for some configurations.\n\nBroadly, the value proposition here is a thorough demonstration of learning rate schedules for fine-tuning with an overall take that comparisons between techniques need be more sensitive to this choice as previous work perhaps has not thoroughly considered alternative learning rates.\n\n"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "## Summary \nThis work focuses on evaluating different fine-tuning strategies after structured and unstructured pruning. The results show that high learning rate schedules (like cosine schedule) attain best performance in many different setting. With this high learning rate random (structured) pruning seems to work as well as the other pruning criteria. Overall the work has a strong coverage of experiments and the results could be helpful to the community. However, I think the work misses some important baselines and require a bit more work on writing. \n\n## Pros\n- A comprehensive coverage of different pruning algorithms is definitely a plus. Experiments are focusing more on the structured pruning methods, which I am not sure useful given the results of [1] (see cons below). \n\n- Authors seem to have a good knowledge of recent structured pruning methods, which makes the study and the experiments convincing/strong.\n\n## Cons\n- I don't think the following statement is true (at least many of the unstructured pruning methods: \"In most cases, pruning consists of three steps: training..prune..retrain\" The paper starts with this premise and ignores many of the other iterative pruning methods (which prune during training). Many of the best unstructured pruning methods are iterative GMP (https://arxiv.org/abs/1710.01878), DNW (https://arxiv.org/pdf/1906.00586.pdf), STR (https://arxiv.org/abs/2002.03231). \n\n- Only exception to the point above is the Table-1 (SFP); however in that table the authors miss an important baseline which is scaling the original training proportional to the Training+FT like it is done in [1, 2, 3]. This baseline should be added to the table and considered wherever possible. \n\n- \"Recently, Renda et al. (2020) proposed a state-of-the-art technique for retraining pruned networks namely learning rate rewinding\" I don't think this sentence is accurate. It's true authors claim SOTA, but I would argue they miss an important baseline in which the original training schedule of the iterative pruning algorithms are scaled according to the training budget as it is done in [1, 2, 3]. In my experience this method performs better than LRW. Therefore it would be nice to include this baseline whenever possible. \n\n- Does CLR results for l1-pruning exceed Scratch-B results of [1]. If yes, this is very important to mention/highlighy. Otherwise, I like to see a discussion about why we should care about structured pruning methods as the main focus of the work seems to be those. \n\n- What is the difference between PFEC and PFEC-B? And why do authors use this acronym? \"l1-structured\" might be a more appropriate choice. And what are the multipliers in Figure-2 (i.e. 1.12x, 1.45x...)? It would be better to use \"sparsity\". \n\n- \"A well-known practice is fine-tuning, which aims to train the pruned model with a small fixed learning rate.\" and \"More advanced learning rate schedules exist, which we generally refer to as retraining.\" I rather call of them fine-tuning or warm-restart, as all networks start with a pretrained network. This terminology would align better with the previous work. Then you call call constant-lr, lrw, clr, etc...\n\n## Minor Points\n- \"Here we hypothesize that the initial pruned network is a suboptimal solution, staying in a local minima.\" I found this statement a bit vague. Networks are usually not not converged after they are pruned (or even at the end of an ImageNet training). We also don't know whether with long enough training the small learning rate finetuning would get same good results or not. It would be nice to make this statement more precise. Maybe something like \"high learning rate would help find better minima faster.\" \n- \"as `1-norm filters pruning\" filter pruning. Structured pruning is used more often similarly \"weights pruning\" -> unstructured pruning\n\"For simplicity, we always use the largest learning rate of the original training for learning rate restarting...\" I didn't understand this statement. Is this for CLR?\n- \"For CLR and SLR, the learning rate is increased from the smallest learning rate of original training to the largest one according to cosine function\" probably the other way around Learning rate is decayed over time?\n\n- \"For ImageNet, we run each experiment once.\" It would be great to run few more to get more precise results before the final version. \n\n- \"under both setting learning rate restarting approaches consistently...\" -> under both settings lr-restart consistently...\n\n- Is Figure-3 a/b MWP? It would be nice to mention this in the title or caption.\n\n- \"Being that said,\" -> That being said\n\n- \"there are notable differents between\" ->  differences\n\n- \"for future works.\" -> for future work\n\n[1] RETHINKING THE VALUE OF NETWORK PRUNING, https://arxiv.org/pdf/1810.05270.pdf\n[2] The State of Sparsity in Deep Neural Networks, https://arxiv.org/abs/1902.09574\n[3] Rigging the Lottery: Making All Tickets Winners, https://arxiv.org/pdf/1911.11134.pdf\n\n## After Rebuttal\n- I thank authors for considering my suggestions. I increase my score to 5. Having a quick look (I am sorry that I didn't have more time) at the new results; most results on structured pruning seem to agree with [1]; with some improvements over baselines when CLR is used when training from scratch. Results on unstructured pruning seems minimal and focuses mostly on one-shot pruning; and furthermore the baseline suggested above (i.e. scaling the entire learning_rate schedule)  is not added to the iterative pruning results. Overall, I like the direction of the paper, but I think the motivation should be improved and results should be distilled.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Re-training matters as much as sparsification",
            "review": "The authors conducted a comprehensive set of experiments on choices of learning rate schedules for re-training/fine-tuning during iterative or after 1-shot pruning of deep convnets.  Empirically, they reported that high learning rate (LR) is particularly helpful in recovering generalization performance of the resultant sparse model.  The results are purely empirical, well-documented observations from well-designed experiments, which is of practical value in practice of network compression, and the consistent, somewhat surprising observation raises interesting questions.  \n\nNotably, this work has brought to attention an important but often overlooked aspect of network pruning: there exist complex interactions between the dynamics of optimization and sparsification, and as a consequence, it is only fair to compare two sparsification techniques when each of them are put in the _best_ optimization setup, respectively.  \n\nI have a few comments that I wish the authors would address here, discuss in revision or note for future work:\n\n(1) Why is large LR helpful in recovering the accuracy of sparse nets?  There is little information provided in these experimental results to shed light on this question.  There has been loss landscape studies of sparse nets during training (such as arxiv:1906.10732, arxiv:1912.05671)--perhaps these could be applied to study the problem.  If the high LR's role were to knock the solution out of bad local minima, then does adding noise to gradients or smaller batch size achieve similar effect at the initial phase of re-training?  \n\n(2) Given a fixed re-training flop budget, after a pruning operation on the network, both (a) weight value rewinding (as in the Lottery Ticket Hypothesis training), (b) re-training LR schedule (as in this work) might be potentially helpful.  How does weight value rewinding interact with LR?  \n\n(3) For the random pruning results in Sec. 4, do fine-grain unstructured pruning methods present the same results?  \n\n(4) Does the result generalize to transformer models?  What about optimizers?  Does Adam present a same story as SGDM? \n\nPage 5, line1 of the 3rd paragraph of Sec. 3.2: typo \"reachs\"",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper that explores learning rate schedules when re-training after pruning, and shows that re-training learning rate can matter more than pruning saliency metric.",
            "review": "# Summary\n\nThis paper analyzes the role of learning rate in re-training after pruning, building on previous findings that changing the learning rate schedule of re-training can result in higher accuracy than low-learning-rate fine-tuning. The paper proposes several learning rate schedules to compare, specifically a cyclic learning rate (gradually ramping up to and back down from the maximum learning rate schedule used during the original training phase) and a compressed version of the original learning rate schedule, and shows that these learning rate schedules outperform standard fine-tuning and also learning rate rewinding, showing that the findings of prior work come from using a higher learning rate in general and not any specific schedule. The paper than shows that choice of re-training learning rate schedule can have more impact on final accuracy than choice of saliency metric.\n\n# Strengths\n\nThe paper, fairly conclusively, finds the following novel results:\n- Re-training with a cyclic learning rate outperforms learning rate rewinding, seemingly leading to a new state-of-the-art re-training algorithm\n- Re-training with a scaled learning rate schedule attains similar accuracy to re-training with learning rate rewinding\n- Re-training a network that has already been pruned and trained with some scheme (e.g., Soft Filter Pruning, Taylor expansions) can reach higher accuracy with cyclic learning rate re-training than standard fine-tuning re-training.\n\nLess conclusively, though still with reasonable evidence, the paper finds:\n- The choice of learning rate schedule when re-training can have a higher impact on accuracy than the choice of saliency metric: specifically, at higher sparsities, re-training a randomly pruned netowrk with a cyclic learning rate schedule results in higher accuracy than re-training a magnitude pruned network with fine-tuning at a low learning rate.\n\n# Weaknesses\n\n- The evaluation of the paper focuses heavily on structured pruning. However, structured pruning can be an unreliable testbed for many of these techniques, as shown by [1]. The paper would benefit from discussion of [1] in relation to the structured pruning results -- for example, can similar accuracy be attained by training a randomly initialized pruned network with the same effective learning rate schedule? Without discussion or evaluation of baselines from [1], it's hard to know quite how to interpret the structured pruning results.\n- The findings about the interplay between pruning saliency metrics and re-training schedule (Section 4), while interesting, are only minimally validated. Specifically, it would be interesting to see full curves of accuracies for each of these techniques like the curves in Figure 4 (at least for weight-level pruning) with a plot showing the accuracy of ResNet-56 with \"Random Pruning + Fine-tuning\", \"Magnitude Pruning + Fine-tuning\", \"Random Pruning + CLR\", \"Magnitude Pruning + CLR\" across different sparsities, which would generate a lot more confidence in this result\n- The paper lacks specific hypotheses which are tested and validated/falsified. Specifically -- it's hard to know what conclusion to pull from Sections 3.1 and 3.2 other than that CLR > SLR >= LRW, and it's hard to know what conclusions to draw from Section 3.3.\n- The more conclusive findings of the paper, that a high learning rate is important for optimization of pruned networks and that cyclic learning rates improve on learning rate rewinding, are a relatively incremental contribution\n\n\n# Overall recommendation\n\n5: Weak reject\n\nI would be willing to raise this score if the authors address some of the weaknesses listed above. Specifically, if the authors can demonstrate that the results on MWP in Table 3 consistently generalize to other sparsities, or generate more confidence in the structured pruning findings (e.g., by showing that they result in higher accuracy than the trained-from-scratch structured pruned networks in [1]).\n\n# Other comments and suggestions\n\n- Minor typo in Figure 2 caption: \"LLRW\"\n- HRank in Section 3.3: if the results are not presented and discussed in the main body of the paper, it'd probably be better to move the entire discussion of HRank to the appendix\n- what does \"Params\" in the tables mean? I assume it means sparsity (i.e., percentage of parameters that are pruned away), but this is never made explicitly clear.\n- why is R-FT missing for MWP in Table 3?\n\n# References used in review:\n\n[1] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. \"Rethinking the value of network pruning\"\n\n\n# Update post author response:\n\nThanks to the authors for the response. The newly reported results (specifically, those of Appendix E.2) satisfactorily address my concerns about both the generalization of the pruning method v.s. re-training scheme results, both in terms of sparsity levels and unstructured/structured pruning (though the observation of the relationship between R-CLR and FT do not hold quite as strongly for unstructured pruning, they do hold at high enough sparsities to be interesting). I’ve raised my score to a 6 as a result.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "initial review",
            "review": "The paper conducts extensive experiments to understand the reason behind the uncanny effectiveness of learning rate rewinding: the usage of a large learning rate.\n\n### pros\n* The paper, in general, is well-written, and the main message is very clear.\n* The paper identifies the reason behind the success of learning rate rewinding through several aspects: retraining cost, model size, and pruning algorithms. It provides guidelines alternative to fine-tuning for practitioners to obtain compact models with better performance after network pruning.\n* The observations of the random pruning are interesting and are aligned with the prior work [1].\n\n### cons\n1. The paper needs to improve its clarity.\n    * It is suggested to include the details for the scores (e.g. 1.12 x in the title) of the subfigures in Fig. 2. Right now it is unclear to me how to calculate the value and why it is meaningful to present the results under these values.\n    * Can you justify why it is necessary to include the learning rate warmup scheme for SLR and CLR, and why the paper only uses 10% of the total retraining budget? How much will the different fractions of the warmup epochs impact the retraining performance? An ablation study is required here.\n    * The observations for random pruning with learning rate restarting are interesting but more details are required. E.g., it is unclear to me the form of performing the random pruning. Is it layerwise random pruning (same sparsity per layer) or global-wise random pruning?\n2. Potential unfair comparison by using CLR. \n    * The paper investigates the impact of different learning rate schedules for the re-training, after pruning on the model pre-trained by the standard stage-wise learning rate schedule. This design choice is sufficient to provide some practical guidelines, but it may also blur the contribution: as CLR is quite different from the other learning rate schemes, it is natural to question if the performance gain is solely due to a better learning rate schedule (but not large learning rate). Can you also provide an ablation study in terms of using CLR for both the training from scratch and re-training, and then compare both the accuracy and the accuracy drop scores?\n    * The paper demonstrates the efficacy of CLR in terms of re-training the pruned model on the standard image classification benchmark. However, it is unclear to me if the same observations can be generalized to other CV tasks or even NLP tasks. It is encouraged to include some preliminary results to argue the generalization ability of the provided practical guidelines.\n\n### reference\n1. Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot, NeurIPS 2020.\n\n### post-rebuttal\nThe authors have addressed most of my concerns, thus I will increase my score from 5 to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}