{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "As the title states (and reads somewhat like an openreview review title), the authors apply the options framework from the RL community to perform hierarchical RL where the option is the dialogue act and the subproblem is the NLG component in task-oriented dialogue (TOD) policy learning. The two technical contributions (beyond the conceptual connection above) is showing that asynchronous updates between the hierarchy levels guarantees convergence and language-model based discriminator to densify the reward structure. Empirical results are solid improvements over recent SoTA findings. \n\n== Pros ==\n+ This is a conceptually appealing application of RL to TOD and they authors had to make additional modifications to get it to work — which will help other researchers in this space.\n+ There are both theoretical and empirical contributions. The theoretical contributions are also insightful and not superfluous to the problem being studied.\n+ Using a language-model based discriminator for reward shaping isn’t completely new (although I haven’t seen in this setting and stated exactly the same), but is interesting and effective.\n\n== Cons == \n+ The writing could use significant work; while the reviewers/rebuttal cleared up many issues, I actually didn’t appreciate the value of this paper in my first read due to the writing (even if the motivation, etc. is sufficiently clear).\n+ Human evaluation is treated as somewhat of an afterthought and there isn’t a deep dive into error analysis of the results. The visualization is a good first step, but there isn’t really a when/why this method works better than others, which is important for a problem where evaluation isn’t conclusive in the best cases. This is also significant since the authors claim ‘comprehensibility’.\n\nEvaluating along the requested dimensions:\n\n- Quality: The conceptual and theoretical contributions are both of high-quality. This is a promising approach to TOD and the authors additions (e.g., async optimization, LM reward shaping) are good examples of applied research. The empirical results are sufficient to above average, but not as strong (although this is partially an artifact of TOD evaluation) \n- Clarity: The motivation is good, but the paper could use some work in writing. Some examples include (1) stating precisely how the option choices are derived (latent variables), (2) mapping out notation in something like a preliminaries section, (3) sketch of proofs in the main body for continuity. If the reader is familiar with the closest cited work, it is a bit easier, but I think some effort in making the paper more self-contained would increase its impact.\n- Originality: Options in HRL is widely known, but applying it to TOD is novel to the best of my (and the reviewers) knowledge. I think many could have come up with the basic idea, but it took some effort to get it to work.\n- Significance: This is a widely studied problem and the approach is fairly convincing. I don’t think it will be ‘disruptive’ or cross-pollinate to other application areas, but will almost certainly be cited within the conversational agent community.\n\nIn summary, the reviewers like this paper a bit more than myself personally — I think it is borderline with a preference to accept while the reviews are a more confident accept. However, the reviewers are also experienced experts in this area. I also do think that the authors handled concerns well in the rebuttal stages and addressed my more pressing concerns. I would encourage the authors to improve the writing if accepted, but I would prefer to accept this if possible.\n"
    },
    "Reviews": [
        {
            "title": "Good paper with both theoretical and empirical contributions. ",
            "review": "Summary\n=========\nAuthors applied reinforcement learning framework to the problem of task-oriented dialog. In particular, they used the option framework to represent the connection between the dialog policy and the natural language generation. Theoretically, they showed that synchronized updates to the low-level and high-level policy may never converge, yet asynchronized updates guarantees convergence. Authors also used a discriminator reward signal to cope with sparse reward (dialog success rate) and better representation of the human evaluation.\n\nEmpirical results on MultiWoz 2 and 2.1 shows improvement over other state-of-the-art techniques.\n\nSummary:\n+ Appealing theoretical contributions\n+ Empirical results are encouraging\n+ The use of discriminator for reward shaping in addition to task success rate is interesting \n- Writing and explanation can be improved. There are few places (see details) that authors have assumptions in mind but do not provide those assumptions until later. Would be great to state them upfront to avoid confusion.\n- The original option framework assumes given options. Given the ending of the paper, I interpreted that the set of dialog acts (i.e. options) are learnt automatically but I could not find this to be communicated explicitly.\n\nQuestions:\nDid you look into the quality of dialogs specifically? In some of our recent experiments we found out that those automatic metrics do not necessarily correspond to great user experience.\n\nDetails\n=========\nAbstract: In o gur work => In our work\nP5: \"oracle dialogue state\": What is the oracle dialog state and how is it calculated? The world oracle conveys the meaning of being absolute truth which sounds a bit unexpected. The same comment is applicable to oracle database (DB) results. Is it possible in your system you queried the DB with wrong parameter? If yes, do you still name the output of the DB as oracle results? => Ah. you explained this in page 6, in Task Description. I highly recommend bringing this assumption earlier to avoid readers confusion.\nP5: Section 4.4: I am still eager to know how you select your dialog actions. Would be great to tell your reader earlier. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More discussion/ablation on the proposed method study required.",
            "review": "Summary\nThe paper looks the problem of lack of comprehensibility that arises when we use RL to train a E2E dialog system to maximise a given reward function. The paper proposes a  HRL/options framework based method to learn a dialog policy over learned latent dialog acts which can then guide the lower level NLG. This along with a regularization reward using language model the paper aims to improve comprehensibility. The show improved performance in MultiWoz dataset.\n\nStrengths\n1. The paper looks at a very relevant problem in dialog research. The ability to use RL along with SL and being able to use RL without compromising on comprehensibility.\n2. The paper proposes Options framework based method for using the hierarchical structure in dialog to learn the dialog policy and NLG in a hierarchical fashion.\n3. They provide a training that guarantees convergence to local maxima.\n4. They show improved total performance in MultiWoz dataset compared to recent, relevant baselines.\n\n\nWeakness/Comments/Questions\n1. The paper starts with the motivation of handling comprehensibility. More discussion is required on a) what are the reasons of loss in comprehensibility in this case (it is briefly mentioned in the intro) b) why their individual design choices and how they handles the different reasons c) some evaluation to verify this\n2. It would helpful to place it more clearly where the contribution of the paper lies in the related work. My understanding is that contribution of the paper is in exploring using options framework to goal-oriented dialog to handle the issue in question. HRL in general has been used previously for goal-oriented dialog, using language models to regularize RL models has been used and pertaining using SL is widely used. If there are particular differences in the above, it would nice to clearly state them and also say why the different choices and verify if the different choices are beneficial compared to the previous ones.\n3. Relevant to some of the above points. It is not clear to me why we cannot use HDSA+R or LARL + NLG + language model reward. Not necessarily these particular combinations. Some discussion on the current design choices and why making the proposed methods features to some of the other baselines is not a way to achieve some benefits is not the right way to do it. It's ok for the proposed method to be one particular way, but that discussion would be useful.\n4. Clarification on the task setting: Is it the case that the agent's current utterance does not decide what the next user utterance is? i,.e the agent is given the ground truth context every time and asked to predict the correct next utterance. That prediction does not affect the way in which the overall dialog goes? If that is the case, that should be made more explicit. In that case, the dialog policy learned is more of a contextual bandit setting. The complexity of learning options would be way different in the two different settings. Just wanted to understand it.\n5. In the 6.2.3 visualization of clusters, it would be very useful to have a visualization of clusters from some baselines on other ways of learning. To see if this is due to some new addition by the paper or is generally present.\n6. There are several parts to the method and there are I assume several differences in the architecture etc with baselines etc. It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure. Otherwise for example it is not clear to me if the improvement in Blue compared to LaRL comes from the extra reward using the language model or from the options framework. What happens when the extra reward for using the language model is added to LaRL (that might be tough if you have to modify others code). what part of the performance is coming from pretraining (especially if using VAE type is novel, then quantifying that is important with and without VAE type SL), etc.\n\nQuestions to authors\n1. It would be great if you could respond to some of the above comments. Thanks.\n\nMinor\n1. Typo in the abstract: *In our work, we\n2. Move to E2E system can be motivated a bit more (allows end-user feedback to be passed through all modules easily and don't have to worry about how a change in one module affects all other modules explicitly etc)\n3. Might be useful to define what is exactly meant by 'comprehensibility'\n4. The intro says \"the goal is absolutely clear\". Not sure if that is really true. For example, it's not clear what the correct reward function to provide to the RL agent is. As the paper points out, the success rate alone is not enough. The dialog needs to be polite, follow natural language, short, etc which are hard to automatically measure. Human evaluation is costly and could also have bias like the paper points out. Just trying to say that automatic evaluation of dialog systems is a hard problem. \n5. I am just curious why you use GRU for the encoder and LSTM for the decoder?\n\nAfter author response\nThe authors have responded to most of my questions/concerns satisfactorily.\nChanging my score from 4 to 6",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting work but the dialog task is basic.",
            "review": "This paper attempts to model task-oriented dialogue system using hierarchical reinforcement learning between the actions policy and natural language generation system. The utterances are encoded with GRU cells, the action policy is one-layer linear model and outputs the mean and variance of a multivariate Gaussian distribution, and the NLG is an LSTM decoder.\n\nThe paper is well written, easy to follow, and adequately motivated. \n\nPros:\n\nHierarchical RL formulation with options for joint action policy and natural language generation\n\nThe paper proposes asynchronous updates between dialogue policy and NLG to theoretically guarantee their convergence to a local maximizer. \n\nIt also proposes using a language models as a discriminator model for reward assignment to further improve the comprehensibility of generated responses.\n\nPre-training of task-oriented dialog models with  variational bayes similar to VHRED (Serban et al., 2017).\n\nExperiments show significant performance improvement over the baselines with both automatic and human evaluations. \n\nAlso, the results show that continuous representation of the action policy in HDNO performs better than the discrete representation in the LaRL, which is very interesting and informative.\n\nCons:\nThe use of an oracle dialogue state and an oracle database search result. This is a major drawback of this work. Recent work in this space are now considering imperfect dialogue state and the corresponding database search results. There is also an increasing need to consider non-trivial database interactions such as booking for more practical applications. \n\nThe evaluation/comparison using updated official evaluator may be missing for some of the more recent work e.g..,\n\nGPT-2 (Budzianowski and Vulic 2019)\nStructured Fusion (Mehri, Srinivasan, and Eskenazi 2019) SOLOIST (Peng et al. 2020)\nDSTC8 Track 1 Winner (Ham et al. 2020)\nSimpleTOD (Hosseini-Asl et al. 2020)\n\n\nQuestions:\n\nWhat do you mean by “Distinguished from a conventional modular system, we additionally give a context to NLG to satisfy the option framework.”? You mean the dialogue context is equivalent to the state space in HDNO formulation?\n\nHow are the oracle dialogue state and an oracle database search results encoded?\n\nWhat kind of model is used for the language model discriminator? LSTM?\n\nHow did you handle the action policy toward the database for booking, which can cause database mutation? Are those ignored?\n\nAre the results in Table 5 based on the updated official evaluator similar to Table 1? If not, then the comparison in Table 5 is not apples to apples.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Tend to Accept",
            "review": "Summary: \nThis paper proposes modeling the hierarchical structure between dialog policy and natural language generator with option network and train it with HRL. It also introduces a discriminator modeled with language models as an additional reward, which further improves the learning procedure's comprehensibility. Besides, this paper has demonstrated the interpretability of the latent dialog act via clustering methods. \n\nPros: \n1. This paper proposed formulating dialog policy as a high-level policy over dialog act and NLG as a low-level policy and train the word policy module using HRL. It is another move towards improving policy learning via HRL.\n2. The proposed model achieves SOTA performance in the policy optimization leaderboard on the MultiWOZ dataset.\n3. It is interesting to see that when formulated as latent factors, the distribution of dialog acts still shows a clustering pattern, which manifests the model's interpretability.\n4. This paper introduced an additional reward using a discriminator of language models. The ablation study shows that this reward can be useful (though not significant).\n\nCons:\n\nThe proposed HRL approach is a direct application of the option framework on a task-oriented dialog system. It is similar to the hierarchical structure in the open-domain dialog system. \n\nMinor issues:\n1. Introduction paragraph 2: When introducing end2end models, there is a lack of citations of recent e2e models including DAMD, SimpleTOD, SOLOIST, etc.\n2. Abstract Line 9:  o gur -> our",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}