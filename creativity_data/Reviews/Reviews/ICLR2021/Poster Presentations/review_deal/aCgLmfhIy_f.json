{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a method for regularizing the pre-training of an embedding function for relation extraction from text that encourages well-formed clusters among the relation types. Experiments on FewRel, SemEval 2010 Task 8, and a proposed FuzzyRed dataset show that the proposed prototype method generally outperforms prior state-of-the-art, including MTB (Soares et al., 2019), which was the strongest. The key, novel idea is to model prototype representations for target relations as part of the learning process. A contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond few-shot learning. This additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research.\n\nReviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. Some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pre-training and target tasks, and the need for several additional baselines. The authors were able to address these concerns, and the reviewers did not raise any follow-up concerns."
    },
    "Reviews": [
        {
            "title": "Clarity and presentation issues; Unclear whether the gains are from the pre-training stage or not",
            "review": "This paper proposes a method for learning representations for relation statements as well as classes (prototypes) for relation extraction tasks. I think the key insight is that the relation statements and classes (corresponding to relation types) can be learned jointly using contrastive training objectives. The paper also proposes to use the similarity metric as 1 / (1 + exp(cos(u, v))) and claims that this will provide a clear geometric interpretation and more interpretability. In the experiments, they first train this framework on distantly-supervised data constructed from Wikidata + Wikipedia and then fine-tune it on several downstream tasks: FewRel, SemEval, and a new dataset they created focused on identifying false positives in distantly supervised data.\n\nOverall, I think the proposed approach is quite reasonable and also seems to work well in the evaluation tasks (learning prototype embeddings instead of taking the average of instance embeddings and adopting contrastive losses between relation statements and prototypes). However, I think the paper has many clarity and presentation issues that make it difficult to evaluate the significance of the work. \n\nFirst of all, I think this pre-training stage on weakly-supervised data is very crucial and the details of the data collection (which relations and how many instances have been used) should be moved to the main body of the paper instead of the Appendix. In realistic few-shot scenarios, you only have a very small number of examples for a new relation k so it is difficult to learn the r_k embedding from only a few examples. My interpretation is that for the FewRel evaluation, the relations must have been already seen in the pre-training stage (given both the weakly-supervised data and FewRel are collected based on Wikidata) unless I have misunderstood something (especially that the model can achieve a good accuracy when there is no training data used in Figure 3 & 4). For the SemEval evaluation, I assume the prototype embeddings must have been learned from the training data but it has 6k training examples so it is fine. I think this point really needs to be clarified and can be a weakness of the approach, especially in few-shot settings.\n\nFor the results in Table 1 & 3, it is not very clear to me whether the numbers of previous approaches are from their papers or re-run by the authors. This should be clarified. My main concern is the pre-training data used in this paper can be different from what has been used in (Soares et al, 2019 -- they didn’t use Wikidata and only consider Wikipedia and the links) and it makes the comparisons unfair. \n\nThe authors claimed that this similarity metric is crucial but there is no ablation study or comparison to other alternatives… How if you just compute the dot product between the two embeddings? I think the comparisons to commonly used similarity metrics need to be added to justify why this design choice is important.\n\nI also don’t know what the L_{CLS} training loss is used for. If z_k is just a set of learnable embeddings (one embedding per relation) and if it is used to predict the relation k, isn’t it just multiplied by another set of K embeddings? What is the benefit here?\n\nAlso, I don’t understand the fixed prototype baseline (denoted as IND). What does “fixed prototypes z that are pre-computed by vectorize extracted patterns for each relation” mean?\n\nTo sum up, I can’t recommend the acceptance of the paper if the above issues cannot be addressed. I am also concerned whether the highlight of the approach is the contrastive losses and prototype embeddings, or it has to be coupled with some type of pre-training (or even specifically pre-training on distantly-supervised data). \n\nMinor:\n- Equation (7): The second S2Z should be S2Z’.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposed a prototype based relation learning method which infers a prototype vector for each relation type to help train a better text encoder.",
            "review": "Summary:\n\nThis paper proposed a prototype-based relation learning method which infers a prototype vector for each relation type to help train a better text encoder. And authors introduced several loss functions including statement-statement similarities and statement-prototype similarities. \nExperimental results over both few-shot and supervised scenarios indicate that these additional training losses improve the performance significantly. Furthermore, the author provided a dataset to indicate that the proposed method can effectively reduce the noise from distant supervision. \n\n#####################\n\nReasons for score:\n\nI vote for marginally positive. Overall, the paper is interesting and the experiments are extensive and promising. However, the main concerns listed below prevent me from giving full acceptance. And I would like to hear further responses from the author.\n\n#####################\n\nMain concern:\n\nBesides promising results shown in the paper, there are some concerns about the concept of \"prototype\" in this paper: \n\n1. If I understand correctly, the pretraining and fine-tuning are using the same data for all the experiments because the author didn't mention extra corpus for pretraining. Therefore prototypes are consistent (by consistent I mean there is one-to-one correspondence) with the annotated relation types in the fine-tuning (This is a little confusing because pretraining usually means using a different corpus).\n\n2. And if prototype relation types have one-to-one mappings with relation types during fine-tuning, the key difference between the proposed method and direct training a relation classifier is that prototype vectors help to train the sentence (also called statement) encoder through S2S and S2Z losses, and then a separate classifier is trained over the statement vectors for classification. Then I'm curious why not directly add S2S and S2Z losses to the classifier? What is the reason behind generating prototype vectors? \nFrom my understanding, \"prototype\" usually means that, in the meta-learning and few shot scenarios, we can generate a new model from the prototype model. However, according to formula 9, during inference, the model uses a nearest neighbor classifier without any model generation. \n\n\n##############################\n\nSome minor comments:\n\n(1) Does formula 2 miss a negative sign for both similarity metrics? Otherwise the larger the cosine, the smaller the metric value, which is dissimilarity.\n(2) it is not clear to me how to get the baseline of fixed prototypes vectors from vectors of extracted patterns. Is it average?\n(3) This work shares similar spirits with Row-less Universal Schema (which has been cited in the reference) where relation type embeddings guide clustering of statement embeddings using distant supervision. A brief comparison will be useful in the related work for completeness.\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice work!",
            "review": "Summary:\nThe authors propose a novel method for learning prototype representation for relations which abstracts the essential semantics of relations between entities in sentences. The learned prototypes are learned based on an objective with clear geometric interpretation and have been shown to be interpretable and robust to noisy from distantly-supervised data. The method has been shown to be effective for supervised, few-shot, and distantly supervised relation extraction. The prototype embeddings are are unit vectors uniformly dispersed in a unit ball and statement embeddings are centered at the end of their corresponding prototypes. The training is done such that intra-class compactness and inter-class separability is increased. The results show that the proposed approach gives state-of-the-art results for few-shot, supervised relation extraction. The authors demonstrate the interpretability and robustness of the method. \n\nQuestions:\n1. It is a bit hard to follow how increasing the number of relation types in Figure 3 improves accuracy although the problem should become harder with more relations. \n2. Also, what is the intuition behind the initial decrease in performance in Figure 4 on increasing the number of instances per type although the same is not observed on other methods. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Better justification on the motivation and experiments",
            "review": "This paper presents a pre-training method for encoders (i.e., BERT) of relation extraction, leveraging distant supervision data. The main idea is to introduce a prototype embedding for each relation in the distantly generated data. The loss function to pre-train the encoders involve several terms, aiming to exploit the intra-class compactness (e.g., the contrastive loss for statement pairs) and the inter-class separability (i.e., constraining the distances between the prototype for one relation and the embeddings of the statements for the same and different relations). The paper incorporates a prototype-level classification loss. Compared to few-shot learning models (e.g., prototypical networks), the paper claims that prototype-based losses can help the model to better address the noisy label issue of distantly generated data due to the exploitation of prototype and instance interactions. The pre-trained encoder is then applied on datasets for few-shot, supervised, and fuzzy relation extraction. The proposed method achieves competitive performance with other methods. Some analysis are also conducted to demonstrate the effectiveness of the proposed model.\n\nOverall, this paper is well written and the proposed method seems helpful for improving the performance of relation extraction on several settings according to the presented experiments. However, I have several concerns regarding the motivation and experiments of the papers as follow.\n\n\n1. The paper loss terms in Equations (2), (3), and (4) seem intuitive. However, they are all based on potentially noisy assignments of labels for statements (due to distant supervision). The paper mentions that as cross-entropy loss only considers instance-level supervision, it is particularly noisy (page 4). However, it is unclear why Equations (2), (3), and (4) can better handle the noise given that the information they rely on is still inherently noisy. Relatedly, a more direct baseline for the proposed model is to just fine-tune the encoder with cross-entropy loss using the standard supervised learning setting with distantly supervised data (i.e., directly predict distant labels from statements). As this baseline seems missing, it is unclear why the proposed losses are helpful and convincing.\n\n2. The paper uses the pre-training approach with distantly generated data. Another approach is to use multi-task learning where the encoder is trained simultaneously on both downstream task and distantly-generated data. How does this approach compare to the proposed method. Also, how does the relations in the distantly generated data differ from those in the few-shot and supervised experiments?\n\n3. In Appendix A, the paper mentions that the encoder is also fine-tuned with masked language model loss. This seems to be an important factor (given its effectiveness in BERT); however, it is not evaluated and discussed in the main paper. How does the performance changes if this masked language model loss is not used?\n\n4. The performance in Table 3 makes the proposed method less convincing. In particular, the proposed method (COL Final) essentially has the same performance as the baseline (IND) which raises a question about the benefits of the proposed techniques. There is no discussion between the performance difference between these two models to better justify the model. The performance gap between the proposed model and the baseline MTB seems minor too. Also, Soares et al., 2019 seem to report a different performance on SemEval than those indicated in this paper (i.e., the best score in there is 89.5). Can you clarify this detail?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}