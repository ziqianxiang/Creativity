{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers raised a number of concerns about the novelty of the paper and comparisons. The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published. I do think however that this paper is quite borderline. I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons. However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques. Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots. In comparison, the experiments in this paper are quite simplistic, using toy domains and \"demonstrations\" obtained from a computational oracle (i.e., another policy). Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline. That said, I would  defer to the reviewers in this case -- I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental. I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples-to-apples comparison.\n\nOne thing I would request of the authors for the camera ready though is: Please tone down the claims. \"Human-like 7 DOF Striker\" is not human-like, it's a crudely simulated robotic arm that was recolored. It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples-to-apples manner the particular algorithmic innovations in the method."
    },
    "Reviews": [
        {
            "title": "Learning domain-invariant representations for observational imitation learning",
            "review": "This paper studies observational imitation learning, a problem setting in which the agent wishes to learn from expert observations, but the state, action, and observational spaces of the expert and agent domains can vary. They introduce DisentanGAIL to tackle this problem by introducing two mutual information constraints in the GAIL framework to learn domain-invariant representations of observations. In particular, DisentanGAIL aims to discard domain information in the presentation, while retaining relevant goal completion information. \n\nOverall, the problem setting is very practical. The performance gain is greatest in the domains where the camera angle changes, which is promising. I like the intuition provided in Appendix A and think some of it should be included in the main text. It motivates well the distinction between domain information and goal-completion, and highlights why previous methods like domain confusion loss can fail. Moreover, I would’ve liked to see some analysis of the experiments to support this hypothesis. For example, a version of Appendix D1 would be valuable to include, and perhaps also another visualization demonstrating that other baselines fail to encode goal completion information. Some of the implementation details in Section 5 can be shortened/moved to the appendix instead.\n\nSome questions:\n\n- What is the difference between DisentanGAIL with DCL and TPIL? The text says that the DCL substitutes the mutual information constraints, so the only difference I see would be learning a distribution over latent representations rather than a deterministic feature extractor. Any intuition why this alone leads to such a big difference in Table 1?\n\n- Without any regularization, I’m surprised DisentanGAIL learns anything at all. Wouldn’t the discriminator then be able to tell the two domains apart (especially if one of the differences is color) and fail to provide a meaningful learning signal to the agent?\n\n- If I understand Figure 3 correctly, the orange line is without any regularization (lower bound) and the green is learning without domain differences (upper bound). How come DisentanGAIL can eventually outperform the green line in some of the experiments?\n\n- What’s the gap between DisentanGAIL and the expert policy? Does the expert always achieve a reward of 1?\n\nOther comments\n\n- Please clean up all Figures and Tables. The text is too small and impossible to read without zooming in very close. \n\n- “Learning” typo in last line of page 2.\n\n----------\n\nPost-rebuttal comments\n\nThank you for answering my all questions and updating the manuscript with some of my suggestions. The additions in section 4 clarify the motivation much better and also highlight the differences with prior work. I've increased my score from 6 to 7.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review #2",
            "review": "This paper proposes a visual imitation learning algorithm that can handle domain shifts between the expert demonstrations and the data generated by the agent. The proposed method is built upon GAIL and scales to image inputs. The authors handle the domain shift problem by learning a domain-invariant discriminator and a statistics network. The invariant discriminator takes in a concatenation of a sequence of latent representations of the observations and tries to predict without relying on the domain information. The statistics network is used for estimating the mutual information constraint between the latent representation and the domain labels, which the authors are trying to minimize in order to attain domain invariant predictions. The authors optimize the standard GAIL objectives along with the mutual information constraints as regularizations jointly. The experiments are done in both discrete and continuous control tasks in Mujoco and the proposed approach, DisentanGAIL, outperforms the prior method TPIL.\\\n\nFor pros, I think the experimental setup is reasonable and the authors conduct relatively thorough ablation studies to show the usefulness of various components such as the prior data constraint, double statistics network, and spectral normalization regularization, which is helpful for understanding the method. The paper is also well written and easy to understand.\n\nHowever, I have a few concerns about this approach, which I will list as follows.\n\n1. Regarding the novelty of this paper, I feel like the approach is not particularly novel. The method is similar to TPIL (Stadie et al. 2017) and the main difference between DisentanGAIL and TPIL is that in DisentanGAIL, the authors set the mutual information constraint to be less than 1 bit, and in TPIL, the mutual information is constrained to 0. I can definitely see why the softer regularization works better, but I wonder if the contribution of this work is substantial given such a simple tweak. Moreover, the authors also propose many techniques to make DisentanGAIL work better, such as using unsupervised data with some regularization, double statistics networks, spectral norm, and etc.. However, these seem to be more like implementation tricks rather than major contributions and I'm unsure if they make the contribution substantial enough for the standard of an ICLR paper. Moreover, adding these additional components definitely seem to make the approach much more complex and might require much more tuning to get it to work, which is another concern.\n\n2. Another point related to novelty and related works, there are several papers that also consider domain-adaptive imitation learning that seem pretty similar to this paper, such as [1, 2, 3]. [1, 2] consider learning domain-invariant features, while [3] proposes a unifying theoretical framework for domain-adaptive imitation learning. I think these methods could serve as comparisons to DisentanGAIL.\n\n3. Furthermore, since the approach seems to be a bit incremental, it would be nice to have the theoretical analysis that could justify the incremental changes and guarantee the convergence to the optimal policy (e.g. attaining similar results in [3]). Unfortunately, this is missing in the paper.\n\n4. The challenging, high-dimensional environments used in the paper are all in locomotion tasks in Mujoco. It would be nice to see more realistic environments such as robotic manipulation tasks like ROBEL and  Adroit etc., which would make the domain adaptation more appealing. \n\nOverall, based on the arguments above, I would recommend a reject for this paper.\n\n------------------------------------------------------------------------------------------------------------------\nPost-rebuttal updates:\n\nAfter reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. Nevertheless, I've increased my score to a 6.\n\n[1] Okumura, Ryo, Masashi Okada, and Tadahiro Taniguchi. \"Domain-Adversarial and-Conditional State Space Model for Imitation Learning.\" arXiv preprint arXiv:2001.11628 (2020).\n[2] Lu, Yiren, and Jonathan Tompson. \"ADAIL: Adaptive Adversarial Imitation Learning.\" arXiv preprint arXiv:2008.12647 (2020).\n[3] Kim, Kuno, et al. \"Domain adaptive imitation learning.\" arXiv preprint arXiv:1910.00105 (2019).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary\n--\nThis paper proposes a method for performing observational imitation learning -- an existing task that seeks to enable an agent to learn from visual observations of expert behavior in order to roughly imitate the expert behavior. The method employs an adversarial imitation learning objective function that incorporates proposed mutual information constraints that are intended to force the representation space to be invariant to the domain of the data sources, and instead only encode goal-completion information.\n\nExperiments illustrate that the method was able to learn and be performant despite visual and embodiment differences in the expert and agent domain on various mujoco environments, exhibiting substantially better performance to a different observational IL method. Experiments also illustrate good performance on a slightly higher dimensional task.\n\nClarity and Correctness\n--\nThe paper has a few somewhat painful clarity issues that make it difficult to understand.\n\n- Section 4 starts with a subsection that describes components of the architecture, when instead it should start with a clear high-level description of the approach.\n\n- S4.2 The relationship between B_P.E and B_P.\\pi and B_E and B_\\pi is unclear. The former two terms were never defined in the preliminaries, so it's not clear whether they constitute additional data that was not mentioned in the assumptions. This ties into my comment about about Section 4 needing an introductory paragraph to lay out the high-level assumptions, inputs and outputs, and idea of the method. The notion of \"prior data\" is used in the paper, but I cannot find a clear description of it anywhere.\n\nOriginality\n--\nAs mentioned in the paper, this paper is closely related to Stadie et al. (2017). There's description of the differences to it in various places throughout the paper, but it would be nice if there were a clear section on the comparison of objective functions.\n\nSignificance\n--\nThe significance of this paper is that it demonstrates a more performant method for performing observational imitation learning, which is potentially more applicable than standard IL approaches that require state or state-action traces.\n\n- Why was TPIL not used as a method of comparison in Fig 3?\n\n- The results shown in Fig 2 have rather short x-axes, with a maximum of 20,000 steps of training. The comparisons would be more informative if training were run for more steps (e.g. 1e6 or 1e7). It is possible that the other approaches end up matching expert performance as well, without any great loss in efficiency.\n\nOther comments\n--\n- S4.2 prior data constraint -- the inequality is backwards (MI is nonnegative)\n- S3.2, trajectories are sequences, use () not {}\n- The logical and notation in S4.2 is very confusing -- the statements inside the parentheses that define d_i aren't boolean truth values, so it makes the definition of d_i confusion. My suggestion is just to use d_i = 1(o \\in B_e), since B_e and B_\\pi are disjoint.\n- Be precise / define what you mean by \"high dimensional\". \"High\" is subjective, and is perhaps not the most appropriate adjective to describe 7 dimensional tasks. Also, make it clear that the high dimensionality is in the action space.\n\nPost-rebuttal comments\n--\nAfter reading the authors' response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns. The inclusion of a clearer motivation, more discussion w.r.t. TPIL, and comparison to TPIL, all enhance my understanding of the contributions of the paper beyond my original review enough for me to increase my score from a 6 to a 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong paper, needs some additional comparisons",
            "review": "## Summary\nThe paper proposes a novel approach for third-person visual imitation learning from observations, ie for imitating a different agent in a potentially different environment purely from visual demonstrations. The main difference over prior work (eg Stadie et al, 2017) that used domain confusion objectives is the introduction of new regularization objectives on the learned representation of the visual scene. Empirically, the paper shows that the proposed regularizations can improve performance across a wide range of third-person visual imitation tasks, transferring between simulated agents with different appearances and/or morphologies.\n\n\n## Strengths\n- the paper is very clearly written and easy to follow, necessary background is adequately covered, all components of the model are explained in detail\n- all novel design choices are ablated in the experimental section\n- experiments are conducted across multiple environments and source/target variations, both appearance and morphology -- empirical performance improvements are demonstrated\n- experiments include higher-dimensional, robotic manipulation environments, more complex than those tested in prior work\n\n\n## Weaknesses\n- **not fully fair comparison to baselines**: since the main novelty lies in the introduction of novel regularization objectives, the \"DisentanGAIL w/ domain confusion loss\" is the main comparison method since the only difference to the proposed method is the representation regularization function. However, this baseline does not have access to the additional collected prior datasets used for the introduced \"prior regularization\" loss. Looking at Figure 2, it seems that the performance of DisentanGAIL w/o Prior Data and DisentanGAIL w/ domain confusion loss are near-equivalent which suggests that the access to the additional prior data might be the main factor that contributes to DisentanGAIL's superior performance --> an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\" is needed, which additionally trains the domain confusion objective on the prior data collected for the DisentanGAIL prior regularization objective, to allow for fair comparison of both regularization approaches with access to the same data\n- **missing baseline results on harder tasks**: on the harder tasks shown in Fig 3 there is no evaluation of the baseline methods, which makes it hard to judge how hard these tasks actually are for prior third-person visual imitation approaches\n\n\n## Questions\n- from my understanding the issue of prior work that constrains the domain-related MI to 0 (described in Appendix, section A) appears for *any* two domains (since, particularly in the beginning of training, there will always be differences in the goal-reaching distributions of expert and policy data). However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI=0 loss from Stadie et al. 2017, works well on many of the tested domains. How does that fit to the explanation made in Section A + the quant results in Fig 5?\n- what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? \n- why does the paper report the \"max cumulative reward so far\" not the expected cumulative reward of the current episode? the latter could give a better idea of the training stability of the different algorithms\n- out of curiosity: the shown scenarios are visually still pretty similar between source and target (eg background etc) --> how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another?\n\n## Suggestions to improve the paper\n- add an additional baseline \"DisentanGAIL w/ domain confusion loss & prior data\", as discussed in the \"weaknesses\" section, particularly for the transfer tasks on the bottom right of Fig 2 in which the discrepancy between DisentanGAIL and the baselines is the largest\n- add evaluation of baselines (particularly DisentanGAIL w/ domain confusion loss (w/ and w/o prior data)) to the harder manipulation environments in Fig 3 to show the benefits of the introduced regularizations\n- since the proposed method addresses a concrete problem of prior work (as explained in appendix Section A) with a clear intuition, it could be nice to add a toy experiment early in the paper that demonstrates this effect empirically for an easy-to-analyze imitation problem, showing that for MI=0 the agent cannot properly learn to imitate since it is unable to capture the relevant information --> such an experiment could help to further motivate the need for the new regularizations\n- the additional assumption of a \"prior dataset\" collected in both domains for additionally constraining the latent representation is first mentioned in section 4.2 --> since this is a different assumption from prior work on cross-domain imitation it would be good to mention this earlier, maybe in a dedicated \"Problem Statement\" section\n- for qualitative matching results in Fig 4 in the appendix it would be nice to show the corresponding matches found when using the domain confusion loss instead of the proposed regularizations to see whether some of the failure cases are interpretable\n- I wonder whether it would be possible to show imitation across agents with more drastic morphology differences in the most challenging 7DOF robotic manipulation tasks. Right now, while there are differences in the link dimensions, the main difference still seems to be in the visual appearance (at least from looking at the provided pictures) --> maybe one could try to eg try to transfer between robots with different gripper morphologies (eg one agent has U-shaped gripper vs another agent with T-shaped \"gripper\" for the pusher task), or from a 4DOF arm to a 7DOF arm etc.\n- as mentioned in one question above: while the agent appearance and morphology changes between the experiments, the largest part of the observation, the background, is usually constant across source and target environment --> it would be interesting to see experiments with different background appearances to see the robustness of the method\n\n## Overall Recommendation\nThe paper is well written and the experiments are covering a wide array of third-person visual imitation problems on which the proposed method shows strong results. The experimental evaluation seems thorough, all design choices are ablated. My main doubts are about the fairness of comparison to the baselines (particularly with regard to the additional data available to the proposed method), and some lacking baseline results on the harder tasks. Therefore I cannot fully support acceptance yet, but if the authors are able to provide the additional evaluations and answer the questions posed above adequately, I am willing to increase my score and vote for acceptance.\n\n\n## Post-Rebuttal Comments\nI thank the authors for their detailed feedback. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting!\n\nI am not sure whether it is standard to show the learning curves with the \"max achieved return so far\" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm. Potentially, the stability could also be increased by adding a learning rate decay schedule?\n\nOverall, the rebuttal addressed my questions and incorporated many of the suggestions, therefore I am increasing my score and vote for acceptance.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}