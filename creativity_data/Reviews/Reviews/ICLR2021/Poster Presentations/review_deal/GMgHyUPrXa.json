{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper initially received mixed ratings, with one reviewer strongly supporting the paper given that the idea of combining unrolled algorithms and NAS is new and interesting, and one reviewer not convinced by the significance of the results. His/her main concern was the use of synthetic data only, which is not realistic. This was a legitimate concern as the performance of sparse estimation algorithms can change drastically when there is correlation in the design matrix. See for instance, the benchmarks in \nF. Bach, R. Jenatton, J. Mairal and G. Obozinski. Optimization with Sparsity-Inducing Penalties. \n\nThe rebuttal addresses this concern in a satisfactory manner and the area chair is happy to recommend an accept."
    },
    "Reviews": [
        {
            "title": "NAS for unrolled LISTA",
            "review": "This paper studies a very interesting new problem of assessing unrolled models in a broader context using NAS methods. LISTA-style unrolling has been popular for deep learning-based inverse problems. But it is quantitatively unclear how good the unrolled models are, among all possible model variations. To fill in this gap, the authors first define a proper search space based on the varying connections and neurons from the unrolled LISTA backbone architecture. NAS is then exploited as the tool to find the best subsect of architecture from the large space. \n\nThis method is very intuitive yet solidly done. Since NAS itself is not always stable, the authors present a number of efforts and discussions to support the reliability and reproducibility of their observations. Sec 2.3 shows their diligence in trying out multiple NAS algorithms; avoiding using weight sharing; and “averaging” the top architectures to smooth out random fluctuations. I appreciate those valuable careful efforts. \n\nThe experimental study in Section 3 is very comprehensive and clearly laid out. Table 1 gives a great gist of their main findings. The authors compare with two natural baselines, LFISTA and Dense-LISTA. They prove NAS can indeed find superior architectures than those hand-crafted in all the different settings. The overall take-home point seems like no big surprise at first glance: unrolling provides a reasonably good architecture; yet if there are abundant training data, more complicated architectures can be discovered by varying connections (varying neurons seems not helpful). \n\nBut, a closer look at those experiments reveals many new finer-grained observations that really intrigue me. For example, it is interesting to see “the denser the better” is not the simple right claim in unrolling; and especially unrolling-based models are advantageous under data-limited training. Another good surprise is to see that all top architectures unanimously adopt soft thresholding as the only neuron type for all layers. The most interesting part of this paper is the \"open the box\" section 4 where the authors discuss the common characteristics of top architectures. Figure 2 is a highly interesting visualization and shows the top-50 architectures are indeed consistent on most connections.  It is very promising if we can understand further (1) why “the later the denser” - could that imply/be interpreted as switching to another higher-order algorithm in later iterations? and (2) why the last layer is particularly all-connected?\n\nI think bringing NAS to learning based algorithm design is a very novel and interesting direction. This paper can potentially turn into a seminal work here to inspire followers. The paper is also well written, and the logic is very clear to follow. Section 5 concludes with a thoughtful discussion on what this work may connect and point to for the entire unrolling field. \n\nI have a few suggestions and critiques for the authors to address:\n\n-\tCan we have a “weighted” version of Figure 2, say the higher-ranked architectures from the top-50 will get more weights when summed up; and in this way would we be able to observe more consensus of what relatively better models (if weighed more) tend to prefer?\n\n-\tLFISTA is a principled way to add more connections to LISTA using momentum. In this paper, the authors claimed their LWA to be better than momentum-based averaging in NAS. I wonder what will happen if replacing the momentum connection in LFISTA with the LWA way: will LFISTA performance improve or degrade?\n\n-\tI remain to be skeptical about the last LADMM example. It is rather unclear to me WHY the learned connectivity patterns from LISTA should transfer to LADMM? Perhaps, that implies some general idea of how dense connectivity could be properly injected in a sparse regression task. I would request to see comparison results from (1) adding dense connections only in the latter half layers; and (2) adding random connections, but with the same total connected percentage as the LISTA pattern. If either 1 or 2 can perform on par or better than the transferred LISTA pattern, the authors should consider tuning their claims to be more specific about what really matters here, than just saying vaguely about “transfer”. \n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Contains extensive experiments but technical contributions are limited",
            "review": "This paper conducts an empirical study on unrolling architecture design, by applying NAS to search the connectivity pattern based on LISTA, and compares the searched model to the original LISTA.\n\nPros:\n+ The paper is clearly written and not hard to follow\n+ Experiments show that the searched architecture performs better than LISTA\n+ The experiments are comprehensive, including various experimental settings that can compare the unrolled architectures from different aspects\n\nCons:\n- Technically, it is a direct application of NAS to LISTA, so the methodological contribution is not very strong\n\nThrough extensive experiments, this paper shows the effectiveness of NAS for searching for a better architecture based on unrolled algorithms, via a case study on LISTA. It tells that unrolling may not be the best choice and better architecture can be found by NAS. However, the technical contribution and novelty of the approaches are not strong enough. I personally think this is a broader line paper and my score is actually between weak acceptance and weak rejection.\n\ngrammar:\n- In the abstract, '.... and are able to discover networks that consistently better....' ->  '.... and are able to discover networks that ARE consistently better....'\n\n====after author response===\nI would like to thank the authors for the detailed response which resolves some of my concerns about the novelty of this paper. I agree that this is the first work (as far as I know) that brings NAS to unrolled algorithms. Unlike what I initially commented, 'my score is actually between weak acceptance and weak rejection', now I am happy to rate this paper as a weak acceptance.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good submission",
            "review": "This paper studied the structure designing problem for a class of neural networks, i.e., the unrolling networks, or more specifically the LISTA model. Specifically, the authors varied the connectivity patterns and neuron types to define a rich searching space for the network design, and then applied the neural architecture search techniques to fined the best networks. \n\nThe studied problem is interesting and important, and the conducted experiments reveals some insights of unrolling based deep networks, which provides some useful guidelines for further research in this direction. The paper is also well written and easy to follow. Overall, I think this is a good submission.\n\nA drawback, concerning about the experiments, of the manuscript is that it lacks applications. Therefore, a suggestion is to apply the learned network structures to more practical problems, such as image denoising and compressive sensing, or something else, to see whether they are effective in practice.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea but the conclusions of this work are not very significant",
            "review": "Summary of contributions: \n\nIn this paper, authors performed a neural architecture search to improve LISTA algorithm for solving the lasso on synthetic data. The main motivation of this paper is to investigate the effectiveness of unrolling in comparaison with a more \"black box\" architectures. The \"averaged\" top architectures found by the study give better performance than LISTA when the  two models are trained on the same dataset.\n\nStrengths: \n\n-The idea of using NAS to study unrolled models seems to be novel to the best of my knowledge. \n\n-This idea is interesting. I definitely agree that the relevance of unrolled architectures should be questioned. \n\n-The authors did a good job to describe clearly their idea and the experiments they conducted (at the exception of the last paragraph \"transferring found patterns to other unrolling\" which lacks clarity in my opinion).\n\nWeaknesses: \n\n-I think that the overall conclusions of the paper are not very significant. The authors only focus on a very simple problem : the lasso on synthetic data (with known dictionaries). In such a simple setting I do not see why one would consider using this trainable model rather than more standard algorithms for solving the lasso (which are significantly faster than ISTA and have convergence guarantees and/or duality gap).\n\n-I am not convinced by the results regarding transferability. Even if the searched model outperforms the other methods in the simplest setting, the searched model seems to be very sensitive to perturbations. Looking at line (i) of table 1 with perturbed dictionary, the gap between searched model and LFISTA is small and the searched model performs as good as Dense Lista. More generally, the searched model seems to perform as good as Dense LISTA in most of the perturbed settings. Again this question the utility of this model. \n\n-The authors only consider the problem where the underlying optimization problem is known. I think it would be much more interesting to study the case when this underlying optimization problem is not known and has to be learnt (that aspect is briefly discussed at the end of the related work section). For example what if the dictionary is not known ? Lista offers the possibility to learn the dictionaries with end-to-end training. What would be the results of the NAS in the setting with unknown dictionary, how would unrolled models perform in that setting ?\n\nSuggestions to authors: \n\n-It would be interesting to consider the case where the underlying optimization problem is not known a priori. In that situation I think that unrolled models make more sense  (unrolled weighted-l1sparse coding for a simple task such as denoising, experiments with learned dictionaries ...). In that setting I would be more curious regarding the results of the NAS study.\n\n-I think it would be relevant to compare the unrolled models to other algorithms for solving the lasso (lars...). In my opinion, it would clarify the conclusion of the paper. (A comparison in term of inference speed, accuracy, computational cost would be interesting).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}