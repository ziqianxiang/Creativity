{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a model for spatiotemporal point processes using neural ODEs. Some technical innovations are introduced to allow the conditional intensity to change discontinuously in response to new events. Likewise, the spatial intensity is expanded upon that proposed in prior work on neural SDEs. Reviewers were generally positive about the contributions and the empirical assessments, and the authors made substantial improvements during the discussion phase."
    },
    "Reviews": [
        {
            "title": "This paper proposed a continuous space and time process for micro modeling discrete time event sequences. The idea of the paper is very interesting by making point processes leverage the flexibility and tractability of flows in their intensity models.",
            "review": "The paper is generally well written. \n\nSection 3 is a little confusing as it's not readily clear which part of the model is using flows. Or what are the flow parameters that needed to be estimated.  How  will the neural network structure look like? How is it trained?\n\nOne might wonder why a flow based model is used among these many deep generative models? Where does this invertibility help out?\n\nThe GRU model is not well elaborated and is a little unclear.\n\nIt would have been interesting to see where the attention model usually attends to. Either in a real world data set or in simple intuitive toy tasks.\n\nEquation 14 and 19 are hard to follow. It's good to elaborate on them. At least in an appendix section due to space limitations.\n\nWill the log-likelihood sufficient for evaluation? Why not more intuitive tasks like event or time predictions? More explanation and justification would have been helpful.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review for #665",
            "review": "This work investigates a new class of parameterizations for spatio-temporal point processes\nwhich uses Neural ODE to enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. \n\nStrengths: this work is essentially an extension of the Neural Jump SDEs [Jia & Benson (2019)] where the temporal dynamics is modeled as an ODE and the spatial pdf is modeled as a history dependent Gaussian mixture distribution. In this work, the spatial pdf is further extended to an ODE based dynamics. For this purpose, three different continuous normalizing flow models are proposed (Time-Varying CNF, Jump CNF, Attentive CNF). Also, a large number of experiments are conducted and baselines are compared to validate the conclusion.\n\nI recommend rejection at the current stage for the reasons below.\n\nWeakness: A major concern is, if my understanding is right, every mark x^(i) is modeled as an ODE of x^(i)_t on [0, t_i] in the in Time-Varying CNF and Attentive CNF, so there are N (the number of points) ODEs in the model. This setup is problematic because any points except the 1st are impossible to happen at time 0, so they impossibly possess a mark x^(i) at time 0 (in fact, any time before t_(i-1) is impossible). A more reasonable way to characterize the dynamics of x^(i) is to model the ODE on [t_(i-1), t_i] which is used in the Jump CNF. I understand this setup contributes to the parallel computation with the reparameterization trick. In fact, this is reason why both Time-Varying CNF and Attentive CNF can be computed in parallel, but Jump CNF cannot. The Attentive CNF can be seen as a generalized version of Time-Varying CNF due to the introduction of history dependence, but the Jump CNF is a different model as stated above. \nAlso, the jump CNF can model the abrupt change of the spatial pdf but the Time-Varying CNF and Attentive CNF cannot. Theoretically speaking, the jump CNF should have a more powerful fitting capability (assuming other parts are same) compared with those two models. Why does the Attentive CNF model achieve a better or close performance than jump CNF in most experiments? Does that mean the dynamics in most datasets have no discontinuity? Maybe a simple synthetic experiment with discontinuity in dynamics can help prove this. \n\nSome specific concerns: some synthetic data experiments with specific setup (e.g. discontinuity) are needed to give a deep understanding of the two proposed spatial CNF models.\n\nTypo: or-->of, the second line from the bottom in the first page.\n 0-->1, the second line of Eq.(19). ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea; carefully designed models; weak experiments; weak accept ",
            "review": "The paper proposes a neural-ODE-based point process for spatio-temporal data. Under the general framework, three particular variants are proposed: they handle data with different characteristics and have different computational efficiency. \n\nPros: \n\nThe idea is interestingly novel. \n\nThe proposed model architectures are all carefully thought through: each model component is well-motivated, being supported by convincing justification. \n\nThe presentation is clear; some technical parts are enjoyable to read. \n\nThe empirical results on log-likelihood comparison look compelling. \n\nCons: \n\nThe experiments are somewhat weak: this is the main reason I didn’t give a higher score. \n\nFor temporal comparison, there isn’t any neural baseline model. \n\nThere isn’t any prediction accuracy comparison. \n\nEmpirical analysis is very limited (maybe cuz of limited experiments conducted). \n\nQuestions: \n\nThe use of * is really non-standard: in statistics and machine learning, * usually denotes somewhat ground-truth. Can authors think of another notation to omit $H$? Or maybe $H$ can be kept since the single-column format is spacious enough to host long equations? \n\nEqn-(8): why not $\\log p(x | t)$? Is this a typo? \n\nRNN (particularly, LSTM) with continuous-time hidden state was proposed by Mei and Eisner 2017, earlier than the cited Rubanova et al. 2019. \nMoreover, the math properties described by eqn-(10--12) also hold for Mei & Eisner 2017. \nCan authors appropriately acknowledge these connections? \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall this is a good paper, despite some minor flaws.",
            "review": "This paper proposes a novel deep approach to the learning of spatio-temporal point processes via normalizing flows. Overall I think this is a good paper, presenting many interesting ideas and may impact further research on point processes.  The combination of flow-based network structure and the probabilistic model--point process should make sense. The formulation and presentation are good, which makes the paper easy to follow. However, there are a couple of questions for the authors to further address:\n1. It seems that the proposed model contains a jump CNF for the mark probability $p^*(\\boldsymbol{x}_t|t)$. I'm not sure if it really makes sense that the probability of mark has a \"jump\" over time. Here such formulation seems to be problematic. It is straightforward for the ground intensity function to consider the discontinuity at the point when an event occurs, as it represents a (self-exiting/inhibitive) temporal point process. The features are often assumed to be homogenous over time. It would be better if the authors can justify such a formulation.\n2. It seems intractable for the model to predict the next event. To compute the arrival time and mark for the next event, one should consider integrals with respect to $\\lambda$ and $p$, which looks quite complex when they involve ODEs. \n3. The authors are advised to further illustrate the attentive CNF. This seems to be a very interesting topic, but the authors only give a brief introduction in very short content. It is not very clear how the authors incorporate the attention mechanism to CNF. \n4. The experiment looks somehow weak.  First, the authors criticize that KDE has a large entropy (variance?). However, the variance of KDE depends on the kernel bandwidth, it is not fair to judge based on just one prefixed kernel bandwidth. Second, the authors seem to miss a couple of baselines that deal with the same task in the experiment. The NHP and the RMTPP are also able to model the spatial-temporal point process if the losses are change to the metrics on Euclidean space. Besides, please consider \nLi, L., & Zha, H. (2014). Learning parametric models for social infectivity in multi-dimensional Hawkes processes. AAAI 2014.\nLi, T., & Ke, Y. (2020). Tweedie-Hawkes Processes: Interpreting the Phenomena of Outbreaks. AAAI 2020.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}