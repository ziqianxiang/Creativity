{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work demonstrates that autoregressive (AR) models for machine translation can can be competitive with their non-autoregressive (NAR) counterparts in terms of practicality. This is a timely observation, given the flurry of recent work on NAR models, whose primary benefit is often cited to be fast inference.\n\nIt was argued that the results are not surprising -- if this is the case, I still think this work merits acceptance because its thesis runs counter to the direction the field as a whole seems to be moving in, and the results are convincing. That said, I agree with the authors that the observation that some encoder and decoder layers are interchangeable, is not self-evident (i.e. it _is_ surprising). This is of course subjective to some degree, so I am making a judgement call here. The work also has value in that it draws attention to some practices regarding evaluation in NAR machine translation literature that could be improved and made more fair (specifically regarding comparison with AR models).\n\nThere were some concerns about whether these models should be evaluated in the small-batch or large-batch setting. The authors have updated their manuscript in response, and it now explicitly discusses both settings. The authors have also run more experiments and added several additional results requested by reviewers to the manuscript.\n\nAll things considered, I am inclined to follow the majority and recommend acceptance."
    },
    "Reviews": [
        {
            "title": "Deep encoders and shallow decoders for NMT",
            "review": "Summary:\nThe paper proposes deep encoder and shallow decoder models for auto-regressive NMT. They compare rigorously to NAR models. They also study three factors: layer allocation, speed measurement and knowledge distillation. They include that with a 12E-D1 model they obtain significant speed-up and can outperform the standard 6-6 AR model and almost always beat the NAR model in terms of quality. They also show that NAR models need deep decoders because they need to handle reordering.\n\nReasons for score:\nI scored this paper a 9. I think this is an important paper which establishes very strong AR baselines for future NAR work in the field. They correctly point out the three issues with the comparisons that many NAR papers make. They conduct various meaningful ablation studies and validate their various hypothesis properly. They also show that certain factors like knowledge distillation need to be applied to both AR and NAR systems. Finally, they advocate for reporting both S_1 and S_max when comparing speed gains.\n\nCons:\n- One issue I had with the presentation of the results was the selection of different formats and language pairs for different experiments. For example, table 2, 3 and 4 report on different subsets of language-pairs. Same with the figures. This might raise questions of whether the authors are randomly subselecting or selecting favorable subsets. I would have liked to see all experiments done on all LPs.\n\n\nMinor comments: \n- Section 2.1: S_max - \"This is closer to practical scenarios where one wants to translate a large amount of text.\" - this is a very subjective statement and I would tone this down. \n- Section 2.2.2: \"Denote respectively by E and D the numbers of encoder and decoder layers.\" -- please fix grammar\n\nMissing citations:\n- Section 1: Along with Sutskever, Bahdanau and Vaswani. please also cite https://www.aclweb.org/anthology/D13-1176.pdf and Wu et al. 2016 (https://arxiv.org/abs/1609.08144) when you mention state-of-the-art NMT.\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "detailed analysis of current shortcomings in non-autoregressive MT research",
            "review": "The authors advocate for fair comparison between autoregressive (AR) and non-autoregressive models (NAR) in non-autoregressive machine translation (NAT) research. They highlight three main aspects where the comparison has not been fair so far in the literature - suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. They perform extensive comparisons between AR and NAR models in these 3 aspects and report interesting results. \n\nPros:\n- The paper is very well written and easy to follow. Limitations of previous works have been well explained, and the motivation behind their work is clearly evident. The experiments are rigorous.\n- The paper shows that autoregressive models with deep encoders and shallow decoders are comparable and sometimes outperform current state of the art iterative semi-autoregressive models in terms of quality-latency tradeoff. I believe these findings can be valuable to the non-autoregressive MT and machine translation community in general.\n\nCons / Suggestions:\n\n- When doing the S_{max} speed comparisons, I am assuming that the maximum amount of tokens fitting in the GPU memory also depends on the length beam. I will suggest the authors to clarify this, and possibly include an experiment showing what speed-quality tradeoff iterative models achieve with different values of length beam. \n- In Section 2.3, the authors should point out that some previous works like Mask-Predict (Ghazvininejad et al., 2019) do report AR baselines with knowledge distillation. \n- Unless I inferred the results incorrectly, Mask-Predict finds almost no improvement in AR baselines when using distilled data, while in this paper, the authors find an improvement of around 1 BLEU point. Can the authors comment on why this could be the case? Can this  be because of variations in the distilled data used, or some difference in model training. I believe that 1 BLEU point is too big a difference to be accounted for as a noise signal. \n\nOverall, this paper proposes a different direction for research in fast and accurate machine translation, and points out the current shortcomings of research in non-autoregressive machine translation in details. I find findings by this paper very interesting and potentially valuable to the concerned research community, hence I recommend its acceptance. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid experiments but more insight is required. ",
            "review": "** Summary **\n\nIn this paper, the authors present a throughout study on the deep-shallow architecture for autoregresstive machine translation model (AR) and non-autoregressive model (NAR). Authors compare the two types of models from speed measures, layer allocation and the usage of knowledge distillation.\nMany results in this paper are interesting and meaningful:\n(1)\tDeep-shallow architecture (in this paper, 12-1) can achieve good BLEU performances across different tasks and speed up inference.\n(2)\tNAR is not as fast as AR when using S_{max} evaluation (i.e., the inference in batch model, where all GPU memory should be utilized).\n(3)\tKnowledge distillation is helpful for both AR and NAR. \n\nThe experiment results are solid and reproducible. \nThe paper is well written. \n\n** Significance **\nAlthough this is a well-written paper with solid results, I still have the following concerns:\n1.\tTo speed up inference, one can easy get to use a shallow decoder to reduce inference time, since the decoding process in AT takes most of the time. The results do not give me so much surprise. \n2.\tWhy the 12-1 architecture can achieve better results than 6-6 architecture on AR, but worse results on NAR? (see Figure 1 (B)(C)) What roles do encoder and decoder play in different modes (i.e., AR and NAR?) The following terms might be helpful to you:\na.\tThe comparison of their training/validation curves to verify whether the improvement is due to better training/generalization.\nb.\tIn Figure 1, the results are mainly obtained on 6-1, 12-1, 6-6. What if we try more combinations?\ni.\tFix the encoder layer as 6/12 and vary the decoder layers {1,2,4};\nii.\tFix the decoder layer as {1,2,4} and try more the encoder layers?\nIn this way, we can see how encoder/decoder layer effect the results (speed and quality).\n3.\tWhat are the differences between the output of 6-6 and 12-1? Any human analysis? I think you can briefly summarize the discovery in the paper with examples attached in the appendix. BLEU is important but you should evaluate your model from more perspectives. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Compelling argument that well-tuned AR methods are superior to current NAR models",
            "review": "Summary: Traditional NTM is done with a large encoder and large autoregressive (AR) decoder. Due to the sequential nature of the AR decoder, inference can be slow due to lack of parallelism (unless done at very large batch sizes). Non-Autoregressive (NAR) models have been proposed to alleviate this problem, but all NAR approaches trade off some translation quality for speed gains. In this paper, the authors claim that an alternative to NAR is to speed up standard AR decoding by reallocating network weights and layers to the (easily parallelizable) encoder, and making the decoder a single layer. They claim that this matches the speed of NAR models while keeping the performance of traditional AR models, making it a better choice in the design space than any NAR models. Comparisons are made to CMLM and DisCo NAR models to justify these claims with experimental evidence.\n\nPros:\n\n1. The results are fairly compelling. A 12-layer encoder + 1-layer decoder model matches the 6/6 model in translation quality, and, as promised, speedups are comparable to the speedups obtained from NAR methods.\n\n2. The point made about knowledge distillation is also valuable. Knowledge distillation is valuable tool for speeding up models, and it is just as applicable to AR speedups as NAR speedups.\n\nCons:\n\n1. In my view, the comparison of S_{max} is misleading. According to the authors, it \"measures speed when translating in mini-batches as large as the hardware allows. This is closer to practical scenarios where one wants to translate a large amount of text\". However, this is emphatically *not* the purpose of NAR models, which are *specifically* for the low-batch-size case. In addition, I disagree that this \"is closer to practical scenarios\"; many user queries are in fact short sentences or phrases, rather than many paragraph articles. Both use cases exist, and comparing NAR vs AR models on S_{max} is comparing a slightly wrong metric.\n\nThis could be improved by instead plotting speed as a function of batch size, rather than choosing arbitrarily the two extremes. A curve of batch size vs speedup is valuable, as done in Fig 2.\n\n2. I found that Section 2.2.1 and 2.2.2, formal derivations of runtime and number of operations, were unnecessary. To me, these formalisms made the paper harder to understand and parse, rather than easier.\n\nThis could be improved by reducing these to key points: (a) encoders are parallelizable (b) T is a significant factor in NAR models, and is around 4-10 for good performance (c) AR decoders are more robust to using fewer layers than NAR decoders, etc.\n\n3. Figure 1 is confusing.\n\nI think this could be improved by converting it into a table with a \"NAR\" section and an \"AR\" section, and each row being a model.  \n\nRecommendation: Accept. Although I had some criticisms of the paper, they pertained mostly to clarity, style, and precise definition of chosen metrics; the key results of the paper are solid. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}