{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "There is growing evidence that optimized deep networks (typically dense in the sense of nonzero parameters) often contain sparse sub-networks that can be trained from scratch to achieve similar performance as the full network. Such “skeletonization” is of obvious importance, given the rate at which deep networks in practice are increasing in size. However, many approaches to find such optimal sub-networks train the full model (and hence implicitly, the intermediate sub-networks as well), which is not a scalable path. \n\nSome recent works show that skeletonization at initialization may provide all the efficiency benefits of sparsity, while minimally impacting accuracy. This work first notes that accuracy in such approaches degrades significantly beyond a certain level of sparsity (around 95%). One of the ideas of this work is to resurrect parameters that were pruned away earlier in this work’s iterative skeletonization, via an approach called foresight connection sensitivity (FORCE) where the “trainability”  of the pruned network is also taken into consideration. An additional idea is “Iterative SNIP”, building on the SNIP approach of Lee et al. (2019). The empirical improvements, and the observations showing the limitations of SNIP and GRASP in the regime of high sparsity, are useful. \n\nThe evaluation and overall contribution were generally appreciated by the reviewers. \n"
    },
    "Reviews": [
        {
            "title": "Paper that proposes iterative versions of techniques to prune at initialization, allowing for sparser networks than SNIP/GRASP",
            "review": "# Summary\n\nThe paper finds that at extreme sparsities (>95%), existing approaches to pruning neural networks at initialization devolve to worse than random pruning. The paper posits that this degenerate behavior is due to the fact that weights are pruned in groups, though the saliency metrics only capture pointwise changes. The paper presents a modified saliency metric based on SNIP, allowing for calculating salience of partially pruned networks; this in turn allows for applying an iterative version of SNIP, as well as a variant of iterative SNIP that allows for rejuvenation. These pruning techniques are evaluated, showing that they maintain accuracy at high sparsities.\n\n# Strengths\n\n- Well-motivated: SNIP and GRASP's drops in accuracy at high sparsities is indeed surprising and worth addressing\n- Discussion of SNIP/GRASP and of Iter SNIP / FORCE is very clear\n- Evaluation of methods is strong, evaluating across different target sparsities, different networks/datasets, and robustness to hyperparameter choices\n\n# Weaknesses\n\n- A discussion/analysis of the differences in behavior between FORCE and Iter SNIP would be warranted, since in the main body of the paper they seem fairly similar in the results. Some discussion of this is presented in App. C4, but this feels a bit lacking in conclusions. Should I always choose FORCE over Iter SNIP? Is there a reason to present Iter SNIP at all?\n- The paper also needs more conceptualization and baselines for what it means to prune \"at initialization\". For VGG, FORCE uses $128\\cdot 300 = 38,400 \\approx 1 \\text{epoch}$ of examples to compute the mask. Learning a mask is in fact a form of training [1]. With seeing this number of examples before computing the mask, it is plausible that a better mask could be found through other simpler means (e.g., training a network and magnitude pruning), or more complex means (e.g., [2] Figure 5 shows a VGG-19 with higher accuracy from a mask on a network that saw 6,400 examples; though [2] doesn't propose a constructive technique, it seems worth considering as an upper bound). I believe the paper would benefit from a more through discussion of what \"at initialization\" actually means, and what appropriate lower/upper bounds for techniques that see large numbers of examples should be.\n\n# Overall recommendation\n\n7: Accept\n\n# Other comments and suggestions\n\n- Small typo: \"We\" is capitalized in the middle of the first sentence of Section 5.\n- Figure 4 middle: the x-axis is very hard to read. Figure 4 right: is there a difference between \"batch\", \"b\", \"iter\", and \"it\"?\n- Figure 4 left: would it be possible to also include the saliency obtained from the FORCE metric in this plot?\n\n# References used in review:\n\n[1] Arun Mallya, Dillon Davis, Svetlana Lazebnik. \"Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights\".\n[2] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin. \"Stabilizing the Lottery Ticket Hypothesis\".\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper, nice problem fromulation",
            "review": "Summary:\n\nThis paper presented a new sensitivity-based pruning methods to cut network connections at initialisation. The proposed method FORCE shows great performance when compared to rencelty proposed SNIP and GRASP. In general, I liked the formulation of the problem, its in-depth analysis and detailed comparison to existing approaches. I see clear contribution in this paper to the field of pruning at initialisation and it is a clear accept from me.\n\nStrength:\n1. The paper comapres to very recently proposed pruning methods (SNIP and GRASP), and shows a great accuracy increase at very aggressive pruning levels.\n2. The paper discussed shortcomings of previous methods in details, and clearly differentiated their proposed method in Section 4. The novelty of this paper is clear to me.\n3. The paper is well written and easy to follow.\n4. The paper has some interesting insights (mask zeroing, exponential scheduling, etc.) and the proposed iterative pruning at initialization is easy to apply in practice. These might add knowledge to the network pruning community.\n\nWeakness:\n1. The paper only investigates, to my understanding, ‘fat’ models. Both VGG19 and ResNet50 cannot be considered as efficient model architectures. It might be more convincing for the paper to test on recently proposed efficient network architectures such as the MobileNet family.\n2. Although the paper has a potential contribution to theoretical understanding of pruning, I would like to argue aggressive pruning at initialisation cannot guarantee model’s post-training performance, and it might limit the use of this technique in reality.\nFined-grained pruning like proposed in this paper is hard to bring any real performance gains in today’s GPUs, especially when it is applied with a mask. \n\nDespite of the weaknesses mentioned above, in general, I think this paper is well-motivated and shows a clear contribution to the community, so it is a clear accept from my side.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid and interesting paper that further pushes the limits of initialization pruning",
            "review": "Summary: Author observed that beyond a certain level of sparsity, existing pruning methods hat pruning at initialization performs even worse than random pruning. The author proposed FORCE and Iterative SNIP to handle this problem. These methods consistently significantly outperforms other methods on this problem.\n\nContributions: A new problem found in pruning. New effective methods proposed by author.\n\nWeaknesses: \nThe author only shows that his methods works better than SNIP and GRASP at super high sparsity. We wonder how it performs when sparsity is not that high.\nSuper high sparsity problem for methods pruning at initializations is a relative small problem. The importance of this problem is weak, although author improved performance in this case. I wonder whether this is truly useful in practice.\n\nThe writing of the paper is good. It is easy to follow the paper.\n\nIt is a narrow area, but the paper is deep and through.\n\nThe Theorem 1 in the appendix seems solid, but it is acutually based on previous paper. Just a litte bit modification on the gaint's shoulder.\n\nBut overall, it is still a good paper, though the area may be very narrow.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}