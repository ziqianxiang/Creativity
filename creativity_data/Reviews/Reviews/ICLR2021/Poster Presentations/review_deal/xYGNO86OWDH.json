{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a broad exploratory analysis of the geometry of token representations in large language models, with a focus on isotropy and manifold structure, and reveals some surprising findings that help explain past observations.\n\nPros:\n- Clear and surprising analytical findings concerning a broad and widely-used family of models.\n\nCons:\n- The paper is a fairly broad exploratory analysis, with no single precise claim that ties together every piece of the work.\n\nI thank both the authors and reviewers for an unusually productive discussion."
    },
    "Reviews": [
        {
            "title": "Thorough, thought-provoking, a bit unfocused and inconclusive",
            "review": "#### Summary:\n\nThe authors investigate the token embedding space of a variety of contextual embedding models for natural language. Using techniques based on nearest neighbors, clustering, and PCA, they report a variety of results on local dimensionality / anisotropy / clustering / manifold structure in these embedding models which are of general interest to scientists and practitioners hoping to understand these models. These include findings of (local) isotropy in the embeddings when appropriately clustered and shifted, and an apparent manifold structure in the GPT models.\n\n#### Reasons for score:\n\nThis is a generally thorough and well-executed paper, and a careful examination of these embedding models is of great general interest to the ICLR community.\nHowever, I feel the analysis is a bit shallow at certain points and consists of reporting (interesting) findings without necessarily delving deeper or adequately explaining them.  I think this paper presents a great jumping off point for further research on the subject, as it certainly raised quite a few questions with me. I support its acceptance but would hope to see the authors address some of the questions raised here.\n\n#### Positives:\n\n- Lots of analysis with several different techniques\n\n- Very interesting and relevant subject area\n\n- Thorough use of recent related work\n\n- The paper is quite well written and organized considering the inherent challenge of writing up research of this sprawling nature.\n\n#### Concerns / Comments:\n\n- Noting the very different behavior of GPT from the other representations — could this be due to the learned positional encodings? This might also explain the Swiss-roll style paths seen when examining the different embeddings of the same word types. Could position along those curved paths be correlated to sentence position of the tokens?\n\n- In this spirit, it would be great to get a better characterization of what the different clusters correspond to, especially in the case of GPT. Could there be a better investigation of the relationship between predictive / causal and non-causal models and these clusterings? \n\n- More generally, the “Swiss roll” observation is intriguing, but since it only appears in one family of models that has a very similar (transformer) architecture to models in which it does not appear, what are we to make of it?\n\n- How can the intrinsic dimensionality at each layer increase with depth? Considering each layer as living on a manifold, the transformation at each layer should act as a coordinate chart for the next layer’s manifold, which should only allow a reduction in dimension. Unless I am missing something, that suggests that these estimators do not have enough samples and/or are measuring something different than the dimension of a manifold. Section 4.4 should explain how this could be happening. \n\n- More generally, I find the low LIDs in GPT hard to understand or interpret without more analysis, and would also like to understand the very large first dimension of the GPT models.\n\n- It would be great to see more suggestions / takeaways for practitioners. What, if anything, does local (an)isotropy imply for deep learning researchers doing work in this area?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A paper of limited novelty, which could be improved by deeper analysis.",
            "review": "Findings:\n- Reproduces various existing findings about anisotropy of contextual representations viewed globally.\n- Contextual representations are highly isotropic within clusters of the representations. \n- GPT representations follow a Swiss Roll manifold, where the most frequent words appear at the head and less frequent words are gradually appended at the bottom.\n- The Swiss Roll manifold is taller in deeper layers.\n- BERT representations fall in a Euclidean space.\n-  The Local Intrinsic Dimension of contextual embeddings is lower than for unigram embeddings.\n\nPros:\n- The manifold analysis of word frequency is intriguing and intuitive.\n- The explanation of experiments was clear in each section.\n- They produce compelling evidence that the global token-level anisotropy of these representations is largely due to membership of large clusters. This is a valuable contribution because it explains previous findings in Ethayarajh 2019 and reconciles them with theoretical expectations.\n\nCons:\n- In Section 2.3 and Section 3.1, the paper gives insufficient credit to Ethayarajh 2019. As far as I could tell, every initial result is a reproduction of a result from Ethayarajh 2019, and the methods are very similar.\n- Though they acknowledge it, the methodology is largely taken from existing unigram embedding analysis.\n- Once they start to identify very well defined clusters, I was very curious about the distinctions between the islands. It would not be difficult to inspect some of the data by hand, so I don't understand why the authors didn't try.\n- The authors offer no analysis for the difference in behavior between different models. I felt like I was reading a taxonomy, and the plots were left for the reader to connect. The authors have presumably spent quite a while thinking about these geometric structures and models, so surely they have conjectures about the behavior they observed or hypotheses they can test.\n\nMinor / style:\n- I didn't realize until the conclusion that your main finding was an explanation of existing claims about anisotropy by considering behavior within the clusters, so that needs more emphasis.\n- The papers you cite do a decent job of explaining why isotropy in the representation space is significant both token- and type-wise. The paper would be a lot more readable if you made a similar effort in explaining background.\n- There is inconsistent use of \"isotropy\" vs \"isotropicity\".\n- Citations needed for claim \"widely believed that the contextual space is so weird\"\n- Needs proofreading for minor grammar and typos (e.g., downstreaming instead of downstream, could resides instead of could reside)\n- Instead of referring variously to high similarity or cosine, silhouette scores, and other measurements of isotropy, it might be clearer to link each concept to isotropy once, and then each subsequent result simply refer to it as isotropy while mentioning the metric. Then the reader doesn't have to constantly remember which metric indicates high isotropy as they read the results.\n\nQuestions:\n- The authors claim to select the clusters that maximize MMS. I read the wording to imply that this optimum is tractable/stable. Is that the case?\n\n(Number rating was updated from 3 to 7 in light of substantial expanded experiments and analysis.)",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An exploratory analysis of contextualized embedding geometry",
            "review": "This paper analyzes the geometry of several contextualized embeddings. The authors show that global anisotropy is caused by strong clustering of word vectors, and that vectors of different word types are isotropically distributed within the cluster.\n\n**Strengths**\n- This work is a nice-to-have extension of [Ethayarajh (2019)](https://www.aclweb.org/anthology/D19-1006.pdf) that dives deeper into the geometric properties of contextualized vectors.\n- The research question is clearly stated (Why doesn't anisotropy hurt performance?) and clearly answered (There's no anisotropy locally).\n- The 3D visualizations provide a better geometric intuition than the flat visualizations that are common in this kind of papers.\n\n**Issues**\n- I don't think that good performance _contradicts_ anisotropy. For example, we already know that the classical static embeddings are also anisotropic [(Mimno and Thompson, 2017)](https://www.aclweb.org/anthology/D17-1308.pdf), and this means that good performance (as measured by downstream tasks) may co-exist with anisotropy. So please consider rewording the beginning of Section 1.2. For example, instead of \"There is an apparent contradiction.\" consider \"It is not clear why ...\"\n- How _representative_ is one random sample from $\\Phi(t_i)$ for measuring $S_\\text{inter}$ in formula (1). You gave an example in the Introduction when the same word type (bank) can have totally different meanings depending on context, and thus (I believe) the corresponding $\\phi_1(\\text{bank})$ and $\\phi_2(\\text{bank})$ may be totally different. Why not taking more samples for polysemous words?\n- Why do you use different distance metrics (Euclidean vs cosine) for estimating LIDs of contextualized vs static embeddings (Table 3)? \n- \"For GPT2, we had hoped to find that some types are associated with one cluster and other types are associated with the other cluster, but that is not verified in our experiments\" -- I think you should look at contexts rather than types (since you're dealing with the contextualized embeddings). It would be interesting to see whether you have the same type in both clusters, and then to look at its contexts. I bet that the contexts will differ.\n\n**Minor issues**\n- \"We find a low-dimensional manifold in GPT/GPT2 embeddings, but not in BERT/DistilBERT embeddings.\" -- but your LIDs are low for BERT/D-BERT layers as well! Why can't you claim the low-dimensionality for BERT/D-BERT embeddigs?\n- I doubt that PTB with 10K vocabulary size gives \"good\" coverage in 2020. You may simply state that this a widely-used dataset.\n- Wiki2 (Merity et al., 2016) is usually referred to as _WikiText-2_.\n- Please consider rephrasing \"experiments\" -> \"analysis\", as you are not conducting _controlled experiments_, but rather performing exploratory analysis of the embeddings.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}