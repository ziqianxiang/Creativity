{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The main contribution of this work is introducing large and carefully curated datasets for benchmarking morality judgments of language models. First of all, I'd like to thank the reviewers for their detailed and thoughtful reviews and for being engaged in discussions with the authors. We believe that the paper is now much stronger than the initial submission.\n\nThe reviewers judged this work as important and largely well-executed.  Some of them have initially raised concerns that the claims are too bold but these seem to have been addressed in the revisions and the rebuttal. R4 is still concerned that the ICLR format is not suitable / optimal for presenting a dataset. While we agree that journal format could be more suitable for this work, we do not see that as enough reason to reject the paper, especially given that the author invested much effort in providing extra details about the annotation and the underlying theories. \n There are also suggestions to expand error analysis but this also seems to have been mostly addressed.\n\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "Important and interesting work, but paper could be improved",
            "review": "This paper presents an interesting data set aimed at testing neural language models’ capability for “natural language ethics” -- determining which natural language statements are more ethical than others.  It’s an interesting and important task and the paper includes a useful data set that will probably see broad adoption.  However, I feel like the current focus of the paper centers on how to build a dataset that is consistent with existing philosophy of ethics, rather than studying the strengths and weaknesses of neural models applied to the task.  As a result it may not be ideally suited for the ICLR audience.  I do feel like the paper could be improved through more clarity and analysis in the experiments, and dialing back at least one claim.\n\nThe dataset construction appears to follow well-established subcategories of ethics, including questions for each subcategory.  The paper makes a convincing case that it covers a wide variety of ethics, although I lack the background to independently verify that.  The construction is very clearly described, with prompts and examples provided for each subcategory.\n\nThe experiments are not described in enough detail.  How many examples were used for fine-tuning (and few-shot operation), in both the Test and Hard Test cases?  This kind of detail should be in the paper body, not the appendix, although I couldn’t find it stated clearly in the appendix either (is the dev set the training data used for fine-tuning?).\n\nWhile the paper constructs an interesting, important data set, and evaluates a number of powerful models, it includes almost no discussion or analysis of model performance.  I would really appreciate experiments that give more insight into what aspects of the problem the models can solve, and which aspects remain difficult, and why.  \n\nInstead, the discussion section after the experiments focuses on previous approaches to machine ethics and how this work differs from that (it is more of a related work section than a discussion section).  The paper and this section in particular can be a bit grandiose which I think gets in the way of the paper’s contributions, claiming e.g. “Our work is just a first step that is necessary but not sufficient for creating ethical AI.”  I don’t believe this paper makes the case that it’s necessary for creating ethical AI (that would be an amazingly high bar to clear; I believe the paper is helpful for creating ethical AI, but “necessary”?).\n\nMinor questions/concerns:\n“utilities are defined up to an offset of a conic transformation” -- I did not know what this meant or why it was true; it seems like a classical result, a citation would help.\nThe fact that I don’t know which data was used for fine-tuning also leaves me a bit confused about the adversarial filtration.  My understanding of adversarial filtering is that it is typically used to filter down an entire data set, and then that single filtered data set is later randomly partitioned into train/test splits.  So the filtering is adversarial, but the train/test split is not.  By contrast this paper seems to choose an adversarial train/test split, which seems more limited (it breaks the assumption that train and test are iid from the same distribution).  More clarity on this would help.\n“this is the first work we are aware of that uses empirical data to inform notions of fairness, ” -- had a hard time understanding, could you say more what you mean by ‘inform’",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful dataset but needs more detailed discussion of the results",
            "review": "I appreciate the work the authors did by collecting a large dataset that can be used as a benchmark of ethical assessment across different moral concepts. The strong side of this work is its connection to the well-established ethical theories and a careful design and discussion of potential limitations of the dataset (e.g. cultural differences and ambiguous judgements). This dataset would be a valuable source for the further research steps in ML ethics if it becomes available for the community.\n\nHowever, there are certain weaknesses in the paper. The results discussion seems not strong enough and more detailed analysis of the results would help this paper a lot. It is not clear what conclusions can be made about the existing models in terms of their ethical performance. Are the models ethical already or not that much? Is the size of the training data and the number of parameters the only/most important factors that affect models' ability to assess ethics? What about differences in architecture and/or input representation? When do models make mistakes, are those mistakes random, are they model-specific?\n\nThe authors claim that larger models are significantly better than smaller ones but do not report variances of performance and/or results of statistical tests. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work and data but paper needs to be improved",
            "review": "Summary: The authors present a large and thoroughly constructed dataset, containing various types of data points, spanning major aspect of ethics. The dataset is constructed based on deep and “old” human understanding of ethical concepts, taking into consideration more modern aspects of building datasets, such as adversarial filtration. They make various claims about how such a dataset can benchmark AI models with regards to their ethical “understanding”. Furthermore, they use this dataset to fine-tune several language models and evaluate the performance of these models on the datasets, showing interesting and promising performance of these models.\n\nStrengths:\n+ The most significant strength of the paper is making available to the community a dataset which I find very important. Although, as I write below, I have some concerns about the claims made regarding how one would use it, I believe the potential benefit of such a dataset is very high and I would be happy to see it being released to the community.\n+ The methodology of constructing the dataset is well thought of, and in general I very much agree with the authors’ claim that “Computer scientists should draw on knowledge from this enduring intellectual inheritance”, or in other words, not re-invent the wheel.\n+ The authors also do a good job in establishing a first use of the dataset in the way of evaluating current language models.\n\nWeaknesses:\n- My main concern is that it is not completely clear to me how the authors suggest using the dataset for developing AI that is more ethical. I can clearly understand that one can use it to train an auxiliary model that will test/verify/give value for RL etc. I can also see that using it to fine tune language models and test them as done in the paper, can give an idea of how the language representation is aligned with or represents well ethical concepts. But it seems that the authors are trying to claim something broader when they say ““By defining and benchmarking a model’s understanding of basic concepts in ETHICS…” and “To do well on the ETHICS dataset, models must know about the morally relevant factors emphasized by each of these ethical systems”. It sounds as if they claim that given a model one can benchmark it on the dataset. If that is the case, they should explain how (for example say I develop a model that filters CVs and I want to see if it is fair, how can I use the dataset to test *that* model?). If not, I would suggest being clearer about the way the dataset can be used. \n- In addition, I personally do not like using language such as “With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge.” Or “By defining and benchmarking a model’s understanding of basic concepts in ETHICS, we enable future research necessary for ethical AI”. I think that even if a model can perform well on the ETHICS dataset, it is far from clear that it has understanding of ethical concepts. It is a leap of faith in my mind to conclude from what is essentially learning a classification task to ethical understanding. I would like to see the authors make more precise claims in that respect.\n\nRecommendation:\nI vote for accepting this paper, at its current state marginally above threshold but provided some clarifications, I find this a clear accept. I think the area of ethical AI is important, releasing a well-constructed dataset is an important step forward and overall this paper should be of interest to the ICLR community.\n\nQuestions and minor comments:\n1.There are missing details about division to train and test sets, numbers as well as how the division was made (simply random? Any other considerations?). These details should be added.\n2. In the Impartiality section there is missing reference to Fig 2 – it is given only later so one does not see the relevant examples.\n\nPost-rebuttal comments:\nMy concerns are resolved. I have changed my vote to acceptance. (7).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important Questions, but Implicit Assumptions and Instructions limit Utility of the Dataset",
            "review": "Disclaimer: I'm going to mention the word \"asshole\" and phrases that include that word in this review, but I'm not going to use it.  The authors use a dataset based on this concept but do not mention it explicitly.  However, I think it's important to mention it because it's an important distinction with the defined goal of creating ethical AIs.\n\nOverview\n=========\n\nTo be clear, despite the framing of the introduction, this is a dataset paper.  The dataset tackles many tasks and, as we'll discuss in the review, perhaps too many.  However, there is an *incredible* amount of effort here, and the writing is clear.\n\nThe paper uses classification tasks to test whether models build on large language models can encode ethical and utilitarian judgements.\n\nPros: Important topic, sound models\n\nCons: Important unstated assumption of models, not enough space to fully address each of the many tasks\n\nTechnical Soundness\n=========\n\nI see no major issues with technical soundness.  Paper does everything that it should: reasonable models, baselines, and best practices in crowdsourcing.\n\nClarity\n=========\n\nThis paper is very clear in what it says: it's well written, well organized, and has a compelling motivation.\n\nA (Lack of a) Theory of Justice\n=========\n\nThis is not something that usually comes up in a machine learning review, I worry about the foundation of the dataset both from the theoretical framing in the paper, in the selection of datasets, and in the MTurk instructions.\n\nFor example, in the \"impartiality\" collection form, the instructions merely state \"reasonable\" vs. \"unreasonable\".  This is clearly insufficient; Turkers are worried about having their work accepted, so they'll use their mental model of what requesters want.  As a result, I suspect that this will avoid any truly tricky issues where reasonable people can disagree.\n\nOur conception of justice is based on assumptions: Rawls has a game theoretic formulation based on being on either side of the veil of ignorance, while Justinian wanted to ensure the dominance of the Christian church (both cited in this paper).  They lead to different conclusions.  For the impartiality task, Rawls would deem \"While working the voter registration desk, I didn't allow a man to vote because he was born a pagan\", while Justinian would accept it.\n\nSo how does this omission affect the dataset?  Given the power imbalance inherent in crowdsourcing, it forces the crowdworkers to guess the requester's values/axioms and try to match them.  The issue is in trying to square this with the high agreement rates.  I suspect that this means that there are no \"difficult\" scenarios.  (E.g., \"I stopped working for MegaCorp because they got a defense contract\").\n\nOne way to resolve this is to explicitly state the values you want to optimize, e.g. using an inventory like:\n\nDeveloping a meta‐inventory of human values\nAS Cheng, KR Fleischmann\nProceedings of the American Society for Information Science and Technology\nhttps://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/meet.14504701232\n(e.g., Assume you are an American who embraces multi-culturalism, equality, etc. ... given that, write a scenario)\n\nAm I the Asshole?\n=========\n\nWhile one should ask this question frequently while writing an academic review, I found the omission of this phrase from the paper slightly problematic (although I understand why the authors might want to avoid using the word \"asshole\").  But, like above, it gets to the problem of assumptions.\n\nBy adopting the conventions of the AITA subreddit, the authors are essentially outsourcing the moral compass of future AI systems to a Reddit subcommunity.  I don't think this paper has been written, but what would a life philosophy look like if one followed the tennets of \"don't be an asshole\"?  I suspect that it would be a good life, but not one fit for beatification.\n\nThe Reddit subreddit licenses behavior that is unkind (e.g., complaining) if it is situationally appropriate (you're a customer in a shop).  It also excuses sins of omission (e.g., not knowing a sensitive topic of conversation when discussing something with a coworker).  MASSIVE DISCLAIMER: I draw these conclusions from anecdata, I could be wrong about this!\n\nI think these are interesting questions, but the lack of space devoted to these questions (it could be its own paper) might let someone believe that the answers in this dataset are \"correct\".\n\nThe Utility of Utilitarianism\n=========\n\nThis seems out of place in the paper and oddly named.  Utilitarianism is about maximizing *societal* utility, while the MTurk instructions are about optimizing an *individual's* utility.\n\nWhile it may be useful to know an individual's utility function, these are situational and dependent on the person (although instructions assume a \"typical US person\", this is underspecified ... mean, median, or modal).  While I might prefer an ice cream cone to hot cocoa on a warm day, the ranking reverses.  Similarly, different people value different things differently (e.g., the value discussion above):\n\nCan We Measure the Marginal Utility of Money?\nJames N. Morgan\nEconometrica, Vol. 13, No. 2 (Apr., 1945), pp. 129-152\n\nOriginality and Significance\n=========\n\nDespite these concerns, the paper if focuses on important questions.  Unfortunately, the ICLR format and length constraints limit the ability of the authors to fully expand on these important questions.  I worry that if this paper were accepted, it would bake in the assumptions made in this paper to future work on AI ethics.\n\nMinior Issues\n=========\n\nCheck that acronyms and capitalization is protected in Bibtex: distillbert, Albert\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}