{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Most of the reviewers agree that this paper presents an interesting idea. Practically implementing a BNN that gains real world speedup is challenging, and as past work [1] showed, the bottleneck could shift into other layers(besides the accumulation). The paper would benefit from a thorough discussion about the practical impact in implementing the proposed method and relation to past work. \n\nThe meta-reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments.\n\nThank you for submitting the paper to ICLR. \n\n[1] Riptide: Fast End-to-End Binarized Neural Networks\n"
    },
    "Reviews": [
        {
            "title": "Good paper; however overheads regarding $f$ needs to be quantified",
            "review": "## Summary:\n\nThe paper introduces a new neural network layer that enables training NNs with quantized activations using reduced bit-width accumulators. The cyclic activation layer makes overflows smooth, instead of being discontinuous and this enables achieving better accuracy on quantized networks on reduced bit-width accumulators. They also introduce overflow and carry penalties to dissuade the training regime from reaching overflow states.\n\n## Strengths:\n* New NN layer (operator), cyclic activation layer that makes quantized accumulated outputs smooth when overflow is present. The authors show that even at large overflow rates, the network accuracy does not degrade that much.\n* Introduction of the overflow penalty to dissuade overflow and together with the cyclic activation layer achieves higher accuracy even at high overflow probabilities.\n* Evaluation using both vector instructions and carry penalty when vector instructions are not present; they show that carry penalty mitigates carry contamination leading to a closer accuracy compared to their vector implementation.\n* Hardware analysis of possible cost savings for reduced accumulator implementations in a MAC unit.\n\n## Weaknesses and Questions for authors:\n\n* Intuitive explanation as to how $f$ was designed is missing.\n* Isn’t computing $f$ need to be done in full precision or in higher precision than the accumulator? For instance say $k = 8$, then computing $k \\times 2^{b-1}$ needs $b+2$ bits to compute. What is the overhead of computing $f$ in higher precision? Based on the results from section 4.5, this overhead may not be that much, but still needs to be quantified. \n* Also, the overhead of computing $f$ varies with $k$. The same goes for hardware analysis. A quantification is needed to understand the tradeoffs.\n* I assume that loss function with regularizers is computed in full precision. Correct me if I am wrong.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea on an important issue",
            "review": "This paper explores to solve an often ignored issue in quantization: accumulation precision. As the bit-width of input scales down, the area/energy cost of the accumulator starts to dominate.  The cyclic method proposed by the authors at the first glance is not intuitive. However, it's surprising that the surveyed models could be tuned to live with significant overflows---as long as it can be tuned, which is enabled by the \"differentiable overflow\" brought by the cyclic method.  There are several issues to be addressed before the paper can be accepted:\n\n(1) In the equations of page3,   the boundary in the 2nd line of f(m) has a 'c(zq)',  is that a typo?\n\n(2) Since the paper only covers CIFAR10 and some ImageNet works that are on the easier side to quantize, such as ResNet18 and VGG, the cyclic method could meet its limitation on ResNet50 and Mobilenet. The authors didn't discuss an important concept: accumulation length. As the accumulation length increases, the events of overflow could rise sharply, and the training could fail without room for the cyclic method to optimize the slope k.\n\n(3) Some comments on the relation between the accumulation length and bit-packing would also be helpful. For example, if the accumulator with 8-way bit-packing is working on the same GEMM, the accumulation length would be reduced by 8---that would be desirable. Although, a higher level reduction would be required then.\n\nIn general, the paper is well written and brings attention to the important topic of accumulation for reduced precision inference. The paper attempts to solve the overflow problem, although not perfectly, with a differentiable \"failure\" approach. The paper provides great hardware insights for hardware/software co-design. Therefore, I recommend the paper to be accepted on the condition that the authors could address my comments fairly.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Original work that allow efficient low-precision accumulators in inference via a novel cyclic activation function.",
            "review": "This paper presents a method (WrapNet) for the problem of efficient low-precision inference. The main contribution is to allow efficient low-bit (e.g., 8 bit) accumulators via the use of a novel cyclic activation function that constrains the value space of the layer outputs, while still allowing good accuracy.\n\nPros:\n\nThe paper is clearly written, although some training details would be nice to see in the main paper instead of the appendix.\n\nCyclic activation functions have been used in other works, but this work is to my knowledge among the first to use a cyclic ReLU and most likely the first to use it in order to reduce the output value space in order to do more efficient accumulation in inference hardware.\n\nThe significance of this work lays in the fact that this is the first paper that allows lower precision accumulators while still demonstrating quite good performance, even in the case of demanding Imagenet classification. The low-power and low-latency inference are of key importance for more widespread use of neural networks in e.g., low-power devices. Actually the performance is almost as good as with other low-precision schemes, which use high-precision accumulators. In addition, the paper has quite extensive study and analysis of performance in various hardware architectures.\n\nCons:\n\nThe method introduces various additional hyperparameters, e.g., regularizer coefficient, transition slope, etc. Although these can be tuned with a validation set.\n\nIn results Table 6, why we are not given results that would be comparable to BWN-QA and TWN-QA. For instance, WrapNet with 3 bits/4bits activation and e.g., 12-16 bits accumulator.  \n\nSome instability of training seems to happen with different initial overflow rates.\n\nThe training setup is relatively complicated  (layer-by-layer training) when using carry variance reduction regularizer.\n\nTypo maybe in page 2 in \"reference (fbg, 2018)\"\n\nEdit: I am happy with the author responses and have raised the score accordingly to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An Empirical Parameter Adjustment Scheme",
            "review": "Summary:\n\nThe authors propose a scheme named WrapNet to further reduce the bit width of accumulation operations in the deep neural network inference process. Through a novel cyclic activation and regularizer penalizing overflow, the proposed method can be implemented in processors with or without specialized vector instructions using less time, area, and energy cost. \n\n\nStrength:\n\n--The manuscript is presented well and quite easy to follow.\n\n--The overall idea makes sense and the proposed method can reduce computational resources and speed up computing greatly in the accumulation operations. \n\n\nWeakness:\n\n--The step-size selection strategy is more like an empirical parameter adjustment, lack of a uniform selection strategy. \n\n-- The results are not so convincing due to the lack of experiments on large-scale models like ResNet50 on ImageNet.\n\n\nComments:\n\n(1) The step-size selection strategy seems trying to balance the quantization error and the overflow rate. However, it is more like an empirical adjustment. When the DNN model and the dataset changes or there is a new task, how can it be used in new scenarios because the overflow rate may differ a lot? I think there is a lack of a uniform selection strategy.\n\n(2) The batch normalization layer may change the distributions of weights and activations. In fact, there are some works which have successfully remove batch normalization with comparable performance. And removing batch normalization can better accelerate the DNN computing. In a network without batch normalization, the overflow rate may differ from DNNs with it. Can you do some experiments for comparison and give a detailed explanation of the proposed method used in the DNN model without batch normalization.\n\n(3) The model is pre-trained with quantized weights and full-precision activations. Can the proposed method be applied into the model from scratch given the estimated step size? And how it performs?\n\n(4) It’s better to give the performance of the proposed method on ResNet50, ImageNet to make the results more convincing. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}