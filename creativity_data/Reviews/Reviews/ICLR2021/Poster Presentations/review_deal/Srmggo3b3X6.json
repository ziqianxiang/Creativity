{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper offers a new take on generalization, motivated by the empirical success of self-supervised learning.  Two reviewers found the contribution novel and interesting, and recommend acceptance (with one reviewer championing for it).  Two reviewers remain skeptical about the value of the paper, and the authors are encouraged to add a discussion about the points made in these reviews.\n\nI agree with the positive reviewers and would like to recommend acceptance."
    },
    "Reviews": [
        {
            "title": "a new notion of generalization, with motivation and some experimental justification",
            "review": "This paper gives a new perspective on generalization, motivated by the success of self-supervised learning, especially on noisy data. They view the generalization error as consisting of 3 independent components: robustness, rationality and memorization. Informally, robustness measures the degradation in training accuracy due to the addition of noise. rationality measures the gap between noisy training and test accuracy and memorization is the gap between the training error on the uncorrupted and corrupted examples. The main point of the paper is that the memorization gap is smaller for self-supervised, simple algorithms compared to full supervised algorithms. They prove that when the noise is defined by a small fraction of labels being randomly flipped, then the memorization error of such simple algorithms can be bounded in terms of their information-theoretic complexity, independent of the representation they produce. (The proof is simple once the definitions are set up properly).\nThe most interesting part of the paper is an experimental study of several training procedures on benchmark data sets, measuring the three notions of error, matching what their theoretical result and overall motivation suggests. While perhaps not directly relevant to the practice of ML, this paper gives some explanation of the success of self-supervision in a toy setting, and is likely to inspire further research.\n--- You study random label noise. What about attribute noise? And what is the noise is not random but adversarial? Do at least the notions still make sense?\n--- the rationality gap is still confusing, and your dog/cat example does not help. More intuition/better examples would be great.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Innovative paper with clear presentation",
            "review": "The present paper aims to understand the generalization capability of self-supervised learning algorithms that fine-tune a simple linear classifier to the labels. Analyzing generalization in this case is challenging due to a data re-use problem: the same training data that is used for self-supervised learning is also used to fit the labels. The paper addresses this issue by implicitly conditioning on the training covariates x and then deriving generalization bounds that depend only on (hypothetical) noise to the labels y. The paper show that, empirically, the dominant factor in generalization error is a certain quantity called the \"memorization gap\", which can also be upper-bounded via theoretical analysis (the theoretical bound seems to be loose by about a factor of 4 compared to the empirical measurement, but is still non-vacuous in many cases). Interestingly, this is *not* the case for standard supervised learning, likely to the higher-complexity models used to fit the labels; in that case the memorization gap is high, but a different gap (called the \"rationality gap\") is large in magnitude but negative.\n\nOverall, the paper is clearly presented, innovative, and has interesting empirical and theoretical results. It seems like a clear accept to me, with my only uncertainty that I am not completely familiar with the related literature. I am also not sure why the authors could not use the Rademacher complexity---are there theoretical obstacles to using it to upper-bound generalization error in this setting, or is the problem that it is too large? If the latter, then have you considered using your approach in settings other than just the self-supervised setting in order to improve on Rademacher complexity bounds?\n\n====\n\nFraming comments / questions:\n\n-I don't like the word rationality, since it has a technical meaning in Bayesian statistics that is not the same as the usage here (i agree they are somewhat similar in flavor, but I think it's confusing to conflate them).\n\n-I'm not sure it's correct to say that SS+S is a dominant methodology. In practice we would almost always do full fine-tuning on the self-supervised representation, rather than just the final layer. Still, starting with final layer fine-tuning is a reasonable start for analysis.\n\n-It seems an important point of your analysis is that we can condition on x and then just look at label noise for measuring generalization. It seems like empirical Rademacher complexity bounds also condition on x, so is there a fundamental difference here? (I think you try to address this in Remark 3.3 but I didn't understand your point there.)\n\n======\n\nA few presentation comments:\n\n-I didn't understand this claim: \" An optimal Bayesian procedure would have zero rationality gap, and indeed this gap is typically zero or small in practice.\"\n\n-Drawing lines between the dots (and shading the area under the curve) in Figure 1 is inappropriate, since the different points don't follow a logical linear progression (it's really just a scatter plot).\n\n-In Fact I, why do we need to take the max with zero? The result is still true even without the max, I believe.\n\n-In Fact I, it would be helpful to comment on the effect of changing eta. Do we expect certain of these quantities to get bigger or smaller in that case? Any heuristic intuition for how to choose the best eta?\n\n-Section 2.1 is a bit dense.\n\n-I liked Figure 2 a lot.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes a 3-term decomposition of the generalization gap",
            "review": "The paper analyzes the generalization gap for self-supervised learning. This paper's contribution includes the proposal of decomposing the generalization bound into three terms: robustness, rationality, and memorization (RRM). The three terms explain the generalization gap with some different perspectives. With the RRM decomposition's help, it proves that since SSS doesn't memorize data, small robustness and small rationality gap will naturally guarantee a small generalization gap.  \n\nAlthough I believe the RRM is novel and might bring some good insights for future studies, in isolation to self-supervised learning, this paper's main results (on the SSS part) seem to be stating something obvious in a fancy way. It is well understood that self-supervised learning has a small generalization gap, considering downstream tasks only learn from a function class with small capacity. For each T_pre, the generalization gap is guaranteed to be small with uniform convergence. The remark 3.2 doesn't really make sense to me. As for SSS, the T_pre is obtained with x sampled from the marginal distribution (with much more dataset), and T_fit is trained from (x,y) pairs generated from the joint distribution. T_pre is not supposed to see the same training examples, especially y. Therefore T_pre should not be memorizing the data samples. \n\nThe prior work mentioned in this paper in understanding SSL targets to explain why the representation learned from pretext tasks can be useful for the downstream task, hence proving a small generalization error (instead of generalization gap), which is more important in understanding the success of self-supervised learning. And assumptions like (approximate) conditional independence were only to show a small approximation error and were not needed for proving a small generalization gap. Therefore removing these assumptions does not seem to be a real contribution here. \n\nIt will be more interesting to me if the paper focuses more on the RRM decomposition and gives some further insights into how to use these terms in the future to improve the generalization performance of certain algorithms or tasks. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not rigorous with poor presentation.",
            "review": "\nThe authors propose to upper bound the generalization gap via three quantities, namely robustness gap, rationality gap and memorization gap, shows that the memorization gap can be bounded via standard learning theory arguments, and empirically show that all of the three terms are small. The authors also argue that if the rationality gap is large, then the performance can be improved.\n\n1. First of all, I think this paper is highly over-claimed. I don’t see how the proposed methods provably indicates the generalization. In fact, there is no theoretical conclusion on bounding the rationality gap and little theoretical discussion on robustness gap. Instead, the authors only show the empirical estimation on the robustness gap and  rationality gap. I would like to say, such inaccurate claim makes me feel uncomfortable.\n\n2. I would like to argue that, we cannot know the exact rationality gap, as we don’t have the data distribution at any time, thus we need a generalization bound to describe the performance of algorithm on unseen data. How do the authors deal with the rationality gap? I don’t feel empirical estimation on ‘test set’ is an acceptable choice, as ‘test set’ is only a batch of sample of real data distribution. The bound proposed by authors is not a generalization bound, thus it is meaningless to talk about the bound is vacuous or not.\n\n3. Moreover, as we don’t know the exact rationality gap, the claim by Theorem 3.1 is also not meaningful. In other words, even if we find the rationality gap is large when evaluating on test data, what we really do is tuning the model using the test data, not improving the performance of the model on data distribution.\n\n4. The authors argue in the abstract that the bound is independent of the complexity of the representation. However, several properties of the representation, e.g. the dimension, will definitely influence the generalization bound. I don’t feel this argument well-supported.\n\nOverall, the decomposition itself may motivate new idea on improving the current algorithms. However, theoretically, I don’t think this paper is a rigorous paper considering the generalization bound. If the authors want to argue the decomposition have some insight on improving algorithm, the authors should focus more on the intuition, algorithm design and empirical justification. If the authors want to argue the decomposition indicate tight generalization bound, then the authors should give rigorous proof on the bound of all three terms and calculate the bound based on the theoretical prediction instead of empirical simulation. There can be some misunderstanding on some of the points in the paper, but overall, with the current presentation, I think this paper is not ready for acceptance.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}