{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors propose an approach for pre-training that involves \"taking notes on the fly\" for rare words. The paper stirred a lively discussion on the reasons for the reported results, which the authors followed-up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. Thus, I am recommending acceptance."
    },
    "Reviews": [
        {
            "title": "An interesting way to accelerate pretraining, it would help to add more analyses and details.",
            "review": "This work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words. It is argued that the inadequate training of rare words slows down the pre-training. The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words. This technique is applied to BERT and ELECTRA and is shown to improve over the baseline. \n\nStrength:\n\n1. This work proposes a simple approach to accelerate the pre-training, with only a small memory and compute cost during training. The empirical study on BERT and ELECTRA supports the claimed improvements. \n\n2. It provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model. \n\nWeakness:\n\n1. It is argued that the proposed approach helps with rare words problem. But it will help to add more experiments to see how much more benefit we can get from it. For example, maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words. \n\nSpecifically, regarding \" we define keys as those words with occurrences between 100 and 500 in the data corpus\", How are the range 100 to 500 chosen? Have you tried it on words appearing lower than 100 or higher than 500? As mentioned above, it would be interesting to see if this approach can be applied to more words or subwords to get even more gains. \n\n2. Some design choices needs more details or explanations. \n\nFor example, why does the NoteDictionary use \"words\" instead of \"sub-words\" as keys? It seems using \"sub-words\" could cover a broader range of sentences with a NoteDictionary of the same size. It will also be easier to use during pre-training, for example, you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra NoteDictionary. \n\nAnother example is how the window size is chosen, since it seems an important new hyperparameter. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The method is simple and seems to be very effective in a certain situation, but we do not know what that situation is and why",
            "review": "The paper proposes an external memory architecture. When encountering the rare words (with a frequency between 100-500), the method will store the average contextualized word embedding of nearby words into a dictionary. Next time it encounters the same rare word, it will retrieve the average embedding and input it into BERT encoder. The experiment results show that given the same number of training steps, adding the external memory improves the MLM loss and significantly improves the results on RTE (Recognizing Textual Entailment) dataset, which leads to a slightly better GLUE score. The experiment also shows that keeping the external memory during the fine-tuning stage slightly degrades the performance.\n\nPros:\n1. The method is simple and easy to understand\n2. The experimental results on GLUE are quite surprising. It shows that we should take note when training BERT but throw away the note dictionary when fine-tuning the model.\n\nCons:\n1. Missing an important citation [1]\n2. The paper does not well explain the surprising results on GLUE. This is a crucial weakness. The comparison of the MLM loss is not very fair because the proposed method has a large external memory. The benefit of the proposed method relies on the improvement of the average GLUE score. However, Table 2 shows the most of the improvement of GLUE actually comes from the improvement of a single dataset, RTE. Without understanding why it improves RTE, the readers do not know when they want to adopt the proposed method for their downstream applications.\n\nClarity:\nThe text is fluent, but the main story is not well supported by the experiment results. The story is that using an external dictionary could accelerate the training, but the main experiment finding actually says that using an external dictionary can very significantly improve the results on RTE dataset while performing similarly on other datasets in GLUE.\n\nOriginality:\nThere has been some effort of using an external dictionary to help the training of BERT [1], but I am not aware of existing papers that apply the dictionary to only the rare words. I also do not know any other work that shows the external dictionary could improve the GLUE scores.\n\nSignificance of this work:\nIf the authors could well explain the experimental results on GLUE and justify the explanation using some analysis, this might lead to more important findings.\n\n\nFigure 3c seems to contradict with Table 1 and 2 because in Table 1 and 2, the GLUE score of BERT (ours) is 83.1 but all the points in the BERT curve in Figure 3c is below 83. \n\nUsually, when a study tries to sell its method as a way to accelerate the training, it means the method reaches some performance faster but the method will converge the same performance eventually. However, Figure 3 does not show that they will converge the same value, so selling the method as a way to accelerate the training is weird. Furthermore, I think the lower MLM loss is due to the extra parameters in the note dictionary rather than the note dictionary accelerates the training. \n\nIt is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [1]. It is also not surprising that it can achieve better performance on GLEU if using the note dictionary during the fine-tuning stage due to the extra parameters. The really interesting results are that the authors report that the model could very significantly improve the RTE task and mildly improve CoLA without using the note during the fine-tuning stage. \n\nIntuitively, the proposed model stores lots of knowledge about the rare words into the note dictionary. Does the fact that the note is not needed in the fine-tuning stage imply that the knowledge about rare words is actually not needed? Does it mean the RTE or CoLA do not contain many rare words or does it mean the rare words do not affect the decision of BERT and ELECTRA in RTE or CoLA? Is the reason of improvement that we could store more interactions between popular words in the parameters of BERT itself because the information of rare words has been stored in the note (maybe you can test this by reporting the MLM loss on the sentences without any rare words)? If that is the case, why do we only stably improve RTE and CoLA? If the authors can show the above hypothesis is true, I think this is a significant contribution because that means this paper provides a way to control what LM should learn when there is a mismatch between MLM training corpus and downstream applications (e.g., MLM training corpus contains many rare words but we should ignore the rare words in the downstream applications).\n\nThis paper lacks a good explanation of the above weird result (in my opinion, the most valuable finding in this paper) and lacks the analysis that supports the explanation. The main paper says that taking notes improves the tasks with the small datasets the most. The STS-b (7k) and MRPC (3.7k) have smaller training datasets than CoLA (8.5k). Why are the results of STS-b and MRPC cannot be stably improved? If the authors really want to explain the performance improvement using the training dataset size, the authors can just randomly sample several small subsets of training data from each dataset and show that the GLUE score improves a lot in that setting. In the appendix A.4, the authors hypothesize that the small proportion of rare words in each dataset of GLUE (from 0.47% to 2.31%) might be the reason that we can ignore the note dictionary during the fine-tuning stage. This also did not explain why most of the improvement of the GLUE score comes from RTE. Moreover, if the rare words are not important in the testing datasets, why do we want to take notes in the first place?\n\nI will vote for acceptance if the authors could answer these critical questions I raise above strongly.\n\n\nMinor:\n1. Although the chance is not high, I think it is possible that parts of MLM improvement could be achieved by simply sampling the sentences containing the rare words more (This is a minor concern. If you do not have time to finish the experiments for this baseline, you can choose not to do it or compare the results after training fewer steps).\n2. I guess the dictionary overhead is small but it should be measured and reported because you say the method accelerates the training. \n\n\n[1] Lample, Guillaume, et al. \"Large memory layers with product keys.\" Advances in Neural Information Processing Systems. 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technique is simple and results are good, but too many questions remain",
            "review": "This paper proposes Taking Notes on the Fly, a technique to improve the training efficiency of language-modeling style pretraining. It works by identifying rare words in the pre-training and adding a “note-taking” component to the masked language model which augments these words with an extra “note” embedding at the input layer. The note embedding is constructed from an exponential moving average of mean-pooled contextualized representations of context windows in which that word was previously seen during training. The notes are dropped in fine-tuning. Experiments find that this pre-training method improves fine-tuning results on English NLP tasks in the GLUE benchmark when used in the original BERT pre-training setup. In particular, the model can achieve similar performance to the original BERT model with less than 40% of the training steps, and similarly for ELECTRA.\n\n### Strengths\n\nThis paper is clearly written, the proposed technique is simple, and the results seem strong. It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity. The paper has a strong backbone and it seems that the proposed technique or something similar may serve as the basis for solid future work.\n\n### Weaknesses\n\nWhile the backbone of the paper is strong, I think it could be improved in its head (motivation) and legs (experimental studies).\n\nFirst, motivation. While the framing around rare words with the COVID-19 example is interesting, I think it has gaps. The introduction argues that since “COVID-19” is a rare word, in the course of training the model may lack the necessary signal to predict the masked word “lives.” But isn’t this fact exactly what should lead the model to improve its embedding of “COVID-19”? Because gradients flow into the embeddings both through the softmax layer and the input layer.\n\nSo while adding to the context may help the model get a foothold with more effective training signal for the masked token, it seems to me that the note could also “explain away” the rare word’s embedding in the input layer, reducing the learning signal on it. If that’s the case, then to the extent that TNF works, it would be by the tradeoff between improving the learning signal at the output layer for all words (and in contextualization) and degrading it at the input layer (for rare words).\n\nAs a broader example, see https://openreview.net/pdf?id=3Aoft6NWFej. That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency, whereas this paper argues essentially the opposite—that shortcuts must be added to hard cases in order to facilitate learning. It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful. Because it’s not clear where that line is, I think it’s not enough to motivate TNF from only one direction. It would be better to also have an explanation of why the note-taking approach does not also make things “too easy.” It’s not obvious to me how to best make this argument, though results from some of the ablations I will suggest below might help.\n\nThis brings me to my second point: Ablation experiments. If the motivation is to improve the representations of rare words in the input, then there are even simpler ways to do this. Experiments with simple baselines and ablations are important for figuring out why exactly TNF works.\n\nFirst, if the note is such a useful addition to the word embedding, why not just use it to update the embeddings directly? At that rate, the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe. This suggests a critical ablation:\n\n* Initialize the word embeddings with word2vec, GloVe, or similar run over the wordpieces in the pretraining corpus. (Weirdly, I can’t find an example of this in the literature. It seems like an obvious thing to try. I may have just missed it.) Indeed, it seems to me that the framing in the paper could just as easily motivate this (much simpler) technique than TNF.\n\nIf TNF outperforms the critical ablation, that implies that its gains are coming from some of the other particulars of the technique, such as 1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or 2) the use of contextualized vectors for note embeddings (rather than the non-contextualized ones in the word embedding objectives). \n\nTo investigate these issues, I would suggest three more ancillary ablations on TNF:\n* Directly update the rare word’s embedding with a version of Eq. 5 rather than keeping a separate note dictionary.\n* Update the note embeddings via backprop instead of Eq. 5. This would amount to “partially tying” the input and output embeddings, giving more freedom to the input layer, which is partly what’s happening in TNF.\n* Pool over non-contextualized instead of contextualized representations in Eq. 4.\n\nFinally, to address the “too easy” vs “too hard” distinction, two more ablations that might help would be:\n* Instead of using an exponential moving average for the note embedding update, just use the pooled context vectors from the last instance of the rare word (i.e., set $\\gamma$ to 1 in Eq. 5).\n* instead of using an explicit note dictionary, augment the input context with retrieved text containing the rare word. See TEK-enriched representations (https://arxiv.org/pdf/2004.12006.pdf) for an example of this. For consistency, the exact last-seen context of the rare word could be used.\n\nThe first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF. This could then serve as a reference point for the second ablation, which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting “too easy.” (although context window sizes might also be a confound here, that could also be controlled carefully.)\n\nAll together I think these ablations would shed a lot of light on why TNF works, and make this work much more useful to researchers who wish to build on it in the future. However, I know I’ve suggested a lot of crazy experiments here. I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important. I am also sure the authors could come up with better ablations than these as well. But my sticking point is the first ablation — initializing with non-contextualized embeddings — which I think is critical. And I think it behooves the authors to address some of the lingering questions (including more written below), even if not all of them.\n\n### Recommendation\n\nUnfortunately, reject. The technique is simple and the results seem good, but the paper does not provide empirically-justified insight on why TNF works. I think ablations and investigation into the “why” aspect is the most important part of this kind of model engineering research.\n\n### More comments & questions\n\nI am left with some more questions about how TNF works:\n\n* How does the quality of the representations of rare words specifically compare in your approach? Does it improve the representations of common words and contextualization at the expense of rare words? While it may be tricky to try to directly assess embedding or contextualization quality, breaking down the MLM perplexities by word frequency (or presence of rare words in the context) after removing the note dictionary might be informative. I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment. But any insight into this issue would be appreciated.\n* If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens, then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training? I am not aware of anyone showing such a thing to work, though I might have missed it. Just a thought.\n\nWhile the pretraining corpus is huge, 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper. Questions:\n* What do the even rarer words look like? Are they just a source of noise? e.g., because they are components of names or don’t have clear and consistent semantic content?\n* What proportion of contexts contain words appearing less than 100 times? It seems that the 20% figure in the paper is meant to apply to your definition of rare words, which appear between 100 and 500 times.\n* What is the word vocabulary size? i.e., how many words appear more than 500 times, and less than 100?\n* Did you do any preliminary experiments with other thresholds? Would you expect this to work with more common words as well? Why or why not? (This may also relate to the “too easy” vs “too hard” issue.)\n\nOn pre-training efficiency results: I think Figs 3a and 3b need to be explicitly qualified a little better. AFAICT, having lower loss here doesn’t necessarily mean the model (modulo the note dictionary) is learning better, because it sees the notes in the input. So we’re looking at the loss in a different setting than we intend to fine-tune in. It’s still interesting to see, but I think it's best to include an explicit caveat.\n\nWhat about training the models for more steps? Will the trend hold and performance improve overall, or will the gains eventually level off as the representations of rare words get better? Especially for pretrained models, since they are used as the starting point for many models, it is often worthwhile to train them longer (as in the RoBERTa paper), so it’s important to understand the usefulness of this method in that regime.\n\n### Typos etc.:\n\n* P.3: neglectable -> negligible\n* P.3: Representation -> Representations (in BERT acronym)\n* P.6 Sec. 4.1: after “MNLI” there is a space missing after the period.\n* P.6: “FULL-SENTENCES” would look better & be consistent with Liu et al if it were in small caps.\n* Please cite the individual dataset creators for the datasets in the GLUE benchmark.\n\n---\n\nUpdate: upped score from 4 to 5; see comment thread.\n\nUpdate again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper with strong empirical results, but lacks convincing explanations for why the method works well.",
            "review": "*Summary*: This paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts (“notes”, stored in a “note dictionary”). The method considerably speeds up the convergence of pretraining BERT and ELECTRA, and the authors furthermore show that these models perform better when fine-tuning on downstream GLUE tasks (likely because the models were undertrained to begin with, so converging faster alleviates this issue).\n\n*Strengths*: The method is surprisingly simple and empirically quite effective. It's especially interesting to see that BERT + TNF at 400K steps has better GLUE performance than BERT at 1M steps.\n\n*Weaknesses*: the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare words---I’m still not entirely sure why this works so well. Do these rare words commonly show up in GLUE (and thus, the method is helping because your representations of rare words are better)? It seems like TNF is actually improving the representations of more-common words as well.\n\n*Recommendation*: 7 Despite the lack of clarity around why exactly this method works so well, the method seems empirically useful and straightforward to apply. I expect that this will be useful to practitioners interested in applying BERT and similar pretraining strategies to new corpora and domains.\n\n*Questions*:\n\nIt’s a bit unclear to me that note-taking itself is required for this to work well...in the COVID example presented in the introduction, if you see the sentence “The COVID-19 pandemic is an ongoing global crisis”, isn’t it possible that MLM itself is sufficient to associate the embedding of “COVID-19” with “pandemic” and “global crisis”? Do you have further evidence to show that note-taking is actually improving the representations of rare words, besides GLUE score (which might not be very indicative, since the rare words might not show up in GLUE).\n\nThe Construction of Note Dictionary: Does 3.47B refer to the number of types or the number of tokens? Why not define keys with frequencies less than 100 in the dictionary as well (since you only use types that show up between 100 to 500 times)?\n\n“It means that to reach the same performance, TNF can save 60% of pre-training time. If models are trained on 16 NVIDIA Tesla V100 GPUs, BERT-TNF can reach BERT’s final performance within 2 days while it takes BERT 5.7 days.”: Is the 2 days vs 5.7 days an actual wallclock measurement? Or, are you hypothesizing this based off of the loss curves?\n\n*Missing / Erroneous Citations:*\n\n“It is well-known that in a natural language data corpus, words follow a heavy-tail distribution (Larson, 2010)” This is more-commonly known in the NLP community as Zipf’s law. Better cites would be:\n  - Zipf G. The Psychobiology of Language. London: Routledge; 1936.\n  - Zipf G. Human Behavior and the Principle of Least Effort. New York: Addison-Wesley; 1949.\n\n*Miscellaneous comments:*\n\n“Moreover, completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance.”: I agree that it’s a bad idea to remove sentences with rare words, but I disagree that the issue is reducing the size of the data---you can always go collect more data and filter it to not include rare words. It’s more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data, which would be harmful\n\n“Our method to solve this problem is inspired by how humans manage information.”: I think the connection to human note-taking is tenuous at best, and would omit it; the motivation remains clear without this.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}