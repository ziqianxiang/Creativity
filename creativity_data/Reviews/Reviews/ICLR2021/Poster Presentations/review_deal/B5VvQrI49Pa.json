{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a method for learning non-separable Hamiltonian dynamics by including a state of the art symplectic integrator (Tao, 2016) into the model training pipeline. This is a nice improvement on the past work that primarily addressed separable Hamiltonians. The reviewers agree that the paper is well written and that the empirical evaluation is solid. The paper was further improved during the discussion period by incorporating the reviewers' feedback. For this reason I am happy to recommend this paper for acceptance."
    },
    "Reviews": [
        {
            "title": "recommend accept ",
            "review": "Summary:\nThis paper describes a deep learning approach for predicting Hamiltonian systems. The original paper enforces conservation in the loss function. Several of the follow-up papers embed a symplectic integrator instead, but these couldn't handle non-separable systems. This paper can both handle non-separable systems and use a symplectic integrator to enforce conservation. They demonstrate their system on quite a few examples and show lower error (both in prediction and the deviation in the Hamiltonian) than a NeuralODE or the original HNN paper. The final example, in Figure 5, shows a compelling visual improvement.\n\nStrong points:\n\nThe Greydanus, et al. NeurIPS 2019 paper on Hamiltonian Neural Networks was very successful and has already inspired many follow-up papers. This paper improves it in several ways (Table 1). Since the ICLR community is similar to the NeurIPS community, I think that this paper would be of interest.\n\nI like that the extension for non-separable systems directly builds off an approach for extending integrators to non-separable systems (Tao, 2016). Further, the Tao integrator is built into the network training. This suggests that it's a robust path to take. \n\nI appreciate that results are reported on quite a few examples to compare NeuralODE, HNN, and the new method (NSSNN). \n\nThe results in Figure 5 are quite impressive! I also think it's cool that the training data only needed to be from two particles. \n\nWeak points/clarification questions:\n\nIn the examples in this paper, the canonical coordinates need to be known ahead of time. The original HNN paper has an example where the coordinates can be learned from data (Pixel Pendulum), and I believe some of the follow-up papers have covered this case as well. Do you have any thoughts on if your method could learn the coordinates?\n\nThe font size in the figures is sometimes hard to read.\n\nIn Section 4.2 & Table 2, is there a held-out test set, or are these all training errors? I would like to see both training & test errors to see if there is overfitting. \n\nMinor points:\n\nI found it confusing that Figure 3 & Section 4.1 refer to an \"ablation test.\" It seems like a test to choose a suitable training set. \n\nDisclaimer:\n\nI should mention that I can't vouch for the proof in the appendix. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Reviewer 3",
            "review": "The work proposes a novel method for solving non-separable Hamiltonian systems, using Tao's approach in which two copies of the phase space are tied together by an additional Hamiltonian. This appears to be a novel proposal, and certainly of interest. \n\n# Positioning w.r.t. related work.\nThe proposed method specifically focuses on non-separable systems. The work by Jin et al. (2020) is mentioned in passing, but this work appears to address the problem of non-separability as well. It would thus make sense to dive deeper into how the proposed method compares to Jin's, both theoretically and empirically. Some arguments require citations, such as \"variational methods are not well suited for tackling such challenges due to heir inherent weaknesses\": which weaknesses? \n\n# Discussion of the method.\nAlthough the method appears to perform favourably in all provided benchmarks, it remains unclear how the method compares on computational and memory complexity. This should be added to the discussion, with a theoretical and/or empirical analysis. It would also be beneficial if areas would be highlighted where the method falls short. \n\n# Experiment section.\nAlthough there is a section called 'Ablation test', an actual empirical ablation study is missing. As the proposed method introduces various moving parts including a specific loss, phase space parametrisation, and specific neural architecture fo H. It is not clear what the relative contribution is of these parts to the improved performance over the baselines. A closer study on this, as well as the influence of the hyperparameters such as t, would shine more light on the characteristics of the proposed method. Moreover, the experimentation section does not convince that the baselines' hyperparameters have been fairly tuned, and if a potential increase of parameters in H might contribute to the stronger performance.\n\n# Clarity of writing.\nI found the paper difficult to follow. The meaning of some phrases is unclear, e.g. 'which contributes robust for wider datasets'. \n\n# Conclusion.\nThe work is interesting, but the analysis of the method is incomplete due to the lack of comparison with a recent competing method, and a missing discussion of the potential tradeoffs involved in choosing this method over others. \n\nCouple nitpicks outside of the review:\n- \"Nonsep_e_rable\" appears a couple of times\n- Line 197: 'systmes.\n\nUpdate: The authors have addressed the most pressing issues with the manuscript. I've increased my score and vote in favour of accept. The section of limitations was difficult to follow and would benefit from a more structured comparison with competing methods. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid work on modeling nonseparable Hamiltonian dynamics, but needs more clarification ",
            "review": "The paper extends the symplectic family of network architectures towards modeling nonseparable Hamiltonian dynamic systems. More specifically, the paper implements a  symplectic integration schema (from Tao (2016)) for solving arbitrary nonseparable (and separable) Hamiltonian systems within a symplectic neural network architecture. The results from several modeling tasks show that the proposed Nonseparable Symplectic NNs (NSSNNs) are more robust and accurate than vanilla HNNs and NeuralODEs when applied to nonseparable Hamiltonian systems. \n\nAlthough the idea of modeling nonseparable Hamiltonian systems with Symplectic NNs was already briefly outlined in the SRNN paper (Chen et al 2020), this paper implements it and further analyses various properties of this approach. Overall, the paper is well structured and well written, however there are still some inconsistencies that need to be addressed and clarified. \n\nNamely, the related work discussion is somewhat handled poorly: For instance, the authors state in only one sentence that NSSNNs are closely related to SympNets (Jin et al 2020), without discussing any further details on how are they related and, more importantly, how they differ. Moreover, from that point on, SympNets are never considered (in the experiments) nor mentioned, even though SympNets are indeed able to model nonseparable Hamiltonian systems. In Table 1, that compares the properties of NSSNNs w.r.t some benchmarks, the authors discus \"TaylorNet\" and \"SSINN\" - these two are never introduced before. I assume the former refers to Thong et al. 2020, but I have no idea about the latter.  \n\nRegarding the choice of \\omega, the authors provide some evidence that the choice of \\omega plays a role as a regularization, where larger values tend to restrain the system. The analyses given in Appendix B show that with \\omega 10 the system already is stable (which also supports the experiments presented in Tao 2016). But then the \\omega is set to 2000 in the experiments, which is orders of magnitudes larger than the analyses. How and why was this value chosen? \n\nLines 206-207 state that from the results in Fig4, it is \"clear that\" NSSNNs can perform long-term predictions but HNNs and NeuralODEs (in the legend they are listed as ODE-nets, are these the same method?)  fail. It is not clear how was this determined, since the results show that NSSNNs are more robust to noise than the other two, NeuralODEs are still able to perform long-term predictions (in a noiseless setting), and HNNs in a both scenarios w/o noise and w/ moderate amount of noise.\n\nSome typos and minor comments:\n\tL1: Hamiltonian systems are not a \"special\" category of physical systems, but is a formalism for modeling certain physical systems (eg. a pendulum, besides within Hamiltonian mechanics, can be modeled within classical (Newtonian) mechanics and Lagrangian mechanics).\n\tL42: \"e.g. see Tong et al. 2020\" -> \"Tong et al. 2020\"\n\tL56: \"degree of Freedoms\" -> \"degrees of freedom\"\n\tL206: \"figure 4\" -> \"Figure 4\"\n\n#Update \n\nI thank the authors for addressing my questions and revising the manuscript, which clarified many of my concerns regarding this work. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Necessary missing component in the Hamiltonian Neural Networks ecosystem.",
            "review": "The authors propose a variation of Hamiltonian Neural Networks (HNNs) with a built-in symplectic integrator. While built-in symplectic integrators have already been used in HNNs, the most popular symplectic integrators only work for separable Hamiltonians. The authors use Tao’s integrator (2016) at the core of their model (HSSNN) to achieve better conservation of energy in systems with non-separable Hamiltonians. Many works have experimented with training with different integrators, and in that sense this is a very incremental contribution, however, Tao’s integrator is a state of the art integrator, and I think the community will benefit from reading about it, and seeing it implemented.\n\nThe introduction and model description part of the paper is clear in general and reasonably easy to follow, and does a very decent job at introducing Tao’s integrators in a very compact way. Many details for reproducibility are missing from the experimental sections of the paper (specially with respect to the implementation of the baselines), but this should be alleviated by the source code provided by the authors. The final section on the multi-particle vortices is probably the most unclear, and should be improved.\n\nConsidering that this is a very empirical paper, with an incremental contributions that only make use of toy environments, it should probably do a bit more of a thorough job at really trying to ablate the model in more sophisticated ways.\n\nI think there are two important baselines missing:\n* Performance of their model e.g. using a RK4 integrator, instead of Tao's. This would really tease apart the the effect of the symplecticity. Note this is an important ablation that is between this model and HNN (which is trained by supervising the gradients, rather than through an integrator).\n* Training HNN using the gradients, but then integrate the learned Hamiltonian using Tao’s integrator at test time (or even better: a time adaptive version of Tao’s integrator). I would be surprised if this does not perform as well as HSSNN (Although as the authors say, requiring the gradients to train HNN is a very rigid constraint, so this would not take away from the importance of their work).\n\nAlso in the multi-particle system, I am surprised the authors did not choose a Hamiltonian Graph Network (https://arxiv.org/pdf/1909.12790.pdf). I do not think at this stage I would need to see this included, but probably a mention to Hamiltonian Graph Networks in the context of multi-particle systems would be appropriate.\n\nAlso, Figure 3 b) and c), seem to have weird patterns that would be interesting to try to gain more insights into because I cannot tell if, for example, the variance in Fig 3c is just noise, or something more intrinsic to the experiment. So I really don’t know what to make of plots Figure 3 b) and c). Maybe plotting them for more environments or adding some form of errors bars would help understand them better.  \n\nSimilarly, in Table 3, I would have expected larger differences between HNN, and HSSNN for the Hamiltonian Deviation column. Maybe it would be possible to use longer trajectories and amplify the differences more? In fact the differences in terms of conservation of energy for Tao’s environment seem to be much smaller quantitatively in Table 3 than qualitatively in Figure 4, was the example from Figure 4 cherrypicked?\n\nI think some additional comparison ablations, like some of those provided in https://arxiv.org/pdf/1909.12790.pdf, would really help gain more insight on the model:\n* Try different integrators at train and test time\n* Train on a range of time steps during training, instead of a single values.\n\nI think the paper is a bit borderline, and in its current form a lean on the side of \"Weak Reject\", but would be happy to raise the score, if the authors can make improvements in the axis mentioned above.\n\nSome questions I am curious about (have not affected my decision):\n* Any particular reason to use an l1 loss, instead of l2? I wonder if this can also cause differences with the baselines which use l2. The prediction error seems to be defined also using l1. Is it not unfair to compare l1 to the baselines, when baselines are trained with l2 loss (or at least they were in their original papers)?\n* Table 1 says that HNN requires the gradients, but if I remember correctly the HNN paper model does mention the possibility of estimating the derivatives with finite differences, which is equivalent to training through an Euler integrator.\n\nSome minor comments (have not affected my decision):\n\nThe text in the experimental and results sections feels a bit rough at times, would recommend rewriting most of it, making sure that the message of each sentence is clear and unambiguous.\n\n> these nonseparable systems exhibit plateau of degrees of freedom, demonstrating complexities that are orders-of-magnitude higher than separable systems (whose degree of Freedoms are typically below 10).\n\nAre not many n-body systems highly separable, yet they have much much higher numbers of degrees of freedom?\n\nFigure 1 could be clearer, the intended message of the crossed-out notation in red is not obvious.\n\nFigure 2b bit unclear. The plot implies n iteration steps, but this is not really conveyed really well.\n\nMissing “/” in line 169\nMisspelled “systmes” (197)\n\nNotation in equation (7):\nVectors (bold) or scalars (italics)\nWhat about the subindex, should I assume you train on one step data, and not on sequences?\nIf the formula refers to the training loss, the limit of the sum should probably be the batch size, and not the number of training samples?\n\nThe term “Strong stability” in Table 1 seems not very scientific. Maybe something like “symplectic stability” or something like that would be more specific.\n\nTable 2: Hamiltonian Deviation (Row “spring”) HNN and NSSNN seem to have the same error, so maybe both should be in bold.\n\nEquation 2016 wrong left side, H(p, q)_predict, should probably be H(p_predict, q_predict) since I assume the analytical Hamiltonian formula is used in all cases, to estimate the energy of the state.\n\nMODELING VORTEX DYNAMICS OF MULTI-PARTICLE SYSTEM section needs more clarity. For example notation in equations 8 and 9 seems inconsistent. Also lines 229 to 232 lack sufficient detail of how the generalization from 2 particles to N particles is achieved. My guess is that the authors just add up all possible pairwise interactions when moving towards systems with higher number of particles.\n\nEDIT: Updated rating after author revisions.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}