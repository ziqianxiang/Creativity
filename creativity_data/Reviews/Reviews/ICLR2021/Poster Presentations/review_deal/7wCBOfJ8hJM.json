{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper extends past work on kNN-augmentation for language modeling to the task of machine translation: a classic parametric NMT model is augmented with kNN retrieval from an external datastore. Decoder-internal token-level representations are used to index and retrieve relevant contexts (source + target prefix) that weigh-in during the final probability calculation for the next target word. Results are extremely positive across a range of MT setups including both in-domain evaluation and domain transfer. Reviews are thorough, but quite divergent. There is general agreement that the proposed approach is reasonable, well-motivated, and clearly described -- and further, that experimental results are both solid and relatively extensive. However, the strongest criticism concerns the paper's relationship with past work.  In terms of ML novelty, everyone agrees (including the paper itself) that the proposed methodology is a relatively simple extension of past work on non-conditional language modeling. However, two of the four reviewers strongly feel that, in light of the potentially prohibitive decoding costs, the positive experimental results are not sufficient to make this paper relevant to an ICLR audience given the lack of ML novelty. In contrast, another reviewer strongly takes an opposite stand-point:  rather, that the results will be extremely impactful to the MT subcommunity at ICLR since they are unexpected (i.e. that a non-parametric model might compete with highly-tuned NMT systems) and very positive across a range of domains and settings (i.e. in-domain, out-of-domain, multilingual) -- further, that the approach has substantial novelty in the context of MT where parametric models are the norm and that it might inspire substantial future work  (e.g. on efficient decoding techniques and further non-parametric techniques) given that it so drastically breaks the current MT mold. The final reviewer shares the concern of the former two about novelty, but is swayed by the experimental results and potential uses for the model (given kNN augmentation is possible without further training) and therefore votes for a marginal accept. After thorough, well-reasoned, and well-intentioned discussion between all four reviewers, the reviews land just barely in favor of acceptance, but with substantial divide. After considering the paper, reviews, rebuttal, and discussion I am swayed by the argument that (a) these experimental results are largely unexpected, (b) they are both extremely positive and offer a new trade-off between test and train compute in MT, and (c) that the paper may therefore inspire substantial discussion and follow-up work in the community. Thus I lean in favor of acceptance overall."
    },
    "Reviews": [
        {
            "title": "The authors extend a method in LM to MT. The experiments show the method is effective in a range of settings.",
            "review": "The paper is an extension of [1]. The task in [1] is Language Modeling, while this paper is doing machine translation with the similar idea. The authors propose a non-parametric method for machine translation via a k-nearest-neighbor (KNN) classifier. Specifically, it predicts tokens with a KNN classifier over examples cached in a so-called datastore and this method can be applied to any pre-trained neural machine translation model without further training. The experiments show that it improves results across a range of settings (in-domain, out-of-domain, and multi-lingual evaluations).\n\nStrengths: \n+ The method is simple and can be applied to pre-trained neural machine translation model without further training.\n+ The experimental results across a range of settings are effective.\n\nWeaknesses:\n- Although the method is simple and does not add trainable parameters, it add the computational cost. The authors mentioned the computational cost briefly but there are no detailed experiments. It would be good to see the authors add more analysis on the computational cost, for example, how it varies with k.\n- Technical novelty over [1] seems to be incremental, where a large portion of the work is essentially regarding machine translation as a language modeling and applying the method in [1] to machine translation.\n\n\n[1] Khandelwal, Urvashi, et al. \"Generalization through memorization: Nearest neighbor language models.\" arXiv preprint arXiv:1911.00172 (2019).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel method with a diverse set of strong results.",
            "review": "This paper describes a nearest-neighbor enhancement to NMT, where internal token-level context representations are used to index into a large data store to find relevant (source, target prefix) pairs. Since the index representation is taken from a pre-softmax representation in the decoder network, no additional training of the NMT model is required. The authors show a diverse range of strong results, from improvements using a data store over the model’s own training data, to improvement from using a collection of domain-specific corpora not present during training used for domain adaptation, to language specific collections to improve capacity of multilingual models. They are also able to show by example how the model makes MT more interpretable.\n\nThis is a very strong paper. It's well-written and easy to read, the method is very novel to MT, and the results are great. The method isn’t practical right now (decoding is two orders of magnitude slower), but it’s very interesting and thought-provoking. I can imagine it influencing a lot of work, even if the actual method doesn’t see a lot of use.\n\nThe only complaint that I could imagine raising against this paper is that the method is not particularly novel in light of recent work on nearest-neighbor language modeling, but in this day and age, with so many papers available, I think it’s actually very important to make these incremental stops in neighboring fields to make the connections explicitly clear. All the great experiments on multilingual MT and domain adaptation also help a lot. To their credit, the authors provide a concise section discussing the changes that needed to be made for the conversion to conditional language modeling (MT).\n\nSmall concerns:\n\nThe exp(d) in Figure 1 is missing a negative: exp(-d).\n\nTable 1: what does the bolding indicate? It looks like statistical significance, but if so, please be clear about what test was used.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Extension of KNN-LM",
            "review": "Summary:\n\nThis submission introduces the kNN-MT approach for neural machine translation, which incorporates the memorize-catching spirit with the nearest neighbor classifier on a large datastore when generating the decoding sentences, together with the neural machine translation model for similarity search. No additional parameters are needed, but the inference cost increases. The authors conduct experiments are different settings, the single language pair translation, the multi-lingual machine translation, and the domain adaption translation. Results show that kNN-MT can easily improve translation performances by searching out related test sentences with non-trivial scores. \n\nComments:\n\nGenerally speaking, the submission is okay and the proposed approach has no big flaws, however, I feel hard to make this submission to be accepted. The main reasons or concerns are:\n1. It is clear that this submission is a direct and straightforward extension of the previously published ICLR-2020 paper: kNN-LM. As the authors also clearly stated in the abstraction. Therefore, in terms of the contributions and differences, they are quite limited. The technique is almost the same, except the key is added with the source language sentence. The presentation of this paper is also similar to kNN-LM. The direct extension of the kNN approach from Language Model to Neural Machine Translation makes me feel hard to recommend, and this makes much more like a technique report of the method extension. \n2. To say about the approach, I acknowledge that this method is effective, as the authors have done with multiple experiments. However, the computation cost is also high. The authors also discussed this in Section 3. It is hard for real-time systems to afford the increased inference cost as this approach made. The improved results with a little increased cost are okay, but too much is not a good choice. Though the authors mentioned there is a trade-off and I also acknowledge this, but it is still not clear what is a good trade-off. \n3. Also, this method highly depends on the scale of the dataset, also the similarity between training and test dataset, if I understand correctly. This assumption can hold for high resource translations, but for low resource translation, this would be limited. This is another drawback of these search-based algorithms. \n\nMinor question: What is the effect if $\\lambda$ is varied?\n\nTherefore, shortly speaking, I feel this paper is straightforward to extend from the previous paper (indeed this is the future work and the answer from the review comments of previous work). This concerns me a lot for another one in ICLR-2021. \n\n---------------\n\nUpdate:\nI thank the authors to give responses to my points, especially the discussion about novelty. But I still feel the success of KNN for NMT is similar for LM, that's why a lot of works study on NMT are also work on LM. Since this KNN method only targets at the decoder side, same as LM model. Therefore, I still feel not novel enough. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new approach for NMT decoding that exploits a translation database of billions tokens with some improvements over state-of-the-art for a huge computational cost. Evaluation and analysis are very well designed and useful for future work.",
            "review": "This work presents an approach to exploits at decoding time a very large translation memory to improve NMT. An extensive evaluation is performed along with some detailed analysis on the important parameters.\n\n\n\nStrengths:\n- the idea is very simple, very easy to understand, and intuitive\n- can be added to existing pre-trained NMT model\n- many interesting applications (domain adaptation, multilingual model specialization for instance) are presented and are mainly the reason why I think paper can be accepted for publication.\n- the paper is easy to read and well-written\n\nWeaknesses:\n-  exploiting a translation memory at test time is not novel (exploitation of billions of tokens is rather impressive but in my opinion making this possible is more an engineering problem)\n- the approach is described within one page, the remainder of the paper is about evaluation and analysis. For ICLR, the paper lacks of substance.\n- the improvements over SOTA English-German are very small considering that billions of tokens are exploited and the high decoding cost.\n- the experiments presented in this paper are not reproducible since unpublishable data are exploited to train the system (eg. CCMatrix)\n- computational cost at test-time is extremely high, as expected. This is probably why nobody tried it before. I do not see how it could be used for real-world applications. Focusing on reducing the computational cost would greatly improve the paper.\n\n\nQuestions/suggestions:\n- \"we also provide scores from Aharoni & Goldberg (2020)\": did you check that these scores are comparable with yours? It is unclear in the paper whether they also used sacreBLEU (insert the sacreBLEU signature in a footnote in your paper to help future work reusing your scores)\n- I recommend to add the decoding time in the tables and a description of the hardware used. Since the major issue of the proposed approach is its computational cost, adding the decoding time would probably encourage future work to try to improve it.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}