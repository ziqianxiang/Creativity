{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a pre-training technique for semantic parsing with an emphasis on semantic parsing and the technical details required to actually make it work in practice. Overall, all reviewers agree that the results are very good, and you see nice improvement across multiple text-to-SQL datasets. Some reservations have been raised on (a) the difficulty of creating the SCFG for generating the synthetic data, but this seems to have been properly addressed by the authors and requires a reasonable amount of effort. and (b) how tailored the pre-training task is to a particular task (text-to-SQL) and dataset (Spider). Overall, I tend to agree that the fact that one sees improvement on Spider is slightly less compelling as the grammar is derived from it, but the authors rightfully claim that consistent improvements are also evident in other datasets, even if the gains are somewhat smaller. One can hope the idea can also be generalized to other setups where synthetic data can be generated and the details of how to combine synthetic data with real data should be useful. "
    },
    "Reviews": [
        {
            "title": "Nice idea of using synthetic data to improve compositional textual understanding",
            "review": "This paper presents a general-purpose pre-training approach for jointly encoding utterances and relational tables in the task of table semantic parsing, where a natural language utterance is transduced into an executable query (e.g., SQL) over relational database tables. A core challenge in table semantic parsing is to understand the compositional semantics in utterances, and further ground salient entities and relations in the utterance onto the corresponding tabular schema (e.g., columns, cells). To improve understanding and grounding of compositional utterances, the authors propose fine-tuning a pre-trained masked language model (RoBERTa) using linearized table headers paired with synthetic utterances generated from a synchronous context free grammar, via an objective that encourages the model to discover the syntactic roles of columns mentioned in the input utterance. Experiments over four datasets demonstrated strong results when the this newly proposed pre-training objective is combined with classical masked language modeling objective.\n\nStrong Points:\n\n* The paper is in general well written and easy to follow;\n* The idea of fine-tuning ``monster'' pre-trained language models over massive open-domain textural corpora using domain-specific synthetic corpora and objectives could also inspire future research on applying pre-trained LMs in low-resource domains with heterogenous (semi-)structured data.\n* Results over four semantic parsing benchmarks provide convincing story of the effectiveness of the proposed approach.\n\n\nDetailed Comments:\n\n[Pre-training Logical Form Coverage] The production rules ($\\beta$'s) in Table 1 are high-level SQL sketches with fixed level of compositionality. While this would help improve the quality of canonical utterances, this will would limit the expressiveness of the grammar, since those production rules cannot be composed or chained. I was wondering if the authors had considered using a more compositional grammar (e.g., the general grammars in Wang et al. 2014 or Herzig and Berant, 2020) with larger logical form coverage.\n\nAdditionally, the 90 productions used in generating synthetic data are curated from utterances in the Spider dataset. While it is not the focus of the paper, incorporating rules learned from a more broad collection of datasets would be helpful, as suggested by the experimental results, where RoBERTa tuned with the proposed SQL semantic prediction objective (GRAPPA+SSP) significantly outperforms training with the classical masked language modeling objective (GRAPPA+MLM) on Spider (Table 3), whose SQL program templates are covered by the synthetic data during pretraining, while GRAPPA+SSP performs on par (Table 6) or slightly underperforms GRAPPA+MLM (Tables 4, 5) on other benchmarks. This is not surprising, as programs in those datasets have significantly different underlying patterns compared to Spider's. For example, WikiTableQuestions has more `argmax` queries, and queries sensitive to the position of rows (e.g., \"What is the host city of the first Olympics?\"). This might suggest increasing logical form coverage is important for further improving the performance.\n\n[Interplay between the SSP and MLM objectives] In section 1, the authors mentioned the importance of carefully balancing between preserving the original natural language representations and injecting the compositional inductive bias through our synthetic data, without details of the pre-training procedure. Are these two objectives used in a particular order, or are MLM and SSP examples sampled randomly during pre-training? \n\nThe experimental results suggest that the key is to combine SSP+MLM, it would be interesting to study the relative impact of the two objectives, e.g., by varying the amount of their respective pre-training examples.\n\n[Encoding Table Contents] The proposed approach only encodes table header as input to the Transformer, while ignoring its contents. Previous works have demonstrated the importance of encoding table contents relevant to the input utterances in semantic parsing. While the downstream parser might have features to capture content information (e.g., a binary feature that indicates if a cell in a column is mentioned in the utterance), it would be useful to directly encode such information during pre-training.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "## After author responses\nBased on the revision of the draft and the authors' responses to the review, I am raising my score to 7 from 6.\n\n---\n## Overall summary\nThe paper proposes a pre-training method useful for training neural semantic parsing models that translate natural language questions into database queries (text-to-SQL). The authors manually write down and then sample from synchronous context-free grammars that can generate SQL queries along with corresponding natural language (but stilted) utterances that have the same meaning. The authors also collect questions about databases and tables from various sources. The combination of the two is used for fine-tuning RoBERTa using an auxiliary objective, then using the fine-tuned RoBERTa in semantic parsing tasks. The authors show strong results on four benchmarks.\n\n## Strengths\n- The authors obtain state-of-the-art results on four different benchmarks which convincingly shows that the proposed methods can help with empirical results.\n- The method is simple and straightforward to implement.\n- The paper addresses a problem domain with significant practical applications and community interest.\n\n## Weaknesses\n- It is unclear how much effort is needed to construct the SCFG. Since the SCFG was constructed directly by examining examples in Spider, the kinds of natural language questions and queries it generates may also be unfairly biased towards the distribution used in Spider.\n- There could have been more quantitative analyses of the method and its component parts in the paper.\n\n## Recommendation\nI am giving a rating of 6 considering the strong results but the relative lack of analysis of the method (ablations, etc). I think further revisions of the paper would benefit from greater analysis of the method. \n\n## Questions\n- What would happen if you use the synthetic Spider data and use it to train the semantic parsing model for Spider?\n- Does the grammar provide for multiple natural language utterances that will translate to the same SQL?\n- The introduction states that \"Our approach dramatically reduces the training time and GPU cost.\" Is there evidence to show this beyond Figure 2? There, the gains don't seem so dramatic. Considering that fine-tuning GRAPPA took 10 hours on 8 V100 GPUs, it seems unlikely that there will GPU cost will be reduced especially if the fine-tuning cost is also included.\n- What happens if the task-specific training data is also used with the MLM or SSP objectives in pre-training?  https://arxiv.org/abs/2004.10964 gives evidence that it can be useful to fine-tune RoBERTa on the downstream task's data using the pre-training objectives.\n\n## Miscellaneous comments\nThe papers for Tapas, Overnight, and TaBERT are duplicated in the references.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "lack of novelty, augmented data is homologous with benchmarks used for testing, improvements are marginal",
            "review": "##########################################################################\n\nSummary:\n\n \nThe paper provides a interesting direction in pre-training for table semantic parsing. In particular, it proposes to first collect a collection of pseudo question-SQL pairs in an automatic way, based on tables from WikiTables tables and tables and databases in the training sets  SPIDER and WIKISQL. After that, masked language modeling and a newly introduced task called SSP, which is, given a natural language sentence and table headers, to predict whether a column appears in the SQL query and what operation is triggered. Experiments on Spider and WikiSQL show that the model achieves new state-of-the-art.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I like the idea of pre-training for table semantic parsing.  My major concerns are the novelty of the method and the experiment. \n1. The merit of a pre-trained model is its ability to horizontally support (to some extend) related tasks, not only limited by one. However, the pre-training conducted in this work is different from standard unsupervised pre-training in that the data used for pre-training is task-specific, i.e. for the task of natural language to SQL generation, and the evaluation is only conducted on natural language to SQL generation tasks. I agree that NL-to-SQL generation is a semantic parsing task, but semantic parsing is a broad area (see Percy Liang's CACM survey), only limited to NL-to-SQL generation. \n2. Even though the authors pre-train on table-based NL-to-SQL generation, as a pre-trained model, the model should be tested on other semantic parsing problems. Taking one step back, even though tasks are limited to NL-to-SQL generation, at least experiment should be conducted on a dataset whose tables are not used to produce pseudo dataset which is used in the pre-training stage. \n3. It is worth to note that the pseudo data used for pre-training comes from the training data of the benchmark datasets used for evaluation. I agree that with the data augmentation process, the pre-trained model can see many newly composed questions and SQLs. However, the development of such a pre-trained seems to be too tailored for the task and for the datasets. In such a tailored way, the improvements are also marginal to me. From 4 tables, except for Table 3, SSP performs comparable to MLM, which reflects that the importance of the standard masked language modeling.\n4. The novelty of the methodology is limited. There is no improvements in terms of neural model architecture. The pre-trained task seems trivial and too specific for SQL generation. \n\n \n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not too novel, but a good solid result",
            "review": "This work explores a pretraining strategy (similar to https://arxiv.org/abs/1606.03622) to the problem of table question answering. More specifically a synchronous context-free grammar (SCFG) is first learned from training data (with manual alignment of entities/phrases). Then the SCFG is used to generate more full supervision data for Roberta model pretraining. The training objective is a combination of two parts: SQL Semantic Precision (SSP) predicts elements in SQL given the question on the synthetic data, and masked-language modeling (MLM) on the natural (training) data.\n\nExperiment on 3 dataset shows significant improvement over RoBERTa baseline (especially when using only 10% of training data). New state-of-the art results are achieved. Oblation study shows the impact of  SSP, MLM, training time, dataset etc., which is nice.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}