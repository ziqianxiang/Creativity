{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers all agreed on accepting this paper, stating that it makes a compelling point about the usefulness of saliency methods to diagnose generalization.  The reviewers found that the experiments were a strong point and applauded the thorough hyperparameter tuning and re-runs for statistical significance.  One reviewer commented that the paper was too dense with information, so much so as to make it difficult to digest.  However, overall this seems like an interesting paper that is relevant to the community and will hopefully foster some good discussion about the shortcomings and future directions of saliency methods."
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "--- Summary ---\n\nThis paper focuses on the confounder problem that spatially-seperated image regions (e.g. shoulders of xray images) might spuriously correlated with the target (e.g. pneumonia). If given a human-labeled region that is deemed important, we can decrease this spuriousness by regularizing the model toward the important region. They not only compare with several existing saliency-based methods (RRR and GradMask), but also propose 2 new methods (ActDiff, Adversarial) inspired from domain adapataion literature that the representation of the classifier should be similar between original image and the masked image (the image that the non-important region is shuffled). They compare in 1 synthetic dataset and 2 xray datasets. They show that (1) these methods (sometimes) hurt generalization when spuriousness does not exist, and (2) the model's saliency map is only weakly correlated with generalization performance, and thus doubting the validtiy of using saliency maps for diagnosing whether a model is overfit to spurious features.\n\n--- Pros ---\n\n1. Very detailed hyperparameter search and lots of repeated run (10) to get good standard deviation\n2. Visualizations of average test images are very intriguing.\n\n--- Major comments ---\n\n1. It seems that when there is covariate shift, the ActDiff performs the best but hurts the performance when no shift exists, but saliency-based methods do not suffer this. But given that you can tune the lambda of the regularization, why could it happen? Can't you just pick lambda close to 0?\n\n2. Some visualization looks suspiciously abnormal. For example, RRR in Figure 5 has a big blur in the middle. Why is that? Also, in Figure 7, Masked focus on the neck to predict but neck is clearly outside of the bounding box. How should we explain this phenomenon? Besides, in Figure 7 upper rows (VPC), Adversarial seems to have better masks by focusing on the lungs, and ActDiff does not. But their IOU is reversed: Adversarial has lower IOU and ActDiff has higher IOU in Table 2. \n\n3. In all the experiments, the validation set is always assumed correct without any background shift. While in real scenario it might not always be easy to access to a validation dataset without any shift. When we instead only have a biased validation dataset, which method will perform better? Will the result change?\n\n--- Minor Comments ---\n\n1. Some hyperparameters for visualization like Gaussian smoothing and thersholding might need justifications. Especially the maximum value capped in 50th percentile seems to be a bit excess.\n\n2. When visualizing the gradients (Fig. 5, 6, 7), maybe we should only include images that model predicts correctly? The gradient of extremely wrong predictions might not be very meaningful.\n\n3. The Figure 8 shows the scatter plot between Test AUC and IOU. It seems Synthetic and Xray SPC have much higher IOU overall. Maybe because their bounding box or confounder is location-wise fixed while only the RSNA VPC has varying bounding boxes?\n\n4. The masked baseline perform much worse in No VPC with AUC=0.5 which is random guessing. But it does not happen in other datasets. Maybe it's because the VPC has smaller bounding box and thus only access to such region is too difficult?\n\n5. The lambda should be included in all the equations (eq. 1 to 5).\n\n6. The final hyperparameter should be reported in the appendix.\n\n--- Overall evaluations ---\n\nOverall I like this paper. The hyperparameter tuning is very thorough. The conclusion is good that great saliency map does not mean better accuracy and vice versa. The experimental results are a bit unsatisfying that no real data is improved. And sometimes the simple masked baseline outperform others. I am happy to increase my score if my major concerns are addressed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to a (very) real albeit (very) specific problem",
            "review": "This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image. \nThe paper falls well within the scope of the conference.\nIt is overall well written, but it is so crammed with information that, in order to comply with paper length limits, its 'digestion' and \ninterpretation becomes difficult at times (this is clear in the case of images, whose meaning often has to be half-guessed due to lack of details)\nThe problem this study deals with is definitely important in the context of medical decision making on the basis of image, and the problems it pinpoints are more than real (small sample sizes, inter-site heterogeneity, bias due to human intervention in the masking process, etc)\nThe proposal seems sound to me and the experiments convincing. My main qualm concerns their specificity, given that they address a very specific domain.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Saliency is a Possible Red Herring When Diagnosing Poor Generalization",
            "review": "The reviewed paper explores the relationship between the quality and spatial distribution of the saliency maps produced at inference time and the model's generalization performance. The authors employed a number of existing methods as well as proposed and implemented their own technique (ActDiff) to align saliency maps with causally plausible regions. All methods were applied on synthetic and real-world data in a series of clever experiments, showing little correlation between saliency map spatial alignment and performance on unseen data.\n\nOverall, the paper seems to be technically sound, claims justified, and supported by the evidence presented in tables and figures.  Importantly, the paper raises a very interesting point, challenging the status quo in the field. The manuscript is relatively easy to read and understand. For all these reasons, I vote for accepting. \n\nMajor questions-concerns:\n* Figures 6 and 7 show mean saliency maps from randomly selected test images. Columns of these images show outputs from different algorithms used in this work. My question is why do saliency maps produced by methods such as Masked on test images show significant activity in the regions that were explicitly made useless for training (by randomly shuffling pixels outside masks)? I believe this requires a more elaborate and explicit answer.\n* Perhaps it makes sense to openly recognize that before completely dismissing the validity of using saliency maps for diagnosing overfitting a lot more datasets must be studied as the two presented in the paper, may not be enough to provide conclusive evidence.   \n\nSeveral minor comments and questions:\n* Some acronyms, e.g. PAC on the first page is not defined before being used.\n* Caption for Figure 1 can be made more clear, as there is no explanation of what \"pathology correlation with site/view\" means. It is only later in the text, the authors add that they have intentionally biased positive cases by sampling them mostly from either one site or one view. But before reading this part, the caption remains confusing for the reader.\n* It might be a good idea to reformulate contributions of the paper into nouns instead of verbs e.g. instead of \"Create a dataset\" - \"A dataset\".\n* A sentence from the related work section, namely: \"Zhuang et al. (2019) was additionally designed...\" should be reformulated. \n\nUpdates: Thanks for the authors' response. I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "On attributions and generalizability: lack of depth and not clearly grounded assumption",
            "review": "Updated recommendation after major changes to the submission: thank you for addressing my comments.\n\nShort summary\n----------------------------\nThe authors investigate the relationship between model generalization under distribution shifts and attribution techniques. They hypothesize that imposing “better” attributions in a model (which they define as being more aligned to a mask selected by domain experts) would increase model generalizability. However, they observe that such constraints hurt the performance under no shift, and do not necessarily lead to increased performance or “better” feature attribution maps under shift.\n\nStrengths\n-----------------------------\nThis work investigates 3 datasets: one synthetic dataset where the effects are well-controlled for, as well as 2 manipulations of real-world data in medical imaging. It includes different and recent techniques to constraint model learning by masked representations, and does not make bold claims about the results.\n\nWeaknesses\n----------------------------\nThere are however a few weaknesses that represent major concerns to me:\n1) The main assumption underlying this work is that improving feature attributions will help generalizability. I however do not think that this is the message that was relayed in past publications on the topic (contrarily to what is mentioned in the introduction) and do not think that “better” attributions is sufficient for better generalizability. My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts. Making this a sufficiency condition is a step that is not well-motivated to me.\n2) The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent model’s decisions and to be sensitive to different factors (including shift-variance, Kindermans et al., 2017, or being susceptible to adversarial perturbations). In addition, different attribution methods will likely display different patterns.\n3) Which is why I am wondering whether other attributions, which are more theoretically grounded (e.g. integrated gradients [Sundararajan et al., 2017] or more recent gradient-based techniques like DeepLIFT, or from non-gradients based techniques like SHAP or occlusion) were investigated.\n4) The authors mention that masks represent a “good” attribution map. However, there is no guarantee that features in those masks are not affected by distribution shifts. This should be discussed as a limitation of mask-based regularizers.\n5) The authors seem to have missed that the “masked” trained classifier also highlights the confounder on the synthetic data, despite it correctly predicting the outputs. I believe this is related to the choice of saliency maps, as “raw” gradients do not “explain” the decision of the model, i.e. it does not display the effect the features have between a bad or neutral decision and the current prediction. For this reason, I would suggest using integrated gradients or another technique.\n6) The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion.\n\nNovelty:\n------------------------\nThe authors propose two novel methods to constrain models by using masked representations. While in theory they seem interesting (e.g. activations have been successfully used for OOD detection), the results obtained vary per dataset and there is no clear advantage in terms of model generalization to be using their technique over other methods.\nI feel that the main assumption is not well-grounded, and hence, to me, novelty does not reside in the field tackled.\n\nClarity: \n----------------------\nMainly, I found the paper well written and the experiments clear. I would suggest some proof-reading (see minor comments, some repeated or missing words). Mostly I would suggest to revise the introduction as it wasn’t clear to me what the purpose of the study was until well into the experiments. I also found that the introduction is not well matched to the main message of the paper (e.g. it does not mention mask-related constraints).\n\nRigor:\n------------------------\nI found that the comparison between the proposed technique and the literature was well explained and sufficient. Hyper-parameters were described and confidence intervals were provided on all results. I enjoyed the fact that three datasets were included. Overall, I think the experiments were well executed.\n\nDetailed comments:\n-----------------------------\n- “A critical assumption underlying these aforementioned works is that properties of the saliency map are indicative of generalization performance.” I do not agree with that statement given my major concerns (1).\n- Intro: masking is mentioned in the key contributions but not before and no justification is used. Such masking also assumes that the shifts across train and test distributions do not impact features in this mask. This seems like a strong hypothesis to me, especially when considering e.g. different imaging sites, or image resolutions.\n- It is unclear whether the goal is to obtain models that generalize better or whether it is to obtain feature attribution maps that are consistent with expert input. These two goals can be misaligned, as displayed in the results and should not be conflated.\n- actdiff method: can lambda be mentioned in the equation? How would such a regularizer perform in a high dimensional layer (curse of dimensionality)? Why use pre-activation outputs? Were any other versions of this formulation tried?\n- How were the masks defined? Is there variability per sample (e.g. different experts) in their definition?\n- The authors refer to the synthetic dataset as the changes in the chest x-ray but then have a separate synthetic dataset. The language is confusing and datasets should be defined before being referenced to.\n- Synthetic data results: the masked model performs best, ignoring the confounder. However, the saliency map reflects that the attributions are high for the confounder. Therefore, I do not see the same “high correlation” between IoU and AUC that the authors mention. In addition, this, to me, reflects the main limitation of attribution maps: they do not reflect the model’s decision. They rather reflect the local effect of a feature on the label (Lipton, 2016, Ancona et al., 2018). I am wondering why the authors select saliency maps compared to e.g. integrated gradients (Sundararajan et al., 2017) or other gradient-based but more mathematically grounded techniques. Given my own experience of attribution techniques, it is likely that using different attribution methods will lead to different conclusions with respect to the correlation between IoU and AUC.\n- I understand the choice in the setup of confounders for both datasets. However I am unsure how this represents real-world settings. For example, training sets might indeed be site-specific, but it would be surprising to me that the test presents the inverse of the confounder. We could for example expect the absence of that confounder (which could be simulated by removing the confounder in the test set for synthetic data), or lower correlations between a feature and the label.\n- ActDiff substantially decreases model performance in the absence of confounders\n- If the goal is to obtain more generalizable models, other techniques could be envisaged when the relationship between label and confounder is not deterministic, like resampling or reweighting. Were these considered?\n\nMinor:\n------------\n- Intro: PAC undefined\n- intro: the authors conflate the behavior of a model and which inductive biases it relies on, with the obtained saliency maps. This is however a complex and, in my opinion, unanswered question.\n- Figure 1: NIH, PC, PA and AP not defined. SPC and VPC are presented succinctly without intuition and we only understand them much later. Maybe this figure should be moved in the results.\n- contributions: point 1 needs proof-reading\n- “out of distribution feature attribution phenomena”: I searched DeGrave et al for this term but could not find it. If not used elsewhere, I would rephrase as this is a confusing formulation: samples/images can be OOD, and these samples can provide feature attributions, but the feature attributions themselves are not OOD.\n- proof-reading of the text is required. Some explanations are poorly framed and can be rephrased (e.g. related works, paragraph 2).\n- Zeiler & Fergus, 2013 (published at ECCV 2014) refers to the work on occlusion, where inputs are masked by a baseline value and the changes in predicted risk is defined as their attribution. While this technique provides one attribution per feature, it is not based on the gradients of the network. This paper also does not refer to the term “saliency”.\n- Formulas 4 and 5, consistency in the formulations is desirable: if showing for binary classification in most cases, it is better to keep this set up for other formulations (even if they could be extended to multiclass), especially as the experiments are run on binary classification. The limitation of GradMask to binary cases could however be mentioned.\n- Figure 5 can be difficult to investigate without colormaps. For instance, it looks to me like the “masked” training model does highlight the confounding factor.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}