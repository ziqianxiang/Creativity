{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper attends to the problem of how to implement dense associative memories (i.e. modern Hopfield networks) using only two-body synapses. This is interesting because modern Hopfield networks have much higher capacity, but at face value, they require synapses with cubic interactions between neurons, which to the best of our knowledge, is not a common feature in neurophysiology (though it should be noted: it is not by any means impossible from a physiological perspective to have cubic interactions at synapses, see e.g. Halassa, M. M., Fellin, T., & Haydon, P. G. (2007). The tripartite synapse: roles for gliotransmission in health and disease. Trends in molecular medicine, 13(2), 54-63.). \n\nThe authors show how the use of a layer of hidden neurons, akin to a restricted Boltzmann machine architecture, coupled with the right energy function, can be used to recover dense associative memory models using only two-body synapses. They also demonstrate how this connects to recent work on the relationship between attention mechanisms in modern ML models and Hopfield network dynamics.\n\nOverall, the reviewers were positive on this paper. The most common critique related to the question of \"biological plausibility\". The authors addressed these concerns by adding some more recognition as to the lack of biological plausibility and more discussions of the relevance to neuroscience. To be candid with the authors, if the goal is indeed to make a more biologically plausible model of modern Hopfield networks, than a fair bit more work would be needed to connect the paper to biology well. As it stands, the only connection is the shift to two-body synapses by using hidden neurons, but this provides limited insight for most neuroscientists, as noted by Reviewer 2. Also, some of the biological examples provided seem strained (e.g. the colour memory example, where there is no physiological reason to posit that we store colour memories using our retina, or the MNIST example, since there is no reason to suppose that animals can memorise thousands of specific MNIST images). But overall, the critique regarding biological plausibility was attended to. The other concerns raises were also largely addressed. \n\nGiven the interesting contributions from this paper, the overall positive reviews, and the decent job at addressing reviewer concerns, the AC believes that this paper should certainly be accepted. A decision of \"Accept (Poster)\" seems appropriate, though (as opposed to an oral or spotlight), given the lack of biological connections in a paper with a stated goal of achieving a more biologically realistic model."
    },
    "Reviews": [
        {
            "title": "Associative Memory in Bio and ML",
            "review": "Summary: In the current work, the authors describe a novel memory structure, and mathematically show how this is a superset of previously published models. The paper could be of interest to anyone investigating the theoretical side of learning and activity rules.\n\nStrong Points: Unifying the multiple discussed architectures, as well as relating back to RBM and classical Hopfield networks is a nontrivial task. Additionally, the use of integrator units (eq 1) provides a clear path how such an energy optimizing network might be implemented in biological networks (eg: LIF neurons).\n\nWeak Points: Biological plausibility is discussed at several points in the paper, specifically regarding the number of units involved in an interaction. However, this plausibility doesn't influence the functional design of the network, and thus feels extraneous. \n\nAdditional Comments:\nRegarding the above: If the feature units are to represent distributed patterns (eg: BERT features), then it should be possible for more than 2 abstract units to interact at a single \"synapse\", so long as the interaction of the actual units (neurons or RELU units) has only two members?\nAre the memory capacity estimates (directly following eq 8) only applicable when the input current is 0? If so, is it possible to derive more general estimates of capacity?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Theoretically sound but the significance is unclear to this reader",
            "review": "This paper provides a mechanism by which recent extensions to the classic Hopfield network model, which as written involve many-body interactions, can be implemented by a more biologically plausible network that uses only two-body synaptic interactions (but more neurons).  \n\nPros:\n\n-- The derivations are sound and well-explained, from what I can tell\n-- Given that Hopfield networks are an important model in the neuroscience community, it is nice to see these extensions to them brought back in to the realm of biological implementation\n\nCons:\n\n-- My main concern with this paper is that I feel like it has not established its significance to neuroscience modelers -- and since the paper is primarily concerned with connecting existing algorithms to biologically plausible implementations, rather than introducing new algorithms, I think it is important to do so.  Dense associative memories and modern Hopfield networks are appealing because of their ability to store memories beyond the O(N) scaling limit of regular Hopfield networks (N = # of neurons).  But in these two-body implementations, the models once again have O(N) scaling in the total number of neurons due to the addition of hidden neurons.  Given this, what is the advantage of these models over the standard Hopfield network?  Are they still more efficient in some way?  More robust?  Do they provide faster recall?  I think a thorough comparison along these lines would be valuable and without it, the paper is less compelling to me.\n\n-- The connection to attention mechanisms feels a bit off in that the equivalence only holds when the update rule is applied exactly once and no more times (I realize this was first shown in prior work).  But given that this paper is attempting to unify several mechanisms under a common framework, it feels like the paper should concern itself with models that really are variants of that framework, rather than introducing additional modifications like cutting off the dynamics early.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, though biological plausibility angle overstated",
            "review": "This paper presents a novel class of associative memory models. The model is expressed as a network with two-body interactions (synapses) and a well-defined energy function, and it is shown to generalise and unify several existing approaches (Hopfield Networks, Dense Associative Memories and Modern Hopfield Networks). Besides its theoretical and computational properties, the model is presented as being more biologically valid/plausible than some of the existing approaches it generalizes.\n\nOverall the paper is solid, well written and highly relevant to the ICLR community. My initial recommendation is to accept. I only have a few nitpicks concerning the \"biologically plausible\" angle.\n\nMore specifically, while I agree that the proposed model is *more* plausible than some of the other approaches discussed, its absolute level of biological plausibility remains limited. The authors recognise this point and devote a paragraph to discussing it at the end of section 2 (\"For the purposes of this paper we defined biological plausibility as…\"), but it seems odd to have this passage buried at the end of the mathematical derivation, rather than up front in the introduction. I suggest that this passage is moved forward to a more prominent location.\n\nFurthermore, given that this is essentially a paper on the theory of abstract associative memory systems, the emphasis given to the biological angle in the title seems eccessive, and the choice of the word \"neurobiology\" somewhat puzzling. In my opinion the title would be a better description of the content of the paper if the reference to biology was toned down.\n\nFinally, in the introduction: \"typical synapses are not highly reliable, and a cortical synapse stores no more than one or two bits of information\". While I agree with the general spirit of the observation that synapses are not typically very reliable, some source should provided to back up the quantitative statement about synaptic storage capacity. I am not a specialist on the matter, but this seems surprising in light of work showing that the capacity of hippocampal synapses can be up to 3 to 5 bits (Bartol et al 2015, Bromer et al 2018).\n\n### References \n\nBartol Jr, T. M., Bromer, C., Kinney, J., Chirillo, M. A., Bourne, J. N., Harris, K. M., & Sejnowski, T. J. (2015). Nanoconnectomic upper bound on the variability of synaptic plasticity. Elife, 4, e10778.\n\nBromer, C., Bartol, T. M., Bowden, J. B., Hubbard, D. D., Hanka, D. C., Gonzalez, P. V., … & Sejnowski, T. J. (2018). Long-term potentiation expands information content of hippocampal dentate gyrus synapses. Proceedings of the National Academy of Sciences, 115(10), E2410-E2418. \n\n----\n\nPost revision update: my concerns have all been addressed.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Would benefit from better explanation of tradeoff and clearer motivation of energy function",
            "review": "The authors proposed a dynamical system that unifies several associative memory models, including the classical Hopfield network and two recently proposed modern Hopfield networks. The dynamical system is described as interactions between two groups of neurons (feature and memory neurons), providing a more biological interpretation of modern Hopfield networks. The proposed system reduces to different associative memory models by choosing different generalized activation functions, each of which maps inputs to a group of neurons into output activity. This manuscript provides sufficient details for understanding, its derivations are correct, and its results are useful in bringing modern Hopfield networks closer to biology.\n\n### Major:\n\n(1)\tMy main concern is that the tradeoff of the modern Hopfield networks discussed is not clearly stated.\n\nFor example, the manuscript at multiple places described modern Hopfield networks as having large or huge memory capacity with respect to $N_f$. I understand that these statements have been used in the literature. However, since the authors are explicitly trying to interpret modern Hopfield networks as network of neurons, it would be appropriate to discuss the memory capacity with respect to both $N_f$ and $N_h$. For example in model B, if I understood correctly, the memory capacity scales linearly with $N_h$.\n\nRelated to the previous point, the number of memory neurons needed should be clearly described. Is it the number of memory patterns stored across all models?\n\n(2)\tThe energy function should be better motivated.\n\nIn equation 2, the authors introduce an energy function containing two Lagrangian functions, and demonstrate that it reduces to energy functions from several previous models when using different choices of Lagrangian functions. While this is intriguing, it can make the reader feels like this energy function came out of nowhere and magically unifies multiple models. I believe the reader would have a better understanding if the authors provide a stronger motivation for the energy function. For example, is the energy function constructed by explicitly trying to connect the Krotov & Hopfield 2016 model with the Ramsauer 2020 model?\n\n### Additional major points:\n\n(3)\tI think the authors should offer richer references to the literature. Here are some examples:\n\nIn the intro, “a cortical synapse stores no more than one or two bits of information” should be followed by a reference since this is not common knowledge. For example, Bartol 2015 eLife estimates a lower bound of 4.7 bits per synapse.\n\nIn p.4, the authors mention BERT-like system without citing the BERT paper.\n\nThe activation function in Eq. 17 is a form of divisive normalization widely studied in neuroscience (Carandini & Heeger 2012).\n\n(4)\tI find it somewhat misleading to refer to the update rule in Eq. 15 as the “attention mechanism in Transformer networks”. I understand this is what Ramsauer et al 2020 said as well, but I think the update rule in Eq. 15 should be simply referred to as dot-product attention, which is used in machine learning at least as early as Bahdanau, Cho, Bengio 2014 and in many settings besides Transformers.\n\n### Minor:\n\nP.2 “integrates our some of the degrees of freedom”  “integrates out…”\n\nIn P.1, the sentence “a small part of a high-resolution photograph may contain only 1000 pixels, but the number of describable “objects” which might occur in such a fragment is far larger” is a bit confusing. \n\n\n-----------------------------------\nPost revision update:\nAll concerns addressed. Score updated.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}