{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "While many work in the literature (PlaNet (2018), Dreamer (2020), SimPLe (2019), etc.) learn world models to perform well on a particular task at hand, the motivation behind this work is that dynamics models benefit if they are task-agnostic, hence would be able to perform a wider range of tasks, as opposed to just doing one task really well. In order to do this, they propose to learn a latent representation that models inverse dynamics of the system / environment rather than capturing information about the task-specific rewards, and incorporate a planning for solving specific tasks in which they can measure performance.\n\nTo show broad applicability of their method, the authors tested their approach on Atari and DM Control Suite (from pixels), and also simple grid worlds to illustrate the concepts, and demonstrated strong performance over SOTA model-free algorithms (even the ones that do not have open-source implementations). Reviewers and myself agree that the paper is well written, easy to follow, and the approach is well-motivated.\n\nAfter the review period, the authors have done work to improve the draft, particularly including ablation studies with and without planning, addition comparisons, and improved visualizations, after taking in the comments and feedback from the reviewers after the initial reviews, which satisfied some of the reviewers. One reviewer asked for a real robotic task, but I feel that while it will help the paper, many existing works focus purely on DM control from pixels, and this work has performed experiments on both DM Control and Atari, two reasonably different domains, and IMO makes up for the lack of real-world robotics experiment. That being said, a discussion on how the proposed method would work in a real-robotic task, as suggested by R4 would be good to have.\n\nI believe the work in its current state is ready for acceptance for ICLR 2021, and should be a fine contribution to the visual model-based RL works. I'm excited to see this work presented to the community, and I'm going to recommend acceptance (Poster)."
    },
    "Reviews": [
        {
            "title": "Interesting work. Need more justification on the main claim",
            "review": "The paper proposes a model-based reinforcement learning method. The method builds a partial model of the environment through learning inverse dynamics, which is the distribution of action sequences that would bring one state to another state. Through training the model with an iterative relabeling scheme, the model is able to learn to reach goals in a subset of DM Control and Atari domains. \n\nComments:\n+ This MBRL formulation is conceptually simple and does not rely on learning the full model of an environment, potentially enabling long-horizon planning while being able to high-dimensional input.\n+ The relationship and differences to the closely related work Ghosh et al., 2020 is clearly stated and elaborated.\n\n- My main concern about the paper is that although the difference between GCSL and the proposed method is exposited in theory (Proposition 3.1), there is no direct empirical experiment explaining how this difference might play out in practice. Specifically, there clearly are performance gaps between the proposed method and GCSL, but I cannot seem to grasp what EXACTLY is causing this gap. Is it ONLY because of the difference stated in proposition 3.1? If so, I'd like to see a minimum toy experiment clearly showing the qualitative differences. If there are other differences, I'd like to see them addressed in greater details. None of the reasons (i)-(iii) stated in section 3.6 are substantiated experimentally. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Planning from Pixels using Inverse Dynamics Models",
            "review": "Summary: The author proposes Goal-Conditioned Latent Action Models for RL (GLAMOR) a novel approach to learn latent world models by modeling inverse dynamics. The proposed approach learns to track task-relevant dynamics for a diverse distribution of tasks and provide a strong heuristic that enables efficient planning. GLAMOR demonstrates good performance against its baselines in terms of achieving accurately goals, sample efficiency and effective planning.\n\n\nReason for the score:\nPaper address an interesting question related to Reinforcement Learning and provides strong results to back the proposed method.\n\n\nPros:\n-good flow of the paper\n\n-Correct situation of the problem with respect to current research\n\n-A good problem to address in Reinforcement Learning area\n\n-Strong experimental results\n\n\nCons:\n-Can the authors talk about how the proposed method perform in a real-robotic task?\n\n-In the equation 5, a_{<i} means?\n\n-in page 3 last para is not clear to me. Can you rewrite or provide explanation of how it is possible?\n\n-In para 3 of Section 3.4, why you sample from Boltzmann distribution and not uniform distribution?\n\n-Have you considered other exploration methods than of epsilon-greedy?\n\n\nQuestions for the rebuttal:\nPlease address and clarify the cons above \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Planning from Pixels using Inverse Dynamics Models",
            "review": "The paper proposes to learn task-agnostic world models in latent space, using a goal-conditioned inverse dynamics formulation and an action priors model, that learn to predict action sequences. The approach is compared against a model-free unsupervised control method and an imitation learning method on the subset of Atari games and a continuous control benchmark, and showed to achieve superior performance in most tasks based on goal completion ratio and sample efficiency. \n\nThe proposed approach learns two networks, the inverse dynamics model and the action prior model, and uses this to plan a sequence of control actions, rather than learning the policy directly as done by the baselines. Although this has shown to achieve more goals in the 2 benchmarks, the authors do not discuss the learning complexity of training two networks compared to one policy network as done in baselines. The authors conduct an ablation study on the computational complexity of planning at test-time for their approach. I believe it would help to evaluate the method better if they also show the number of trials used when comparing to baselines. \n\nCan the authors provide any real-time performance guarantees at test time?\n\nIt's not clear to me how the exploration and exploitation is traded off in the Data Collection section. The planner is used to generate a sequence of actions but an e-greedy policy is used for exploration. This is confusing to me.\n\nThe paper claims that the inverse dynamics formulation learns to represent controllabe aspects of the state. Has this been theoretically proven before? And have any empirical studies been done to show that this actually helps model-based RL to perform better than when using forward models? It would be interesting to compare against model-based RL techniques based on forward models, if avaialble, for this work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper tackles the problem of visual goal completion. The authors proposed to learn inverse dynamics (actions instead of states) as the knowledge of the world for planning. They tested their results on several benchmarking tasks and compared to recent methods proposed on goal-conditioned RL and policy learning.",
            "review": "The authors adopted an interesting idea for learning inverse dynamics which is different from common approaches where oftentimes a state-transition model is learned for planning. Though in their scenario this does not make any guarantees on the improvement on generalization, conditioning on the rich semantics could be contained in actions, it might be a justification for learning the inverse dynamics model for tasks that test generalization capabilities. \n\nHowever, the idea of using relabeling to facilitate the learning of goal-conditioned policies and world models is not new as discussed by the authors. This makes the learning framework somewhat less significant in novelty. More specifically, the authors put quite a lot of effort into distinguishing their method from a recent method GCSL. The differences actually lead to a question on the action prior. I'm curious if this prior distribution is different from a uniform distribution over all available paths starting from $s_1$, it is not well justified why we need to parameterize this with LSTM. \n\nThere are also several, I believe, typos that might need fixing. The first term in Equation (9), should it be $\\mathbb{E}[-\\log p_{\\theta}(a_1, \\cdots, a_k | s_1, s_k)]? In the experiment section, the model was trained with 500k agent steps while in the figure it was 5M, this leads to the problem of sample efficiency (section 4.6).\n\nOverall, I feel that though the idea of adopting inverse dynamics for learning action model is interesting, it is still somewhat incremental to prior works with a similar framework. The missing justification and details also contribute to the decision of putting a decision of borderline during pre-rebuttal period.\n\n================================================================================================================\n\nThe authors has addressed my concerns well in the discussion period, I have increased the score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}