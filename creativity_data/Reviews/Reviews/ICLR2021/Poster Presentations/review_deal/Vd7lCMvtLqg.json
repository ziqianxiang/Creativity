{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a method to cope with large vocabulary sizes. The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. They give an end-to-end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process). They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. Finally they give a variety of experiments, particularly in language and recommendation tasks. The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users. \n\nThis paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis. One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (e.g. if the ideal number of anchors changes over time). "
    },
    "Reviews": [
        {
            "title": "This paper proposes a practical solution to cut the embedding storage. But the Bayesian interpretation is not persuasive and there are still missing pieces in the experiments.",
            "review": "This paper introduces a row-rank approximation of embeddings using “anchors”. It also proposes a probabilistic interpretation of their method as a non-parametric Bayesian dictionary learning model, which can be inferred by optimizing the small-variance asymptotic objective.\n\nWhat I agree with the authors are:\ni) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million).\nii) The initialization for basis vectors is extremely important and should be updated through training.\niii) Experimental results look reasonable in this paper.\n\nWhat I feel confused about are:\ni) Why interpret this method in a Bayesian non-parametric way? To be more specific:\ni.1) The final objective function (5) does not involve Bayesian posterior inference. If you want a point estimation of sparse representation + learnable anchors, you don’t need a Bayesian model.\ni.2) Bayesian non-parametric is useful because it can automatically learn the model size. In your case, |A|. You mention this point in Figure 3, but there is no online learning result showing that your model has the capacity to grow the model through training. One example is “Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes” by M. Bryant and E. Sudderth.\n\nii) The vocabulary size in your experiment is decent, but not very big. Normally in a recommendation system, the vocabulary size can be the number of users, which is at least 100M. Normally the embedding size is around 16 to 64 in real systems. The proposed method could be a huge gain in storing such a huge embedding table. But I cannot see an experiment at this vocabulary level. Even rough results at this level could make this paper much stronger.\n\niii) This is a minor point, but AUC results in MovieLens besides MSE can reflect the ranking quality in recommendations.\n\nOverall, this paper proposes a practical solution to cut embedding storage. But the Bayesian interpretation is not persuasive and there are still missing pieces in the experiments.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A neat idea to pose sparse embedding model as a sparse linear combination of dense latent vectors. Nice interpretation as a Bayesian prior. Good empirical evidence on multiple NLP and Information Retrieval tasks. Might need comparison against some other sparse models like Compostional Embeddings, SNRM, SOLAR.",
            "review": "This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. When the vocabulary size |V| runs into several 100Ks or millions, it is impractical to store one dense vector per label. Hence the paper proposes to only store a few anchor/latent vectors (the matrix is A with |A|<<|V|). All label vectors are expressed as linear combinations of a 'few' anchor vectors. To train this end-to-end, we need a transformation matrix T such that T*A = E (E is V\\times d embedding matrix). T has to be structured, i.e., each row of T has to be sparse and positive only (although negative weights are also fine, I'm not sure if weight redundancy is that important). \n\nThis pipeline is trained end to end using YOGI optimizer for regular gradient updates and 'proximal gradient descent' for T which does soft thresholding with a lower bound of 0 (accomplishing both sparsity and positivity part).\n\nThis design admits multiple ways of initializing the anchors A. And the authors perform experiments with both frequent token vectors and random anchor vectors (both have their merits, random seems to be a robust choice).\n\nThe authors provide a statistical interpretation of their approach using a generative formulation to the embedding vectors in terms of the latent vectors (using a Indian Buffet Process membership matrix Z).\n\nThe experiments span two major domains, NLP and Information Retrieval. Across multiple NLP datasets, ANT outperforms Sparse-Coding (Chen et. al. 2016)  and Post-Sparse-Hash (Guo et.al. 2017). On the IR task with MovieLens dataset, the primary comparison is against SLIMMING (Liu et.al. 2017). While gains are substantial on the NLP tasks, they seem minimal on the MovieLens task.\n\nI've listed most pros above. The cons are here:\n1. The idea seems a little similar to Compositional Embeddings (Shi et. al. 2020, Ginart et. al.2019). It might warrant a discussion or comparison.\n2. There are other sparse embedding methods like SNRM (Zamani et.al. 2018) and SOLAR-Sparse Orthogonal ...(Medini et.al. 2020) which might be comparison candidates at-least for IR tasks.\n3.  The precision in table 1 for ANT anomalously increases when |A| is reduced. ANy explanation as to why this happens? The information bottleneck is supposed to reduce precision right?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An ingenious two-step method on representation learning",
            "review": "In this paper, the authors proposed a method to learn efficient representations of discrete tokens. They took a two step approach: in step 1, they learn \"full fledged\" embeddings for a subset of anchor tokens. In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. This two-step approach reduced the overall number of parameters. The sparse matrix T can also encode domain knowledge (e.g. knowledge graphs). In the experiment section, the authors showed that their approach has good performance on several language tasks, with far fewer parameters.\n\nIn general the paper is well written and the flow is easy to follow. I find the main idea plausible and ingenious. For language tasks and word embeddings, anchoring method has been shown to be effective in several tasks already (e.g. [1] http://papers.nips.cc/paper/8152-the-global-anchor-method-for-quantifying-linguistic-shifts-and-domain-adaptation). The authors took two steps forward: 1) instead of in [1] where the entire vocab is used for anchoring purposes, the authors used a subset of tokens which reduces the amount of parameters. 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. Although this paper is probably related to other strains of research (e.g. leaning manifolds for IR/NLP where anchoring is also a key concept, which the authors could have admittedly surveyed more), I particularly liked the fact that the two-step procedure decomposes two tasks that are often mixed together for embedding tasks: learning representation vs learning relations.\n\nWhile the authors claimed that they can further impose domain knowledge in the learning process (which I think this is at least a good attempt), this part in general feels a bit less convincing. To be specific, there can be a variety of knowledge (like related, is a subset of, analogy, etc.). It is not clear how the distinction of different types of knowledge can be incorporated. What the authors proposed is lumping them into the notion of \"positive pair\" and relax constraints on them. This may or may not suffice (for the purpose of adding domain knowledge), but on paper, there is a chance that some finer structures of the domain knowledge may get lost. It's not clear how much gain (especially the experiment section, for fair comparison purposes where other methods do know use domain knowledge in particular) is from incorporating domain knowledge; an ablation study might help.\n\nAnother question is about training. It's not obvious to me how to guarantee that for every row of T, there is at least 1 non-zero element. Is some specific tuning needed for rows corresponding to rare words? How the regularization strength $\\lambda$ on T is selected?\n\nThe reduction of parameters is while keeping task performance is illustrated quite well in the experiment section. Their method does not reduce the theoretical complexity (still linear w.r.t. vocab size, as T must have at least one element per row), but in practice the reduction (which mostly comes from savings of dimensionality) is quite obvious.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}