{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work is likely to lead to more connections between machine learning and neuroscience at a fine-grained level where ML methods can help explain and understand neural circuits.\n\nTo encourage this, it would be helpful if authors described the biology of the PN-KC-APL network and the known constraints over possible formalizations of that network. The authors present one formalization, but little discussion is given toward the design space for such models. Are there other possible ways to describe the PN-KC-APL network? Are all alternate ways to do so equivalent to the model presented here? What properties are unknown and how could they affect the formalization presented here?\n\nOverall, reviewers agree this is a good submission."
    },
    "Reviews": [
        {
            "title": "a biological network inspired model that learns unsupervised word embeddings",
            "review": "This paper presents a simplified model of a biological system, and asks the question of whether it can learn unsupervised word embeddings. The objective function and its optimization are clear and unreasonable. (Note: I do not have the expertise to comment on whether this is an accurate characterization of the working of the biological system.) The authors connect the optimization problem to other recent learnings in this area such as sparse learnt projection -- this perspective is appreciated. \n\nTo establish the quality of the embeddings, the authors compare them to GloVe and BERT embeddings on word specific tasks like similarity and contextual tasks on datasets like SCWS. In these comparisons, the quality of embeddings is relatable to GloVe, and inferior to BERT. There is also a small qualitative demonstration of the embeddings (Fig. 3) This tells us that the embeddings are meaningful, and their quality is comparable to Glove, if not the newer generation embeddings. I think this is a good outcome for the model they develop.\n\n For these reasons, I am generally in favor of accepting the paper. However, I have reservations about particular aspects, and request the authors to edit the following content.\n1. SOTA claims. GloVe is a great candidate to compare a new methodology to, but is not SOTA for unsupervised embeddings. There are many other unsupervised non-DL methods (e.g. Word2vec) which do better than GloVe on specific tasks. It is important to clarify and cite appropriate SoTA for each task as of the time of writing.\n\n2. I am not sure the comparison to GloVe for contextual evaluation is the most appropriate. There are other works (e.g. Word2Sense, ACL'19 and citations therein) for stronger contextual embeddings. Please bechmark against this.\n\n3. I feel the comparison with BERT is mostly irrelevant. I think comparison with GloVe and related work are sufficient to show that the new methodology has something going for it. I would not have expected it to compete with BERT like models nor serve all their use cases. Therefore, claims that training is faster than BERT are also mostly unnecessary. \n\n4. I would instead place more emphasis on qualitive and quantitative evidence that the embeddings capture contextual meanings, polysemy etc. Consider expanding section 3.2 with more evidence.\n\nWith these changes, I would make a stronger recommendation.\n\nRevised rating to 7 after revision.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The method is interesting but the experimental evidence is not good enough",
            "review": "Although the paper does not say so, my understanding is that the proposed word embedding method actually first perform Kmeans-like clustering on the context vector shown in Figure 2. In the binary word embedding of each word, we set the dimensions corresponding to the k closest cluster centers to 1 and 0 otherwise. Most parts of the paper are describing how the simple word embedding method is related to the neural system of a fruit fly and showing the method achieves comparable performance compared with GloVe in word similarity tasks, context-sensitive word similarity datasets, and document classification tasks.\n\nPros:\n1. It is interesting to see the connection between a biological neural system and word embedding.\n2. As far as I know, the method is novel.\n3. Such a simple and biologically plausible method could reach reasonable performance. \n\nCons:\n1. The experiments have not demonstrated that the method is a useful word embedding method for NLP researchers. Some of the experiment designs, comparisons, and presentations are misleading.\n2. Some related work and comparisons are missing.\n3. The authors actually do not explain why this method works well in machine learning perspectives (similar to a biological neural system is not a very strong explanation for many people).\n\nClarity:\nThe paper is easy to understand, but it does not explain why the method works from the ML perspective.\n\nOriginality:\nAs far as I know, the method is novel.\n\nSignificance of this work:\nThis might have a large impact on the neuroscience field.\n\nThe method is compared with GloVe most of the time. First, GloVe actually does not perform well in word similarity tasks compared with word2vec/SGNS [3]. In terms of document classification, the results are mixed and I think the authors should try more datasets to know which method is better. In addition, GloVe is trained on a different dataset, so the result is not very comparable. Second, GloVe is a dense method. The proposed method should be compared with binary word embedding or compressed word embedding methods such as [1,2]. It will be even better if you can show that it is better than sparse representation such as PPMI in [3], even though PPMI is not binary and uses much more dimensions (you might want to use the dot product values instead of binary value in Equation (4) to improve your method and make the comparison fairer). \n\nAs I mention at the beginning, I believe the word embedding is actually a binary projection to Kmeans-like cluster centers in the context vector space. Here is why. Let's first assume W_{mu} and v^A are normalized and ignore the normalization term /p to build the connection. Then, the l2 distance is (1 - dot product). Equation (1) tells us that we are minimizing the l2 distance between v^A and its closest cluster center (i.e., W_{mu}). Finally, Equation (4) clearly describes the top k projection. If I am right, I think adding this perspective will make readers understand the method better. If I am wrong, please explain why and try to explain what the method is actually doing in some machine learning perspectives. \n\nIf I am right, we will find the description section 3.2 might be misleading. The main reason that the average cosine similarity within the cluster is higher might be because the binarization projection makes the word embeddings locally collapse to small cluster centers, which makes the word embeddings cannot capture the fine-grained statistics details. You can visualize the word embedding in a 2D space to see if I am right. If I am right, this is not an advantage in my opinion.\n\nIf the authors could show that the proposed method is state of the art compared with binary word embeddings in a fair setting (e.g., train on the same dataset), I will vote acceptance. Otherwise, I am very likely to keep my weak rejection vote.\n\nMinor:\n1. The definition of p before Equation (1) is unclear. Why do you have a mod here?\n2. The optimization method in Equation (2) is unclear. What does dt mean? Is t the time step in iterative optimization? Is this a kind of gradient descent? I guess the differential equation comes from neural science and it needs more explanations to make NLP researchers understand. It would be better if the author can compare with other optimization methods such as gradient descent and EM algorithm for Kmeans clustering.\n\n\n[1] Tissier, Julien, Christophe Gravier, and Amaury Habrard. \"Near-lossless binarization of word embeddings.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n[2] Wang, Yuwei, et al. \"Biological Neuron Coding Inspired Binary Word Embeddings.\" Cognitive Computation 11.5 (2019): 676-684.\n[3] Levy, Omer, Yoav Goldberg, and Ido Dagan. \"Improving distributional similarity with lessons learned from word embeddings.\" Transactions of the Association for Computational Linguistics 3 (2015): 211-225.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Biologically inspired word embeddings with interesting results",
            "review": "Summary: \nThe authors present a formalization of a simple biological network (the mushroom body of the fruit fly), that allows very efficient “biologically inspired” word embeddings. They train this network to generate both static (context-independent) and context dependent embeddings, and evaluate these embeddings using several metrics comparing mainly to GloVe embeddings, and to some extent to BERT embeddings. Although the results are sometimes inferior, they are overall comparable, and importantly achieved at significantly lower computational resources. The main contribution of this work is not this specific network formalization (which is nice), but rather demonstrating that formalizing biological networks can generate more efficient algorithms, that achieve results comparable to the complex algorithms used ubiquitously.\n\nStrengths:\n+ The approach of the paper, namely generating word embeddings using a formalization of a well-known biological network, is interesting and inspiring\n+ The authors evaluate the embeddings using both intrinsic and extrinsic evaluation methods, which is important\n+ The learned embeddings are surprisingly strong, given the small and computationally efficient network. Furthermore, the embeddings seem to separate concepts well. These results support the idea that correlations between words and contexts, can indeed be extracted from raw text (in an unsupervised manner) by this network. I find this important because although the biological network of the fruit fly clearly did not evolve to perform NLP tasks, the fact that it evolved to process input from several modalities seems to allow it to learn efficient embeddings. \n\nWeaknesses:\n- I would have liked to see comparison of the resulting embeddings to more than GloVe. E.g. see https://arxiv.org/pdf/1901.09785.pdf, who discuss intrinsic and extrinsic evaluations of embeddings and compare to many embeddings. For example, GloVe is actually not the best for word similarity. \n- For the extrinsic tasks describes in the document classification section, I would have liked to see a comparison to BERT as well, not just GloVe (or even other embeddings). I think these tasks can indeed use context.  \n- I think it would also be useful to evaluate on more extrinsic tasks, although given the space constraints I understand this may be difficult.\n- Finally, it would have been good to see more analysis of the embeddings, e.g. by doing error analysis in one or more of the evaluation methods.\n\nRecommendation:\nI vote for accepting this paper. I very much like the approach of looking at biological networks and trying to learn from them how one can efficiently perform computations. I would have liked to see a more in-depth comparison and analysis in some places, but the results as-is are promising, interesting and inspiring enough to be of interest to the ICLR community.\n\nQuestions and minor comments:\n1. In Figure 2, the legend reads to me as if the context words are in light blue, but in the figure both target word and context words are in light blue.\n2. I think it would be helpful to make a comment that context in this paper is essentially Bag Of Words, as there is no positional information.\n3. In the semantic similarity section, the authors refer to the supplement for binarized GloVe embedding results. I looked at the supplemental results and I found them compelling. I recommend at least saying the conclusion in the main paper (that your binary word embeddings are competitive with binarized GloVe embeddings), rather than just sending the reader to the supplement. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}