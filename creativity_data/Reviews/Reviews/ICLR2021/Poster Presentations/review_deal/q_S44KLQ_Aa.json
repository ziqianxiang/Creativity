{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers and AC liked the basic idea of how this paper improves on ALISTA, and the initial scores were high. Because the contributions rely quite a lot on empirical demonstrations, the reviewers asked for more experiments, changes to experiments, and timing results. The revision and rebuttal addressed most of these requests.  The multipath channel estimation problem was interesting though outside the scope of the AC and reviewer's expertise, so it is hard to evaluate how helpful the method is in that particular setting."
    },
    "Reviews": [
        {
            "title": " A reasonable improvement over LISTA with somewhat insufficient experiments.",
            "review": "This paper adds LSTM to adjust the step size and threshold for LISTA, an optimization algorithm of sparse regression problems.\n\nUsing LSTM to dynamically determine those optimization parameters seems to be reasonable.\nStill, I found the experiments to be a bit insufficient to convince me of its improvement over LISTA or even most vanilla solvers: Figure 3,4,5 all showing MSE to some iterations when only one optimizer reaches its optimality, or even not one reaching optimality. I also think the other algorithms, with the step size tuned better, could be reaching faster convergence, however, the paper fails to mention how the parameters are tuned for those methods. On top of this, no mentions of the actual running time of the optimizer as LSTM could be really slow.\n\nsmall questions:\n\nWhat is the point of the study showed in figure 1&2 as by definition,  as $W$ is already supposed to be an isometric mapping?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting application of LSTM to ALISTA based on empirical observations",
            "review": "This paper extends the framework of ALISTA, a variant of learned ISTA called Neurally Augmented ALISTA (AG-ALISTA), which significantly reduces the number parameters in the model (down to 2 scalars per layer, one for step size and the other for the threshold in soft-thresholding function). Specifically, the authors use a LSTM to generate these two parameters in each layer along iterations, taking reconstruction error related signals as input. This method is based on (1) the previous previous finding of the relation of the step size and threshold with the $\\ell_1$ signal recovery error; and (2) the empirical observation of the correlation between the $\\ell_1$ signal recovery error and reconstruction error. Experiments in synthetic setting show the superiority of AG-ALISTA over ALISTA and other variants that follow it, especially in settings where the compression ratios are challenging, which is claimed to be more realistic in real-world settings.\n\nPros:\nThe most interesting and novel part of this paper is the use of LSTM for the generation of the step size and threshold parameters. The authors use two types of signals related to reconstruction error as the input to the LSTM, which is based on the restricted isometry due to sparsity and observed to be reasonable. And the observation of \"not weak\" correlation between the $\\ell_1$ recovery error and previous reconstruction error also makes it reasonable to use a LSTM structure. These are all interesting observations. Also, the synthetic experimental results do corroborate the effectiveness of the AG-ALISTA model.\n\nCons:\n- Firstly, it is kind of a pity that the authors do not provide some real-world experiments, e.g. compressive sensing, where the compression ratios are challenging indeed. I think this paper would be strong with taht kind of experiments.\n- I think eqn (11) and (12) hold when the measurements are noiseless; otherwise they are not exactly accurate. However, noiseless experiments are not presented, nor is there discussion about the noisy/noiseless cases.\n- Another concern is that baseline performance of ALISTA with 40dB noises when N=500 seems to be worse than that reported in the previous works. Is it because of the training strategy used? According to the description in this paper, I guess the authors do not use the layerwise training but end-to-end training? This is not clearly stated in the paper.\n\nOthers:\n- In the last paragraph of page 3, \"However, the thresholds that make the error bound tighter vary depending on...\" Either \"tighter\" or \"vary\" should be deleted.\n\nOverall I think this is an interesting paper. I am willing to further raise my score if the authors address my concerns/questions.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Improved method for sparse recovery by augmenting ALISTA with good theoretical and empirical motivation",
            "review": "### Summary\n\nThe paper shows that augmenting analytic learned ISTA (ALISTA) with a small LSTM that predicts step sizes and thresholds improves empirical performance in terms of sparse reconstruction compared to comparable baselines, especially as the compression ratio increases (i.e. ratio of measurement dimension M to sparse vector dimension N).\n\nThe proposed method is also nicely motivated by an intermediate step in the ALISTA reconstruction error bounds, where predicting thresholds adaptively given knowledge of the L1 error between estimate at the $k$th step and true target $x^*$ can allow use of a tighter error bound.\n\n\n### Strong points\n\nS1: Clear explanation of the method in terms of its relation to prior work and theoretical motivation.\n\nS2: Nice empirical exploration of theoretical motivation (i.e. examining correlation between various quantities of interest in Figures 1 and 2)\n\nS3: The approach seems to achieve superior performance compared to comparable baselines.\n\n### Weak points\n\nW1: Only uses synthetic data for evaluation. This is fine to study the properties of the method, and sparse recovery is a general method, but I think the paper would be stronger if the authors also used the approach for some kind of real world task or application. Or at least used parameters for synthetic data that match a real world task, e.g. maybe something from communications. This would also help motivate the method's superior performance for higher compression ratios.\n\nW2: A bit more empirical validation of the theoretical arguments would strengthen the paper. In particular, Figure 8 shows that the ratio of threshold to step size versus iteration $k$ is roughly proportional to the true L1 error, but looking at the bound in equation (7), I wonder if there's a stronger empirical validation, e.g. would it be possible to compute the coherence $\\tilde{\\mu}$ and check that the ratio is bounded below by coherence times L1 error?\n\n### Recommendation\n\nI think this is a good paper and I recommend acceptance. I would be inclined to increase my score if the weak points I mentioned above were addressed, and depending on answers to my questions below.\nThe paper is clearly written, the method well-motivated both theoretically and empirically, and achieves superior performance compared to competitive and comparable baselines.\n\n### Questions\n\nQ1: how important is the LSTM architecture? Have you tried other types of RNNs, e.g. vanilla RNN or GRU?\n\nQ2: I was a little confused about the LSTM notation. Does c, h \\leftarrow LSTM(c, h, [r, u]) mean that the output is split into these two vectors? A bit more explanation of this notation would be helpful.\n\nQ3: relating to a weak point I mentioned above: is there some motivation for the choice of synthetic data parameters M, N, S, K, H?\n\nQ4: is Figure 3 the mean reconstruction error over the 10k-example test set? Would it make sense to report std devs across examples for each method?\n\n### Other comments:\n\nC1: I found the Figure 1 caption to be a little hard to understand and there's a typo. Adding some punctuation could help: \"=15, (a) and (b), and non sparse vectors .., (c) and (d)\"\n\nC2: Caption of figure 1 says \"whereas there is no obvious correlation for non-sparse vectors.\". I wouldn't say there's no obvious correlation just from looking at the scatter plots. For figures 1 and 2, how about measuring and reporting correlation measures, like Pearson and/or Spearman? I suggest Spearman b/c the correlation doesn't seem entirely linear in the Figure 2 plots. Adding these measures would quantify the degree of correlation.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "SUMMARY:\nThe paper at hand introduces Neurally augmented ALISTA (NA-ALISTA) which is an extension to the previously proposed analytical learned iterative shrinkage threshold algorithm (ALISTA). Both algorithms belong to the class of learned optimization algorithms for solving the compressed sensing problem, i.e., methods that have parameters which are learned via backpropagating through multiple iterations of the algorithm. The key novelty of the NA-LISTA is the LSTM network used to predict thresholds and stepsized used by the algorithm. The experiments show that this adaptive approach improves the performance of ALISTA.\n\nSTRENGTHS:\n1. After reading the paper I have the impression that the proposed method is thouroughly evaluated. The experimental setup is clear and well-described. Interestingly, the performance of their approach does not depend on wether u^(k) or r^(k) is fed into the LSTM-cell.\n2. The results look very promising and the proposed NA-LISTA algorithm seems to consistently outperform the other discussed ISTA variants.\n3. The method is well motivated.\n4. The paper is very clearly written and well positioned in previously existing literature. All notation is introduced beforehand and it is easy to follow.\n\nWEAKNESSES:\n1. The authors claim that the computational time per iteration is not strongly influenced by the forward pass through the LSTM, because of its relatively small architecture. However, I would have liked to see wall-clock time comparison for the different ISTA variants.\n2. No completely novel theoretical insights. The lemma and the threorem are adapted from Liu et al. (2019).\n3. I find the formulations \"An algorithm which approximates such thresholds, resulting in a tighter error bound, is the aim of this paper.\" somewhat misleading, since there is not tighter bound explicitly stated within the paper.\n\nCONCLUSION:\nOverall, I would recommend to accept this submission. The method is solidly justified and the experiments are convincing. I would like to see wall-clock time comparisons in the final manuscript or in the supplemental material, but this is not a major issue in my opinion.\n\nMINOR REMARKS:\n- At some points throughout the paper the definition of variables, e.g., N=500, is not written in math mode.\n- Also some citations are not correctly formatted, e.g. \"Candes, Romberg, Tao and Donoho (Cand√®s et al., 2006; Donoho, 2006)\" should be \"Candes et al. (2006) and Donoho (2006)\".\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}