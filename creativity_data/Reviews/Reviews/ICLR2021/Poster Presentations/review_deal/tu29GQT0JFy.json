{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "All the reviewers highlight that the paper addresses the important issue of extending deep latent variable models to handle missing non at random data, which are known to be very difficult. The authors suggest modeling the mechanism of missing values and perform inference  using amortized importance weighted variational inference and demonstrate the capacities of their approach on many experiments. The paper highlight the trade-off between the complexity of the data model and that of the missing data mechanism. The authors appropriately answer reviewers comments, add new experiments varying the percentage of missing values, and give more details on the methodological part. \nI also think that this is a valuable contribution to the community, that the literature is well covered (the historical statistical litterature and the ML one), and that it provides new insights and methods to tackle this difficult problem. \n"
    },
    "Reviews": [
        {
            "title": "A generic VAE model for MNAR data",
            "review": "The paper introduces a deep latent variable model (DLVM) for missing data problems where the missing mechanism is missing not at random (MNAR) and therefore cannot be ignored. It presents an approach for fitting the model based on importance-weighted variational inference and reparameterization trick, and demonstrates the application of the proposed method in simulated and real data sets.\n\nPros:\n\n-The paper addresses an important problem of dealing with MNAR data, by introducing a DLVM model that allows for incorporating prior information about the type of missingness (for example, self centoring) into the model. This extends the applicability of DLVM models to a wider class of practical problems.\n\n-The paper is clearly written.\n\n-The experimental results demonstrate the trade-off between data model flexibility and missing model flexibility.\n\n\nCons:\n\n-The proposed approach is a relatively straightforward extension of the existing work (MIWAE, Mattei & Frellsen, 2019), using somewhat standard VAE techniques.\n\n-I’m not sure the experimental results really demonstrated the advantages of the proposed methods against existing ones, particularly in the settings of no prior knowledge about the type of missingness mechanism. Though seeing examples of the trade-off between data model flexibility and missing model flexibility is nice.\n\nOverall, I think the results in the paper should be useful in a number of applications and the paper has enough contributions to merit publication.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting contribution to handle missing data not at random",
            "review": "Review:\n\nThis paper handles the problem of missing not-at-random (MNAR) by extending the MIWAE model to MNAR scenarios. To do this, they use the reparametrization trick in the data space to get the stochastic gradients of the lower bound.\n\nMinor questions/comments:\n\n- At the beginning of section 2, $\\mathbf{x}_i$ is defined as a row the data matrix. However, I have noticed that the $i$ subscript is dropped several times across the text and left simply as $\\mathbf{x}$. Maybe a small comment indicating this at the beginning could help to not get confused while reading section 2.\n\n- In section 4.4. the authors say that both a categorical and a Gaussian observation model are used. I was under the impression that this paper was only evaluated on real attributes, mainly based on the use of MSE and RMSE as evaluation metrics. Are the datasets in Table 1 also a mixture of discrete and continuous variables? In that case, how do the authors compute the MSE of the discrete variables in Table 3 and, if it applies, in Table 1?\n\nSummary:\n\nThe problem of MNAR is very important in practical scenarios, specially when handling tabular data. The paper was well written and explained, and although it can be seen as a simple extension to the MIWAE model, nonetheless I consider it to be a relevant and interesting contribution.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Relevant contribution to MNAR data imputation with potential for improvement in experimental validation",
            "review": "Overall I very much enjoyed reading this paper! The manuscript is very well written, the related work section is sound, the motivation is clear, the methodology is well formalized and the experimental validation is strong in some aspects. \n\nThe formalisation of the missingness process is very intuitive. Also the literature referenced provides one of the most comprehensive overviews on the topic in the statistics community, on the deep learning side it seems that there is a lot of research covered on the side of deep learning latent variable models based on variational auto encoders (VAEs), but the complementary work on Generative Adversarial Networks (GANs) appears to be less well covered. I’m myself more familiar with the VAE approach, and the authors correctly mention the implicit assumptions of GAN approaches, but as GANs offer an intuitive parametric and flexible way of modelling the missingness mask, I guess it would be helpful to see them in the comparison. \n\nIn the first paragraph on page 5 the authors mention that the approach with the reparametrization only works if the data is continuous. I might be missing something, but rather than using a sampling based approach as in Mohamed et al, referenced by the authors, maybe it would be an option to use something like the Gumbel Softmax? In the experimental section the authors use that approach for the recommender system data, with limited success, it seems, compared to a plain gaussian likelihood. But conceptually it would be good to comment on why that’s not possible? \n\nMy main concerns with this work is the experimental validation. The experimental settings explored are UCI data sets, image data and recommendation systems. It’s great that the authors provide such a comprehensive and heterogenous experimental validation in terms of data sets. What I found a bit limiting is that the not-missing-at-random process was so simple and restricted, and that the missingness ratio was not explored systematically. Many studies on imputation use experimental validation that explores missingness ratios between 0 and 100% of the values and in particular the MAR and MNAR settings are explored by, e.g. sampling a random quantile of a feature to condition the missingness on. This is very simple to implement but would allow for a much more realistic account of missingness structure. Especially for a study like this, which makes a presumably important and strong contribution to the field of missing data imputation I would recommend to demonstrate the effectiveness of the proposed approach by a more realistic experimental setting. \n\nAnother recommendation would be to highlight the advantage of the proposed approach with more synthetic data experiments as in figure 1b, maybe one linear and one non-linear data manifold. That would allow to control for more parameters like the rank of the data and the noise, their covariance structure (strength and independence of features and noise, respectively). But that’s not really necessary I think, the authors did a great job with figure 1b! ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The way that this paper adapts deep latent variable models for missing not at random data is quite straightforword and lacks technical depth.",
            "review": "This paper proposes an approach to training deep latent variable models on data that is missing not at random. To learn the parameters of deep latent variable models, the paper adopts importance-weighted variational inference techniques. Experiments on a variety of datasets show that the proposed approach is effective by explicitly modeling missing not at random data.\n\nP1: The related work is well done. This paper reviews the most related studies in several lines of research, including missing data concepts and theories in statistics, missing not at random data in various applications, deep latent variable models for missing data, etc.\n\nP2: The experimental results are quite extensive. This paper conducts experiments on a wide range of datasets from different domains: censoring on multi-variate datasets, clipping on image datasets, and bias on recommendation datasets. The paper also compares the proposed approach against a representative selection of state-of-the-art approaches.\n\nC1: The main concern for this paper is the lack of technical depth. Using variational distribution to derive a tractable lower bound of of a joint likelihood function, using Monte Carlo estimates to unbiasedly approximate the lower bound, and using a reparameterization trick to obtain unbiased estimates of gradients of the lower bound are well-established techniques. It is important for this paper to highlight what is the novelty of the proposed approach in terms of technical innovation.\n\nC2: This paper argues that the proposed approach allows for incorporating prior information about types of missingness. However, it is not clear to me what is the prior information and how does the proposed approach leverage the prior information. It is highly recommended for this paper to provide a formal formulation of the prior information in missing not at random data. It is also recommended for the paper to elaborate how the proposed approach uses the prior information and underlying motivation.\n\nC3: The last sentence in the 4th page of this paper states that it is possible to show that a sequence of objective functions converges to the joint likelihood function. To make the statement more convincing, it would be better if the paper could include a proof that the sequence of objective functions theoretically converges.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}