{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes.  It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/). "
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "This paper proposes a multi-hop transformer method for the video-based object permanence task. The proposed method performs multi-hop reasoning via the encoder-decoder architecture of transformers over critical frames in the video. To mitigate the problem of lacking ground truth for the middle hops, the paper proposes some interesting training tricks. Overall, the paper is well organized and easy to follow.\n\nReasons to accept the paper:\n1. The paper extends multi-hop reasoning techniques that are widely used in NLP domain to video domain, which may inspire other researchers working on other video-based tasks that require multi-hop reasoning.\n2. The paper proposes a new benchmark dataset, which requires longer reasoning chains.\n3. Experiments on the CATER dataset achieve state-of-the-art performance on the object permanence task.\n\nReasons to reject the paper:\n1. The paper claims to address the problem of biased video reasoning, however, all the experiments in this paper are done on a synthetic dataset and it's not clear why such dataset can rule out the possibility of having biases.\n2. The proposed method is somewhat dedicated to a proposed object permanence task, which may not be general enough to be extended to other video-based tasks.\n3. The paper motivates the task using real-world examples, such as asking \"which car was responsible for the accident\". However, such question seems much harder to answer even with the proposed technique for object permanence task. Also, the paper only shows experimental results on synthetic dataset, and leaves experiments on real-world video datasets as future work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper is an interesting work but more ablation and justification are needed",
            "review": "\nThe authors proposed a multi-hop transformer, which takes information encoded in forms of object track and image track as input, to reason over the critical frame sequence to locate the final location of the object of interest. Although I like the idea presented in the paper, I think there are several aspects that will strengthen the paper.\nPros:\n1.\tThe idea of using transformer in a recurrent manner for reasoning in videos is intuitive and interesting. \n2.\tThe authors show the effectiveness of the proposed model by superior quantitative results and the visualization of the most attended object in the inference process of one video sequence.\n3.\tThorough ablation is provided for the proposed multi-hop transformer.\n4.\tThe paper is written and presented well.\n\nCons:\n1.\tOne baseline comparison is missing. The tracking baseline seems to be from prior works rather than from the tracking results produced in the proposed frameworks. It is important to know how well the tracking component itself performs.\n2.\tMore ablation on the framework should be provided. The authors used the DETR for object detection. How does the final performance benefit from using this transformer-based object detection model? Is the proposed framework only compatible with transformer-based object detection?\n3.\tAlthough I like the visualization results, it is still a question whether the most attended object is the most important one in the final results or not [1]. It will be interesting to see the gradient visualization rather than the attention visualization.\n[1] Sofia Serrano, and Noah A. Smith. 2019. Is Attention Interpretable.arXiv preprint arXiv:1906.03731. \n\n\n\n==============\n\n\nI have read the authors' rebuttal information. The authors have addressed my concerns with additional ablation studies and experiments to verify the effectiveness of the proposed multi-hop transformer. And also some experiments are provided  to illustrate whether the most attended object is the most important one. \n\nTherefore, I am still standing on the previous justification for accepting the submitted manuscript.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Major concerns about the algorithm description.",
            "review": "This paper introduces an architecture (Multi-Hop Transformer) for spatio-temporal reasoning in video, focusing on a localisation task for scenes where the object of interest is often occluded (Snitch Localisation task in CATER). The model extracts objects using an external object-detector and predicts objects’ trajectories using the Hungarian algorithm. The Multi-Hop Transformers learns to hop over unnecessary frames, by focusing only on a set of few critical steps (at each time step, the next critical frame is the one containing the most attended object). To alleviate the problems encountered during the training procedure (such as error propagation), several auxiliary training methods are proposed to guide the first few hops or to ensure contrastive debias. Additionally, a harder version of the existing CATER dataset is created, to alleviate the temporal bias existent in the previous version of the dataset.\n\nPro:\n- The “dynamic stride” proposed in the Multi-Hop Transformer is a novel and interesting idea, showing promising results in tasks requiring object permanence. Moreover, the combination of global features and object-centric representations is sound and suitable.\n- The ablation studies (Table 1-2 but also those from the supplementary material) are carefully designed, showing the importance of each component and offering a fair comparison between the methods. \n- The temporarily unbiased version of CATER dataset, where the last visible snitch is distributed uniformly across the length of the video, is crucial for improving the evaluation of different models in video reasoning tasks. \n\nCons:\nThe method is poorly described in the paper (especially in Section 4): the writing is very hard to follow, without a clear structure of ideas. Several details about the sub-modules are missing and the proposed techniques are not supported by any intuitive motivation.  Some example for this are the followings:\n- In the second layer, the Transformer_f module receives as input both U and H. Does it send messages across all types of edges (between tokens in U, between tokens in H and across them)? Same question for Transformer_s. \n- Since the video query should be a single token (the input has dimension 1xd, thus making me think that it is preserved along the algorithm since additional information about the intermediary shapes are not provided), what is the purpose of the first self-attention layer (column 4 in Fig. 3). What tokens should this layer combine? \nThe meanings of each latent representation are not provided (or presented too vaguely e.g. “by reasoning about the relations between the object entities and how would each object entity relate to the reasoning performed by the previous hop or global information”). From the paper, it seems that the \\epsilon variable starts by representing a query embedding, but becomes a video representation immediately after. Should that token be seen as an encoding for the entire video, or as a representation of the object of interest across the video (the snitch in this case)?\n- Why does the Transformer_f send messages between all time steps and not only the U_masked?\n- Since CLEVR_h has a uniform distribution of the last visible frame, why does the Hopper-transformer (last frame) have such a high performance (in Table 2)? Is it able to learn anything else besides the 7% cases when the snitch becomes last visible in the 13th frame?\n- The purpose of the tracking module (the Hungarian algorithm applied on top of predicted objects) is not clearly mentioned. Is it used only for consistency reasons,  to help the model in preserving the positional index across frames, or do those matching-edges guide the transformers in any way?\n- The proposed architecture is composed of several big components (transformers). Thus a comparison against existing methods in terms of the number of parameters and flops would be useful.\n- Since the additional training techniques bring considerable improvement, it seems to me that the proposed algorithm is not very robust to different kinds of noise encountered during training. This could be a problem when applying to real-world scenarios when probably the object detector will not be as accurate as the one used in this synthetic setup.  Do you think that the general algorithm, but especially all the heuristics proposed for the auxiliary losses, will suffer because of that?\n\nMinor:\n- In G.1 (appendix) there is a typo: “stacking 6 transformer decoder layers and 6 transformer decoder layers”\n- In the “Interpretability” paragraph, the sentence … learns to perform snitch-oriented tracking automatically, without having per-frame supervision“ is confusing. Even if no snitch-level supervision is used to guide the localisation across video, the per-frame supervision is used to learn the detector. I agree that other datasets could be used in real-world scenarios, but in this case, the per-frame annotation is used instead.\n\nI really think that the idea of multi-hop and “selective” pointers in transformers is valuable and useful for video reasoning. Moreover, I appreciate the empirical study presented in this work. However, I have major concerns about the way the algorithm is explained in the paper. Several crucial parts are not clearly explained and the usage of the two transformers inside the multi-hop method are not motivated at all. Taking these into consideration, I don’t think that this work is in a proper form to be published since several sections should be rewritten entirely. In conclusion, I recommend the rejection.\n\n########### UPDATE #########\n\nI thank the authors for clarifying the method description, providing additional experiments and actively engaging in discussion. Since my main concern regarding the writing part was addressed I change my rating and I agree with the acceptance.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommend accept",
            "review": "This paper presents Hopper, a method that performs multi-hop reasoning to address the problem of object permanence in videos.\n\n### Strengths:\n- Very good results in the Snitch Localization task. Good and sensible baselines.\n- Good ablations, experiments and visualizations. Also good justification of results both for Hopper and for baselines.\n- They introduce a method that is modular, and clearly separates several stages of processing (while still making everything end-to-end trainable). This provides good understanding of the system, and makes ablations and comparisons with baselines easier (for example, baselines can work a the per-module level).\n- The multi-hop transformer reasoning module they introduce is intuitive, and it is very well executed.\n- Overall well written and good positioning among state of the art literature.\n- Contribution of a new CATER-h dataset that corrects some biases of CATER for the specific task the paper is solving. Good explanation of the need for this dataset, both by analyzing the dataset statistics, and by looking at the results of the baselines on the two datasets.\n\n### Weaknesses:\n- A lot of supervision and limited dataset.\n\t- Object annotation class and bounding boxes are available.\n\t- Fixed set of objects and attributes, with no variation across samples (synthetic data).\n- The tracker learns based on the class. If there are two objects belonging to the same class, it will probably not distinguish them (see Figure 15a). Probably humans could watch a video and follow the snitch even if all objects looked exactly the same, just by following their movements, and this method is, _by construction_, not doing this. Therefore, Figure 15a is not just some very hard case, but a case that the system as it stands cannot solve. This implies that the tracking done by the system is a very weak form of tracking, and it is more a relabeling of objects. Actually, to solve the task the system only needs to know 1) where the snitch was before and 2) where did the container move (just reidentify by classification). While this can be improved, the main contribution of the paper is in the step after this, so it is OK to leave it for future work.\n- In some cases the method feels very specific for the presented task and dataset. This is acceptable to some extent because it is a hard problem and it doesn't have to scale right away, but I am concerned it is too specific about this formulation and cannot be used. There are a lot of specific intuitions for this specific problem and setting (not interesting for any other case). See for example page 6. More specifically, an example is the heuristic for computing the occluder, which would fail if the snitch has moved between frames, and it is visible but somewhere else (and this case is not even generalizing to a slightly different task, but a potential situation in the current dataset). While these heuristics are optional and the authors present ablations, the feeling is general for the whole method. \n- Some parts of the method could use more clarity:\n\t- Figure 3 is confusing, as it does not exactly follow Algorithm 1. It would be convenient to use the same names as in the rest of the paper for the layers and the inputs/outputs.\n\t- Why the attentional feature-based gating (line 11 algorithm 1) is necessary? Why doesn't Transformer_f directly output the final U_update?\n\t- Why two transformers are necessary, actually? The masking could be applied at the beginning of the first one and just use one. Note this is not the same as the \"hopper-transformer\" in the baselines.\n\t- t is computed with a softargmax. However, it is used in places that require it being a hard number, for example in line 2 in the algorithm. This implies these places have to detach the gradient from t. Apart from line 2, is there any other place that requires this? What about lines 12 (Masking) and 5 (Extract). My understanding of the paper tells me that at least the Masking() module requires a hard t. In what case is the gradient actually propagated through t?\n\t- Teacher forcing: the paper argues that hop 2 should focus on the first occluder. What about hop 3, what is it supposed to predict?\n\t- For line 14 in the algorithm, are the attentions of the heads averaged? \n- About the 5 steps: Looking at the examples (eg Figure 15b) it looks like there are much more than just 5 key steps to follow. However, Hopper never uses H > 5 in the shown examples (while H=5 is actually the minimum number of hops that are allowed to the system). Why is that? It looks like as soon as it has a chance, the system tries to predict the last frame available, even if it is not the most convenient (it can always get later, to the last frame). This could make sense for CATER, but the shown examples are for CATERh . This is important because the main point of Hopper is that it can select where to attend. I would appreciate some intuitive explanation and statistics of the number of steps Hopper takes.\n- Unclear what the model is _really_ learning. While it is not learning temporal biases, there are potentially a lot of other biases it can be exploiting. It would be very interesting to have analyses to answer this question. Otherwise it is hard to believe the models are really \"solving\" this task.\n\n### Additional comments and questions:\n- When an object is occluded or contained by another one, the goal is to predict the position of the first object, or the position of the occluder?\n- Is the 1 fps chosen for any specific reason? The solution to the problem is actually different depending on the sampling rate. For example, it would be possible that in between frames there is a key move that we do not see.\n- \"Humans realize object permanence by identifying key or critical frames where objects become hidden\", and similar sentences throughout the paper. Citation?\n- I think the third sentence in the introduction refers to the first one. It is confusing because it reads as it is referring to the second one instead. I would move the second one to later when the authors talk about object permanence (line 20).\n- If the key point of the paper is the multi-hop transformer, an interesting comparison would be the same system without DETR, and instead using only the true class label and bounding box (remove steps A and B). At least to see how much these are a bottleneck, or how important they really are.\n- Analyses on learned attributes. The classes are combinations of 4 attributes. Why not learning them separately? Is the model creating different represenations for each specific combination? Would it generalize to a new object or to a new combination of attributes not seen during training?\n- Will the code for the paper and the cater-h dataset be released?\n\n### Final recommendation\nOverall, I believe this paper should be clearly accepted to ICLR, as the strengths outweigh the weaknesses. This paper is not the final solution to the object permanence task, and it has a lot of possible improvements, but it is a good step.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}