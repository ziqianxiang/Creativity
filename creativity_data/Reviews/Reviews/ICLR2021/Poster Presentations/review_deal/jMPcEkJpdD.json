{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviewers liked the self-supervised learning of compressed videos, noting that it is an \"exciting topic\" and an \"important problem\", although they found the proposed methods (PMSP andCTP) less exciting. Reviewers were satisfied with the execution and the extensive experimental studies. AC felt the community may benefit from the paper's intuitive integration of self-supervised learning and the compressed video's signals (I and P frames, residuals, motion vectors, etc). "
    },
    "Reviews": [
        {
            "title": "Good topic but the approach lacks novelty",
            "review": "Summary:\nThe authors study a new problem â€“ self-supervised learning for compressed video understanding. It can dramatically save the compute and storage requirements for action recognition. The experiment results on several datasets demonstrate that the proposed approach can achieve the best results over the baselines.\n\nStrength:\n1. Self-supervised compressed video understanding is an exciting topic. It can save computation and storage for online video understanding.\n2. The two pretext tasks are efficient and straightforward for compressed video understanding.\n \nQuestions:\n\n1. Figure 2 is misleading, T_I is k times smaller than T_M,3D Deconv should be on the top and 3D Conv should be on the bottom; the image cubes are also incorrect -- they are supposed to be placed in H-W-C plane. Please re-organize the text and the figures to make the main text clear.\n2. The authors design two pretext tasks according to the heuristic: pyramidal motion statistics prediction and correspondence type prediction. I want to say the authors have done a fair evaluation, but the approach's novelty is limited. I would like to see more discussion about the intuition of the proposed method.\n3. Why not compare with the contrastive learning-based approaches for self-supervised learning. We can still get the positive pairs from the same video, though it is difficult to augment the  videos give the compressed videos.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a fair paper yet not surprisedly interesting",
            "review": "This paper is not bad in the sense that:\n1. important problem -- how to leverage compressed video such a natural and efficient format is very important while under-explored by the community\n2. extensive experiments -- clear gains on standard benchmarks demonstrate the proposed method's gains. good ablation studies in like table 3 demonstrate the effectiveness of each proposed pretext task. \n3. the paper explored the how to better fuse multiple streams and two pretext ssl tasks i.e. PMSP+CTP, which all turn out to be successful\n\nYet, the way fusing multiple streams using techniques like multi-modal attention and the pretext tasks i.e PMSP and CTP -- they look fair while do not surprise me. It is quite natural to think of these techniques to improve the current models. For example, correspondence has been used by UCB researchers to do SSL in video yet for other task. \n\nBut I do acknowledge it takes significant work to validate these findings and conduct such comprehensive explorations and comparisons. I do believe these findings are valuable to the broad video community and thus will have good impact. Thus, I lean to accept this paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper proposes an approach to self-supervised learning from videos. The approach takes advantage of compressed videos, using the encoded residuals and motion vectors within the video codec. Using encoded videos has been shown to reduce computation time required by decoding videos. Previous works have explored compressed videos for supervised recognition, showing the potential, while this paper introduces a way to leverage compressed videos for self-supervised learning.\n\nThe paper is well written and easy to follow. The proposed IMRNet shows benefit over previous supervised compressed video approaches. The paper proposes two self-supervised tasks related to compressed videos: one to predict the spatial locations with movement and one to predict the temporal matching of the inputs (Correspondence Type Prediction).\n\nThe Correspondence Type Prediction tasks are fairly standard, and have been explored by many previous works (which are cited). The main difference being the use of the compressed video features instead of rgb frame or audio.\n\nAn interesting observation is that the two tasks when combined don't seem to be any better, roughly 0.1%-0.3%, which is likely within noise on smaller datasets like HMDB and UCF101.\n\nThe comparisons in Table 3 are missing comparisons to state-of-the-art approaches. For example,\n- \"Cooperative learning of audio and video models from self-supervised synchronization\", NeurIPS'18 \n- \"Audio-visual scene analysis with self-supervised multisensory features\", ECCV'18\n- \"Evolving Losses for Unsupervised Video Representation Learning\", CVPR'20\n\nall outperform this approach. While those approaches use audio features and are not specifically focused on compressed videos, there's nothing about them making them incompatible with compressed videos using say only I-frames. I think Table 3 could be improved by comparing to existing approaches for non-compressed videos as well as using those approaches on compressed videos. This would help show the benefit of the proposed tasks.\n\nThe paper has some missing details. One of the claims is that compressed videos is faster, which is shown in Table 2. But there are some missing details, for example, the number of training epochs is not reported (only that 30 are used for warmup). How many does this use? How much faster is this method than non-compressed method?\n\nThere are also some other related works, e.g.\n\"AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures\", ICLR'20\nproposes a similar idea to the bidirectional connections and\n\"AssembleNet++: Assembling Modality Representations via Attention Connections\", ECCV'20\nproposes ideas similar to the multimodal gated attention.\n\n\nOverall, the paper is interesting. I think some of the experiments could be strengthen to better compare to existing self-supervised approaches.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}