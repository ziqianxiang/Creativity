{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Well-written paper that proposes a flow-based model for categorical data, and applies it to graph generation with good results. Extending flow-models to handle types of data that are not continuous is a useful contribution, and graph generation is an important application. Overall, the reviewers were positive about the paper, and only few negative points were raised, so I'm happy to recommend acceptance."
    },
    "Reviews": [
        {
            "title": "Proposes normalizing flow algorithms for modeling discrete distribution",
            "review": "Summary:\nThe paper considers the problem of modeling discrete distributions with normalizing flows. Authors propose a novel framework “Categorical Normalizing Flows”, i.e CNF. By jointly modeling a mapping to continuous latent space, and the likelihood of flows CNF solves some of the bottlenecks in current algorithms. With experiments on some synthetic domains, and benchmarking tasks like Zinc250, the authors empirically demonstrate that CNF-based algorithms perform competitively and often improve significantly on related approaches like Latent NF, discrete flows.\n\nReasons for the score:\nI vote for accepting the paper, with minor improvements. The problem is well motivated, and the proposed algorithm improves significantly on previous approaches. I would encourage the authors to include generated samples/visualizations which may reflect some pathological failure modes of the algorithm. This would help with the readability of the paper, and improve understanding.\n\nStrengths:\n+ The CNF framework helps scaling normalizing-flow based approaches, and improves the stability and performance on standard benchmarks.\n+ GraphCNF outperforms baselines like GraphAF/GraphNVP on Zinc250 molecule generation. \n+ On language modelling tasks, CNF improves on Latent NFs and works competitively as an LSTM baseline.\n\nWeaknesses:\n- Plots such as comparing the training loss of different approaches would help understanding the stability of the learning algorithm. \n- Adding some visualizations/samples from the learned models would help with clarity of section5.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "categorical flows",
            "review": "The paper investigates normalizing flows for the general case of categorical data. The authors propose continuous encodings in which different categories correspond to unique volumes in a continuous latent space. \n\nUsing the proposed Categorical Normalizing Flows (CNF) the authors present GraphCNF, a permutation-invariant normalizing flow on graph generation.\n\nIn particular the paper presents a new approach for normalizing flow on categorical data. Then a three step generation approach is presented for graph generation with CNF.\n\nThe paper is well written and the proposed solution is clearly presented in sections 2 and 3.\n\nThe first results on molecule generation are very good when compared to those obtained with other approaches. The second experiment on graph coloring shows the validity of the proposed CNF even on an NP-hard optimization problem. Even the results on language modeling are encouraging. A final experiment shows how CNF can model discrete distributions precisely.\n\nAn experiment could be done on real-world datasets usually used for density estimation in order to assess the validity of CNF in therms of likelihood. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ICLR 2021 Conference Paper1592 AnonReviewer1",
            "review": "# Summary #\n\nThis paper proposes a new normalizing flow model for categorial data, where the typical dequantization is not applicable. \n\nWe assume a categorical sample x has S variables, and each attribute x_i is a categorical variable. \nWe want to model the probability mass of the S variable categorical data, and devise an invertible map that can convert x into the continuous latent variable z. \nFor simplicity, we assume that each attribute x_i has its own latent continuous probability distribution p(z_i). \nWe expect an encoder, q(z_i|x_i), map categorical x into a continuous space where all categories are well partitioned. For that purpose, the paper proposes to formulate q(z_i|x_i) by a mixture of logistic distributions. \n\nA graph generative flow model is proposed as an application. \nExisting flow models for graphs do not handle the categorical data in proper manners and are permutation-dependent. \nThe proposed categorical flow can develop a permutaion-invariant graph generative flow model. \n\nThe proposed model performs better than the existing graph flow models in the molecular graph generation experiments. Typical invalid generation examples include isolated nodes. If we only focus on the largest sub-graphs, the proposed model can almost perfect graph generations. \nThe permutation-invariant nature of the proposed model results in a stable performance on the graph coloring problem, while the baseline RNN models are deeply affected by the order choices. \n\n# Comment\n\nI found the mixture of logistic regression is a good idea. Figure 2 and 3 in appendix indicate that this formulation can pratitioin the latent space into categories. \n\nI have a few questions to confirm my understanding of the paper. \n\nQ1. The proposed categorical normalizing flow with K-logistic mixture provides an approximated invertible map for the true distribution of the categorical samples x. Is this correct? Namely, there is a non-zero KL divergence between the evidence P(X) and the marginalized ``likelihood'' q(X)?? \n\nQ2. The paper says \"we do not have a unknown KL divergence between approximate and true posterior constituting an ELBO\". Does this mean we can compute the KL(q||p) analytically for the categorical normalizing flow? \n\nConcerning the Q2. it is better if the final objective function to maximize/minimize, and the actual procedure for model training is clearly written in the main manuscript or the appendix. \n\nConcerning the molecular graph generation experiments, I'm interested in how the latent representations of the graphs are distributed in the space of Z. It is preferable if the paper can provide a visualization of the latent space for the actual molecular graph generation experiments, not the simulated ones of Figure 2 and 3. \n\nPresentaions of the experimental results in the main manuscript totally rely on the tables. However, current tables are not much effective to tell the significance of the proposed method. \nPlease consider visual presentations: we may use plots or bar graphs to compare several methods for example if the detailed numbers are not important. \nThe actual numbers can be moved to the appendix. \n\n# Evaluation points\n\n(+) A new approach to apply the normalizing flow. \n\n(+) Truly permutation-invariant NF for graph generation is great.\n\n(-) insufficient explanations for the optimization procedure\n\n(-) more visual results may improve impressions of the manuscript (especially for non-expert readers)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Inspiring formulation to work with categorical data using flows. ",
            "review": "### Summary:\n\nThis work uses the idea of variational inference to map categorical data to continuous space affording the use of normalizing flows. Authors use several ideas to increase their framework's applicability--factorized distribution assumption, use of multi-scale architecture for step-generation, and permutation invariant components—achieving favorable results on several problems. The approach seems to be especially useful when data is non-sequential.\n\n### Strength:\n\nI like the idea; it is simple and seems to be very useful in specific problems, especially those without a natural order on categorical variables. The authors also make an excellent effort to gather empirical evidence in favor of their method. The literature review is comprehensive, and the method is well-placed among the cited papers. Several sections are well written and have a nice natural flow. I find the application to graphs an excellent use of the multi-scale architectures in coupling-based flows and compliment the authors for the superb visualization in Figure 1. \n\n### Concerns:\n\nI found the most critical section in the paper least clearly explained--section 2.2. I appreciate the overall idea of this work and believe I understand the work well, yet the use of ill-defined terms and lack of clarity hamper my confidence. \n\nIn my current understanding of the paper, $p(z)$ is modeled using a large flow. Authors use the motivation that $KL[q||p]$ is negligible and approximate $p(x_i|z_i)$ using $q(z_i|x_i)$. With this, eq. 2 can be used to train; however, I am unsure if that is indeed the case; I will appreciate it if the rebuttal absolutely clarifies this.  To make it easier for the authors, I effort to pinpoint the specific instances where they lose me in section 2.2. \n\nIn the first paragraph, I find the last line: \"Specifically, as normalizing ..\" a bit hard to parse. In particular, the term \"one distribution\" is unclear. The next paragraph tries to explain why the decoder becomes almost deterministic; despite using words like \"therefore\" and \"as a consequence,\" I found the explanations insufficient. \n\nThe term \"overall encoding distribution\" is unspecified in the paper,  and I am skeptical of the use of \"true decoder likelihood.\" Maybe I am missing something obvious, but why does the expression for \"true decoder likelihood\" involve the variational approximation? Is it because we work under the assumption that the KL divergence is negligible? \n\nAlso, I am unsure what the objective is when you have \"removed\" the ELBO (see the first line, last paragraph, section 2.2.). \n\n \n\n#### Minor bits:\n\"... we do not have *an unknown ...\" --using an in place of a.\n\n### Updates after the rebuttal\n\nI find the revised manuscript to be clear and more transparent. After reading the reviews, the response, and the extended appendix, I am increasing my score and vote to accept this paper. \n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}