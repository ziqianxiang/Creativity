{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes multiplicative filter networks (GaborNet and FourierNet) as functional approximations of deepnets. The proposed networks are a sequence of multiplications linear functions of sinusoidal or Gabor filters. The authors show that in some cases the performance of proposed networks outperforms the existing deepnets using ReLu activations. This representation is notably simpler as well. Moreover, compared to classical Fourier approach, the proposed method scales to higher dimensions in practice as well.\nThe downside of the paper is that it is not clear how to empirically use exponentially many Fourier functions. Moreover, proposed methods have more parameters, and the additional parameters are linear in size of the hidden layer.\n\nThe paper is clearly written and the authors improved the quality of the paper and added additional experiments to support their claim through the review process and I appreciate that."
    },
    "Reviews": [
        {
            "title": "Novel formulation using multiplicative filters on function approximation for low-dimensional-but complex functions ",
            "review": "Summary:   This paper complements a class of recent work on function approximation for image function representation that leverages random fourier functions and sinusiodal activation functions (SIREN).  The main insight in the paper an alternative scheme that just repeatedly applies nonlinear filters (sinusoids, gabor wavelet functions) to the networks input and multiplies together linear functions of these features.  The authors show that due to multiplicative properties of fourier/ gabor filters the end resulting mapping is also represented by a linear combination of fourier/gabor filters. Essentially the entire function is a linear function approximator over an exponential number of fourier/gabor basis functions with a low-rank, polynomial number of coefficients of the MFN network.  Experiments are done to compare the proposed FourierNet, GaborNets against past work (FF, FF with positional encoding, FF Gaussian SIREN).  The results are competitive with other methods and in certain cases GaborNets outperform in PSNR benchmarks for various problems.\n\nPros:  I like the paper very much, it is very well written and clear. The idea is novel.  \n\nCons: The empirical benchmarks are the same as used in the SIREN paper.  I would liked to see a discussion or analysis on tradeoffs between using SIREN or GaborNet (i.e. in what settings they are appropriate and perform perturbation analysis). \n ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, but some missing analysis",
            "review": "Summary.\nThe paper proposes a novel neural architecture for representing complex functions over on a low-dimensional domain (e.g. images on coordinates). Instead of following standard deep-learning ideas, the method stacks linear layers, but modulates each layer with a non-linear function (e.g. Fourier or Gabor functions). It is shown that this results in a linear combination of exponentially many Fourier/Gabor functions. Extensive experiments are performed, following the complete suite of experiments in SIREN [Sitzmann 2020], and it is shown that the proposed method outperforms available neural methods, except on video representation and signed-distance functions.\n\nReasons for score.\nThe paper proposes a novel method and shows superior performance on some tasks. However, generalization performance comparison for standard signal processing is missing. Hence, I suggest a weak accept.\n\nPro.\n- simple and novel idea that fits nicely into standard frameworks\n- experiments demonstrate its superiority on several tasks, even generalization compared to Fourier Features\n- well written and easy to understand\n\nCon. \n- although theoretically shown, it remains empirically unclear if the method is able to leverage the exponential number of functions. I.e. an experiment to compare with standard Fourier and Gabor transformation should be performed. As a Fourier basis can represent any finitely sampled signal perfectly, a generalization analysis should be performed akin to pg.6 bottom. Put another way: when is the neuronal representation useful as opposed to just useing a Fourier/Gabor transform?\n- it should be worked out more carfully what's novel throughout the paper. E.g. in section 4.2. \"while ReLU fails spectacularly\" - this has already been shown in the SIREN paper, and needs emphasis for readers not aware of this.\n\n\nMinor.\n- pg. 3 top: WË†{(i)} the dimensionality should be transposed, no?\n- do all the methods in Figure 1 have the same numbers of free parameters? \n- Table 3: is the relative performance stable wrt. to the number of iters? would be good to see the whole plot of performance vs. iters for all methods in the appendix.\n- Figure 4: Ground -> Ground-truth; why is there a square box for SIREN and GaborNet? (it's not in the SIREN paper)\n\n\nUdpate: with the additional experiments and corrections in the paper, I believe the paper is in good shape and contributes to the literature in the field. Hence it should be accepted.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "novel proposal to replace conventional compositional networks to learn better representations, showing some marginal quantitative improvements over state-of-the-art, generalization capability remains a question.",
            "review": "**Summary**\nThis paper proposes two schemes to learn better representation, one based on Fourier features and another on Gabor filters. The idea is to place nonlinearity into the feature encoding rather than network layers like ReLU, and the goal is to learn better representation with more compact models. Preliminary results outperform existing techniques on several image and video representation tasks. One generalization is shown for image completion.\n\n**Strength**\nThe idea is simple but seems effective. It's interesting to leverage Gabor filters' capability in spatial and frequency support to enable better feature encoding. Results show a better reconstruction of high frequency details and experiments are comprehensive covering multiple aspects of image and video tasks.\n\n**Weakness**\n\n    - I do not see how MFN 'largely outperforms' existing baseline methods. It is difficult to identify the quality difference between output from the proposed method and SIREN -- shape representation seems to even prefer SIREN's results (what is the ground truth for Figure 5a). The paper is based on the idea of replacing compositional models with recursive, multiplicative ones, though neither the theory nor the results are convincing to prove this linear approximation is better. I have a hard time getting the intuition of the advantages of the proposed method.\n    - this paper, and like other baselines (e.g. SIREN) do not comment much on the generalization power of these encoding schemes. Apart from image completion, are there other experiments showing the non-overfitting results, for example, on shape representation or 3D tasks?\n    - the proposed model has shown to be more efficient in training, and I assume it is also more compact in size, but there is no analysis or comments on that?\n\n**Suggestions**\n\n    - Result figures are hard to spot differences against baselines. It's recommended to use a zoom or plot the difference image to show the difference.\n    - typo in Corollary 2 -- do you mean linear combination of Gabor bases?\n    - It's recommended to add reference next to baseline names in tables (e.g. place citation next to 'FF Positional' if that refers a paper method)\n    - In Corollary 1, $\\Omega$ is not explicitly defined (though it's not hard to infer what it means).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Incremental work with better performance than previous work",
            "review": "This paper propose a new network architecture, multiplicative filter network (MFN),  for implicit function reconstruction. The proposed network incorporate elementwise multiplication of nonlinear filters to replace to conventional nonlinear functions, e.g. ReLU, for better signal reconstruction. Two different multiplicative filter network is studied, FourierNet and GaborNet, which used Fourier and Gabor functions as the nonlinear functions in the MFN. Experiments have shown that the proposed method can outperform SIREN under the similar level of computational complexity.\n\nPositive:\nThe paper is easy to follow and the proposed method seems to be working well especially for the GaborNet. \n\nNegative:\nOn top of SIREN, this paper looks incremental. Although it makes some theoretical study that the proposed FourierNet and GaborNet can favorably reconstruct original signals, there is no theoretical comparisons with this work and SIREN. There is only empirical study to show that the results of the proposed method is better than SIREN (GaborNet only, the FourierNet performs worse than SIREN).  \n\nOpen Question:\nI have checked the SIREN and its source codes. Although SIREN can reconstruct original image favorably, its network structure is unfriendly for image editing and/or other image processing tasks. Have the authors of this paper encounter similar problem in MFN?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}