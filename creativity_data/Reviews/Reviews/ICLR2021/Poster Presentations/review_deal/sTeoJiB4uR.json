{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper develops an approach to training generative models with binary weights. The reviewers are split. Two reviewers regard binary networks and the general theme of reducing the computational cost of training as important, and the presented work as a solid contribution. Two reviewers raise concerns about the motivation and the quality of the results. The authors' responses somewhat alleviated the quality concerns of R3, but not the concerns of R4. Overall, the reviewers lean on the positive side. There is disagreement on the importance of the problem, but there is clearly a non-trivial subset of the community that welcomes research in this direction. The AC supports acceptance."
    },
    "Reviews": [
        {
            "title": "The authors propose methods to make weights and/or activations in variational autoencoders and flow-based networks binary.  This is an important topic because of the growing cost of training computing systems.  Main contributions are the authors' method for binary weight normalization and analysis around expanding the number of parameters once a network is binarized, and assessing the performance impacts.",
            "review": "This paper describes a method to binarize weights and activations of variational autoencoders and flow-based networks.  This is an important issue as these methods are valuable to solving unsupervised problems, but are rapidly growing in size, necessitating large and expensive computing systems. And, the literature of low and binary precision hasn’t considered these use-cases to date.\nThe choice of ResNet VAE and Flow++ are good representative candidates for methods in use widely today.  However, the largest models we see in the world today are knowledge representation and language models (transformers)). Showing that binarization will work for these models would be a valuable next step.\n\nThe authors call this a “framework” to go to binary weights and/or activations, but this is really a set of methods shown to work on a particular NN. Further work is needed to describe a more generalized set of methods. The authors do a good job of motivating their method for the VAE and Flow++ model, but I don’t see an extraction of a fundamental principle that might be broadly applicable to most neural networks.  A major contribution of the paper is Binary Weight Normalization (BWN) and the binarizing method for residual layers.  There is something deeper there.\n\nI like this work as it is an increasingly important consideration to data science teams in building solutions that can work within real-world constraints.  However, binarization is still seen as a method to try and reduce the computational load of something that is already being done.  This makes it difficult to build systems and computing architectures that commit to binary representations. I see this work as a steppingstone to something more general, which can be the basis of building better and cheaper computing substrates. A stronger paper would have uncovered a universal concept.\n\nSection 5.1: It doesn’t appear there is a really principled way to decide which parameters to binarize.  There is some discussion of residual layers as being the least susceptible to performance degradation, but it would be a stronger paper if some of this was explored a bit more through experimentation. Again, extracting a principle here would be useful to the field.\n\nSection 5.2: This vector of improvement is an entire research topic on its own. I think this might be the most interesting direction highlighted by this paper; how does precision/binarization interplay with hyperparameters to reduce the absolute computing requirements, which increasing algorithmic performance?\n\nSection 5.3 Ablations, where is figure 3?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Motivation is not clear and prior work is incomplete",
            "review": "The authors propose to binarize weights and activations of generative VAE and Flow++ models. As Weight Normalization is commonly used in these models, the authors notice that Euclidean norm of binary [-1;1] vector is a square root of it’s length, such that Weight Normalization can be reduced to affine scaling. They propose to call this scaling Binary Weight Normalization, and evaluate it on CIFAR and ImageNet datasets.\n\nOverall, motivation for binarizing generative models is unclear. To the best of my knowledge, training such generative models is an active research topic, and, unlike in image classifiers, object detectors, language and translation models, are not applied in practice in real systems where execution time and memory footprint matters. Researchers who train such generative would not apply binarization as:\n- training such models is already difficult and optimization is often unstable, so adding an extra variable might make it even more difficult;\n- to keep baselines simple;\n- final execution time does not matter.\nIt is also very likely that, in case such generative models find applied use-cases where execution time and memory footprint would matter, they would have undergone several research iterations and updates such that new binarization techniques would need to be developed.\n\nAs affine scaling is no longer a weight normalization, it is probably incorrect to call it that. An experiment ablating an architecture with/without the proposed binary weight normalization is missing. Also, as generative models are often evaluated qualitatively, the paper would benefit from including several samples from binarized models, to show that quality does not degrade.\n\nLiterature review also lacks several more recent binarization techniques such as [1] and [2] (more can be found there). It is not clear if one were to pick a simple (or more recent) binarization technique they would encounter difficulties.\n\nOverall, due to unclear motivation, introduction of binary weight normalization which is not weight normalization, and incomplete literature review I propose rejection.\n\n[1] Mark D. McDonnell, Training wide residual networks for deployment using a single bit for each weight, at ICLR 2018. \n[2] Gu et al. Projection Convolutional Neural Networks for 1-bit CNNs via Discrete Back Propagation, at AAAI 2019.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting model but there are some questions to be addressed",
            "review": "The paper introduces the first way (to the best of authors knowledge) of building generative models with binary weights. Also the case with binary activations is considered. The authors consider two SOTA generative models (flow++ and RVAE) and develop technique to binarize all weights (and possibly activcations) in residual layers. They show that residual layers can be binarized with relatively small drop in performance and further binarization of remaining blocks in computational graph leads to significant degradation. Binary modification of weight normalization is suggested although no ablation is study is performed so it is unclear how crucial is BWN for robust learning. The training process itself is pretty standard way of training binary DNNs - they use STE + truncation of real-valued weights counter-parts.\n\nI have several questions on the proposed methodology:\n1. To compute density in flow++ model you need to be able to differentiate Jacobians. How would you compute them given the inputs are binary? Have you used STE for this purpose? If so then how can we be sure that what we get is correct (normalized) density function? I think this issue should be addressed in the paper since nomalizing flows by design assume continuous variables.\n2. Please define what is $n$ in you paper. Is it the total number of all weights in you network/layer or is it a number of weights in single convolution filter?\n3. You provide performance of binarized generative models and show that they require a lot less memory than their real-valued analogues. It would great to see what performance can be achieved by smaller real-valued generative model that requires approximately same amount of of memory as binarized network. Then we could understand what is better - to use large binary networks or small real-valued ones for generative modeling.\n4. My major concern is the visual quality of the images obtained by binary models. To be honest after I saw them I decided to downgrade my mark from 6 to 5. It seems that with such quality the model is quite useless. Do I understand right that there is a clear visual difference between objects generated by real-valued and binarized networks of similar size?  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of 'Reducing the computational cost of deep generative models with binary neural networks'",
            "review": "The contributions of this paper are the following:\n - it extends previous work on binary neural networks to the case of deep generative models which perform density estimation (VAEs and Flows)\n - the results show that the proposed approach works well, with the expected trade-off but closely matching the much larger real-valued networks\n - in order to do so, it introduces a novel technique for binarizing weight-normalized layers, which are used in these generative models\n - the results show the advantage of this technique, and they illustrate it is particularly important for ResNet layers.\n\nThe paper is very clear (except for one element noted below). Originality is clear but not very high, as this contribution may be seen as a low-hanging fruit, albeit a useful and well-executed one.  As these kinds of architectures are becoming more and more used in various settings, the techniques used here (e.g. using ResNet layers) may be applicable more widely, increasing the significance of this work.\n\nMinor fixes suggested:\n\n* The semantics of equations 3 and 6 is not clear. I imagine the objective is to explain how the gradient on the binary weights and activations is converted into a gradient on the real-valued ones, but this should be explicited.\n\n* In page 1 (bottom line) and 4 (1st line after eqn 9), the authors cite Rezende & Mohamed 2015, Dinh et al 2017 , but they should include first in this list the Dinh et al 2014 paper which actually introduced the notion found in Flow models, albeit under a different name (NICE).\n\n* Appendix A: the authors should also show the images generated with full precision, so we can compare visually.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}