{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Clarity: The paper is well-written with illustrative figures.\n\nOriginality: The originality of the paper is relatively restricted, mainly due to the resemblance with the work [1]. However, there are important differences, that the authors nicely pointed out, and we encourage them to include these in the final version of the paper.\n\nSignificance: The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum-based optimization algorithms in training deep neural networks. While the paper could be considered \"another algorithms for training NNs\", the papers illustrates nicely the main arguments, and is backed up with more than sufficient experimental results.\n\nMain pros:\n- In the main pros, AC and reviewers admit the phenomenal job in responding to reviewers' questions and requests\n- The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.\n- After the reviews, The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e-4 was the best value.\n\nMain cons:\n- One reviewer requires more explanation why the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8)."
    },
    "Reviews": [
        {
            "title": "Rewiew",
            "review": "Summary: Based on the assumption that the rapidly decrease step size \\delta \\omega leads to a solve effective convergence of \\omega. This paper proposes to use  a projection step to remove the radial component, and thus reduce the norm of training parameters, and faster the training procedure. The experiments look good to me while the derivation of this paper based on many assumptions and conjectures.   I tend to accept this paper at this time. However, I am not an expert at this area I will not be sad if this paper is reject by other reviews.\n\nminor problems:\n1) Can the author explain more on how to derive eq.(4) ?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good insights, promising results, but missing important related work",
            "review": "This paper points out that momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights.  To solve the problem, two algorithms called SGDP and AdamP are proposed, which project the updates to tangent space of the parameter. Experiments on several tasks including image classification, language modeling, etc show the effectiveness of the proposed algorithms. The idea to study the integration of BN and momentum is interesting. The analyses and proposed algorithms provide guidance to practitioners. \n\nQuestions:\n\n(1) What is the difference between the proposed algorithm and the algorithm in paper \"Cho & Lee, Riemannian approach to batch normalization\"? Eq.(10) in your paper is exactly the same with Eq.(6) in [Cho & Lee]. It is an important related work which should be cited and compared with.\n\n(2) What does \"lies on the geodesic\" mean in Proposition 3.1? What is the relation between Proposition 3.1 and the convergence of the algorithm?\n\n(3) It is not clear which results are proposed in related work and which results are proposed in this paper. Is Lemma 2.1 a new result? If not, it needs a citation.\n  \n(4) The theory part analyzes the effect of momentum, while the experiments shows the effect of weight decay. How does weight decay influence the norm growth theoretically? Why not conduct experiments with varying momentum coefficient? \n\n(5) The theory part shows the negative effect of momentum. Does it mean that \"SGD is a better choice than momentum SGD\"? Momentum SGD is a standard algorithm to train deep neural networks with BN in practice but not vanilla SGD. How to explain it?\n\n(6) What is the additional computational cost of the proposed algorithms compared with the baselines?  \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
            "review": "###################################################################\n\nSummary:\n\nThis paper shows that momentum-based gradient descent optimizers reduce the effective step size in training scale-invariant models including deep neural networks normalized by batch normalization, layer normaliztion, instance normalization and group normalization. The authors then propose a solution that projects the update at each step in gradient descent onto the tangent space of the model parameters. Theoretical results are provided to show that this projection operator only adjusts the effective learning rate but does not change the effective update directions. Empirical results on various tasks are provided to justify the advantage of the proposed method over the baseline momentum-based (stochastic) gradient descent and Adam.\n\n###################################################################\n\nReason for the Score:\n\nOverall, this paper could be an interesting algorithmic contribution. However, there are relevant points needed to be clarified on the theory and experiments. My first main concern is that theoretically it is hard to justify that the proposed projection-based update yields smaller parameter norms than the baseline momentum-based update. My second main concern is some baseline results in the experiments do not match those in existing literature, and no error bars are provided in the empirical results even though the improvements of the proposed methods over the baseline methods are small. \n\nCurrently, I am leaning toward rejecting the paper. However, given additional clarifications on the two main concerns above in an author response, I would be willing to increase the score.\n\n###################################################################\n\nStrong points:\n\n1. The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum-based optimization algorithms in training deep neural networks. \n\n2. The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.\n\n3. The paper is well-written with illustrative figures.\n\n###################################################################\n\nWeak points:\n\n1. It is not clear to me that the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8).  The parameters of the model evolve differently under these two update rules. Throughout the training, the update p_t in equation (11) is different from the update p_t in equation (8). As a result, it is hard to compare ||q_t|| in equation (12) and ||p_t|| plus all the terms ||p_k|| in equation (8). \n\n2. The improvements of the proposed SGDP and AdamP over the baseline SGD and Adam are small across experiments, and thus error bars are needed to validate that these improvements are not due to randomness. However, no error bars are provided for the empirical results in the paper.\n\n3. The reported baseline results for audio classification are worse than those reported in (Won et al., 2019).\n\n4. The baseline results for adversarial robustness seems to be much higher than reported results in (Madry et al., 2018). Also why are the values of epsilon used in the paper quite small (80/255 and 4/255 vs. 8 in (Madry et al., 2018)).\n\n###################################################################\n\nAdditional Concerns and Questions for the Authors:\n\n1. Adam normalizes the gradient by its cumulative norm. This can help eliminate the small step size issue since the norms of the gradients become smaller during training. Can you provide a similar simulation as in Figure 3 but using Adam, AdamW, and AdamP?\n\n2. What are the baseline results, reported in existing literature, on ImageNet for ResNet18 and ResNet50 trained with the cosine learning rate schedule in 100 epochs? Can you please link me to the previous papers that report those results? The paper you cite, (Loshchilov & Hutter, 2016), does not report those results.\n\n3. In section 4.1, the authors say “For ResNet, we employ the training hyperparameters in (He, 2016)”. However, the training hyperparameters for ResNet used in the paper are not from (He, 2016). In (He, 2016), the models are trained for only 90 epochs without using cosine learning rate.\n\n4. The proposed update is more expensive than the baseline momentum-update. The paper also reports that the proposed update incurs 8% extra training time on top of the baselines for ResNet18 on ImageNet classification while resulting in only small improvements over the baselines. It is needed to compare the proposed optimizer with the baseline momentum-based optimizer trained with more epochs and potentially with an additional learning rate decay.\n\n###################################################################\n\nMinor Comments that did not Impact the Score:\n\n1. The paper proposes not only AdamP, but also SGDP. It is better if the authors remove AdamP in the title.\n\n2. In figure 4, is the y-axis the test or train accuracy?\n\n###################################################################\n\nReferences:\n\nMinz Won, Sanghyuk Chun, and Xavier Serra. Toward interpretable music tagging with self- attention. arXiv preprint arXiv:1906.04972, 2019. \n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. URL https://openreview.net/forum?id= rJzIBfZAb. \n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. \n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\n\n###################################################################\n\nPost Discussion Score:\n\nAfter reading the rebuttal from the author and the comments from other reviewers, I am still not clear if the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8). However, the authors have addressed all of my other concerns. I decided to increase my score for this paper from 4 to 5. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper that adresses the negative impact of momentum on scale-invariant parameters, but misses comparison with [1]",
            "review": "### Summary:\nThis paper studies the hurtful effect of momentum on scale-invariant parameters (such as weight matrices of linear layers followed by Batch Normalization (BN)): Indeed, momentum tend to increase the norm of the parameters, but the effective update size of scale-invariant parameters is inversely proportional to their norm, which leads to \"_premature decay of effective step size_\". The authors propose the improved SGDP and AdamP to reduce this issue, and show how it improves the performances of a wide variety of models (ResNets, MobileNets, Transformers, etc...) on many different tasks (ImageNet classification, adversarial training, audio classification, etc...)\n\n### Strengths:\n+ This paper addresses the optimization of scale-invariant parameters, which can be found in pretty much all the state-of-the-art models.\n+ The paper is well written and contains a good balance of illustrative examples, theoretical analysis and experiments. Good job!   \n+ The empirical evaluation of the proposed algorithms is quite large, and many tasks and architectures are considered.\n+ I also really like the \"cos(w, Grad_w)\" hack to automatically detect scale invariant parameters in the model. This is a very neat trick!\n\n### Concern:\nMy main concern is with the statement that \"_our paper is first to delve into the issue in the widely-used combination of scale-invariant parameters and momentum-based optimizers\"_ . Indeed the authors have missed [1], which also adapts SGD with momentum and Adam to work on scale-invariant parameters, and showed performance improvement over vanilla SGD and Adam. Now, the approach of [1] is slightly different, in the sense that they propose to keep the scale-invariant parameters on the unit sphere S^1. Also, [1] performs the projection onto the tangent space before computing the momentum term (and second order moment in Adam), while this projection is done after in SGDP and AdamP.\n\nIn any case, I do believe proper comparison with [1] throughout the paper is required, due to the conceptual similarity of both methods. It would be particularly relevant  to show how the angles between SGDP and the SGD of [1] compare in Figure 1 (I think they would be identical, although not 100% certain). Also, I'm curious to see if the proposed SGDP and AdamP work better than the algorithms proposed in [1], as the nice automatic learning rate decay of BN disappears when keeping the scale-invariant parameters on the unit sphere (which would be an advantage of the proposed SGDP and AdamP).\n\n### Reasons for score:\nOverall I really liked that paper, but I don't think it would be fair to accept it without an in-depth comparison with [1].\n\n### Questions / Comments:\n- I like the toy example in Figure 1, although I'm not sure I understand what you mean by: \"_Compared to GD, GD+Momentum speeds up the norm growth, resulting in a slower effective convergence in S^1, though being faster in the nominal space R^2_\". It seems to me that the angle between the optimal solution and GD+momentum is smaller than the angle between the optimal solution and GD. I'm not sure to understand how do you observe a slower effective convergence in  S^1?\n- (3.1, real-world experiments and F) What happens if you increase WD when using SGD? It seems that the best values reported for SGD in Table 1 and Figures F.* are with WD=1e-4, which is also the highest value reported. One can imagine even better results could be obtained using WD=3e-4 or 1e-3. It would be nice to have this result for completeness.\n- Reporting mean and standard deviation across several seeds would have been nice, but I'm going to let this one slide, since results are reported on a lot of different tasks and architectures.\n\n[1] Cho, Minhyung and Lee, Jaehyung, _Riemannian approach to batch normalization_, NIPS 2017\n\n___\n\n### After author response and paper revision\n\nFirst, I would like to congratulate the authors for their amazing work during the rebuttal period.\n\nMy main concern, the comparison against [1], has been perfectly addressed in appendix G (both \"conceptually\", by highlighting the differences between both methods, and showing the advantages of the proposed SGDP and AdamP, but also empirically on ImageNet, where a fair comparison with proper hyper-parameter tuning has been performed).\n\nThe authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e-4 was the best value.\n\nWith all these changes, I think the paper is good and I recommend its acceptance.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}