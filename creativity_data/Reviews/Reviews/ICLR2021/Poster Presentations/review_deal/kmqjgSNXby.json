{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is about the use of autoregressive dynamics models in the context of offline model-based reinforcement learning.\nAfter reading the authors' responses and the other reviews, the reviewers agree that this paper has several strengths (well written, easy to follow, the approach is novel and simple to implement, the empirical evaluation is well executed and the results are reproducible) and it deserves acceptance.\nThe authors need to update their manuscript by keeping into considerations all the suggestions provided by the reviewers (clarifications and additional empirical comparisons)."
    },
    "Reviews": [
        {
            "title": "Simple extension, convincing empirical results",
            "review": "Summary\n\nThe paper studies offline policy evaluation (OPE) and optimization in the model-based setting. The main methodological contribution of the paper is using autoregressive models for the next state and reward prediction. The authors demonstrate that autoregressive models achieve higher likelihood compared to feedforward models on 9 environments from RL Unplugged [1] offline dataset. Given that model likelihood is only a proxy quality metric in OPE and control, they further demonstrate a positive correlation between likelihood and OPE estimates. The paper shows quantitatively that using autoregressive models results in more accurate OPE estimates than for feedforward models and model-free benchmarks. Finally, the authors apply autoregressive models for offline control and achieve higher returns than for feedforward models.\n\n\nStrengths\n- The paper is written clearly and generally easy to follow.\n- The proposed modification is simple, straightforward to implement, and demonstrates convincing results consistently on different environments. For example, the median rank correlation between OPE and ground truth is the best against 7 OPE baselines on 9 environments from RL Unplugged.\n- The experimental setup follows the standard practices (e.g. using a validation set for hyperparameter selection) and the details necessary for the reproduction of the results are provided (e.g. optimizers, learning rate schedules, number of epochs, architectures).\n\nWeaknesses\n- The authors claim that “standard feedforward dynamics models assume that different dimensions of the next state and reward are conditionally independent given the current state and action”. In other words, p(s’,r|s,a) is claimed to be equal to p(s’_1|s,a) … p(s’_n|s,a) p(r|s,a) when using a feedforward model. The statement is incorrect unless we use a linear function approximator as a model. However, this mistake does not affect much the quality of the paper.\n- Using autoregressive models does not address aspects that are specific to the offline setting. Providing results for the online setting will be helpful for understanding whether autoregressive models should be favored in general for model-based reinforcement learning.\n\n\nRecommendation\n\nThe reviewer votes for accepting the paper. The paper is well-written, the proposed extension is simple to implement and convincingly outperforms baselines on a variety of environments.\n\n\nNotes\n1. Appendix A.2 is identical to Section 5.1 of another submission [2].\n2. The abbreviation FQE is used throughout the paper but expanded only in the appendix.\n\n\nReferences\n\n[1] Gulcehre, Caglar, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, Rishabh Agarwal et al. \"Rl unplugged: Benchmarks for offline reinforcement learning.\" arXiv preprint arXiv:2006.13888 (2020).\n\n[2] Anonymous. Benchmarks for deep off-policy evaluation. In Submission to International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=kWSeGEeHvF8. Under review.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Autoregressive structure that predicts each coordinate of next state sequentially instead of all coordinates simultaneously helps for better offline RL tasks such as OPE",
            "review": "The paper proposes extra conditioning in a dynamics model, wherein each dimension of the next state is generated on previous dimensions as well as previous state and action. This allows for a richer model (as a model without conditioning on previous dimensions is a special case).The paper claims that this additional conditioning adds a better inductive bias in certain tasks.\n\n\nThe new models fit data better (Deepmind suite, RL Unplugged data) in terms of log-likelihood. The authors study the impact of better models on OPE and on policy optimization. The paper is generally well written. The contribution seems straightforward.\n\n\nWhy is this relevant only in continuous control tasks? Is this inductive bias really a general pattern observed in multiple tasks? What happens when there is no structure a priori (e.g., independence holds): do you lose in terms of sample efficiency?\n\nShouldn't any dynamics model be able to enrich replay buffer? \n\nThe default ordering may be completely arbitrary, how is the new dynamics model able to cope with this in the experiments? In other words, it is unclear if P(s_i|s_{j<i},...) is appropriate at all without knowing the ordering.\n\nAlso since p(s|s_prev,a) = \\Prod_{i}p(s_i|s_prev,a,s_{j<i}) by definition, how is the feedforward model restrictive and making explicit conditional independence assumption? It seems that the feedforward model is not restrictive but too general and explicitly capturing this sequential dependency across states is useful.\n\n>> We ultimately care not about the log-likelihood numbers, but whether or not the dynamics models are faithful in policy evaluation and optimization.\n\nThe above needs clarification. What is the faithfulness property? Also, why would we not care about log-likelihood?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results by applying autoregressive dynamics models to batch policy evaluation/optimization",
            "review": "#### Summary\n\nThe authors consider the usage of autoregressive dynamics models for batch model-based RL, where state-variable/reward predictions are performed sequentially conditioned on previously-predicted variables. Extensive numerical results are provided in several continuous domains for both policy evaluation and optimization problems. The results showcase the effectiveness of autoregressive models and, in particular, their superiority over standard feed-forward models.\n\n#### Pros\n\n- The paper is very well-written and easy to follow. The experiments are described with sufficient details to understand the results\n- The usage of these autoregressive models for model-based RL is, to my knowledge, novel\n- The paper presents extensive experiments on several challenging domains. The results are convincing and significant. In particular, they show that autoregressive models are superior to feedforward ones\n\n#### Cons\n\n- The paper's sole contribution seems to be empirical since autoregressive models are (as acknowledged) not novel, though their application to this setting is.\n- While the empirical results are very convincing, I did not find much intuition on where this big improvement over feedforward models comes from (see detailed comments below).\n- The ordering of the state variables might be a limitation (again, see below).\n\n#### Detailed comments\n\n1. As mentioned above, I did not find much intuition on the better performances of autoregressive models vs feedforward ones. As I am not entirely familiar with the system dynamics of the considered domains, do you think that they possess any property which makes autoregressive models more suitable than feedforward ones (e.g., strong correlations between next-state variables)? Aren't the transition dynamics deterministic in most of the considered domains?\n\n2. Since the reward in most of the considered domains is (I suppose) a function of state, action, and next-state, could it be that one of the reasons behind the worse performance of feedforward models is that they try to predict the reward as a function of state-action only? Would their performance change if they explicitly modeled the reward as a function of s,a,s'?\n\n3. Related to the previous point, the autoregressive model naturally predicts a reward as a function of s,a,s' since r is considered as the (n+1)-th state component. But what if we re-ordered the state variables with r as the first component instead of the last one? Would the performance change?\n\n4. More generally, do you think that the ordering of the state variables might be a limitation? For instance, could there be an ordering of these variables that makes the model perform well and one that makes it perform poorly? While in, e.g., image/text generation problems where autoregressive models are applied we have a natural ordering between the variables involved (e.g., by space or time), here there seems to be no particular relationship between state variables with similar index. Maybe some additional experiments could help in clarifying whether this could be a limitation or not.\n\nSome minor comments/questions:\n\n- In Eq. 2, should the product be up to H-1?\n- Before Sec. 3, a citation for \"behavioral cloning\" could be added\n- Sec. 5.3: the FQE acronym was not introduced\n- Fig. 4: what is \"r\" above each plot?\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}