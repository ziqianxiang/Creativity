{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper analyzes BatchNorm through a series of experiments and shows that BatchNorm improves training and generalization by preventing excessive growth of the activations of the previous to the last layer. The paper received mixed reviews. Two reviewers find that the paper brings clear and important new contributions to the understanding of BatchNorm, and appreciate the experimental evaluation, though some suggestions for improving the experiments were also provided. The other reviewer appreciated the contribution of regularizing against explosive growth in the previous to the last layer, but did not find the results to be very convincing as they can limit the learning rate. The authors responded that, contrary to the reviewer's claim, the learning rate is not limited. Overall, while there are some aspects that need improvement, and the authors should address those in the final version, this is a solid paper that brings an interesting contribution to ICLR."
    },
    "Reviews": [
        {
            "title": "Interesting paper with potentially significant observations",
            "review": "## Paper Summary\n\nThis paper aims to improve our understanding of BatchNorm's positive effects on training and generalization by identifying its constituent effects, and then replicating those effects with simpler techniques. Through a series of experiments using these simpler techniques, the authors show that perhaps the most important mechanism by which BatchNorm improves training, and especially generalization, is by preventing excessive growth of the activations of the layer before the output layer.\n\n## Strengths\n\nThe paper makes a particularly strong case for studying the relationship between the \"embedding norm\" -- the norm of inputs to the final layer of the network -- and successful training/generalization. It does this by\n\na) showing that use of BatchNorm can implicitly control this norm, especially if weight decay is applied to BatchNorm parameters.\n\nb) showing that in networks trained with BatchNorm, the value of this norm remains much lower than in those trained without BatchNorm.\n\nc) showing that using simple L2 regularization penalty on this norm, its value can be prevented from growing and, in combination with Fixup initialization, networks without BatchNorm can achieve similar generalization performance as those with BatchNorm.\n\nI think this constitutes a good amount of initial evidence that this technique may be useful for both researchers and practitioners.\n\n## Weaknesses\n\nI did not find major weaknesses, though it remains a little puzzling why all the clearly positive results with activation norm penalties matching or outperforming BatchNorm are only obtained on ImageNet. This indicates that perhaps more tasks/datasets should have been explored by the authors (the paper uses only two datasets).\n\n## Review Summary\n\nDespite the weakness above, I think the results are sufficiently interesting to be published and evaluated by the wider community. I recommend accepting this paper, though results on more diverse tasks and datasets will certainly improve this paper (and my rating).\n\n## Minor comments\n\n- Sec. 3.2: Authors say \"Zhang et al. show residual networks without normalization have explosive dynamics with conventional initializations. In particular, they show that the output scale of the networks grows exponentially with their depth $L$\". It is incorrect to attribute these to Zhang et al. instead of Balduzzi et al. Please correct this.\n- Sec. 4.3: I don't see \"modest improvements\" with standardizing loss on CIFAR-10 in Table 1. Given the standard errors, I would say that there is effectively no improvement.\n- Section 2.1 uses $\\rho$ in Swish instead of $\\beta$ which is mentioned in Sec. 4.2.\n- Sec. 3.1: \"Wang and Manning (2013) has\" should be \"Wang and Manning (2013) have\"\n- Table captions should be on top, not bottom of the tables, according to ICLR formatting guidelines.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Heuristic studies on the understanding the regularization of BN",
            "review": "The paper empirically studies the regularization of BN. It proposes the point that the BN's effect is connected with the regularizing against explosive growth in the final layer. To motivate this point, it takes a single-layer case and shows the BN approximately penalizes on the norm of the feature embedding thereon. Two regularizations are proposed according to the point and are used to justify it. \n\nIn the experiments, it combines the proposed regularizations with Fixup and tests on CIFAR-10 and ImageNet along with three typical architectures including, WideResNet, ResNet-50, and Efficientnet. The observed phenomena support the claims and further find that prevent the explosion of the norm can recover most of the improvements of BN. Also, the proposed regularizations achieve great performance on benchmarks via properly setting the coefficient. \n\nThe studied direction is very important and would bring some new understanding to the ICLR community. Motivated by the simple case and connections with the previous techniques, like dropout, fixup, standardizing are really good points. The reviewer can see clear differences and contributions to the literature. Experiments are conducted on large-scale benchmarks and different architecture. So the results and conclusions tend to be general. Overall the paper is easy to follow.\n\n-----------------------------------------\nSome concerns and comments are listed below.\n\nThe experiment would run a grid search to select a coefficient for the L2 regularization. Table 1,2 only list the performance of the best choice. The reviewer wonders how sensitive would the coefficient affect accuracy, especially on ImageNet. (The reviewer guesses the experiments of Figure 3 is not conducted on ImageNet.) If it is significant, the method may not suitable for common use. \n\nIn Sec. 4.4, the author claims that the generalization gap is associated with the growth in the norm. But, the gap shows in the very late phase, while the norm growth starts at the very beginning. There's no strong correlation.\n\nIn all experiments, the proposed regularizations perform based on the Fixup initialization. The reviewer wonders how the performance change if you change an initialization (e.g. Xavier Init), though the concept of your regularization is the same as the Fixup. To see if the proposed regularization can help training even without proper initialization, like the last layer BN experiments did in Sec.4.4. Also, the paper currently does not prove the importance of the initialization via any ablations but directly use the Fixup.\n\nTo sum up, the reviewer thinks the paper finds some points about the regularization effects of BatchNorm, but not much principal, and would rate 6.\n\n\n-------------------after rebuttal----------\nI thank the author for your answer. Here're the response to your latest reply.\n\nFor point 1, I am aware of your performance and encourage you to add the discussion in the main paper.\n\nFor point 2, I still doubt it since I think your claim may only hold for the ImageNet experiment on the Outputs Norm term. According to figure 2, the network does not faster converge with the help of BN. The regularization gap happened in the late stage. But, the gap of two networks on the feature embedding norm and mean output norm happens at the very beginning and keeps increasing. (except  ImageNet experiment on the Outputs Norm term)\n\nOverall, I think the paper may bring some insights to understand the BN and would like to keep my original score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper aims to disentangle and study the regularization effect of batch normalization. By directly placing penalty on either the standardization of intermediate layers or the L2 norm of the output feature, the authors strive to match the performance boost from Batchnorm. Through experiments, the author came to the conclusion that it is the prevention of the explosive norm in the output feature that is the main contribution of batchnorm in network regularization.",
            "review": "** Overview\nThis paper is generally well written and well organized. It is easy to read. It strives to disentangle the two possible effects of batchnorm in neural network in order to study its main contribution to network regularization. The proclaimed two possible effects are\n1. standardizing the intermediate activations\n2. regularizing against explosive growth in the final layer.\nBy dropping the batchnorm layer and replace it with one of the two penalty functions --  penalty on normalization or penalty on the l2 norm of the final layer -- the authors claimed that much of the performance gain from batchnorm is recovered from the norm regularization of the final layer. Although the paper is interesting, I do not find the results to be very convincing (please see \"questions\" section for clarification)\n\n** Pros\n1 Well written and well organized.\n2 Experiments are conducted to explain the disentangled effect of batchnorm in regularization.\n\n** Cons and Questions.\n1. Even though the batch norm layer has the effect of standardizing intermediate layers and limiting the size of the final layers, it is not necessarily true that their effects can be replicated by simply putting a penalty on the network loss during training. In particular, In table 1 and table 2, the authors use very small penalty coefficient lambda to replicate the \"standardizing effect\", and claimed that \"we found that higher coefficients led to divergence at high learning rate.\" This leads me to think that the results from directly penalizing the effects of batchnorm are unconvincing. \n\n2. One of the benefit of batchnorm is that it allows for a significantly larger learning rate. After dropping the batchnorm and putting the penalty on, does such benefit still exist? I would very much prefer to see that the maximal \"allowable\" learning rate does not decrease after dropping the batchnorm layer.\n\n3. The authors claimed a connection between dropout and penalization of the output feature in equation (5). However, this plausible connection is only valid when features are decorrelated -- which seems to be a very strong assumption.\n\n4. The authors also make a difference between \"feature embedding L2\" and \"Functional L2\", whose difference seems unnecessary when weight decay penalty is used because of equation (5). Is that correct?\n\n5. If I understand correctly, it is the explosion of gradient instead of the output feature that makes training method hard to converge. Can the authors elaborate on the merit of limiting the feature size? After all, even if the feature is large, a rescaled weight matrix in the final layer can easily bring everything back to normal size.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}