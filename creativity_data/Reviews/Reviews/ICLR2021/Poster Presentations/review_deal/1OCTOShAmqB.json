{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper investigates the training dynamics of simple neural attention\nmechanisms, in a controlled setting with clear (but rather strict) assumptions. Some reviewers\nexpressed caution about the applicability of the assumptions in practice,\nbut nevertheless there is agreement that the results deepen our understanding and enrich\nour toolkit for reasoning about attention.\nIn support of this, in the discussion period, it was emphasized that the work uses different techniques than\nmost current work in this direction. I am therefore confident that the paper will be useful, and recommend acceptance.\n\nI strongly encourage the authors to improve the clarity of the work and thorough\ncitation, as suggested by the reviewers."
    },
    "Reviews": [
        {
            "title": "Not a sizeable contribution",
            "review": "This paper studies the dynamics of attention in a task of simplified topic modeling, over the course of training for a specific model, where the context vector is the sum over words in a sentence of their embedding weighted by the exponential of the dot-product their key embedding with a global query vector, normalized. Due to the simplification of the topic modeling problem (two null-intersect sets of words: topic vs. non-topic), they consider the embeddings of the non-topic words to be fixed over the course of training for their theoretical analysis. The applicability of the theoretical result is close to zero, and a somewhat known property (e.g. in word2vec, Mikolov et al. 2013). The experimental results include two parts. One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration. The other is on SST2 and SST5 (movie comments and ratings, sentiment analysis), where the results are poor (obviously, as the model is simple), e.g. yielding 79.59% on SST while the SOTA is 97.4, and BERT base is at 91.2. The analysis is interesting, but does not lead to new insights.\n\nFor a simple analysis, the paper is at times hard to follow, and could benefit from more structure (presenting \"what\" before \"how\") and better notation (e.g. \\nu, v, $v$ all attached to (forms of the) the context vector).\n\nOverall, the contribution does not seem sufficient enough for inclusion at ICLR. The paper could be a good fit for a workshop on topic modeling or attention-based models.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for On The Dynamics of Training Attention Models",
            "review": "(Summary)\n\nThe paper investigates the dynamics of attention mechanism by configurating a controlled experiment on a simple topic classification task and training via gradient descent. Each random sentence in the training data is synthesized to include only one topic word among many. Then the authors try to find an intrinsic mechanism that triggers the attention model to discover the topic word and accelerates training via mutual promotion. They further experiment the evolution of models during optimization when no clear distinction between topic and non-topic words exist like in real data.\n\n\n(Originality and Contribution)\n\nThe paper proposes an artificial topic classification task and shows a positive score-and-embedding-norm relationship for the topic words to which the model must attend to. The authors also show that attention mechanism is highly helpful when the classifier has only limited capacity. They also demonstrate mutual promotion effect that leads a faster dropping of training loss than the fixed score and fixed embeddings. This discovery sounds to be original and relevant contribution to the field.\n\n\n(Strength and Weakness)\n-\tStrength: Design and run novel controlled experiment. Extensive analysis.\n-\tWeakness: Too much notational overloading. Writing quality.\n\n\n(Concerns, Questions, and Suggestions)\n\n1) It is unclear why $M >> N$ implies that a topic word appears more frequently in the sentences than a non-topic word. Section 3 describes that each sentence consists of only one topic word, then combining with $m$ non-topic words drawn uniformly at random. When the total number of topics $N$ is much smaller than the size of non-topic word dictionary $M$, what increases frequency of topic word?\n\n2) Overall simplifying some notations and avoiding notational overloading would greatly increase the readability of the paper.\n\n3) To reduce confusion, it would be great to change the iterator of summation for the partition function Z into $\\sum_{w’ \\in S_k}$ rather than using the same $w$.\n\n4) In Lemma 1, assume $q \\neq 0$ -> $q \\neq \\vec{0}$.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "This paper provides theoretical insight into the mechanisms by which a simplified attention model trained with gradient descent learns to allocate more mass to relevant words in the input.\n\nIn a limited toy setting, the authors derive a closed form relationship between word-score and word embedding norm in a simplified, one layer attention model. The theoretical findings are verified empirically both on the toy task and on a more realistic sentiment classification benchmark. Due to the extreme simplicity of the setting considered, as well as the number of assumptions made, it is unclear to me what to make of these results. In particular, it seems that the setting considered (fixed query attention over bag of word embeddings) is very different from real use cases of attention.\n\n**Pros**\n- The closed-form relationship between attention score and embedding norm during SGD training is novel as far as I know\n- The theoretical results are well justified in experiments: in particular the predicted \"SEN\" relationship seems to match the prediction.\n\n\n**Cons**\n- Large number of assumptions, the validity of which is unclear in practice: in particular\n   1. The assumption that the query vector is (1) a parameter and not a function of the inputs (as in self attention or cross-sentence attention) and (2) is fixed. I don't know of many \"real world\" attention networks that work this way, after all one of the main appeals of attention is its \"content-based\" nature\n   2. Assumption 1 that the score and embeddings of non-topic words don't change during training. First, this seems like something that could be proven from the earlier assumption that the topic words are updated more frequently. And second it is unclear if it holds for a real task (and a different model where eg. the attention layer attends to higher layers rather than just the embeddings)\n- Confusing notation makes the paper hard to follow (see remarks for examples)\n- Unclear takeaway: what does this paper tell us about attention as it is used in practice? \n\n\n**Remarks**\n-  5.1: \"The “negative effect” cannot last long because the topic word embedding moves along the gradient much faster than the non-topic words due to its high occurrence rate\": This is true in the toy example in the paper, but is this the case in practice? For instance in sentiment classifications there are many words to describe sentiment that are infrequent (cue Zipf's law). Moreover, in realistic settings there will be non-topic words which appear very frequently (stop words such as \"the\", \"a\" in English). \n- Lemma 1: while it is true that fixing q doesn't change the capacity of the model, it will definitely change its training dynamics (which is very much the theme of the paper as per the title). How important is it to fix q from this perspective?\n- A lot of the math would be easier to read if the dependence of some variables (\\hat v, Z,...) on a specific sentence was marked explicitly (eg. Z_S instead of Z)\n- The notation in Lemma 2 was extremely confusing to me, due to the sudden introduction of the bracket notation and the awkward spacing with both equations on the same line. I would recommend at least putting both on separate line, and also reorganizing so that the LHS of the second equation is only ds_i/dt (move the mean to the other side)\n- In 3. I think using \"\\mathbf R\" for the dictionary is unfortunate (too similar to \\mathbb R). Overall I found the separation between topic and non-topic words dictionaries confusing. Why not have a global Vocabulary V, a set of topic words T and refer to the remaining words as V\\T?\n- In 2. \"[Hahn and Brunner] show theoretically that the self-attention blocks are severely limited in their computational capabilities when the input sequence is long, which implies that self-attention may not provide the interpretability that one expects.\": can you clarify this sentence? Limitation in computational capabilities does not seem to entail limited interpretability in general (see linear models for instance).\n- Typo in citations in 2.: \"Hahn (Hahn, 2020) and Brunner (Brunner et al., 2019)\" -> \"Hahn (2020) and Brunner et al. (2019)\"\n- Typo in 3. \"The training set [...] are\" -> \"The training set [...] is\"\n\n---\n\n**Post Rebuttal**\n\nIn my review, the main concerns were (1) validity of assumptions, (2) confusing writing/notation and (3) unclear takeaway. The rebuttal appropriately addressed (1), although I am looking forward to the revision to see how this is discussed in the paper itself. I cannot really say anything about any improvements on the writing (2) without seeing the revision, but I am confident that the authors can address most of the issues pointed out by myself and other reviewers. Regarding (3), unclear takeaway, after reading the authors' response as well as the other reviews, my concerns are somewhat assuaged (partly because the assumptions were addressed better), although I am still unsure how or if the results in this paper could be expanded to realistic attention models.\n\nThere are additional issues I raised during the discussion (general lack of citations in particular), however this can be fixed fairly easily for the camera ready so I am willing to give the benefit of doubt and raise my score to 6 (borderline accept)\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A potentially valuable theoretical starting point, requiring significant assumptions.",
            "review": "Summary:\n\nThis paper aims to prove and illustrate that attention components are defined during training by gradients that mutually amplify the embedding and score associated with crucial features. In particular, a word embedding with a high magnitude increases the gradient following the attention score for the same word, while a high attention score increases the gradient directed at the word's embedding. In addition to a proof that treats behavior during training as a dynamical system under a large suite of assumptions, they test the analytic predictions on a synthetic dataset following the same suite of assumptions. They then test on a natural language data set and discuss where it diverges from the analytic and synthetic findings,  concluding that the difference is a result of competition between different words associated with a label.\n\nPros:\n1. We currently lack any substantial theory about attention modules and why they work. Although their model is simplistic, it could provide essential groundwork for analytic understanding of these popular systems. I would even consider it fairly realistic relative to a lot of the assumptions required for theoretical results in training dynamics research. Currently theory of attention is grounded in infinite-width networks, an assumption this paper does not make. \n2. The synthetic results appear to substantiate this theoretical result.\n3. They find an interesting result that, in more realistic settings, the learning dynamics follow particular patterns on the words that are paired together with more versus less predictive words. The framing of these effects in terms of competition between possible topic words is clearly inspired by considering which assumptions behind their proof have failed, which is evidence that the thinking behind their proof is potentially valuable.\n\nCons:\n1. It's not clear how dynamics like these would generalize to multilayer attention networks like BERT.\n2. The assumptions behind the theoretical and synthetic empirical results are simplistic:  The existence of a large vocabulary of \"non-topic\" words required to keep the variance of embedding negligible in out of focus words, the presence of only one topic word. There is also the very common assumption of Lipschltz continuity.\n3. The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity, but only presents an example of two words as evidence. I want to see quantitative evidence of the pattern.\n4. The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals.\n\n Questions:\n1. Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency?\n2. In Lemma 1, there is a reference to the attention block's capacity which is difficult to decipher. What do you mean here by capacity?\n3. The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training, but not later. Have you checked the actual variance that would be associated with word embeddings late in training?\n4. What is actually meant by a word being \"paired\" with another word in the natural language experiments?\n5. Did I misunderstand something in interpreting gradients amplifying the embedding and score as directed towards v and k respectively in this simplified model?\n\n Minor:\n1. Notation is difficult to follow at times because several unrelated concepts use almost the same symbols: $s_i$ indicates score, but $S_i$ indicates a sentence; $\\tau$ indicates learning rate, but $\\mathcal{T}$ (which looks identical as a subscript) indicates a set of sentences.\n2. In discussing early alignment of attention to syntax, Clark et al. 2019 was concurrent with https://www.aclweb.org/anthology/P19-1580/",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}