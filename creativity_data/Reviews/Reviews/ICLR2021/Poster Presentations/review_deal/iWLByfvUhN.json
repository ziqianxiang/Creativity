{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a hybrid VAE-normalizing-flow for extracting local and global representations of images.  While the reviewers found the model itself to be \"conceptually simple\" and \"straightforward\", all were convinced by the empirical evaluation that, indeed, interesting representation learning is going on, resulting in a unanimous vote to accept."
    },
    "Reviews": [
        {
            "title": "Official Blind Review #1",
            "review": "Summary: The authors propose a novel combination of VAEs and Flow models, where the decoder is modelled through a conditional flow taking as input a “local” representation of the size of the input image and a “global” representation output by the encoder. The authors evaluate the proposed method on density estimation, quality of generations and linear probing on a variety of datasets and show improvement over state of the art.\n\nGreat:\n* Conceptually simple method that seems to work quite well in practice, for this class of models. \n* The linear probing experiment is quite convincing in justifying the use of “global” and “local” characterizations of the learned representations. So are the interpolations.\n\nCould be improved:\n* It’s not clear to what extent each of the proposed refinements to Glow (reorganization, different splits, fine-grained multi-scale architecture) improves Glow’s performance. \n\nThe authors propose a novel combination of known methods, evaluate it extensively and show considerable improvements over current state of the art. A clear accept.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting general ideas, but critical flaws in the presentation",
            "review": "##### Summary\n\nThis paper aims to improve a Normalizing Flow generative model, in particular\nGlow, by conditioning the flow on global information of the image in the form\nof a latent vector learned with the VAE framework. This latent vector is\ninjected at the scale and bias terms of the affine coupling layers of the flow\n(inspired by what Style-GAN does at the batchnorm layers). \n\nUnfortunately, critical aspects of the method remain unclear or unspecified. To\nthe best of my understanding, the paper lacks a clear explanation of the\ncomplete pipeline used for training the method and the final objective\nfunction. For evaluation, sampling and likelihood computation, procedures are\nnot completely specified either.\n\n##### Pros\n- The general ideas of the paper are well motivated. Combining the advantages of\n  explicit likelihood and latent variable generative models is in my opinion an\n  extremely interesting research direction.\n- The authors propose architectural improvements to Glow and craft a conditional\n  version that can effectively incorporate additional information to the flow.\n- The authors model achieves improved or competitive density estimation,\n  sampling, and downstream performance across various datasets, compared to the\n  state-of-the-art.\n- Some degree of local and global properties disentanglement is demonstrated in\n  the qualitative results, showing the proposed direction is a promising one in\n  that regard.\n\n#####  Cons\nThe main drawback is in my view the presentation of the method. The method part\nof the paper (Section 2) mixes theoretical justification with architecture\ndetails and fails to clearly explain the full pipeline and training\nobjective. This made it very difficult if not impossible to analyze it and draw\nconclusions. The second half of the paper is dedicated to experimental results,\nyet the sampling procedure and likelihood computation are not clearly explained\neither.\n\nI think the submission would have been much stronger if the authors clearly\nexplained the whole method and dedicated more of the paper to a careful analysis\nand justification of the design decisions and of some of the claims\n(e.g. avoiding posterior collapse, global/local disentanglement).\n\n \n\nGoing into detail, by reading the abstract I get the impression that the VAE\nframework is used, and the normalizing flow is used to model the generative\ndistribution $p(x|z)$. Yet the abstract also claims to only use a plain\nlog-likelihood objective as in explicit likelihood models, instead of the VAE\nELBO. Alternatively, I thought the latent code was learned with a separate VAE,\nbut the introduction states that the generative flow is \"[embedded] in the VAE\nframework to model the decoder\".\n\nAssuming that the VAE framework is used with a (conditional) normalizing flow\nfor decoder, Section 2 introduces more confusion when the authors state \"we feed\n$z$ as a conditional input to a flow-based decoder, which transforms $x$ into\nthe representation $\\nu$ with the same dimension.\" This is confusing since\ntypically the decoder input should be a low-dimensional representation, but here\nit seems to be the image as well, so the concept of decoding seems ill\nplaced. Moreover, if this is the case, what would be the point of the\nreconstruction term in the ELBO if invertibility guarantees perfect\nreconstruction? Shouldn't the flow output $\\nu$ be a stochastic latent code as\nin VAEs? Is a prior density regularization imposed on $\\nu$ as well?\n\nFinally, another option would be that everything is trained with the negative\nlog-likelihood cost of the normalizing flow. This would be consistent with the\nclaim in the abstract that only a plain log-likelihood is utilized. But in that\ncase, what is the justification for using a stochastic encoder if it is not\nregularized? How can it be guaranteed that $z$ will not be ignored? What is the\nrole of $z$ during sampling? Does the likelihood computation involve $z$ or just\n$\\nu$?\n\nI apologize for writing my internal thought process but I also wanted to convey\nthat even if the method was clarified to be some of the options I described, or\nanother one, many of the design decisions would still require extra justification\nand analysis.\n\n\nOther comments:\n- Although Figure 1 shows capturing of some global color properties, looking at\n  the interpolations in Figure 5, there seems to be little variability\n  w.r.t. $z$, so maybe the claims of learning \"decoupled\" and \"disentangled\"\n  representations require more justification.\n\n- About the initialization of the weights of the last linear layer with zeroes\n  (Section 2.1). Wouldn't this create a null output at thus a null backward\n  gradient in the first iteration? Even if a non-zero bias was used, wouldn't an\n  unbreakable symmetry condition be produced?\n\n\n************\nAfter Rebuttal:\n\nI thank the authors for their multiple clarifications, and apologize for my initial misunderstanding.\nI understand now that the flow model is used to compute $p(x|z)$ as a function of $x$ and $z$. Maybe the \"decoder\" terminology is a bit confusing here, but this is quite a nice idea overall. It would have been nice to see multiple samples from  $p(x|z)$ for a fixed $z$, to evaluate the expressiveness of the model.\n\nI'm raising my score to acceptance.\n\nPS: Some typos remain in the revised version, eg: \"varnishes\", \"tne\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Decoupling Global and Local ....",
            "review": "The paper introduces a mixture of flows with the specific intent to disentangle global and local representations, to improve visual quality of samples. Architectures are borrowed from style-gan literature. The contributions are mainly architectural and empirical. The demonstrates improved visual quality over other normalizing flow methods. \n\nStrengths\nThe paper introduces a straightforward latent variable model for flows which is designed in such a way that training on NLL gives good sample quality, which is measured in FID. Even further, the model also performs quite well on the NLL objective itself. Since the focus of normalizing flow literature has been mainly on NLL, I think this paper is a nice complement to existing literature. \n\nWeaknesses:\n- The paper never puts the equations from the background section together in the final objective. It would be helpful to have an equation representing the final objective with some of the relevant variables (i.e. log p(x) >= ... with variables x, z, and v). \n- The paper does not introduce many novel theory or methods. This is not really a problem, but the paper can be better connected to existing work and clarified in this respect. The model the authors propose is very reminiscent of \"infinite mixtures of flows\" as outlined by (Papamakarios et al. \"Normalizing Flows for Probabilistic Modeling and Inference\" page 32). An example would be \"Continuously Index Flows\" by Cornish et al.. Note their method was introduced with a different intend in a different manner, so I think it would only make the paper better by citing these methods. \n- Conditioning on a context variable in flow layers is not new (see Kingma et al., \"Improved variational inference with inverse autoregressive flow.\"  and Lu et al. \"Structured Output Learning with Conditional Generative Flows.\"). This is not really a problem, but again this should be clarified. \n- How much added computation is required by the encoder model plus FCnet in eq. 7 compared to the Glow-refined model on which the proposed method is based? \n- Perhaps the naming \"compressing encoder\" is not particularly useful. It implies a direct connection to actual image compression, which is as far as I understand not the case. Other than that, this seems like a fairly standard VAE encoder other than the size difference between x and z.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but I'm not sure about the author claims. Are you really inferring global representations?",
            "review": "Pros:\n\n-> I like the idea of conditional generative flows, where a low-dimensional embedding captures high-level features and a larger embedding latent space captures local representations. \n-> Use a compression encoder to enforce informative low-dimensional embeddings\n-> Good experimental results\n\nCons:\n\n-> Can you really say that the low-dimensional embedding z is capturing GLOBAL variables? To me, global variables in a generative model drives non iid samples, hence correlating samples with each other. For instance, see the paper \n\nDiane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. In Thirty-Second AAAI Con- ference on Artificial Intelligence, 2018.\n\n-> The author should clearly explain the training cost function. At the end they are combining a flow generative model (typically trained using ML) with an encoder VAE like posterior approximation. I guess they optimize some soft of ELBO, but it is not clear to me yet.\n\nOverall, I like the paper and would like to see it accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}