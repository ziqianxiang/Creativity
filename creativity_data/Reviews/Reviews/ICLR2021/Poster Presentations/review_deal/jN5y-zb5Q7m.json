{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes information-theoretic quantification of epistemic uncertainty in autoregressive models. \n\nThis is a difficult problem that receives much less attention than the unstructured case. The paper is well-written, contributes novel and tractable-to-estimate measures which are analysed formally and empirically with convincing experiments on ASR and NMT. \n\nThe reviewers and myself are overall pleased by this submission. The discussion phase went well and most concerns have been resolved. \n"
    },
    "Reviews": [
        {
            "title": "AnonReviewer2",
            "review": "* **Summary**:\nThis work introduce rigorous information-theoretic measures for structured prediction tasks. It proposed metrics for both `\"total uncertainty\" (entropy) and \"knowledge uncertainty\" (MI, EPKL and RMI) on the sequence level, introduced efficient Monte-Carlo approaches to estimate them in practice. Finally, author conducted thorough experiment to evaluate the effect of ensemble strategy and choice of metric for error detection and OOD detection in ASR and NMT.\n\n* **Strength and Weakness**:\n * (Strength) Uncertainty quantification in structured prediction is an important but less explored topic. This work provided a much needed, information-theoretic framework to both conceptually quantify and empirically compute uncertainty measures for different sources of uncertainty.\n * (Strength) Thorough experiment on two tasks (ASR and NMT), where authors designed experiments to quantitatively measure uncertainty quality (Sequence-level Error Detection and OOD Detection) under different metrics and ensemble strategies.\n * (Weakness) The experiment conducted are mostly empirical where the ground truth is not necessarily known. Given the theoretical nature of this work, it might be good to conduct simulation study under known truth to examine the estimation quality of different metrics.\n\n* **Recommendation**: Acceptance. I believe this work provided a nice theoretical framing of different uncertainty measures for an important and less-explored area (structured prediction). It also conducted thorough empirical investigation showing the relative merit of different modeling and measure choices on two standard structure prediction task. The information contained in this paper should be of sufficient interest to ICLR community.\n\n* **Minor Comments**:\n   * Missing reference on bottom of page 4: \"rarely used in practice as it yields poor predictive performance ?.\"\n   * Middle of page 6: \"Rejection curves are summarised using the Prediction Rejection Ratio (PRR)\". Make sure to mention PRR is introduced in detail in Appendix D.\n    * Top of page 3 \"As will be shown later, RMI is particularly attractive ..\". It might be helpful to point to the section where the \"attractiveness\" of RMI is illustrated (Equation 12-13?).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "After rebuttal: I would like to thank the authors for paying attention to the comments and providing additional experiments and results. \nI updated my score. \n\n==================================\nThis paper investigates uncertainty estimation for autoregressive structured prediction models. The authors explored several metrics for computing uncertainty in structured neural network based models, such as expected pair-wise KL-divergence, mutual information, and reverse mutual information (RMI). The authors demonstrated the efficiency of the proposed RMI using machine translation and speech recognition models for error detection and out-of-distribution detection.\n\nOverall this paper is clearly written and investigates highly relevant topic. However, some clarifications are needed. First, I find the name of the paper a bit misleading, there are plenty of structured prediction models, and the authors investigated only neural based and autoregressive ones. \n\nSecond, I know the authors mentioned that they did not compare to prior work since it did not consider auto-regressive models, however, I feel that a comparison is still needed. There are a few papers (some of which the authors already mentioned), that suggest different ways to estimate uncertainties in AST and MT. I would expect to see a comparison to some of these methods. \n\nComments to the authors: \n\n1) In \"Practical Considerations\" there is a question mark, probably a missing citation?\n\n2) The improvements in Table 1 seems pretty marginal. Can the authors report the variance of the estimator too? \n\n3) \"The results also show that uncertainty-based rejection works better for ASR for NMT.\" → \"The results also show that uncertainty-based rejection works better for ASR than NMT.\"\n\n4) \"A better, but more expensive, approach to assess uncertainty estimates in NMT is whether they correlate well with human assessment of translation quality.\" → did the authors try to use subjective evaluations? \n\n5) \"Three OOD datasets are considered, each covering a different form of domain shift. First, LibriSpeech test-other (LTO)...\" → I find this definition of OOD a bit problematic. LTO is still part of librispeech, so I would not consider that as OOD.\n\n6) \"Notably, it was found that token-level Bayesian model averaging consistently yields both marginally better predictive performance and more robust estimates of uncertainty.\" → One reason for that is maybe since the authors are normalizing their probabilities at the token level and not at the sequence level? did the authors investigate such models that are normalizing at the seq level such as: [1].\n\nI'm willing to increase my score if the above questions will be answered \n\n[1] Collobert, Ronan, Christian Puhrsch, and Gabriel Synnaeve. \"Wav2letter: an end-to-end convnet-based speech recognition system.\" arXiv preprint arXiv:1609.03193 (2016).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New measure of epistemic uncertainty quantification for structured prediction but novelties are incremental",
            "review": "This paper proposes two different measures of knowledge (epistemic) uncertainty in structured prediction with an autoregressive model and discusses how to compute their approximations. The main contribution is the proposed reverse mutual information (RMI) as a measure of epistemic uncertainty in structured prediction. Experiments on benchmark datasets demonstrate the effectiveness of the proposed method in error detection and OOD detection.\n\n\n### Clarity\n#### Pros\n- This paper clearly states the main contributions, assumptions, experimental settings.\n\n#### Cons\n- The naming of reverse mutual information is confusing since mutual information itself is symmetric, unlike KL-divergence.\n- Table 1 shows the predictive performance with an ensemble of autoregressive models, however, it is unrelated to the main contribution of the paper. The proposed RMI is not used to improve the performance of the ensemble of autoregressive models.\n- Eq(4), $\\tilde{\\theta}$ is used without definition. And there is no $\\mathcal{D}$ on the right side of the equation. Why is $q(\\theta) = q(\\tilde{\\theta})$?\n- In Eq(12), how is $y$ sampled from $P(y\\mid x, \\mathcal{D})$? Do you mean to sample from $q(\\theta)$ instead of $p(\\theta\\mid \\mathcal{D})$?\n- Though the two ways, EP and PE, for combination of the ensemble of autoregressive models are both reasonable, only EP can be derived from assumptions in Eq (7). PE assumes the probability $P_{PE}(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\mathcal{D})$ factorizes over the conditional probability ${P}(y_{l} \\mid \\boldsymbol{y}_{<l}, \\boldsymbol{x}, \\mathcal{D})$.\nThis should be made clear, otherwise can be confusing.\n\n- On page 4, above Eq (10), it is claimed that \n> (8) only considers the probabilities of individual tokens $y_{l}^{(s)}$ along a hypothesis $y^{(s)}$ while (9) considers the entire conditional distribution over each $y_{l}$.\n\n  It is not clear why this is the case. In my understanding, (8) computes the entropy of the joint distribution $P(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\mathcal{D})$, \nwhich considers all the conditional distribution $\\mathrm{P}\\left(y_{l} \\mid \\boldsymbol{y}_{<l}, \\boldsymbol{x}\\right)$, no matter using EP or PE. Correct me if I am wrong.\n  \n- (Minor)\nTypos:\n  - Page 4, \nabove Eq (15),\npractical considerations\nas it yields poor predictive performance ? -> remove ?\n  - Page 4,\n  above Eq (10), while (8) yields A -> while (8) yields a\n  - Page 4,\n  below Eq (13), like (8)(9) -> like (8)(10)\n  - Page 5,\nabove Eq (16),\nthe models can combined -> the models can be combined\n\n\n### Originality\n#### Pros\n- The paper makes clear its main contributions, including introductions of different measures of uncertainty in structured prediction, the examination of two choices of the combination of ensemble models. The proposed reverse mutual information (RMI) improves the performance in OOD detection in structured prediction.\n\n#### Cons\n- The contribution of the paper is minor with the proposed RMI measure.\n- The proposed RMI for epistemic uncertainty estimation is not used to improve the performance of the ensemble of autoregressive models.\n\n\n### Significance\n#### Pros\n- Experimental results show improvement of the proposed method over baseline methods.\n\n#### Cons\n- The paper presents ablation studies of different methods for knowledge uncertainty estimation, but it is unclear whether the proposed method achieves state-of-the-art performance on different datasets for error detection and OOD detection. For example, compare to MC-dropout and other recently proposed methods without structured prediction with autoregressive models.\n- The contribution is not very significant, but mostly an extension and application of uncertainty measures to autoregressive models.\nFor example, the reverse mutual information (RMI), seems to be related to the Reverse KL-divergence proposed in [1]. More discussion is suggested on how the reverse version improves over the original version as is done in [1].\n- Besides OOD detection, it is also interesting to show whether the proposed uncertainty measure can improve other tasks, such as adversarial sample detection, and active learning. For example, [2] presents more thorough experiments in these different tasks.\n- The paper proposes two ways to combine the ensemble of autoregressive models, EP or PE. Experimental results are provided on these two ways of combination.\nHowever, it would be better if more theoretical analysis to compare these two could be given based on different assumptions. For example, analysis of the computational complexity of the two choices.\n- Similarly, it would be better if complexity analysis can be provided for computing approximations of the entropy using relative-entropy chain-rule in Eq (9),(11),(13), and using the traditional ways in Eq (8),(10),(12). Since it is claimed at \"no extra cost\" in the paper.\n- It is discussed in the paper that EPKL, RMI, MI all measures knowledge uncertainty, and RMI = EPKL - MI. And only MI 'cleanly' decomposes into total and data uncertainty. \nA natural question is that if we decompose total uncertainty using RMI, i.e. Total uncertainty = data uncertainty + RMI + res, what does the remaining term res represent?\nFor example, if RMI gives a better measure of knowledge uncertainty as shown empirically in the paper, then the remaining term in the total uncertainty may measure other types of uncertainty rather than data uncertainty or knowledge uncertainty? Or there are other ways to represent total uncertainty and/or data uncertainty correspondingly?\nCorrect me if I am wrong.\nAnd it would be better if more discussion can be provided in the paper.\n\n[1] Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness\n\n[2] SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}