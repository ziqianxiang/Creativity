{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": " This paper proposed a new method to prune neural networks using a continuous penalty function. All reviewers suggest acceptance (some are on borderline though) as the authors did a good job in the rebuttal phase. AC also could not find any particular reason to reject the paper (in particular, the overall writing is clear) and thinks that this paper is a meaningful addition to ICLR 2021. "
    },
    "Reviews": [
        {
            "title": "Incremental change to pruning methods with limited validation",
            "review": "This submission proposes a way to prune neural networks using a continuous penalty function. \n\n== Pros ==\n\nThe experiments include some interesting illustrations beyond the basic accuracy comparisons. This includes illustrations of how weights are distributed between the layers (Fig 5), ablations on different components of the penalty function, and experiments on the transferability of the selected sparsity pattern.\n\nThere is a very complete description of the training procedure. Writing overall is clear. Code is included with submission. The code is readable and easy to get running.\n\n== Cons ==\n\nMany previous works propose a new sparsity penalty function, and associated optimization, for neural network training. As well as (Lemaire et al) and (Louizos et al) cited in the submission, there's is works such as:\n\n  * Srinivas et al. \"Training Sparse Neural Networks.\" CVPR 2017.\n  * Manessi et al. \"Automated Pruning for Deep Neural Network Compression.\" ICPR 2018.\n  * Zhu et. al. \"Improving Deep Neural Network Sparsity through Decorrelation Regularization.\" IJCAI 2018.\n  * Yang et. al. \"DeepHoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures.\" ICLR 2020\n\nWithin this general class of approaches, though, the specifics of the penalty function and optimization in this paper are novel. Whether that is a valuable novelty depends entirely on the empirical results: since the higher-level idea is well-explored this adds to the literature iff the submission has made the best design choices among these other methods.\n\nDespite their overall heft, the experiments have some missing pieces that make it hard to evaluate this. To start, the evaluation is on a less common (within the sparse neural network literature) choice of backbones for each dataset. See:\n\nBlalock et. al. \"What is the State of Neural Network Pruning?\" Sections 3.3 & 4.2\n\nWider-ResNet would be expected to have greater redundancy than ordinary ResNet, perhaps making an easier problem for sparsifying. However, this choice is inherited from BAR.\n\nOrdinary ResNet-50 is a more common choice, but less so on CIFAR-10. This choice differs from the corresponding experiments in the (Liu et al) work that is compared against, that uses VGGNet, DenseNet-40, and ResNet-164.\n\nThe code appears to have partial support for a much wider variety of models (see line 19 of pruning.py), though maybe not all are actually supported: for instance r32 and r152 don't seem to be included in the branches in models/resnet.py lines 421-428.\n\nAnd generally there are few comparisons made. This is possibly unavoidable due to the overall fragmentation of metrics. Choices of comparisons are the more closely similar papers in the literature (this is good), but lacks more some of the simple baselines (that can often do surprisingly well):\n\nGale et. al. \"The State of Sparsity in Deep Neural Networks\"\n\nI don't see experiments that show proposed optimization has consistent convergence across runs. Experiments dealing with \"stability\" are either highlighted single comparisons, or look at stability w.r.t. hyperparameters. Some of the reported accuracy differences in comparison are quite small, especially at higher levels of sparsity, so seeing the standard deviation of multiple runs of each method compared might be especially important.\n\n== After Rebuttal ==\n\nI had previously missed that Lemaire et. al. had some of the key comparisons (simple baselines done for the same architecture/dataset pairs) that are necessary to judge this method. I thank the authors for pointing this out. It is also helpful that these baselines are now included for completeness.\n\nC.6 still seems like an indirect measure of optimization stability, but it is reasonable to conclude from it that the differences from BAR are real, given the very low standard deviation in accuracy.\n\nI remain on the side that this is still a relatively incremental addition to the pruning literature, but I am now satisfied that it is well-validated.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations",
            "review": "This paper proposes a new deterministic pruning strategy that employs continuous Heaviside function and crispness loss to identify a sparse network out of an existing dense network. Experiments show its effectiveness and robustness. Generally, it is well-written and easy to follow. Some minor issues are shown below.\n\n1.\tPseudo codes for the functions mentioned in Algorithm 1 should be provided to show them clearly. \n2.\tThe proposed algorithm is a deterministic algorithm and may fail in complex network pruning problems. An in-depth analysis of its limitation is needed.\n3.\tIt is recommended to use different labels for different algorithms in the figures.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review on Paper1752 ",
            "review": "Summary\n\nThis paper presents a new method for structure pruning called ChipNet. The ChipNet employs continuous Heaviside function with commonly used logistic curve and crispness loss to estimate sparsity masks. A combination of above three components is helpful to obtain approximately discrete solutions for a continuous optimization scheme.  As a result, it is possible to get a highly sparse network out of an existing pre-trained dense network. Experimentally, ChipNet outperforms other previous structured pruning methods by a large margin. \n\nOverall,  I don't think this paper has any critical drawbacks, and there are only a few comments. Therefore, I recommend accepting this paper.\n\nStrength\n\nThe motivation for this paper is clear and quite interesting. Also, the performance of the proposed pruning method is much better than previous baseline methods, and experiments were performed in various ways to show the effectiveness of it. \n\n\n\nComments and Weakness\n\nI have not much to criticize this paper, but I have some questions and comments as follows.\n\nIs it possible to get a similar result by applying the proposed method to the originally lightweight network such as Mobile Net?\n\nAre there comparisons with NAS-based methods?\n\nIs the proposed method applicable to networks for other tasks such as detection and segmentation?\n\n\nThere is a submitted paper (to this ICLR) that includes continuous relaxation of discrete network structure optimization for network growing (not pruning). \n\n“Growing Efficient Deep Networks by Structured Continuous Sparsification”\n\nThe direction of the paper is totally different, but there seems to be a similar part in terms of continuous relaxation. It would be good to mention the difference with this paper w.r.t. continuous relaxation.\n\nAlso, I would like that more related works are mentioned in recent techniques using continuous relaxation(approximation).\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official review3 : interesting approach but required clear justifications and experiments",
            "review": "##########################################################################\n\n\nSummary:\n\nThe paper provides a budget-aware regularizer method to train the network to be pruned. Existing methods based-on the regularizer method suffer from satisfying the user-specified constraints and resort to trial-and-error approach. The authors leverage logistic function and continuous heaviside function for continuous relaxation of the discrete variable to select the channels to be pruned.  There are four types of budget constraints (channel, volume, parameter, FLOPs).  \nThe experiment results on several different budget constraints (even in extreme condition) are quite promising. Also, these methods can be applied in transfer learning (transferrable network structure).\n\n##########################################################################\n\n\nReasons for score:\n\nOverall, I vote for marginal acceptance. I like the budget-aware constraints concept and its tightness which is shown in figure 3.\nHowever, I have several concerns about the papers (see cons) and authors need to consider these concerns in the rebuttal period. \n\n\n##########################################################################\n\n1. The budget-aware constraint is explicitly introduced and easy to implement.\n \n2. Ablation study in figure 5 provides insights into the network channel pruning.\n\n##########################################################################\n\n\nCons: \n \n(1) Experiment results about the other several pruning experiments are omitted. Therefore, I concern that the other readers are confused about the performance of the proposed method over the other pruning methods. Particularly, I wish to check the performance with [1].  Also, Experiments on the ImageNet (large dataset experiments) should be added for future research.\n\n(2) Details on the forward steps are omitted. (See 'Forward(x,W,z)' in Algorithm 1) Readers should have prior knowledge to guess the details on the forwarding steps.\n\n(3) Justification of the usage of two functions (logistic and heaviside function) is not enough. Two separate functions for continuous approximations are quite confusing. For example, '(Logistic(\\psi)+1)/2' can be used for continuous relaxation or 'Gumbel trick' can be used. \n \n(4) This might be the most critical one. Since the budget constraint is given based on the l2 loss with the target budget, '\\alpha2' should be tuned like the other regularizer based method. From a critical perspective,  there is no difference with the other regularized-based method in the sense that the trial-and-error on the tuning '\\alpha2' is stilled required and very large '\\alpha2' could degrade the performance of the network.\n\n\n=======================================================================================================\n\nAfter rebuttal : \n\nSorry for the delay. The authors address my concerns and reflect them on the revised version. I'm keeping my rating.\n\n\n[1] DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures, ICLR 2020.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}