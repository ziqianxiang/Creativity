{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes to lear molecular descriptors that account for the 3D structure of molecules. This is done by using first a \"Hamiltonian Engine\" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and second, a message passing algorithm that uses the predicted structure as input. The reported experimental results show state-of-the-art performance.\n\nStrengths:\n\n1 - Relevant contribution through the Hamiltonian Engine.\n\n2 - Strong empirical results.\n\nWeaknesses:\n\n3 - Some reviewers mentioned that the readability of the paper could be improved.\n\nI recommend the authors to also take into account the concerns of AnonReviewer1 to\nimprove the paper."
    },
    "Reviews": [
        {
            "title": "Interesting approach for incorporate 3d inforamtion in molecular fingerprints, but lacking results",
            "review": "This paper proposes to use 3d conformations for learning molecular fingerprints by 1) training a generative model to predict the 3d coordinates and 2) use those to train a \"fingerprint generator\" to obtain fingerprints by learning to predict molecular properties. The paper is well structured and clearly written. The intuition behind the Hamiltonian engine is nicely explained in the discussion section. The idea of a two step procedure to include 3d structure information in the training even when it is not available at test time is interesting and original. \n\nUnfortunately, some of the design decisions did not became quite clear to me: Why is the relaxation modeled with an MD-like dynamics approach instead of structure optimization, i.e. following the gradient of the energy? Although, here is an ablation study for leaving out the dynamics that performs worse, this might rather indicate that the initial guess of the positions is not good enough to find the correct equilibrium structure. Given the construction of the network that generates the initial positions, this seems quite likely, since it uses some canonical ordering (no permutation invariance) and does not include some notion of rotational symmetry. In contrast, there is previous work that demonstrates that this is possible with autoregressive models (Mansimov et al, Sci Rep, 2019; Gebauer et al, NeurIPS 2019). These methods also achieve much lower RMSDs than the presented approach on QM9 data (both <0.5 A RMSD). Another baseline would be to find a good positional guess, e.g. with RDkit, and then optimize with an ML force field that has already been used for MD (Schuett et al, J Chem Phys, 2019) or relaxation in crystal structure prediction (Podryabinkin, Phys. Rev. B, 2019). \nMoreover, since the datasets without 3d positions are labeled with RDkit (which i guess uses the same method as in the RDKit baseline of Table 1?), it would be interesting to see whether these structures would also be sufficient for the property prediction of QM9 in Tab. 2. Currently, I am not convinced that the more accurate positions (w.r.t. DFT equilibrium) obtained by HamNet actually improve the prediction that much. In particular, since the improvement compared to prediction without conformations in Table 2 is not significant according to the reported error bars.\n\nPros\n------\n+ Interesting idea, clearly written paper\n+ Physically motivated approach\n+ Improvements against simple baselines and ablations\n\nCons\n-------\n- Missing of important baselines (see above)\n- Only \"multi-mae\" reported for QM9. This makes it hard to judge the quality of the prediction and compare to predictions of individual properties in many other papers (DimeNet, MPNN, SchNet, HIP-NN, DTNN, PhysNet, etc). Please list also individual metrics and compare to previous work.\n- Datasets without 3d positions are labeled with a computationally cheap method from RDKit.  This does not make much sense: Why then not directly use these positions?\n- Timings for the prediction should be added to judge computational cost compared to a fast force-field or semi-empirical method.\n\nUpdate:\nThe responses cleared up some aspects of the Hamiltonian engine and I adjusted my score accordingly. Still, additional baselines would improve the paper answer some open questions and validate some of the claims: Is the Ham. Eng. indeed faster than optimizing with a cheap force field? Is there then an advantage compared to featurization of the RDKit coordinates with one of the many MPNNs for 3d coordinates? This would be interesting to analyze.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A very interesting work, but with a lot of questions from the unclear presentation.",
            "review": "Summary: This paper presents a novel neural network module called Hamiltonian Neural Networks to learn representation of molecules. The module consists of two main components: 1) Hamiltonian Engine (HE), and 2) Fingerprint Generator (FG). The HE 1) takes a molecular graph with atom and bond features as inputs, and first generate \"generalized\" positions p and momentums q via GNN+LSTM. These ps and qs are fed into a discrete Hamiltonian system with dissipation, and produces (generalized) \"conformations\" of molecules. The FG also takes a molecular graph with atom and bond features + the generalized positions and momentums from 1) as inputs to generate the final vector embedding of the input molecule. HE is trained to fit the input 3D conformation with a weighted combination of two types lossses (K-RMSD + 5 * ADJ-k loss). Posing this \"Hamiltonian\" inductive bias to the model, the paper demonstrated that the prediction performance over multiple molecular tasks from MoleculeNet are all improved. Also, the analysis of HE module or the hyperparamter sensitivity analysis are provided.\n\nComments:\nThis is a very interesting work in the light of molecular representation learning. The 3D geometries of molecules, i.e. conformations, are actually not rigid-body-like, rather has a degree of freedom varied according to the physical rule as we see them in molecular dynamics. It would be reasonable to see that this inductive bias explitly as \"Hamiltonian system with learnable parameters\" actually brings empirical performance improvements. \n\nI enjoyed the paper and core ideas, but the description and presentation of the (complicated) method's inner workings would need to be improved. In particular, it'll be nice to make clear the following points:\n\n- There are no description on the classifier or regressor head for downstream tasks of MoleculeNet, and how to train the entire network. This should be explicitly included in the paper. The paper only provides how to get the fingerprint, but seemingly there are no explanations or description on how to train this FG (or the entire module of HE+FG). We'll need to add some task-specifc heads after HE -> FG for MoleculeNet tasks. Then, is HE first trained with K-RMSD + 5 * ADJ-k loss, and the entire HE + FG fitted with the task-specific supervision loss? Or the entire HE + FG fitted with K-RMSD + 5 * ADJ-k loss + alpha * task-specific supervision loss (with some alpha)?\n\n- The model desciprion of Figure 1 is unclear and rough, and it was quite painstaking to get the information on what are learnable parameters and what are just states or intermediates. Also, the p and q from HE to FG would be also hard to capture at first glance. I understand that the module is quite complicated and not easy to depict it in one figure, but I'll appreciate if the authors can include all learnable parameters in Figure 1. (For example, like the model figure, Fig.4, of the DimeNet paper).\n\n- Related to the above problem, some \"ablation study\" would be quite unclear. For example, \"HamNet (real conf.)\" in Table 2 is to tweak HE somehow, or just feed the real 3D conformations into FG with momentums of 0s (with removing the entire HE processing)?? In the first place, the HE used 32-dimensional vector for ps and qs. Is it fair if this means to use 3-dimensional vector instead (at least, need to pass it to MLP for converting the same dimension?). What if we use 3-dimension for d_f??\n\n- \"HamNet (w/o conf.)\", \"Ham. Eng. (w/o LSTM)\", \"Ham. Eng. (w/o dyn.)\" are also quite unclear.\n\n- It is quite interesting to see \"HamNet (ours)\" was better than \"HamNet (real conf)\" even for QM9, and moreover, given that for most tasks, the 3D input conformation is generated by RDKit distance geometry with UFF. But the internal conformation of HamNet is actually \"d_f\"-dimensional (32-dimensional), and this difference could come from this expressiveness. To confirm this, it would be nice to include the performance of \"HamNet (ours) with d_f = 3\". (By the way, since the 2018.09 release of the RDKit, ETKDG is the default conformation generation method. What if we use this instead of the distance geometry...?)\n\n- It would be very nice to add some remarks on related work for readers. The name and method would definitely remind related work such as \"Hamiltonian Neural Networks (Greydanus et al, NeurIPS 2019)\" , Machine-learning potential for MD (see [a][b] below, for example), and and \"learning to simulate\" methods. See the “delta graph network” (DeltaGN) baseline of the cited workshop paper by Sanchez-Gonzalez et al, 2019. \n\n[a] Chmiela et al, Machine learning of accurate energy-conserving molecular force fields (Science Advances, 2017)\n[b] Chmiela et al, Towards exact molecular dynamics simulations with machine-learned force fields (Nature Communications, 2018)\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #3",
            "review": "**Summary**\nThe paper proposes a new method for generating fingerprints for small molecules. It is based on two components: a \"Hamiltonian Engine\" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and 2) a message passing algorithm that uses the predicted structure as input. The reported experimental results demonstrate state-of-the-art performance.\n\n**Strengths**\n o The manuscript addresses an important problem\n o The Hamiltonian Engine is a non-trivial contribution, which should be of broad interest.\n o The reported results seem to significantly outperform the current state of the art\n\n**Weaknesses**\nThe paper is information-dense and in some places difficult to read. \n\n**Recommendation**\nI recommend this paper be accepted. To my knowledge, the presented method is novel, and the results are convincing. Furthermore, it is an interesting contribution on the interface between physics and machine learning, which should be of interest even beyond the small-molecule ML subfield. The manuscript would however benefit from a slightly clearer presentation (see below).\n\n**Detailed feedback**\n\nAlthough I'm generally positive about the article, there are several places where I would recommend sentences could be improved to more clearly communicate the central ideas in the paper:\n\nPage 1. The last paragraph on page 1 should make it clearer what the role of the Hamiltonian Engine is. In my opinion, the choice of the word \"preserve\", is particularly confusing - it seems to me more like a \"prediction\" or a \"reconstruction\". The work \"preserve\" made me think that the positions from the original molecule were used to initialize the positions and momenta of the simulator (\"preserve\" thus meaning the ability of the forcefield to stabilize a given structure). Figure 2 cleared this up for me, but that does not appear until page 7. Also, the authors write that HamNet \"simulates the process of molecular dynamics\", but it is not clear why  a Hamiltonian Engine is needed at all - couldn't you simply have used a short MD simulation with an established molecular force field? \n\nFigure 1. Could anything be done visually to show how the Fingerprint Generator uses the output from the Hamiltonian Engine? Currently, the two methodological components of the paper seem a bit disconnected in the figure.\n\nPage 4. \"Graph-based neural networks are used to initialize these spatial quantities.\" It would be informative if you could add a few lines to motivate this choice. Couldn't you have used some idealized positions as a starting point for your simulation? Later on in the paper you mention that you use RDKit to create positions when they are not available in the data - couldn't you then just always use RDKit to create an initial structure - or would this be less accurate or too slow?\n\nPage 5. \"conformations always converge to a local minimum\" Perhaps you could comment on the nature of the learned energy landscape.  Do we expect it to be a highly multimodal landscape, with risk for convergence to suboptimal local optima? Is this why you place so much emphasis on the initialization of your simulations?\n\n**Clarifying questions to the authors**\nPage 4. \"we simplify the Lennard Jones potential\"\nWhy do you make the simplification to r^{-4} - r^{-2}? It seems to me that you could have calculated the standard r^{-12} - r{^-6} equally well from the r^2 that you have available.\n\nPage 4. \"we empirically normalize the relative atomic mass\".\nWhat is the reason for this normalization? It it just to improve the training behavior of the model? Is this a hyperparameter that needs to be optimized?\n\nFigure 2. Does the Step 0 correspond to an initialization produced by the GCN+LSTM? The positions look very compact - wouldn't you have expected them to look more realistic?\n\nPage 6. \"tuned with the Universal Force Field\" Could you have done a distance geometry initialization but using your learned forcefield for subsequent fine-tuning?. Would this be superior to using the Universal Force Field?\n\nPage 7, \"where the Hamiltonian Engine is removed and all q,ps are transformed from real coordinations\"\nIf you are only given structures, how to you obtain the momenta?\n\nPage 8. I found it interesting that it was necessary to map the positions and momenta to a higher dimensional space for optimal performance. Do you know if this overparamterization is necessary purely to facilitate gradient-based training, or whether there are more fundamental reasons for modelling them this way?\n\n**Minor comments**\nPage 1. \"For examples, Attentive Fingerprints have yet become the de facto state-of-the-art approach\".\nTo a reader not intimitely familiar with these references it is not clear what you mean by this sentence. Is Attentive FP a 2D method? I think there is also a \"to\" missing before \"become\".\n\nPage 4. \"but atoms with coincide positions\". \"coincide\" -> \"coinciding\"\n\nPage 4, \"we do not explicitly require \\hat Q to conform to labeled coordinations of atoms\". This was a bit difficult to read. I assume you mean that you only predict Q up to a rotation and a translation. Perhaps rephrase.\n\nPage 5. \"Speaking for detailly\". Rephrase.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}