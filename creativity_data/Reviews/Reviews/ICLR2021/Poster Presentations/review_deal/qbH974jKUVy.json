{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper seeks to empirically study and highlight how disentanglement of latent representations relates to combinatorial generalization. In particular, the main argument is to show that models fail to perform combinatorial generalization or extrapolation while succeeding in other ways. This is a borderline paper. For empirical studies it is also less agreed upon in general where one should draw the line about sufficient coverage of experiments, i.e., the burden of proof for primarily empirically derived insights. The initial submission clearly did not meet the necessary standard as the analysis was based on a single dataset and studied only two methods (VAE and beta-VAE). The revised version of the manuscript now includes additional experiments (an additional dataset and two new methods), still offering largely consistent pattern of observations, raising the paper to its current borderline status. Some questions remain about the new results (esp the decoder). "
    },
    "Reviews": [
        {
            "title": "interesting, but limited study on the ability of disentanglement to generalize",
            "review": "Summary\n\nLearning disentangled representation is often considered an important step to achieve human-like generalization. This paper studies how the degree of disentanglement affects various forms of generalization.  Variational autoencoders (VAEs) is trained with different levels of disentanglement on an unsupervised task by excluding combinations of generative factors during training. At test time the models are used to reconstruct the missing combinations in order to measure generalization performance. The paper shows that the models support only weak combinatorial generalization. The paper also tests the models in a more complex task which explicitly required independent generative factors to be controlled. The paper concludes that learning disentanglement representation is not sufficient for supporting more difficult forms of generalization.\n\nStrengths\n\nThe paper studies 4 types of generalization, interpolation, recombination to element, Recombination to range, extrapolation. \n\nIt shows beta-VAE can achieve reasonable generalization by interpolation, not the other three types. \n\nWeaknesses\n\nThe paper's study is limited to beta-VAE and dSprites dataset. However, it makes broad claims on the role of disentanglement in generalization.\n\nBeta-VAE has limitations in disentanglement. It is not clear other disentanglement approaches such as Wasserstein auto-encoder, InfoGAN-CR (ICML'20) would not generalize much better. \n\nThe study is on unsupervised disentanglement. Unsupervised disentanglement has inherent limitations, see \"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\" ICML'19.\n\nThe paper should conduct experimental studies on other datasets, e.g. those in the above reference.\n\nFor image composition tasks, it states \"concatenating input representations with the actions and linearly combining the resultant vectors\". It will be great to explain the insights.\n\nDecision\n\nThe paper has some interesting results on the role of disentanglement in generalization. However, the paper's study is very limited to specific model and a single dataset. Therefore, it is below acceptance threshold.\n \n----Post-revision update---\n\nThe authors have provided results on a second dataset 3DShapes and two more models – Factor-VAE and a perfectly disentangled model. However, the construction results of the GT decoder is much worse that other models, see Figure 2; it does not reconstruct the details of the \"heart\" shape even for training and the edges of the \"square\" are not straight. This begs the question how good the GT decoder is. The open question is, what is the generalization capability of a GT decoder that can both reconstruct and disentangle perfectly?\n\nWasserstein auto-encoder has been shown to disentangle better and the regularization term is on the aggregate posterior instead of individual samples. Without results on WAE, the paper should refrain from making broad claims on disentanglement.  Furthermore, it would be interesting to investigate GAN based approach such as InfoGAN-CR as well.\n\nFor an experimentation paper, it should be more thorough and go beyond just two shape datasets.\n\nI applaud the additional results the authors provided. I still think the paper is borderline (more toward 6 now). If it fixes the aforementioned weaknesses, I would recommend accept. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and relevant analysis, but the conclusions aren't clear enough yet",
            "review": "\nSummary\n---\nA large body of work creates disentangled representations to improve\ncombinatorial generalization. This paper distinguishes between 4 types of\ngeneralization and shows that existing unsupervised disentanglement approaches\ngeneralize worse to some and better to others.\n\n(introduction)\nThere are 3 types of combinatorial generalization. Each requires a learner to\ngeneralize to a set of instances where 0, 1, or 2 dimensions have been completely held out.\nPrevious work has not distinguished between these kinds of generalization when\ntesting how disentangled representations generalize. This work does that to\nunderstand the relationship between disentanglement and combinatorial\ngeneralization in a more fine grained manner.\n\n(approach)\nThroughout this paper, beta-VAE and a recent variant are trained with varrying levels of\ndisentanglement (controlled by beta) to reconstruct d-sprites images.\nThese images contain simple shapes and are generated using 5 ground truth\nlatent factors. The ground truth latent factors allow disentanglement to be\nmeasured (using Eastwood and Williams 2018), essentially by checking whether\nthe ground truth latent factors are linearly separable in the learned latent space.\n\n(experiment - plain reconstruction)\n* reconstruction error differs for different types of combinatorial generalization (holding out fewer dimensions is easier)\n* reconstruction error is not highly correlated with disentanglement\n\n(experiment - compositional reconstruction)\nInstead of reconstructing the input, a version of the input with one attribute changed is generated.\n* generation error differs for different types of combinatorial generalization (holding out fewer dimensions is easier)\n\n(conclusion)\nUsually disentanglement is encouraged to achieve combinatorial generalization, but this paper presents a simple experiment where it doesn't do that.\n\n\n\nStrengths\n---\n\nThe central claim of the paper may help clarify the disentanglement literature.\n\nIt seems very useful to taxonomize generalization in this way.\n\nThe writing and motivation is generally very clear. The figures are easy to understand and help demonstrate the narrative.\n\nThis paper aims to characterize an existing line of work in detail rather than proposing a new approach/dataset/etc. I like work of this nature and would like to see more like it.\n\n\n\nWeaknesses\n---\n\n\n1. The relationship between disentanglement and generalization is clearly or quantitatively demonstrated:\n\nThe most interesting claim in this paper is that disentanglement is not necessarily correlated with combinatorial generalization, but this claim is not clearly supported by the data.\n\n* The main support comes from table 1. Here higher D-score does not necessarily mean lower test NLL. This observation should be made quantitative, probably just by measuring correllation between D-score and test NLL.\n\n* Table 2 seems to contradict this claim. In that case higher D-score does mean lower test NLL.\n\n\n2. The taxonomy of generalization is a bit too specific to be useful and a bit incoherent:\n\nThe difference between \"Interpolation\" and \"Recombination to element\" generalization\nis not clear to me. Each of the purple and red cubes in figure 1a represents\na combinations of rotation, shape, and translation factors.\nIt may be that it makes a difference when some dimensions are categorial\nand others are continuous, as in the Interpolation example, but this doesn't\nseem to really solve the factor because continuous latent variables\nare still latent variables. I see some vague intuition behind this distinction,\nbut the paper does correctly identify the precise distinction.\n\nFurthermore, this taxonomy of generalization seems limited to me.\nIt seems like \"Recombination to element\", \"Recombination to range\", and \"Extrapolation\"\njust hold out a different number of dimensions (e.g., \"none\", \"rotation\", and \"shape and rotation\", respectively).\nThis begs the question of what happens when there are 4 generative dimensions?\nIs generalization when 3 of those are held out also called \"Extrapolation\"?\n\nI think more work needs to be done to create a taxonomy which precisely and clearly generalizes\nto N latent factors and creates a more coherent distinction between combinatorial and\nnon-combinatorial generalization.\nHowever, I think it's possible to create a better taxonomy and that it\nwill probably be very useful to do so.\n\n\n3. The paper should test the idea more thoroughly, on more datasets and on more disentanglement approaches. For example, it could include other datasets or tasks with different ground truth factors of variation (e.g., 3D chairs [1]). It could also include more disentanglement approaches like [2].\n\n\n[1]: M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d\nalignment using a large dataset of cad models. In CVPR, 2014.\n[2]: Esmaeili, B. et al. “Structured Disentangled Representations.” AISTATS (2019).\n\n\n\n\n\nComments / Suggestions\n---\n\nDescribe the disentanglement metric in more detail. From the beginning disentanglement is treated differently from combinatorial generalization. It's not immediately clear what disentanglement is that makes it different and why that's interesting to study. For example, initially one might think that beta-VAE is inherently disentangled.\n\nCan this taxonomy of generalization be generalized to continuous domains? For example, can it be generalized to any (typically continuous) hidden layer a neural net learns?\n\n\n\nPreliminary Evaluation\n---\n\nClarity - The presentation is quite clear.\nQuality - The claims are not quite well enough supported. The experiments that were run don't support a clear conclusion and more experiments should have been run to support a more general conclusion.\nNovelty - I don't think anyone has catalogued the performance of disentanglement methods in terms of a generalization taxonomy.\nSignificance - This paper might help clarify the disentanglement literature and more broadly help people think about combinatorial generalization.\n\nI like this paper because of its clarity, novelty, and significance. However, I think the quality concerns are significant enough that it shouldn't be accepted at this stage.\n\nFinal Evaluation (Post Rebuttal)\n---\nThe author response and accompanying paper revision clearly and effectively addressed each of the 3 main weaknesses I pointed out, so I raised my rating.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting study, but needs more experiments",
            "review": "Summary:\nThis paper studies the performance of models producing disentangled representations in the downstream task of combinatorial generalization. The experiments suggest that models producing disentangled representations do not generalize well enough. \n\nPros:\n- The paper is well-written and easy to follow. \n- The authors propose four novel benchmarks to systematically study the ability of a model to generalize. \n\nConcerns:\n- The key concern is that the paper does not present enough experiments to support the authors' claims. The study was conducted only for one dataset; I would suggest to include several other datasets in your study, e.g., MPI 3D, Shapes 3D, Cars 3D datasets. Also, the results would be stronger if the paper presented the assessment of other disentanglement specific metrics; see, for example, MIG [1], Modularity [2], etc.  \n\nComments/questions:\n- The combinatorial generalization task looks similar to the abstract reasoning task; it was shown that disentangled representations help in this downstream task [3]. How do you think, why it does not hold as well for combinatorial generalization?\n- Perhaps it would be interesting to vary random seeds in addition to $\\beta$ values; it was shown in Locatello [4] that random seeds sometimes have a stronger influence on disentanglement scores than model hyperparameters.\n\nMinor comments:\n- In some places, you write \"generalization\", in other -- \"generalisation\".\n\nUPD: The authors addressed my concerns and added additional experiments. The paper is improved, therefore, I increase the rating.\n\nReferences:\n\n[1] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen- tanglement in variational autoencoders. In Advances in Neural Information Processing Systems, pp. 2610–2620, 2018.\n\n[2] Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic loss. In Advances in Neural Information Processing Systems, pp. 185–194, 2018.\n\n[3] van Steenkiste, Sjoerd, et al. \"Are Disentangled Representations Helpful for Abstract Visual Reasoning?.\" Advances in Neural Information Processing Systems. 2019.\n\n[4] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. 2019.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good start, but restricted experiments limit the conclusions, and the discussion could be refined",
            "review": "Post-revision update\n------------------------\n\nThanks to the authors, I think that the revision provided by the authors makes the paper substantially stronger. The inclusion of the more complex Shapes3D dataset substantially improves the experiments, and I think the discussion has improved. I have revised my rating to a clear accept in accordance.\n\nOriginal Review\n----------------------\n\nThis paper evaluates the role of disentanglement in generalization. The authors begin by articulating a useful distinction between different kinds of generalization by interpolation, recombination or extrapolation. They then train VAEs (and variations thereof) on a controlled, synthetic dataset, using two different training paradigms. They show that the models only generalize well to one of the most elementary types (recombination to element), but do not extrapolate well to the more difficult kinds of generalization. They also show that disentanglement does not seem to correlate with better generalization.\n\nI find this paper to be marginally above the acceptance threshold, but it has room for improvement (see below).\n\nStrengths:\n\n* Generalization and the role of compositionality and disentanglement therein are very important issues.\n\n* I like the articulation of the different kinds of generalization, and the clear illustration thereof.\n\nAreas for improvement:\n\n* The relationship of these results to Locatello et al. (2019) would be worth discussing further. They also showed that disentanglement did not necessarily lead to better generalization, with a wider range of experiments (though therefore perhaps less deep in evaluating different types of generalization).\n\n* The experiments are very narrow. The paper uses a single dataset (though with two different tasks), and it is very toy. As noted by Locatello et al. (2019), the inferences drawn from a single dataset may be very biased. While simple synthetic datasets can be useful for allowing more carefully controlled experiments, it would be useful to explore the same experiment\ns using different datasets with different features (e.g. color and texture). It would also be useful to understand generaliza\ntion in richer, more realistic datasets (see below).\n\n* It would be useful to show some of the major claims in a less opaque way. For example, to show the (non)relationship between disentanglement and generalization, the authors could make a plot with D-score on the x-axis, and generalization performance on the y-axis (with different plot panels for the different types of generalization, perhaps). \n\n* One reason that exploring other datasets is important is that the particular inductive biases of the models may facilitate generalization along certain feature dimensions. For example, the fact that convolutions are generally (relatively) translation-invariant but not rotation-invariant mean that the model might more easily extrapolate to unseen translations. In order to draw broad conclusions, it would be useful to both explore more diverse datasets and quantitatively analyze in more detail t\nhe generalization along different dimensions.\n\n* Increasing realism can produce qualitative improvements in compositional generalization in some settings. For example, Hill et al. (2020) showed that e.g. generalization was better in a 3D setting than 2D setting, and that an RL agent showed 100% compositional generalization in a setting where a classifier only showed ~80%, for instance (although this generalization was recombination, not extrapolation). Thus, the poverty of the stimuli may alter the paper's conclusions. Even if your study is useful, it's worth discussing this limitation more explicitly.\n\n* The paper raises the question of why disentangled representations are not more effective in supporting compositional generalization, but it's worth asking why we assume that they would. Disentanglement $\\neq$ compositional representations $\\neq$ systematic generalization. Fodor & Pylyshyn suggest that compositional representations are necessary for systematic generalization, but they don't certainly don't provide an empirical definition of how to evaluate compositionality of representations. Indeed, it's hard to define such a notion: \"The question of whether a model [generalizes] according to compositional task structure is distinct from the question of whether the model’s representations exhibit compositional structure. Because the mapping from [...] representations to behavior is highly non-linear, it is difficult to craft a definition of compositional representations that is either necessary or sufficient for generalization\" (Lampinen & McClelland, 2020). Your results, along with those of Locatello et al (2019) and others, lend support for this argument. Disentanglement in some middle layer of the network does not seem to show a causal role in generalization, presumably in part because the processes intervening between that representation and the output are nonlinear, because that nonlinear decoder is also capable of failing for some combinations of latent representations even if the representations themselves are compositional, and/or because disentanglement is not a sufficient notion of compositionality. Given these difficulties, I think you could potentially extrapolate further than you do, to ask whether we should be worrying about representations at all, rather than just evaluating (and improving) behavioral performance on the different types of generalization you articulate.\n\n* However, even empirical evaluation is challenging in more naturalistic settings. The notion of \"disentanglement\" or \"composition\" may be harder to define in realistic datasets. It's not clear what the appropriate decomposition of a complex naturalistic image is — objects are a natural place to start, which is why Higgins and others have focused on this type of decomposition. But what counts as disentangled in a visual scene of e.g. a forest? Is each leaf an object that must be disentangled in its own right? Should the color of each piece of bark on each tree be represented by its own dimension, since *in principle* it could vary independently? This seems unreasonable, which is perhaps why disentanglement is usually demonstrated on very simplistic datasets. Yet a human can of course *attend* to any particular aspect of the scene to disentangle that dimension as needed. In the real world, human-like performance might require the ability to construct *new decompositions on the fly,* because the appropriate decompositions may change as the task or data shifts. That is, the idea of seeking a priori disentanglement with respect to fixed dimensions might not be the right way to go about achieving human-like generalization, especially if we want that generalization to extrapolate to new data and new tasks. (C.f. Lampinen & McClelland, 2020 for some other related discussion.)\n \n* It also seems likely that the processes that allow humans to exhibit strong generalization may require extended or additional processing, rather than a single feed-forward pass as in a VAE. This would be necessary to allow the sort of attentive disentanglement described in the previous point. For example, the Stroop effect in cognition seems to me to illustrate feature entanglement, which requires higher level control processes to resolve the appropriate response. The paper does discuss the idea that other mechanisms or architectures might be involved in the discussion, but it seems it bears more elaboration, especially w.r.t. the above points about the definability of disentanglement with real world data.\n\n\n\nReferences\n-----------\n\nHill, Felix, et al. \"Environmental drivers of systematicity and generalization in a situated agent.\" International Conference on Learning Representations. 2020.\n\nLampinen, Andrew K., and James L. McClelland. \"Transforming task representations to allow deep learning models to perform novel tasks.\" arXiv preprint arXiv:2005.04318 2020.\n\nLocatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. 2019.                                     ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}