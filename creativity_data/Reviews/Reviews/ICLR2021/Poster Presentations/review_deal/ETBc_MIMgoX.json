{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references."
    },
    "Reviews": [
        {
            "title": "Intriguing, simple idea, but worries about experimental methods",
            "review": "The authors introduce AMIGo, an approach to curiosity in which an adversarial \"teacher\" agent proposes goals that the \"student\" agent attempts to achieve. The student obtains an intrinsic reward of +1 when it achieves the goal, and this augments the extrinsic reward additively. The teacher proposes a goal at the start of each episode, and whenever the student reaches a goal. The teacher is positively rewarded if the student achieves the goal after some threshold t*, and negatively rewarded otherwise. t* is incremented at fixed intervals. They then demonstrate that this outperforms a variety of suitable baselines.\n\nThe algorithm proposed has great strength in its simplicity. It does bear a considerable similarity to Sukhbaatar et al 2017 (their ASP baseline), but AMIGo appears to be more flexible, and they put forth evidence that it considerably outperforms ASP on their benchmarks. AMIGo is something that could be adapted very easily to many other settings, and it is a quite agnostic framework. The experimental results do show considerable advantages over baselines.\n\nI am, however, concerned about how the evaluations were done. At the very least, I find that the main text fails to describe what seem to be important caveats that can only be found in the appendix. In particular:\n-For baselines, suitable hyperparameters were found using a subset of tasks (tasks thought to be easier), and, if positive results were achieved, the same hyperparameters were run on the rest of the tasks. If scores of zero were obtained for the initial subset, the baselines were not run at all and were assumed to score zero. \n-As noted in the main text, some of the baselines were developed for partially observed environments with an egocentric view, so they let the baselines run in both modes (with algorithms with multiple modules, they varied this independently for both). While this might seem like simply giving the baselines more chances to succeed, there is a problem when this is coupled with the first practice of assuming zero scores. Some of these baselines obtained no score in the fully observed setting on the \"easy\" task subset, and hence were not run on the hard environments. They did better in partially observed modes, and hence obtained their hard environment scores in these partially observed modes. These partially observed modes are potentially much harder!\n\nWhile one might be able to make some claim of the form \"if they can't do the easy task, they can't do the hard,\" I am worried that these choices, used together, really obfuscate the performance differential. Note the quite high score variance that AMIGo achieves on the harder environments. Given such high variance, models which were simply not run in the fully observed setting could have actually performed better were they given the chance (and also perhaps needed further hyperparameter tuning). At the very least, I think this is a really unclear way of benchmarking things and should be corrected so that we do not have these statistical worries.\n\nAs a result, I cannot at this time recommend acceptance, though I am certainly open to being persuaded that my worries are unfounded. I could have missed something about their experimental methods, or environmental particulars could somehow make the above treatments perfectly reasonable.\n\nA more minor concern: I do wonder about the extent to which this method applies in other settings. Is this time-threshold reward method applicable to a broad array of settings, or is this particularly useful in minigrid and similar sorts of maze  navigations? Are there situations in which time-to-complete is not a good proxy for difficulty? Is it important that the goals be represented in some very compact way, as they are for these experiments? These are questions that would, I think, need additional experiments. The authors did a considerable amount of evaluation, so I hesitate to simply ask for more, but I do wonder if this approach is particularly suited to a narrow range of environments.\n\nI am also curious as to why the ASP baseline did so poorly. Its method seems to be quite similar, so more commentary about what makes one fail badly while the other succeeds would be really useful!\n\n*Updated score based on below discussion*\n\n\n*Updated score again based on below discussion*\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "SoTA results on MiniGrid, similar ideas in previous methods, additional discussion about related works is needed.",
            "review": "This paper proposes a setter-solver or teacher-student scheme for training goal-conditioned agents (student/solver) in a discrete environment MiniGrid. In their method called AMIGo, the teacher takes the initial state of the student at the beginning of each episode as its input and proposes an intrinsic goal for the student to fetch, the student in each step takes an action and gets a reward combining both the extrinsic reward from the environment and the intrinsic reward, and the teacher gets a reward at the end of an episode or when the intrinsic goal is reached. The major idea is to make the goals proposed by the teacher staying at an appropriate/medium level of difficulty, i.e., fetchable for the student but not too easy. To do so, they use a threshold to define the teacher reward: they issue a positive(negative) reward to the teacher if the student spends more(less) steps than the threshold to reach the goal. And they adaptively increase the threshold if the student keeps successfully reaching the intrinsic goals. In experiments on five environments of MiniGrid, AMIGo outperforms five baselines with different types of intrinsic reward and set up new SoTA results on the studied tasks.\n\nThis paper is well-written and demonstrates a reasonable curriculum strategy by training a teacher model to propose goals in a discrete grid world (though largely simplified from realistic scenarios). The experiments show remarkable improvements over existing curiosity-driven or intrinsic reward-based methods and set up a new SoTA on those tasks.\n\nMy major concerns are the novelty of the idea and the generality of the proposed method.\n\n(1) Though the authors discussed the difference of AMIGo to GoalGAN, in which the teacher is instead a GAN generating continuous goals, the main idea behind the two are very similar, i.e., to train a teacher model proposing goals of medium difficulty, and they both use threshold(s) on rewards for this purpose. Although I agree that they use different architectures for the teacher model (for producing different types of goals), and the GAN in GoalGAN does not take student's initial state as input since it is not dealing with procedurally-generated environments, these are natural choices adapted to the specific environments/tasks for testing these methods. The main contribution of this paper, from the perspective of proposing a novel curriculum generator, is incremental.\n\n(2) Another related curriculum generating strategy is to balance the difficulty and diversity/representativeness of goals/tasks when proposing them to train the goal-conditioned student. It worth discussing the difference/advantage/limitation of the proposed method when compared to them. To name a few,\n\nPortelas et al., \"Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments\", CoRL 2019.\nFang et al., \"Curriculum-guided hindsight experience replay\", NeurIPS 2019.\n\n(3) The experiments only demonstrate the method in one type of environment. Although the MiniGrid is an appropriate environment to test the proposed curriculum generating method due to its simplicity, which allows the authors to rule out other factors and mainly focus on the quality of selected goals, it is not clear whether/how the proposed method can be easily generalized to other more practical and/or complicated environments/tasks such as robotic control, navigation, traffic, and gaming. More discussion on how to adapt AMIGo in those tasks can benefit researchers following this work.\n\n(4) The student is a goal-conditioned policy and it can be conditioned on one goal per time. During training, when is the student conditioned on the extrinsic goal or the intrinsic goal? Is there a schedule of assigning which goal to the student?\n\n(5) The student can also self-propose a goal by hindsight experience replay (HER) and this has been demonstrated to be effective in alleviating the sparse reward problem. It is interesting to see a discussion or comparison to HER methods. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Cute idea, but may be difficult to generalize and scale up",
            "review": "This paper introduces a teacher agent in reinforcement learning framework. The teacher's responsible for setting goal for the student agent (the \"main\" agent). The teacher acts in adversarial way (but also somewhat cooperative) in setting the goal. That is, the goal should be achievable by the student, yet the goal should not be too simple that the student stops learning towards harder goals, and eventually achieve the original goal from the environment. Experiments show proposed method is able to learn okay performance for hard environments, even though most other methods fail to score at all.\n\nPros:\nI think this is a very interesting approach and has great potential. The idea is natural and experiments make a lot of sense. This should partially credit to clear writing and examples. The authors provide only one set of results in main paper, but have more ablation studies and variants of reward forms. Those results will be helpful to provide more insight into how well the agent handles the task.\n\nCons:\nMy major concern with proposed method is that it seems limited to particular applications, such as grid world. As the authors also pointed out, even proposing goals in this paper's setting is a challenging problem. For now, I don't see an automatic way to proposing goals for other reinforcement learning tasks, such as Atari games and Go. In those tasks, I feel teacher's goals may need to be designed incorporating human's understanding of the tasks (i.e. capture a stone in Go). This makes me feel the proposed method is not very generalizable, but I am open to change my opinion if authors provide a good answer.\n\nIf for those tasks, a human-generated set of goals are needed by teacher agent, then how do we correctly attribute the performance gain we saw in this paper? That is, are the gains over baseline methods from 1) we have a better training framework, such as curriculum learning, or 2) the goals are set in a way that incorporate our understanding of the environment?\n\nMy second concern is about scalability of proposed method. For some applications, it may be unnecessary to have a teacher that sets intermediate goals (e.g. AlphaGo -> AlphaZero). How does sample efficiency for AMIGo compared to baselines? I saw results related to number of steps for AMIGo alone, but not baseline methods. Conceptually if a teacher sets many intermediate goals, it's likely that the student will train more number of steps and possible to achieve better performance. However if this procedure requires 10x more number of steps to achieve on-par performance for easy/medium level tasks, then it will be important to acknowledge that. When will the student need a teacher vs. w/o a teacher seems also worthy studying.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Goal Generation for Directed Exploration",
            "review": "## After Author Response\n\nThe author response for most of the points of confusion and weaker sections of the draft are satisfactory. I thank the authors for incorporating some of the feedback during this discussion. I am still on the fence about the generality of this idea, and how much the teacher objective is specific to the minigrid tasks. But overall I find this idea interesting and believe it adds value to the field.\n\n## Summary of the paper:\nThis paper proposes a goal-\"generating\" module (called the teacher) that will propose goals in procedurally generated grid world domains. The way they train this teacher is through policy gradient updates, with the teacher getting positive rewards if the agent achieves the provided goal but takes longer than a threshold to reach it, and negative rewards if the agent is unable to reach the goal or reaches it too quickly.\n\nThe threshold is gradually increased based on the agent's capabilities, allowing for more and more difficult goals.\n\nBoth the agent and the teacher additionally get rewarded for solving the external task. So the teacher is directed to provide goals that will also solve the given task, and the agent also learns to not just solve the given goal.\nOther auxiliary objectives for the teacher are entropy regularization for diverse goals, and episode boundary awareness.\n\n\n\n## Strengths:\n\n+ This paper presents a clear reward based mechanism to train the teacher. It also presents a reasonable answer to the question of how a difficulty threshold should be set for the teacher. It provides evidence that this goal-generation approach helps in directed exploration, is useful for general RL tasks (beyond goal-based RL), and speeds up solving of a variety of hard grid world environments.\n\n+ Figure 3 shows the changing thresholds and the subsequently generated targets for the agent, which is a good experiment to show that this increasing threshold mechanism works as intended.\n\n+ The ablations in the appendix are illustrative and show the different considerations and how different components of the system interact.\n\n\n## Weak Points:\n\n- The suggestion that the goal generation is adversarial seems slightly strong, with not much evidence to show that the generated goals are adversarial to the agent. The goals could also be generated progressively farther and farther due to the moving threshold.\n\n- The way the problem is set up and communicated makes it seem oriented towards grid-world problems exclusively. There is no definition of what a goal is. In fact, the problem setup itself is communicated rather vaguely, with terms being introduced as required rather than the entire setup being introduced clearly up front. Setting up the problem more clearly, with respect to RL literature (maybe refer to the Sutton and Barto book or alternative setups), being clear about what a goal is and how that goal is being used in the context of the grid world would make the paper clearer to read and understand.\n\n- It is also unclear whether the teacher rewards are discounted according to the number of steps taken by the agent, or if it operates at a lower temporal frequency. The entire teacher policy training regimen needs more explanation and clarity.\n\n- Given that the goal-generation here is to give an (x,y) location to travel to, it is unclear why an approach like GoalGAN cannot be adapted to this domain.\n\n- An additional question to ask would be whether the approach of Zhang et al can be adapted as a baseline to compare to this work. I can understand if it is not applicable, since that approach is specific to goal-conditioned RL, but would like some clarity on it.\n\n- When explaining the type of goals, the authors mention that this \"includes picking up keys, opening doors and dropping objects onto empty tiles\". How is an agent supposed to know whether it is supposed to move on to a square or drop an object there. The teacher does not seem to communicate what kind of goal is being generated. If the kind of goal is not communicated, wouldn't the agent do the simplest thing possible, and just move over the square, instead of figuring out that it should have dropped an object at the location?\n\n- The auxiliary task of \"Episode Boundary Awareness\" is a little unclear. My understanding is that the teacher is rewarded for selecting goals where the object type at that location changes if the episode changes. An illustrative example or clearer language is necessary to understand this additional objective and how it is helping the agent. This is very important as it seems that this auxiliary reward is essential in getting the teacher to learn goals that are useful for eventual agent success (as evidenced by the ablations in Table 6). In fact, I am curious as to why the performance of the system if this particular reward is removed drops drastically and is similar to the extrinsic reward being removed.\n\n- The main weakness of the approach for me is that it is unclear how this system scales to other RL problems. Since there is no definition of what a goal is considered to be, and no other examples given except grid worlds, it is hard to see what a broad application of this idea would be.\n\n- The learning curves (Figure 4) seem to show that the training with AMIGo is pretty high variance, compared to the baselines. Is there an explanation for this variance? Example, performance on OMmedium drops to 0 pretty regularly.\n\n\n## Additional Comments:\n\nThe related work section needs some additional work. While the authors present a broad array of related work, there are some suggestions to round off this section.\n\nFor intrinsic motivation, the authors should include work by Andy Barto (\"Intrinsic Motivation and Reinforcement Learning\", 2013) and other related work, as well as the optimal rewards framework by Satinder Singh's group (\"Where do rewards come from?\") along with work by Jonathan Sorg (ICML 2010) and more recently work by Zeyu Zheng (\"On Learning Intrinsic Rewards for Policy Gradient Methods\"). These views of intrinsic motivation as a vehicle for more than exploration is something that is not very clear in the current draft, even though the authors do present some alternative work like empowerment.\n\nFor curriculum learning, there are recent surveys that on curriculum learning for RL (\"Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey\", Narvekar et al.; \"Automatic Curriculum Learning For Deep RL: A Short Survey\", Portelas et al.) that should be referred to when explaining curriculum generation in the context of RL.\n\n## Conclusion\n\nOverall I would like to see clearer communication of some of the ideas as well as some explanation from the authors that will give me confidence that this idea is more broadly applicable.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}