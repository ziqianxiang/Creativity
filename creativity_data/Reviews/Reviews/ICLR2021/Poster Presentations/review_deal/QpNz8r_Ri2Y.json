{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.\n\nThe experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning. \n\nThere are concerns on writing style and comprehension. \n- The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods. \n- The presentation is very dense and quite hard to grasp, even with the Appendix.\n- The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper. \nI would recommend including the response to R1 in the paper.\n\nOther relevant and concurrent papers to potentially take note of:\n- Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization (https://openreview.net/forum?id=wiSgdeJ29ee) \n- Robust Offline Reinforcement Learning from Low-Quality Data (https://openreview.net/forum?id=uOjm_xqKEoX)\n\nGiven the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re-writing to make the manuscript more accessible, which in turn to increase impact of this work. "
    },
    "Reviews": [
        {
            "title": "This paper studies offline model-based RL, proposes a new framework to learn model representation that is robust under distribution shift.",
            "review": "- Clarity and Originality:\nThis paper is well-written and easy to read. The motivation is clearly stated: the original paper [Liu, et al., 2018] highly relies on the marginal action probability ratios to calculate the IPM metric, which suffers the curse of horizon issue. This paper addresses this by utilizing the discounted stationary distribution, which could be estimated using a similar DualDice trick. Theoretically the paper shows an upper bound for the policy evaluation error, which is a balance of model fitting error and the distance between the stationary distributions under behavior policy and target policy. This serves as a reasonable loss function to train the model. Furthermore, the learned model is also being used in learning task through iterative procedures. Empirically, the proposed method compared with the original Rep-BM in OPE setting, and various offline learning method in learning setting.\n\n- Significance:\nThe paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.\n\n-  Questions:\n1. For the evaluation task, it seems the only baseline is RepBM, how's the method compared with other off-policy evaluation methods, such as IS, DR? \n2. Typically how do you choose the hyper-parameter $\\alpha$, which controls the balance of model fitting and invariance in model representation learning?\n3. Compared with Rep-BM, this work seems to upper bound $|R(\\pi)-\\hat{R}(\\pi)|$, instead of MSE, which also results in different loss functions in training M, one is L2 loss, one is log-likelihood, how does this affect the empirical results?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting model-based offline RL",
            "review": "In this paper, the authors propose a model-based approach with representation balancing (RepB-SDE)to cope with the distribution shift of offline reinforcement learning. RepB-SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy in the representation space. RepB-SDE adopts the estimation techniques of DualDICE and a novel point is that RepB-SDE plugs this trick into the model-based representation learning and proposes an effective model-based offline RL algorithm.\t\t\t\t\t\n\nThe combination of policy estimation techniques and model representation learningin this paper looks interesting and its theoretical derivation is sound. The experiments demonstrate the effectiveness of RepB-SDE over almost 10 baselines in the popular offline RL benchmark D4RL. But this experiment part can be improved, e.g., including the state-of-the-art offline model-free baseline (Kumar et al., 2020; CQL).\t\t\t\t\t\n\nThis paper is well written, especially its methodology and experiment parts are clear. But I have some concerns about the motivation of RepB-SDE in the introduction part, where this paper says that \"However, recent offline RL studies mainly focus on how to improve the policy conservatively while using a common policy evaluation technique without much considerations for the distribution shift.\" BCQ (Fujimoto et al., 2019) considers the Q-learning (value-iteration)framework of abstracted MDP induced by the given dataset and uses an ensemble of action samplings to deal with continuous action space. From the Q-learning perspective, BCQ can realize effective policy evaluation in the tabular case or with rich expressiveness of Q function. The policy evaluation technique of BCQ does not need importance sampling (see (Haarnoja et al, 2018; SAC) similarly) orDualDICE-like techniques. In order to better presentation, the introduction part needs to be well-motivated and justify recent offline RL algorithms fairly. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "dense presentation gives the reader a hard time",
            "review": "Summary:\nThe paper deals with batch RL (aka offline RL). It builds on \"Representation Balancing MDP (RepBM)\" and tries to improve the procedure by \"stationary DIstribution Correction Estimation (DICE)\". DualDICE\", a further development of DICE, is used for this purpose. \nThese procedures are known in each case.\nMain claims:\n„We empirically show that the model trained by the RepB-SDE objective is robust to the distribution shift for the OPE task, particularly when the difference between the target and the behavior is large.„\nThe introduced „model-based offline RL algorithm based on RepB-SDE“ has „state-of-the-art performance in a representative set of tasks“ from D4RL.\n\nStrong points:\nThe paper makes no exaggerated claims.\nThe treatment of the newer, related works is very good.\nThe experiments are extensive.\nI like the formulation „behavior-agnostic setting where we do not have any knowledge of the data collection\nprocess.“  This expresses the, in my opinion, correct view of the real situation well, while the assumption that there is a \"behavior policy\" that generated the data is not true in general. It may have been different people at different times who performed the actions while the data set was recorded.\n\nWeak points:\nThe work is on the one hand very specialized, on the other hand just an incremental modification of existing methods.\nThe presentation is very dense and quite hard to grasp, even with the Appendix.\n\nRecommendation:\nBorderline accept.\n\nQuestions:\nIn the Appendix a potential limitation to deterministic environments is mentioned. Is this just a special case, or is this a true limitation? If it is a true limitation, then this should be mentioned not only in the Appendix, but in the main text.\nWhy are the uncertainties (standard error) of the other methods not also given in Table 1?\n\nAdditional feedback with the aim to improve the paper:\nPlease be more explicit to make the text more understandable. It should be clarified early that the procedure is applicable to continuous state spaces and continuous action spaces---the initial consideration of the stationary distribution uses discrete state and discrete action spaces. Accordingly the spaces S, A, Z should be defined.\n\nPlease correct missing capital letters in the bibliography, e.g. mdps, Algaedice, gaussian\nAnd fix \"\\phi-divergences\" -> \"$\\phi$-divergences\"\n\n\"16GB\" -> \"16 GB\"\n\nIn Figure 1 the measurement points are connected by lines. Actually lines in such a plot are reserved for a fit to the points or a theoretical curve. The plot would therefore look more scientific if the points were not connected.\n\n\n===================\n\n(Nov 24)  I increased my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Assumptions need better motivation",
            "review": "Strength: \nModel learning is an important component for offline RL, which is usually done independently from policy evaluation / optimization. The authors propose a new model learning method for offline RL that takes policy evaluation error into consideration / as regularization. \nAn upper bound is derived to guarantee the worst case performance. In terms of policy evaluation, the authors show empirical advantages over previous model-based offline OPE algorithms. In terms of control, the authors show empirical advantages over existing model-based and model-free algorithms in the challenging D4RL dataset.\n\nWeakness & Points to be clarified:\nMy major concern is the assumption used in the paper. \nThe assumption about B_\\phi in Theorem 4.3 looks not well motivated. When should we expect there exists such a B_\\phi? How large are B_\\phi and \\bar{k}? If B_\\phi and \\bar{k} are very large, I feel the bound in theorem 4.3 can be very loose. I think the paper may benefit from clarifying more on this assumption.\n\nMinor comments:\nThe authors compare with both model-based and model-free approaches in the control setting.\nIn OPE, however, only model-based approach is compared. I would suggest to add more model-free baselines, e.g., Fitted-Q-Evaluation [1] and DICEs, to motivate the necessity for learning a model.\n\nOverall I think the empirical results are convincing and I am happy to increase my score if the assumptions are further clarified.\n\n[1] Voloshin, Cameron, et al. \"Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning.\" arXiv preprint arXiv:1911.06854 (2019).\n\n===================\n\n(Nov 24) The authors addressed my concerns in the reply so I increased my score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}