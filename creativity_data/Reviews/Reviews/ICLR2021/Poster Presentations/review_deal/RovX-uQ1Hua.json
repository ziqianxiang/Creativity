{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is well-written, it is clear and concise. The idea of learning to generate text from off-policy demonstrations is interesting. The results experimental results are good. The authors seem to address the concerns raised by the authors during the rebuttal."
    },
    "Reviews": [
        {
            "title": "Emergency review",
            "review": "This paper proposes a new reinforcement learning approach to training generative neural models of text.\n\nThe paper's core motivation is that optimizing text generation model with the cross-entropy H(p_data, p_model) gives too much leeway to the model to assign weight to low quality outputs. The proposed solution, is to minimize H(p_model, p_data) to ensure high precision. A variety of practical solutions are proposed to achieve this, presented under the lens of reinforcement learning. Experimental results show that the proposed training algorithm, \"GOLD\", achieves better results in terms of generation metrics on a selection of conditional generation tasks (MT, summarization, etc...), despite a lower perplexity.\n\nOverall the proposed method seems promising, although it relies on a lot of approximations and specific design choices, the effect of which is not always clearly explained or empirically investigated\n\nPros:\n- Clear motivation\n- Good results on a variety of tasks\n- Clear improvement on long sentences\n\nCons:\n- a lot of optimization \"tricks\" introducing additional hyper-parameters are only mentioned in passing, and their effect is unclear without ablation studies. Some examples:\n  * lower-bounding of the importance weights with u: What effect does the value of u have? with the current value, how many of the importance weights are actually used?\n  * Periodic updating of the policy used to compute importance weights: why is this necessary? What effect does the periodicity of the update have on results?\n  * Truncation of the future trajectory after 5 steps. Why 5? Why not using multiplicative discounting?\n\nQuestions & remarks:\n- The \"high recall vs high precision\" problem mentioned in section 2 is very reminiscent of the inclusive/exclusive behavior of the forward/reverse KL divergence, perhaps this is worth mentioning\n- In Section 2 again, on the issue of initializing the model with the MLE, the authors say: \"However, this would bias the parameters towards the MLE solution, thus often leads to marginal gains in practice.\". I think that this claim should be substantiated via a proper citation or additional experiments.\n- In the related work: \"Reward Augmented Maximum Likelihood\" (Nozouri et al. 2017) is worth citing as well\n- In references, some papers are cited as arxiv papers even though they were published at archival conferences. This should be fixed. Examples: \n  * \"Globally Normalized Transition-Based Neural Networks\": ACL 16\n  * \"Minimum Risk Training for Neural Machine Translation\": ACL 16",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but scope is overstated, significance based on current results still not clear",
            "review": "TEXT GENERATION BY LEARNING FROM OFF-POLICY DEMONSTRATIONS\n\nThe authors propose an `````\"off-policy\" approach to training sequence generation models. The approach is based on using ``\"behaviour policy'' or demonstration state distributions and corrects for the action distribution, through a local importance weight, which effectively reduces to the current probability under the policy being trained. Reward functions that assign identity reward to demonstrations, and reward according to the (log/linear) probability of the current sequence under the model are considered. Results suggest that the proposed approach (GOLD), can be utilized after MLE training and before on-policy RL training to improve results, but the experiments are in serveral respects inconclusive (see below for further details).\n\nStrengths:\n\n- The paper is well written, and the approach is straightforward and clearly explained (caveats below).\n- Several results suggest GOLD leads to better performance over MLE, and boosts the performance of on-policy RL fine-tuning after MLE (see below for limitations wrt this)\n-It is interesting that GOLD reduces exposure bias so much, given that only the action distribution is being corrected for.\n\nCurrent Limitations: \n\n- The approach is pitched as a new approach to sequence training, and is quite dismissive of strong existing work that looks at hybrid ML/RL objectives, when in fact this approach is applied and only makes sense when applied after MLE training (citations missing, see last bullet for some). More specifically, the overall objective (in terms of importance weight, and optionally reward, c.f. equation 4) self-trains in a way that improves precision at the expense of recall, which the authors themselves state 'encourages the learning algorithm to focus on “easy” examples'. This drops modes, as evidence by poor perlexities reported in table 1, and so it is only appropriate as a fine-tuning step after MLE, or a transition step before doing on-policy RL on a sequence metric.\n- The significance of the results is not clear. There are several issues with the results. First, important details regarding the (on-policy) policy-gradient methods that are investigated are missing. Does PG mean REINFORCE? REINFORCE with a learned baseline or other baseline? The baselines utilized in ``PG\" and PPO will have a major effect on the results. Is there any warmup for these on-policy RL algorithms, in terms of learning rate, or curriculum learning (Ranzato et al., 2016 as cited)? Again, these are crucial to performance. This is important because the paper should really compare GOLD directly to on-policy results (missing), and/or show that GOLD is a useful step that leads to gains when strong on-policy finetuning is done (done, but not convincing, due to missing details on on-policy RL). In addition, the most appropriate techniques to compare to are those cited in the \"High Precision Text Generation\" subsection in section 6, but these are not compared to: but this is exactly the primary objective of the work!\n-The premise of the paper is to overcome exposure bias and attain higher precision, but the exposure bias issue has been shown to be less severe than originally postulated, and diversity in generation is extremely important in many (most) applications (c.f. Caccia, M., Caccia, L., Fedus, W., Larochelle, H., Pineau, J. and Charlin, L., 2018. Language gans falling short. arXiv preprint arXiv:1811.02549.). Furthermore, mode dropping is an element of GAN/RL training that hybrid ML/RL models have striven to overcome (c.f. Norouzi, M., Bengio, S., Jaitly, N., Schuster, M., Wu, Y. and Schuurmans, D., 2016. Reward augmented maximum likelihood for neural structured prediction. In Advances In Neural Information Processing Systems (pp. 1723-1731).; Ding, N. and Soricut, R., 2017. Cold-start reinforcement learning with softmax policy gradient. In Advances in Neural Information Processing Systems (pp. 2817-2826); Sabour, S., Chan, W. and Norouzi, M., 2018. Optimal completion distillation for sequence learning. arXiv preprint arXiv:1810.01398; so many more...)\n-As discussed above, the approach is more limited in scope than advertised. This would definitely need to be corrected to consider accepting the paper.\n\nOverall Assessment:\n\nThe paper is well written and the approach is straightforward and interesting, but the premise of the approach, and the experiments supporting the signficance of the method need further strengthening. My initial evaluation leans toward rejection, but I look forward to the authors response.\n\nquality:          7/10 (+solid formulation, +experiments on several datasets and models with analysis)\nclarity:           6/10 (+well explained, -scope of paper currently overstated, -important experimental details currently missing)\noriginality:    6/10 (-more std. application of off-policy RL basics to sequence learning, +intuitive choices for reward fns, policy importance weights)\nsignificance: 5/10 (+important problem, -significance of experiments currently not clear, -importance of overcoming exposure bias and sacrificing recall for precision currently not adequately supported/defended) \noverall          5/10 (overall evaluation based on current limitations)\n\nPost-rebuttal feedback:\n\nAuthors, thank you for your feedback. The additional comentary and results around diversity have strengthened the paper. However, my other concerns, as described below, remain, and so I have not increased my score, but I have indicated that if an effort is made to address these remaining issues, I would recommend that the paper be accepted.\n\nAfter reading the reviews and the authors responses, I have several remaining concerns.\n\n-First and foremost, the authors have confirmed that the on-policy results that they compare to are using weak baselines to normalize reward (constant, avg. of last 100 steps). Strong context-dependent baselines are known to be crucial to the performance of on-policy methods. The authors attempted to do some experiments with a MIXER variant without a learned baseline (Ranzato et al, 2016) given my concerns, but MIXER without a learned context-dependent baseline is not MIXER! This is serious, as the conclusions stated in the paper cannot be made until the technique is compared properly to on-policy methods (i.e. that at least utilize context dependent baselines... those with learned q functions [e.g. An actor-critic algorithm for sequence prediction, Bahdanau et al, 2016] are often even more effective)\n\n-The authors did not tone down their claims, or criticisms of on-policy methods requiring MLE pre-training in the paper, despite the fact they also initialize with and ML model, and interleave ML updates during training! The tone of the paper is clearly in need of revision, as I discussed in my review.\n\n-This is not a major concern of mine, but it worth mentioning that the novelty of this paper is actually on the low side. This is a standard application of truncated off-policy learning, and the cited paper out of Bengio's lab (Serban et al, 2017) is in the text domain, and describes essentially the same off-policy approach (although this paper is arguably more clearly presented, and more focused and thorough wrt investigating off-policy variations). In addition, as another reviewer mentioned, the connections to and related work that considers Jenson-Shannon and reverse-KL minimization are strong (and interesting), but they are not discussed/referenced at all.\n\nWith all that said, for the most part, I actually like the paper. If the language/position of the paper is toned down/updated, and the results are updated to include stronger on-policy baselines (regardless of the outcome), I think that the paper would be above the acceptance threshold.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reformulization of Text generation training as an off-policy RL  objective where samples are obtained from the training data ",
            "review": "This paper formalizes training of conditional text generation models as an off-policy RL objective, specifically, in the limit case where samples are only obtained from the training data. The motivation behind this is that MLE objective optimizes recall -i.e. increasing the prob of all correct sequences that could be generated by humans as an output to a certain input context. While for certain tasks such as MT or summarization it is often sufficient to focus training to generate 1 single correct Translation or summary (see cons: for comments on the effect of this on sample diversity).  Therefore explorations in traditional PG by generating examples from an auto-reg model is unnecessary in this context. Therefore, using importance sampling, PG objective \\E_{x \\sim \\pi_\\theta} is modified to \\E_{x \\sim \\pi_{data}} with the incorporation of the importance ratio and given a uniform sample probability of the training examples. As shown in eq(4), The gradient updates from this loss are identical to the standard MLE loss reweighted by the global reward and the current model probability of the training example, enforcing the model's current belief of the training data. This aligns with the previous intuition of enhancing precision on the expense of recall. Given this formulization this work experiments with three types of rewards: a constant reward and 2 MLE based rewards.    \n\nPros: \n- formalization of text generation problem as an off-policy RL one, the formalization itself is still interesting, although the fact that in practice no interactions with the environment is needed and all rewards are differentiable. \n\n- The discussion about Precision vs Recall objectives interesting (Although could have been elaborated further discussions see cons 1)  \n- comparison with strong baselines on 2 tasks and to MLE on 4 tasks QG and summ, Xsum, and MT achieving convincing results on automatic metrics and human evaluation.   \n- Interesting analysis of exposure bias evaluation of quality w.r.t length compared to MLE models. \n\nCons:\n- Peaking the output distribution of conditional NLG model could enhance the generations precision but on the expense of sample diversity. It is expected that GOLD will more peaked than the one of MLE this can be seen in table2 when increasing the beam size or the top-k sample has little effect on GOLD. This might be suitable for in the case of conditional seq2seq problems,\nsuch as Machine Translation, where focusing on a single good output for a given input makes sense,\nbut is clearly in-adapted when focusing on language models, or paraphrasing applications where sample diversity is a requirement.\n- the choice of constant baselines -60 and 0 is not justified, how did you select such values and why the choice of a constant baseline rather than a simple baseline of mean reward?\n\n\nQuestions: \nQ1: is the δ reward just a constant reward of value 1? as examples are just coming from the training data.\n\nQ2: Although I like the formalization of training seq2seq models as an off-policy RL problem.  As it could open more frontiers to more exploration of advanced RL training objectives (e.g. on/off policy interpolation ..etc), the case of this work where all demonstrations are from the training data and no interaction with the env. is required makes the RL formulization a bit hard to digest in this context. I was wondering if and easier formulizations could have been achieved for e.g. A formalization that starts from directly optimizing the rewards using standard supervision since they are all differentiable. \n\nQ3: related to Q2, Could you comment on the conceptual differences between for example maximizing the reward R_p (the MLE based reward) under the proposed RL framework using training data as demonstrations and optimizing the MLE directly?\n \nQ4: Since you initialize \\pi_\\theta with the MLE model, could one say that all that GOLD does is to increase the peakiness (Choshen et al. 2020) of the MLE model achieving higher precision on the expense of sample diversity which is arguably not necessary for some tasks. If that is the case could one achieve similar gains just by adjusting the temperature of the softmax or by letting the MLE loss overfit the training data more (achieving higher validation PPL by maybe better task reward)?  \n\nQ5: What is the motivation behind the lower bound u to the importance weight?\n\nMissing Reference:\nOff policy, PG has been proposed in (Parshakova et al. 19) for optimizing global sequence rewards in a distributional perspective rather than an optimization perspective as shown in this paper. \n\nDistributional Reinforcement Learning for Energy-Based Sequential Models, Parshakova et al. 19\nhttps://arxiv.org/abs/1912.08517\n\n\nminor: \n- Typo in the caption of table 5 \"Win % generations from MLE that are better than GOLD\"  should it be the opposite?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "This paper proposes a method to train generative models of text using reinforcement learning from off-policy demonstrations. This helps solve the problems of exposure bias and mismatched objectives in standard learning schemes such as maximum likelihood estimation and policy gradient optimization on metrics like BLEU. In the proposed method (GOLD), the authors use policy gradient combined with importance weighting to train the model using just the off-policy demonstrations, i.e. human-written text. They experiment with three different reward formulations, and demonstrate improvements over MLE baselines on tasks like summarization and machine translation. \n\nStrengths:\nNovel method for training a generative model for text with off-policy learning.\nExperiments are well executed and provide interesting insights. \nWriting is mostly clear.\n\nWeaknesses: \nOne or two open questions that could improve the insights provided in this paper (points 3, 5 below)\n\nQuestions/comments for authors:\n1. It might be worth mentioning somewhere (Introduction?) that this method is for conditional text generation. Although I can see it working for unconditional generation (e.g, language models) as well, as long as human evaluations are performed?\n2. Approximation to importance weighting: “In practice, we have found it is sensitive to optimization hyperparameters…” Could you provide a bit more detail/explanation on this? Does it result in lower BLEU scores or does the model not converge at all?\n3. It is interesting that the sum of probabilities reward (GOLD-s) performs better than GOLD-p. To me, this seems like maximizing the product of the exponentials of the probabilities (i.e. $\\prod_i e^{p_i}$ ) instead of the product of probabilities. This might naturally make the distribution more peaked and hence prevent mass over low quality sequences. So, I’m curious if maximizing the cross-entropy using the exponential distribution (e.g. $\\hat{p_i} \\propto e^{p_i})$ or adding some auxiliary loss to encourage this would help prevent assigning higher probabilities to low quality sequences as well?\n4. “To avoid negative reward… we lower bound $p_{MLE}$”. Isn’t the reward always negative since you take $\\log$? I think this is because of the baseline applied later on, but as is, this sentence may confuse readers. \n5. As mentioned, the goals of MLE and GOLD seem to be orthogonal (maximize high recall and precision). Why not combine the two objectives? Would that result in a better model?\n6. Table 5 caption seems to say MLE generations are preferred to GOLD, but this contradicts the claim in text. Is there a typo somewhere? I would also recommend using more explicit. column names (rather than win, lose) to make it clear which method is preferred. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work on high-accuracy text generation",
            "review": "\n#### Summary\n\nThis paper introduces a new optimization method for text generation that improves upon directly optimizing MLE. It frames text generation learning as an off-policy reinforcement learning (RL) problem using demonstrations (the training examples). The authors also discuss why off-policy learning is more suitable for text generation than on-policy learning. After simplifications and approximations, the proposed optimization objective comes down to a form that is similar to MLE, but upweighs training examples that are more likely under the learned policy $\\pi_\\theta$ (\"easy\" examples) and having higher estimated rewards (considering the future).\n\nExperiments on a variety of text generation tasks and models show that the proposed learning method, GOLD, is able to consistently outperform direct MLE optimization and on-policy learning. The authors also discuss the advantages of GOLD, including an expected higher precision (and lower recall), less performance drop on longer prediction, robustness to decoding algorithms, etc.\n\n#### Strength\n\n- High-accuracy text generation is a valuable research direction, given the well known problems in modern text generation models such as repetitions and hallucinations as mentioned in the paper.\n- The proposed learning method is soundly formulated in the manner of off-policy RL. It also makes intuitive sense as upweighing certain training samples based on MLE.\n- The experiments justified the general performance improvement the specific advantages brought by the proposed method.\n\n#### Weakness\n\n- As I see, it's not adequately discussed in the paper the cost of sacrificing recall. Intuitively, sacrificing recall in text generation would cause the model to lose output diversity, such as overly favoring the most common sentences like \"I don't know\" in dialogs. Also, examples are not provided when the proposed method fails to generate good output when other methods can. Although this concern is partly addressed by the good performance of the method in tasks like MT where losing recall can hurt performance for sure, I still feel that more discussions in this line will increase the concreteness.\n\n#### Questions & Suggestions\n\n- Empirically, the proposed learning objective (Eq. 4) has two more factors than MLE: the learned policy likelihood and the estimated reward. It would be interesting to do an ablation study to see how the method performs with each one of them separately, in order to confirm the effect of each factor.\n- In Table 2, you reported results on dev set. Why not reporting the test set (or both)?\n- In Table 5, you stated that \"win\" is the percentage of MLE outputs better than GOLD. That would lead to the conclusion that MLE is better than GOLD since \"win\" is consistently higher than \"lose\". Should it be the other way around?\n- Comparisons to other high-precision text generation methods would be interesting to see (although I understand that most of them are contemporary).\n\n#### Typos\n\n- Section 3.3: \"In addition\" is used repetitively\n- Table 3 caption: \"BLUE\" -> \"BLEU\"\n- Figure 1(a-d) captions: \"zoomed in\" is confusing, since it can mean \"scaled up\". Change to \"clipped\"?\n- Page 7, line 7: missing space after \".\"\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}