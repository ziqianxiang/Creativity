{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper is a variant of the large growing class of Neural ODEs, and adds dependency on a time delay to the baseline, which allows to model a larger class of physical systems, in particular adding the possibility of crossing paths in phase space.\n\nAfter initial evaluation, the paper was on the fence, with 2 reviewers providing favorable reviews, and 2 reviewers recommending rejection. A particular important issue raised was positioning with respect to prior art, [Dupont 2019], with some substantial overlap between the papers; requests of theoretical discussions of the class of studied systems and its properties.\n\nMost of these remarks have been addressed by the authors, in particular positioning and experimental comparisons.\n\nThe AC judged that the paper had been sufficiently improved and recommends acceptance."
    },
    "Reviews": [
        {
            "title": "Using delay differential equations to improve the modeling capabilities of Neural ODEs",
            "review": "The paper builds on the Neural ODE framework by using delay differential equations (DDEs) instead of ordinary differential equations (ODEs). This can help in the modeling of systems with a time delay effect, and overcome many limitations of the Neural ODE framework.\n\nUsing DDEs is a novel technique in machine learning and can complement and build on the framework of Neural ODEs and help model systems with time delay dependencies and overcome some limitations of ODEs.\n\nThe paper was well written with a clear description of the model and theory to support it as well as the algorithm to train it. The experiments are well described though they lacked the model parameters and more description on their significance.\n\nMajor points:\n\n1) Model parameters for the experiments were not listed. This can be especially important for systems with a time delay effect as in Figure 6 and 7. Was the same delay used as in the equation of each system? What happens when the delay is different? \n\n2) While some complex systems were used to demonstrate the potential of the proposed model, as in Figure 7, the abstract claims that the model can successfully model chaotic dynamics yet that was not shown in the experiments. The dynamic regime for these systems with their chosen parameters were not clear together with the effect of time delay on these dynamics. Exploring this more closely can demonstrate the full potential of NDDEs.\n\n3) The authors claim that unlike NODEs, NDDEs can overcome the problem of mutually intersected trajectories in phase space. They point to the experiment in Figure 6 as evidence of that. I feel there needs to be more discussion on this point both theoretically, proving that DDEs can indeed model intersections in phase space, and with a clearer demonstration experimentally.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea, nice illustrations, preliminary expts, not much theoretical support",
            "review": "-- The motivation for resorting to DDEs over ODEs is laid out reasonably well, although I think the intuition behind why NODEs cannot learn certain functions is lacking (specifically, that trajectories cannot cross due to the lack of \"memory\" provided by the delay).\n\n-- Figure 2 could be more informative. As it stands, it only indicates that the initial state of a NDDE is specified by a function over an interval, rather than a single vector, which is all that is being used.\n\n-- Algorithm 1 is challenging to understand without some knowledge of how DDEs are solved.\n\n-- In the experiments, NDDEs are only compared against NODEs, but it seems like ANODEs are the real competition. Notably in the image experiments performed in the ANODE paper they report substantially higher accuracies than NDDEs.\n\n-- This is mentioned in the discussion section, but I think the paper would benefit from an experiment involving a real-world dataset with some sort of delay component, ideally in which NDDEs outperform NODEs/ANODEs. The existing synthetic experiments are designed to show the strengths of NDDEs, while the image classification tasks don't substantially differentiate them from NODEs.\n\n-- The number of function evaluations is not reported anywhere, which seems like a significant statistic when NDDEs are substantially more complex than NODEs.\n\n-- Figure 7 could probably be improved by reducing the number of different parameter values reported (e.g. 3 columns instead of 6). The description also lists 5 parameter values but there are 6 columns.\n\n-- The discussion paragraph \"Extensions of the NDDEs\" suggests a generalization to more complex delays, but all of the models in this paper ensure that the delayed value is always the constant initial value. Also, modeling the initial function as an ODE is mentioned, and I wonder whether the implication is that this would involve learning the initial function as a NODE itself. Either way, it's not obvious how either of these would yield better representations.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting  and promising extension of NODEs that leverages Delay Differential Equations but would benefit from further study",
            "review": "This paper contributes to a recent line of researches linked to Neural Ordinary Differential Equations (Chen et al. 2018) and variants. \nThis new family of deep neural networks (NODE) generalize ideas from Residual Networks and consider continuous dynamics of hidden units using an Ordinary Differential Equation (ODE) specified by a neural network. Computing in such networks consists in taking x=h(0) as input and define the output layer h(T) to be the solution of an ODE initial value problem at time T. The main ingredient which makes learning possible is the backpropagation algorithm through the last layer ODE solver, that relies on the adjoint sensitivity method (Pontryagin et al., 1962).\n\nThis work considers Delay Differential Equations instead of ODE, which allows to implement more complex dynamics and thus achieve estimation of more complex functions. The contributions of the paper are the following:\n(1)\ta novel deep continuous -time deep neural network defined by the following equations\nFor a given delay \\tau (never discussed), Neural Delay Differential equations define dynamics of the form:\ndh_t\\dt = f(h_t,h_{t-\\tau}, t ;w),  t \\geq 0,\nh(t) = \\phi(t), t \\leq 0\n        In this way the network can take into account a former hidden layer.\n(2)\tthe derivation of the adjoint sensitivity method for delayed equations, which seems relatively straightforward and which is backed by two proofs in the appendix.\n(3)\tA novel learning algorithm that implements the forward for h and the backward pass for h,lambda (the augmented variable) and dL/dw (L is the loss) by a piece-wise ODE solver, dealing with the different delayed states.\n(4)\tExperiments on 2D toydatasets and on classic differential equations such as Mackey-Glass are shown to exhibit the ability of DDE to cope with those dynamics, in constrast to ODE.\n(5)\tExperiments on image datasets\n(6)\t\nThe strong point of this paper is of course the proposal of the new variant of NODE, which comes with a novel algorithm and overcomes some limitations of NODE. I was interested by the examples of functions not covered by NODE and covered by NDDEand easily convinced by that. \n\nThere are some weak points in this work. Some of them could be easily improved I think, others call for further  work.\nRelatively Minor points\n-\tThe paper is not self-content and does not a very good job in explaining the context of Neural ODE. I suggest that you more clearly describe NODE, as a hypothesis class and then as a learning algorithm.\n-\tThe two uses of NDDE concern modeling time-series  or implementing a classification/regression function. Can you each time precise inputs/outputs, samples you use in training. Just for clarity, the reader guesses of course but its rather slows down the reading this absence of notation and formalization ?\nMajor points and questions to the authors:\n-\tMore importantly, as a novel algorithm is introduced, I expect to see a complexity in time analysis. As for NODE I understand that the complexity in space is favorable. \n-\tAn associated question is the role of tau, the delay. How is it chosen ? I imagine that if tau converges towards 0 we find the behaviour of NODE ? what was its value in the two real-world experiments? Was it selected by cross-validation ?  Is it too computationally heavy to consider multiple delays ? \n-\tEventually, I have doubts and questions about the real opportunities for using NDDE in the two real word datasets: yes the divergence of NODE poses problems : in these experiments no doubt that NDDE has a more stable empirical behavior, with a small variance. However, in fine, the average performance is nearly of the same order, a significant difference only on MNIST but with a very bad score considering this is an easy problem. I won’t qualify these results as exceptional and I kindly engage the authors to remain modest.\n-\tNow can we cope without delays by introducing new states in the way augmented  Neural ODE are built (Dupont et al.) ?\n-\tFor what kinds of classification problems NDDE could be more interesting than NODE – The question is certainly difficult.\n\nIn conclusion, the paper tackles a very interesting topic and I had pleasure to discover it (with the help of literature). Although the idea to consider DDE instead of ODE is incremental, its relevant and novel, and certainly promising and worth to be explored because it addresses some of the issues of NODE.\nHowever I stay on my hunger on different points, there are pending questions that require to be answered before acceptance.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting model class, improper characterization and comparison to prior work",
            "review": "This paper presents a modeling class of parameterized first-order\ndifferential equations that are conditional on some delayed\npast states. This is a promising direction for the community to go\nto push past current ODE modeling limitations, but I recommend for\nrejection in the current form due to improper characterization\nand evaluation with respect to prior work (more details below).\n\n# Strengths\nTo the best of my knowledge this is a novel modeling class\nfor neural ODEs. Delay ODEs are well-studied in contexts\noutside of the machine learning community that would be\nuseful to bring into it.\n\nOn the representation capacity, delay ODEs are able to\novercome one instance of an intersecting path issue that\nthe usual first-order neural ODEs are unable to represent.\n\nThe application to population dynamics and Mackey-Glass systems\nthat are naturally represented an delay differential equations\nseems reasonable and well-motivated that baseline methods may\nnot be able to capture this modeling class.\n\n# Weaknesses\nThe strongest weaknesses of this paper are the characterization\nand evaluation in comparison to [Dupont 2019], which points\nout some of the same modeling issues and proposes an\nalternative way to fix them.\nEven though this paper cites [Dupont2019], it omits that they consider\nidentical experimental settings and does not include their results:\n\n  1. Figure 4 and 5 are almost identical to figures in [Dupont2019]\n\n  2. Table 1 presents results in MNIST and CIFAR10 and is identical\n     to the setting in [Dupont2019]. The neural ODE baselines\n     are almost consistent with the ones reported in [Dupont2019], but\n     [Dupont2019] outperforms the method presented here, and are\n     omitted from the table. [Dupont2019] additionally\n     considers the SVHN dataset.\n\n# Other comments and questions\nIs the adjoint method for the NDDEs very different than the adjoint\nmethod for NODEs? It could be insightful to intuitively explain\nthis.\n\nThe extension to more general delay differential equations seems\npromising and working out the details in this general case in\naddition to the case of having a single delay would add to the paper's\ntheory and methodological contributions.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}