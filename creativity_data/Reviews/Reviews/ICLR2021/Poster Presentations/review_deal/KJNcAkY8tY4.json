{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies whether neural networks with different architectures, especially different width and depth, learn similar representations. All reviewers agree that the investigations are thorough and the experimental discoveries are convincing and well explained. Good work. I recommend accept."
    },
    "Reviews": [
        {
            "title": "This paper studies the effects of width and depth on neural network representation.",
            "review": "In this paper，the author studies the effects of width and depth on neural network representation.\n\n\nIn this paper，the author studies the effects of width and depth on neural network representation. This paper conducts lots of experiments on CIFAR-10, CIFAR-100 and ImageNet with different network architectures and apply the CKA to measure the similarity between representation of each layer. As a result, they find a characteristic block structure in the hidden representations of larger capacity models which is also dependent to the size of dataset. This work has the following advantages:\n1、\tWell-arranged and detailed experiments which strongly support the final conclusion.\n2、\tExquisite figures that well displays the experiment results.\n3、\tThe concept of “block structure” in ResNet is novel. And all of the experiments and analysis illustrate there really exists blocks with similar representation in overoptimization models. And this phenomenon can guide researchers to design networks well.  \nHowever, there are some disadvantages or doubts in my opinion:\n1、\tThis paper lacks of further explanations about the CKA or HSIC tools. I can’t fully understand how the similarities between representations of each layer are measured.\n2、I wonder if the block structure arises dependent to the residual blocks. I want to see more experiments with other network architectures.\n3、I think the analysis of effects of width and depth on neural network representation can well guide researchers to design networks. So，I expect to see an modified network architecture or a method to balance the network size and accuracy . However, this paper is just about theoretical analysis based on experiment phenomenon. Thus, I think this paper is lack of some innovation.\n\n4. Some previous related works is better to be appreciated: \n\nC. L. Philip Chen et al Broad Learning System: An Effective and Efficient Incremental Learning System Without the Need for Deep Architecture，IEEE Transactions on Neural Networks and Learning Systems，2017 \nC.L.P. Chen, Z. Liu, and S. Feng, Universal approximation capability of broad learning system and its structural variations， IEEE transactions on neural networks and learning systems 30 (4), pp. 1191-1204.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper but the choice of similarity function is not clear",
            "review": "The paper aims to get a better understanding of differences between wide and deep neural networks through an empirical evaluation. It does it through a similarity analysis, and an analysis of which errors wide and deep residual networks do.\n\nThe paper is well written and should not be difficult to reproduce. Most of the questions I got while reading where addressed in the paper, in an extensive evaluation. The most interesting and somewhat surprising finding is that even though two networks with different number of parameters and layers but with the same accuracy make very different mistakes, and there is a pattern to it. The weakest part is the similarity analysis, which does not seem to reveal much new. It has already been known that deep networks have layers which do not contribute significantly to final accuracy, can be proved or even forced to learn more useful representation. Would be interesting to apply the similarity analysis in a network with reinitialized layers. Also, the choice of specific similarity function and it's benefits/drawbacks are not discussed. Could the same analysis be achieved with a simpler similarity function? Is it computationally efficient to compute?\n\nOverall, there are definitely valuable contributions in the paper, so I propose lower score only due to the unclear choice of similarity function, as described above.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "engaging approach that opens more questions",
            "review": "This paper explores characteristics of resnet networks that arise with different capacity.  In particular, a blockwise structure of behavioral similarity arises when the network has large capacity, regardless of whether that capacity was introduced with greater depth or with greater width (similarity is measured using CKA, which compares the pattern of pairwise comparisons among elements in a minibatch between two different sets of layers/activations).\n\nThis is an interesting method and characterization of resnet behavior, with thorough experiments that tie together different aspects of the approach:  CKA is used to show a type of blockwise similarity, much of which is subsequently explained, and related experimentally to classification performance using linear probes through the layers.  Finally, it opens questions for future research, such as: despite qualitatively similar layerwise behavior, why does larger depth vs larger width produce different types of classification errors?  Might any of the differences in patterns that develop for deep vs wide in CKA relate to the spatial scale of regions that most influence each layer (the effective receptive fields)?\n\nOverall this looks like a rather thorough investigation.  I have a few questions of things I don't quite know how to interpret, below, which I feel could be made clearer.\n\n\n\nQuestions:\n\n- Fig 1:  In the last two plots of each of the top two rows, it appears the block starts to develop from the \"middle\" out, in two sections, as opposed to from the first or last layer in a section.  Is there a reason for this and what does it correspond to?\n\n- Sec 5.1:  Is there an intuitive description of the first principal component in R^n, as opposed to R^p1 --- it seems easier to think of the latter, being the activation vector that captures the most variance over n datapoints, but the one used here is from the transpose, with the vector in R^n (i.e. coefficients of data samples):  What does this mean and how does it relate to the block structure and/or residual connections?\n- Given that much of the blockwise structure is due to the first principal component, can anything be said about the structures that arise after removing it?  How do these behave when increasing depth vs width and do these relate at all to the different classification results?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting exploration of how depth and width of deep neural networks affect the learned representations",
            "review": "This paper explores core questions related to how depth and width of deep neural networks affect the learned representations. The authors attempt to develop an empirical understanding of the behavior of neural network architectures after training on real-world data. In my humble opinion, the paper is very clearly written, presenting at the beginning of each section the scientific question they try to answer. The questions that the authors try to answer are pertinent, the conclusions are consistent with the results obtained, and statistical tests are employed to check the existence of statistically significant differences between the different approaches tested. I have several questions that the authors should take more as comments or suggestions than criticisms:\n\n- The paper focuses on the family of ResNet models. Do the authors have solid reasons to believe that their findings generalize to other neural models (other ConvNets, recurrent, generative,...) and problems (regression, dense prediction,...?\n\n- Apart from the fact that \"some layers that make up the block structure can be removed with minimal performance loss\", what are the main practical consequences of the findings of this research? \n\n- The authors state that \"on ImageNet, [...] wide networks perform slightly better on classes reflecting scenes, whereas deep networks are slightly more accurate on consumer goods\". Do the authors have any intuition about the reason for this behavior? What more general conclusions could be extracted about the performance of deep vs wide neural networks? \n\n- Related to the previous point, in the last years, the recommendation (for instance, in [1]) was to go as deep as possible, and not care too much about wide neural networks. However, if I'm not mistaken, according to the results obtained by the authors \"the average accuracy of these groups [deep and wide models] is statistically indistinguishable\". Could the authors comment a bit more on this? How all these experimental results corroborate, complement or refute some of the already existing theoretical results regarding depth and width in neural nets?\n[1] Eldan, Ronen, and Ohad Shamir. \"The power of depth for feedforward neural networks.\" Conference on learning theory. 2016.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}