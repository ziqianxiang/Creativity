{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a set of synthetic tasks to study and discover the inductive biases of seq2seq models.\n\nAuthors did a great job in convincing all the reviewers except R5 in their rebuttal. I do not find any serious concerns from R5's review. I personally think this is a very useful analysis paper."
    },
    "Reviews": [
        {
            "title": "Why is this important?",
            "review": "The paper introduces a series of new datasets and task and investigates the inductive bias of seq2seq models. For each dataset, (at least) two hidden hypothesis could explain the data. The tasks investigated are count-vs-memorization, add-or-multiply, hierarchical-or-linear, composition-or-memorization. The datasets consists of one sample with varying length (amount of input/output pairs), which is denoted as description length. The models are evaluated on accuracy and a logloss. An LSTM, CNN, and Transformer are all trained on these datasets. Multiple seeds are used for significance testing. The results suggests that LSTM is better at counting when provided with a longer sequence, while the CNN and Transformer memorizes the data, but are better at handling hierarchical data. What this paper excels at is a thorough description of their experimental section and their approach to design datasets specifically for testing inductive bias, which I have not previously seen and must thus assume is a novel contribution.\n \nHowever, I lean to reject this paper for the following reasons\n- The paper tries to fit into the emerging field of formal language datasets for evaluating the capacity of deep learning methods. However, they do not build on any of the recent papers in the field. A new dataset, especially a synthetic one, should be well motivated by shortcomings of previous datasets and tasks in the field. I find the motivation and related works section lacking in that sense.\n- We already know that LSTMs can count https://arxiv.org/abs/1906.03648 and that transformer cannot https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00306\n- It is not clear to me why these results are important? Who will benefit from this analysis? Why are the current AnBnCn and DYCK languages that formal language people work with insufficient?\n- LSTMs do not have the capacity to perform multiplication. I donâ€™t know why your results suggest otherwise. You would need to incorporate special units that can handle multiplications in the LSTM, such as https://arxiv.org/abs/2001.05016\n\nUpdate\n\nFirst I'd like to thank the authors for their detailed rebuttal. I have upgraded my recommendation from 3 to 4. As mentioned in my review I believe this approach is interesting. However, as pointed by reviewer2, the experimental section lacks completeness. I think this experimental section would be suitable for a workshop, but not a conference. I am excited to hear you are considering to use this method as an inspiration for real problems. I'd like to see the paper resubmitted when you have obtained such results.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice work on quantifying the inductive biases: Super interesting but no surprising and practical findings",
            "review": "The Topic of this paper, to investigate the inductive biases of different neural network architectures, is super interesting. They do this by considering the extreme case of when there is only one training example and define a set of biases based on the type of solutions the model converge to when all of the options are equally optimal based on the given training example. \n\nThe authors claim that minimum description length is a sensitive measure of inductive biases. As far as In understood the idea is that a solution (rule) can be simple for one learner, while more complicated for the other one, based on their architectural differences. And by measuring the minimum description length of data generate by a certain rule under a specific learner, we can tell how simple the rule is for the learner.  I am a bit puzzled by the way this claim is phrased (I don't have anything against the main argument though). Isn't minimum description length itself an inductive bias that is probably applied on all these models? The way I see this is that a model that achieves lower description length is finding a simpler solution and the simplicity of the solution can be characterised by the different solution preferences that is defined in this paper (e.g. memoization vs counting). I would really appreciate if the authors provide a clear distinction between different sources of the inductive biases and their interactions in their experiments.\n\nA nice point about the paper is that in their experiments they evaluate the sensitivity of the preferences of different architectures to their hyper-parameters and it seems these preferences are consistent for most cases, but not surprisingly they observe that hyper-parameters like dropout rate can affects the generalisation behaviour of the models. That would be much nicer, if there were a bit more connections in the paper, about how these biases in the extreme case of only one training example cascade and affect the performance of the models when trained on larger scale datasets. For example, in a simple and still abstract case of gradually increasing the number of training examples, how the behaviour of these models change? Or in case of the point about few shot learning that is mentioned in the conclusion, I think it would be nice to have some experiments to show that the models will have these inductive biases and preferences even after being pre-trained on large datasets.  \n\nIn the end I agree with the authors that\n`Overall, our study shows that the inductive biases is more complicated than it seemed in these prior works and a more careful analysis is crucial.`\n\nAnd while I think quantifying inductive biases is a super interesting, challenging and important problem to address, this paper needs a bit more work to be more impactful in terms of either providing us with practical insights about the inductive biases of different architectures, or providing us with benchmark or tools to be able to fairly compare the inductive biases of different models, or just by more clearly identifying the main challenges for doing this in a beneficial way (e.g., because of the complicated interactions between different sources of inductive biases and how this all changes when the amount of data and the capacity of the models increase/decrease).  \n\n\n***\nPost Rebuttal: I like this paper and would vote for it to get accepted on the merits of: (1) Their finding about how MDL can be an indicator of inductive biases of the models; (2) introducing an experimental framework for studying inductive biases of the models. I'd also like to appreciate the authors' efforts to address reviewers concerns in the rebuttal. I agree with reviewer #5, that the paper can be better contextualised in the related research area but I think the paper is improved from this aspect a little bit during rebuttal and this is something that in general can potentially be fixed for the camera ready version. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Insightful analysis paper of seq2seq architecture biases",
            "review": "The paper studies inductive biases that are encoded in three main seq2seq architectures: LSTMs, CNNs, Transformers. Using one existing (fraction of perfect agreement) and one proposed metrics based on description length, and four dichotomy-like tasks, the authors show that, among other things, CNNs and Transformers are strongly biased to memorize training data, while LSTMs behave more nuancedly depending on input length and dropout. Unlike the first metric, the proposed metric takes values in a wider and more fine-grained range; the results are correlated for both of them. I also appreciate the attention to hyperparameter tuning and investigation of their effects in experiments. In general, the manuscript is well written and apart from a few minor questions can be accepted in its present form.\n\nQuestions:\n- SGD was found to often generalize better than its adaptive variants, like Adam (e.g. Wilson et al. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems. 2017), yet in your experiments there seems to be an opposite effect of changing the optimizer (Appendix C). Could you comment on why this is the case?\n- Regarding the tendency of large dropouts to hurt memorization: to what extend does this help the peer bias in a task? It seems that hindering memorization seem to cause a complementary increase in count or add-mul ability (Table 6). Is there a value for dropout (or a combination with other hyperparameters) when Transformers would start showing a counting bias?\n\nMinor:\n- please use alphabetic literature sorting\n\nUPDATE: I thank the authors for their detailed replies and running additional experiments. This resolves my questions and I'd keep my recommendation to accept the paper.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting work",
            "review": "This work proposes to use description length (DL) to measure the seq2seq model's inductive bias. \n\nStrength: It is clearly shown that DL gives more smooth measurements than FPA. Also, the experiments are principled, and well designed. And the conclusion are interesting and clear. I do believe that this framework can be utilized by future work in this direction to get a better understanding of seq2seq models.\n\nFinally, the paper is well written, I am able to get the author's points.\n\nWeakness: My major concern is the scale or completeness of the experiments. The authors concentrate on a training set of a few samples, which is far away from the usual large-data setting for LMs. \n\nMoreover, the training data concerned only exhibit one or two kinds of bias, while in real data, there are usually various kinds of biases. I'm interested to see what the model will pick, when facing different kinds of structures in the data. For example, will the model favor easy rules than more complicated rules? \n\nIn addition to the different variants of seq2seq models, I'm also interested to see whether encoder-decoder model have different biases with pure LMs (only decoder, e.g., GPT-2).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}