{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors present a simple modification of existing byzantine resistant techniques for training in the presence of worst case failures/attacks. The paper studies two of the strongest attacks to date, that no other method, till now, has been able to address. The novelty is significant for the related byzantine ML literature. The authors further do a fantastic job in their experiments and sharing reproducible code. Some weak aspects of theory are in fact attributed to what the metrics and guarantees that the related literature studies. The novelty of this paper does not lie so much in the theory contribution, but more so on their experiments and presented intuition. I believe this will be a paper that people will build up on and the ideas presented here are of solid value and importane."
    },
    "Reviews": [
        {
            "title": "Technical contribution seems to be lacking",
            "review": "I am having trouble identifying the contribution of this paper. A slight distributed tweak on the completely standard approach of running SGD with momentum is proposed as a defense to Byzantine attacks. A number of defenses augmented with this approach is studied experimentally against two recent attacks. There are no clear guarantees of the proposed mechanism that I could find as the paper seems to be focused exclusively on experimental results. \n\nThe experimental results by themselves, while they have a certain merit, seem of little lasting value -- there is only two (rather ad hoc) attacks considered, which I didn't find particularly natural or important otherwise. Overall, combined with the fact the Byzantine setting itself is already a stretch in terms of realism, this paper doesn't seem to have much lasting value. I would suggest identifying and studying broader classes of attacks/defenses of which presented ones are special cases and giving at least some guarantees in terms of what the proposed approach provides. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Extensive experiments with some theoretical analysis",
            "review": "# Contributions\n\nThis paper presents a novel method to tackle the Byzantine faults problem. By using a local momentum, this method can be extended to all other existing robust algorithms. The authors also provide some theoretical analysis of the effect of their algorithm. Finally, comprehensive experiments are shown and analyzed.\n\n# Strong points\n\n1. The local momentum can be easily combined with other existing robust algorithms, which makes it more practical.\n\n3. This paper has comprehensive experiments that compare the combination of different attacks, defenses and datasets.\n\n4. The authors provide well-written code to reproduce all experiments.\n\n# Weak points\n\n1. The experiments can be improved by running on larger datasets and larger models. Higher dimensionality may also impact the performance. But it is unrealistic to run thousands of large scale experiment, so this weak point is understandable.\n\n2. The assumption is too strong. Namely, each worker can sample from the global dataset, and the norm of \"real\" gradients are bounded.\n\n3. In the theoretical analysis, the variance-norm ration is defined as $r_t^{(s)} = \\frac{\\mathbb {E} \\| \\mathcal{G} - \\mathbb {E} \\mathcal{G} \\|^2}{\\| \\mathbb {E} \\mathcal{G} \\|^2}$. However, when at the (local) optimal, $\\lambda_t^2 = \\| \\mathbb {E} \\mathcal{G} \\|^2 = \\mathbf 0$. Maybe define $r_t^{s}$ as $r_t^{(s)} = \\frac{\\| \\mathbb {E} \\mathcal{G} \\|^2}{\\mathbb {E} \\| \\mathcal{G} - \\mathbb {E} \\mathcal{G} \\|^2}$ can resolve this problem and remove the requirement that $\\lambda_t > 0$.\n\n4. When stating the definition of $\\nabla Q$ being Lipschitz, I think it should be $\\| \\mathbb {E} \\mathcal{G}_t - \\mathbb {E} \\mathcal{G}_u \\| \\leq l^2 \\| \\theta_t - \\theta_u \\|^2$. But it doesn't affect the final result.\n\n# Recommendation\nAccept. This paper proposes a new way to solve the Byzantine faults problem, along with some theoretical analysis, extensive experiments and well-written code.\n\n# Optional improvements\n\n1. Page 3, the first sentence of Adversarial Model paragraph, \"... as the minimization of ...\", do you mean maximization?\n\n2. Definition 1, $(\\alpha, f) \\in [0, .... \\frac{\\pi}{2} [ \\times [0, ..., n]$, is it a typo?\n\n# Update\nThough the theoretical analysis is a bit weak, I think the experiments are quite good. The code can also run without any issue, which is a significant contribution in my opinion.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper is well written but need more formal and statistical analysis",
            "review": "The paper's method is quite simple and it argues that the latest distributed stochastic gradient descent (SGD) state-of-the- based on the Byzantine model can be tackled using momentum-based versions of (SGD). There is a slight issue with the Nesterov variant that the authors are using. I don't know they have used the formulation to calculate gradients at $\\theta_t-\\alpha v_t$ instead of calculating gradients at $\\theta_t-\\mu v_t$, where $\\mu$ is called momentum. Other than that they have used the aggregating functions which are previously studied and known to $(\\alpha, f)$-resilient. There is not much contribution by the authors in this area. Although they studied these aggregation rules under recently presented attacks. \nIn supplementary material, authors show that computing momentum-based gradient at workers under some assumptions they can keep the variance-norm ratio low but they already have discussed that this might be an issue as standard deviation can be negative. More insight and analysis are required and I am not sure how it may behave other attacks that are not studied in this paper. The results are quite detailed but due to the huge number of combinations, it is difficult to summarize all settings, and the numbers already stated are not in favor. When half of the machines are Byzantine 20\\% accuracy is recovered in 49.25\\% cases which is not a favorable number. The number of stats for results are missing like average recovered accuracy, median and basic analysis charts could be more useful than adding charts of different combinations of possible experiments.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Hard to get in the way of something with momentum",
            "review": "Summary: \nThe paper describes an approach to counteract Byzantine attacks for distributed stochastic gradient descent by using the momentum of the gradient computed at the workers, which relies only on memory of the previous momentum. This seems to thwart current attacks in the majority of scenarios tested. The theoretical analysis seems appropriate. The empirical results are accompanied by precise details and should facilitate reproducibility (given a week of computation time). \n\nStrengths:\nThe paper is well organized. The proposed approach addresses known attacks. The results are done in a rigorous and reproducible manner. Related work is highlighted.\n\nWeaknesses:\nThe text in the introduction is a bit overreaching. Perhaps, \"main driving force behind the successes of machine learning\" replaced by \"main optimization algorithm used throughout machine learning\". Also, is not clear that data that are not \"well-sanitized\" and the existence of \"software bugs\" could be considered colluding adversaries. These situations certainly do not constitute omniscient adversaries.  Their inclusion does not seem within the scenarios considered.  In this vein, I don't find much realism of the addressed attack scenarios. \n\nThe nature of the failure mechanism for cases when the defense is not successful could be made more clear or investigated more.\n\nThe related and future work section were helpful, but the text in that section are too terse, especially when new concepts are introduced. More text describing these concepts and relating them to the situations addressed by the paper would be helpful. \n\nConclusion:\nI think the paper is  well written contribution to the literature on Byzantine attacks for stochastic gradient descent.  I think it will be significant to researchers in the area. However, it is outside my expertise and I found interpreting the contribution difficult. \n\nOther suggestions:\nFigure 5 is difficult to understand without a contrast of what it would be without the proposed momentum at the workers.  \t\n\nSection 5 could be expanded. It is not clear why the \"suspicious-based\" fault tolerance is considered a more appropriate defense. Could the proposed momentum at worker approach make it easier to detect attacks (rather than defend)?\n\nThe detailed results of learning curves in the Appendix are easy to grasp, but the number of figures seems excessive versus summarizing the loss at a certain iteration or the number of iterations to a certain performance level.\n\nMinor:\nThe font seems different as compared to other submissions.  \n\nIn the abstract, \"with seeds 1 to 5\" -> \"with specified seeds (1 to 5)\"\n\nIn section 3.1 the definition and then redefinition in equation 2 is a bit confusing. Why not differentiate the notation?\n\nReferences to books titles like Bottou's should be capitalized. \n\nPage 14  \"Basically, and\" -> \"Basically,\" ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}