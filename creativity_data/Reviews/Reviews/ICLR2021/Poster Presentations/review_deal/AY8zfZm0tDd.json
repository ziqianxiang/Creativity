{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper develops an effective model-free algorithm that achieves high sample efficiency. The empirical performance is appealing, which is comparable to model-based policy optimization and significantly outperforms SAC. The paper is well-written, and contains rigorous ablation studies. Weakness: the theoretical analysis is Section 3.1 is not thorough yet,  and it would be helpful to include more numerical comparisons with the Maxmin approach by Lan et al. "
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "[Summary]\n\nThis paper proposes Randomized Ensembled Double Q-Learning (REDQ), a new model-free RL algorithm that aims to improve the sample efficiency over existing model-free methods. Experiments on Mujoco show that REDQ achieves better sample efficiency than popular model-free methods such as SAC and is comparable with model-based methods such as MBPO. The paper further provides extensive ablation studies that justify the necessity of the algorithmic components in REDQ and show that improved Q estimation bias may have been the key reason for the performance gain. The paper also provides some theoretical analysis of the Q estimation bias.\n\n[Pros]\n\n--- The empirical performance of REDQ seems rather strong: significantly better than SAC and can match or exceed MBPO depending on the environment.\n\n--- Section 3 provides a rather convincing explanation of the performance gain through the perspective of the Q estimation bias: REDQ manages to achieve a low bias in terms of both average and std across (s,a) pairs. In comparison SAC fails on both grounds, while AVG (naive ensemble without the in-target minimization) achieves a low std but still a rather high average. The theoretical analysis (Theorem 1) also helped improve my understanding on this front by illustrating how the factors (M, N) could affect the bias. \n\n--- The ablation study in Section 4 is very detailed and answers a lot of questions I had (e.g. importance of M, N) and I liked it a lot. \n\n--- Overall the paper is quite clearly written and conveys the message quite clearly.\n\n[Cons, and comments]\n\n--- My main concern is that the most similar approach Maxmin (Lan et al. 2020) which the authors cited multiple times was not comprehensively tested in the experiments. More concretely, Maxmin was not presented in the main plots (Figure 1 and 2) and only showed up (and in a modified fashion) as an ablation point in Figure 3 where it seemed like its performance was pretty bad. From the original Maxmin paper it seemed like they did not try Mujoco; is it the case that Maxmin did not really scale up to Mujoco? \n\nSpecifically, it is a bit disturbing that Maxmin did not appear in Figure 2 which studied the Q estimation bias. Compared with the baselines in that figure (SAC20, AVG), Maxmin sounds much better in terms of reducing the Q estimation error (it also has both ensemble and in-target minimization). I am quite curious how Maxmin does on the bias, and specifically if it does well on the bias but performs worse than REDQ, what is really going on.\n\n\n--- The key algorithmic novelty of REDQ seems to be the combination of two existing ideas: an ensemble of N=10 networks, as well as in-target minimization over only M=2 randomly sampled networks from the ensemble. I am not entirely sure whether this could be considered as of enough novelty in this area (I am not super familiar with the literature here, so authors / other reviewers please feel free to correct me if I am wrong.) Also, given that the ablation studies showed that having both parts (large N, small M) is indeed important, at this moment I am only thinking of this as a weak concern.\n\n------\nAfter rebuttal: Thank the authors for their efforts in the rebuttal and revision. The authors' response to the Maxmin question (along with the revised discussions) sound convincing to me. I would like to keep my original evaluation and would lean towards acceptance for this paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good idea and well written",
            "review": "Summary\nThe paper proposes three techniques that altogether greatly improves the performance of soft actor-critic (SAC), resulting in a new algorithm called REDQ. (1) A higher update-to-data ratio, which speeds up the critic update. (2) Using the average ensemble Q for the policy gradient, therefore reducing its variance. (3) Taking the min of a small subset of the ensemble Qs to compute the target Q, therefore reducing the Q bias. The paper also performs extensive ablation studies to prove the importance of each technique.\n\nRecommendation\nOverall I think the paper deserves an acceptance. The proposed solution is simple, effective, and justified.\n\nStrengths\n1. REDQ is simple and effective. Implementation requires little change to the backbone of SAC. The authors also provide example code. Fig 1 shows clear advantage over SAC.\n2. Ablation studies are rigorous: e.g. Fig 3 even studies fractional M = 2.5, when M = 3 is clearly under-performing.\n3. The paper is very well written. In particular, the most important algorithm block and experimental results are shown early, leading to a smooth reading experience. \n\nWeaknesses\n1. The paper analyzes the normalized standard deviation of Q bias, but in fact the gradient of Q against actions is more important (line 12, Alg 1). It would be great to see analysis on the gradient as well.\n\n2. Some (minor) missing studies. \n(a). I am surprised that “Weighted” performs much worse than REDQ  on Ant (Fig 3g). The authors suggest overfitting as a potential cause. But I am curious whether exploration is the real issue, and whether the same happens on other envs.\n(b). Algorithm 1 suggests training Q G times before training the policy once. Does training the policy more frequently help? It makes sense since the avg Q has lower variance.\n(c). Why not train up to 1, 3, 10 million steps as in the SAC paper (Fig 1)? Especially for Humanoid, The performance isn’t near 6,000 as reached in the SAC paper.\n\nOther feedbacks\nMujoco envs are rather deterministic and the noise comes from the policy itself. Have you considered other more noisy envs? The ensemble could have a larger impact there.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review 1",
            "review": "Summary: This paper proposes a new model-free algorithm called Randomized Ensemble Double Q Learning (REDQ) for the optimal control problem. Also, this paper empirically shows that the performance of REDQ is not worse than the model-based algorithm, MBPO. \n\nComments: \nPro. \n1. Empirical studies significantly show that REDQ takes less environment interactions but achieves much higher average return compared to SAC algorithm. Also, the performance of REDQ is even better than MBPO in the Hopper problem and Humanoid problem.\n2. The theoretical analysis introduces the relation among two hyper-parameters (M, N) and the expected random approximation error. \n\nCon. \n1. In Section 3.1 Theoretical Analysis, the theoretical result is not complete enough. The Maxmin Q-Learning paper, Lan et al. (2020), also proves that Maxmin Q-learning algorithm has a vanishing approximation variance (with N tends to infinity). This result is vaguely mentioned below Theorem 1 but there is no rigorously proof provided. \n2. The authors claim that the REDQ algorithm is at least no worse than MBPO. This statement is based on the empirical studies or intuitive explanations rather than theoretical analysis. It will be better to provide more comprehensive analysis of the algorithm. \n\n%--------------------------------%\nI thank the authors for clarifying my questions and concerns. The authors have included further theoretical developments in the revision, and they look satisfactory to me. Overall, I tend to accept this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A variant of double Q-learning, with randomized ensembling",
            "review": "This work proposes a modification for double Q-learning, termed as randomized ensembled double Q-learning (REDQ). REDQ maintains $N$ different Q functions, and for each update, the target value is a minimization over $M$ randomly chosen Q functions, where $1 \\le M \\le N$. In addition, REDQ adopts a high update-to-data ratio to improve the sample efficiency. Empirical results show that the proposed method outperforms state-of-the-art model-based algorithms in certain tasks with continuous action space.\n\nOverall, I think this is a well written paper. \n\nPros:\n\n- The algorithmic idea of REDQ is quite intuitive and reasonable. In particular, setting $M$ and $N$ separately allows for more flexibility (compared with double Q and other variants): by increasing $M$, one can achieve a smooth transition from over-estimation of value functions to under-estimation (as validated by a simple theoretical analysis). Moreover, this idea is quite general and can be easily plugged into many existing off-policy model-free algorithms.\n\n- The numerical experiments are quite extensive. The results convincingly show that the proposed REDQ algorithm achieves a minor underestimation bias with low standard deviation, leading to better overall performance. In addition, REDQ is also computationally efficient.\n\nCons:\n\n- The novelty in the algorithmic idea of REDQ is a bit simple and not very significant. \n\n- The theoretical analysis of this work is quite limited, e.g. lacking the (most basic) asymptotic convergence analysis in the tabular case.\n\nA minor comment: it might be better to use \"\\gg\" and \"\\ll\" in latex, instead of \">>\" and \"<<\". ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}