{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes an efficient method to train generative models on multimodal data using a contrastive approach. Usually training such models requires significant training data to be able to learn patterns. The authors propose a variational autoencoder approach that enables multimodal learning of models using a data-efficient approach, and shows the effectiveness of the approach on challenging datasets.\n\nThe authors have mostly addressed the feedback of the reviewers and done some of the necessary changes to the paper (e.g., adding more results and missing related work). They should make sure to address any lingering concerns about the paper, mentioned by the reviewers in their post-rebuttal feedback."
    },
    "Reviews": [
        {
            "title": "Interesting contribution on Multimodal VAEs",
            "review": "Summary\n\nThe paper proposes a contrastive multimodal generative model framework for including unrelated datapoints as well as related datapoints in the learning of the multimodal model. Using such a contrastive formulation, the paper shows that the proposed model outperforms previous work on four desiderata for multimodal generative models. \n\nStrengths\n+ The paper is a novel take on contrastive learning in a probabilistic generative modeling framework, which is interesting\n+ The paper reports extensive experimental results\n\nWeaknesses\n\nWhy not directly use some energy based model with a contrastive loss to learn such multimodal generative models? It feels somewhat odd to both normalize p(X, Y) and also require unaligned examples as supervision. With an energy based model one can do joint generation, conditional generation (coherence) and synergy studies. A discussion on this would be really helpful!\n\nA minor point, one could also consider the case where “r” is latent, that would actually bring up a very relevant case in vision and language where one crawls the web and the vision and language pairs may be apriori related or unrelated (but it is not known if they are or not). Also, intuitively, why is it that using unrelated data makes us believe that the models should be more data efficient? It would be great to give more intuition on that point. \n\nEqn. 2: The way the equation is written down (given the motivation above) it is not clear that the sum in the second term should be inside the log. Why was this choice made, instead of a more standard contrastive learning approach of summing over the distance with the negative examples (which would make the sum outside the log?). Also, paragraph 1 page 4 is very confusing as the model is talked about as generating when training but as I understand there is no generation when training (only computation of likelihoods?). \n\nAppendix B proof: It appears that the proof might be incorrect as the sum over X that we have in Eqn. 4 is not over all X but over a sample of N examples randomly chosen. It then appears incorrect to claim that this is the same as PMI (as opposed to some kind of an approximation?). \n\nBaseline explanation: It is not clear how the proposed approach is applied to the JMVAE objective (Table. 1), which adds additional terms to the regular ELBO. Are those additional terms also used? Clarification on that would be very helpful. \n\nOverall, it would be very useful to get more insights on why the proposed approach is expected to yield a better multimodal generative model based on the four criteria. Is the idea that generative models without explicitly being given negative supervision are optimistic in which datapoints they consider to be related? How does contrastive training affect \"coverage\" metric as discussed in Vedantam et.al. (Triple ELBO)? Essentially, any insight on when one should and should not do contrastive training would be useful to add to the paper.\n\n**POST REBUTTAL**\nI read through the other reviews and the author rebuttal. I thank the authors for addressing all of my concerns in the rebuttal. Overall, this is an interesting contribution in the multimodal VAE space and would make for a good poster at the conference, and I am happy to keep my original rating for the paper. \n\nIn terms of energy based models here is one recent work which comes to mind which might be useful to extend to multimodal settings: \nDu, Yilun, Shuang Li, and Igor Mordatch. 2020. “Compositional Visual Generation and Inference with Energy Based Models.” arXiv [cs.CV], April. https://arxiv.org/abs/2004.06030.\n(note the quality of generations in Fig. 5)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Multimodal VAE using contrastive-style objective.",
            "review": "This work presents a generative model for multimodal learning. The paper maximizes or minimizes the pointwise mutual information between data from two modalities considering a novel random variable relatedness to dictates if data are related or not.  This is realized by casting multimodal learning as max-margin optimization with the contrastive loss for the objective. For the optimization, the paper considers the IWAE estimator. As per the experiments, the paper considers MNIST-SVHN and CUB Image-Captions dataset and perform evaluations across four metrics. Using the experiments, the paper demonstrates that the proposed approach improves multimodal learning, data-efficient learning, and label propagation. \n\n- The paper is well written and easy to follow.\n- The experimental design is praiseworthy as the paper considers different aspects in evaluating multimodal learning via a generative model. The optimization of the proposed approach is also carefully considered. \n- The experimental results across all three tasks demonstrate the efficacy of the proposed approach. \n- One weakness that I find is a lack of comparison against the GAN based models. Can the authors do such a comparison? Or at least discuss why and how GAN based models could be integrated with the current approach? \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " concerns about the derivation and the connection between motivation and the final objective.",
            "review": "The paper tries to minimize the difference of the PMI between related and unrelated pairs of multimodal data but arrives at a very different objective with many approximations. It can be plugged into existing VAE based methods and improve learning performance and data efficiency. My major concern is about the derivation and the connection between motivation and the final objective.\n\nClarity and correctness:\n\nThe presentation of Section 3 should be further clarified. In particular, I wonder about the validness of Hypothesis 3.1. First, it is not clear to me which joint distribution is used in the definition of PMI in the hypothesis? It is not specified and I guess it is the model distribution after training according to the derivation in Appendix A. It is kind of strange because we make a hypothesis about the model after training and optimize the model to reach the hypothesis. \n\nThe derivation is also unclear, according to Eqn. (6), the objective should be the difference between both joint and marginal distributions. I'm not fully convinced by simply ignoring the difference between the marginal ones. I note that the authors mention that the marginal term is not relevant to the relatedness but ignoring it is not optimizing the difference between the PMI, right?\n\nEqn. (6) to Eqn. (2) is also unclear, why we optimize log sum p(negative) instead of optimizing sum log p(negative)? log sum negative is quite like an approximate of log marginal, which seems to contradict the discussion under Eqn. (6). \n\nThe resulting final objective (4) does not obviously connect to the original motivation: maximizing the difference between the PMI values of related and unrelated pairs. I checked Appendix B while the authors claim that it approximately optimizes the joint loglikelihood with an IPM regularization. I have several questions: i) how does this connect to the original motivation? ii) how does the approximation in Eqn. (7) affect training? iii) can we directly optimize Eqn. (7) instead of Eqn. (4)?\n\nThe estimate of the final objective also involves unknown approximation. In particular, the paper does not use CUBO and IWAE properly to form a valid bound of the objective. \n\nWith so many approximations, I can hardly figure out the key factors of the proposed method and say about the technical contribution.\n\nExperiments:\n\nI'm not an expert in multimodal learning. The paper mainly focuses on comparison with VAE based methods. The evaluation metrics are not commonly used but chosen following a related work. The paper claims that the main empirical contribution is its data efficiency, which seems to be verified.\n\n===========\n\n**after rebuttal**\n\nSome of the issues are clarified. I updated my score to 5.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper deals with multimodal VAEs and addresses their problem of requiring lots of \"related\" (i.e., weakly-supervised) samples. To tackle the sample inefficiency, the paper proposes a contrastive objective. ",
            "review": " The paper proposes a contrastive objective that (1) minimizes the distance between \"related\" samples while (2) maximizing the distance between randomly paired samples. Existing multimodal VAEs optimize (1) via different multimodal ELBOs. The novelty lies in the optimization of (2) which can further benefit from unimodal samples for which no \"related\" samples of the other modality are available---this can be viewed as a semi-supervised approach for weakly-supervised multimodal data.  For the estimation of (2), the paper experiments with two different estimators, IWAE and CUBO.\n\nThe paper claims three contributions:\n- (C1) improve multimodal learning of existing multimodal VAEs by extending them with objective (2)\n- (C2) improve sample efficiency of multimodal VAEs by extending them with objective (2)\n- (C3) further improve sample efficiency by bootstrapping \"related\" samples from a larger pool of unimodal samples\n\nOverall, the empirical results support the claims of improved sample efficiency, but there might be a potential problem with claim (C1), due to the one-sided selection of metrics used for evaluation.\n\n\n## Strong points\n\nThe paper tackles a practical problem (sample efficiency) with a\nstraightforward solution (leverage unpaired data) that can be easily\nimplemented and seems to work well across a family of models.\n\nStrong empirical results demonstrating improved sample efficiency. For the\nconsidered datasets, the proposed objective improves sample efficiency \nsignificantly, requiring only 20% of the dataset to reach a\nperformance comparable to not using the additional objective. Additional\nresults showing that the sample efficiency can further be improved by a\nprocedure for bootstrapping paired samples from a set of unpaired samples.\nThis is an interesting idea that has been explored in semi-supervised learning,\nbut the present paper applies it to a weakly-supervised, multimodal data.\n\nThe paper nicely connects the objectives used in multimodal VAEs to contrastive\nlearning. It extends the different ELBOs used in multimodal VAEs with an\nadditional objective for minimizing the similarity of randomly paired samples.\n\n\n## Weak points\n\nMy main objection is with regards to claim (C1), which states that the new\nobjective improves multimodal learning. The problem is that the choice of\nmetrics might be too one-sided, measuring only the \"relatedness\" in terms of\nground truth labels, but not the generative quality (of a generative model). As\nsuch, the used metrics (linear classification accuracy; classification accuracy\nof a pretrained classifier on generated samples) ignore the diversity of\ngenerated samples. For example, a model could achieve perfect scores by\ngenerating a single \"canonical\" image of the corresponding class, akin to mode\ncollapse. I understand that generative quality is not the paper's primary goal,\nbut it should not be overlooked, since otherwise, it begs the question of why to\nuse decoders at all and not just apply contrasting in latent space. The\nablations in the appendix indicate a trade-off between the \"relatedness\"\nmetrics and generative quality, suggesting that the proposed objective yields a\nlooser ELBO (Figure 13) and results in a worse generative performance (Figure\n12). If such a trade-off exists, I would recommend presenting it more\ntransparently in the experiments and phrasing the respective claim more\nconservatively.  Further, all previous multimodal VAEs report log-likelihood\nvalues, so it seems natural to provide these numbers, too.\n\nIn the related work section, it is not clear how the authors conclude that contrastive methods are limited to \"specific tasks\" and how the proposed model overcomes this limitation. If the statement refers to the manual\ndesign of a contrastive objective, I would argue that previous work includes\nexperiments showing that learned representations still generalize to other\ndownstream tasks, such as classification and object detection. If the authors\nrefer to generative tasks in particular, this should be stated more clearly.\n\nOverall, it is a good paper with strong empirical results for the sample\nefficiency argument. Yet, I tend towards a weak accept, due to the one-sided\nevaluation. I will be happy to adjust my score if the paper presents the\ngenerative results more transparently in the experiments.\n\n\n## Questions\n\nIn the experiments of Section 4.3, do I understand correctly that when using\ne.g. 20% of the data, you do not use any of the remaining 80% as \"unrelated\"\nsamples for contrasting? Put differently, I understand that you only use\nrandomly paired samples among the 20%-subset for that purpose, correct?\n\nIn the description of the datasets you mention that multiple pairings are used\nto create a dataset of \"related\" samples.  Have you also tried to reduce the\nnumber of pairings instead of taking $n$ percent of the dataset? Does this make\na difference with respect to classification, coherence, or generative\nperformance?\n\n\"Relatedness\" in this work is based on log-likelihoods computed in sample\nspace.  However, contrasting in latent space might provide a more meaningful\nmeasure of \"relatedness\". Have you considered computing the objective (or at\nleast the part that estimates the \"unrelatedness\") using latent representations\ninstead?\n\nIn Section 4.5 it is not quite clear how you estimate the \"optimal threshold\".\nWhat measure is this threshold based on? Do the results in Figure 8 depend \non this threshold and would it be reasonable to show an ablation across\ndifferent threshold values?\n\nIn the paragraph above Equation (2) you mention the margin $m$.  It is not\nclear why $m$ is not used later on. E.g., Is it an additional hyperparameter in\nthe model?\n\n\n## Additional Feedback\n\nIn Hypothesis 3.1., you use the pointwise mutual information (PMI) and state\nthat PMI(x, y) > PMI(x, y'). As far as I understand, PMI is zero when two\noutcomes are independent, but it can be negative when two outcomes are not\nindependent, which would violate the \"uncontroversial assumption\".\nAlternatively, maybe one can use the mutual information between two random\nvariables instead of PMI between outcomes?\n\nThe related work section could benefit from a short paragraph on\nweakly-supervised learning, which seems to be an important theme in the present\npaper.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}