{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors study co-ordination in multi-agent systems. Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents' actions, they then use this to form a plan which forms the agents' intention which is then communicated to the other agents.\n\nThe major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest. During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test. While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication"
    },
    "Reviews": [
        {
            "title": "Paper is well written, clearly explains the proposed approach, compares to appropriate state-of-the-art baseline methods, but there is significant room for improvement in the experimental results",
            "review": "Summarization and Strengths:\nThis paper studies how to learn coordinated behavior among multiple agents by learning a communication protocol. Specifically, compared to existing works, this paper proposes to generate messages based on not only current information but also future information (referred to as imagined trajectory) (Section 4.1). Additionally, the attention module in Section 4.2 is learned to weigh between current and future information dynamically. Overall, the paper is well written, clearly explains the proposed approach, and compares to appropriate state-of-the-art baseline methods.\n\nWeaknesses: \nWhile the motivation of this work is clear, the experimental results fail to support the claim. The results in Figure 3 show a small performance difference between IS (proposed approach) and baselines due to the large variance. Performing a statistical test, such as the t-test, to verify whether the proposed approach achieves statistically significant results than baselines will be helpful. \n\nFurther Questions:\n1. In general, learning the attention mechanism improves results, but is it necessary for the proposed approach? Adding an abbreviation analysis, such as the performance difference between with and without attention, will be helpful.\n2. The action predictor (Equation (2)) predicts the peer agents' actions based on the agent's own observation. While the peer action prediction based on the agent's own observation supports the decentralized execution, the local observation may not include sufficient information about the others depending on a multiagent domain and thus the action prediction can fail. Instead, wouldn't it be better to predict the peer agents' actions based on both the agent's own observation and received message m_{t-1} because the received message can contain useful information about the peers' behaviors? \n3. As noted in footnote 1, the message at m_{t-1} is used for all H prediction steps. How does the performance differ with respect to different H values?\n\nMinor comments: \n1. Typo in Page 1: ''How to harness ... partial observation.'' -> ``How to harness ... partial observation.''\n2. In Equation (2), the subscript \"t\" is missing (e.g., \\hat{a}^{i-1} -> \\hat{a}^{i-1}_{t})\n3. Adding pseudo algorithm in Appendix will further clarify the proposed approach\n4. For PP domain explanation (Section 5.1), it is unclear whether prey can run away from predators. In general, preys can run away in PP, but in this paper, preys seem to be fixed and cannot move around.\n5. For PP and CN domain explanation (Section 5.1), it is unclear what each agent observes.\n6. The intention prediction is also related to the theory of mind and opponent modeling (e.g., He et al., ICML-16, Raileanu et al., ICML-18, Rabinowitz et al., ICML-18), which can be added in the related work section (Section 2).\n\nHe He, Jordan Boyd-Graber, Kevin Kwok, Hal Daum√© III. Opponent Modeling in Deep Reinforcement Learning. ICML-16\n\nRoberta Raileanu, Emily Denton, Arthur Szlam, Rob Fergus. Modeling Others using Oneself in Multi-Agent Reinforcement Learning. ICML-18\n\nNeil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S.M. Ali Eslami, Matthew Botvinick. Machine Theory of Mind. ICML-18\n\nI have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " well-written and technically sounds",
            "review": "Summary: This work proposes to use 'intention' of each agent to enhance the message sharing scheme of MARL. For training, MADDPG is used as a backbone MARL and implement ITGM and AM on top of it. The proposed method is compared with several baseline approaches from three different environments.\n\nStrengths:\n+ The paper is well-written and technically sounds. \n+ The motivation is clear.\n\nWeaknesses:\n- This work wants to see the effectiveness of the use of intention as a communication scheme under MARL. However, in general, this is not the first to use intention. I suggest to review some relevant works and show how 'intention' has been explored in a similar/different way in the literature. \n- Although visualization of imagined trajectory is given in Figure 4, it does not fully demonstrate the idea of this approach. More qualitative evaluation seems required to validate from various aspects. \n- It seems that attention is used to capture which waypoint is more important in an imagined trajectory rather than whose imagined trajectory is more important. If so, is it really important? Where is the ablative study of using attention?\n- Intuitively, when two agents set the same goal (catching same prey), what is the rationale of determining who maintain the goal or who change the plan? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for paper \"Communication in Multi-Agent Reinforcement Learning: Intention Sharing\"",
            "review": "[Summary]\n\nPaper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). An extra self-attention model is also stacked over the trajectories to trade off the length of prediction and the possible explaining away issue.  The whole model is trained via a canonical MARL objective while the trajectory prediction model utilizes direct supervision collected from the environments. Experiments on several toy MARL benchmark demonstrates the effectiveness of the proposed method.\n\n[Strengh]\n\n+) The idea of communication with imagined intention is motivated properly with rich psychological background and also technically sound.\n\n+) The paper is overall clear and well-written. I found there are enough technical details to reproduce the main results of the main approach.\n\n+) Still limited though in terms of the converted domains, the empirical evaluations deliver impressive results over a reasonable collection of counterparts.\n\n[Weakness]\n\nThe main concerns I have with this submission lies in the overall novelty and the evaluation of the proposed method. After reading this paper, indeed I find the authors failed to capture some important research in this narrow area and it's still not clear how does the proposed method really works and whether it is sensitive to some specific implementing factors.\n\n-) The idea of intention sharing based on prediction is essentially not novel esp. in the MARL domain. In the CogSci & AI community, theory-of-mind (ToM) and its application in multi-agent execution (either collaborative or zero-sum games) have been well studied for years [1, 2, 3]. Although I may agree that there are few prior works [4] on sending these predictions as messages to other agents, it does not really introduce that many new ideas to how collaborative or adversarial agents could benefit from such ToM-based intention prediction. However, the authors failed to capture these counterparts in their discussion and evaluations. Specifically, I would like to see how does the proposed method formulate its intention prediction differently than the prior work and whether it could enjoy advantages in performance with such differences.\n\n-) The proposed method is essentially quite complex (stacked prediction, transformer, etc) than its Bayesian counterparts, while the authors only provide an overall evaluation against several MARL baselines. It will be critical to also conduct a serious ablation study given the overall complexity of the proposed method, i.e. whether to use the transformer, the architectural choice of the transformer (num. of heads, etc), and the length of predicted trajectories (H). Also, the disagreement between the predicted trajectories and actual observation and its relation to the performance should also be investigated.\n\n-) Although the selected tasks are all canonical to MARL, given the fact a growing number of recent MARL learners have been moved on to more challenging tasks, I feel it would be necessary to include some mini MOBA games or other tasks with vision-based observations. \n\n[1] Bayesian models of human action understanding\n\n[2] Theory-based Social Goal Inference\n\n[3] Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution\n\n[4] Machine theory of mind\n\n[Suggestions&Questions]\n\n(1) Add ablation studies on the use of self-attention model and the length of prediction (H).\n\n(2) Visualize&compare the predicted and observed trajectories, add some discussion on how such disagreement would affect the performances.\n\n(3) (minor) Try the proposed method on mini MOBA games or tasks with vision-based observations.\n\n(4) Add citations to the related work on ToM and its application in MAS.\n\n[Post-rebuttal]\n\nI have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4's concern on the significance of results is valid. Therefore, I will keep my initial justification as is. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting method for an important problem. Would be improved by ablations to know what matters.",
            "review": "---- Summary ----\nThe paper proposes a new method for training communication in multiagent systems. The method involves agents producing imagined future trajectories using learned environment dynamics, and then communicating some parts of these trajectories to other agents. This communication scheme is evaluated on several cooperative communication domains, outperforming other methods.\n\n---- Reasons for score ----\nThe paper addresses an important problem, the method is sensible, and the empirical results suggest the method proposed works. My main concern is a lack of ablations, which makes it hard to tell which parts of the method are important.\n\n---- Pros ----\n1. The problem is well motivated. The existing literature on learned multi-agent communication primarily addresses the problem of communicating information about partially observed states. However, this is not all there is to communication, and the communication of agent intentions is an important topic to address.\n\n2. The method suggested - basing messages on predicted future trajectories - is sensible and well motivated.\n\n3. The experiments compare the method to a good range of baseline algorithms for multiagent communication, and it performs well.\n\n---- Cons/Questions ----\nThe method proposed has several moving parts, and without ablations it is hard to tell which are the most important pieces. There are a few experiments I would like to see added to work out what is important in the method:\n* A comparison to the uniform attention case. While the qualitative analysis suggests the attention mechanism is meaningfully used, it would be good to show how this affects performance.\n* An ablation where all the attention is put on the first timestep, to show the effect of the imagined trajectories. While this is similar to the other communication methods discussed in the paper, I am not sure whether it directly corresponds to any of them.\n* A comparison to a case where the agents receive only their own messages to condition their next actions. At the moment is unclear whether the advantage is gained from sharing intentions between agents, or by having a prediction of the future.\n\nAll the environments used are modifications of existing environments; was there a reason to change these? Particularly, the Cooperative-Navigation environment looks like it could have been used as-is, for an easier comparison to previous work?\n\n---- Post-rebuttal ----\nThe additional ablation studies on the attention and MADDPG-p are helpful, and address most of my concerns - though it is still not clear to me whether any of the baselines compared to is exactly equivalent to the method used with attention weights (1, 0, 0, ...). I share the concerns of Reviewer 4 about the significance of the results - this is still not in the paper, and not present for the new experiments. Overall, I have not changed my rating.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}