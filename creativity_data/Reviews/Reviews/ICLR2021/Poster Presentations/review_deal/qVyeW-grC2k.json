{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper attempts at providing a general benchmark for evaluating/analysis of long range transformer models, consisting of a 6 evaluation tasks. The main goal of the paper is to remove conflating factors such as pretraining from model performance and keeping the benchmark accessible. All reviewers agreed that these are important positive aspects of the paper and the presented analysis/results are useful.  \n\nWhile reviewers generally feel positive about the work, there are some critical concerns on how useful this benchmark is in practice, how generalizable are the results, and whether the benchmark is good at what is intended for. For example, the vanilla Transformer model performs very well on all the proposed tasks, making me question on what we can actually learn about long range dependencies through this benchmark. In addition, most tasks are synthetic and all models fail on 1 of the 6 proposed tasks. \n\nTherefore, I think LRA should be viewed more as a tool for analysis, or as authors nicely put in their response, it should be viewed as a means to \"encourage hypothesis driven research instead of hillclimbing or SOTA chasing.\". \nDuring discussion period with reviewers, while acknowledging the above-mentioned issues, this strength was highlighted as a valuable contribution. Therefore, given the general positive sentiment about the work, I'd recommend accept."
    },
    "Reviews": [
        {
            "title": "Benchmarking efficient transformers",
            "review": "This paper proposes a suite of long sequence processing tasks to benchmark efficient transformer variants in terms of their accuracy/speed tradeoff. The benchmark tasks are constructed following six desiderta: generality, simplicity, challenging, long inputs, probing diverse aspects, and accessibility, resulting in six tasks mostly in the form of sequence classification where the sequences range from 1k to 16k tokens. Ten existing efficient transformer models are evaluated on these tasks and their accuracy/speed tradeoffs are compared on a relatively fair basis.\n\nPros:\n1. This work is well motivated and timed. Given the attention efficient transformer variants recently attracted, it is important to have a common benchmark for comparability of results.\n2. The diversity of the proposed tasks (in terms of types of tasks and modalities covered) enables a full grasp of the performance among different types of tasks, especially exposing the weaknesses. For example, it is interesting to see that kernel-based methods fail on tasks requiring hierarchical structures.\n3. This benchmark is accessible for academia : from table 2 it takes at most 10G GPU memory for a normal transformer.\n\nCons:\n1. For generality this suite of tasks only considered transformer encoders, but not the autoregressive decoding process, so the tasks here are mostly sequence classification tasks only. I think it is a limitation of the benchmark here. For example, all efficient transformers fail on path-x, but that don't mean that path-x is not useful. It is the limitation of the efficient transformer method itself if it does not support efficient autoregressive decoding.\n2. While the benchmark proposed here considers image and text, it'd be interesting to add audio processing as well, such as hot word detection.\n3. Pro 3 might be a downside of this benchmark as well, the tasks here can be mostly solved by a normal transformer while one reason we want efficient transformer is to apply it to extreme situations where normal transformers are infeasible, and there seems to be more focus on inference speed but not training/inference memory.\n4. Related to Con 3, it might be nice to separate out training and inference (memory/speed).\n\nQuestion for the authors:\nfor speed and peak memory, do you only measure at inference time? Is there any evaluation of training statistics like memory and time to convergence?\n\nTypo:\nThis is the most comprehensive and extensives -> extensive\n\nOverall, I think this would become an important bechmark for comparing efficient transformer approaches, and I would recommend its acceptance if the issues I mentioned above can be mitigated.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not a perfect benchmark, but still useful",
            "review": "This paper proposes benchmark tasks for the new crop of efficient Transformer models, an evaluation suite which the authors dub the \"Long Range Arena.\" These tasks are selected to require hierarchical modeling of long-distance dependencies, and specifically to *exclude* pre-training or a need for pre-trained models (making many of the NLP tasks which motivate these papers unsuitable).  Many of the tasks have a somewhat artificial flavor: purely artificial (ListOps), treating images as sequences of pixels, or byte-level classification rather than word level or word piece level. Regardless, the paper shows interesting variation in the tasks across the different models, particularly on ListOps. BigBird seems to work the best out of the box, but the authors caution against reading too much into this, as they could not extensively tune hyperparameters on each of these models.\n\nI think this paper addresses a desperate need in the literature, which is a way of making sense of all these Transformer variants that have been proposed. In this sense, I like what it's doing. My main reservations concern the somewhat artificial nature of the tasks and the generalizability of the results.\n\nNATURE OF THE TASKS: This paper is probably wise to not conflate distinctions in architecture with distinctions in pre-training, and exploring both of these factors is too much and too complex for a single piece of work.  Papers such as the Longformer have primarily evaluated on NLP tasks like coreference and question answering; however, the \"long-range\" abilities on these datasets only add a few percentage points on the given metrics, and they rely on pre-training to be successful. However, I feel like the distribution of long-range effects in natural language is very different than the distribution of effects here: you have effects like entity coreference that aren't captured well by these tasks (these don't matter much for text classification and are absent from ListOps and the image tasks). So I'm not fully convinced that a model that does better on this benchmark suite would naturally do better on problems we really care about.\n\nThe tasks here do feature some nice effects: explicit hierarchy (ListOps), local composition (the byte-level tasks), and regular long-range correspondences (the image tasks). It's not clear to me how critical this last category is, as the 2D layout is an inductive bias that would almost be built into the architecture, but that's a separate discussion. However, the point remains that this benchmark does make choices about what is and isn't important, *particular* when you average across the datasets (ascribing each one equal weight), and I can imagine another benchmark being proposed focusing on a different set of effects. So I'm not sure how to judge this benchmark suite versus others that might exist.\n\nRESULTS: This leads to my second point, about the results. I do like how the paper couches the results, explains the findings, and avoids overclaiming. I agree with the authors that, with some tuning, many of these approaches could possibly substantially better at various of these tasks. We might also see that some of these approaches are fundamentally not well-suited to these tasks.  But again, given the somewhat artificial nature of the tasks, what are we likely to actually learn? I foresee a future with ~10 papers hillclimbing on this benchmark, and I'm not convinced that those architectures will be any better than these when applied to new settings, LM pre-training, etc.\n\nThat said, the differences on ListOps are pretty interesting.  I wonder if Path-X has fundamental identifiability issues due to its scale: for example, there are now more geometries the model can learn from the sequence, and possibly there are somehow just too many of these for learning to effectively figure out which is correct. (I don't have a more formal argument, but it feels like a case of exponential numbers of hypotheses outstripping the size of the dataset as things scale up.)\n\nAll this said, I tend to come down positive on this paper: I think it brings some clarity to this space and will be a very useful starting point for others in the literature. And I don't think this meta-benchmark is necessarily more flawed than others like GLUE.  But nor do I think it's perfect.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and thorough analysis, unclear whether this is a useful benchmark",
            "review": "*Summary*: This paper proposes a new benchmarks for the host of recently-proposed transformer variants focused on efficiency and scaling to longer sequence lengths (xformers). The authors reimplement and study the performance of 10 xformers on their benchmark. Furthermore, the authors conduct a study of the memory consumption and speed of the models on their text classification benchmark.\n\n*Strengths*: Detailed and thoughtful comparison of 10 models aimed at more-or-less alleviating the same problems. The tasks span a wide range of sequential data modalities.\n\n*Weaknesses*: It’s not entirely clear to me that LRA is best-positioned as a benchmark, rather than an analysis tool; the authors themselves seem to also note this (“Hence, the results provided in this paper are not meant to be a final authoritative document on which xformer is the best”). This toolkit seems more useful to me as an analysis tool---the choice of tasks itself also reflects the benchmark’s values. For instance, a NLP researcher might care more about performance of ListOps, since it’s possible that these results have more relevance for the type of structure found in natural language.\n\nInstead of seeking to rank models (calling LRA a “benchmark” and having an “average” LRA score directly feeds into this), I’d like to see this toolkit shift toward being more customizable for individual user questions and values; similarly, this paper might take a more analytical approach to empirically studying the performance of these 10 models on this starter set of tasks, versus trying to rank them.\n\nDespite this lack of clarity of goals, I think that this toolkit and study offer useful contributions.\n\n*Recommendation*: 7 . While it remains unclear to me that this paper has a useful benchmark contribution, the analysis of existing models is valuable. Furthermore, the observation that inherent tradeoffs in performance and speed make no model the one-size-fits-all option is important; in light of this, I’d like to see the authors move towards making their toolkit better for determining what the “right” option is for a given user’s use case.\n\nComments and Questions:\n\nFigure 2: y axis label should be “Span”, not “Apan”\n\nSome abstraction is taken with respect to hyperparameters---a single set of hyperparameters is used across all models. Do you have a sense of how much this can potentially impact performance? For instance, for a single task, comparing results when you use this single set of hyperparameters vs individually tuning models for the task.\n\nDo you think that future developers of xformers should “hillcimb” on LRA?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " An important, well-executed benchmark (though consisting of non-realistic tasks) for evaluating transformers on long-range tasks.",
            "review": "This paper presents LongRangeArena (LRA), a new benchmark for evaluating models such as Transformers on tasks that require long-range processing. The authors present a suite of 6 tasks, each requiring access to tokens that are hundreds and even thousands of tokens apart from the target token. Their tasks share a set of important properties such as being challenging and yet simple (though not realistic, see below). The authors make a thorough comparison between 9 models designed to support efficient processing of long inputs, and show nice tradeoffs between performance, speed and memory usage. Overall, this paper makes an important contribution and despite some limitations should be accepted to ICLR.\n\nThe paper tackles an important problem: evaluating long-range transformers. There have been multiple efforts recently to design models that scale better than the quadratic time and space complexity of vanilla transformers to long sequences. Despite the large number of works, there hasn't been a standard benchmark to compare them against each other, which makes it harder to evaluate the quality of each of them. This paper addresses this problem, and presents a relatively good solution: a single, diverse benchmark that requires long-sequence processing. The new benchmark is likely to promote research on this important topic.\n\nMy main concern with this benchmark is that it doesn't involve any realistic task. The proposed tasks are either synthetic (ListOPs) or made artificially hard by forcing byte-level processing or flattening images to vectors. This means that there is no real value in performing well on these tasks, other than being a potential proxy to other tasks with similar characteristics, assuming such exist. I think relating this benchmark to real tasks would have made it much stronger. I realize that the authors made an attempt to control for different variables such as structure, but I am still not convinced that models that do well on this benchmark will be useful for anything real.\n\nOther comments:\n1. The paper would benefit from discussing the differences between the different models. A very minimal discussion is given on the last page, with a pointer to a survey paper. While I realize this is not the main focus of the paper, the different models are an important part of it, and without such even basic comparison, it is hard to appreciate the analysis presented in the results section (e.g., the last sentence on the Results on ListOps).\n2. The requirement that the tasks should allow for \"probing diverse aspects\" of the models is somewhat under-delivered. The authors discuss some probing aspects of the first 3 tasks, but do not come back to these later in the paper, which was a bit disappointing.\n\n\nMinor: \n1. Adding random/majority baselines to table 1 would make it easier to appreciate the results\n2. Results on ListOps: \"the inductive bias of the xformer *model* plays \" -> *models*",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}