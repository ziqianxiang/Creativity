{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers generally found the idea interesting and the contribution of the paper significant. I agree, I think this is quite a neat idea to investigate, and the paper is written well and is engaging to read.\n\nI would encourage the authors to take into account all of the reviewer suggestions when preparing the camera-ready version. Of particular importance is the name: I think it's bad form to appropriate a name already used in other prior work (proto-value functions, which are very well known in the RL community), so I think it is very important for the final to change the name to something that does not conflict with an existing technique. Obviously this does not affect my evaluation of the paper, but I trust that the authors will address this feedback (I will check the camera-ready)."
    },
    "Reviews": [
        {
            "title": "Interesting idea that is well-investigated",
            "review": "### Summary:\nThe paper proposes passing the parameters of a policy to the value function attempting to learn estimates of the return for that policy. This allows the value function to generalize across policies and estimate values for arbitrary policies. The paper derives several algorithms for various objectives and value functions, and empirically investigates the deterministic versions.\n\n### Pros:\n- Several new algorithms are proposed\n- The new algorithms can generalize across policies\n- The new algorithms can estimate the value of unseen policies\n\n### Cons:\n- Only the deterministic algorithms are empirically investigated\n- Computation and memory cost seem quite high (the critic takes all of the actor’s parameters as arguments)\n- Empirical results seem mixed\n\n### Decision\nI recommend accepting the paper for publication.\n\nThe paper investigates a simple, interesting, original idea—including the actor’s parameters as inputs to the critic—fairly thoroughly. Several actor-critic algorithms are derived using expressions for the gradient of various performance measures obtained by including the actor’s parameters as inputs to the critic.\n\nThe benefits of doing this are illustrated by some experiments, and the deterministic versions of the new methods are compared with reasonable competitors (DDPG and ARS) in other experiments. Unfortunately the results seem somewhat limited by the number of runs that can be conducted by parameterizing the policies and value functions as neural networks and experimenting on the chosen environments. Overall the empirical results seem mixed; in many environments it’s fine to just disregard the second part of the gradient that is dropped in DDPG and computed by PVFs. However, that’s not the fault of the new algorithms, and there are some environments where not dropping the second part of the gradient is helpful.\n\nThe paper is clearly written for the most part, with the exception of some parts of the related work that are overly terse (i.e., the connection with UVFAs could be expanded). Other parts of the related work seem frankly unrelated (i.e., predicting gradients of RNNs from their inputs in the 90s, and mapping weights of CNNs to their accuracy), and I would recommend removing them in favour of moving the more detailed comparison of PENs and PVFs into the main paper.\n\n### Miscellaneous comments:\n- Grammatical error in the final sentence of the abstract: “Their performance is comparable to the one of state-of-the-art methods”\n- “In practice, like in standard actor-critic algorithms, we use a noisy version of the current learned policy in order to act in the environment and collect data” This should probably read standard deterministic actor-critic algorithms.\n- I was disappointed to see that only the deterministic algorithms were implemented and analysed. Even if the stochastic versions of the algorithm are only demonstrated in a simple linear setting, that would be better than just not investigating them at all.\n- The paper doesn’t mention related work that fixes the Off-PAC policy gradient theorem, which gives an expression for the true gradient of the off-policy objective without requiring PVFs (Imani 2018).\n- Passing the actor’s parameters to the critic seems to necessarily break the requirement of compatible features for the actor to follow the true gradient of performance (Sutton 2000). It might be good to mention this.\n\n### References:\n1. Imani, E., Graves, E., & White, M. (2018). An off-policy policy gradient theorem using emphatic weightings. In Advances in Neural Information Processing Systems (pp. 96-106).\n2. Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems (pp. 1057-1063).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting connection to DPG, few technical errors",
            "review": "On page 2, in the background section: the discounted state distribution, what you wrote is not a distribution (doesn't sum to 1). In order to define this $d^{\\pi_\\theta}$ properly, you can multiply everything by $1-\\gamma$. The interpretation is that you \"reset\" in your initial distribution $\\mu_0$ with probability $1 - \\gamma$ at every step, or continue in the discounted stationary distribution with probability $\\gamma$.\n\nIn think that theorem 3.1 is incorrect. I think that this is meant to describe an off-policy setting where we are collecting data from $\\pi_b$ but want the policy gradient for $\\pi_\\theta$. In this case, the importance sampling weight should be $\\frac{d_\\theta(s,a)}{d_b(s,a)}$ not $\\frac{\\pi_\\theta(a|s)}{\\pi_b(a|s)}$ (where $d_b$ is the discounted stationary distribution, see above comment too). Equation 9 follows from the chain rule (because the Q function now depends on $\\theta$ explicitly) using the off-policy formulation in Degris (2012), which is incorrect.\n\nNotes:\n- PVF: to me this acronym is strongly synonymous with Mahadevan's proto-value functions (PVFs), circa 2007. How about \"PBVF\" instead? Maybe I'm old\n\n> we optimize for the undiscounted objective\nthis should be reflected in your notation and problem formulation\n\n> can be used only for episodic tasks\nit doesn't have to. See \"regenerative method\" in Monte Carlo estimation literature ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting in terms of idea, but unclear advantage over common approaches",
            "review": "— idea:\n\t\nA new class of value functions is introduced where the value function takes the parameters of the policy as input, in addition to its common inputs (state or state-action). The proposed type of value functions, PVFs, are also useful for off-policy learning and generalizing over policies while the common value functions lost their information about previous policies.\n\n— comments:\n   \n1- Regarding the first algorithm, PSSVF, until converging, the data that is stored in the replay buffer does not correspond to a \"reasonable\" policy, unless having a prioritized replay buffer. I am also concerned about it being over fitted to the early policies and not being able to  overcome this. I see in the experiments that using PSSVF, policy is converged but am not convinced about it.\n    \n2- Another concern of mine wrt the proposed PVFs is about their sample efficiency. It would be interesting to see a comparison between DDPG and PVF based methods on their sample efficiency.\n    \n3- In the experiments section (4.2), expect for a few cases, ARS is either the best one or does not differ significantly from PVF based methods. Having this in mind, my question is that have you tried to find the best set of hyperparameters for ARS and DDPG as well as your proposed method? If the answer is no, I would like to see that experiments where ARS and DDPG have their best set of hyperparameters.\n    \n4- I am also interested in seeing results for deep policy zero shot learning. In section 4.3, authors just mention: \"When using deep policies, we obtained similar results only for the simplest environments.\" which is not convincing without showing results.\n    \n5- As mentioned in the last part of the paper, the proposed method hugely suffers from the curse of dimensionality. However, as an initial step, PVFs seem interesting and could be beneficial in terms of learning generalized value functions.\n\n-- minor issues:\n    \nIn the first line of the paragraph above the experiments section (4), starting with \"Algorithm 4 (Appendix) uses an ...\", there is a redundant \"and\". One of them should be removed.\n\n\n\nOverall, I liked the idea presented in the paper and would like to see what their next step would be. But the current version of the paper could benefit from more in depth experiments. I believe the most important weakness of the paper lies in the experiment section. It can be much richer and more insightful.\n\n\n\n[1] Sutton, Richard S., et al. \"Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.\" The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2. 2011.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reject for theoretical reasons. (Update: the theoretical issues were cleared up)",
            "review": "**Update**\n\nI have updated my score to 7.\nOne of the points that was not explained in the original paper was that (ignoring function approximation effects) an optimal solution for $J_b$ (the OffPAC objective) will be optimal also for the original off-policy RL objective $J$ (i.e. estimating the on-policy objective in an unbiased manner from off-policy data). From this point of view, I agree that optimizing $J_b$ directly is an interesting question, despite the fact that the exact gradient for $J_b$ may be less similar to the gradient of $J$ compared to the usually used approximate gradient of $J_b$ that drops the $\\nabla_\\theta Q$ term. It still remains unclear which of the two methods has a theoretical advantage over the other in the function approximation setting (in terms of optimizing for $J$); however, because it is unclear, it is interesting to evaluate the method proposed here and to perform experiments as done in the paper to try to find out which method performs better.\n\nThe results were mixed; however, the evaluation is fairly thorough and some potential advantages of the new methods such as generalization in the $\\theta$ space and zero-shot learning were explained.\n\nThe discussion in the paper is much improved compared to the original version. Also, additional ablation studies such as testing what happens when the $\\nabla_\\theta Q$ term is dropped were added (when $Q$ includes $\\theta$ as an input). Moreover, LQR experiments for $Q(s,a,\\theta)$ and $V(s,\\theta)$ were added in the appendix (the results here do not give as good a match as the $V(\\theta)$ formulation gave, but they are reasonable).\n\n______________________________________________________________\n1. Summarize what the paper claims to contribute. Be positive and\ngenerous.\n\nThey propose to include the policy parameters as an input\nto the value function, so that the value function could generalize\nacross different policies (there are 2 other concurrent works with a similar\nidea, one they have cited and discussed \"Policy evaluation networks\"\nhttps://arxiv.org/abs/2002.11833, another is submitted to ICLR2021 on\nopenreview https://openreview.net/forum?id=V4AVDoFtVM).\nThey put the policy parameters theta as an input to the value function\nin 3 cases $V(\\theta)$ (PSSVF), $V(s, \\theta)$ (PSVF), and $Q(s,a,\\theta)$ (PAVF).\nThey propose new policy gradient theorems for the $V(s,\\theta)$ and\n$Q(s,a,\\theta)$ cases (but I believe these to be theoretically flawed).\n\nThey perform experiments testing $V(\\theta)$ in 2 cases: 4.1) (sanity check\nexperiment) visualizing and testing for correctness on an LQR task,\n4.3) zero-shot learning: after training a policy pi using the $V(\\theta)$ method,\na new policy is reinitialized pi_new and trained from scratch using only the\ntrained $V(\\theta)$ without interacting with the environment. The interesting\nbit was that $\\pi_{new}$ managed to outperform the learned policy during\ndata collection $\\pi$ (this implies that the $V(\\theta)$ function managed to\ngeneralize. It would have been nice to, in addition to $\\pi_{new}$, also\nsee whether $\\pi$ could have been improved by just continuing to optimize\nit without interacting with the environment, but this was not done).\n\nThey tested $V(\\theta)$, $V(s,\\theta)$ and $Q(s,a,\\theta)$ on MuJoCo tasks\ncompared to augmented random search (this is similar to evolution\nstrategies) and to deep deterministic policy gradients (DDPG). And\nthe performance did not change much, and sometimes all the new methods\nfailed when DDPG worked (on the reacher task).\n\nThe final experiment 4.4 was for offline learning with fragmented\nbehaviors, i.e. they do not observe full episode data for a fixed\ntheta, which makes it impossible to learn $V(\\theta)$ directly, but\n$V(s,\\theta)$ can be learned by TD methods (also note that the data is\ncollected from a different behavior policy). Then they test a similar\nzero-shot learning procedure as they did for $V(\\theta)$ at different\nstages of the learning (but as far as I understood, for $V(s,\\theta)$ they sampled\ndata from the replay buffer when training the policy (thus not fully without\ninteracting with the data). Perhaps the authors can clarify this), and\nshow that the newly learned policy can outperform the behavior policy,\nthus demonstrating the generalizability of the method.\n\n\n2. List strong and weak points of the paper. Be as comprehensive as possible.\n\n\\+ The experiment on zero-shot learning is nice to show that the $V(\\theta)$\nfunction can generalize.\n\\+ The paper is clearly written.\n\\+ They discuss a lot of related work.\n\\+ The experimental methodology seemed mostly good and honest, and\nwas explained in detail in the appendix (some nice points: They include\na sensitivity analysis showing quantiles of the performance.\nAlso the final best chosen hyperparameters were evaluated with\n20 new seeds, separate from the 5 seeds used during hyperparameter tuning).\n\n\\- The new policy gradient theorems seemed flawed. Also some discussion\naround off-policy learning seemed incomplete.\n\\- The methods were not shown to experimentally lead to major gains.\n\\- One of the difficulties with searching in parameter space is how\nto deal with large parameter spaces. The two concurrent works considering\n$V(\\theta)$ proposed solutions to this issue by embedding the policy into\na smaller space. In the current work no solution is proposed. The\nexperiments on zero-shot learning using $V(\\theta)$ were only good with\nlow-dimensional linear policies.\n\\- A sanity check experiment on LQR was performed for only $V(\\theta)$ (which was\nthe only one for which the gradient was theoretically sound); it would\nhave been good to do similar experiments for the other ones.\n\\- I would expect $V(s,\\theta)$ to outperform $V(\\theta)$ due to using the\nstate information, but this did not appear to be the case.\n\n\n3. Clearly state your recommendation (accept or reject) with one or two\nkey reasons for this choice.\n\nI recommend rejecting the paper due to the theoretical flaws in the newly\nproposed policy gradient theorems using $V(s,\\theta)$ and $Q(s,a,\\theta)$. Also,\nthe practical advantages of using $V(s,\\theta)$ and $Q(s,a,\\theta)$ were not shown.\n\n\n4. Provide supporting arguments for your recommendation.\n\nThe theoretical issues in this paper start in equation 1. They write:\n\"... we can express the maximization of the expected cumulative reward\nin terms of the state-value function:\"\n\n$J(\\pi_\\theta) = \\int d^{\\pi}(s) V(s) ds,$  (in the paper)\n\nwhere $d(s)$ is the discounted state visitation distribution. However, this\nis not the RL objective. The RL objective would be\n\n$J(\\pi_\\theta) = \\int d^{\\pi}(s) R(s) ds.$    (what it should actually be)\n\nThe authors probably took their objective from the work by\nDegris et al (2012, https://arxiv.org/pdf/1205.4839.pdf);\nhowever, in Degris'12, $d(s)$ is _not_ the discounted\nstate visitation distribution. It is the limiting distribution as\n$t \\to \\infty$, which is a stationary distribution. When $d^{\\pi}(s)$ is stationary,\nthen the two objectives become equivalent: $d(s)$ does not change\nfrom one time step to the next, so the difference between the objectives\nwill be just a $1/(1-\\gamma)$ constant factor. Putting aside this issue,\nprobably the limiting distribution formulation is not realistic as most\nRL researchers consider the episodic setting, so using a discounted\nstate visitation distribution is probably better. However, the newly\nproposed policy gradient theorems do not appear sound for the true RL\nobjective using $R(s)$.\n\nNext, they replace the distribution $d^{\\pi}(s)$ with a\ndistribution $d^{\\pi_b}(s)$ gathered using a behavioral policy (so they are\nworking off-policy). However, they do not apply an importance weighting\ncorrection for the distribution shift, and just ignore the importance\nweights (Note that this is also done by Silver et al (2014) in deterministic\npolicy gradients, and by Lillicrap et al (2015) in DDPG, so it is not that\nstrange per se, as long as it gives better practical performance. However,\nit should at least be acknowledged that the importance weights are being\nignored). Note that they still apply an importance weight on the actions\n($\\pi(a|s)/\\pi_b(a|s)$) once the state is sampled from the data buffer, however,\nthis does not correct for the distribution shift from $d^{\\pi}$ to $d^{\\pi_b}$,\nso the policy gradient computed using such a method will necessarily be biased.\nFor example, see the following works for examples that try to deal with the\ndistribution shift problem:\nMunos et al (2016, https://arxiv.org/abs/1606.02647),\nWang et al (2016, https://arxiv.org/abs/1611.01224),\nGruslys et al (2017, https://arxiv.org/abs/1704.04651)\n\nPutting aside the issue of whether ignoring the distribution shift is OK,\nthe main issues are the new policy gradient theorems derived from this\nformulation. Both the $V(s,\\theta)$ as well as $Q(a,s,\\theta)$ formulations appear\nflawed:\nIn the $V(s,\\theta)$ case they propose the policy gradient:\n\n$\\nabla_\\theta J(\\theta) = \\int d^{\\pi_b}(s) dV(s,\\theta)/d\\theta ~~ds$ in equation 8.\n\nHowever, the true policy gradient is:\n$\\nabla_\\theta J(\\theta) = \\int \\mu(s) dV(s, \\theta)/d\\theta ~~ds,$\nwhere $\\mu(s)$ is the start-state distribution. Actually they wrote\nthis also in equation 7, when they considered the $V(\\theta)$ formulation,\nbut for some reason sampled from $d(s)$ instead for $V(s, \\theta)$ when computing\nthe policy gradient in the $V(s,\\theta)$ formulation.\n\nIn the $Q(a,s,\\theta)$ formulation, they add an extra $dQ/d\\theta$ term to\nthe policy gradient. Their motivation is the following:\n\n$\\nabla_\\theta J(\\theta) = \\int d^{\\pi_b} (dQ(a=\\pi(s,\\theta),s,\\theta)/d\\theta) dads$\n  $   \t= \\int d^{\\pi_b} dQ(a,s,\\theta)/da*da/d\\theta + dQ(a,s,\\theta)/d\\theta~~ dads$\n\nHowever, this derivation stems from the flawed definition of J that is\nnot maximizing the sum of rewards over the trajectory distribution, but\nmaximizing some other objective that sums the value functions at all states\nin the trajectory distribution. My strongest argument for why the original\noff-policy derivations by Degris et al and Silver et al are less flawed is\nthe following:\nIf we are on-policy, i.e. $\\pi_b = \\pi$ and $d^{\\pi_b} = d^{\\pi}$ we would want\nthe off-policy policy gradient theorem to be unbiased, hence it should\nrevert to the standard policy gradient theorem. In the formulations\nof Degris and Silver, this is indeed the case, and these theorems would\nbe unbiased in the on-policy setting. The new theorem in the current\npaper, on the other hand, would have an extra $dQ/d\\theta$ term, which would\nbias the gradient. Therefore, I do not see any good theoretical reason to\nadd this term. Moreover, the practical performance did not improve, so\nthere is little evidence to suggest it as a heuristic either.\n\nIf someone would say that the original policy gradient\ntheorem requires the $dQ/d\\theta$ term, I would urge them to look at the original\nproofs---there is no approximation, these theorems are exact for the true\nRL objective based on maximizing the rewards over the discounted trajectory\ndistribution. The intuition is that the remaining $dQ/d\\theta$ term for the\nremainder of the trajectory from a time-step t is estimated by summing\nthe $dQ/da\\*da/d\\theta$ or $Q\\*dlog/d\\theta$ terms for all the future time-steps.\n\nAnother more minor theoretical issue in the paper is that while the\ntheory considered the discounted state visitation distribution, the\ndiscount factors are not added into the policy gradient in the algorithmic\nsections. This omission is common, and tends to work well as a heuristic\n(but it should at least be mentioned that such an approximation is made).\nSee the following papers for more discussion on this:\nNota and Thomas (2020, https://arxiv.org/abs/1906.07073)\nThomas (2014, http://proceedings.mlr.press/v32/thomas14.html)\n\n\n5. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. \n\nHow did the computational times compare? Was there much of an overhead to\nusing the more complicated critics including theta as an input?\n\n\n6. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\nFor me to change my assessment, first the theoretical issues should be\nfixed or cleared up.\n\nNext, I have some possible suggestions:\n1) Test also $V(s,\\theta)$ on LQR as well as on zero-shot learning while sampling\ns from the initial state distribution $\\mu(s)$. This does not require interacting\nwith the environment (because you never apply any action), and I would consider\nit fair in terms of comparing to $V(\\theta)$. If the learning from the TD error\nis working well, I would expect it to outperform the $V(\\theta)$ formulation\nin the zero-shot task. \n2) Test the parameter value functions using the standard policy gradients\nwithout adding the $dQ/d\\theta$ term. Because you are using $Q(a,s,\\theta)$, there\nmay be some learning to generalize across different policies due to the\ntheta input, so it may outperform the original policy gradients without\nchanging the policy gradient theorem. Actually, it would have been better to\nperform such experiments as an ablation study from the beginning anyhow.\n3) Test $Q(a,s,\\theta)$ also on the LQR task to show it's correctness\n(for example by sampling s from the initial state distribution and computing\nthe action). It may also be nice to test it in the zero-shot task as well.\n4) Perhaps test combinations of the various gradients, for example taking\nthe average of the $V(\\theta)$ gradient with the policy gradient using $Q$\n(i.e. taking the average of two equivalent policy gradients).\n\nIf the above points are convincingly done, I may increase to marginal\naccept. The current contributions are not enough for me to go higher than\nthat: taking away the proposed new policy gradients, the main contribution\nis to add $\\theta$ as an input to $V$ and $Q$, which I think is not enough.\nMoreover, the advantage of adding $\\theta$ as an input was not shown convincingly\nusing compelling evidence. Currently the most compelling evidence is the\nzero-shot task, which shows that there is some generalization happening in\nthe $\\theta$ space; however, what is missing to me, is a demonstration of how\nthis additional generalization helps in solving the original task in a more\ndata-efficient manner. Perhaps interleaving the policy search with longer\nsessions of off-line learning (without any interaction) using $dV/d\\theta$\nto take advantage of the generalization may improve the data-efficiency\nand show the advantage of the new method (exaplaining good practices on how\nto do this may be a useful contribution). I think it would also be important\nto show compelling evidence that including the s input helps in learning\nbetter $V$ and $Q$ functions. Perhaps there are also other ways to better\nshow the advantage of the method.\n\nAnother option may be to change the problem setup, so that the new policy gradient theorems would be more sound. For example, using the original formulation of Degris'12 where $d^{\\pi_b}(s)$ is the limiting distribution as $t \\to \\infty$ would make the new policy gradients correct; however, the standard setup would not correspond to this. One setup that would correspond to this objective is the following: an infinite horizon continuing setting, where the agent is never reset into the initial distribution, but has to continually change the policy to improve. The learning would iterate between running one behavioral policy until it converges to its stationary distribution, then optimizing a new policy while in the off-policy setting, then switching the behavioral policy to this new policy, and repeating the process. In this situation, $d^{\\pi_b}(s)$ can be seen as the initial distribution for the new policy, and in this case the new policy gradient theorems would make sense. My previous argument about wanting the policy gradient theorem to be unbiased in the on-policy case would also be satisfied, because if $d(s)$ is stationary then \nthe $dQ/da\\*da/d\\theta$ and $dQ/d\\theta$ gradients would differ by only a constant factor.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}