{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The focus of the submission is shape-constrained regression, particularly the goal is to learn monotonic, 'reasonably rich' functions. In order to tackle this task, the authors extend the monotonic regression framework (Gupta et al., 2016) which scales less benignly in the input dimension. They propose to use lattice functions with parameters having Kronecker product structure (, and their ensembles). The resulting function class can be (i) stored and evaluated in linear time (Proposition 1), (ii) characterized / checked from monotonicity perspective (Proposition 2). The efficiency of the approach is demonstrated in three real-world examples.\n\nShape-constrained regression is a central topic in machine learning and statistics. The authors propose a parametric family to learn monotonically constrained functions. The storage and the evaluation of the resulting functions are both fast (linear), and the numerical experiments are encouraging. The submission can be of definite interest to the ICLR community. "
    },
    "Reviews": [
        {
            "title": "The paper provides a computationally efficient method for the modeling of monotonic functions. However, the authors should investigate various shape constraints, such as convexity, positivity, etc.",
            "review": "In this paper, the authors study statistical models based on the Kronecker-factored lattice (KFL) and investigated the condition that the monotonicity holds. The KFL provides efficient and flexible modeling for monotonic shape constraints. Furthermore, the ensemble of KFL is proposed, and its approximation ability is theoretically investigated. Some numerical experiments indicate that the KFL trains faster with fewer parameters with comparable prediction accuracy to existing methods. \n\nThis paper was assigned as an emergency review. So I did not have sufficient time to understand the content of this paper deeply. \nThe paper provides a computationally efficient method for the modeling of monotonic functions. However, the authors should investigate more variety of shape constraints such as convexity, positivity, etc.\n\nProposition 2 is an interesting result. However, more detailed studies on the shape would be necessary for the publication. For example, is it possible to derive the condition that the function is increasing in a variable and convex in the other variable?\n\nProposition 3 seems relatively straightforward. Supplementing untrivial statements would be good for readers to understand the significance of the proposition. \n\nIn the numerical experiments, the authors found that the hyperparameter V is important to tune the capacity of the proposed statistical model. Is there some theoretical support for that finding? Showing a clear insight would be excellent.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Faster model with the factorization",
            "review": "The paper proposes to use Kronecker factorization on a lattice for a monotonic lattice. Thanks to the factorization, the computational cost is improved to linear compared to exponential. The paper went on to extend to use an ensemble of KFLs due to each factorized lattice is too restricted. Compared with the baseline, monotonic KFL achieves good accuracy with much faster run times on the experiments.\n\nThe approach seems to strike a good balance between computation and representation of power by choosing an ensemble of factorized functions.\n\nI am not familiar with the motivation for monotonic functions and why the datasets/tasks such as query result matching requires the resulting function to satisfy monotonicity. For this reason, I am not sure about the significance of the work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Novel reparametrization of monotonic lattice regression which shows significant empirical gains. ",
            "review": "**Summary**\nThe authors propose KFL, an efficient reparametrization of monotonic lattice regression using Kronecker factorization. The goal is to achieve efficiency both in terms of computations and in terms of the number of parameters. The authors show that the proposed KFL has storage and computational costs that scale linearly in the number of input features. \nExperimental results show that KFL has better speed and storage space. The authors also provide necessary and sufficient conditions for a KFL model to be monotonic with respect to some features. \n\n**+ves**\n+ This appears to be a new parametrization of monotonic lattice regression with parametric and computational advanages\n+ Theoretical results show necessary and sufficient conditions for KFL to be monotonic, and this allows the design of training algorithms; there are also theoretical results on the capacity\n+ Experiments on public and proprietary datasets show that KFL maintains error rate while needing less time to train and fewer parameters. \n\n**Concerns**\n- The gains in training and eval time (especially compared to simplex) seem modest. However the number of parameters is significantly reduced. I wonder if a larger dataset/more complex task would demonstrate the benefits more clearly. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review Comment #1",
            "review": "(Added on 11/29/2020) **Post Rebuttal Comment**\n\nI thank the authors for sincerely replying my review comments. I think the authors answered my questions. \n\nAddtional Comments\n\n- Section 3.4: $(c_1(x[1]),...,c_D(x[D])$ â†’$(c_1(x[1]),...,c_D(x[D]))$ (Add a right parenthesis)\n\n---------------------------------------------------------\n\n**Review Summary**\n\nTheoretical claims about the condition about single KFL's monotonicity and the expressive power of the ensemble of KFL's are correct. Also, the ensemble of KFL demonstrates empirical performance comparable with existing methods with fewer computational complexities up to 24-dimensional feature vectors. In the experiments, this paper learns the monotonic function using the ensemble of KFL, whose weak learners are monotonic. However, I wonder whether it is theoretically justified (see \"Soundness of the claims\" section).\n\n**Summary of the Paper**\n\nThis paper proposed Kronecker-Factored Lattice (KFL) for learning monotonic functions, which is computationally efficient than the existing method in terms of input dimension. This paper derived a necessary and sufficient condition that KFL is monotonic, which is easy to check. This paper also proposed an ensemble of KFL and showed that its expressive power is sufficiently strong that any lattice-point-interpolating function when the number of base learners is sufficiently large. This paper applied the ensemble of KFL for learning monotonic function using three datasets and confirmed that the proposed methods performed comparably with the existing methods with few computational resources and time.\n\n**Claim**\n\nIf I understand correctly, the main claims of this paper are as follows. I assume them and evaluate the paper based on whether it supports them.\n- Claim 1: The computational and storage cost of the proposed method is efficient. (Contribution 1, 4)\n- Claim 2: We can use KFL to learn monotonic function (Contribution 2)\n- Claim 3: The ensemble of KFL is sufficiently expressive (Contribution 3)\n- Claim 4: The proposed method empirically performs well (Contribution 4)\n\n**Soundness of the claims**\n\nCan theory support the claim?\n- Claim 1 is supported by the discussion in the last paragraph of Section 2. The proposed method is $O(D)$ evaluation time and $O(\\sum_d \\mathcal{V}[d])$  parameters, which is typically linear in $D$.\n- Regarding Claim 2, the author proposed a training method (Section 3.3, Paragraph 3) of KFL that is justified by Proposition 2.\n- Claim 3 is supported by Proposition 3. This paper showed that the expressive power of the ensemble of $T$ KFL is strictly increasing in terms of $T$ up to some $T_0$. This paper also showed that the $T$ ensemble of KFL can represent any lattice-point-interpolating functino for any $T\\geq T_0$.\n- In the experiments, this paper learned monotonic functions using the ensemble of KFL by imposing monotonicity to base learners (i.e., KFL). However, if I understand correctly, the expressive power of the ensemble of monotonic KFL was not studied. It is true that any function in $\\mathcal{L}(\\mathcal{V})$ can represent the sum of KFLs, which are not necessarily monotonic. However, it is not known we can express the monotonic function using monotonic KFL's only.\n\nCan empirical evaluation support the claim?\n- Claim 1 is supported by the fact that the training time in Tables 3, 4, and 5 is reduced. Regarding storage cost, the number of parameters is reduced.\n- Claim 2, 3 do not have corresponding empirical support due to their theoretical nature.\n- Claim 4 is supported by the test accuracy in Tables 3, 4, and 5. At least we can observe clear improvement from multilinear and simplex methods by Gupta et al. (2016) when appropriately configuring  and $M$. However, from Figure 4--6, the ensemble of KFL does not perform well when $V$ and $M$ is small.\n\n**Significance and novelty**\n\nRelation to previous work (What is different from previous work)?\n- Gupta et al. (2016) also proposed a method for learning monotonic function using a function that interpolates lattice points linearly. They discussed the difference between the proposed method and Gupta et al. (2016). Specifically, Gupta et al. (2016) takes $O(2^D)$ time to evaluate a $D$-variate function and has $O(\\prod_d \\mathcal{V}[d])$ parameters, which is typically exponential in $D$. On the other hand, those of the proposed method are typically linear (Claim 1).\n\nNovelty\n- The idea of the proposed method is reasonable to me in that we employ Kronecker Factorization to reduce the computational and storage complexities and compensate for the reduced expressive power by ensembling. The idea of ensembling is also found in Canini et al. (2016); however, this paper's strength is that it backbones this idea by theoretical results (Proposition 2, 3), which are novel.\n\n**Correctness and Clarity**\n\nIs the theorem correct?\n- The proofs of propositions are correct, so far as I check.\n\nIs the experimental evaluation correct?\n- I do not find any inappropriate points in experimental settings.\n\nIs the experiment reproducible?\n- The implementation of the proposed method is made public. This paper used three datasets in experiments. One dataset (Adult Income) is made public, and the other two datasets are proprietary. So, it is hard for us to reproduce the same experiments using private datasets.\n\n**Clarity**\n\nCan I understand the main point of the paper easily?\n- Yes, I can understand the backgrounds of the proposed method explained in Section 2. The mathematical descriptions of this paper are clear.\n\nIs the organization of paper well?\n- Yes. I did not find any problem with the organization of the paper.\n\nAre figures and tables appropriately made?\n- Yes\n\n**Additional feedback**\n\n- Section 4, Paragraph 1: This paper says that for each monotonic feature $d$, $w^{(m)}_d$ is projected to satisfy the monotonicity constraints of Proposition 2 Condition 3. I think this projection operation is not obvious per se. So, I want this paper to clarify it.\n- Proposition 3: Is there a case in which $M$ is strictly smaller than the upepr bound $|\\mathcal{M}_\\mathcal{V}| /  \\max_i \\mathcal{V}_i$ ? It is merely my intuition but since $T$ can take an arbitrary tensor of size $\\mathcal{V}$, there might not be such a case.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}