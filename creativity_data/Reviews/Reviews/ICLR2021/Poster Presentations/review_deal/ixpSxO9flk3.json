{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors proposed to train an energy based model with a hierachical\nvariational approximations. The entropy can be tricky in hierarchical\nvariational approximations.  The authors suggest using the auxillary\nsamples to guide an importance samples to compute the gradient of the\nentropy. They evaluate their approach on a slew of models. The idea is\nstraightfoward and could potentially be applied to other hierarchical\nvariational models out side of the energy-based model setting.  The\nauthors were responsive and clarified many agressive questions. I'd\nask the authors to clean up two things\n\n- Equation 8 would be easier to follow if it kept the expectation from\n  equation 6 thereby making z_0 feel like it materialize out of thin\n  air\n\n\n- A more detailed discusion of when the proposal is good and what could\n  be missed out\twhen relying on the generating z to center the proposal"
    },
    "Reviews": [
        {
            "title": "An overall nice idea, but can be stronger with more comparison/dicussion with previous work.",
            "review": "This paper proposes a new method on training energy-based models with maximum likelihood. Instead of using MCMC approaches to sample from the EBM, authors follow previous work on training neural generators for faster sample generation. In particular, authors consider a special generator where the output is convolved with Gaussian noise. The score function of this generator can be estimated with self-normalized importance sampling, which is then used to estimate the entropy term through the reparameterization trick. Authors demonstrate that their new method is able to train EBMs efficiently, and improves the stability and performance of JEMs compared to MCMC-based training approaches.\n\n#### Pros\n* The method is more computationally efficient compared to MCMC-based approaches. As the title suggests, no iterative MCMC approaches are needed. Though the technique is inherently similar to Dieng et al. (2019), authors replaces the HMC sub-routine with a carefully designed variational approximation.\n\n* Experiments on JEMs are particularly interesting. Authors demonstrate that their method can train JEMs in a stable way and outperforms baselines on classification accuracy, sample quality, out-of-distribution detection and semi-supervised learning.\n\n* Writing is clear and easy to follow.\n\n#### Cons\n* From my perspective, the biggest disadvantage is related to various additional hyper-parameters. In VERA training, $\\gamma$ controls the gradient penalty and $\\lambda$ controls the contribution of the estimated entropy gradient. Both requires considerable tuning for optimal performance. However, typical MCMC-based approaches do not require gradient penalty, and tuning $\\lambda$ is unsatisfying — shouldn't $\\lambda \\equiv 1$ for real maximum likelihood training? \n\n* The idea of training a neural generator with the dual form of the likelihood objective has been explored before. Authors have compared with MEG, which uses a similar objective. I think adversarial dynamics embedding is also a necessary baseline, since it uses the same objective and has a special design of the neural generator to make entropy computing tractable. It would be better if authors will include it in both NICE and JEM experiments.\n\n* Authors cited (Song & Ermon, 2019a) when pointing out the difficulty of MI estimation. The reference below should also be included as it actually appeared earlier in 2018.\n\nMcAllester, David, and Karl Stratos. \"Formal limitations on the measurement of mutual information.\" International Conference on Artificial Intelligence and Statistics. 2020.\n\n* Authors did not include methods such as MEG in the JEM experiments. Any reason?\n\n----------------\nPost-rebuttal\n\nI appreciate the authors' response and additional comparison against previous work. I do think that proper comparison with previous work is important, as it allows us to know better when and where the proposed approach is beneficial. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An extension of previous works, but demonstrated well",
            "review": "**Summary**: This paper presents a method for improving training of energy-based models. Rather than drawing samples using persistent contrastive divergence / MCMC, this approach parameterizes a separate model, which is trained to directly output samples. This effectively adds an additional KL divergence to the objective. The authors use a particular form of sampling model (a latent Gaussian model), borrowing a few tricks for getting entropy estimates out of the model. Results are demonstrated on a few qualitative setups, but most of the results are centered on improved JEM on sample quality, out-of-distribution detection, and semi-supervised learning. The main benefit of the approach seems to be speed and stability, however, the authors also claim that minimal tuning is needed.\n\n**Strong Points**: Overall, the paper is fairly clear in its description of the background, method and experiments. Likewise, the tables, figures, and algorithm box provide clear demonstrations of key aspects of the approach. The descriptions in Eq. 6 are also useful for unfamiliar readers.\n\nFrom my understanding, the technical aspects of the paper appear to be largely correct. Like previous papers, the authors use an objective that includes both the energy function and the approximate sampler. This includes the entropy of the sampler (from the sampler’s KL). The authors opt for a Gaussian latent variable model as the sampler. Evaluating the entropy thus entails an integral over the posterior of this model. The key technical contribution of this paper is to approximate this integral not with MCMC sampling, but instead use an importance-weighted variational approximation. This involves drawing a sample from the prior, then sampling points from a Gaussian around this point.\n\nThe experimental results in the paper are highly thorough. The authors demonstrate their method in settings where exact log-likelihood estimates are feasible (a flow-based model and probabilistic PCA). This allows the authors to demonstrate both improved samples and log-likelihood performance. In these cases, the authors compare with several relevant baselines. In the main experiments of the paper, the authors incorporate their approximate sampling scheme into JEM, a recent hybrid discriminative/generative model. Experimental results demonstrate that using the approximate sampler generally results in (slight) improvements over JEM, with the exception of semi-supervised classification, where the proposed method is substantially better. In all cases, experiments are performed using standard, benchmark datasets, and the authors compare with competitive, recent methods.\n\nThere are substantial details on the experiments in the appendix. This, combined with the experiments across multiple datasets and settings, suggests that these results are reproducible. However, given the fact that this methods requires training multiple models, exactly reproducing the results will likely require the corresponding code (which the authors did not provide).\n\nWhile the method itself is not highly novel, the thorough experimental results somewhat compensate for this aspect. In particular, at least in the “toy” experiments, the authors compare with MEG, a relevant, related baseline. This demonstrates that, although the proposed method is a natural extension of previous works, the contribution is nevertheless useful.\n\n**Weak Points**: One weak point of this paper is with regard to novelty. The main contribution of this paper is in estimating the integral of the sampling model’s entropy with an importance-weighted variational approximation. This is a fairly natural extension of previous works on learning sampling models, simply making the estimation procedure more efficient. On its own, this is not a highly substantial contribution. It’s also not clear that this is the best set of design choices for this problem. For instance, would the issues with the entropy gradient not be obviated with an exact log-likelihood model, such as a flow-based model / autoregressive model?\n\nSome of the theoretical considerations of the setup appear to be skimmed over. The authors present the approximate sampling method with an equality in Eq. 4. However, in practice, the sampling model will not be able to reach the max, due to inherent limitations in the model class. Further, in Algorithm 1, the energy-based model and the sampling model are trained jointly, rather than training the sampling model inside of an inner loop (for the max operation). This is analogous to amortized inference in VAEs, where training the encoder and decoder jointly typically implies that the encoder cannot fully maximize the ELBO for the decoder. This phenomenon is referred to as “lagging inference networks,” (He et al., 2019) and the performance gap is the “amortization gap” (Cremer et al., 2018). Similarly, there will be a “sampling gap” here due to a “lagging sampling model.” While the method still seems to work well in practice, the fact that this is an approximation is almost entirely missing from the paper. Note that, to generate samples, the authors do indeed run additional MCMC steps (see appendix).\n\nFinally, although the experiments are comprehensive, it’s not clear whether the improvements over JEM are highly substantial. In addition, in the case with the largest gap, semi-supervised learning, it’s unclear why the proposed method, VERA, should dramatically outperform JEM. Given that this is the most substantial result, it seems as though further investigation should be performed to analyze the reason for the boost in performance. Further, the experiments on JEM only compare the baseline models (which uses MCMC) with the proposed (amortized) method. A more complete set of experiments would also include other learned sampling methods, such as MEG.\n\n**Accept / Reject**: Although the method is not highly novel, the experiments are fairly comprehensive in demonstrating the proposed method. Analyses are performed in multiple settings, across multiple datasets, comparing with the relevant baselines. At the very least, this paper demonstrates an efficient method that seems to outperform previous methods in simpler tasks and improve JEM on more difficult tasks. For these reasons, I would be in favor of accepting this paper. With a more detailed motivation/explanation of the design choices, discussion of the quality of the approximation, and some additional sampling model baselines on the JEM results, this paper could be further improved.\n\n**Questions**:\n\nCould you elaborate on the issue with the score function estimator in Eq. 6? Is this simply a matter of having a high-variance estimator?\n\nIn the importance weights, should $q_\\phi (x)$ be included in the denominator?\n\nIn Algorithm 1, are you using a mini-batch of data examples but only one sample from the model. If so, why not use a batch of samples?\n\nThe MNIST samples don’t look great, even for VERA. Why is this the case?\n\nIs using NICE in Section 5.1 representative of energy-based models? Flow-based models tend to have their own issues associated with stability.\n\nWhy is VERA better at semi-supervised learning as compared with JEM?\n\n**Additional Feedback**:\n\nMethod:  \nI would not compare Eq. 5 with a GAN decoder, as GANs define an implicit density.  \n“or simple a diagonal Gaussian” —> “or simply a diagonal Gaussian”\n“to the gradient of our model’s likelihood…”: should specify that this is the EBM.  \n\nEBM Training Experiments: \nI would put zeros in front of all decimals to be consistent.  \nNot sure if it’s fair to conclude that entropy regularization is unnecessary for preventing mode collapse from this experiment. This seems like a limited setting.  \n\nSection A.2:  \n“Equation equation 7” —> “Equation 7”\n\nFigure 7:  \n“Unconditional CIFAR10 Samples” —> “Unconditional MNIST Samples”",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An improved algorithm to train EBM-based models",
            "review": "This paper proposes an improved algorithm to train EBM-based models, called Variational Entropy Regularized Approximate maximum likelihood. The basic idea is to formulate the intractable partition function as an optimization problem with an additional entropy term. To estimate the gradient of the entropy term, the authors then propose a variational formulation for approximation.\n\nThe idea is interesting but not very exciting, and there consists of technical details that need to be carefully deal with. One of my biggest concerns is that since the algorithm relies on importance sampling to estimate the gradient of log marginal likelihood, the variance can be arbitrarily large. Specifically, the importance weight is the ration between a joint distribution and a proposed variational distribution \\xi. One can see that a sample from \\xi is a zero-mean Gaussian distribution, whereas q(z, x) should not. I suspect this would make the ratios to be vary unstable with high variance, which seems to contradict with the goal of the paper. In other words, I am not convinced why the importance sampling based method can work better than existing methods.\n\nIn addition, this importance sampling based method is introduced to avoid sampling the posterior distribution with HMC. I think this step can be considered as an efficient approximation to the  method, I am expecting HMC should perform at least as good as the proposed method but less efficient. The results in Table 2 seems to agree with this (although not completely). I wonder how is the running time comparison? Also, since HMC based method is a competitive method, why don't you consider this in other experiments such as those in Section 6?\n\nAlso, since the proposed method is claimed to outperform the recent SSM method. I think the same experiments as in the SSM paper should be conducted for comparison.\n\nBTW, the generative images in Appendix are too small to be informative.\n\n=========After rebuttal======\nAfter the rebuttal, my main concern remains. Specifically, the paper defines a variational distribution q(z|x0 via a hierarchical construction: z_0 ~ N(0, 1), z ~ N(z_0, \\eta I), which is essentially a zero-mean Gaussian. And I suspect the this is a bad variational distribution and it will induce high variance. The author said they didn't the hierarchical construction to define the variational distribution, because they fix z_0 after sampling. I don't think this is a formal way of defining a variational distribution. One reason is that even if they fix z_0, the proposal distribution will be a z_0-mean Gaussian, and the mean is randomly drawn from N(0, 1), which will not match the true posterior distribution (they only optimize the variance parameter). I think this should be make clear and investigated in more details. I will keep my initial score.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}