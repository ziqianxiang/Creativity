{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors present a study where they investigate whether meta-learning techniques leverage the underlying task distribution. To do so, the authors come up with two conditions, in the first they generate tasks using a grammar and in the second condition, which is the null condition essentially, the tasks have the same statistical properties as the compositional task but they are not derived from a simple grammar. The authors find that while humans are better in the compositional condition, models are better in the null condition.\n\nAll reviewers have been positive with this work, but some concerns were raised regarding clarity around the use of some terms, such as compositionality. The rebuttal period has been very productive and the reviewers have acknowledged the improvements on the manuscript. \n\nAll in all, I think this is a good study to appear on ICLR and I believe researchers would benefit from the design of the study that will perhaps open new opportunities around careful evaluation of meta-learning agents."
    },
    "Reviews": [
        {
            "title": "Some nice work, but contains overly broad claims and interpretations",
            "review": "This work is an exploration of model behaviour upon meta-learning tasks with compositional structure. The authors discover that, unlike humans, machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks, and hence cannot match the performance of humans. Conversely, when the task is structured to leverage other statistical patterns, models do well. \n\nTaken as a whole, this is a nice piece of work. The presentation is well crafted, and I believe the experiments are well planned. There are many nice analyses and some welcome statistics, such as shown in Figure 3. The authors are commended for their work. I wish to lay out a few criticisms, and I'd like the authors to know that the points are all very easily fixable. \n\nThe authors design a set of structure-learning tasks using a generative grammar. The exact details of the grammar are not given, and the reader is to rely on rough intuitions based on the figures. I encourage the authors to spell out some more details of the methods.  \n\nThe authors argue that the non-compositional boards could not have been generated by the defined grammar, which seems fair, but they also argue that these boards are necessarily non-compositional. I am having a lot of trouble with this statement, because it is not entirely clear by what the authors mean by compositional, as it hasn't been clearly defined. This is a particular problem in the machine learning field as a whole, as it pertains to research on compositionality; rarely is the term defined in any rigorous sense, and from paper to paper there are seemingly different definitions. I encourage the authors to clearly explain what entails compositionality as they refer to it here, and to explain what makes their non-compositional boards non-compositional according to their definition. The \"non-compositional\" boards might not match the generative grammar as used in the compositional setting, but it does not entail that there does not exist a compositional grammar that can produce the non-compositional boards. In fact, due to the discrete, simplified nature of the game, it would seem almost certainly true that there exists *some* generative grammar that can produce the non-compositional boards seen here. It might not be a \"simple\", \"interesting\", or human interpretable one, but it would nonetheless be a grammar, and would be compositional. This fact makes it all the more important to define compositionality.\n\nIt seems to me that what is more precisely being illustrated is the ability for humans to perceive, and infer the implications of abstract, \"simple\" *structures*, and not compositional rules or grammars per se. Without fully defining compositionality and establishing the non-compositionality in the null setting, I'm afraid that the results are misstated and misrepresented. A note to the authors: I'm fully aware that I could be misunderstanding how the MLP+Gibbs sampling method here can entail non-compositionality, and am more than open to being corrected on this matter. I look forward to a discussion in the rebuttal. \n\nThe previous point leads me to a broader point about the background presented throughout. The authors include many broad, and quite bold statements regarding humans, and how they learn, especially in regards to their capacity for \"learning compositionality\". It is claimed that humans learn rapidly, with very few samples, which is contrasted with machine models that require an enormous amount of data. This is true in some superficial sense, but does not account for the the entire evolutionary trajectory that produced humans; indeed, humans at birth are not blank slates to the degree that randomly initialized neural networks are. It's also claimed that humans learn compositional representations. While this idea is certainly in vogue in some circles in cognitive science, it is certainly not widely agreed upon. I'm not even sure how such a strong statement can be proven. The citations given point to a cople computational modeling papers, which, in my opinion, insufficiently corroborate such a claim. Moreover, it's not even clear how many non-artificial pieces of data that humans deal with are even truly compositional (for a taste of the issues and controversy surrounding one particular example --- language --- see the following entry: https://plato.stanford.edu/entries/compositionality/). Please note that this is not at all to say the views presented in this paper are necessarily false. I merely suggest that the authors take another pass at their writing, and tame a few of the broader, bolder claims about the nature of human learning, because it's not clear that they are necessarily true, either. \n\nI believe the authors have missed out on some possible interpretations to their results. They claim that a meta-learned model cannot learn the compositional structure of the tasks, since meta-learning is insufficient to establish the inductive biases required for compositional understanding. However, the inductive bias of a model is determined by more than its parameters (which in this case, are established via meta-learning). The functions a model comes to learn are also dependent on the nature of the computations, manifest through the architecture, and other such things. The authors are well aware of this, since they include a condition wherein the model uses convolutions. But the fact that the convolutional model does better entails that there might exist further architectural variants that do even better than it, and potentially, better than humans. If this were the case, then we'd no longer be able to claim that meta-learning cannot establish the proper inductive bias for compositional understanding. Therefore, the existence of a gradient in model performances warrants more careful wording in regards to the claims; we cannot so broadly categorize meta-learning as insufficient for establishing the right inductive biases for compositional understading without caveating according to the other sources of inductive bias. One piece of proof that the humans understand the compositional structure of the task is that they improve over the course of the task. But could this not simply reflect the fact that humans have a better capacity to improve behaviour over short time periods? In other words, how do we know that the problem with the models doesn't have to do with, say, working memory, rather than compositional understanding per se?\n\nAltogether, this is a well put together paper. There is a lot of interesting work here, and the authors have done well to explore various facets of the setup. My main criticisms have to do with the way the work is pitched, and the way the results are interpreted. It is very much presented with a veneer that speaks to a very particular crowd in cognitive-science inspired machine learning community. But since the views in this community are not necessarily broadly shared, many of the statements, interpretations, and claims come across as quite strong and not fully corroborated. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting exploration that could have benefited from a more precise definition of terms",
            "review": "This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm, and compare the form of that bias to the inherent bias that humans have.    The authors point out, rightly, that meta-learning algorithms have meta-biases and it is important to understand these, from both the scientific and engineering perspectives.  It is well written and raises good questions.\n\nThere is a cleverly constructed test domain and a set of well-executed computer and human experiments (I think---I don't really know about how to construct a human experiment.)\n\nUnfortunately, I can't end up agreeing or disagreeing with the claims made in the paper, or really understands how well they are supported by evidence, because I find that they use terms that don't seem to be sufficiently technically well defined.\n\nFor example:\n- what exactly is compositional structure?  \n- what is statistical structure?\n- what is your measure of task complexity?\n\nHow can we tell if what the agent learns is compositional?  Is that an externally measurable property of the agent's behavior and the way it generalizes to new environments?  Or is it a property of the internal representation?   (It is common to have an intuition that \"compositional\" also implies \"compact\" or \"low complexity\" in some sense.)\n\nIt feels like generalization be a way to get more clearly at the presence of a compositional representation:  could you train on small grids and have the learned agent generalize to big ones?  It seems like if a fixed-size representation can generalize to very large instances, then that is more clear evidence of compositionality (but then I'm thinking of compositionality as a property of a representation, not of externally-measurable behavior.)\n\nI also feel that I don't quite understand the meta-learning training regime.  What exactly constituted a \"task\" from the meta-learning perspective?   Is it a single board?  If so, then the meta-learning problem is to learn the task distribution, in some sense.   I was expecting something more \"meta\":  that is, to test whether the system is actually meta-learning the *idea* of compositionality, it seems like set-up would be that a task corresponds to a particular grammar with multiple boards drawn from the distribution induced by the grammar;  then we'd know that it had meta-learned compositionality if it could learn *new grammars* quickly.\n\nSmaller points\n- I didn't completely understand the production rule (nor the examples) for the loop structure.\n- It would help me understand the task set better if there were a slightly more in-depth description of the chains, trees, and loops and described how the grammar generates the compositional tasks in figure 2. Is it not the case that every connected configuration of red tiles could be described as a tree?\n- Rather than showing just one number for the final performance, It would be helpful to show learning curves for the RL algorithms so the reader can assess the stability, convergence, etc. Similarly, learning curves for humans would be interesting, but less important since I assume they just look flat.\n- \"is develop\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting work",
            "review": "*Summarize what the paper claims to contribute. Be positive and generous.*\n\nThis paper offers an interesting comparison between humans and a meta-learning algorithm [1,2] learning to uncover structured patterns on a 7x7 grid. The patterns are either generated using simple “compositional rules” (lines, loops or trees) or sampled from a distribution that has almost identical 0th, 1st and 2nd order statistics as the ones generated through the “compositional rules”. The board is initially covered, and the agent (either a human or a RL meta-learning algorithm) is tasked with selecting one by one which tiles are to be uncovered. The aim is to guess which tiles are covered by the pattern (red tiles) and avoid the background (blue tiles), thus uncovering the red tiles as accurately as possible. The authors go on to show supporting evidence that most likely, humans are using a compositional inductive bias when trying to uncover the hidden pattern, whereas the meta-learning algorithms do not. This conclusion is arrived at using the relative performance of both humans and the algorithm on the cases where the pattern was generated using the two different schemes. They then go on to conclude that there is a strong difference between the strategy learned by the algorithm, which seems to use mostly the statistics, compared to humans that seem to make use of a compositional inductive bias.\n\n\n[1] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl^2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n[2] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.\n\n\n*List strong and weak points of the paper. Be as comprehensive as possible.*\n\n**Strong points**\n\n- The paper is very clear and very well written. I like the simplicity of the experiment and the approach taken by the authors in trying to uncover whether humans and a particular algorithm follow similar strategies when solving tasks.\n- I see no reason why this approach should not be re-used by other researchers, and I hope that it will inspire others to go to similar lengths when analysing the operation of their algorithms. Specifically, I consider this a strong point of the paper as I believe it offers an obvious and accessible set of future work.\n\n**Weak points**\n\n- It is not completely clear how the statistically similar patterns are constructed. Is it possible to get patterns that are identical to the compositionally generated ones? If so, have you checked and removed these? Please elaborate a bit more on this aspect, show more examples of the non-compositional patterns, and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C.\n- One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se, but rather in the manner through which they were generated. This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans. My point does not mean to invalidate the findings and conclusion, but rather to emphasise that perhaps we shouldn’t be looking at this type of experiments, as they are “doomed to fail”. Of course this might be obvious in hindsight, but I would be interested to read what the authors think about this.\n\n\n*Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. Provide supporting arguments for your recommendation.*\n\nI recommend accepting the paper, mainly for the two strong points earlier. I believe what the length at which authors have gone to for understanding what an algorithm does (or what it doesn’t do) should be an example for our field, which often lacks imagination when it comes to analysing proposed models. I do think that a condition for accepting is at least ensuring that the details of all the generated samples (compositional or not) are included in the paper. How many (unique) of each were generated/used? How did you decide the compositional-passing/not compositional passing split. Did you check whether any non-compositional pattern matched the compositional one? If so, how and did you reject that sample?\n\n*Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.*\n\n- In the intro: “Second, humans represent this learned information compositionally”. In my opinion this is a very strong statement about the nature of the representation in the human brain and it’s best avoided unless there’s neurophysiological evidence. Consider rephrasing to a softer version with a reference.\n- 4th paragraph in the introduction: “Our methodological contribution in this work is *to* develop novel tasks.”\n- Last paragraph of introduction: “Since a large swath of real-world tasks contain compositional structure..” Which ones? Please include some examples. Also, is it that they contain compositional structure (i.e. in the way they are generated) or that the compositional inductive bias that humans exhibit can more efficiently solve them?\n- Second sentence of 3.2 Results: “We demonstrate that humans have a clear bias toward compositional distributions, without extensive training and even while directly controlling for statistical complexity.” Couldn’t we claim that the patterns generated by the compositional distributions are easier/simpler? They are after all only 3 different rules. Wouldn’t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones? You did touch upon the issue of spatial proximity later on. Can you touch upon this point too? I think it’s a possible (and perhaps simpler) alternative explanation of why humans would be better at them.\n- Appendix A.1. The hyperparameters are reported in an unnecessary accuracy - I would assume that results are not sensitive to that many significant figures. Please consider 2 or 3 s.f. in scientific notation for simplicity.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting tasks, but model limitations affect the conclusions",
            "review": "Post revision update\n-------------------------------\n\nThe authors have been very helpful and addressed many of my concerns, and I think the revised paper is a substantial improvement. I have rated the paper as a 7, although I do have some lingering concerns.\n\nMost crucially, it's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior, since, for example, both models significantly outperform humans at the tree rule. However, the authors do point out that humans perform better at these tasks than the null distribution. Still, I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ. However, I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard, at least future researchers will be able to see the full pattern of results to draw their own conclusions.\n\nOriginal Review\n---------------------------\n\nThis paper proposes to explore whether meta-learning approaches can exploit a compositional structure in their tasks to generalize, and compares this ability to humans. To do so, the paper introduces a grid dataset consisting of generative grammars for generating compositional grids, as well as a null task distribution which is non-compositional but matches on certain low-order statistics. These tasks are interesting and new. They have humans and agents perform a task to reveal rewarding squares on the grid, and compare to agents that meta-learn this task. Human subjects perform better at the compositional distribution, whereas models perform better at the null distribution. They conclude that \"compositional structure remains difficult for these systems and that they prefer other statistical features\" and that this \"highlights the importance of endowing artificial systems with this bias.\" While the topic is timely, and the tasks are interesting, there are a number of limitations to the model and training which I think seriously limit the conclusions. I think that the task is really only compositional for a model which is able to fixate on different locations on the grid. Thus, I recommend rejecting for now, although I think a revision with a more sophisticated model and more thorough discussion could be a valuable contribution.\n\n\nStrengths:\n\n* Interactive tasks for humans and models are a great improvement over prior toy datasets on compositionality, e.g. SCAN.  \n\n* I think the tasks are very interesting, and offer directions that aren't really address by prior compositional generalization datasets that mostly rely on composition of words, or on composition of visual properties like color and shape.\n\n* It's great to see actual comparisons to human performance, and careful thinking about how to evaluate compositional generalization vs. other types (though this could be developed further, see below).\n\n\nAreas for improvement: \n\n* Is generalization equally good for humans under each rule type? If not, this might affect the conclusions. For example, perhaps humans would be very good at inferring chains, but not the more complex structures (like trees). If so, it could be that \"compositionality\" per se is not the construct underlying their performance, but rather \"chains\" or some other, simpler construct. Because performance is not presented broken down by structure type, it is difficult to determine whether some pattern like this could explain the results. Thus, it is difficult to conclude that compositionality is the factor underlying the results. \n\n* There are a number of features of the agent that confound the comparison with the humans. These limit the ability to draw a strong conclusion about e.g. \"the importance of endowing artificial systems with [a compositional] bias.\" \n    * Why are the main comparisons run using the non-convolutional model, when the convolutional one is clearly more closely matched to humans (as the discussion acknowledges)? It seems like most (although not all) the difference in performance is due to spatial bias, rather than compositionality. \n    * Indeed, this spatial bias is partially addressed on the input (by the convolutional experiments), but it is *never* addressed on the output. Humans know that there is a spatial structure to the tiles they are clicking on, the agent can access this information only implicitly.\n    * To address this, one could build a recurrent attention model (e.g. Mnih et al, 2014; Gregor et al, 2015) which can make visual saccades around the grid before deciding whether to reveal the square at the current point of fixation, or whether to fixate to another location. This would likely match the human process better, since the humans are likely fixating their gaze on the locations they are considering, rather than fixating in the center of the grid without moving their eyes. It's also motivated by the observation that agents generalize better if they receive ego-centric input rather than visual input fixed on the grid (c.f. Hill et al, 2020; Ye et al, 2020). This is an important issue to the claims at stake. For an agent which could fixate on each location it was considering, the compositional rules would be much more consistent than for an agent that perceives the whole grid from a fixed perspective. For a fixating agent, the compositional rules would also be much more consistent than the null distribution. In fact, I would suggest that it is *only from the perspective of a model which can fixate that this distribution can be considered compositional at all.* How can we tell that the difference between the humans and the model isn't due to the human ability to fixate, rather than some abstract bias toward compositionality?\n    * The paper may not be able to address all the ways in which the model's experience of the task is unlike humans, but then the there should be a *corresponding tempering of the conclusion that the comparison to humans says something specific about the difference between the model and humans.* That is, given the current experiments, the discussion of this paper should focus at least as much on the limitations of the present model as on general conclusions about failures of the model class and the need for additional inductive biases.\n\n* Furthermore, exploring compositionality in toy tasks can be misleading. Hill et al. (2020) show that compositional generalization is significantly improved in more realistic settings (for example an RL agent that executes actions over time achieves 100% compositional generalization on a task that a feed-forward classifier only achieves 80% generalization on). They argue that toy stimuli remove one of the most important elements for training deep models — the rich environments in which humans, also, are trained. Even if the input and output of the model and humans were better matched on this dataset, it may be misleading to conclude something as general as \"the importance of endowing artificial systems with this [compositional] bias\" without giving these models training on a distribution of stimuli and tasks that more closely match the rich variety which humans experience over development. Of course, it is not feasible in practice to do so (yet). But this limitation and its relevance to the conclusions should at least be acknowledged in the discussion. \n\n* The paper could use some more discussion of the distinction between statistical patterns and compositional rules. This seems like an important point, but it wasn't entirely clear to me. Compositional rules correspond to a certain statistical distribution over grids. The fact that up to 2nd order Ising statistics are matched does not mean that the distributions of outcomes are matched \"statistically,\" it merely means they match in certain low-order statistics. It's not clear if these are the right statistics by which to compare the distributions (especially for the non-convolutional model, which has no spatial awareness). The paper would be strengthened by justifying the choice of these statistics more carefully, and articulating a clear distinction between what counts as a \"statistical\" pattern vs. a rule. \n\n\n\n\n\nReferences \n---------\n\nGregor, Karol, et al. \"Draw: A recurrent neural network for image generation.\" arXiv preprint arXiv:1502.04623 (2015).\n\nHill, Felix, et al. \"Environmental drivers of systematicity and generalization in a situated agent.\" International Conference on Learning Representations, 2020.\n\nMnih, Volodymyr, Nicolas Heess, and Alex Graves. \"Recurrent models of visual attention.\" Advances in neural information processing systems. 2014.\n\nYe, Chang, et al. \"Rotation, Translation, and Cropping for Zero-Shot Generalization.\" arXiv preprint arXiv:2001.09908 (2020).\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}