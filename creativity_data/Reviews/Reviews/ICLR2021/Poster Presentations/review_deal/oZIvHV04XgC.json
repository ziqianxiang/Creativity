{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a new online contextualized few-shot learning setting, with two associated datasets (notably, including one obtained from trajectories within the real-world Matterport3D reconstructions). A simple recurrent contextualized extension of Prototypical Networks is also proposed as a stronger baseline, demonstrating the need for incorporating such context. The reviewers all agreed that this is an interesting setting combining continual and few-shot learning, offering a more realistic problem that mirrors those that might be encountered by embodied agents. The authors provided very detailed rebuttals, answering some of the questions and concerns raised by the reviewers. In the end, all reviewers agreed that this paper would contribute a significant novel setting, and so I recommend acceptance. I encourage the others to include modifications related to some of the comments, such as strengthening/clarifying the setting including metrics, details of the method, etc."
    },
    "Reviews": [
        {
            "title": "An interesting and new learning setting",
            "review": "#################################\n\nSummary:\n\nThe paper presented a new setting of online contextualized few shot learning to mimic human learning. This setting combines continual learning and few shot learning, and additionally considers context switch. Specifically, a learning method is presented with a sequence of samples that might come with labels. The method is then tasked to classify the current input into known categories, or recognize the input as belonging to a “new” category, while at the same time updating the model for known and new categories. Two new datasets (hand-written characters and indoor images) were constructed to support the learning setting. An extension of Prototypical Network (Snell et al.) was explored for this new setting. The results were compared against several baselines and were quite promising. \n\n#################################\n\nPros:\n\n* A novel setting of continual few-shot learning that considers context switching. The motivation is well articulated (naturalistic human learning). The setting has great potential to address some of the key challenges in AI (e.g., embodied vision, robotics, etc).\n* New datasets to support the proposed setting. Those dataset might facilitate future research in this direction.\n* The paper explored several baselines for the proposed setting, including an interesting extension of ProtoNet. The experiments are solid and the results are promising.  \n\n#################################\n\nCons:\n\n* The evaluation metric will need some thoughts\n\nProper evaluation metric is a critical component for the proposed learning setting. While the authors did provide a short description of the evaluation metrics (AP and N-Shot accuracy), those metrics lack some details and are not well justified. My understanding is that both metrics are accumulated from the starting to the current step and across all sequences. This was not particularly clear from the text. Also the definition of AP is different from standard average precision (the TP/TN/FP/FN definitions are different). Some more description is needed in the text. \n\nHow do these metrics capture catastrophic forgetting? For example, the accuracy / AP vs. the time interval between the current label and the last time the same label was observed.  \n\n\n#################################\n\nMinor comments:\n\nPage 3 paragraph 2: “focuses on more flexible ...” not a sentence. \n\nTable 1 and 2: Why the std of AP metric is not included?\n\nFigure 5 caption does not match the layout. Is this 1-shot accuracy? I did not find the description in the paper. \n\n#################################\n\nJustification for score:\n\nA good paper proposing an interesting learning paradigm. I’d expect some more discussion of the evaluation metric. Otherwise, I am happy to accept the paper. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New realistic learning paradigm",
            "review": "Summary:\n\nThis work aims to make a realistic learning setting by combining few-shot learning and continual learning in the online setting. Similar to few-shot learning, the model needs to adapt to new classes with a few samples (at least in the beginning). Similar to continual learning, the model needs to learn new classes over time while being tested on the older classes as well. When encountering a new class, the model is expected to recognize that. Similar to the online setting, model evaluation happens on each trial, after which the model can be updated with that data (labeled or unlabeled). This new paradigm is called Online Contextualize Few-Shot Learning.\n\nThe authors recognize the importance of spatio-temporal context in human learning. Building on this, they propose a new dataset, RoamingRooms, that incorporates such context. The authors propose a new method, Contextual Prototypical Memory, to tackle this problem. It makes use of an RNN to encode contextual information and a prototype memory to remember previously learned classes.\n\nPros:\n1. The new setting proposed, Online Contextualized Few-Shot Learning, is a very realistic setting. Few-shot learning misses that classes are repeated over time. Continual learning misses that new classes are not processed and learnt in groups. This new paradigm combines the two settings and improves on their shortcomings to make it more realistic. Additionally, this is all done in an online setting.\n2. The use of spatio-temporal context in creating the dataset and the model is realistic and interesting.\n3. The proposed model is simple, with components added specifically to make use of the additional information in the dataset (the RNN) or to output additional information required for the task (the new class detection branch).\n\nCons:\n1. Authors mention that there exist some datasets under a very similar setting, namely CORe50, OpenLORIS, and synthetic task sequences of Omniglot and Tiered-ImageNet. If something similar does exist, the authors should report numbers on these datasets rather than RoamingOmniglot.\n2. Authors mention that [1] proposed a model for a setting very similar to Online Contextualized Few-Shot Learning. Even if this method detects new classes by thresholding the probabilities for novel class detection, it should be used as a baseline method.\n3. Clever fine-tuning is competitive, if not the state-of-the-art, for continual learning [2] and few-shot learning [3]. How does this perform as a baseline?\n4. Average precision is used as the metric for this setting. What about the maximum F-1 scores?\n\nNotes:\n1. There is a lot going on in this paper. The writing can be made less redundant and more to the point to incorporate more details.\n\n[1] Massimo Caccia et al. Online Fast Adaptation and Knowledge Accumulation: A New Approach to Continual Learning.\n[2] Hang Qi et al. Low-Shot Learning with Imprinted Weights.\n[3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel problem formulation with a more realistic evaluation setting, a novel data set, and a new method that works well in this setting",
            "review": "############## Summary ##############\n\nThis paper presents a new definition of the continual learning problem which attempts to bridge the gap between artificial settings typically used to evaluate continual learning and the way the real world requires us humans to perform. Concretely, this setting involves changing environments, where the agent is expected to autonomously detect when new classes are encountered. The changes in the environment include both spatial and temporal cues to enable the agent to differentiate between environments. Additionally, this work creates a new data set and adapts an existing one to this novel setting, and provides a method that can deal with it, as validated empirically.\n\n############## Strengths ##############\n\n1. The provided evaluation setting is close to what we would want in a continual learning setting.\n2. The provided data sets are potentially useful for evaluating future algorithms.\n3. The presented method is effective in handling this challenging evaluation setting.\n\n############## Weaknesses ##############\n\n1. While the evaluation setting is closer to a human setting, it appears that there is a pre-training stage where the agent prepares for that evaluation setting, which is not described in detail.\n2. The techniques used to create the Roaming version of Omniglot could potentially be applied to different existing data sets, but this is not described in the paper, limiting the usefulness of it.\n3. Since the authors do not create additional benchmarks following this procedure, the evaluation is only on two data sets, which limits the reader's ability to assess the benefits of the proposed approach.\n\n############## Recommendation ##############\n\nI recommend this paper for acceptance. I believe that problem settings that become increasingly realistic is a valuable direction of work, and although this manuscript perhaps oversells how realistic their proposed setting is, I believe it is still a step in the right direction. The introduction of a novel data set on Matterport3D could be impactful to future researchers working in this field, and provides a more realistic benchmark than many existing ones. Finally, the proposed approach is novel and effective.\n\n\n############## Arguments ##############\n\nThe main contribution of this work is to propose a novel problem formulation for continual few-shot learning, which is closer to realistic continual learning. I believe this in itself is interesting. The evaluation setting requires the agent to automatically detect when new classes are encountered, and simultaneously classify objects into existing classes. However, from the experimental section, it appears that the authors' proposed method requires a pre-training phase, where the agent encounters many evaluation scenarios like this one, in order to perform well in the final evaluation scenario. This is never explicitly stated or explained in detail, but if it is the case, it is highly unrealistic, as it requires the agent to essentially live multiple lives, before being able to perform well in a \"test\" life. I would encourage the authors to explain this in more detail, and more honestly assess how realistic their problem formulation is. Even if the pre-training phase is unrealistic, the evaluation phase is still realistic, which is valuable. This should be explicitly stated to make the contributions of the paper clearer. \n\nI very much liked the proposed RoamingRooms data set, and I also believe that to be a valuable contribution. The fact that a similar data set can be created from Omniglot interesting. Would it be possible to create \"Roaming\" versions of other synthetic benchmarks? What are the limitations for this? Since we typically expect continual learning methods to be evaluated in various benchmarks, it would be valuable to see diverse benchmarks for this new setting, or at least the details for how to create them. This could also enable a more comprehensive evaluation of the proposed approach in this submission.\n\nThe idea of using the recurrent net to output the thresholds used to make decisions of when to detect instances as novel is quite interesting, but I was left lacking an intuitive description of what this should do and how.\n\nIn terms of the empirical results, I believe them to demonstrate a substantial margin of improvement with respect to baselines, and the set of baselines to be sufficient. The ablative tests show the usefulness of each part of the proposed approach.\n\n############## Additional feedback ##############\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nIntro\n- What is a trial?\n- Hyperref to Figure 4 leads to Section 4.\n- This section states that comparisons will be made against few-shot baslines, which made me think that no comparisons would be made against continual learning baselines. Please clarify that you indeed compare against continual learning baselines.\n\nSec 2\n- Last paragraph of FSL has an incomplete sentence, and it remains unclear how Caccia et al. (2020) compares to the submission. Since this is clearly the most closely related work (as evidenced in Table 6), it is very important to make this comparison as complete as possible.\n- Omniglotor --> Omniglot or\n- Very comprehensive and well-written related work section, clearly outlining various lines of related work, including recent (and even concurrent) efforts that handle very similar settings to this one.\n\nSec 3\n- Figure hyperrefs point to captions (below the figure).\n- At this point, the problem formulation should clearly state how training will be carried out. This section does not mention the fact that the agent will first encounter various evaluation sequences and use those to generalize to unseen evaluation sequences. It mentions multiple few-shot sequences and train/val/test splits, but it is largely unclear at this point how these various sequences are supposed to be used.\n- The way the problem setting is presented, it makes it hard to differentiate it substantially from few-shot or continual learning, since it mainly focus on evaluation being done continually. It seems like a crucial part of the distinction is that the agent is evaluated even on unseen classes.\n    - This seems to be closely connected to work on open world and open set learning. Could the authors provide some guidance into how it compares to that setting? [1]\n\nSec 4\n- When doing online average, the method seems to assume that the current prototype p_{t-1} is fixed. But, if the model parameters change over time, which I expect they do in a continual learning setting, this prototype would be different if recomputed. How is this taken this into account?\n\nSec 5\n- I would've liked to see the ablation tests on RoamingRooms as well.\n- My only concern is that the evaluations are only on two different data sets. \n- In terms of the baselines, it is unclear which can leverage previous sequences to learn the new ones. I believe this is only possible with OML and variants, whereas all the other methods can only leverage data from the current sequence. \n\n\nAppendices\n- Thanks for providing lots of details on the experimental setting for baselines.\n- The additional visualizations are useful, especially the one of the learned thresholds over time (Figure 9).\n- These appendices should be referred to in the main text so the reader knows to look for them. Same for the provided videos.\n\n\n[1] Geng, C., Huang, S. J., & Chen, S. (2020). Recent advances in open set recognition: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting continual learning paradigm with few shot learning ",
            "review": "The paper proposes a new learning paradigm that combines both few-shot learning(FSL)  and continual learning (CL) to provide a more realistic learning environment rather than the traditional train-test-retrain approach in FSL. Two environments are proposed, along with a novel dataset. The evaluation seems to be thorough, with strong baselines (conventional approaches adapted to the proposed setting). A novel approach is proposed based on augmenting ProtoNets with contextual memory and is shown to have consistently strong performance compared to the baselines on both tasks.\n\nStrengths:\n+ The paper is very well written and reads very nicely. I particularly liked the motivation for the task.\n+ The use of contextual memory (incorporating both spatial and temporal context) is very interesting and is a promising approach for both FSL as well as a general learning architecture for visual event perception.\nConcerns:\n- While very excited by the potential of the proposed learning environment, I am a bit confused about how the actual implementation/evaluation maps to the motivation. For example, from what I can see, all baselines (including the proposed model) have access to the number of classes that are present in the data (k). A softmax-based decision function forces the model to choose one of these classes, either based on some online learning-based features or just through storing examples. Now, the premise (that of knowing when and what to learn) is not quite satisfied here and thus the metric (Average Precision) doesn't quite capture the entire picture. I think a better metric, IMHO, would be the harmonic mean of novel class detection and known class prediction. This would allow us to actually ascertain whether the models are learning novel instances and not just getting \"lucky\".\n- The baselines that are chosen are classic approaches to few-shot learning. Not many continual learning approaches are tested and I think that would make for a better comparison. Again, the actual learning setting is quite a bit more simplified than what is claimed.\n\nOverall, I think it is a very nicely written paper with some issues with evaluation settings that might be exaggerating the performance of baselines.\n\n==== Post discussion Update ====\n\nI am updating my score to accept after the discussion.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}