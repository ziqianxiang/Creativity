{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a framework for joint differentiable simulation of physics and image formation for inverse problems. It brings together ideas from differentiable physics and differentiable rendering in a compelling framework."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "- Summary\n\nThis paper presents a framework for performing both differentiable physics simulations and differentiable rendering. This fully differentiable simulation and rendering pipeline is then employed to perform system identification tasks, directly from video frames, being able to match or outperform both visual-based and state-based baselines. Moreover, the potential of this framework to be applied for visuomotor control is also demonstrated.\n \n\n- Pros\n\nThis method unified advances in the differentiation of both physics simulation and rendering.\n\nThe experimental results demonstrate a good ability to perform system identification for diverse parameters and control directly from videos.\nThe ability to identify parameters or direct control tasks directly from images is useful, since it reduces the need for direct supervision/annotation in the form of state information.\n\nThe presented simulator supports a variety of \"domains\", such as rigid and deformable body dynamics, cloth simulation, and these are efficient enough to be run faster than real time (at least for simple tasks).\n\n\n\n- Cons\n\nOverall, the proposed method is mostly a unification of pre-existing techniques from different fields, such as differentiable rigid and deformable body dynamics, differentiable rendering.\n\nThe paper itself admits that a limitation of this method is that it currently \"has limited capability to handle contact-rich motion that introduces a large number of discontinuities\", which limits its applicability to real-world scenes. It cannot also currently handle joints. All of these would be important for possible robotic applications, for example.\n\nThe tasks demonstrated in the experiments are simple, and issues from model mismatch does not seem to have been thoroughly evaluated (see comments below for more).\n\n\n\n- Reasons for score\n\n[Edit: Score updated, see discussion below]\n\nOverall, given the \"pros\" described above, notably the interesting results achieved for system identification and control directly from video frames by combining differentiable physics and rendering into a single framework, I recommend this paper for acceptance. Given some of the concerns raised in the \"cons\" and in more detail in the comments below, I for now will score this paper as a little above the acceptance threshold.\n\n \n\n- Additional comments\n\nThe scenarios used for the system identification and control tasks are fairly simple, with usually only a single object and few contact points.\nWas the ground truth for the scenarios in the system identification tasks generated using gradsim itself? If so, isn't it unfair that it is compared to other models (e.g., pybullet), for which there would be model mismatch? (While not mismatch would be present for gradsim)\n\nAlong the same direction, the experiments present a section on \"Impact of imperfect dynamics and rendering models\". It would also be interesting to see a quantification of the impact of model mismatch (possibly both while using the same renderer, i.e. only dynamics mismatch, or also different renderers)\n\nIn the experiments section, it is said that \"Inference ... is done by picking an initial guess of the mass (at random)\". From what distribution is this random initial guess picked from? What are these starting guesses in relation to the true parameters?\n\nThe section on \"Impact of shading and texture cues\" seems a little too short, which renders it hard to understand in detail what is going on.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting research direction that will spur follow up work despite preliminary nature of evaluation",
            "review": "This work presents a fully differentiable physics simulation coupled with neural rendering such that input video can be used to estimate object properties or find control policies to move those objects by trying to generate the same video at the output.\n\nThe paper is well motivated by presenting a natural progression of ideas from this literature and it does a thorough job discussing related work. The paper is light on details in section 3 and it is necessary to refer to the appendix to get a complete picture. Overall, the technical contribution is solid and thus worth accepting the paper even if the validation is with relatively simpler experiments since they are sufficient to motivate this direction to be further researched. Below are a few comments to aid in improving the current work:\n\n- All experiments use what I am guessing are input (desired) videos from the same pipeline and then later hiding some parameters (to be learned). While this is a good validation the learning done here is still 'in distribution'. It would be useful to see if video (even simplistic) from a different simulator or simplified from a real world video could be applied. To what extent is this possible and are there any fundamental limitations that prevent this at the moment?\n\n- Analysis is mostly with one object in an empty scene. Are there technical limitations to handling realistic scenes where there are multiple objects and those objects interact with each other as well the environment? How does this affect performance wrt forward and backward pass timings? With such experiments, it would be helpful to understand if the released code can be easily extended to such (more complex) settings or if someone would need to start a new implementation from scratch.\n\n- The scale on the loss landscape is quite small, '0.4 pixelwise mse'. How good does the initial guess need to be to stay in the range, do the curves in fig 3 continue the trend beyond these values for larger error?\n\n- Reality gap: while this is discuss in reference to visual appearances, since the current experiments deal with synthetic scenes, the more relevant topic to discuss is the reality gap wrt physics and object motions. Experiments designed to study this would boost confidence in this approach.\n\n\nOther comments:\n\n- How much does the performance depend on good initial guess?\n\n- Currently a single impulse is used to set things in motion, can this be extended to handle more continuous actions?\n\n- Presenting qualitative results for baselines would be helpful\n\n- Some baselines not clearly explained: average, random, ConvLSTM\n\n- How does performance scale with the length of the video?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "gradSim: Differentiable simulation for system identification and visuomotor control",
            "review": "This work focuses on the problem of estimating object physical properties from video sequences. \n\nThe proposed framework combines differentiable physical simulations and differentiable rendering to map physical parameters into images differentiably. This paradigm is then used to recover physical parameters from image sequences by means of gradient based optimisation.\n\nValidation of the proposed method is carried through two main synthetic applications, parameter identification and visuomotor control. \n\nAlthough the proposed approach still requires 3D ground truth information to yield reliable estimates, it is and encouraging step towards unsupervised physics understanding from image/video data. \n\nPositive:\n\n-Crucially and differently from previous attempts, the proposed approach does not require 3D supervision - except for geometry and appearance of the static scene (i.e. at t=0). \n\n-Approach is clever, simple and yields interpretable representation\n\n-First step towards physics understanding from videos\n\nNegative:\n\n- I would improve the quality of the visualisations and plots in the paper (e.g. I found Figure 6 impossible to read)\n\n-  How to differentiate through the physical simulator was not obvious to me. I would have appreciated a more detailed explanation of how that is done in practice for one of the physical problems studied in the paper to be included in the main manuscript, in an effort to make the paper more readable. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}