{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces two regularizers that are meant to improve out-of-domain robustness when used in the fine-tuning of pretrained transformers like BERT. Results with ANLI and Adversarial SQuAD are encouraging.\n\nPros:\n- New method with concrete improvements in several difficult task settings.\n- New framing of adversarial generalization.\n\nCons:\n- The ablations that are highlighted in the main paper body don't do a good job of isolating the specific new contributions. (Though the appendix provides enough detail that I'm satisfied that the main empirical contribution is sound.)\n- Reviewers found the theoretical motivation very difficult to follow in places."
    },
    "Reviews": [
        {
            "title": "not super convincing results. important ablations missing",
            "review": "## Summary:\nThe paper proposes two regularizers for finetuning pretrained mask LMs to improve the robustness of NLI and SQuAD models. When evaluated on adversarial NLI and SQuAD datasets, adding the regularizers to regular finetuning achieves a robust accuracy comparable to adversarial training baselines; adding the regularizers are added to adversarial training baselines archives extra robustness gains.\n\nThe first regularizer is an implementation of the Information Bottleneck principle specialized for contextual text representations. In the general IB objective, we seek to maximize the mutual information between the representation and the label, as well as minimizing the mutual information between the representation and the input. In this paper, the authors targets the token-level BERT representation. This design choice was not discussed in detail, but I assume it’s motivated by the observation that our models’ lack of robustness is often manifested in an overreliance on local, superficial features.\n\nThe second regularizer has a similar motivation. But instead of minimizing the mutual information between the input and the representation, the “anchoring feature” regularizer minimizes the mutual information between the global representation and the token level ones, specifically those that are “nonrobust and unuseful”. To identify “nonrobust and unuseful” tokens in each input, the authors use a heuristic based on input gradient norm, similar to how interpretability people generate heatmaps for text classification.\n\n## Detailed comments\nInterpretation of experimental results and choice of baselines: The abstract claims that “InfoBERT achieves state-of-the-art robust accuracy”. This is not accurate. The best numbers reported in this paper are achieved by applying InfoBert regularizers to FreeLB adversarial training. This can be seen as an ensemble of two (or three, if you count the two regularizers separately since they can be applied independent of each other) adversarial training methods. Ensembling usually helps robustness (see for example Tramèr et.al ICLR 2018 Ensemble Adversarial Training: Attacks and Defenses). For a fair comparison, FreeLB should be ensemble with another adversarial training method, or with FreeLB applied to a second model. When applied individually, the gain from InfoBERT has a much smaller advantage compared to the baselines.\n\nMissing evaluations and ablations: An obvious ablation is missing: apply the two regularizers separately. I’m especially curious if both lead to gains on top of FreeLB. The paper has a sort of disproportionate treatment of the two regularizers. Both theorem 3.1 and 3.2 are talking about the IB regularizer, while a lot of design choices for the anchoring feature regularizer are proposed without justification or verification, e.g. the portion of useful and robust tokens. The anchoring feature regularizer relies on various heuristics (definition of usefulness and robustness), and if it turns out to be the main contributing factor to InfoBERT, it would be good to know - if others would like to apply InfoBERT on other tasks, they might need to tune these hyperparameters.\nFormulation of the method: The authors cite the “localization” of the IB principle in the IB regularizer as part of the novelty of the method. However, this kind of localization can be found in e.g. Li & Eisner 2019: Specializing Word Embeddings (for Parsing) by Information Bottleneck (EMNLP), which is one of the first applications of IB principle to NLP with pretrained, contextualized representations. In the anchoring feature regularizer, the use of input gradient norm for filtering “nonrobust and unuseful” tokens is reminiscent of how interpretation methods generate saliency maps for text classification. Both are missing from the references.\n\nImplications and verification of theorem 3.2: this is a minor point but in my opinion theorem 3.2 is a bit of an overkill just to solidify the intuition that “the performance gap becomes closer when I(X_i, T_i) and I(X’_i, T’_i) decreases”. It would be nice to verify empirically through experiment that this theorem is correct.\n\nFinally, I encourage the authors to evaluate the method on more tasks and attacks, and especially focus on comparing against the naive adversarial training baseline. It would be good to have a better understanding of how much gain InforBERT brings and what are the most important factors.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "InfoBERT review",
            "review": "\n##########################################################################\n\nSummary:\n\nThe paper proposes a novel learning framework for robust (against adversarial attacks) fine-tuning of pre-trained language models, that is based on information theoretic arguments. It introduces two regularization mechanisms and investigates their efficacy on various tasks.\n\n##########################################################################\n\nReasons for score: \n\nOverall I vote for accept. The approach is novel, interesting and well presented. The theoretical results seem to be sound. It also seems to outperform competitors in the field of adversarial language models. Concerning the experiments some questions remain, but I hope that the authors will address them in the rebuttal.\n\n##########################################################################\nPros: \n\n1. The idea is interesting and well formulated. The theoretical results seem to be correct to me.\n\n2. The approach is tested on several standard datasets used in adversarial language models. It seems to outperform previous approaches.\n\n3. The paper is well written and clearly structured\n\n\n##########################################################################\n\nCons: \n\n\n1. In my view the experiments seem to show a tendency towards a slightly worse performance on the more difficult tasks in comparison to the competitor methods. Thus, the better overall performance on the ANLI data could be driven by the easier tasks.  \n\n\n2. I couldn't find a clear description of the \"global representation\" Z. A more explicit description would be helpful.\n\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n\nPlease address and clarify the cons above \n\n\n#########################################################################\n\nMinor comments: \n\n1. Is definition 3.1. a standard definition or is it introduced by the authors? \n\n2. Page 4, definition 3 contains an incomplete sentence (\"The q(x')....\").\n\n3. Page 6, \"Evaluation Metrics\": it should be stated witch argument is maximized.\n\n4. Page 16, Lemma A1: in the Proof of the lemma I think that all instances of Y_i should be replaced with T_i. In formula (13), in the rightmost term the token index n should be i-1.\n\n5. Page 17, formula (33): H(Y|Y) should probably be H(T|Y). Same goes for equ. (36).\n\n6. Squares might be missing in formulas (37) to (43).\n\n7. A reference to formula (44) would be nice.  \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Weak accept",
            "review": "This work (InfoBERT) proposes additional objectives for transformer finetuning to obtain models more robust to adversarial inputs. The authors first propose a mutual information based information bottleneck objective, next the authors propose an adversarial loss inspired method for identifying robust features and a subsequent objective to emphasize the mutual information between global representations and these robust features. The experiments demonstrate that InfoBERT consistently outperforms other adversarial training approaches on a variety of adversarial evaluations.\n\nI largely follow the logic behind the derivation, however I find some of the details unclear. I would like to see proofs for the theorems as well as an explanation of the assumptions under which the theorems hold. The experimental results are convincing, however there are no ablation studies to disentangle the performance contributions of the two proposed objectives. For the first point, the questions I have are as follows:\n\nfor equations 1-3, I find the integral notation to be a bit odd - isn't it common practice to put the dydt at the very end of the integral? Also you should consider leaving out punctuations from equations\nfor equation 5, why is there another 1/N inside the square brackets?\nwhy is equation 7 true in the general case? Suppose n=1, is this essentially saying that any sample from an empirical distribution would provide a lower bound for the true distribution?\nI'd like to see a proof for Theorems 3.1 and 3.2\nhow do the authors define stability and robustness? The manuscript talk about them in vague terms and they do not seem to be precisely defined\nhow does equation 9 follow from 6? Can you put in the intermediate steps? Also in this case what is N and what is M? And what happened to the multiplier n from equation 6?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}