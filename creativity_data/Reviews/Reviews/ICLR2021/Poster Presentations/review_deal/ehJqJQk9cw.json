{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a personalized federated learning method, which personalizes by computing a weighted combination of neighboring compatible models. Reviewers uniformly liked the quality of writing and level of novelty, and agree on the relevance of the problem and solution. The solution was deemed creative and particularly impactful in the important case of heterogeneous data on each node, and experiments showed convincing improvements. The discussion between reviewers and authors was constructive and has lead to further improvements of the paper. Slight concerns remained on privacy with all models stored on the server, and breath of personalized FL benchmarks used, but reviewers agreed the contributions overall are still significant enough. Future work remains on the theory of the proposed model."
    },
    "Reviews": [
        {
            "title": "Interesting approach, comparison with clustered FL approaches missing, insights limited",
            "review": "The paper proposes a new FL method that computes in every communication round for each client a personalized model as starting point for the next round of federation. The paper defines the client-specific objective as some loss function of the weighted combination of all (or subset) models on a client-specific validation set. This personalized weighted combination of the models especially fits situations where not all clients have congruent objectives such as in non-IID settings. The paper evaluates the proposed FL algorithm on standard datasets for image classification by comparing to alternative FL methods. \n\nPros:\n- The paper targets a relevant topic\n- The proposed approach also works in out-of-client-distribution settings\n\nCons:\n- The authors mention related clustered federated learning approaches (Sattler et al., 2020; Ghosh et al., 2020; Briggs et al., 2020; Mansour et al., 2020), but do not compare against any of these methods.\n\n- The main advantage of the proposed method is that it also works in out-of-client-distribution settings (otherwise the performance is comparable, see Table 1), i.e., when the target distribution which is not the same as their local training data distribution. However, this section is very short and the evaluation not very insightful. I recommend to more carefully investigate this property of the proposed method.\n\n- I would like to see some more theoretical analysis and insights. For instance, Sattler et al. 2020 proved that their clustered FL approach is able correctly identify the clustered (assuming that data was generated from K different distributions) if the empirical risk computed on the loss approaches the true risk. Can you give similar guarantees?\n\n- You should also report the communication overhead of your method and compare it with related approaches. Downloading multiple models definitely increases overhead. Methods such as the clustered FL (Sattler et al. 2020) allow to compute specialised models for free, whereas you need to download multiple copies of the model at every communication round. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to personalized federated learning. Some design choices ask for more analysis",
            "review": "EDIT: Based on the author's modifications to the manuscript, I have increased my score from 5 to 6.\n\nThis paper introduces an approach to personalized federated learning. After each communication round, nodes receive full models from other nodes. The nodes construct a weighted average of the received models so that the mix performs well on their local *validation* data. The first interesting design choice is an approximation to the node-local optimal weighting problem. The second contribution is a scheme that decides which local models to share among which nodes, avoiding excessive $O(K^2)$ communication of full models. The proposed model is experimentally demonstrated to be competitive or better than previous work on an extensive array of tasks. \n\nThe writing is generally clear. I did have the feeling that the abstract, introduction and related work sections were slightly repetitive and could be made more concise.\n\nThe setup is partially motivated by data privacy. It does not require any data to be shared between users. It does, however, requiring many full models to be exchanged. While there might definitely be scenarios in which this is reasonable, the large amount of communication could be prohibitive in many practical scenarios, and exchanging local models might still raise privacy concerns.\n\nAlthough I think the framework presented in this paper is interesting and potentially relevant, I do see shortcomings in the current manuscript:\n- The two contributions (the approximation to the optimal weighting problem + model exchange coordiation scheme to save bandwidth) could be better motivated. For the weight computation, it is not completely clear to me why this is called 'first-order'. What is this a first order approximation of? What is the effect of this approximation compared to the 'real' solution or other choices you may have considered? For the model exchange coordination scheme, I am missing some details in the description. I am not confident I could implement this scheme from the paper. I would also expect to see some form of ablation study here: what is the effect of varying the number of other models shared with nodes. What is the effect of the parameter $\\epsilon$, etc.'? From Table 1, it seems that sharing more models with workers (n=10) is not necessarily better than (n=5). \n- There seems to be no explicit scheme to keep the personal models of the nodes close together. I would like to see some experimental or theoretical analysis or insight into why the models don't drift apart until they cannot benefit from each other anymore. How do the weights w_n evolve over time?\n\n\nA few minor remarks:\n- In Eqn. 5, I wonder if the denominator is correct. Is it dividing by a vector?\n- Is division by zero never a problem in the proposed weighting normalization scheme?\n- I couldn't follow the sentence \"where clients (1) given a local objective clients their received models and\".\n- In \"from some distribution $D_j$ and local\" (notation section), I believe $D_j$ should be $D_i$.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting view of FL personalization, with some concerns",
            "review": "This paper proposed FOMO, a personalized FL framework. Comparing to existing methods that conduct local adaptation starting from a single global model, FOMO keeps track of all models, and personalize by computing the weighted version among all models.  The experimental results show improvement comparing to existing methods.\n\nThe following points from this paper are novel and interesting:\n\n(1) personalization via weighting multiple models is intuitively more stable, comparing to client-side gradient descent. Especially for non-iid settings, personalize from a set of models, offers more flexibility and robustness to personalize, than personalize from a single model. Moreover, assuming having a validation dataset, the client with multiple models can have an easy way to choose an existing model.\n\n(2) The method to approximate optimal w, is clean and shows interesting improvement. Although the experiments are not done on FL golden standards such as EMNIST/Shakespear, the experiments and reasonings are well presented and well conveyed.\n\nHowever, there are a few points that need to be taken care of:\n\n(1) FOMO assumes that the server keeps track of all clients' models,  this changes the classical assumption of FL: all model updates should be anonymous. This leads to privacy issues. The author addresses the privacy concern in section A.3, yet I am not satisfied. Here is the reason: (a) when any model is downloaded to the client, there is no scheme to avoid the client to analyze the model since all models share the same structure, only weights are different. With the other's model, a malicious client can do some harm, which needs differential privacy (DP) guarantees. (b) The DP approach adds noise to updated models, which could lead to a significant personalization performance drop, which hasn't been discussed in the paper, and DP is critical to ensure data privacy. In my opinion, FOMO introduces a powerful idea of downloading multiple models to improve personalization, yet introduces the problem of privacy.\n\n(2) Comparing to keep track of models of all clients (I am not satisfied with the claim in A.3), further research, should consider keeping track of multiple models, with anonymous FL updates. The clients can use multiple models to update, while since the models are updated via anonymous gradients, then no privacy issue will arise.\n\nI feel this is an interesting direction for FL personalization research, while the privacy issue is not properly addressed. I am expecting the authors to address the privacy part with some updated experiment. Given the above concerns, I set my score to 5. If the privacy issue is properly addressed I am glad to increase the score.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper addresses an important topic in federated learning which is personalization. However, more experiments/discussion to be added in regard to comparison with other methods.",
            "review": "**Paper Summary:** The paper addresses an important topic in federated learning which is personalization. The authors propose a two steps process to achieve the personalization: 1. Figuring out which models to send to which clients; 2. Computing their personalized weighted combinations for each client. To determine the weights, the authors use first order approximation. \n\n**Pros**: \n1. Personalized federated learning is an important problem, and I see a lot of value in practical applications of federated learning\n2. The proposed method is simple and intuitive\n**Cons**: \n1. I see strong claims in the paper such as 'we are the first to our knowledge to enable transfer to new specific data domains of interest through personalized FL. ' \n2. Some references are missing in the related work sections such as (Dinh et al. NeurIPS 2020), (Fallah et al. ArXiv 2020), and (Peterson et al. ArXiv 2019).\n\n**Areas to Improve**: \n1. I like to see more experiments to be added in regards to comparison with some recent works especially with (Dinh et al. NeurIPS 2020).\n2. Would be helpful to provide a convergence analysis of the proposed algorithm.\n\n**Missing References:**\n1. Personalized Federated Learning with Moreau Envelopes (Dinh et al. NeurIPS 2020)\n2. Private Federated Learning with Domain Adaptation (Peterson et al. ArXiv 2019)\n3. Personalized Federated Learning: A Meta-Learning Approach (Fallah et al. ArXiv 2020) \n\n**Minor Concerns:**\n1. Grammatical error here 'given a local objective clients their received models and'\n2. Eq. 6 some errors in the superscript of \"\\theta\" \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}