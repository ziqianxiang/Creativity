{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents work on efficient video analysis.  The reviewers appreciated the clear formulation and effective methodology.  Concerns were raised over empirical validation.  The authors' responses added additional material that assisted in clarifying these points.  After the discussion the reviewers converged on a unanimous accept rating.  The paper contains solid advances in efficient inference for video analysis and is ready for publication."
    },
    "Reviews": [
        {
            "title": "Review for VA-RED: Video Adaptive Redundancy Reduction",
            "review": "Summary\nThe paper presents a framework to reduce internal redundancy in the video recognition model. To do so, given the input frames, the framework predicts two scaling factors to conduct temporal and channel dimension reduction. The remaining part is reconstructed by cheap operations. The authors show that the framework achieves favorable results on several benchmarks.\n\nStrengths\n+ The paper is well written.\n+ Strong quantitative and qualitative results (Consistent improvement over the state-of-the-art baselines).\n+ Solid ablation studies and analysis.\n\nWeakness\n- The core idea of using a light-weight neural module to maximize the video model efficiency is not novel.\nFor example, AR-Net already has shown that a policy network can decide the video input resolution, i.e., spatial dimension, adaptively, leading to improve both efficiency and accuracy. In this work, the key concept is basically the same and only the primitive operation unit, i.e., temporal and channel dimensions, is different.\n\n- Please provide experiment-level comparisons with AR-Net\n\n- The following SOTA video models are missing in the reference.\nDynamoNet: Dynamic action and motion network, ICCV 2019\nVideo Modeling with Correlation Networks, CVPR 2020\nRubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition, ECCV2020\nDirectional Temporal Modeling for Action Recognition, ECCV2020\n\nQuestion\n\n- Can the idea be applied to dense video understanding tasks, such as video semantic segmentation?\n\nRating\n\n- Aiming at acquiring an efficient model in a data-driven manner is indeed important for video models.\nWhile the key idea is not novel, the paper has obvious empirical contributions that may help communities to invigorate future researches in this direction. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This submission proposes to adaptively reduce redundancy in temporal and channel dimension to make existing video networks more efficient.",
            "review": "Pros:\n1. The motivation of the submission is clear and the writing is easy to follow. The problem being addressed is a practical and important issue in deploying video models to real applications. \n\n2. Experiments are thorough. This submission has done experiments on both action recognition and action localization task. The performance is promising on most video networks. \n\nCons:\n1. This submission proposes to address the problem of slow video inference, but the only metric they report is the Gflops. We know that Gflops is not a good indicator for speed comparison. Sometimes, a small Gflops does not mean fast speed. So I recommend to add a \"speed\" column in every table of this submission, to compare the actual inference speed among the methods. \n\n2. Regarding the searching method, can authors justify what are the differences to X3D? X3D also search the optimal combination of temporal dimension and channel dimension. Are the differences only on adding efficiency loss?\n\n3. The efficiency loss is widely used in searching efficient image models, like in Proxylessnas. What is the contribution and motivation of using it here in searching video models? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a novel framework called $\\text{VA-RED}^2$ to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance.  This paper is well-written and conducts extensive experiments to validate the performance of proposed approach.",
            "review": "This paper proposes a novel framework called $\\text{VA-RED}^2$ to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. \n\nThe authors have done extensive experiments on video action recognition tasks and spatio-temporal action localization task in the area of video understanding. For the video action recognition task, experiments are carried out using Mini-Kinetics-200, Kinetics-400, and Moments-In-time datasets. For the action localization task, J-HMDB-21 dataset is used. Results show that this framework is promising, which reduces the computation but main the performance. \n\n\nQuestion: \n1. X3D-M in the original paper achieved at top-1 74.6 for Kinetics-400 dataset (but in table 4 reports clip-1: 61.8, video-1: 67.9) and FLOPs is 4.73 (6.20 reported in the paper). Can authors explain why there is a difference here? \n\n\nMinor:\n1. Reduce ratio fact for channel-wise dynamic convolution $r=\\frac{1}{2}^{p_c}$ such as in Fig.2 on page 4 and equation 7, equation 8 in supplementary materials on page 13. I think it would make more sense representing it as $(\\frac{1}{2})^{p_c}$. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}