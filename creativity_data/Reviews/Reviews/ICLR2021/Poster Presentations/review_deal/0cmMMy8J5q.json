{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This is a well-written paper proposing a promising a series of zero-cost proxies for NAS. Overall, the reviewers were convinced that the approach is sound and the results overall support the use of zero-cost proxies (although they are a bit weak in some cases, e.g. rank correlations in A.3). Despite some concerns amongst the reviewers around the technical novelty of the method, mostly due to the use of estimators from the pruning-at-init literature, this is promising work at the intersection of different sub-communities in ML."
    },
    "Reviews": [
        {
            "title": "Solid paper with extensive experiments. Results are mixed.",
            "review": "Overall, this is a solid work with interesting ingredients. The main contributions are mostly in terms of empirical results rather than a new search method.\n\nStrengths\n* The paper is very well-written and easy to follow.\n* The work presented simple yet effective ways to combine zero-shot proxies with existing NAS methods: (1) to initialize standard NAS algorithms, or (2) to guide the search process. Both approaches are interesting and have well demonstrated that zero-shot proxies are complementary to existing methods.\n* The paper has provided extensive evaluations over different metrics, search spaces and datasets. The results are presented in clear manner and are fairly convincing.\n\nWeaknesses\n* The technical novelty is limited in the sense that (1) the idea of zero-shot NAS is not new (Mellor et al., 2020), and (2) the best zero-shot metric (synflow) is a straightforward application of the existing work.\n* Results for the rank correlation for top 10% models (A.3) are a bit disappointing because it seems to suggest zero-shot proxies are not able to identify high-performance architectures when used *alone* without leveraging other NAS methods, which are usually more expensive.\n* While the method provides speedup at the early-stage, it does not seems to offer gains on the high-performance region on top of strong methods (e.g., predictive methods in Table 3).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper on analyzing zero-cost proxies",
            "review": "# Summary\nTo reduce the cost of NAS, this paper focuses on zero-cost proxies to estimate the performance of network architectures without any training.\n\nSpecifically, the authors propose a series of zero-cost proxies based on recent pruning literature. They also propose two practical strategies: zero-cost warmup and zero-cost move, to integrate zero-cost proxies into existing search algorithms (e.g., random search, RL, evolution). \n\nExtensive experiments and analysis on NAS-Bench-101/201/NLP/ASR validate the usefulness of the proposed zero-cost proxies. Integrating zero-cost proxies into existing search algorithms significantly improves the sample efficiency.\n\n\n# Strong points\n1. Designing zero-cost proxies for NAS is a promising direction to reduce its cost. This paper evaluates a series zero-cost proxies and demonstrates their usefulness.\n2. The idea of zero-cost warmup and zero-cost move is very appealing. Only using zero-cost proxies might not achieve competitive performance, but this idea provides a practical way of using these zero-cost proxies that will yield nice performance and a great reduction in search cost.\n3. The experiments and analysis in this paper are extensive and solid, which provides many insights for the community to understand these zero-cost proxies.\n\n\n# Weak points\n1. “with synflow we compute a loss which is simply the product of all parameters in the network”. Why synflow using this loss? To make the paper self-contained, it will be helpful to give more clarification on this.\n\n  Small comment: Explicitly explaining the Hadamard product notation is also helpful.\n\n2. For some zero-cost proxies, we need to pass a minibatch of data to the network. Is the proxy sensitive or not to the choice of the minibatch? I am not sure if the “128 samples” are fixed or not in Table 4.\n\n3. Why can r16c8 outperform the full training baseline r32c16? This seems to be counter-intuitive. Any explanation for this?\n\n  Also, how many models are used in Figure 1, all the 15625 models in NAS-Bench-201?  It seems you will have to re-train the models under different configurations.\n\n4. For econas+ (r16c8e15), what’s the learning rate schedule: (1) anneal to 0 (or a small number) at epoch 15, or (2) still follows the LR schedule of 200 epochs? The same question applies to other points in Figure 1. The first way (anneal to 0 at epoch 15) is more common and usually the better choice. It helps people to understand how strong this baseline is if you can confirm the implementation details in this part.\n\n4. Figure 3 needs to be better organized to group the results for the same dataset together. It’s hard to compare different proxies in this form.\n\n  The text says that snyflow is the best. But I notice that snip and fisher are actually slightly better than synflow, if we ignore SVHN, which is less important than ImageNet I think.\n\n# Justification of rating\nThe zero-cost proxies used in this paper are mostly inspired by recent pruning literature. So, I consider this paper as more like an analysis paper that evaluates these known metrics for zero-cost proxies. But I need to point out that these known metrics were not designed for NAS initially. So, this paper provides new insights to the community.\n\nI generally feel positive about this work due to its extensive analysis and the nice idea of zero-cost warmup/move. Most of the weak points mentioned above are for clarification. I also encourage the authors to further refine the figures and writing to make this paper better.\n\n# Additional comments\n\n1. Figures1&2 are blurred. The visualization style needs to be improved to make it easier to interpret.\n\n2. Figure 6 y-axis is labeled as “normalized training time”. But from the text, the y-axis label should be speedup?\n\n3. Table 2, the meaning of boldface is unclear.\n\n# After rebuttal\n\nThe authors did a good job of answering my concerns. I keep my original positive rating.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Zero-Cost Proxies for Lightweight NAS",
            "review": "############################################################\n\nSummary\n\nThis paper provides an extensive empirical evaluation of zero-cost proxies which can be combined with existing NAS methods to speed up search time. The proposed method utilizes ‘pruning-at-initialization’ works which computes gradient-computation at initialization as a proxy for performance of the given neural architectures. Through extensive experiments, this paper compares between conventional proxies and ablation study on five NAS benchmarks and shows the validation of the proposed proxy. \n \n############################################################\n\nReasons for score\n  \nOverall, I vote for weak rejecting. I like the idea of zero-cost proxies which decides the performance of neural architectures at initialization and the high utility of the zero-cost proxies available combining it with various NAS methods. However, my major concern is the limitation of zero-cost proxies where the correlations between the top 10% performance neural architectures and the values estimated by zero-cost proxies look low to use as a proxy for searching for the best neural architectures.  Hopefully, the authors can address my concern in the rebuttal period.  \n  \n############################################################\n\nStrong Points \n  \n1. This paper provides zero-cost proxies that can estimate the performance of the given neural architectures at the initialization step saving time and resources.\n2. The idea of this paper is simple yet effective, where this paper is inspired by pruning-at-initialization methods and adopt those methods as proxies for NAS methods.\n3. This paper demonstrates the effectiveness through extensive experiments. It first performs evaluations among several zero-cost proxies candidates to select the best one, synflow and then combines synflow and conventional NAS methods to conduct experiments on various NAS-benchmark datasets.\n\n############################################################\n\nWeak Points\n\n1. My main concern is that zero-cost proxies may be not enough to use for searching the best neural architecture. The correlation between synflow which is the best one of zero-cost proxies and neural architectures for CIFAR10 from NAS201-Bench201 search space is 0.74 (Table 1). However, the correlation drops to 0.18 (Table 7 of Appendix) when we compute it between synflow, and neural architectures which have top 10% ranking performance on CIFAR10 from NAS201-Bench201 search space (Table 7). In case NAS-Bench-ASR, it drops to 0.03 (Table 7) which is nearly random. I believe while zero-cost proxies are suitable for estimate the performance tendency of all neural architectures from the search space, they are needed to be improved for the problems which search the best neural architectures. During the period, I hope the authors address my concern by performing additional experiments. \n2. I think this paper has the property of the benchmark paper since this paper brings several zero-cost proxies applying it to existing NAS rather than designs a novel proxies. Thus, I expect that this paper shows more usecases combining zero-cost proxies with conventional NAS methods to demonstrate the usefulness of zero-cost proxies.  \n3. I recommend the results of cifar10 and imagenet120 of NAS-Bench 201 move to the original paper from the appendix. \n4. Could the author shows the results of the Figure 4 as the form of the Table with exact values by selecting the results?\n5. Clarity of the paper looks needed. For example, \n\n      (1) missing reference or model name\n\n      (2) dataset description of NAS-Bench 201 of 4.1\n\n      (3) the detailed description of experimental setup of 4.1.2.\n\n      (4) the detailed description of warmup technique. What meaning of (1000) of warmup (1000) in Figure 4.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This is a paper that barely meets the acceptance criteria.",
            "review": "This paper provides an interesting direction in the neural architecture field. In particular, it proposes a series of zero-cost proxies. To design such proxies, the authors evaluate conventional reduced-training proxies and recent pruning algorithms. These proxies use just a single minibatch of training data to compute a model’s score. \nReasons for score:\nOverall, I vote for accepting. On the one hand, I think the idea of utilizing pruning algorithms in NAS is very attractive since it offers a way to avoid the heavy proxy training and evaluation phase. On the other hand, the experiments in this paper are very sufficient and strong. My major concern is about the clarity of the paper. Hopefully, the authors can address my concern in the rebuttal period.\n\nPros:\n1. The paper takes one of the most important issues of NAS: the computation burden in proxy training. For me, the problem itself is real and practical.\n2. The proposed “zero-cost” proxies are very attractive for its extremely cheap computation cost. The cheap computation allows us to explore much large search space, which is of great importance in NAS. Moreover, these proxies require very little memory, which is also very attractive.\n3. This paper provides extensive experiments to evaluate performances of existing pruning algorithms on a vast variety of datasets. The experimental results have a high reference value.\n\nCons:\n1. In Eq. (1), it seems that you use a Hessian Matrix to compute grasp metric. However, the computation of a Hessian Matrix is very expensive. Is it a bug? Or you have some techniques to bypass the computation of the Hessian matrix? It would be better to provide more details here. \n2. For Section 3, it would be better to provide more details about how you aggregate these metrics, which seems not very clear for me.\n3. As far as I know, these techniques are not easy to integrate with some differentiable NAS algorithms, such as DARTS. However, differentiable NAS occupies an important position in all NAS methods. So, I suggest that you can point out this limitation in your paper for the sake of rigor.\n\nSome typos：\n1.  INTRODUCTION： operate at at-> operate at\n2.  3.1 CONVENTIONAL NAS PROXIES (ECONAS): \n in many prior work->  in many prior works\n  One main findings->  One main finding\n3.  5.1 ZERO-COST WARMUP:  \nthe number of models for which we compute -> the number of models for which we compute(T)\n4.  6 CONCLUSION:\n     Compute-> computing\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}