{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper studies the representation learning problem in the linear bandit setting, where each bandit \"task\" shares a common low-dimensional representation. The paper introduces a novel algorithm, it provides theoretical regret guarantees, and it illustrates the effectiveness of the proposed method in a number of experiments.\n\nThere is a general agreement among the reviewers about the relevance of the problem and the contribution of the paper. The authors properly addressed concerns about the novelty (e.g., comparison with linear bandit and low-rank structure) and about the underlying assumptions. Although some of them do seem relatively strong (and in some cases stronger than the state-of-the-art in bandit, such as the distribution on the contexts), it is indeed non trivial to understand whether such assumptions can be easily relaxed in the representation learning context. \n\nThe novelty of the algorithm is more on the specific problem and set of assumptions, but it mostly relies on known principles (e.g., using method-of-moment for estimating the underlying representation). In this sense, I see this paper more as a useful addition to the fast growing landscape of representation learning methods in online learning, rather than a breakthrough. Also, the structure of the algorithm seems very \"theoretical\" in nature, since the explore-than-commit approach is very rarely a good strategy in practice. \n\nAnother issue the authors clarified in their revised submission is the actual improvement obtained in the bounds depending on the parameters T, k, d, N. In this respect, I still would like to encourage the authors to further illustrate the regime where the bound is actually better than for the single-task approach. For instance, they could consider N fixed to a convenient value and produce a plot with x-axis T and y-axis the regret bound and report different curves for varying values of k and d. This would further clarify to the reader when representation learning can *provably* improve over plain single-task learning.\n\nOverall, given the general support from the reviewers and the revised version of the paper, I consider this contribution is significant enough to propose acceptance. As mentioned above, I believe it will serve as a reference for developing further the literature in this domain."
    },
    "Reviews": [
        {
            "title": "A theoretical study of the impact of a shared low-rank representation for multi-task linear bandits",
            "review": "This article provide a theoretical analysis of the impact of learning a shared low-rank representation in multi-task linear bandits.\nTwo types of linear bandits are considered: the finite actions and the continuous actions settings.\nFor the finite-actions setting the authors propose an algorithm called MLinGreedy and provide a theoretical analysis with a tight minimax regret bound (Theorem 1 & 2). For the continuous-actions setting they propose an \"explore-then-explored-then-commit\" algorithm called E2TC. They provide an upper-bound for the regret (Theorem 3), and a lower bound (Theorem 4). A gap remains between upper and lower regret bounds for this setting.\nTheses bounds are roughly constituted of two parts: one part for the cost of learning the low-rank representation and one part which correspond to the cost of a linear bandit on a perfect low-rank representation.\nIn section 6 the authors provide two experiments for the finite actions setting. One experiment on a synthetic environment, another which is simulated-from \"real-life\" data. These experiments underline the advantage of learning a shared low-rank representation against a baseline where that tasks are considered independently. It does not expose situations where, according to the bounds, it should be at disadvantage.\n\nIf we omit the numerous typos that I listed at the end of this review, I found the paper clear and well written.\nI did not have time to delve deeply into all the demonstrations, but the proof of Theorem 1 seems solid.\nAs explained on page 5 it relies on three lemma. Lemma 2 give guarantees on the low-rank approximation. It  is based on a Hoeffding concentration inequality combined with an epsilon-net argument. Lemma 3, borrowed from (Han et al., 2020), and Lemma 4 show that this estimate works well with the doubling schedule of MLinGreedy.\n\n## Pro:\n- The paper is well written\n- The maths seem solid\n- The results give a theoretical insight on the impact/benefit of representation learning in the specific case of linear bandits.\n\n## Con:\n- The multi-task linear bandit models rely on several assumptions and simplifications which obfuscate its realism. These assumptions are however justified thoroughly on page 3 and 4.\n- I did not like the bias toward improvement of the introduction: for instance, according to the bound, if k is in Omega(d), the cost of learning the low-rank matrix is linear in d which results in a regret which is worse than the one of the \"naive\" approach for large dimensions. The authors should not be afraid to develop on the cases where the shared representation does not improve, it will not devaluate the significance of their work.\n\n## Minor remarks & typos:\np1: I am not a fan of the \"Provable benefits of\" phrase in the title. This work is more about the impact of shared low-rank representation and when it can improves from the naive, but much simpler, independent-tasks approach. As mentioned on page 5 it requires T to be in Omega(k) to improve. \"Impact of\" would be more appropriate.\np1: \"In this paper, we\" -> \"We\"\np2: The \\mathbold{x}_{n,t,a_{n,t}} notation is heavy and  not really needed. As mentioned on the bottom of page 3 one may interchangeably use \\mathbold{a}_{n,t} and \\mathbold{x}_{n,t,a_{n,t}} to refer the same action, so I would get rid of the heaviest notation.\np3: \"line of work analyzed\" -> \"line of work that analyzed\"\np3: \"as our algorithm\" -> \"with our algorithm\"\np3: \"In this paper, we\" -> \"We\"\np4: \"In this assumption\" -> \"With this assumption\"\np4: \"for at each task\" -> \"for each task\"\np4: \"In each round\" -> \"At each round\"\np4: \"task are sample from\" -> \"task are sampled from\"\np5: \"up to an constant error\" -> \"up to a constant error\"\np5: \"we use an method-of\" -> \"we use a method-of\"\np6:  \"value decomposition of \\hat{B}\" -> \"value decomposition of \\hat{M}\"\np7&8: the theoretical bounds should appear on Figure 2 and 4.\np8: Replacing the Time Horizon n with the number of tasks T as on Figure 2 would be more informative.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting problem but has several limitations",
            "review": "This paper studies the benefits of learning a low-rank feature extractor in multi-task linear bandits. Specifically, the paper studies the setting where an unknown common linear feature extractor $B \\in R^{d \\times k}$ maps the original $d$-dimensional contexts $x$ to a $k$-dimensional representation. Essentially, for multi-task linear bandit problem $r_t = \\theta_t^T x$, this paper assumes the matrix of model parameters $\\Theta \\in R^{d\\times T}$ is low-rank and be factorized as $\\Theta = BW$ with rank k. The paper proposed algorithms to estimate both $B$ and $W$ in finite actions setting and infinite actions setting. In finite action setting, the proposed solution is a greedy algorithm while in infinite actions setting, the proposed solution is a explore-the-commit method. Theoretical result shows that the regret is $O(T\\sqrt{kN} + \\sqrt{dkTN})$ and matches the lower bound. Simulation result shows that the algorithm outperforms baselines running independent linear bandit algorithms. \n\nPros:\n1) The problem studied in this paper is interesting and important. It is a well-known concern that linear bandits has a $O(d)$ regret for infinite actions and $O(\\sqrt{d})$ regret for finite actions, which is too large for practical problem. Many recent studies aim to address this problem from varies perspectives, such as low-rank structure, sparsity, etc. The authors proposes to estimate a low-rank feature extractor for multi-task linear bandits to reduce the regret.\n\n2) The paper is generally well-written and easy to follow (only a few confusing descriptions, mentioned below). The required assumptions are presented and discussed clearly.\n\n3) The analysis of regret upper bound and lower bound are good contributions.\n\n\nCons:\n1) The paper introduces a very strong assumption (Assumption 2) that the context features are sampled from Gaussian  (a stochastic context setting), and is a serious limitation of this paper. Most linear bandits research focus on the adversarial contexts setting, where the solution can also solve bandits with stochastic context but not the other way around. The gaussian feature assumption suggests that arms are equally informative and thus the classical exploration-exploitation dilemma does not exist.  It is not clearly discussed in the paper that why this paper must be limited to the stochastic feature setting. The paper heavily follows Theoretical results from [1] and both adversarial and stochastic contexts settings are discussed in [1]. What is the challenge to propose a solution for adversarial contexts, for example a linear UCB based solution? \n\n2) It seems unclear on how to estimate both $B$ and $W$ and what is the estimation quality. According to the description the algorithm is doing an explicit matrix factorization. Since matrix factorization does not have a closed form solution, what is the guarantee of the estimation quality? In Lemma 2 the analysis requires the estimated $\\hat B$ and $\\hat W$ minimize the square loss so that the square loss with estimated $\\hat B$ and $\\hat W$ is smaller than the square loss with true $B$ and $W$(the first step of derivation)-- is this guaranteed to be achievable?\n\n3) The authors argued that the problem of learning a low-rank feature extractor has not been studied in the bandit setting before, which seems incorrect: [2] studies a very similar problem that tries to estimate a hidden projection matrix for linear bandits with low-rank structure. In [2] the project matrix is estimated by PCA.  I strongly suggest the authors to compare this paper with [2]. One difference is that in [2] the low-rank structure is about the features (thus can be directly estimated from the features), while in this paper the low-rank structure is about the parameters $\\Theta$. One potential advantages I can think of is this paper achieves regret in $O(\\sqrt{k})$ while [2] has regret in $O(k)$ -- is it because of the assumption on features are sampled from gaussian? \n\n\nOther questions:\n\n1) What is the baseline naive algorithm in Figure 1? It seems to be linear regression + greedy strategy and I would suggest the authors to clarify it.\n\n2) Is rank $k$ assumed to be an input to the algorithm? What if the algorithm does not know it or have a wrong knowledge of $k$?\n\n\n==========================================================================================\n\nWhile I still hold my concern on the i.i.d. assumption of the context as it is less interesting both practically and theoretically, the author response and the revised paper clearly resolve my other questions and concerns.  I am increasing my score to 6.\n\n\nReferences:\n[1] Han, Yanjun, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W. Glynn, and Yinyu Ye. \"Sequential Batch Learning in Finite-Action Linear Contextual Bandits.\" arXiv preprint arXiv:2004.06321 (2020). \n[2] Lale, Sahin, Kamyar Azizzadenesheli, Anima Anandkumar, and Babak Hassibi. \"Stochastic linear bandits with hidden low rank structure.\" arXiv preprint arXiv:1901.09490 (2019).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good combination with representation learning and bandit",
            "review": "Summary:\n\nThis paper introduced the representation learning techniques into the linear bandits and showed that representation learning could improve the regret bound when multiple tasks shared a common low dimensional linear representation. The authors also proved a lower bound and extended the algorithm to the infinite-action setting. They also present experiments to validate their theoretical findings.\n\nPros:\n(1) The idea of representation learning + linear bandits is very interesting.  Their results are also impressive which showed that combination will be better than naive algorithm. \n(2) The paper is well written and easy to read. Their proof part looks good.\n\nCons:\n The assumptions (1,3,4) are based on the representation learning setting. Some of them seems very strong compared to the general linear bandits, e.g.,  assumption 4. Is this fair to compare your result with naive algorithm ? The naive algorithm could have better regret bound under the same setting. e.g., if the source tasks are diverse enough, the linear bandits will become much easier.\n\nMinor Comments:\nThe author should use the better notations for bandit setting. It is strange that using T as the number of bandit tasks since T usually will be used as the time horizon of the experiments.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "*Summary*\n\nThis paper theoretically studies the benefits of representation learning in linear bandit problems. The key assumption is the existence of a common linear feature extractor. Two different setting are studied. In the finite-action setting, the authors provide the MLinGreedy algorithm that achieves matching upper and lower bounds (up to polylog factors). In the infinite-action setting, the authors provide the $E^2TC$ algorithm that can achieve lower regret than the naive method when the number of tasks is large. Experiments on both synthetic and real-world data are conducted, which confirm the theoretical results.\n\n*Assessment*\n\nOverall I'm leaning towards acceptance. Representation learning in sequential decision making problems is an important problem that many readers of ICLR will care about, and it is valuable to offer some theory insights into this problem. Besides, the paper is overall clearly written and enjoyable to read. Still I have some questions regarding the assumptions that the theoretical results are based on (see questions).\n\n*Questions/Comments*\n- The context vectors $x_{t,a}$ can be arbitrarily chosen in LinUCB (Chu et al. 2011). Is it possible to allow for adversarially chosen context in the finite-action setting?\n- Intuitively, what is the reason that we do not need explicit exploration in MLinGreedy? I assume it has something to do with Lemma 3, which seems to rely on Assumption 2 ($\\lambda_{\\min}(\\Sigma_t) \\ge \\Omega(1/d)$). Following the previous question, Assumption 2 looks rather strong; is there any good motivating example showing that Assumption 2 is likely to be true in practice?\n- It would be better to give a concrete motivating example at the begining to give the readers a better idea of what $N, T, d, k$ look like order-wise.\n- There's a gap between upper and lower bound in the infinite-action setting, but this is acceptable as a conference submission.\n\n*Minor comments*\n- Page 4, first line: $[w_1, \\dots, w_T]$ should be in bold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper presents small step as an advancement towards a much needed theoretical support for a popular practice of representation learning in linear bandits. The authors support their findings by experimental evaluations.",
            "review": "- Pros.\n   -  Learning the representation is crucial to model an efficient linear MAB. However, there is lot of room to include the characteristic of the learned representation into theoretical guarantee. This paper throws a light on this interesting problem.\n   - The paper is well-written and the bounds look convincing. I have not gone through the detailed derivations (in the appendix), but the overall idea looks good.\n\n- Cons.\n   -  It would have been better if the paper could throw some light on other variants of representation learning.\n   -  Linear bandit is quite popular news/ad recommendation systems. However, posing hand-writing recognition on MNIST data as linear bandit seems to be unnatural. There are DNN based approaches that solve the problem with a great accuracy. It will be interesting to see how does the algorithm perform on a real data set of news/ad recommendation.\n   - Assumption 2 is a quite strong assumption to make.\n   \nI believe the exiting work: A Contextual-Bandit Approach to Personalized News Article Recommendation by Li et al (2010) deserves a citation in this paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}