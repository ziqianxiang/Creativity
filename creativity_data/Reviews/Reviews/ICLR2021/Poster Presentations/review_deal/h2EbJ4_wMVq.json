{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work describes a system for collaborative learning in which several agents holding data want to improve their models by asking other agents to label their points. The system preserves confidentiality of queries using MPC and also throws in differentially private aggregation of labels (taken from the PATE framework). It provides expriments showing computational feasibility of the system. The techniques use active learning to improve the models.\n\nOverall the ingredients are fairly standard but are put together in a new (to the best of my , admittedly limited, knowledge of this area). This seems like a solid attempt to explore approaches for learning in a federated setting with strong limitations on data sharing."
    },
    "Reviews": [
        {
            "title": "Solid but unsurprising system for federated classification system with privacy",
            "review": "Summary:\nThe authors combine several cryptographic techniques to create a federated systems that allows several entities to run classification against all the model held be the participants without revealing information in the process. In particular, the sample to be classified is not revealed to any other party, and differential privacy is used to protect the training data that was used to train the models. A central semi-honest coordinator is used to aggregate the results and add the differential privacy without learning any private information.\n\nPros:\nThe strength of this works lie in combining relevant techniques and to show experimentally that the resulting system does improve over using a local model both when the training is distributed evenly or in skewed manner while taking privacy considerations into account.\n\nCons:\n- From a cryptographic point of view, the combination of techniques is somewhat expectable.\n- I'm wondering about the low statistical security (${2^-23}$). This seems to be related to the usage of (unbounded) integer secret sharing. Would it be possible to use secret sharing modulo an integer, in which case the security could be perfect?\n- I think it would be easier to follow if steps 1-3 were combined in the description because they all take between the same pairs of parties. The exact techniques used don't seem to matter as long as the output secret sharing is the desired result, namely the one-hot vector.\n- I find the term collaborative learning somewhat overblown because the proposed protocol only runs classification collaboratively.\n\nOverall:\nDespite the points above, I'm in favor of acceptance because the paper seems to improve on previous work, and because it is written very well.\n\nMinor issues:\n3.3: leakeage\n4.1: odd juxtaposition in the formatting of \"arg max\" and \"sum\"\nFigure 3: very hard to read in black-and-white\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Fairness seems very cool, but no convinced by privacy part",
            "review": "\n\nThis work motivated by healthcare and finance where separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. This paper propose Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentially and privacy in a collaborative setting. This work also discussed about fairness. I liked this part, since it seems very cool. However, I'm not convinced by the method in this work is better than InstaHide (I could be wrong).\n\n\n\n\nMinor comments\nIn Section 2.1, this paper should be discussed, since it proposed a way to ``encrypt'' the images/texts. \n\nInstaHide: instance-hiding schemes for private distributed learning\nhttps://arxiv.org/abs/2010.02772\nICML 2020\nYangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora.\n\n\nTextHide: Tackling Data Privacy in Language Understanding Tasks\nhttps://arxiv.org/abs/2010.06053\nEMNLP 2020\nYangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora\n\nIn Section B, it lists many theorems/definition about differential privacy. In Section C, it list many backgrounds about sampling. In Section D., it list many definitions on Fairness. I don't quite see the point of having them in appendix, since none of them got mentioned in Appendix E, which is the proof of the main theory result in this paper.\n\nThis paper is closely related differential privacy. I think this paper should also be mentioned somewhere.\n\nPrivacy-preserving Learning via Deep Net Pruning\nhttps://arxiv.org/abs/2003.01876\nYangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, Kai Li.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "CaPC Review. Decision: Accept.",
            "review": "This paper works on the problem of collaborative learning while preserving both confidentiality and privacy of the data points. It combines techniques from secure multi-party computation and differential privacy for the same, and improves on confidential inference and PATE in the process. The new technique is called CaPC. Finally, it states empirical results as evidence for the improved accuracy.\n\nWeakness:\n1. The evaluation is done on just two datasets. So, it is a little hard to judge whether the techniques would generalise or not.\n2. The writing of the paper itself is not that great because it is difficult to understand the low level details of the experiments.\n3. They talk very little about improving on the fairness guarantees.\n\nStrengths:\n1. Their techniques enable collaborative learning even in settings where the local architectures of different parties are different.\n2. The algorithms they provide improve on fairness.\n3. Their empirical results are better than the previously known methods.\n\nEvaluation: I believe the combination of secure multi-party computation and differential privacy is not totally new, but since it yields decent results, I would say that the paper deserves a chance to be accepted.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}