{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "I agree with the reviewers' comments. The technique proposed in the paper is very interesting, and although the method itself is not particularly surprising (it's \"just\" chaining two compressors), it's a really nice way of framing and studying the problem. On the other hand, the experiments _are_ relatively weak, and I think there is significant potential for improvement here (especially with an added 9th page of text). I encourage the authors to add some more convincing experiments in future versions of the paper."
    },
    "Reviews": [
        {
            "title": "Recommendation to Accept",
            "review": "Summary: The paper studies compression of gradients in distributed training with the goal of minimizing communication costs.  The main contribution of the paper is the development of an *induced compressor* that takes as input, a possibly biased compressor, and outputs an unbiased compressor with similar error variance as compared with the original compressor.  As the output of the induced compressor is unbiased, it obviates the need for error feedback, which is needed when biased gradient estimates are used.\n\nReason for Score:\n\nI vote for accepting the paper.  \n--> The idea of an induced compressor is simple (after the fact), and elegant, and advances state of the art in gradient compression. \n-->The paper has solid theoretical reasoning via a convergence analysis that does demonstrate that for certain (common) parameter choices the induced compressor is expected to have better convergence than the biased counter part\n-->   Satisfactory experiments to back up the theoretical insights. In particular, the output of the induced compressor requires more bits as compared with the underlying biased compressor. Empirical results show that, an induced compressor obtained from a class of biased compressors (e.g., top K gradients) has better performance than a biased compressor from that class with the same communication cost.\n\n\nPros:\n\n--> This paper resolves a problem of current gradient compression techniques: the good compressors that require a small number of bits/values to represent the gradients are inevitably biased, and the bias needs to be compensated via error feedback techniques. It shows that there are good unbiased compressors and provides a recipe for constructing them.\n\n--> The paper is well-written and based on a sound theoretical reasoning.\n\nCons:\n--> I would like a more detailed discussion on the effect of the unbiased compressor, that is used in the development of the induced compressor. How sensitive is the performance/compression to the choice of the unbiased compressor? \n--> I wonder how the theory would apply to weakly convex or non-convex settings. \n\n==================================\nComment after author responses.\nOn reading the author's responses to my (and other) review comments, my recommendation remains unchanged. This is a solid piece of work. \n\n\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "General scheme for converting biased compressors to unbiased compressors - Not entirely convincing but adds to the discussion",
            "review": "**Quality and Clarity**\n\nThe paper is generally well written barring a few typos (see Queries and Suggestions below). The problem is clearly described and the solution is well motivated. The main theoretical result (Theorem 2) might be a bit hard to parse for readers unfamiliar with the theoretical results in this space but the overall explanation is clear.\n\n**Originality and Significance**\n\nThe main approach for converting biased compressor to unbiased compressors appears to be novel and flexible. The theoretical analysis of distributed compressed SGD using unbiased compressors can also be useful to the community since the derived upper bound can be easily applied to any unbiased compressor.\n\n**Strengths**\n\n1. The approach for converting biased compressors to unbiased compressor does not seem to have been stated in this fashion before and provides an elegant framework for combining previous approaches. \n\n2. The convergence analysis of unbiased compression is general and can be applied to any unbiased compressor and thus should be useful to the community as a whole.\n\n3. The proposed approach appears to be more flexible than biased compression + EF and the authors show how it can be extended to Federated Learning settings, and also admit techniques like variance reduction and acceleration using momentum.\n\n**Weaknesses**\n\n1. Neither the theoretical analysis (comparison of upper bounds) nor the experiments (comparisons for some specific biased and unbiased compressors) in this paper convincingly show that the proposed approach will always be theoretically and practically better than Error Feedback.\n\n2. The right choice of unbiased compressor $\\mathcal{C}_2$ for a given biased compressor $\\mathcal{C}_1$ is not clear. The authors show that if $\\delta_2 = \\delta_1$ then the upper bound of Theorem 2 will be tighter for the induced compressor $\\mathcal{C}$ than for $\\mathcal{C}_2$. However it is not clear if that will be enough to outperform EF since there will still be some extra variance. Indeed in the experiments the authors consider several unbiased compressors and while Top-a + Rand-(K-a) seems to beat Top-K + EF no general guidelines are offered.\n\n2. The flexibility of the proposed approach which makes it amenable to the extensions  in Section 4 is not illustrated in the experiments which are limited to a direct comparison of the proposed approach with EF.\n\n**Queries and Suggestions**\n\n1. Why do both $\\mathcal{C}_1(g)$ and $\\mathcal{C}_2(e)$ need to be communicated if $\\mathcal{C}$ is unbiased? (paragraph after Theorem 3)\n\n2. While I understand that it is probably not possible to demonstrate that the proposed approach will outperform EF in every single scenario, I believe including some guidelines about choosing biased and unbiased compressors appropriately will greatly increase the impact of this work. For eg. Are all combinations of biased and unbiased compressors acceptable? Should one just care about ensuring that $\\delta_1 = \\delta_2$? What are good choices of unbiased compressors for some popular biased compressors? If it is possible to answer these questions, the paper will definitely be more useful to readers.\n\n3. I would also recommend adding experiments on at least one of the extensions from Section 4 to highlight the flexibility of the proposed approach.\n\n4. Typos: \ni) Missing 'of' in Section 2, line 4.\nii) Last term in RHS of first equation in C.1 should be $+||x||^2$\niii) Missing '2L' in 3rd inequality in C.2\niv) Superscripts and subscripts have been interchanged in some terms in the proofs of Lemmas 6 and 7.\nv) Second term in LHS of first equation of C.3 should be $\\mathcal{C}_2(.)$\n\nOverall, while I am not quite convinced that the paper achieves its stated goal of showing that the proposed unbiased compression approach always outperforms biased compression + EF, I believe it adds enough to the discussion in this space to merit acceptance.\n\n**Comments after Author Response**\n\nI thank the authors for their response. My opinion of the potential of this paper to encourage further discussion in this area is unchanged. I can already see from the response on choice of biased and unbiased compressors to combine that there is plenty of scope for future work that builds on this idea. Regarding the comparison with EF, I appreciate the additional intuition provided in the author response on the drawbacks of EF and hope to see improvements to EF or more exhaustive comparisons between EF and the approach proposed in this paper in future work. As I had already recommended acceptance I am leaving my score unchanged. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, but experimental evaluation is very limited",
            "review": "This paper proposes an approach to reduce communication overhead in distributed training. The developed approach consists of a compressor operator (C(g)) that reduces the size of such messages (g). Specifically, the authors proposed a technique that transforms any biased compressor (e.g., Top-k) into an unbiased one, referred to as induced compressor, with better convergence guarantees and properties than the former. The advantages of the induced compressor are supported by a theoretical framework developed in the context of distributed compressed stochastic gradient descent (DCSGD). Moreover, the theoretical findings are validated with experiments in the CIFAR-10 dataset using two different image classifiers (Resnet-18 and VGG11 networks.).\n\nPros:\n- The contribution of the paper is very nice. The idea to turn a biased compressor into an unbiased one is appealing and makes totally sense to me.\n- The theory part is good and gives interesting insights.\n\nCons:\n- The experimental evaluation is not convincing. It is restricted to one dataset only. Why did the authors not perform experiments on a second dataset to validate the approach?\n- In my understanding one uses compression methods in FL to reduce the communication overhead. The proposed approach, however, seems to double the number of bits transmitted. The dimension \"transferred bits\" is missing in the experiments. I would like to see some comparison of Top-k + EF and the proposed approach wrt to bits transmitted vs. accuracy.\n- Does the proposed approach also work well for non-iid data? How does the approach work with methods combining sparsity and communication delay, e.g., Sparse Ternary Compression (Sattler et al. 2020).\n\nMore experimental evaluation is required to better understand the benefits and limits of the proposed method.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "I would recommend for now weak reject. I have some questions regarding the use of induced compressor and empirical experiments. I hope authors can address them in rebuttal.  ",
            "review": "Summary: \n\nThis paper proposes a framework of compressed communication that can be used to deal with error induced by contractive compressors and can serve as a superior alternative to the existing framework (compressed communication with error feedback (EF)). The proposed framework outperforms EF in terms of memory requirement, communication complexity, and technical assumptions. The extension to federated learning with partial participation, and the numerical experiments on real-world dataset illustrate the applicability of this framework.\n\nStrength:\n\n1. The paper compares two algorithms, i.e., distributed compressed SGD (DCSGD) and compressed communication with error feedback (EF), by analyzing their convergence rates. The paper provides a new theoretical analysis for DCSGD, with weaker technical assumptions and tighter bounds.\n\n2. The induced compressor proposed in the paper can transform any biased compressor into an unbiased one, which can then be used in various existing methods designed for unbiased compressors.   \n\nComments: \n\n1. The paper concludes that the standard DCSGD algorithm is preferable to DCSGD with error feedback by comparing two algorithms' best-known convergence rates. It may not be appropriate because it is possible that the rate of standard DCSGD is theoretically tighter than that of DCSGD with error feedback (i.e., better theoretical bound). In other words, the difference in bounds of two algorithms is possibly due to the different theoretical analysis methods or different technical assumptions used in two algorithms, rather than the algorithm itself. How do those analysis methods used for finding convergence rates in two algorithms compared to each other? How can we make sure that the improvement is caused by the algorithm itself, but not the theoretical bounds?\n\n2. Federated learning with partial participation has been studied in other works. I suggest the authors citing these papers and highlighting the differences. For example,\n(1) Reisizadeh, Amirhossein, et al. \"Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization.\" International Conference on Artificial Intelligence and Statistics. 2020.\"\n(2) Sattler, Felix, et al. \"Robust and communication-efficient federated learning from non-iid data.\" IEEE transactions on neural networks and learning systems (2019).\n\n3. The benefit of using a biased compressor is its low variance; this is what improves the performance. However, if a biased compressor is transformed into an unbiased one using an induced compressor, then don't we lose the benefits of using such a biased compressor? What is the benefit of transformation using an induced compressor if the variance is not low anymore? Why not use an unbiased compressor (e.g., C2) directly? Is it because of the additional unbiased \"error feedback\" the induced compressor can provide?\n\n4. In experiments (Figure 3), authors compare various methods with one set of compressors (1) Rand-K; (2) Top-K + EF; (3) induced compressor with C1 = Top-K/2 and C2 = Rand-K/2; (4) Top-K, where K is a tunable parameter but is the same among all compressors. I wonder how their performances vary if the compressors use different values of K. For example, how do the compressor Rand-K/2 and the induced compressor with C1 = Top-K/2 and C2 = Rand-K/2 compare to each other (i.e., using unbiased C2 directly instead of transformation)? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}