{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors introduce new evaluation criteria and methods for identifying salient features: rather than earlier approaches which attempt to 'remove' or marginalize out features in various ways, here they consider robustness analysis with small adversarial perturbations in an Lp ball.  For text classification, a user study is included which is appreciated.\n\nIn discussion, the authors addressed many points and all reviewers converged to recommend acceptance.\n\nA couple of points could be discussed further if space permits:\nthe impact of type of perturbation employed; and\nthe connection between optimizing for adversarial robustness and optimizing for insertion/deletion criteria."
    },
    "Reviews": [
        {
            "title": "Good ideas for explanations and benchmarking, needs more motivation and more detailed evaluation",
            "review": "Summary: The goal of the paper is to present a new explanation benchmark and explanation method for ML models. I think the benchmark is interesting but insufficiently motivated, especially given existing concern in the field about the degree to which small local perturbations yield biased results because of saturation. The explanation method seems to work well, even on removal-based benchmarks, which is great! Evaluation is limited w.r.t. datasets and methods compared against, however. Overall, some serious positives and important negatives make my impression borderline, leaning reject. However, addressing the weaknesses well could definitely raise my score.\n\n**Update**: The authors' update is comprehensive, well-thought out, and demonstrates significant improvements to the paper; I have raised my score to reflect this.\n\nObjective: Use the adversarial robustness framework to develop both a benchmark for ML model explanations and new methods to actually explain ML models.\n\nStrengths: \n* I think the distinction from previous removal-based work, which to my knowledge is the primary benchmarking focus, is well-posed.\n* The adversarial robustness framework is presented very clearly and seems to get at a different question than previous benchmarks, which is a useful contribution.\n* The explanation method seems well-designed.\n* Experiments show very good results for Greedy-AS.\n* I really appreciated Table 2, because it does not seem like enough to show Greedy-AS does the best job of optimizing for the thing (only) it optimizes for. These additional results are impressive, though I wish I understood why Greedy-AS does better on them.\n\nWeaknesses:\n* From the introduction, I am a little concerned about this method because of its intentional focus on small perturbations. This is a well-known problem with gradients that integrated gradients improves on by adding a reference; gradients measure small perturbations around the input, but do a bad job detecting important features because nonlinearities often saturate around the inputs. I think there's a lot of empirical evidence that IG improves on grads for this reason. The benchmark is subject to this issue -- I tend to think a priori that a test where grads shows good performance may be a flawed test -- and I'd tend to think the explanation method would suffer for this reason as well. Overall, it is known that explanation methods exist that do not need references; these just tend to be worse on a lot of benchmarks, for reasons like saturation. There's a lot of room here to boost the value of the paper, in my opinion. (1) Explain why saturation isn't an issue with this method and (2) Tie the good results with Greedy-AS to what seems to be the key motivation for its introduction -- the independence of reference. If you could show that IG, etc are giving worse results *because* they're being biased by their reference, it'd be very helpful.\n* It makes sense to me that Greedy-AS does better on the benchmark that inspired it, but why does it do better on the removal-based methods? In some sense, this is exactly what methods like SHAP should be optimizing for, so the biggest drawback to this result (which is a good result for your method!) is that I don't have an intuition for why it should be the case. I'd also be interested to see a comparison with a method like Expected Gradients (Erion et al 2019, Sturmfels et al 2020), which averages IG-type explanations over multiple references by default. I also would like to see results on a low-dimensional, maybe tabular dataset, because when SHAP is entirely sampling-based it will have trouble with large numbers of pixels. I'd also be interested in seeing the SHAP results on text. I think it may do a lot better in that case. Even if SHAP wins on tabular or text data, this could be a good thing for your paper, because then it'd be clearer under what conditions Greedy-AS works well.\n* A related concern is the choice of explanation methods in the experiments; for example, IG is one of the best performers in the Table 1/2 benchmarks, but is not used in Figures 4 and 5. Some justifications for the methods picked in these figures would be good; right now it seems like some of the lower-performing methods are being displayed while ideally the strongest benchmarks would be displayed. I think some justification is required here before publication. \n* I'm actually more sold on the explanation method than on the benchmark; I think there's not quite enough justification for the benchmark aside from \"other methods are biased by the reference.\" However, there's not much empirical or theoretical basis for believing the use of a reference is intrinsically bad (as discussed above, references may induce bias, but small perturbations can be biased by things like saturation). I know it's hard to show why one benchmark is better than another, but if the benchmark is a key contribution I think I need more analysis on why it's a better benchmark.\n* I'd also appreciate a bit more detailed results on the non-MNIST experiments, to convince me the method works well in many settings.\n* Overall, having more non-image experiments, and associated discussion, could be helpful. Right now almost all results are images, with one small text example, and I wouldn't be surprised to see very different results on different types of data.\n* I'm concerned the method could be very slow, given that the adversarial optimization is itself embedded in a sampling-based procedure for the Greedy-AS method. Can you add some discussion of the runtime?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Many interpretability techniques center on identifying the subset of “most relevant features”. In this work, the authors propose defining this set as the set of features that are easiest to attack adversarially (in an $L_p$ sense). \n\nFirst off, the paper was somewhat difficult to read because of a frustrating amount of vertical space LaTeX hacking, so much so that the spacing between sections and paragraphs is even less than the normal spacing between sentences. This is not a good way of squeezing everything into 8 pages. \n\nApart from that, the paper is comprehensive in its experimental evaluation, with a good suite of appropriate baselines, sanity checks, and human studies. I think that it is an interesting complement to the current suite of feature attribution techniques. Conceptually, it is quite similar; as the authors note, there have been many related techniques that try to “remove features” by adding noise to them, setting them to a baseline value, or blurring them; here, the authors instead consider adversarially perturbing them, and they propose a modified greedy strategy that seems to work well. It is a bit unclear to me why considering adversarial perturbations is a priori any more convincing than, e.g., considering blurring out or adding noise to the selected features, but they serve slightly different purposes and it's reasonable that these methods could be complementary.\n\nEmpirically, this method leads to gains under insertion and deletion metrics, compared to existing techniques. I had a couple of questions about these baseline comparisons:\n\n1. I’m a bit confused as to why BBMP does so poorly under the insertion and deletion games, given that it explicitly optimizes for these. Could the authors discuss this? From Figure 11, it looks like the implementation of BBMP seems unoptimized; at least, it returns degenerate results that are very different from the examples shown in the original paper. There's a follow-up https://arxiv.org/pdf/1910.08485.pdf that similarly optimizes for a mask of a fixed size, which seems to outperform many of the other methods here, so I'm confused why the results are so different.\n\n2. S5.2: The authors say that SHAP obtains a better performance under the deletion criterion because it exploits adversarial artifacts. Shouldn’t this problem be even worse for the Greedy(-AS) methods, which explicitly seek to exploit adversarial artifacts? Why are we not seeing that?\n\n**Details:**\n\n3. Assumptions 1 and 2 are not precisely defined; please either state them formally, or make it clear that they are informal motivations. \n\n4. Definition 3.1: is $\\epsilon_S^*$ a function of $x$ and $y$? How are the overall Robustness values calculated then? By averaging? \n\n**Update**\nThank you to the authors for the detailed responses and revisions. I have adjusted my score upwards. I think this is a useful exploration of an alternative means of quantifying feature importance, with intriguing results: somehow, optimizing for adversarial robustness also seems to optimize for the score on insertion/deletion games. Further exploration of that issue (or at least, elevating some of the many appendices on that issue to the main text) would, in my opinion, increase the impact of the paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper focusing on evaluating explanations through robustness analysis, using adversarial perturbations. Doubts about the basis for that and the \"human\" study. I dont know if another ML/AI will be more suitable for this paper than ICLR..",
            "review": "The main contribution of this work is the proposal, implementation and validation of a new evaluation criteria/measure for explanations based on robustness analysis. The main idea, as I understand it, is innovative, adversarial perturbations: other feature explanation solutions rely on removing features, the method here presented learns a set of features so that we get the smallest perturbation that change in outcome for the selected set and largest perturbation that changes outcome for the rest of the features.\n\nI think that interpretable ML and  Explainable AI need more work on evaluation, and providing measures for that is relevant in these areas. So, I wonder if other conferences in ML/AI will benefit more of this work than ICLR. In general I find the paper strong, interesting and innovative, technically sound.\n\nThe paper provides a very strong empirical analysis using various datasets, even if for me it was hard to follow all the figures, explanations, methods Greedy/Grad. It is hard to interpret some of the figures, fig 2 and 3 provided, since we need to analyze pixels, and check of they are “less noisy”, “rather noisy”, “noisy” etc….\n\nI am not very familiar with using “human study” for a user evaluation, use user study. “Human” study sounds odd to me. \n\nI was hoping that the user study announced in the abstract and introduction would validate the presented metric, and the methods studied. This is really important, since we make explanations to people. I was disappointed to see that the so called “human study” is described in just few lines, telling that 10 people (without specifiying anything else, what kind of participants were those?) labelled the sentences to show that Greedy matches best their labelling (I guess that 5 people per Grad/Greedy). It is hard to see how this user study strengthens the paper, what is the difference between this evaluation and the other presented in the paper were other “humans” observe things on the data/images? It looks to me that the user study is an afterthought, a good attempt to complement the paper, but either is properly done or removed.\n\n\nBut one of my main concerns is actually is the proposed solution, like the actual motivation or basis for it, can they really identify features that are responsible for the predictions made or just find regions that have high sensitivity to changes (adversarial perturbations)? It might be that I got lost in all the evaluations, and I cannot figure out if the real features the model used to make the prediction will be found, or just those that make the prediction change…. Is that so? Are those the same features?\n\n\nWhile reading, I found some small grammar issues; please, double-check the text. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An effective explanation method based on robustness",
            "review": "The paper proposes an effective explanation method based on a notion of robustness defined by the authors.\nThe paper is well presented and easy to follow. It compares against state-of-the-art methods and provides valid and statistically significant experimentation. The explanations returned are different from the competitors and useful.\n\nHowever, some points that should be addressed before publication.\nFirst, I suggest to add the evaluation measures for robustness presented in \nHara, S., Ikeno, K., Soma, T., & Maehara, T. (2018). Maximally invariant data perturbation as explanation. arXiv preprint arXiv:1806.07004. \nThe evaluation of Hara is similar to the one adopted w.r.t Insertion/Deletion.\nIn addition, an evaluation measure for robustness different from everything presented in the paper but quite important for having a different validation is the one proposed in \nMelis, D. A., & Jaakkola, T. (2018). Towards robust interpretability with self-explaining neural networks. In Advances in Neural Information Processing Systems (pp. 7775-7784).\nbased on the local Lipschitz estimation.\n\nSecond, an explanation similar to the one reported in the paper is returned by the method presented in \nGuidotti, R., Monreale, A., Matwin, S., & Pedreschi, D. (2019, September). Black Box Explanation by Learning Image Exemplars in the Latent Feature Space. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 189-205). Springer, Cham.\nAnother method that should be considered for comparison as it is based on robust anchors is the one presented on:\nRibeiro, M. T., Singh, S., & Guestrin, C. (2018, February). Anchors: High-Precision Model-Agnostic Explanations. In AAAI (Vol. 18, pp. 1527-1535).\nIt would be interesting to compare the Greedy proposals with these two methods both quantitatively and qualitatively.\n\nThird, an important aspect that is missing for the model proposed is either an analysis of the complexity or even better, the running time for returning explanations. In this way, the user can understand which is the best compromise between robustness and speed.\n\nMinor issues:\n- The results on textual data are not as convincing as those on images. The authors should treat this part of the presentation carefully and with more attention.\n- The authors do not specify in the main paper (or it is not easy to find) which are the classification models explained.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}