{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a novel method for representing persistence diagrams by embedding them to a Poincare ball.  The representation is learnable, and unifies essential and non-essential features.   The experimental comparisons with existing representation methods show significant improvements in performance.   \nThe flexible data-driven embedding to a suitable geometric space is a novel idea, which will certainly advance the usefulness of TDA.  The  experimental resutls demonstrate well the advantage of the proposed repesentation.  The authors have also addressed the review comments appropriately, with some extra experiments.  This is also a good addition.   "
    },
    "Reviews": [
        {
            "title": "Insufficient Experiments",
            "review": "In this paper, the authors proposed a new representation of persistence diagrams that can include `''essential features''. Essential features correspond to the intrinsic topology of the underlying space that will not die during the filtration. To include the fact that the essential features are infinitely far from other normal features in the diagram, the authors proposed to use a Poincare ball representation, which maps the diagram into a disk whose boundary is infinitely far from inside. \n\nThe authors further proposed a classifier that learns the parameterization of the embedding of a diagram in the Poincare ball. The presentation learning procedure seems to be similar to (Hofer et al. 17), except for using a Poincare ball representation instead of the Euclidean square/triangle representation. Experiments are carried on graph classification tasks and image classification task.\n\nOn the positive side, I think the proposed representation well unified essential and non-essential topological structures. It is elegant and well-thought. A stability theorem is proven (in a similar manner as other known representations). The references are also reasonably complete.\n\nHowever, I am having doubts on the practical motivation of the representation in the learning context. To me, essential features can be considered by simply adding the histogram of their birth times as additional features (to the neural networks at an appropriate layer). I think this approach is a natural and necessary baseline to be compared with.\n\nGenerally, the empirical results are not particularly strong. On graph datasets, the proposed method is only winning 2 out of 5 times. Many important baselines are also missing. For graph classification methods, state-of-the-art classifiers such as GIN and GraphSAGE should be at least compared with. For topological classifiers, many kernel methods could be compared with: PWGK, PI (both of which were used in the other experiment), also Sliced-Wasserstein Kernel. I honestly think an ensemble of these methods and the histogram of birth times of essential diagrams can be easy to tune and perform better. \n\nOther questions/comments:\n\nThe references are not standard and need to be fixed. \n\nFor graph with Rips filtration, why are there 1-dim essential homology? Wouldn't all 1D homology features be killed eventually?\n\nDetails of the classifier (how the representation is learned) is still not clear even after reading the section in the supplemental material. \n\nExperimental details are missing. I understand this is 80% training 20% testing. But how many folds were used to evaluate? Some baseline numbers are very similar to the numbers in the GIN paper. However, the GIN paper was using a 10-fold cross-validation. So there is some discrepancy in the experiments. I would appreciate if the authors could kindly elaborate.\n\nOverall I think this is a nice mathematical formulation to incorporate essential features into the representation of persistent homology. But the practical usage in learning is not very convincing. Fundamentally, the essential features are completely different from non-essential ones. There is nothing in between them. Thus the benefit of a unified representation does not bring much more information than simply treating them separately.\n\n\n** After rebuttal: \nI am increasing the score to 6. I appreciate the authors' response to the reviews. They did the additional experiments I asked for. \n\nAs I stated in the original review, I really liked the unified approach. It is elegant and is nicely presented.\n\nAfter reading the authors' response to R4, it is clarified that the 1D essential homology is because the computation over all threshold is too expensive. I think there might be an opportunity to better justify this paper: we often have to stop the filtration early due to computational concern. This unified representation could potentially be a good solution for this: without computing the actually death time, the unified solution can still 'learn' the real death time of the 1D essential classes. The authors might want to discuss or ideally empirically verify this in the final version. For example, can you show that using the new approach, and stopping earlier during the filtration, the unified classifier can be as good as when we run the whole filtration and compute the real death  time for all 1D classes. Moreover, it will be ideal if the authors can manage to show that the unified approach can actually learn the real death time for these fake essential classes (I do not know how). This way the paper can potentially have a bigger impact. \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction but needs to be motivated and discussed more",
            "review": "The authors propose to learn representations of topological persistence diagrams in hyperbolic spaces (Poincare balls). They provide a step-by-step methodology, and an algorithm for creating the representation. They also compare their approach with various graph classification and other ways of representing persistence diagrams.\n\nStrengths:\n=========\n1. This is the first work that learns hyperbolic representations of persistence diagrams.\n2. The methodology is sufficiently clear and the paper is reasonably well-written.\n3. Applications to graph data as well as image data show some promise.\n\nThings to improve:\n=================\n1. My biggest question in this paper is around the assumptions the authors have made and the motivation. They have mentioned that essential features have infinite persistence and truncating the persistence is not the best thing to do. I agree, however, there is really only one essential feature - the 0th order persistent homology group that becomes a single connected component at a scale that is sufficiently large. Since this \"essential\" group is one and the same for all point clouds that we compare,  they always match to one another, and it is safe to say that this truly essential group can be ignored in ideal cases. All other \"essential\" groups are only artifacts of our construction, which happen because we do not use a large enough scale for computational or some other reason.\n\n2. Even in an ideal setting (with large enough scales), it would be good to discuss why hyperbolic coordinates make sense. Why do we expect the distance between the persistence homology module to have some hyperbolic trend (see fig. 2 in https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/). Some thought and discussion around this would be helpful to understand the motivation of the approach.\n\n3. Based on fig. 2 I was initially under the impression that individual points will be mappable between persistence diagrams and hyperbolic representations, but it took some more reading to understand that there is one single representation for the entire persistence diagram in the hyperbolic space. Perhaps the authors can make it clear somewhere (even in Fig. 2).\n\n4. How is x0 chosen in (11). Why is summing in tangent space meaningful? Why not some  other operation?\n\n5. What is the absolute state-of-the-art  for Table 1 and Table 2? I am not holding this as a negative, but it may be good for the readers to know where TDA methods stand in these applications.\n\n6. Along similar lines, are there any applications that are especially suited to TDA that the authors can demonstrate? \n\n7. The authors seem to have used multiple runs for results in Table 1, but the text in Sec. 4.1 does not indicate how it is done. Please elaborate.\n\n8. Why are some values missing for some methods in Table 1.\n\n9. Why does P-Eucl make sense as a baseline? Is this like a so-called ablation study for this procedure? How are the representations created for P-Eucl? Please feel free to add more details on this in supplement.\n\n10. In Sec. 4.2, do the  compared methods also use the same simplicial construction (cubical complexes constructed in a  specific way)? If so, would the compared baselines work better with some other construction?\n\nIn summary, this is an interesting direction, but clarity is needed regarding the motivation,  and presenting extra details along the way will also be appreciated.  Finally, it could make sense to think about  an appropriate application.\n\nUpdate post-rebuttal\n==================\n- I am happy with the thorough engagement by the authors and their clarifications. So  I am bumping the up the score a notch.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Learning Hyperbolic Representations of Topological Features",
            "review": "The authors propose to learn a representation for the persistence diagram (PD) in the hyperbolic space to incorporate the essential features (i.e., infinite persistence). The authors show that the hyperbolic representation has stability. Empirically, the authors illustrate that the hyperbolic representation for PD compares favorably with other baselines on graph and image classification.\n\nThe motivation to use hyperbolic representation to include essential features for PD is interesting. The authors give some details about the background (e.g., persistence diagrams, Poincare ball). However, the main part of the framework and how to learn the parameters of the embedding are missing.\n\nAs I understand, the authors propose: \n\n(1) to use some auxiliary transformation to lift points of $R^2$ into $R^m$ (however, there are no properties or information about this auxiliary transformation, and how one can do it)\n\n(2) projects points in $R^m$ into the Poincare ball, parameterized by $\\theta$. Why ones need this parameterization, and why this parameterization is important in applications? (It seems the authors combine projection with some transformation here?). It is better in case the authors give the projection (w.r.t. what distance?) and then transform the projected points. \n\n+ There is no description of the space of $\\Theta$ for the parameterization?\n\n(3) combine the representations of each point in PD. It seems that the authors use the sum of all points w.r.t. hyperbolic manifold to represent PD.\n\n+ I also concern about the usage of exp and log map at point $x$ for this combination. Typically, the tangent space is just simply a \"flattened\" space of the hyperbolic space at point $x$, it can only preserve the geometry for some close neighbor points of $x$ (this \"flat\" approach has a large distortion for those points which are far to $x$. It is better in case the authors give more discussion about how to choose $x$, and how it affects the geometry of the hyperbolic?\n\nThe main framework to learn those parameters is not presented in the main manuscript, and it is also unclear how one uses the learned hyperbolic representation for the downstream task?\n+ It seems that the authors incorporate the procedure to learn those parameters inside the networks for classification and learn end-to-end?\n\nSomehow, it seems that the authors propose to use a hyperbolic embedding within a neural network as a classifier for PDs. (in my opinion, the novelty may be incremental in case the authors simply replace the Gaussian-like embedding of Hofer et al. into a hyperbolic embedding inside a neural network. However, the hyperbolic representation for PD may be still interesting by itself. )\n\nOverall, I think this work seems interesting and has good potential. The authors may need to describe more details about the neural framework use to learn the parameters of the hyperbolic representation.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A novel embedding method for persistence diagrams",
            "review": "# Synopsis of the paper\n\nThis paper proposes a novel embedding or representation algorithm for\npersistence diagrams, i.e. topological descriptors. Existing methods are\nrestricted because they do not feature *learnable* or *trainable*\nparameters for their representations (with the exception of methods such\nas the one by Hofer et al. or PersLay). This method, by contrast,\npresents a trainable embedding to the Poincaré ball, thus representing\npersistence diagrams in a hyperbolic space.\n\nThis has the advantage of being more appropriate for representing\n*essential features*, i.e. features of infinite persistence. Thus,\nit is possible to perform end-to-end training of neural networks\nthat use persistence diagrams as their input.\n\nExperiments with graph classification and image classification tasks\ndemonstrate the utility of the proposed method.\n\n# Summary of the review\n\nThis is a very well-written paper, with a strong contribution to\ntopological data analysis and machine learning. The paper is technically\nsound, apart from some minor inconsistencies, which I shall discuss\nbelow. It is exciting for me to see how to obtain fully-trainable\nembeddings here, and I am happy to endorse this paper.\n\nThere are a few issue that I would like to see rectified in a revision\nof the paper, though:\n\n1. Some details on the method are missing. This concerns primarily the\n   learnable auxiliary transformation from Eq. 7. While the paper\n   subsequently discusses potential choices for this function, this is\n   not specified. In fact, the method tying everything together even\n   cites this equation again, without providing a definition for it.\n\n   This needs to be rectified---I am assuming that one could, for\n   example, use a transformation such as the one provided by Hofer et\n   al.; is this correct?\n\n2. The experiments on graph classification are lacking some comparison\n   partners. Specifically when discussing topology-based approaches, it\n   is useful to compare to other topology-based approaches. Here are\n   some suggestions:\n\n      - Hofer et al.: *Graph Filtration Learning*\n      - Rieck et al.: *A Persistent Weisfeiler–Lehman Procedure for Graph Classification* \n      - Zhao & Wang: *Learning metrics for persistence-based summaries and applications for graph classification*\n\n    There is an overlap of the data sets assessed using these methods,\n    so the appropriate results could be cited and used. I wanted to\n    raise this concern primarily because I am aware of other methods\n    obtaining somewhat better classification performance in some cases.\n    \n    It would strengthen the paper immensely if it could perform some\n    form of 'ablation study', i.e. highlighting to what extent the\n    improved results are driven by the embedding in hyperbolic space, or\n    the choice of filtration, etc.\n\n3. Adding to this, some details of the method need to be described\n   better; only the supplements briefly mention how $m$, the embedding\n   parameter, is chosen in the end, but I would prefer a more in-depth\n   analysis of the choice of this parameter. Is it sufficient to pick,\n   say, $m = 32$ for all practical purposes? Or does it make sense to\n   concatenate the representations afterwards, as discussed in the\n   supplements? Adding more details will aid in understanding the paper\n   and, ultimately, promote further adoption of the method.\n\n# Detailed comments\n\n- In the abstract, I would say that 'Existing methods are restricted in\n  terms of their expressivity'. I would use 'bottleneck' only to refer\n  to the eponymous distance, in order to prevent confusion.\n\n- The definition of 'filtration' in the introduction is incorrect;\n  a filtration is a *sequence* of subspaces. \n\n- I would not refer to diagrams being *injective*. I understand what is\n  mean here, viz. the fact that under some assumptions (!), one can\n  reconstruct the input space from a diagram (this is known as solving\n  the inverse problem), but I think this might be slightly confusion\n  here. Why not focus on the expressivity properties of diagrams and the\n  fact that one can *approximate* their inputs under some conditions?\n\n- The comment on 'extended persistence' on p. 2 is slightly imprecise;\n  'extended persistence' is well studied now, but PersLay is indeed the\n  first 'deep learning' method incorporating it. This should be\n  clarified.\n\n- When discussing filtrations and homology groups, I would stick with\n  $\\subseteq$ instead of $\\subset$. The former is more generic and would\n  make the write-up more consistent, as currently both forms are being\n  mixed.\n\n- As outlined above, I would suggest to be more verbose when it comes to\n  the description of the method in terms of the individual functions,\n  i.e. $\\psi$ and $\\rho$.\n\n- Am I mistaken or could $\\rho$ also be *any* universal approximator,\n  such as a set function or, more generically, a deep neural network?\n  Would it be possible to use the method by Hofer et al. (2017) to\n  obtain this map and *then* subsequently train a better hyperbolic\n  embedding?\n\n- When running the experiments, is $m$ fixed or is the best $m$ selected?\n  I see that $m \\in \\{2, \\dots, 12\\}$, but I do not understand how this\n  is actually used in the network. Figure 4 is somewhat helpful here,\n  but it would be interesting to see what happens for a range of\n  parameters.\n\n- It is my understanding that the Euclidean method P-Eucl is only driven\n  by $\\rho$. Is this correct? How critical is this choice in practice?\n  Moreover, how was it chosen for this paper? I am asking because it is\n  clear that the hyperbolic embedding helps improve predictive\n  performance, but I wonder what would happen if one uses an\n  'underpowered' $\\rho$ function.\n\nAll in all, I feel that this could make a very strong addition to the\ntopological machine learning literature!\n\n# Style & clarity\n\nThe paper is very well-written, the authors are to be commended for\nthat. I have a few minor suggestions, some of which are more personal\npet peeves, but which might help make this paper shine even more!\n\n- 'root of a complex polynomial' --> 'roots of a complex polynomial' (?)\n\n- I would prefer not to use citations as nouns, i.e. I would prefer\n  writing 'Kusano et al. (5)' instead of 'In (5), Kusano et al.'; the\n  former strikes me as more readable and also translates well to\n  different citation styles.\n\n- The image of $f_n^{i,j}$ is *the* $n$th persistent homology group\n\n- Whenever possible, I would use $\\operatorname{text}$ for operators or\n  functions, instead of 'raw' $text$. This concerns for example the rank\n  function but also the degree function.\n\n- I would write 'Poincaré' everywhere\n\n- 'persistent diagram' --> 'persistence diagram'\n\n- 'persistent image' --> 'persistence image'\n\n- 'Vietories' --> 'Vietoris'\n\n- 'gray-scale' --> 'grey-scale' (or vice versa, if American English is\n  the preferred spelling)",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}