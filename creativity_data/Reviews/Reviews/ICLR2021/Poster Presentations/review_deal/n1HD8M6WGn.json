{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes fine-grained layer attention to evaluate the contribution of individual encoder layers. This departs from the standard transformer architecture where the decoder uses only the final encoder layer. This paper investigates how encoder layer fusion works, where the decoder layers have access to information for various encoder layers. The main finding of the paper is that the encoder embedding layer is particularly important. They propose SurfaceFusion, which only connects the encoder embedding layer to the softmax layer of decoders, leading to accuracy gains.\n\nThere was some disagreement among reviewers about this paper. Overall, I found this a simple but effective contribution with interesting findings that can help future research in seq2seq models. Some of the weaknesses (discussing other relevant works, discussing other variants of FGLA, adding new experimental results) have been addressed in the updated version of the paper. One of the reviewers suggested running additional experiments on GLUE-style tasks (with a masked language model) to be really sure if the technique is convincing, and particularly try it with larger models (T5 was suggested). While adding those experiments would be a plus, I disagree that this is crucial - this paper is focusing on seq2seq tasks and is already considering several tasks: summarization, MT, and grammar correction. The results found by this paper are interesting and can foster future research extending this beyond these 3 tasks. Even if larger models can make the improvements smaller, there are many inconveniences in just increasing scale (memory consumption, energy consumption, etc.) It is my opinion that the community should value research that tries to understand the weaknesses of smaller models, rather than relying on large scale models to solve all problems."
    },
    "Reviews": [
        {
            "title": "Interesting idea, but I have questions",
            "review": "This is an interesting idea where the authors propose \"SurfaceFusion\", where they use the source embeddings learned by the encoder to modulate the output of the decoder at the final layer. The authors claim this is because the embeddings contain valuable information that is lost during encoder processing because the encoder lacks the capacity to represent both semantic and surface features. The authors then show through a series of experiments that attending over the encoder embeddings is useful, and propose a way to integrate the information from the embeddings directly into the last layer of the decoder, showing that this improves experimental results.\n\nFew comments:\n\n1) there are some grammatical and spelling errors, e.g.: \"analyses\", \"which is the EncoderFusion expected to solve\"\n2) while the premise and analysis are interesting, i am curious about the reasons behind the design choice of \"SurfaceFusion\". If the encoder source embeddings are very important, and layerwise/finegrained attention perform similarly well, why not simplify the approach and simply add/concat them to the output of the encoder (or somewhere else where it makes sense)? have you tried this instead of introducing additional hyperparams?\n3) what are the costs in terms of wall-clock time when introducing an additional softmax operation for every token in the decoder?\n\n---\nupdate: \n\nThanks for the clarifications. I've read the response and other reviews and have updated my rating.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting paper investigates how encoder layer fusion works in a multi-layer encoder",
            "review": "#####################   Summary   ####################\n\nThis paper introduces the fine-grained layer attention to evaluate the contribution of individual encoder layers and investigate how encoder layer fusion works, where the decoder layers have access to information for various encoder layers as opposed to only the final encoder layer as in standard Transformer.\n\nBased on the observations that the encoder embedding layer is important to the success of encoder layer fusion and the uppermost decoder layer pays more attention to the encoder embedding layer, this paper proposes SurfaceFusion, which only connects the encoder embedding layer to the softmax layer of decoders, leading to quite substantial gains in metrics such as BLEU.\n\n#####################    Strengths   ####################\n1. This paper is really clearly written. The paper is easy to follow and understand.\n2. The proposed SurfaceFusion is well-motivated by a series of experiments on different Seq2Seq tasks.\n3. The experimental results are very solid.\n\n#####################   Concerns   ####################\n\nI hope the author can select several following concerns as feedback.\n\n1. In this work, the authors find that in the summarization model, almost all decoder layers focus more on the encoder embedding layer. In other words, compared with the translation model and correction model, the summarization model tends to use more surface features from the encoder embedding layer. Therefore, it is reasonable to speculate that the summarization model will receive more improvements from the proposed SurfaceFusion. However, the experimental results in Table 1 show that the translation and correction models achieve higher improvements than the summarization model. \n\n2. Figure 1 shows that the uppermost decoder layer also pays attention to other encoder layers, why SurfaceFusion only uses the encoder embedding layer? The authors claim that “although the layer attention model partially alleviates this problem, it potentially introduces unnecessary intermediate encoder representations.” Can you show some visualizations/experimental results to support this hypothesis that the intermediate encoder representations are unnecessary for the uppermost decoder layer?\n\n3. The experiments show that the encoder embedding layer is beneficial for all decoder layers, why the proposed SurfaceFusion does not consider connecting the encoder embedding layer to all decoder layers. A much wider empirical investigation would be appreciated.\n\n4. Along the same line, some relevant works are omitted [1][2][3][4][5][6], especially for the work from [1], which also considers the connections between the lower encoder layers and uppermost decoder layer, and boosts similar or even higher results on the same benchmarks (EN-DE and EN-FR) as this work. I would suggest the authors take a discussion and a comparison to that work.\n\n5. The margin of some gains in Table 1 is small. Statistical significant test and error range are highly appreciated.\n\nOverall, this paper is really clearly written, the proposed SurfaceFusion is novel and well-motivated, and experimental results are very solid. I recommend the authors to enrich this work by deepening their analysis. A much wider empirical investigation would be appreciated.\n\n[1] Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning.\n\n[2] Neuron interaction based representation composition for neural machine translation. In AAAI 2020.\n\n[3] Dynamic layer aggregation for neural machine translation with routing-by-agreement. In AAAI 2019.\n\n[4] Learning deep transformer models for machine translation. In ACL 2019.\n\n[5] Dense information flow for neural machine translation. In NAACL-HLT 2018.\n\n[6] Multi-layer representation fusion for neural machine translation. In COLING 2018.\n\n#####################    After Rebuttal   ####################\n\nI thank the authors for responding to the comments and have read them carefully.\nThe authors have addressed all my concerns in the rebuttal and I vote for acceptance of this submission.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Comments on the paper",
            "review": "The paper proposes a *fine-grained layer attention* (FGLA) to analyze Encoder Fusion method. \n\nWhile the introduction of FGLA allows for the analysis of the contribution of individual encoder layers, it changed the model’s architecture. Thus, when a model with FGLA is trained, it’s optimized differently such that the use of each separated layer in the encoder is modeled explicitly through attention weight. As a result, the contribution of each layer found in Seq2seq with FGLA might not be the same as in the standard Seq2seq. Therefore, I think the conclusion might not be appropriate for the standard Seq2Seq model. In other words, the source representation bottleneck hypothesis might not hold true for standard Seq2seq.\n\nCompared to the previous fusion method that uses a scalar per layer as attention weight (i.e., *Transparent Attention*) in the work of Bapna et. al., 2018, FGLA generalizes it by making a vector per layer. This generalization is straightforward and simple. The later analysis in the paper is carried out on FGLA. However, I find that the same analysis can be performed with Transparent Attention. The introduction of FGLA seems not well-motivated in this regard. In terms of interpretability, Transparent Attention is more interpretable since there is one scalar per layer, whereas FGLA has to report average weight per layer (Figure 1).\n\nFor experimental results, FGLA is not compared with other fusion methods. Thus it’s unclear how the proposed method positions itself in the previous work on Encoder Fusion.\n\nA previous work of ([Nguyen and Chiang, 2018](https://www.aclweb.org/anthology/N18-1031/)) proposed a similar way of fusing source embeddings directly to the output of the decoder. Their work can be seen as Surface fusion proposed in this paper.\n\nWhile the motivation for this kind of fusion might be obvious for machine translation, it’s less so for summarization and grammatical error correction. I think it’s worth spending some text/examples on why this is needed for those tasks.\n\n\n**References**\nImproving Lexical Choice in Neural Machine Translation. Nguyen and Chiang, NAACL 2018\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Thorough analysis of encoder fusion asking for more focus on impact",
            "review": "The authors perform a thorough analysis of encoder fusion for Transformers: which encoder layer should the N-th decoder layer attend to? It turns out that the final decoder layers often attend to the encoder embeddings, leading the authors to provide them to the last decoder layer which leads to small improvements of performance on machine translation, summarization and grammar correction tasks. These are nice results, but the gains are small and the models are tested in the very basic configurations. These tasks and techniques, as well as some numbers used to claim state-of-the-art, are from a few years ago (e.g., SOTA on en-de translation is higher currently than the authors claim and higher than their number). It would be interesting to see if the presented conclusions hold for larger models - esp. for a T5 Transformer on masked language modeling, as this would be a more commonly used model in 2020. Unluckily, it is quite possible that increased activation size may negate the benefits of the authors' technique. It may well though make it even more important -- it would be really good to know! Lacking these experiments, we cannot recommend acceptance at this point.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}