{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Quantization is an important practical problem to address.  The proposed method which quantizes a different random subset of weights during each forward is simple and interesting. The empirical results on RoBERTa and EfficientNet-B3 are good, in particular, for int4 quantization.  During the rebuttal, the authors further included quantization results on ResNet which were suggested by the reviewers. This additional experiment is important for comparing  this proposed approach with the existing methods which do not have quantization results on the models in this paper. "
    },
    "Reviews": [
        {
            "title": "Official Blind Review #5",
            "review": "This paper introduces Quant-Noise that quantizes a random fraction of the network at each step instead of quantizing the entire network. The experiments show that the proposed Quant-Noise can improve the accuracy of quantized neural networks. \n\nPros:\n1. The proposed technique is simple and easy to use.\n2. The proposed Quant-Noise shows very impressive accuracy improvements for int4 quantization. \n\t \nCons: \n1. The novelty of the proposed method is limited. Especially when combined with product quantization, Quant-Noise is essentially a form of structured dropout. I agree adding structured dropout may lead to some accuracy improvements. But I do not think this is one of the contributions of this paper. Besides, why Quant-Noise works for int4 quantization is not clear. \n2. The improvement of Quant-Noise for int8 quantization looks very limited. The proposed method seems to be useful for low-bit quantization, but not very effective for int8 quantization. If so, I think the authors should focus on low-bit quantization in the experiment section and test the proposed method on more low-bit quantization settings  (e.g., 2bits quantization, 3bits quantization, mix-precision quantization, etc).\n\nOverall, I think the novelty of the proposed method is limited and why it works is not very clear. Besides, I think the proposed method should be tested on more low-bit quantization settings.  I recommend \"Marginally below acceptance threshold\".",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice Results but More Analysis is Required. ",
            "review": "Contribution:\n1. conduct Quantization-Aware-Training via introducing quantization noise during optimization. Further, Quant-Noise can be applied to existing trained networks\n2. The proposed method is validated on multiple models like Transformer, ConvNet, and more challenging EfficientNet-B3. Experiential results show that the proposed method is practical.\n\n\nCons:\nProduct Quantization (PQ) is not of much novelty since it was studied by Stock et al. This paper combines fixed-point and PQ and tries to claim that 'Fixed-point quantization and Product Quantization are often regarded as competing choices'. However, this claim is under-explained. \n \n\nOne of important novel points in this paper is subset quantization. A potential advantage of subset quantization is to mitigate the Gradient Mismatch problem caused by STE. This motivation makes sense to me because the gradient of unselected parameters is unbiased. This unbiased gradient will improve the quality of the gradient for the previous layers. However, a similar training scheme for both pruning and quantization was studied in related work [1, 2].\n\n[1] Guo, Yiwen, Anbang Yao, and Yurong Chen. \"Dynamic network surgery for efficient dnns.\" Advances in neural information processing systems. 2016. \n[2] Zhou, Aojun, et al. \"Incremental network quantization: Towards lossless cnns with low-precision weights.\" arXiv preprint arXiv:1702.03044 (2017). \n\nThere are some technical differences among these methods. However, considering such a similarity, more analysis is necessary to help us understand why subset quantization benefits (and helps reduce gradient mismatch). Otherwise, the novelty of this submission will be incremental. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work proposes a simple modification to the STE-based quantization aware training to improve the accuracy of quantized neural networks. This proposed method achieves significant accuracy improvement when using INT4 quantization on ImageNet and Wikitext. The introduced quant-noise function is flexible as it can combine pruning with quantization to further reduce the size of the network size.",
            "review": "I agree with the three key contributions listed in the paper. The paper is well written and clearly articulates a contribution to the literature. The proposed Quant-Noise is intuitive and straightforward. The experimental evidence is provided for both image classification and language modeling tasks. Most of the related works are cited. The paper does not contain a theory part, but wherever possible, equations are provided to illustrate how the method works.\n\nConcerns:\nFor the experimental results, only one network is used to evaluate the proposed method. With different architecture, the accuracy gap between the full precision and quantized models could be different. It will be helpful to validate the proposed Quant-Noise on multiple networks. Moreover, the experimental evidence only shows results for INT4, INT8, and iPQ. As the proposed method should also work for fixed-point quantization and even binarization. The effectiveness of the proposed approach can be better justified by comparing the results with other SOTA fix-point quantization and binarization schemes in [1-4].\n\nI also have a question about the selection of the weights. The current scheme selects a random subset of the weights to add the proposed noise, which means the weights in different layers are chosen with the same probability. For pruning, existing work [5] finds that it is desired to prune different fractions of weights for each layer. Is there a better way to select the subset of the weights?\n\nReasons for score: Overall, I am leaning towards accepting the paper. I like the simple idea of controlling the imprecise gradients in BNN training by only quantizing a subset of weights at each iteration. However, fewer comparisons are made with other SOTA quantization/binarization methods. I would consider raising my score if the authors could address the aforementioned concerns.\n\n[1] Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm\n\n[2] Searching for Low-Bit Weights in Quantized Neural Networks\n\n[3] Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks\n\n[4] LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks\n\n[5] Pruning neural networks without any data by iteratively conserving synaptic flow",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents a new technique to quantize the weights and the activations of neural network models during the training. The authors first introduce Quant-noise, a method to simulate quantization during the training. Then Quant-Noise is applied to quantization methods such as int8 and Product Quantization, which are combined to obtain the final results. ",
            "review": "Reasons for Score:\nThe work proposed in this paper is novel and very well presented. The claims are supported by experimental results on different neural network models and applications, showing a good trade-off between accuracy and neural network compression.\n\nStrengths:\n1.\tThe Quant-Noise idea for quantizing the weights and activations of neural networks is innovative, and it can be easily applied to various quantization methods for compressing the neural networks and saving energy. \n2.\tThe results are very good in terms of compression rate and accuracy, and the experiments are obtained on complex state-of-the-art neural network models and applications.\n3.\tThe combination of int8 and Product Quantization results in hardware-deployable compressed models.\n4.\tThe background and the methods used are clearly explained, also from the mathematical and formal point of view. \n5.\tThe paper is overall clear and well written. \n\nMinor Comments:\n1.\tWhich is the training time overhead of having quantization noise during training?\n2.\tThe percentage (%) should be indicated in the vertical axes of Figure 2.",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official review comments #2",
            "review": "In this paper, the authors propose a new compression-aware training method utilizing quantization noise as a regularization technique. During training, only a different random subset of weights is quantized for forward propagation such that controlling the amount of noise is a way to improve model accuracy when extreme compression is applied.\n\nEven though the motivation is interesting and the selection of recently developed models (including EfficientNet-B3 and RoBERTa) is reasonable, this reviewer has the following critical concerns:\n \n- It is hard to understand what the paper aims to describe because several equations in the paper seem to be very confusing or incorrect.\n1) Since $z$ is an integer in equation (2) (i.e. $z$ is a rounded value), $(round(W/s+z)-z)*s$ is equal to $c=round(W/s)*s$. Then, it seems that we just round the real-valued weights with only a scaling factor without zero-point. This reviewer is wondering whether $z$ means 'zero-point'. It is very confusing.\n2) In equation (5), as far as I understand, $n$ seems to be the term related to activation, but there is no explanation about $n$.\n3) The term in equation (8), $y_{noise} * x^T$, needs to be explained in the aspect of gradients. If we follow equation (8), then we may update the weights with only the product of input and output of a layer. This reviewer cannot understand what the authors mean by equation (8). To understand the training process of the proposed quant-noise method, equation (8) seems to be important. However, there is no explanation for how it was derived.\n4) Some scalar values (e.g. $W_{kl}$ or $I_{kl}$) are denoted by using matrix notations.\n\n- There is no enough analysis on why quant noise can improve the quantization accuracy. The training process can be affected by various hyperparameters such as learning rates, dropout prob., weight decay, and so on. The authors are encouraged to present how regularization-related hyper-parameters are affected and to clarify the effects of the quant-noise method on the model accuracy. The current manuscript lacks such an analysis. In addition, there are no graphs showing the training process with the proposed quant-noise method. The training graph with various learning rates or regularization parameters could have elaborated on the contributions of this paper.\n \n- The comparisons in Table 1 are not fair and detailed analysis and discussion of the experimental results are required. The authors show poor results trained with QAT and claim that the proposed method can improve the accuracy significantly. But, this reviewer cannot understand why the quantization-aware training method is not better than the post-processing method (i.e., intN quantization). Also, it should be clarified whether int4 quantization and int8 quantization are post-processing quantization or not.\nWhen training with QAT, it will be affected by many hyper-parameters and there may be a decrease in performance (PPL and Top-1). It should be discussed that the QAT results are well-trained results.  \n \n- Moreover, it is difficult to compare the effects of quant-noise because there have been no previous works that have performed quantization in the models described in the manuscript. The authors select three models to show the effects of the method those are Transformer LM, EfficientNet-B3 (why not B1/B2?), and Roberta. But, the rather well-known models (e.g., ResNet, MobileNet, BERT, and so on) could be better than the selected models in the manuscript. For example, a highly related paper, iPQ paper (Stock et al., 2019), used the ResNet-18 and 50 models. This reviewer is wondering why the authors do not choose the models for easy and fair comparisons. \n\n \n\nIn summary, this reviewer cannot estimate the advantages of the proposed technique because the training process is not described in detail and the experimental results are not fair.\n\n \n <Minor comments>\n\n- In Table 3, I think the specific model name should come instead of \"adaptive input\".\n\n- typos\n 3page:\n     (weights) simulatneously -> simultaneously\n     (Bayesian) intepretation -> interpretation\n 4page:    (and) learns -> learn\n 8page:     (These) comparison -> comparisons\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}