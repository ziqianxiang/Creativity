{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a novel recurrent network called RIMs for improving generalization and robustness to localized changes. The network consists of largely independent recurrent modules that are sparsely activated and interact through soft attention. The experiments on a range of diverse tasks show that RIMs generalizes better than LSTMs.\n\nThe overall feedback from reviewers is positive: the paper is well written, the idea is interesting, and the experiments cover a wide range of diverse tasks.\n\nThe main concerns of most reviewers are the fairness of comparison, the limited novelty, and lacking details on how and why the system works. The authors pointed out that RIMs are not a straightforward combination of attention and RNN, and it has fewer parameters than LSTMs. They also conducted ablation study to demonstrate the benefits of RIMs and provided the missing details in the revised version.\n\nIn summary, this paper presents an important research direction for systematic generalization using modularized network. The paper is well written, the idea is novel and interesting, and the experiments cover a wide range of diverse tasks. Hence, it makes a worthwhile contribution to ICLR and I am recommending acceptance of this paper.\n"
    },
    "Reviews": [
        {
            "title": "An excellent paper in nearly all respects",
            "review": "The authors propose to learn what they term recurrent independent mechanisms (RIMs), a new recurrent architecture with components with nearly independent transition dynamics.  RIMs exhibit excellent generalization on tasks in which the factors of variation differ systematically between the training and evaluation distributions.\n\nThe paper is very well organized and well written, with relatively simple and consistently clear explanations of key concepts. The authors are precise in this language use, noting for example that they use the term \"mechanism\" in two slightly different ways (footnote 1).  \n\nThe authors state that their central question is\" how a gradient-based deep learning approach can discover a representation of thigh-level variable which favour forming independent but sparsely interacting recurrent mechanisms in order to benefit from the modularity and independent mechanisms assumption.\"  The paper could be improved by more clearly explaining the unique benefits of a gradient-based approach to this task.\n\nThe experiments in section four are particularly thoughtful and well-designed.  Rather than merely comparing performance on some  set of benchmarks, the authors aim to elucidate key capabilities with specific experiments. ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting new method, needs some clarifications",
            "review": "[Update after author's responses]\n\nI appreciate the responses provided by the authors. I think they answer my questions. In consequence, I update my review to favour accepting the paper.\n\n---\nThis paper introduces a new architecture composed of semi-independent recurrent networks that interact with each other, and with their environment, through attention. Furthermore, an attention mechanism is also used to select which networks are allowed to update their internal state at any given time. The authors present several experiment to demonstrate that this architecture outperforms simple LSTMs on various tasks.\n\nThe method seems interesting, and the experiments indicate some benefit. However, in its current state, the paper makes it very difficult to properly evaluate the method, due to a lack of explanations.\n\n- It is often unclear how many parameters/neurons the RIMs use.  As a result, we can't know whether the comparison to the LSTMs is fair. Please indicate *total* number of neurons and trainable parameters for both LSTMs and RIMs in all experiments.\n\n- Section D.4.1 and Figure 7 seem to contain important information about how the system works, but it is totally incomprehensible -e.g. how are the masks generated and moved around? Please rewrite this with an explanation of what exactly is going on.\n\n- I would appreciate some explanation about how exactly the system differentiates through the choice of which RNNs to activate or leave dormant. This seems to represent a hard all-or-nothing change, through which no smooth interpolation is possible, affecting all future time steps. How can we find a gradient over this step choice when backpropagating through time?\n\n- Related: how is the training actually performed? Do you backprop error through time over the whole history? I didn't see any description of the training process in the paper - though I may have missed it. There should certainly be one!\n\nIf these explanations are provided (and if the comparisons with LSTMs are fair) I think the paper would be acceptable. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper is okay, but I have concerns.",
            "review": "After rebuttal:\nI appreciate authors' detailed responses and an updated version of the paper. They mostly clear my concerns and doubt. I increase my rating to accept. \n--------------------------------------\nSummary:\nThis paper introduces a module that ensembles independent RNNs using a multi-head attention mechanism. This proposed recurrent independent mechanism (RIM) includes multi-head attention, top-k activation section, input attention, and communication modules. \nThe experiments on a range of diverse tasks show that RIMs generalizes better in many tasks than LSTMs. \n--------------------------------------\nPros:\n+ The paper is clearly written.\n+ The related works and the difference with the proposed model are explained in details. \n+ The experiments cover a wide range of scenarios from copying task to reinforcement learning. The additional experiments in Appendix are helpful. \n--------------------------------------\nConcerns:\n1. *Novelty:*\nIn my understanding, the core idea is essentially combining mutli-head top k attentions with RNNs. I appreciate that authors includes necessity of the proposed module and their insights. However, this paper simply combines existing works and thus lacks novelty. I ask authors to clarify it if I missed anything.\n\n2. *Model capacity:*\nAuthors claim that high performance with RIMs is not due to the increase of model capacity, and the model size is significantly reduced with RIMs when the comparing model has the same number hidden units. \nRelated to this, I have suggestions:\n    1. The model size of the proposed and comparing models should be reported. \n    2. Additionally, adding latency and FLOPs would be helpful. \n\n3. *Sparsity:*\nAuthors mention that sparsity is necessary in this model. How is the proposed model comparable with other sparse networks? Increase sparsity by adding an existing technique [1-4] in the standard LSTM can be another baseline for this model. Some previous works are listed here: \n    1. K-winner-take-all [1], local winner-take-all [2]\n    2. Dropout [3,4]\n\n4. *Missing references* regarding sparsity: [1-4]\n--------------------------------------\nMinor comments:\n- References of the model are missing in Table 1. \n- Page 8: 'allow the RIMs **ot** communicate with' -> 'allow the RIMs **to** communicate with'\n--------------------------------------\n[1] Majani, et al., On the K-Winners-Take-All Network, NeurIPS 1988.\n[2] Srivastava, et al., Compete to Compute, NeurIPS 2013.\n[3] Srivastava, et al., Dropout: A Simple Way to Prevent Neural Networks from Overfitting, JMLR 2014.\n[4] Molchanov, et al., Variational Dropout Sparsifies Deep Neural Networks, ICML 2017",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not clear whether performance gain is actually due to RIMs capturing independent mechanisms",
            "review": "The authors argue that the world consists of largely independent causal mechanisms that sparsely interact. The authors propose a new kind of recurrent network (RIM) that presumably distills this world view into inductive biases. RIMs consist of largely independent recurrent modules that are sparsely activated and interact through soft attention. They tested RIMs on a number of supervised and reinforcement learning tasks, and showed that RIMs perform better than LSTMs and several other more recently proposed networks (e.g., Differential Neural Computers, Relational Memory Core, etc.) in these tasks. In particular, RIMs can generalize more naturally than other networks to out-of-distribution test set in presumably modular tasks.\n\nThe motivation for RIMs makes intuitive sense, even though it is perhaps debatable whether the largely independent causal mechanisms in the world should each be captured by a single RNN. The manuscript is easy to follow, the idea is quite interesting, and the model is empirically tested across a wide diversity of tasks.\n\n(1)\tMy main concern is that it is not clear to what extent the improved performance is due to the core concept of recurrent independent mechanisms, or due to other factors such as the use of attention and more hyperparameter tuning.\n\nI don’t believe the merit of this work should be judged exclusively in its improved performance over other architectures. However, since the authors focused most main figures on performance, it is worth better understanding the cause of that performance gain.\n\nIn some experiments, RIMs are only compared against LSTMs, and it is not clear whether the gain over LSTMs is due to the use of attention. In the authors’ submission last year, a similar question was raised by R3, and the authors correctly pointed out that in several other experiments, RIMs fared better than attention-based models such as Transformer and RMC. However, in these experiments--as far as I can tell—RIMs benefit substantially from more extensive hyperparameter tuning. From table 1 and 4, we see that the advantage of RIMs over competing architectures, especially attention-based ones, are similar in magnitude to the performance difference caused by reasonable hyperparameter variations.\n\nTo alleviate this concern, the authors could potentially show results on a few datasets studied in the papers of RMC, DNC, etc.\n\n(2)\tMy other related main concern is that the authors proposed RIMs as a response to a world full of largely independent objects (or variables), yet never showed that RIMs would break when the world substantially deviates from this ideal.\n\nTo convince readers that the sparsely activated nature of RIMs is suitable for our world, the authors could design a world with densely interacting objects, for example balls attached with springs, and shows the failure of RIMs, despite the same amount of hyperparameter tuning, in comparison to other architectures. We will have a much better understanding of RIMs when we know how to break it.\n\n(3)\tThe authors motivated RIMs as more natural for capturing the largely-independent components of the real world. However, it’s not clear from the authors’ results (Fig. 7, Figs. 22-26) that each recurrent module actually learns the dynamics of individual objects.\n\nFor example, Fig. 7 is titled “Different RIMs attending to different balls”, but as far as I can tell, this conclusion is not actually shown in the figure.\n\n---------------------\nEdit after author's responses\nMy first concern is addressed by the authors' response. My other two concerns were not really addressed, but I think these concerns should not preclude this manuscript from getting accepted. So I'm updating my score.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}