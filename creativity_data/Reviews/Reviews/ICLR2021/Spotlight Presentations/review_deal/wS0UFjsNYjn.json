{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper addresses a method for unsupervised meta-learning where a VAE with Gaussian mixture prior is used and set-level inference, taking episode-specific dataset as input, is performed to calculate its posterior. In the meta-testing phase, semi-supervised learning with the learned VAE is used to fast adapt to few-show learning. Reviewers are satisfied with the author responses, agreeing that the method is a principled way to tackle unsupervised meta-learning. \n"
    },
    "Reviews": [
        {
            "title": "Interesting method for unsupervised meta-learning",
            "review": "This paper proposes a method for unsupervised meta-learning based on using a variational autoencoder (VAE). The variational autoencoder model they use differs from the typical one in that it considers episode-specific datasets, where the approximate posterior can be computed as a function of the set (using transformer architecture) rather than using an individual example. Additionally, they use a mixture of Gaussian distribution as a prior, whose parameters are learned per-episode using the EM algorithm. For the supervised evaluation phase, in order to adapt the learned prior to the few-shot dataset setting, semi-supervised EM is run using both support and query sets to adapt the mixture of Gaussian distribution to the evaluation dataset. Then, the query set predictions are obtained using the learned prior and posterior from the VAE model. Experimental evaluation is conducted on the Omniglot and Mini-ImageNet benchmarks and the proposed method is compared against other unsupervised meta-learning methods, mainly CACTUs and UMTRA. An interesting aspect about the Mini-ImageNet experiments are that because learning the VAE directly for this high-dimensional data may be difficult, the authors use features from a SimCLR-trained model as input for their VAE model. The proposed method seems to perform favorably across both of the benchmarks when varying the number of \"shots\". \n\nPros\n* Whereas previous work in unsupervised meta-learning involved creating unsupervised episodes for meta-training (via augmentations or clustering of unsupervised model features), this paper takes a very different route but still seems to achieve very good performance.\n* The authors were able to scale their model to the Mini-ImageNet dataset by using SimCLR-trained features and with this choice, the final model attains good performance on the benchmark compared to previous work.\n\nCons\n* This is a not necessarily a big con but a point that could be clarified further. How are the number of components for the GMM decided for meta-training? How does the choice of the number of components impact how the GMM is used at evaluation-time? Would it not pose an issue that during training we may have more/less components than are actually necessary at evaluation-time depending on the number of classes we are considering at evaluation-time? Is it the case that a separate model needs to be trained if number of evaluation classes is changed i.e. from 1-shot, 5-class to 1-shot, 10-class?\n* I believe the paper could be improved by adding an algorithm description of how exactly the model is trained and how evaluation takes place in terms of exact steps The algorithmic pseudocode can reference equations within the paper but I believe this would greatly help in terms of understanding how to recreate the exact training and evaluation procedure for the proposed model.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The semi-supervised meta-learning setting considered in the paper is interesting, and the use of an embedded mixture model to construct pseudo labels in this setting is a nice development. However, the submission lacks clarity on and motivation for the components of the algorithm, and there are some concerns with the empirical evaluation and the absence of references to some very relevant work.",
            "review": "The submission proposes an algorithm for the semi-supervised meta-learning (unsupervised meta-training + supervised meta-testing) setting of [1], which adapts the few-shot learning + evaluation setting of [2, 3] by omitting classification labels at meta-training time. The algorithm makes use of a variational auto-encoder (VAE) formulation defined over a hierarchical model that describes the decomposition of a dataset into tasks of datapoint-target pairs (i.e., the meta-learning setup). The prior distribution of the hierarchical VAE is taken to be a mixture of Gaussians to facilitate the construction of pseudo-labels at meta-training time. The algorithm is evaluated on the Omniglot and miniImageNet few-shot classification tasks (with labels unused at meta-training time).\n\n##### Strengths: \n\nThe semi-supervised meta-learning (unsupervised meta-training + supervised meta-testing) setting is interesting and worthy of study as an analogue of unsupervised learning. The datasets used in the empirical evaluation are appropriate, although they do not represent the most complex image datasets used in few-shot classification evaluations (cf. meta-dataset [8]).\n\nThe use of a Gaussian mixture model (GMM) for the prior distribution of a variational auto-encoder (VAE), which allows an analytic solution for a subset of the variational parameters, is conceptually interesting, although its use would not be restricted to the meta-learning setting. Using it to construct pseudo-labels (as well as incorporate labels when available) for the semi-supervised meta-learning setting is a nice development, although not a significant advance from the use of $k$-means in CACTUs.\n\nMeta-GMVAE attaints higher performance than semi-supervised few-shot classification setting comparison methods (CACTUs & UMTRA) on the Omniglot and miniImageNet benchmarks; moreover, it approaches the level of a supervised few-shot classification method (MAML) on the Omniglot benchmark (although this supervised comparator does not represent state-of-the-art performance on this benchmark).\n\n##### Weaknesses: \n\n1) **Clarity**: The algorithmic components of the submission were very difficult to get straight. In the development of the algorithm in Section 3.2, the submission does not adequately discuss why and how particular subcomponents are employed, and various points about the different algorithmic components are made in the text without sufficient explanation or integration; some examples are:\n\n    - The VAE formulation is introduced without precedent just above equation (1). It is also a bit of a red herring because it is not subsequently used, as is, in the algorithm.\n\n    - At \"The difference of our model from original VAE is that we utilize a set-level variational posterior $q_\\phi(\\mathbf{z}_j |\\mathbf{x}_j , D_i)$, for inferring isotropic Gaussian distribution, to encode characteristics of a given dataset $D_i$. Specifically, we utilize self-attention mechanism (Vaswani et al., 2017) on top of a convolutional neural network.\" This is the first time an \"isotropic Gaussian distribution\" is mentioned in the method, self-attention is not explained further, and there is no explanation of how the convolutional neural network (CNN) fits into the whole framework. For example, it is not clear from this section whether (and if so, how) a CNN is used in addition to the SimCLR feature representation.\n\n    - \"...we set the prior distribution as a mixture of Gaussians (GMM), where $y$ is a discrete random variable indicating the component of a latent variable $\\mathbf{z}$\". $y$ and $\\mathbf{z}$ are not yet defined except by reference to the VAE formulation in (1), but that was insufficiently explained as a part of the algorithm.\n\n    More specific details for reproducibility are not described in the text (e.g., how the GMM parameters are initialized for EM; what some of the variables ($\\mathbf{z}$, $\\mathbf{x}$, $\\phi$, $\\theta$) refer to in the implementation). On top of this, results would be extremely difficult to reproduce: While component architectures and experimental setups are detailed in the appendix, how everything fits together is not adequately described. \n\n    More broadly, the submission would benefit significantly from an algorithm box to convey how all the components interact and which components act episodically (at the task level) vs. at the level of the entire dataset.\n\n2) **Quality**: The experimental evaluation does not provide a measure of variance (e.g., 95% confidence interval) in Table 1, which should be provided to ascertain the significance of the reported improvement.\n\n  The algorithm uses the SimCLR representation learning objective to pre-train the feature extractor, while the comparison semi-supervised meta-learning approach use less performative methods as feature extractors (CACTUs: BiGAN, ACAI/DC; UMTRA: a simple, 4-layer CNN). An ablation study that ablates the use of SimCLR with Meta-GMVAE is necessary to ascertain whether the improvement is due to using SimCLR vs. using components attributable to Meta-GMVAE.\n\n3) **Originality**: Highly relevant work on GMM priors for VAEs is not cited in the submission: [4, 5]. The submission also does not discuss variations on the VAE that address the meta-learning setting (e.g., [6, 7]), which also demonstrate how the VAE formulation in (1) derives from a hierarchical model (cf. the non-hierarchical model on which the original VAE formulation is based).\n\n##### Minor points:\n\nThere are errors in reproducing the results from [1] in Table 2 of the submission (some percentages are incorrect); these errors do not affect the ranking of comparisons.\n\n##### References:\n\n[1] [Hsu, Kyle, Sergey Levine, and Chelsea Finn. \"Unsupervised learning via meta-learning.\" In ICLR, 2019.](https://arxiv.org/abs/1810.02334)\n\n[2] [Vinyals, Oriol, Charles Blundell, Timothy Lillicrap, and Daan Wierstra. \"Matching networks for one-shot learning.\" In Advances in neural information processing systems, pp. 3630-3638. 2016.](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning)\n\n[3] [Ravi, Sachin, and Hugo Larochelle. \"Optimization as a model for few-shot learning.\" In ICLR, 2017.](https://openreview.net/pdf?id=rJY0-Kcll)\n\n[4] [Dilokthanakul, Nat, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. \"Deep unsupervised clustering with gaussian mixture variational autoencoders.\" arXiv preprint arXiv:1611.02648 (2016).](https://arxiv.org/abs/1611.02648)\n\n[5] [Jiang, Zhuxi, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. \"Variational deep embedding: An unsupervised and generative approach to clustering.\" In IJCAI, 2017.](https://arxiv.org/abs/1611.05148)\n\n[6] [Hewitt, Luke B., Maxwell I. Nye, Andreea Gane, Tommi Jaakkola, and Joshua B. Tenenbaum. \"The variational homoencoder: Learning to learn high capacity generative models from few examples.\" In UAI, 2018.](https://arxiv.org/abs/1807.08919)\n\n[7] [Garnelo, Marta, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Eslami, and Yee Whye Teh. \"Neural processes.\" In ICML, 2018.](https://arxiv.org/abs/1807.01622)\n\n[8] [Triantafillou, Eleni, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin et al. \"Meta-dataset: A dataset of datasets for learning to learn from few examples.\" In ICML, 2020.](https://arxiv.org/abs/1903.03096)",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, I hope for follow-ups without VAEs",
            "review": "The problem which the authors attempt to solve is unsupervised meta-learning (UML), ie. learning in an unsupervised way such a model of a dataset, as to be able to perform meta-learning (here: few-shot classification) later. I see their contribution as two-fold:\n1. Proposing a framework for solving UML consisting of sampling subsets $D_i$ of a full dataset $D_u$, training a generative model based on both datapoints ($x_j$) themselves and the particular subset $D_i$ and using it in a semi-supervised fashion.\n2. Implementing a model in this framework based on a VAE. Here, the latent variable $z$ doesn't just compress information about a datapoint (as in a classical VAE), but is also able to encode (in an abstract way) the position of this datapoint in the subset $D_i$ (ie. \"task-specific label\"). To be able to capture this (arguably richer than in classical VAEs) distribution, authors use a GMM to model the variational distribution.\n\nBecause MLE of GMM is intractable, authors have a two stage optimization process:\na) Finding a task-specific (ie. encoding info about $D_i$ \"classes\") parameter $\\phi^*$ via EM and b) Optimizing the ELBO given $\\phi^*$ as usual. During meta-testing, the $\\phi^*$ parameter is estimated in a similar way using the test-time samples $x_i$ (trying to embed the new task into the learned manifold) and then latent variable $z$ is sampled conditionally based on $x_i$ and the expected value of the constructed distribution $p_{\\phi^*}(y|z)$ estimated via Monte Carlo.\n\n1. While I am neither a VAE expert nor enthusiast, I consider the proposed model principled: while the two-stage optimization mechanism is not ideal (as may make it harder to optimize compared to end-to-end differentiable models), learning a single distribution describing both elements we care about: images and their placement within a dataset seem to match the problem better than previous pseudo-labels-based methods.\n2. I particularly like introduction of the general framework (1.) (which is not emphasized in the paper). I believe that it should be possible (not necessarily straight-forwardly) to extend the proposed model to other generative models. To make it clear, I wouldn't expect this extension from the paper under review (what I'm proposing is basically yet another paper), but the opening of this direction of research is a big plus.\n3. Paper is easy to understand.\n4. The presented results, while competitive compared to the previous UML SOTA, are only presented on somewhat toyish problems (Omniglot, mini-imagenet). While it is understandable that it'll be hard to train a Meta-GMVAE on more complex datasets (as it's only harder than classic VAEs, which are already struggling with higher-dimensional tasks), presenting the results only on small datasets (even if this is the current SOTA and other methods do it) somewhat undermines the overall motivation to UML: to be able to use vast amounts of unstructured data while building ML models.\n5. I am not able to comment on the novelty of the work: I am barely aware of the contemporary VAE/UML literature. I will be willing to modify my score based on other reviewers' opinions in that regard.\n\nQuestion/proposal:\nIn Sec. 3.2. authors write \"assuming that the modalities in prior distribution represent class-concepts of any datasets\". Why would this be the case? This seems intuitive; I feel like there could be a nice theoretical argument why it would be the case.\n\nI find the model principled and new. It solves an important problem in a natural way, improving over SOTA and opening the potential for follow-up research. I weakly question the use of VAEs, which feels like it is limiting the method (making hi-dim UML impossible), but am aware that is more of a complaint against a well-established research domain than the contribution of this paper itself.\n\nTypos:\n1. Abstract:\n... from unlabeled data which can capture ...\n... shares the spirit of unsupervised learning in that they both seek ...\n2. Sec. 1:\n... effectiveness of our framework, we run experiments on ...\n3. Sec. 2:\n... One of the main limitations of ...\n4. Sec. 3.2.:\n... inferring isotropic Gaussian distribution, to encode ...\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple and effective extension of Mixture of Gaussians to a VAE model to perform meta-learning.  Results are good, but novelty seems lacking.",
            "review": "The paper goal is to learn unsupervised feature representations that can be transferred between few-shot classification tasks.  The paper models the class-concepts with a Mixture of Gaussians prior, and uses Variational Autoencoders to model the latent representations between the tasks and the samples.  \n\nThe presentation is clear and straightforward.  The idea is to use a GMM and use an Expectation-Maximization (EM) approach to learn the mixture.  To tackle the intractability of the variational posterior $q_\\phi (z_j | x_j, \\mathcal{D}_i)$, the paper proposes to use a Monte Carlo approximation.  For the meta-test, the model is tuned using EM in a semi-supervised fashion.\n\nThe experiments show the superiority of the Meta-GMVAE and the compared methods on the Omniglot and Mini-ImageNet datasets.  \n\nNevertheless, I find the idea simple, yet compelling.  The idea of adding GMM to enhance the modeling capabilities is a well known fact, and that has been explored before.  For instance, some recent publications applying the GMM idea (not that the final application and overall implementation may differ from meta-learning---see my comment below):\n- Dilokthanakul et al., Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders, https://arxiv.org/abs/1611.02648\n- Zhao et al., Truncated Gaussian-Mixture Variational AutoEncoder, https://arxiv.org/abs/1902.03717\n- Guo et al., Variational Autoencoder With Optimizing Gaussian Mixture Model Priors, 10.1109/ACCESS.2020.2977671.\n- Yang et al., Deep Clustering by Gaussian Mixture Variational Autoencoders with Graph Embedding, 10.1109/ICCV.2019.00654\n\nIt seems from the presentation that the main difference is the application to the meta-learning approach.  The authors should explain better what the contribution is and how it contrast to the existing literature of mixture models applied in variational modeling.\n\nPros:\n- Simple and effective idea.\n- Use of well known methods with simple approximators.\n- Good results on the presented experiments.\n\nCons:\n- The contribution is not clear.  I'm on the fence of whether the usage of the GMM to a new task is enough to guarantee a publication.\n\nOverall rating: ~~I'm giving a 5 due to the lack of clarity in the contribution and added novelty.  However, the presentation is good, and the explanations are clear.~~\nI'm updating my rating to accept the paper due to the comments and updates on the paper.  The proposed flexible usage of the GMM is novel from the existing literature.  The changes in the paper improved its clarity, and the contribution is better presented in contrast to existing work.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}