{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a broad framework for unifying various pruning approaches and performs detailed analyses to make recommendations about the settings in which various approaches may be most useful. Reviewers were generally excited by the framework and analyses, but had some concerns regarding scale and the paper's focus on structured pruning. The authors included new experiments however, which mostly addressed reviewer concerns. Overall, I think is a strong paper which will likely be provide needed grounding for pruning frameworks and recommend acceptance. "
    },
    "Reviews": [
        {
            "title": "novelty & experiment at scale",
            "review": "This paper proposes a detailed analysis on pruning heuristics, and its applications to early pruning. It thoroughly analyzed magnitude-based pruning, loss-preservation based pruning, and gradient-norm based pruning. The paper demonstrated the results on CIFAR-10 and CIFAR-100 datasets. it's very timely research to guide the audience which heuristic is better. My major concern is the novelty over existing pruning heuristics, since the techniques have all been proposed before. The other concern is the evaluation and the scale of the dataset. Given the results in table 2 different by less than a percent, and Cifar training is very noisy, it's hard to tell the difference. Just like the Lottery Ticket hypothesis works on Cifar but does not work on ImageNet, different pruning heuristics needs to be verified on the large scale ImageNet dataset in order to be convincing. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "## Summary\nThis paper studies different families of pruning criteria and their impact on training dynamics (especially early training). Stemming from the observations, authors provide improvements to the 1st and 2nd order saliency methods. \n\n## Pros\n- Authors provide simple and useful explanations to various pruning criteria that are based on the Taylor approximation of the loss functions.\n- Even the authors don't mention this in the contributions, they propose some improved versions of existing criteria. For example the updated taylor score with $\\theta^2g(\\theta)$ or absolute valued GrasP. This is great and it might worth focusing on these criteria further providing further evidence on their usefulness. Currently, they seem a bit arbitrary. For example, why not third power $\\theta^3g(\\theta)$ or additive biasing of magnitude $(g(\\theta)+c)*\\theta$. I recommend authors to run their versions in unstructured setting too.\n\n## Cons \n- Authors choose to focus on structured pruning since resulting networks are dense and acceleration is straight-forward. However,  they miss an important work on structured pruning [1]. This relatively well-known work shows that pruned (structured) networks can be trained to full accuracy from scratch. In other words, their value lies on doing some kind of an architecture search over layer widths. The motivation of the work needs to be revisited in the light of these results. Since we can retrain pruned networks from scratch, it probably doesn't matter which neuron we choose and therefore which criteria is better. Unstructured pruning doesn't have this training from scratch issue, and I recommend authors to at least include and maybe shift the focus to unstructured pruning. \n- \"but requires specially designed hardware (Han et al. (2016a)) or software (Elsen et al. (2020)). While results in this paper are applicable in both settings, our experimental evaluation focuses on structured pruning due to its higher relevance to practitioners.\" All neural networks require special hardware if you want to accelerate them. I think a better motivation here is to point out to the difficulties at accelerating sparse operations and limited availability/support for such operations in existing frameworks. And I am not sure how useful structured pruning algorithms are given the results of [1].\n- \"The larger the magnitude of parameters at a particular instant, the smaller the model loss at that instant will be.\" This is likely to be true in simple settings, however it is not a sufficient condition; especially for networks with batch norm. You can arbitrarily scale neurons if there is a batch-norm and you can come-up with arbitrary ordering if needed. I recommend re-phrasing this observation and/or stating the assumptions better (I don't remember seeing any assumption on the network itself). How the regularization or gradient noise will effect this statement? \n- \"Thus, the parameter with the most negative value for Θ(t)g(Θ(t)) is likely to also have a large, negative value for Θ(t)H(Θ(t))g(Θ(t))\" This is not clear to me. Assume 1d case where Θ(t)= -1; g(Θ(t))=2; H(Θ(t))=-1 -> Θ(t)g(Θ(t))=-2; Θ(t)H(Θ(t))g(Θ(t))=2. I can see the correlation in the figure but it doesn't seem like an obvious thing. Maybe because hessian don't have many negative eigenvalues?\n\n## Rating\nI found the results and analysis interesting, however motivation needs to be updated. The work would also benefit from including unstructured pruning experiments. \n\n## Minor Points\n- \"Recent works focus on pruning models at initialization (Frankle & Carbin (2019);...\" Lottery Ticket paper prunes after training and show existence of some initializations that achieve good performance.. \n-  Equations 6/7 $\\frac{dL}{dt}= ||g(\\theta)||^2$ assuming gradient descent shouldn't be a learning rate?\n- \"...than magnitude-agnostic techniques.\" Which methods are these? As far as I see, all methods use magnitude information in their formulas directly or indirectly.\n- In Table:1, I recommend authors to bold both scores if they lie within the std of each other; so that we can identify which improvements are significant.\n- It would be nice to show how the temperature parameter is used for GrasP\n\n[1] https://arxiv.org/abs/1810.05270\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Excellent, clear paper with compelling insights and empirical results",
            "review": "Summary:\n\nThe authors study proposed importance metrics for pruning neurons/channels in deep neural networks and analyze what properties of parameters are favored by each approach by studying the relationship between model parameters, gradients, 2nd order derivatives and loss. Through this analysis they develop a rich understanding of the consequences of different pruning criteria and use their understanding to propose modifications to existing techniques that produce higher quality models across different settings.\n\nPros:\n\nThe framework used by the authors is clear and easy to understand but also very general. The authors’ mix of empirical results and theoretical analysis makes a convincing case for the accuracy of their observations. The authors go beyond observation and analysis and use their insights to design new approaches to pruning that outperform existing techniques. The paper is well written and well organized.\n\nCons:\n\nThis paper has few limitations. The main limitation is that all experiments were conducted on relatively small datasets (CIFAR). Given that is has been shown that some techniques in model compression produce state-of-the-art results on small tasks but fail on larger models and datasets [1, 2], I’d encourage their authors to further validate their insights on a larger dataset (i.e., ImageNet).\n\nComments:\n\nI found that the authors waited a long time to explain the term “gradient flow”, which was important in sections 1-3 but not fully detailed until the start of section 4. On page 1 the authors say in parenthesis that gradient flow is “gradient descent with infinitesimal learning rate”, but I found this explanation was not clear. The second sentence of section 4 “the evolution over time of model parameters, gradient, and loss” was much more clear. I’d encourage the authors to potentially work some of these details earlier into the text.\n\nReferences:\n1. https://arxiv.org/abs/1902.09574\n2. https://arxiv.org/abs/2003.03033\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Use of gradient flow to derive interesting relationship and interpretations for pruning early",
            "review": "The paper contributes to explaining why saliency measures used for pruning trained models may (or may not) also be effective for pruning untrained or minimally trained models, by developing the relationship between those saliency measures and different forms of the norm of model parameters based on the evolution of model parameters via gradient flow (basically derivatives w.r.t. time). This result leads to several interesting interpretations that could shed some light on on-going efforts to understand recent methods of pruning early-on (e.g., pruning at initialization or after minimal training) and potential extensions to existing saliency measures. The idea of employing gradient flow is novel for its purpose and seems to be accurately executed.\n\nThe main concern is that there is a gap between the flow model and the actual optimization method used in this work (SGD with momentum), or more generally standard optimization methods for deep learning. In this regard, the claim of “evolution dynamics” seems a bit exaggerated and remains as theoretical; experiments are strictly speaking not entirely valid to support it either. (minor) Related work is written as if pruning is only done via saliency-based methods (e.g., “pruning frameworks generally define importance measures”) without taking into account various others such as optimization based methods employing sparsity inducing penalty terms. On a different but related note, the writing becomes a bit loose when it comes to referencing existing methods; it is worth correcting them and clarifying the scope/focus of this work.\n\nFurther questions:\n- Why do you study structured pruning *only*? The provided reasons (“unstructured pruning requires specially designed hardwares or softwares” or “higher relevance to practitioners”) don’t seem valid enough if the purpose really lies in analyzing. Can you provide any results for unstructured pruning?\n- Can you provide evidence to support the claim “GraSP without large temperature chooses to prune earlier layers aggressively” (besides Raghu et al. 2017)?\n- Based on Tables 1 and 2 the proposed extension to loss-preservation method works the best, while the differences across different methods seem a bit marginal. Is my understanding correct?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}