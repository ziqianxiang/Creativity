{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The authors propose self predictive representations (predicting the agents own future latents of a forward model with data augmentation) as a means of improving the data efficiency of agents. The reviewers found the paper to be compelling (after the authors made adjustments) and pointed out that the method is likely generic and might be widely applicable. Experimental improvements in the work are significant, and the method is well explored."
    },
    "Reviews": [
        {
            "title": "a data-efficient RL method with strong empirical performance",
            "review": "This paper proposes a method to improve data-efficiency of DQN (specially, the Rainbow DQN) by training the reinforcement learning agent to jointly minimize the DQN loss and a loss for multi-step predictions on its own latent states.\n\nPros\n+ The proposed approach achieves signficant performance improvement as compared to existing algorithms in the low data regime on Atari games.\n+ The paper is well-written and generally easy to follow. Fig. 2 provides a nice illustration on the algorithm.\n\nCons\nThe approach provides an interesting combination of several heuristics, but it isn't always clear how these work and whether these are necessary components. \n- Both the target encoder network and the target projection head uses the EMA trick, and an additional prediction head q is used. There is some discussion on the EMA trick in the experiment section for the target encoder, but there is little discussion on the the target projection head and the prediction head.\n- Why is the prediction loss chosen to be negative cosine similarity? Does using quadratic loss work?\n\nMinor comments\n- Alg. 1: a loop over experiences in the minibatch seems to be missing, and RL loss(s, a, r, s'; \\theta_{o}) should be a function of the mini-batch not just a single experience (s, a, r, s'). Is s' perturbed as well?\n- Specifying the network parameters in notations will improve clarity (though many papers don't). In this paper, sometimes the network parameters are included in a notation, but sometimes not (e.g. L^{RL} and $L^{RL}_{\\theta}).\n- Strictly speaking, cosine similarity = 1 - 0.5 * normalized l2 loss (not really proportional)\n- Is the algorithm's performance sensitive to the choice of K?\n- The paper states that \"using a separate target encoder is vital in all cases\", this is confusing as the paper states later that with data augmentation, this is not needed (\\tau=0 works best).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comprehensive experiments but can use more analysis",
            "review": "This paper proposes Self-Predictive Representations (SPR), a self-supervised representation learning algorithm designed to improve the data efficiency of deep reinforcement learning agents. The main idea is to maintain a target encoder network and a target projection network (the moving average of the corresponding online networks) which can be used as a similarity prediction loss for transitions in the latent space. This loss essentially replaces the reconstruction loss typically used in similar latent space models. \n\nThe paper is very well written and easy to read and understand. Figure 2 clearly demonstrates the proposed method and the text does a good job at filling in the details. The experiments are relatively comprehensive (using the 100K sample threshold on Atari games proposed by SimPLe) and the authors compared the performance of the proposed methods with majority of key works on rl from pixels. The appendix also provides more detailed results and the implementation details. The authors also included an anonymous link to a repo which includes the code which is always a plus. The implementation is based on Rainbow as mentioned in the paper.\n\nThe main idea of he paper is very intriguing: that the learned state representation can be improved by self forward prediction. The idea is generic enough to be used in similar problems to improve the sota models.  However, I wish the Analysis section was bigger than half a page as I have hard time understanding *where* this improvement is coming from. There are interesting  ablation studies and insights in Section 5 but the authors could provide more. For example, the role of input augmentation, prediction horizon (dynamic modeling in the text), the magic of target network as well as the importance of tau, could be disentangle and explored more. \n\nThe paper can also be improved further by including a wall-clock time analysis. The method is clearly outperforming Rainbow but it comes at the cost extra parameters and computation. The authors listed \"not relying on reconstructing raw states\" as a benefit of the proposed method but I'm curious how does the proposed method compares to such methods, both the ones that unroll in the pixel space (e.g. SimPLe) and the ones that only use it as an auxiliary loss during training (e.g. Dreamer). By comparison I mean in terms of performance and wall clock time. Regardless, Dreamer seems to be a an interesting base model to compare to. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "The paper introduces a method to improve sample-efficiency in reinforcement learning, called Self-Predictive Representations. The authors use predictions in the latent space by using a learned transition model. To further improve their method, they add data-augmentation to enforce consistent the representation to be consistent over multiple views of an observation. Their method is tested on the 100k steps Atari environment and compared to other state-of-the-art methods. The results show the proposed method to significantly outperform previously suggested methods.\n\nIn its current form, the paper is marginally below the threshold in my opinion.\n\nThe reasons for my decision are as follows:\n-\tThe main point of contention is the choice to use different numbers for random seeding. While I understand the motivation stated by the authors to make results comparable to the original baselines/state of the art, it results in several problems. The error bars in Figure 3 are not meaningful as they are based on different numbers of seeds for different methods. Furthermore, I am surprised that the authors did cite Henderson et al., 2018, but do not address the problems that arise from using only a small number of random seeds (as shown in Henderson et al., 2018, Figure 5).\n-\tThe paper is not consistent in notation. The authors introduce MDPs without states but observations, which is simply not the correct definition. They move on and replace observations with states in section 2.2, without every introducing or defining it (states). Lastly, Fig.2 uses X_t, which is also never introduced or defined.\n-\tIn 2.5, the authors communicate their choice for different parameters as based on ‘early experiments’. Without further clarification, this statement is not meaningful as it is highly ambiguous.\n\nDespite the aforementioned issues, the paper is mostly well-written and the presented idea is valid and important. As the main criticism is the choice of random seeds in the experiments, I would accept the paper if this is accordingly addressed. \n\n------------------------------------------------------------------\n\nThe authors sufficiently addressed all questions in their rebuttal and I will therefore increase my score.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting integration of RL and self-supervised representation learning that warrants further investigation.",
            "review": "The authors propose learning self-supervised representations that are consistent across future time steps.  To do this, the authors augment a modified Data Efficient Rainbow with the self-supervised architecture from BYOL; however, instead of applying it to two augmentations of the same view, they match a sequence of states to each other.  The online sequence is generated as follows: the current state at time t is passed through the online encoder to generate z_t, from which representations {z_t+1,...,z_t+k} are generated via an action-conditioned transition model.  These k states are then matched with the target encoding of the observations {o_t+1,...,o_t+k} via cosine similarity.  They experimented with applying augmentations, as well as dropout of the features of both online and target networks (without augmentation).  The reward is also predicted from the state z_t, to ensure that the representation learned is still useful for the task.  The authors evaluate SPR on sample-efficient Atari, which is limited to only 100k environment steps (400k frames).\n\nThe paper admirably integrates a powerful self-supervised representation learning technique with the framework of reinforcement learning, by enabling the contrasting of multiple consecutive states via the use of a multi-step dynamics model.  However, inherent to the process of adapting these techniques is the need to explore design decisions and assumptions made, and their effects.\n\nHere are a few questions to consider, that the reader would potentially be interested in:\n1) Why was the original state not also matched against a target encoding (with a different augmentation) - essentially 0-step BYOL?\n2) To improve the quality of the dynamically generated states as well as the transition model itself, would it not be better to learn from the multi-step reward signal as well?   This would ensure consistency between the predicted states and the encoded states.  The gradient can be stopped before it flows through the online encoder.\n3) In BYOL, the authors match one augmentation passed through the online network to another augmentation passed through the target network.  By symmetry, however, the second augmentation passed through the online network should also match the first augmentation passed through the target.  Thus, BYOL is computed as a sum of these two cosine similarity losses.  Would it not also make sense for SPR to apply this symmetry?  The representation from the past should match that of the future, but is it not desirable for future representations to match the past for consistency (within a reasonable frame length)?\n4) The authors of BYOL report sensitivity to the transformation set used and the batch size, despite not relying on negatives.  Do these considerations transfer to SPR?\n5) Representations learned from contrastive self-supervised techniques such as BYOL and SimCLR have been shown to be good for a variety of applications, such as linear evaluation, semi-supervised tasks, and transfer learning.  Do these benefits also apply when adapted to RL via SPR - for example, transferring across tasks?\n\nAlthough there are other benchmarks that could be used to evaluate SPR (such as the DeepMind Control Suite) and present a stronger case, the existing experimental results are already compelling.  Ultimately, the idea is presented clearly and the writing is solid.  However, a more careful analysis of the implications of the design decisions made and how the pros and cons of these self-supervised representation learning approaches transfer when integrated into RL would separate this paper from an incremental work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}