{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper received two clear accept, one accept, one borderline accept and one reject review. R4 identified that the paper falls short in discussing recent works from CVPR and ECCV 2020 on the image inpainting and completion tasks which also tackle challenging scenarios in these tasks. The authors improve their related work section with these more recent works while pointing out that the task still remains unsolved and they propose an effective technique towards the solution. The meta reviewer recommends acceptance based on the following observations. \n\nThe submission proposes a GAN architecture for image inpainting using co-modulation, which is similar to the weight modulation in StyleGAN2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable. The main novelty of co-modulation appears to be interesting as well as being generalisable to different tasks. The approach is shown to perform well in the image painting with large-scale missing pixels and some image-to-image translation tasks. Furthermore a new metric P-IDS/U-IDS is proposed to evaluate the perceptual fidelity of inpainted images. "
    },
    "Reviews": [
        {
            "title": "Good performance, novelty is a bit limited.",
            "review": "Update: Thanks for the response from the authors. The comments 3/4/5 from authors convincingly address my concerns. Regarding other classifier-based metrics, note that not all of them requires separate training and testing procedure. For example, the leave-one-out 1-NN accuracy [1] does not. Also, I'm still not sure about the technical novelty. Therefore, I keep my original rating.\n\n[1] Xu, Qiantong, et al. \"An empirical study on evaluation metrics of generative adversarial networks.\" arXiv preprint arXiv:1806.07755 (2018).\n\n---------\n\nThis paper proposes generator architecture for image inpainting using co-modulation, which is similar to the weight modulation in StyleGAN2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable.\n\n\nPros:\n+ The paper is well-written and easy to read.\n+ The proposed method significantly outperforms the baseline DeepFillv2. The results look qualitatively convincing.\n\nCons: \n- The novelty of this paper is a bit limited. It seems to me that the proposed co-modulation is a straightforward extension of conditional modulation with stochasticity. \n- It seems most of the improvement over DeepFillv2 is from the better and larger network backbone (StyleGAN2). The improvement from conditional modulation is a bit small (Table 5 in the appendix).\n- In Figure 5, the authors argue that KID is subject to huge variance. However, it is not clear to me if KID really has a larger variance than FID. Figure 6 shows that FID and KID have very similar (relative) variance. It is argued that KID can hardly distinguish w=1 vs w=2. But from Figure 4 right, it seems to me that FID has the same problem too.\n \nQuestions for the rebuttal:\n- What is the running time of the proposed classification-based metrics?\n- The proposed co-modulation follows the “late-fusion” strategy: processing the image and random information independently, then concatenate them before modulation. Is it helpful to fuse them earlier, using an MLP to process the concatenated information before modulation?\n- The paper argues that \"however, to the best of our knowledge, we are the ﬁrst to formulate the discriminability as ψ a simple scalable metric\" when discussing other classifier-based metrics. I'm not totally convinced why the proposed metric is more scalable than other ones, e.g., the two-sample test. It would be great if you could elaborate on that.\n\nOverall, I find the empirical results of the paper are pretty impressive. The technical novelty is however a bit lacking. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Large Scale Image Completion via Co-Modulated Generative Adversarial Networks ",
            "review": "In this paper, the authors propose a general approach for image completion with large-scale missing regions. The key is to combine image-conditional and modulated unconditional generative architectures via co-modulation. The presented approach has demonstrated strong performance in the image painting with large-scale missing pixels and some image-to-image translation tasks. A new metric P-IDS/U-IDS is proposed to evaluate the perceptual fidelity of inpainted images.\n\nStrength:\n- The idea of co-modulation is quite interesting and has demonstrated strong results in various tasks.\n- The paper is well-written and well-motivated.\n- The solution combines the best of two worlds in image-conditional and unconditional image generation.\n- A new metric P-IDS/U-IDS is proposed for perceptual evaluation, verified by the correlation to human preferences.\n\nWeakness:\n- Only one image inpainting DeepFillv2 is compared in the experiment. Other image inpainting methods such as gated convolution, partial convolution can be also evaluated.\n- The results in the main paper are mainly faces that have structured information. It would be good to move some outdoor results in the supplement to the main paper. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Literature research is questionable",
            "review": "The paper proposes multiple contributions.\n\nThe paper identifies the following problem: current inpainting methods are suitable for small missing regions, but do not do well for large missing regions. I think the exposition is outdated and does not consider new work published at CVPR 2020, ECCV 2020, and possibly other venues.\nThe paper \"Recurrent Feature Reasoning for Image Inpainting\" explicitely points out \"However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center.\" and the results seem to show many examples of inpaiting for large missing regions.\n\"Image2StyleGAN++ ...\" show the inpainting of large regions as application and specifically mention the large-scale inpainting problem.\n\"Image Fine-grained Inpainting\" states that \"Benefited from the property of this network, we can more easily recover large regions in an incomplete image\"\n\"DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting\" even mentions the problem of large missing regions in the title of the paper.\nThere are also other inpainting papers that one should look at. I didn't check the publication date or the relationship in detail at this point in time: \"Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature Equalizations\", \"Deep Generative Model for Image Inpainting with Local Binary Pattern Learning and Spatial Attention\", \"Deep Generative Model for Image Inpainting with Local Binary Pattern Learning and Spatial Attention\", \"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\".\nMain concern for this submission is the literature research and I would like to see this addressed in the rebuttal.\n\nThe paper proposes an architecture modification the authors call co-modulation. The idea is to have the normalization of the generator layers not only controlled by either a random vector, or an input image, but by both. I think this overall idea is clear, but the details (including Fig. 3) are not that clear. I would say it's a nice, but smaller idea, that is suitable for publication if it leads to good results.\n\nThe paper also proposes a new way to evaluate GANs using the proposed P-IDS and U-IDS score. The main idea here is to use a pre-trained feature transformation (the inception network) on real and fake images and then to evaluate the images using a linear classifier.\nI do not think the evaluation strategy of masking a random square of size wxw is that convincing. A square of 1x1 is a single pixel and this is not a meaningful image manipulation. Even changing a square of 8x8 pixels is not especially meaningful. The fact that your metric can distinguish between these manipulations is not an indication that the metric can distinguish between high quality and low quality results.\nThe user study is more meaningful and gives some indication that the new metric is better. This looks promising and I like this result.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple yet effective idea with great results",
            "review": "This paper proposes an image completion method that can deal with large-scale missing regions. The proposed method employs a co-modulation technique to bridge the gap between conditional and unconditional GANs. It can then take the advantages from both sides with the consistency offered by conditional GANs and stochasticity provided by the unconditional GANs. The paper also proposes new image quality metrics, Paired/Unpaired Inception Discriminative Scores (P-IDS/U-IDS), for measuring the image quality of the inpainted images. Experiments show that the proposed method significantly outperforms DeepFillv2, a state-of-the-art freeform image completion method, on examples with large missing regions. Overall, I like the idea and the paper’s results. It represents a clear progress to the image completion problem with large missing regions. \n\n**Strengths**\n+ The idea is novel and makes sense. It is simple yet effective to the target problem. The idea of extending modulation to co-modulation is interesting. The co-modulation architecture design allows the proposed method to explore the generation capability of the unconditional GANs while maintaining the consistency of the completed image.\n+ The results are excellent. The paper and the appendix's visual results show that the proposed method can inpaint images with large missing regions very well. \n+ The paper is generally well-written. Figure 2 illustrates the basic idea very well. \n\n**Weaknesses**\n- Although the experiments show a good correlation of the proposed P-IDS/U-IDS metrics to the user study results, it is not clear how well the metrics reflect perceptual fidelity. Do these metrics work best for measuring the quality of inpainted images? How were the fake images generated for fitting the linear SVM? Is it possible to generalize to other image restoration problems?   \n- Although the main target is image completion, the paper claims that the proposed co-modulated GANs also works well on image-to-image translation. However, the paper only demonstrates the application on a simple edge2image problem, from edge images to commodity images. It would be better if the paper presents more image-to-image translation examples for validating how well the proposed method can handle image-to-image translation.\n\n**Post-rebuttal**\n\nThe rebuttal addresses the raised issues. An experiment supports the generalizability of the proposed metric. A new image-to-image translation experiment on COCO-Stuff is added. It shows the proposed method's potential for other translation tasks (the results are in Fig. 16, not Fig. 15). Taking the rebuttal and other reviews into account, I would like to recommend accepting the paper for its excellent results and simple yet effective idea. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review round 0 of “ Large Scale Image Completion via Co-Modulated Generative Adversarial Networks”",
            "review": "In this article, authors proposes to bridge the gap between image-conditional and recent modulated unconditional generative architectures with a generic co-modulated gan architecture.\nThey also proposes a new inception score based on linear SVM in order to measure linear separability in a pre-trained feature space.\n\nThe paper is easy to read and the experimentation part supports the claims made in the theoritical sections.\n\nI have one question though: Why using a SVM in the computation of Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS) and not a simpler linear model ?\nHow the training of the SVM goes especially on large datasets ? (not in term of accuracy but in term of computation power)",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}