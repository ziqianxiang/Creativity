{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper provides a novel generalization bound for neural networks using knowledge distillation. In particular, they argue that\n\n\"test error <= training error + distillation error + distillation complexity\" where the distillation complexity is typically much smaller than the original complexity of the neural network. This is motivated by the empirical findings that neural networks can typically be significantly compressed in practice using KD without losing too much accuracy. \n\n\nI found this result novel and the direction is very promising. This is a clear accept for ICLR. "
    },
    "Reviews": [
        {
            "title": "An interesting theoretical study on the generalization bound, while comparison between existing bounds may make it more convincing",
            "review": "This paper provides an upper boundary of the generalization error of networks: the sum of its training error, the distillation error, and the complexity of the distilled network. Then, it also provides an instantiation of the lemma applicable to residual networks, an explicit compression analysis via pruning with a corresponding generalization bound, and empirical supports.\n\n**Pros**\n\n* The idea of applying distillation to tackle the generalization dilemma seems interesting, and the proof seems rigorous.\n* Experiments in section 2 on the width independence, depth, and random label are reasonable for me.\n\n**Cons**\n\n* A comparison (e.g. on the tightness of the bound, or the correlation with generalization) between the proposed distillation-based generalization bound and existing generalization bounds in literature may help demonstrating the effectiveness of the proposed bound.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "some comments",
            "review": "The generalization performance of learning algorithms characterizes their ability to generalize their empirical behavior on training examples to unseen test data, which provides an intuitive understanding of how different parameters affect the learning performance and some guides to design learning machines. Different from the traditional error analysis, this paper focuses on bounding the divergence bettween the test error and the training error by the the corresponding distillation error and distillation complexity, e.g., test error  is bounded by training error + distillation error + distillation complexity. The current learning theory analysis may be important to understand the theoretical foundations of distillation strategy in deep networks. However, some theoretical issues should be illustrated to improve its readability, e.g,. \n1)What is the relation between the original network complexity and the corresponding distillation error +distillation complexity? \n2)Is the derived upper bound (e.g., Theorem 1.2) tighter than the traditional one? Please present a table to compare it with  the related error bounds.\n3)What are necessary/sufficient theoretical conditions for the effectiveness of distillation strategy (from the generalization error bounds)?\n4)It may be better to state some discussions for the lower bound on the generalization error. Does it also relate with distillation complexity?  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good paper",
            "review": "The paper overall is of good quality. The story of the work is well-written which makes the contributions easier to digest. One suggestion would be to comment a bit more on the relevance of the margin distribution for readers that are unfamiliar with it, for instance, in Figure 1, the term margin distribution is thrown without explaining why one should look into it. \n\nThe topic of this work is of great significance given that understanding the generalization error of neural nets is relevant to the community.\n\nIn terms of theoretical contributions, the results seem sound although I did not check the proofs in much detail. Most of the proofs are based on known techniques to find upper bounds. Authors also show some experiments on the behavior of their bounds, and in particular, on margin distributions.\n\nOverall, I like the idea of bounding complex models by distilled models (simpler ones), and as authors point out, to make this approach work one needs to have a solid way to find such distilled models. If my understanding is correct, the \"weak\" part of the work is that authors do rely on sparsification to find distilled models. While valid, my intuition is that such distillation process could still lead to large bounds. Which seem to be the case of Figure 3(b)? \nTo give an example, consider a task where an overparametrized complex network generalizes well in practice, it seems unlikely to me that a (very) sparse or pruned network would \"perform similarly\". That is, the \"sparsified\" model to be \"close in performance\" to the complex model would not be simple enough to give a non-vacuous upper bound. I would appreciate if authors can comment on this concern.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting approach to generalization bounds for deep neural networks. ",
            "review": "The paper provides generalization bounds for seemingly complex neural networks\non the basis of much simpler ones. That is a good idea and something that is currently very relevant I think and the approach seems to be the natural one to take. \n\nEssentially the bounds proved, bound the out of sample error\nwith a form of in sample error, average difference in predictions between complex and simple network, and complexity term for the simple network in terms of Rademacher complexity. \n\nThe authors provide a general framework which can be applied and show a particular way of using it and use recent results (Bartlett et al) to provide interesting application of the framework.\n\n\nIn terms of the actual bound achieved there are a few things I feel should be discussed more.\n\nIn Lemma 1.1.\nThe in-sample error. First it is not the in-sample classification error but the sort of a margin error that essentially is never zero for any prediction. Usually margin errors have a\nlinear penalty on the wrong side of the margin and zero on the correct side of the margin.\nSecond, there is a factor 2 in front of it. Normally, and in uniform convergence bounds, there are no constants in front of the in-sample error. (The other constants are of no concern).\n\nThe authors state that their work can be applied to  generalization bound of Arora et a. 18 that only worked for a compressed network but not the original one. Given the above comments about the actual bound achievable it is not clear to me what exactly one would get out of \"distilling\" the construction in Arora et al, but it seems it does not become the same bound as for the compressed network as shown by Arora et al. Comments on that would be appreciated.\n\nAnother small question: What does the early distillation phase on page 4 means (below lemma 1.3)?\n\n\nI like the experimental setup, particularly using gradient descent to try and find a network to distill to.\n\nOverall, I think the paper is well written including the proof sketches that make me believe the statements are actually provable (I did not rigorously check)\n\nOverall, I think this is an interesting paper and should be accepted. \n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}