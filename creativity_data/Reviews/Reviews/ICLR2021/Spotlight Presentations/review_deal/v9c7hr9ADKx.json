{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "Reviewers all agree on acceptance for this paper. The initial issues with clarity seem to have been addressed by the authors.\n\nThe paper introduces a new transformer-based architecture for MARL that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multi-task training. The method also produces more interpretable agents.\nThe paper shows results on the Starcraft multi-agent challenge (not the full game of Starcraft, but still a recognised and widely used multi-agent benchmark). The method produces solid results both in terms of final training performance and zero-shot generalisation.\n\nAlthough reviewers are generally supportive of this paper, they mention that the Starcraft challenge used is somewhat simple (only few units used), and that the transformer-based architecture may not be applied to domain which lack the proper structure. \n\n"
    },
    "Reviews": [
        {
            "title": "A novel work, but lack of strong experiments",
            "review": "1. In this paper the authors proposed a transferrable framework for multi-agent RL, which enables the learned policies easily generalize to more challenging scenarios. This seems to be a good contribution to the community of multi-agent RL. It bears a potential to handle large-scale tasks with only limited training data, while also demonstrates more explanable policies.\n2. However, the experiments seem to be insufficient. The authors only investigate scenarios for 3 vs. 3, 5 vs. 7, which are still the easiest cases in the StarCraft II combat tasks. I suggest the authors to try more on 20 vs. 30 StarCraft combat task or more challenging scenarios, or the hundres or thousands levels of multi-agent tasks like that provided by MAgent environment[1]. And a comprehensive comparison with a similar work [2] following the curriculum learning pipeline is also worth a trial. This will make this work a strong one.\n3. A more profound analysis is needed for the experiment part. Besides the performance gains, insightful understanding of how the designed model works is also necessary.\n\n[1] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, Jun Wang. Mean Field Multi-Agent Reinforcement Learning. ICML 2018.\n[2] Kun Shao, Yuanheng Zhu, Dongbin Zhao. StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning. IEEE Transactions on Emerging Topics in Computational Intelligence, 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " This paper studies the interesting problem of universal multi-agent reinforcement learning for multiple tasks, utilizing a new transformer-based model approach. It has been demonstrated that the new approach proposed in this paper has significant advantages over the existing state-of-art methods in terms of final performance, training time, as well as transfer capability for multiple tasks.",
            "review": "In my opinion, this paper is a very good paper: novel in the approach, high impacts in both theoretic sense and practical sense, and well-written. The problem of universal multi-agent reinforcement learning for multiple tasks is very interesting and challenging, and the methodology proposed in this paper is inspiring and the demonstrated experimental results are very impressive. \n\nMain contributions of this paper are as follows, which are very significant and impressive:\n [1] The proposed UPDeT-based MARL framework outperforms RNN-based frameworks on state-of-the-art centralized functions by a large margin in terms of final performance.\n[2] The proposed model has strong transfer capability and can handle a number of different task at a time.\n[3] The proposed model accelerates the transfer learning speed so that it is about 10 times faster compared to RNN-based models in most scenarios.\n\nThe paper presentation is in good quality, the concepts and the methodologies were explained clearly. The provided experiments section is also convincing and somewhat extensive, described and presented very clearly, which supports the main claimed contribution very well.\n\nThe authors also point out some promising future research direction on top of this work, which is also helpful.\n\nOverall, I strongly support this paper to be accepted and published in ICLR. The experiment results are impressive, and the proposed methodologies are inspiring and novel. I believe that many people in the research community would find it valuable/inspiring and benefit from this paper.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A promising transformer-based MARL architecture with convincing results on SMAC. Clarity and readability need to be improved.",
            "review": "\n### Summary and claims of the paper\n\nThe authors propose several Transformer-based architectures that can be used in combination with a MARL (multi-agent reinforcement learning) algorithm to tackle multi-agent environments. They identify a particularly suitable architecture they call UPDeT which they claim combines the following advantages:\n 1) Can be used in the multi-task setting, where the number of entities and therefore the inputs changes from one task to the next.\n 2) Can handle a varying number of actions per entity (e.g. the controlled units could have N actions and each enemy unit corresponds to 1 action (which one to target), allied units correspond to no actions).\n 3) Is more explainable because the attention weights of the transformer can be inspected to see which information a given output depends on.\n 4) Outperforms the standard RNN approach on specific tasks.\n 5) Contrary to the standard RNN approach, the architecture is able to zero-shot generalize to tasks with different numbers of entities and learns significantly faster than the RNN architecture when fine-tuning on a new task. (This is in my opinion the most important and interesting claim).\n\nI have collected these claims from different parts of the paper and had to add some of my own interpretation to disambiguate what was meant. Please correct me if I misunderstood any of the claims. It would be nice if all of these were stated clearly in one place in the paper.\n\nPoints 1-3 can be argued based on the design of the architecture alone, while points 4) and 5) are supported by experiments using the SMAC (StarCraft Multi-Agent Challenge) environment.\n\nA summary of UPDeT from my understanding:\nIn MARL a group of independently acting agents needs to be trained to maximize a reward. Each agent has a separate set of Q values for each action that it can issue at a given time. In UPDeT, the Q values for one agent are computed by applying a transformer to all of the entities that the given agent can see. The resulting transformer output is then mapped into Q values depending on the positions in the transformer output. E.g. in the case of SMAC the position of the currently controlled unit corresponds to many actions, like moving left/right/up/down. The position of enemy units corresponds to 1 action each (targeting that unit) and the position of allied units (also controlled by the network, but not by this application of the transformer) corresponds to 0 actions. The mapping from transformer output to N actions is handled through a linear layer that is shared between entity groups (e.g. outputs for all enemy units get transformed with the same linear layer). A recurrent state through time can be added either globally for the whole transformer, or added to the individual units.\n\n### Discussion of prior work\n\nThe section on prior work is split between references to transformer and MARL literature.\nFrom what I can tell it is complete enough to position the paper sufficiently in the literature.\nThe one thing that I think is missing is a reference to the AlphaStar paper (https://www.nature.com/articles/s41586-019-1724-z) since this is an example of an agent that uses a transformer and is also applied to StarCraft.\nIn contrast to this work it doesn't use a MARL algorithm to control multiple units.\n\n### Are the claims supported?\n\nClaims 1) and 2) are supported directly by the design of the architecture.\nI struggled to understand the details of the architecture for some time. In my opinion this part of the paper could really benefit from a clearer presentation of the architecture (more details and some suggestions for improvement are in the next section).\n\nI'm somewhat unsatisfied with how claim 3) is handled in the paper, because the authors don't include any demonstration of explainability (e.g. a particular instance where the attention weights make it clear how a certain decision is made). Simply claiming that the architecture is explainable because a transformer is used is not enough, because in my experience there can be plenty of instances where we can't interpret how a given decision was made through the attention weights. It would be good if the authors would either give an interpretable example, or soften their claims of explainability and remove it from the main set of claims.\n\nClaims 4) and 5) are handled pretty well in my opinion.\nIn my experience the SMAC challenges are decently complex and allow meaningful differences in performance between algorithms to be measured.\nThe results in figure 4 support the design decisions in the architecture and show that the proposed architecture outperforms the standard RNN approach.\nThe only thing I'm unsure about here is the \"vanilla transformer\" architecture, which doesn't seem to be working at all. Is it possible that the transformer output is not conditioned on the different action types? E.g. this could be achieved by adding a positional encoding to the transformer corresponding to the different actions that can be issued. In any case such an architecture doesn't make a lot of sense since the actions are not related to the entities processed by the transformer. Maybe the aggregation transformer should be the \"vanilla\" version and the vanilla version can just be removed to make the paper simpler?\n\nClaim 5) is supported by a separate set of experiments where the agents are trained with a given number of units, and then switched to a task with a different number of units. UPDeT still performs reasonably well after the switch (zero-shot generalization) and quickly recovers to full performance during fine-tuning. I found this result genuinely interesting and I like how it is presented in the paper, e.g. by including the from-scratch curves as well.\nAs pointed out in the paper, the GRU baseline had to have some of its weights reset when switching from one task to another due to the changed action space. I wonder what would happen if a similar technique to UPDeT was used on the GRU, e.g. by emitting actions from different action heads depending on the identity of a unit as the GRU is unrolled (ideally it should be bidirectional). That way the resetting could be avoided and a fairer comparison between RNN and transformer could be made. This is not a crucial addition to the paper, but could be interesting.\n\n### Presentation and clarity\n\nI think the presentation and clarity of the paper should be improved significantly.\nThere are currently many typos, grammatical mistakes and missing words that made it hard for me to understand the paper.\nThe sections 3.2, 3.3 and 3.4 in particular are really crucial for understanding the paper and have some mistakes that confused me significantly.\nFor example in the bottom of formula (5) the attention is applied to three identical variables $R_i^l$ (this is done in other places in the paper as well, I'm not sure how to interpret it).\nIn Figure 3 the embedding outputs are labeled $e_{B,1}\\cdots o_{B,k}$, but I think this should be $e_{B,1}\\cdots e_{B,k}$.\nI found Figures 2 and 3 very difficult to understand, maybe because they show a very general version of the idea rather than the specific one that was tested by the authors.\nThe most confusing part of Figure 2 for me was the alternate white/colored filling scheme on the various tensors. What does it mean for a position to be colorless or colored here?\nI found Figure 6 in the appendix extremely useful for understanding the architecture. Maybe this should be part of the main text?\nI can appreciate that the authors want to present a general view of their idea (a \"framework\" rather than a specific architecture), but personally I would have understood the paper faster if it would focus on the specific case used for SMAC.\nThere are some references to \"training speed\" in the paper. It took me some time to figure out that this refers to the number of update steps rather than the actual speed of training the network. It would be good to clarify that.\n\nOverall I think the impact of the paper could really benefit from improving presentation and clarity.\nI have not taken this aspect into account in my rating.\n\n### Conclusion\n\nPros:\n - Benefit of using a transformer in a MARL setup is demonstrated in a non-trivial environment.\n - The proposed architecture goes beyond \"obvious\" approaches like the pooling the output of the transformer and the benefit of this is demonstrated.\n\nCons:\n- The paper is currently difficult to understand in several important places.\n- I think the use of transformers as presented here is only valid for environments with structured observations, e.g. where actions map nicely to specific entities in the observation.\n\nOverall I think the paper presents a promising approach to building MARL architectures. The approach makes sense and the experiments are insightful, but the presentation of the paper needs to be improved.\n\n*Edits after author comments and revision:*\nThe authors have greatly improved the readability of the paper. I also appreciate the addition of section 4.4. which seems like a reasonable attempt at supporting the increased interpretability of the architecture. I feel that the paper has improved, but it wasn't quite enough for me to raise the rating from 7 to 8, so I'm leaving it at \"Good paper\".",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}