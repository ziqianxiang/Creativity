{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This work presents a method to combine EBMs and VAEs in two stages. First, the VAE model is learned; second, an EBM-based correction term is learned via MLE. The methodology is novel and of interest to the ICLR community."
    },
    "Reviews": [
        {
            "title": "Interesting paper that learns EBM as a correction of VAE.",
            "review": "This paper proposes a model that corrects VAE by an energy-based model defined on image space. The model is learned in two phase. The first phase learns the VAE model, while the second phase learns the EBM correction term by MLE. Experimental results show that the proposed method outperforms pure EBM defined on image space and also pure VAE models by large margins. \n\n- pros: the paper is clear written and easy to follow. The ablation study shows clearly the advantage over baseline methods. Sampling from EBM on image space is hard. With VAE as a backbone, the sampling can be transferred to the latent space and the residual \\epsilon in the image space, which is much more friendly to MCMC sampling. \n\n- cons:\n1. The energy term is used to correct only on image space. Would be interesting to see if VAE can be corrected by a latent EBM where the energy function is defined on (x, z). \n2. After learning, would long-run MCMC sampling chain remain stable and mix well? It would be interesting to diagnose the long run chain behavior, and compare the difference of sampling in the space (\\epsilon_x, \\epsilon_z) and (x, z). \n3. For the synthesized results of CIFAR-10, it seems that some patterns appear repeatedly (e.g., the white dog face). Is the model suffered from mode collapsing problem? \n\nOverall, it is a good submission that proposes a principled method to combine VAE and EBM and demonstrates strong empirical results. I tend to accept this paper.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "Strengths:\nThe paper provides a thorough overview of recent work towards training EBMs.  \nThe approach generates high quality image samples by combining EBMs and VAE based models. \nThe paper is well written and is easy to follow \nI find it quite interesting that a combination of both models leads to significant overall improved generative performance \nI also enjoyed the proposed change in the paper -- and it seems to elegantly solve several problem in EBM training. \n\nWeaknesses:\nMy most major concern is that since we are utilizing a maximum likelihood objective to train models, it would be good to evaluate  the overall likelihood of the trained model, even if only in the  2D domain. \nThe histogram of likelihoods of data points is a bit disappointing -- it falls a similar trend of other EBM models, but it would nicer if it followed a Gaussian distribution \nWhat happens when more Langevin sampling steps are applied to the model? (greater than the few used in training) \nI'm also curious on what sampling only the trained energy model looks like (without using the trained VAE parameterization) at evaluation time \nI would also be curious to see how the trained EBM, with the VAE generator  can compose together with other models. See for example [1].\n\n[1] Yilun Du, Shuang Li, Igor Mordatch. Compositional Visual Generation and Inference with Energy Based Models. NeurIPS 2020\n\n#### Post Rebuttal-Update\n\nI thank the authors for responding to my concerns. I enjoyed reading the paper and maintain my rating.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Pros:\nthis method proposed to use VAE+EBM for generative modelling. Unlike other VAE+GAN/EBM-liked model, it added a EBM after VAE.\nOverall method is easy to understand and follow.\nTo accelerate the training, the authors also applied a buffer to store the previous examples for easy sampling.\n\nCons:\nIn the experiment, the authors compared other models with VAEBM, it is reasonable to compare the results with reported scores in other works, however, since the architecture is a fairly important factor (such that swish instead relu, resblock instead of cnn, weight norm instead of spectral norm), etc, is it also reasonable that the improvement is partially contributed by such design of architecture.\nSo I will suggest that the authors should use the same architecture design (choose other one or two models for all tasks), and test whether the proposed method can actually gain that much of improvement.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A promising symbiosis between VAEs and EBMs",
            "review": "The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs. The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x, z) VAE space using reparameterization. The method is shown to achieve high quality samples on several modern image datasets, good FID scores and mode coverage. Ablation studies show the contribution of the different elements.\n\nThis is, in my opinion, a very good work, which combines a novel and well-motivated idea with clear writing and extensive experimental evidence.\n\nSome comments and questions:\n- Does the separate twos-stage training enable the model to reach the optimal point that can be reached in joint training, or is it an approximation? If its an approximation, I think it should be discussed or perhaps bounded.\n- Does the combined model allow computing the likelihood? Can it be evaluated and compared to other models in terms of bits/dimension (e.g. as in VAE or NVAE)?\n- It might be interesting (not something that I think is mandatory) to measure the NVAE log-likelihood of samples generated by the combined model compared to samples generated just by the NVAE.\n\nTo summarize:\npros:\n- novelty\n- significance\n- experimental evidence\n- quality of writing\n\ncons:\n- combining two separately trained models - perhaps sub-optimal\n\n**Update: I thanks the authors for their answers and revised version and keep my positive rating.**\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}