{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper posits that VAEs, if made sufficiently deep, are able to implement autoregressive models, and could possibly outperform them. Experimentally, the authors attempt make VAEs sufficiently deep so that they are able to outperform autoregressive models on image generation. The authors use a variety of tricks to scale the depth of the model to up to 78 stochastic layers, and achieve SOTA, or near-SOTA NLLs on a number of datasets. Furthermore, in comparison to other models (in particular the recently proposed Nouveau VAE), the models achieve these scores using far fewer parameters.\n\nAlthough the tricks are a bit ad-hoc and the novelty is a bit weak, the experimental results are quite strong and would be of interest to anyone working on VAE research. Moreover, one of the weakness of the paper, a lack of ablations, was addressed during the rebuttal. All reviewers believed that the paper should be accepted, and I see nothing in the paper or the reviews to suggest otherwise."
    },
    "Reviews": [
        {
            "title": "A good paper: a clear idea supported by a robust set of experiments",
            "review": "1. Summary\nThis paper shows that deep hierarchical VAEs can outperform state-of-the-art autoregressive models on images. The authors first argue that autoregressive models are special cases of hierarchical VAEs and that hierarchical VAEs are universal approximators. They introduce a simple top-down (LVAE) architecture that scales past 70 layers. Furthermore, the model can be trained without using freebits or KL annealing -- although additional tricks are required (gradient skipping and prior warmup). They demonstrate that likelihood performance is correlated with depth and report state-of-the-art performances on multiple image datasets.\n\n2. a Strong Points\n- the contribution is clear: the idea the hierarchical models can outperform autoregressive models is clearly stated and supported by a short theoretical section and relevant experiments (likelihood vs. depth + benchmark)\n- the proposed model is simpler than existing methods and is trained using a simpler objective (no freebits/KL-warmup):\n- the authors demonstrate state-of-the-art likelihood on multiple datasets: the Very Deep VAE indeed outperforms large autoregressive models\n- the authors demonstrate that the method is scalable: proof of concept on FFHQ 256 and 1024.\n\n2. b Weak Points\n- the part on parallel generation is unclear: are you referring to each of the latents for one layer being sampled independently? (i.e. $q(\\mathbf{z}^l | \\mathbf{z}^{l-1}, \\mathbf{x}) = \\prod_d q(z^l_d | \\mathbf{z}^{l-1}, \\mathbf{x})$)\n- section 5.2.1 is unclear: \"unlike autoregressive models, this scaling does not require greater training resources\". In my opinion, using larger images does requires greater training resources because of the increased image definition.\n- Although the code is provided, the experimental protocol is not well described in the paper (learning rate, number of epochs, hidden size, ...)\n\n3. Recommendation\nI recommend accepting this paper.\n\n4. Recommendation Arguments \nThis paper presents a simple, yet solid, story. The empirical results strongly support that \"very deep VAEs can outperform autoregressive models on images\" and the authors introduced a minimal architecture allowing to do so.\n\n5. Questions to the Author\nPlease elaborate on the weak points.\n\n6. Feedback \nFigure 2: the sentence \"latent variables are observed variables\" is odd. Latent variables are unobserved by definition. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very good paper improving deep VAE performance beyond autoregressive models, ablation studies could further strengthen it",
            "review": "**summary** \nthe paper puts forward an idea that deep-enough VAE should perform at least as well as autoregressive models. Authors explore this in the context of image generation, and construct VAE model that is a generalisation of typical autoregressive architectures. They use several tricks to ensure stable training of very deep VAEs and show that final performance exceeds all autoregressive models. This experimentally supports their claim that very deep VAEs encompass autoregressive models.\n\n**pros**\nThe idea of perceiving VAE architectures as strictly more powerful and potentially efficient is very appealing. Given the recent work on improving deep VAE training(like Vahdat & Kautz (2020)) this paper takes another step in this direction by effectively, as it seems from the text, removing the depth limitation for training such VAEs. The tricks used to stabilise training are pretty ad hoc, but their effectiveness, showed experimentally, is important in advancing the field.\n\n\n**cons**\n* The main criticism I have is around ablation studies that justify the proposed architecture choices and training stabilisation tricks, as well as comparison to other tricks in the literature (e.g. Vahdat & Kautz (2020)). Of course the positive result speaks for itself, but the paper would be even more convincing with some details on the exploration that led to the final model.\n\n\n\n**questions**\n* it would be good to clarify in the text how exactly sampled latent variables from lower layers are decoded into the images to produce Fig. 4: is the idea to pass those latents down the top-bottom path and just not add new latents in the node \"+\" within the topdown block?\n* In Section 5.2.1, it is unclear why models with 32x32 and 1024x1024 resolutions have equal number of parameters: is this because ResNet blocks used at different resolutions share parameters?\n* Did the authors experiment with methods of slowing down the training of the prior, other then stopping it for the first half of training? It seems that exponentially averaging prior parameters might be another way of doing it, although the exponent will become another hyperparameter.\n\n**comments**\n* Further investigating the relation between using NN interpolation in upsampling and having active latents in all layers would be very useful. \n* I particularly enjoyed the perceptional shift that the paper advocates for, i.e. that VAE and autoregressive models are not competing approaches, but rather VAE is a more general one and it encompasses the latter.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A very deep and very good VAE",
            "review": "**GENERAL**\nThe paper claims that high quality of generated samples and SOTA bpds are achievable by VAEs if the model is deep enough (deep in terms of the number of stochastic layers). The authors explain the architecture that resemblances the U-net architecture, and explain its building blocks. Interestingly, they are able to learn VAEs with up to 78 stochastic layers, and achieve SOTA bpds on CIFAR-10, ImageNet-32, ImageNet-64, FFHQ-256 (5-bit), and setting a great result on FFHQ-1024 (8bit).\n\n**Strengths:**\nS1: The authors are capable of training VAEs with over 45 stochastic layers (up to 78).\n\nS2: The proposed architecture does not contain any extra \"tricks\", it is relatively simple. This is a great plus for the paper!\n\nS3: The presented theorems are interesting additions to this rather practical paper.\n\nS4 The experiments are well performed and the ablation studies are insightful.\n\nS5: Generated images are of very high quality! Even a reflection in a glass of a generted lady is better than samples of CIFAR10 in many papers.\n\n**Deficiencies:**\nD1: The prior is not explained in the paper! Without this information, it is hard to properly understand what kind of problems occur during training q(z|x) and p(z).\n\n**Remarks:**\nR1: The proposed heuristic method for training q(z|x) reminds of the following paper:\nHe, J., Spokoyny, D., Neubig, G., & Berg-Kirkpatrick, T. (2019). Lagging inference networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534.\nIt would be interesting to compare at the conceptual level both heuristics.\n\nR2: It seems that the authors do not use BatchNorm. Is it correct? This would be also interesting to discuss, because in the following paper:\nVahdat, A., & Kautz, J. (2020). Nvae: A deep hierarchical variational autoencoder. arXiv preprint arXiv:2007.03898.\nthe BatchNorm is indicated as an important component for achieving a deep VAE.\n\n**Questions:**\nQ1: What kind of prior was used in this paper?\n\nQ2: Is BatchNorm indeed irrelevant for VAEs?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A strong empirical contribution on hierarchical VAEs",
            "review": "Summary\n--------------\n\nThis paper provides evidence that \"very deep\" hierarchical VAEs can outperform autoregressive and flow-based models albeit using less parameters on image density estimation tasks.\n\nIt seems natural to think that a hierarchy of latent variables progressively compressing information would be useful for image modelling, with top latent variables capturing more abstract/general features and bottom latent variables capturing lower-level details. However, recent success of flow based and autoregressive based models such as PixelCNN seemed to invalidate the need of such hierarchy of latent variables and to \"compress\" pixel-level information. Here, the authors show that a simple hierarchical VAE architecture inspired by previously proposed ones can outperform autoregressive models if it's made sufficiently \"deep\". I think this is an important contribution. With respect to previous work, this work relates to the concurrently proposed \"Nouveau VAE\" but obtains better results with less parameters and considerably less involved customization of the architecture.  The authors report impressive results on multiple datasets generally using less parameters than competing models. Additionally, sampling from very deep VAEs is considerably cheaper than in autoregressive models.\n\nThe authors also attempt at showing that learnt latent variables implement a hierarchy of information which could be useful to have in general. This point is a bit weak and not well demonstrated in the paper.\n\nPros\n------\n\n- Strong results on multiple tasks with a method that was previously thought to have plateaued in performance\n\nCons\n-------\n\n- Originality / novelty is a bit weak\n- Clarity can be improved\n\n\nDetailed Remarks\n-------------------------\n\n- Figure 4 is not totally convincing as high-level features in the first image are not always maintained in higher-resolution realizations (sample in the first row seem to have glasses then they disappear?). Could you include more samples to back this claim? Do you think of a way of understanding whether high-level variables maintain general info (maybe by probing the posterior samples for some downstream attribute ?)\n\n- I find it hard to understand what is going on in Table 1 (left). In Section 5.1, referring to Table 1, what do you mean by \"grouping layers to output variables independently instead of conditioning on each other\" ? In Table 1, what do you mean by \"masking\" in the sentence \"with masking introduced such that the effective stochastic depth is lower\" ? I cannot find any other references to masking. \n\n- In Figure 3, what do you use as the pooling operation? (2, 2) max pooling ?\n\n- An ablation study of the proposed modifications to the architecture and training tricks would be useful, e.g. what's the most single important modification that makes the model work ? Is it the neighbour upsampling ? Is it the 1/\\sqrt(N) init of the last layer ? Is it the skipping gradient trick ?\n\n- How is Figure 4 obtained ? When you say \"The rest of the high-resolution variables can be output in parallel, largely independent of each other\", are you referring to the fact that you sample from the top 1x1 layer and then sample independently the other zs from the learnt prior e.g. p(z_4x4) ... p(z_64x64) without ancestral sampling ?\n\n- Section A.1: \"Without loss of generality, we simplify notation by assuming each vector-valued latent variable zi\nonly has one element, which we write as zi\", do you mean each latent variable is \\in R ? It'd be good to mention that\nyou assume an architecture with an auto-regressive learnable prior p_\\theta(z_i | z_<i) or refer to Eq. 2.\n\n- Section A.2: I am not sure you need this sentence: \"Without loss of generality, we simplify notation by assuming each vector-valued latent variable zi only has one element, which we write as zi\", as it seems copied from A.1.\n\nGrammar\n-------------\n- Section 5.2: \"an learned\" -> \"a learned\"\n- Section 4.1: \"the the\" -> \"the\"",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}