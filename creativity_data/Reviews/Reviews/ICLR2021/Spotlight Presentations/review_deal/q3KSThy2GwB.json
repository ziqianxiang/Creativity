{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper introduces a method for approximating real-time recurrent learning (RTRL) in a more computationally efficient manner. Using a sparse approximation of the Jacobian, the authors show how they can reduce the computational costs of RTRL applied to sparse recurrent networks in a manner that introduces some bias, but which manages to preserve good performance on a variety of tasks.\n\nThe reviewers all agreed that the paper was interesting, and all four reviewers provided very thorough reviews with constructive criticisms. The authors made a very strong effort to attend to all of the reviewers' comments, and as a result, some scores were adjusted upward. By the end, all reviewers had provided scores above the acceptance threshold.\n\nIn the AC's opinion, this paper is of real interest to the community and may help to develop new approaches to training RNNs at large-scale. As such, the AC believes that it should be accepted and considered for a spotlight."
    },
    "Reviews": [
        {
            "title": "A promising study (second review: even better than I thought!)",
            "review": "## Second review\n\nThanks for taking all my comments seriously. After clarification of the difference with RFLO I see that this work is even richer than I thought and I increase my grade to 8. It seems that other reviewers did not appreciate that training a network without back-prop requires nontrivial engineering and theoretical considerations which are well described in this paper, I truly think it is a pity if this work is not accepted.\n\nI fully agree with the difference between RFLO and Snap-1 that you describe in your reply, and I think it would be really great to put that somewhere in the paper. As you suggest it would be great to explain that you did not use random feedback weights for RFLO.\n\nThis would also be a great opportunity to explain how did you extend RFLO to a GRU network in Figure 3. I find it a bit puzzling, that RFLO appears worse than an untrained network in Figure 3 (even early in training as in seen in Figure 3.B). Is there any additional difference in the network model for these two baselines, like one is using leaky RNN and the other one GRU or something like that?\n\nI find the piece of JAX code incredibly rich. It would be great to publish that along with the paper! JAX is not yet very well spread, and we see here that it is a very promising tool for custom gradient in RNNs.\n\n## Summary\n\nThe authors describe new algorithms to train sparse recurrent neural networks, these algorithms are described as variants of RTRL. These methods, called SNAP-$n$, use the same induction as in RTRL but approximate the true Jacobian matrix $J$ by a sparse matrix where each coefficient is set to zero if the corresponding parameter does not influence the corresponding state variable within $n$ steps. These alternatives to BPTT alleviate the memory consumption growing otherwise linearly with the sequence length. \n\nA theoretical complexity analysis and simulation experiments are carried out. The simulations are performed on the character prediction task and a synthetic copy task. The authors report that the network reaches performance comparable to BPTT and sometimes better (snap-3 leads to better copy task performance with GRU, snap-2 seems already better with LSTMs, however it requires many more FLOPS).\n\n##  General opinion\n\nCongratulations to the authors. This is an important topic since recurrent networks are not efficiently trained with BPTT. More intensive and rigorous research are needed to find suitable alternatives. The SNAP idea is simple and appealing, and the results encouraging (even though it still requires a large number of FLOPS).\n\nI recomputed rather carefully the complexity analysis for sparse RTRL, snap 1 and snap 2 and arrived at similar results. Theoretical results seem to be correct and the experiments are credible. \n\n##  Requires clarification\n\nOne negative comment is that snap-1 has been published before in other forms as explained below The writing makes it sounds more novel that it actually is and some comparisons with other algorithms are unfair. It would be great to correct the shot but snap deserves publication anyway since it also provides a rigorous analysis of the full snap family and snap-1 had not been explored in such details in the interesting context of sparse networks.\n\nRFLO for leaky RNNs is basically exactly snap-1 with the additional burden of carrying random feedbacks. It is written three times that snap-1 is better than RFLO but it would be great to comment on the differences and explain where is the difference of performance coming form. The random feedback was introduced in RFLO to avoid the transport problem for biological plausibility. It does not seem relevant to do this extra approximation step if RFLO is used for performance and not plausibility. If the random feedback is the main difference, one should clearly say that the algorithm are otherwise identical. If the authors see other differences, it would be great to indicate what they are.\n\nThis very same algorithm (snap-1, RFLO) was also published under the name e-prop 1 [a] although the theory was derived differently. The authors had shown that e-prop 1 (aka snap-1) works well on the copy task, a word prediction task with LSTMs (Figure 4 in [a]), ATARI games and TIMIT (more recent work).  The authors had also suggested an amelioration called e-prop 3 that improved the performance and kept the same time complexity as BPTT unlike snap-2 and snap-3. Maybe it is relevant to comment on the relationship between snap and this paper [a] ?\n\nIn case the authors were not aware of this, an other interesting approximation to RTRL was published in [b], the authors may or may not comment about that too.\n\nThere are technical details that should be given for clarification:\n- The authors might want to provide details on the computation of the complexity of one or two of the essential component of the table to make it more accessible to the reader. If I am not mistaken I think that the complexity results are true \"up-to a proportionality constant\" for a general RNN model, maybe it would be great to write that in the caption for instance. The complexity is written in the table in the form A + B, I understood than A is the complexity in the forward pass and B is the complexity in the backward pass, but maybe it should be explained. Maybe there is also an easy way to confirm those numbers with the empirical number of FLOPS given later in the paper ?\n\n- In Figure 4, I cannot find out what the colored dash lines are meant to be. This caption is rather short and there is an opportunity to add some information: for instance what is \"curricula max\".\n\n- In Figure 2, the author use k^2 by they have also introduce the letter p. Is that not meant to be the same thing? It probably depends on the RNN model? Before doing the calculation of the complexity myself I did not know whether p included already the coefficient d, I do not think that it obvious that zeroed coefficients are still considered as \"parameters\" in the \"number of parameters\" p. Maybe this can be said in the caption?\n\n- If I understand correctly the pruning method in appendix B, the network is first trained until convergence with BPTT and then, the best architecture is fixed to be retrained with snap? It would be great to clarify this paragraph because it is not easy to read. If that's the case there could be a substantial transfer of information by passing on the \"winning\" architecture from BPTT to the SNAP training, in particular for very sparse networks. Is that really necessary? How much would the performance decrease? As a control, I would guess that for something like 75% sparsity the network can be trained from a random matrix. Also I do think that there are now simple methods available for training much sparser networks from scratch, and it would not require this pre-BPTT step.\n\n- It would be so great to have some details about the implementation: what software did you use to perform this \"remake\" of a forward propagation? Did you have to implement a custom C++/cuda code, maybe use JAX ? Did you use sparse matrix cuda kernel, how good was it? Maybe the code will be shared, if not any details are welcome.\n\n\n[a] Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets\nGuillaume Bellec, Franz Scherr, Elias Hajek, Darjan Salaj, Robert Legenstein, Wolfgang Maass\nhttps://arxiv.org/abs/1901.09049\n\n[b] Kernel RNN Learning (KeRNL)\nChristopher Roth, Ingmar Kanitscheider, Ila Fiete\nhttps://openreview.net/forum?id=ryGfnoC5KQ\n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "iterative work on adding sparsity to RNNs, need more clarification",
            "review": "## Second Review\nThe author's thoughtful response has clarified most of the missing details in the paper. It is true that idea is interesting and theoretical analysis are promising. However, I still have issues understanding failure conditions. If the method is purely based on trial and error to determine optimal threshold for sparsity, then it requires many engineering tricks. Thus I would request authors to provide such details, such that young researchers can extend this work to create better training paradigm for RNNs. Other issue as pointed out by other reviewers is using only 3 runs to report results.  I really appreciate explanation about difference between 2 variants of GRU and how it adds sparsity to the model. Additionally JAX code helped in understanding many key apsects of the work. I would encourage authors to make it public, and provide key insights for training RNNs using snap. Nonetheless, the proposed method and theoretical analysis are insightful and I believe this to be a first step towards building scalable RNNs which efficiently gets rid of credit assignment issue. This paper does add significant pedagogical value, which can benefit complex task such as grammatical inference. I'm increasing my score from 5 to 7 and I hope this paper is accepted. \n\n## Summary\nPaper introduces snap which adds sparsity to the influence matrix extracted from RTRL which acts as a practical approximation for RTRL, snap is extension of prior work on snap-1 used to train LSTM, and authors have shown that one can train dense as well as sparse RNNs using snap achieving similar performance as BPTT on real as well as synthetic dataset. Few clarifications in terms of snap working and few key information w.r.t to parameters are missing.\n\n## Clarification\n\nHow does one evaluate level of sparsity required for any given task? At what n (sparsity ratio) optimal performance is observed which leads to better performance. It is well known that full RTRL (forward propagation helps compared with backward propagation), especially in case of continual or online learning [Ororbia and Mali 2020] and copy task (KF-RTRL, UORO). Does current sparsity measure work on variant of RTRL? And how do you determine top k to select k elements for creating a sparse matrix? is it random like 70-80-99? Many key details are missing, and authors are requested to provide more information to better understand model flow.\n\nIn appendix authors talk about “modeling performance of the two variants of GRU. which has been shown to be largely the same, but the second variant is faster and results in sparser D_t and I_t”. I am confused, what is the relationship between sparsity and two variants? Please provide some numbers explaining how sparsity is increased by moving reset gate after the matrix multiplication.\n\nHow does sparsity measure is introduced in this work? Does model stay consistent whenever regularization approaches such as zoneout or dropout are used or introduced into the model? Do you observe the similar performance? Does network roughly converge to similar performance with optimal sparsity or sparsity measure changes as other regularization approaches are introduced? Did you do grid search for language modelling task or copy task (beside learning rate)? If so please provide details? Citation and comparison missing with Sparse attentive backtracking, which in theory can work with sparse networks and its temporal credit assignment mechanism can help in introducing sparsity [ke and goyal 2019]. \n\nAuthors states that “In order to induce sparsity, we generate a sparsity pattern uniformly at random and fix it throughout training” What is the range for random uniform? Is model sensitive whenever sparsity pattern is changed while training (may be per epoch or k epochs). How can one ensure that the sparsity pattern at start is the optimal one for any network? Does similar pattern work for all GRU, LSTM, RNN or one needs to adapt scheme based on architecture?\n\nAdvantage of snap-2 and 3 over snap-1, snap-1 is similar to (Hochreiter & Schmidhuber, 97) work on training LSTM, what modification is introduced on snap-1 beside training it on GRU? And sparse networks. It is still unclear what advantage these 3 variants add. It is important to show speed (with various sparsity, convergence plots or else these variants would have similar performance and memory requirement compared with vanilla RTRL\n\n\n[Ke and Goyal 2018] Ke, N.R., GOYAL, A.G.A.P., Bilaniuk, O., Binas, J., Mozer, M.C., Pal, C. and Bengio, Y., 2018. Sparse attentive backtracking: Temporal credit assignment through reminding. In Advances in neural information processing systems (pp. 7640-7651).\n\n[Ororbia and Mali 2020] Ororbia, A., Mali, A., Giles, C.L. and Kifer, D., 2020. Continual learning of recurrent neural networks by locally aligning distributed representations. IEEE Transactions on Neural Networks and Learning Systems",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice paper; experiment protocol needs improvement",
            "review": "## Post response update\nThe author's response has clarified most of the missing details in the paper. I still have an issue with reporting results for 3 runs --- even if the variance is small for 3 runs, that does not imply that there won't be outliers when one does more runs. Nonetheless, the proposed method is insightful and the paper has significant pedagogical value. I'm moving my score from 6 to 7 and I hope the paper is accepted. \n\n## Summary\nThe paper tackles the structural credit-assignment problem for a recurrent network when parameter values in earlier time-steps can impact the prediction in the future. The most common approach to achieve this structural credit-assignment, in the current deep learning literature, is BPTT. BPTT, however, is not suitable for online learning. First, the computation and memory requirements of BPTT grow with the length of the sequence. Second, BPTT does not spread computation uniformly --- all the computation happens at the end of an episode. This is not suitable for an online learner that has to learn and react in real-time. An alternative to BPTT is RTRL. The computation and memory needs of RTRL is distributed uniformly across time-steps and RTRL makes it possible to learn at every step, but it requires an intractable amount of memory and compute for even modestly sized networks. \n\nThe limitation of both BPTT and RTRL necessities research on new algorithms. Ideally, we want algorithms that spread the computation uniformly and are still scalable. SnAp, the methods proposed in this paper, is one such algorithm. The general idea behind SnAp is to approximate the gradient by only taking into account the impact of a parameter $w_j$ on an activation $h_i$ only if $w_j$ influences $h_i$ with-in n steps. A similar algorithm was used in the original LSTM paper that was equivalent to SnAp with n=1 for the LSTM architecture. This paper generalizes the algorithm used in the original LSTM paper in two dimensions. First, it generalizes it to methods beyond the specific LSTM architecture, and second, it can keep track of influence of parameters across $n$ steps instead of just 1. While the cost of SnAp increases quickly as $n$ increases, the authors propose a promising direction for keeping the cost down. They argue and show that for highly sparse RNNs, SnAp can be scaled to $n > 1$. \n## Review \n\n### Strengths \nThe paper is well written. It summarizes the prior work concisely and explains the two views of computing the gradient for an RNN --- the recursive view used by RTRL and the unrolling view used by BPTT --- clearly. The need for sparsity in RNNs is well-motivated and the observation that sparsity in RNN slows down the propagation of influence of a parameter on a state is interesting. The new algorithm, SnAp, is clearly presented as an approximation to RTRL. The paper also does not make unsubstantiated claims and explains the merits and limitations of the proposed method clearly. Overall, I'm highly impressed by the quality of the paper and the merits of the idea. \n\n\n### Weaknesses \nWhile the paper excels in writing quality and the proposed method is sound, the empirical evaluation of the method has several issues. First, it's not clear how the hyper-parameters for all the methods were tuned. Were the parameters tuned for SnAp and inherited for other methods? Were they tuned independently? The paper mentions that it used $\\beta_1=0.9$ and $\\beta_2=0.999$ for the Adam Optimizer without explaining how they were chosen. \n\nMany details of the experiment setup are not fully specified. For example, in page 6 the authors mentioned that they use a one-layer MLP to get 1024 hidden units which are mapped to 256 unit software, but do not clarify if a non-linearity is applied to the 1024 units. The paper also omits how $\\theta$ is initialized. \n\nThe experimental results have no error margins, and are the mean of only 3 runs. Ideally, authors should repeat the experiments for over 20 runs and report the standard error of the mean. Even if they are limited by available compute and are unable to do many runs, they should at-least report the standard-error for however runs they do (Note that standard error computation is biased for few runs and it might be a good idea to apply bias correction. More details here: https://en.wikipedia.org/wiki/Standard_error). \n\nParameters sweeps should be extended if the optimal parameter is at the edge. The authors report that they tried $10^{-3}, 10^{-3.5},$ and $10^{-4}$ and found $10^{-3}$ to be best. However, all this tells me is that a higher learning rate could have been even better. They should include larger learning rates in the sweep for the experiment. \n\nIt's not clear if the authors are (1) finding the best learning rate and then re-running for 3 seeds for the best learning rate or (2) simply running the experiments for 3 different seeds for all learning rates and reporting the best results. The former is a sound strategy whereas the latter suffers from over-estimation bias. \n\nGiven the issues in the experiment methodology, I'm giving the paper a weak accept for now even though I think that the paper excels in many ways. The issues identified can easily be fixed during the discussion period and I would be more than happy to change my score to an accept or a strong accept after a revision that fixes the experimental issues. \n\n\n### Questions \n1. In page two, the paper introduces a function $g_{\\phi}$ for mapping the state to the target and then says that the goal of the learner is to minimize loss with respect to $\\theta.$ Shouldn't the loss be minimized with respect to both $\\theta$ and $\\phi$? Or are the authors implying that the readout layer is fixed? (Or perhaps $\\phi$ is a subset of $\\theta$?)\n\n2. It's not clear to me from the write-up how the sparsity pattern is choosen empirically. Is the idea to run the RNN for n-steps, empirically observe enteries in $J_n$ that are zero, and fix those enteries to be zero for all future steps? If yes, could the initial weights of the RNN and the data make an entry in $J_n$ to be zero even if it would not have been zero for a different initialization and data-stream? ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review #4",
            "review": "This work presents a method, named SnAp, that takes Real-time Recurrent Learning derivations and proposes to approximate its computations with sparse approximations to make them more computational tractable. The method is an alternative to overcome the truncation in backpropagation through time (BPTT) over long term temporal structure. The method assumes a sparsity pattern on the parameters of the network that leads to the relationship on how the gradients could be updated. Finally, the method is evaluated against BPTT, UORO and RTRL on character-level language modeling and a copy task.   \n\n=================\n\nThe method is simple and a complexity analysis has been included. The experimental section seems limited in only showing a performance comparison with other methods. A better analysis in aspects of the method (like capacity for learning long-term temporal patterns) is lacking.\n\n==================\n\nWhen does the Snap/RTRL is not applicable? Are there cases where BPTT is applicable and Snap/RTRL is not? It would be nice to clarify such cases so readers can understand when Snap, RTRL, or BPTT are the right or better solutions. \n\nThe motivation behind the method is to overcome the limitation of the truncation behind BPTT for learning long term temporal structure. However this seems to be evaluated with very short sequences overall (128 for language modeling and maybe less than that for the copy task, the length is not present). How well the Snap method would work when training for sequences of thousands of elements, where BPTT is well known to struggle?\n\nThe relationship between neurons can vary every time the parameters are updated. Is it assumed to be fixed over the entire training? What would happen if the pattern is updated every few gradient updates? Also, complexities for Snap in table 1 don’t seem to consider the computational time of computing the sparsity pattern (even for a random pattern).\n\nWhen n is big, the experimental results show a better and competitive performance to BPTT. However, in such cases (like Snap-3) the computational cost becomes very expensive compared to BPTT by at least 2 orders of magnitude, and the matrices become more dense. What are the results of TBPTT when stateful training is applied and how do they compare in such case? Also, the copy task has been solved with fewer neurons in previous works. \n\nThe Copy task details are unclear for someone that doesn’t know the work from Mujika et al., 2018. Please describe all the details. What is the length L used in the experiments? What is the length of the overall sequence? What does “data time” mean in the plots of Figure 4?\n\n==================\n\nMy concerns behind the limited practicality of the method, and the limited experimental results given the hypothesis that the method can learn long-term temporal patterns. These are my considerations not to accept this paper.\n\n==================\n\nMinor issues:\n\n-Use bigger fonts in the plots, and diagrams.\n\n-In 5.1, do you use SGD or Adam?\n\n-Figure 3: leave space between caption and figures\n\n-Table 1 caption: “Below” -> “Above”\n\n-It would be nice to mention the relationship between |\\theta| and k, for each recurrent cell case.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}