{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposed to defend against model stealing attacks by dataset inference. The paper received unanimous rating of \"Good paper\" and \"accept\". The reviewers praise this paper insightful and well written. There are active discussion between the reviewers and authors, which further clarify some of the issues. Given the positive review and overall rating, the AC recommends it to be an spotlight paper."
    },
    "Reviews": [
        {
            "title": "interesting",
            "review": "The submission proposes to defend against model stealing attacks by dataset inference. It is argued that other watermarking techniques are not robust enough to be transferred during model stealing.\n\nOwnership resolution is performed by kind of a membership inference attack that the model owner runs on the potential copy. As only the owner knows the true training data, the \"attack\" can be amplified by a few samples. This leads to a rather strong signal in order to show model provenance. Final decision is formulated as a hypothesis test in order to get a confidence. The argumentation why for larger models this is not dependent on some sort of over fitting is not clear.\n\nWhile the idea is interesting, it is somewhat at odds with others goals in machine learning. In general, it is not a desirable property if the trained model contains \"artifacts\" related to the training data. This on the one hand could be overfitting artifacts that hamper generalization or more prominently could mean that the model does not preserve privacy. If such overfitting is not a major issues and privacy is not a concern, the approach seems viable. However, there should be an extended discussion under which situations/models the method is feasible. (e.g. privacy preserving learning?)\n\nI somewhat disagree with the practice of moving the related work to the appendix. this is a vital part of a paper and not an appendix. this looks like breaking the page limitation. overall, having a 19 page submission is not quite following the submission guideline (although the additional material is useful). this should not become practice - in particular as some parts in the main submission could be more concise. In particular, as the actual algorithm is quite simple (which I don't want to see as a disadvantage as it's well motivated)\n\nOverall, the paper is insightful -- also adding to the understanding of the connections to membership inference. Also the attention to important details like query efficiency is taken care of.\n\nIt is bit disappointing that the method is not evaluated on more complex classification problems. In particular, as membership inference (which this method is based on) can be even weaker on those larger and well trained models. The claim about other watermarking techniques no being effective - although probably true - was not substantiated (unless I've missed). at least there is no comparison to other attribution/provenance techniques.\n\nWhile this submission is a very interesting and enjoyable read, in the end it fails to fully convince how large the impact will be. Questions are open w.r.t. models with privacy guarantees and larger models  ... and also in the end, this technique is still effected by fine tuning. It is unclear to what extend this is an issue.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "the proposed dataset inference approach is novel",
            "review": "This paper tackles a timely problem of detecting model stealing attacks. The proposed identifies the stolen model by investigating whether the model is trained on the same dataset as the victim model. \n\nPros:\n-\tThe proposed method of dataset inference for model stealing detection is novel.\n-\tThe experiments provided by the paper are comprehensive, including different assumptions of data access, model access, and query access, etc. Adaptive attacks are considered in the paper as well.\n-\tThe paper is well-written and easy to follow.\n\nCons:\n-\tZero-shot learning ([9] in the paper) cannot be directly used as a model stealing attack, because the proposed approach in [9] requires more information of the victim model than the input and output pair. It would be better if the paper could investigate other data-free model stealing approaches (e.g., [1][2]). Also, it is not clear to me how the proposed dataset inference approach is applied to data-free model stealing attacks, since the training dataset used by the attacker is different from the victim’s dataset. If the attackers use some synthetic data to steal the model, will the proposed approach work?\n-\tWhat are the white-box and black-box settings in Section 4.1?\n\n\n[1] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 512–527. IEEE, 2019.\n\n[2] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pp. 506–519, 2017.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work and findings, but current draft needs work",
            "review": "**Update** : Since nearly all of my issues have been addressed, I have changed my rating from 5 to 7. Best of luck :)\n\nSummary:\n\nThe authors study a subset of model stealing by introducing dataset inference, a process to identify whether a suspected adversary's model has private knowledge from the original model's dataset. Through their experiments on two related tasks in the vision domain (CIFAR-10,100), the authors demonstrate their approach for dataset inference that combines statistical tests and decision boundary-based checks to make claims of model theft. They base their algorithms on theoretical results in toy settings (proofs shifted to additional materials).\n \n##########################################################################\n\nReasons for score: \n\nOverall, I feel the final observations are quite useful and pave new methods for asserting model theft in real-world settings. Although the proposed techniques are not novel (in fact, a lot of existing literature is not cited, giving the impression that a lot of proposed techniques are contributions of the paper itself), the end results and observations can be useful in practical settings. I feel the authors need to make a lot of changes to the current draft before it can be ready for acceptance. \n \n##########################################################################\n\nPros: \n  \n- Mentioning points such as how issues like these can actually get to courts in real life and are covered by intellectual theft laws is great to see. Since the proposed solution tackles a real-world (potentially) problem, it makes sense to incorporate real-world implications as well.\n\n-  Using theoretical analyses to justify the proposed methods is a great way to approach a problem like this. Testing them out on simpler problems to see how and most importantly, why they work, is important.\n\n- Evaluation over multiple model-stealing attacks is a great way to both show the robustness of the proposed attack method, as well as identify strong (fine-tuned models) as well as weak (label-query attacks) points of the algorithm.\n\n- The query efficiency of the proposed method is quite remarkable. A method that can use these little samples in a real-world setting to assert model stealing (or check for it) with high confidence can be very helpful.\n\n- Page 8: 'gradient-based approaches are sensitive to numerical instabilities' is a great and very valid point. An adversary could in fact even deliberately resort to gradient masking to avoid such an attack.\n \n##########################################################################\n\nCons: \n\n- By biggest and foremost problem is with the threat model. In all of these settings, it is assumed that the party accusing the adversary of stealing dataset access provides actual data from its own training dataset. However, since this is a real-world scenario, there should also be some way for the accuser to provably show that the data points it is using for the test are in fact from its own training set. Also, it should be made clear what level of overlap in the datasets used is problematic. This parameter would vary from problem to problem and thus cannot be one fixed value. Nonetheless, it should be a parameter of the testing process.\n\n- The term 'dataset inference' is slightly misleading. In some portions, the authors say \"we propose to identify stolen copies by showing that they were trained on the same dataset\". In other places, they imply that the adversary used a \"subset\" of that knowledge. It is important to both be consistent in this definition, and to quantify it: nowhere in the paper is it mentioned how much overlap is acceptable as \"sheer chance/possible via domain knowledge\", and how much is \"definitely stealing\".\n\n- Key implementation details, such as how the datasets were split to be used by the target and adversary to train their models are omitted. \n\n- CIFAR-10 and CIFAR-100 are highly related datasets (same images). Claims made in the paper would hold more value if they can show similar trends on some other datasets. The authors could try some high-dimensional datasets such as tiny-Imagenet, or even something as simple as MNIST.\n\n- In several places in the paper, authors claim that watermarking models does not work in the sense that trying to copy a model does not copy the watermarks, since there is a difference in distributions. This is not true: recent works have shown honeypot attacks that transfer watermark behavior even when someone tries to copy a model using normal data. Please refer to Section 3.2.2 of [this work](https://arxiv.org/pdf/2009.12153.pdf) \n\n- When using sample points to assert model stealing, does the party arguing for their case release them all together, or one by one. One may argue that the sequential release of sample points could potentially be games (a malicious party could easily look at the target model and accuse them wrongfully). What are the countermeasures against such a scenario? It seems the final judgment can only be made by a third party by the other two parties revealing large subsets of their training sets and algorithms. This might not be possible in real-world scenarios, where datasets used to train models are bound by certain privacy-protection laws (GDPR, etc).\n\n- Section 2 \"V suspects theft\": what are the criteria here? Is it based on similarity in model performance, or pure suspicion? Since this section sets up the threat model, it might be beneficial to be as exact as possible.\n   (a) \"may gain access to a subset of\". As asked above, how big of a subset is a problem? Since the subset is of Kv and not Sv, it may be possible that they are not overlapping at all, or barely overlap. Would it still be a problem? \n   (b) Definition 1: \"prove that some knowledge\" since this is a definition, it should not be hand-wavy. In my opinion, the \"some\" here should be quantified perhaps by a parameter that captures the overlap in the two datasets and integrates them in the definition. \n   (c) Definition 2: Please rewrite the last point as a conditional assignment. It is hard to read in its current form\n\n- Section 3.3, 'Failure if Membership Inference' refers to two labels: b=1 when the sample is from S, or b=0 when the sample is from D. However, S is a subset of D and thus the first case will always be true. Assuming you mean 'sample is from S - D', please fix this.\n\n- Insufficient literature review. There are many works in the field that deal with the same problem and similar approaches, even though they do not define them as \"dataset inference\". Please search for some works on 'property inference': the current problem can be posed as differentiating between the dataset suspected to be used, versus some other dataset.\n\n- Although the white-box method looks at finding the 'closest' data points, the black-box method samples random directions and takes steps. Assuming this is intended to be a proxy of the white-box setting, it should take the form of sampling random initial directions and then, over all the samples, picking the closest directions.  Also, the white-box setting may give significantly different results if either of the models were trained with an adversarial robustness objective. It would be interesting to see how the attack performs in that case.\n\n- Section 4.2 claims that membership inference attacks do not take confidence into account and that this is a key difference from the proposed method in this work. This is not true: ROC curves for such attacks can be analyzed to set rejection regions for such claims. \n \n- Section 4.2, last line: \"publicly available data that is not used for training of Fv\" Is there a specific reason to use data that was not used to train Fv?\n\n- Page 8, 'White-box access is not essential to DI' claims the Blind-Walk approach is 'non-targeted'. This can also be made true for the white-box setting by adjusting the minimization objective to be non-targeted, instead of making it targeted and targeting the top-k classes.\n\nPlease address and clarify the cons above ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}