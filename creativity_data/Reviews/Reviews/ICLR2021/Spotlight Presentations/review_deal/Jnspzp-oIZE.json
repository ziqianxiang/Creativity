{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "\nThis paper addresses a crucial problem with graph convolutions on meshes. \nThe authors identify the issues related to existing networks and devise a sensible approach.\nThe work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations.\nThe reviewers unanimously agree on the both the importance of the problem and the impact the proposed work could have. \n\nSuggestions for next version:\n- The paper is unreadable without the appendix and somehow it would be better to make it self-contained\n- Additional references as suggested in the reviews.\n- Expanded experiments as suggested by R4, will also improve reader's confidence in the method. \n\nI would recommend acceptance. I would request the authors to release a sufficiently documented and easy to use implementation. This not only allows readers to build on this work but also increase the overall impact of this method."
    },
    "Reviews": [
        {
            "title": " Authors address a crucial problem with graph convolutions on meshes: the isotropy of the kernel that clearly limits the deformation of the mesh. If anisotropic kernels are  introduced than convolutions have to be equivariant with respect to the gauge and this is what this paper achieves with parallel transport.",
            "review": "Graph convolution has been defined to be permutation equivariant to the neighborhood vertices. If one were to define an anisotropic kernel then a reference edge would have to be defined corresponding to edge angle 0. Figure 1 is very illuminating.\n\nEquivariance with respect to this reference can be achieved only with a special mechanism. The authors here proposed message passing via edge transporters.\nThe crux of the approach is in equation (2) and in particular in $\\rho(g_{p \\rightarrow q} \\in [0,2\\pi)$. What happens is that the feature vectors of the adjacent vertices have to be transported to the center node, namely a transformation from the local coordinates in p to the local coordinates in q correcting, thus, the underlying gauge difference. This step includes also an alignment of the tangent planes. I strongly believe that appendix A belongs to the main paper.\n\nThe second crucial point is that equivariance imposes a linear constraint on the kernels $K_{Self}$ and $K_{Neigh}$. This allows a kernel to be written as a linear combination of 20 kernels for $K_{Neigh}$ and $K_{Self}$ resulting in 24 only unknowns for one layer.\n\nThe related work section is comprehensive. \n\nThe experiment on shape correspondence achieves performance comparable to the state of the art spiralNet++. The paper would benefit from other experiments on manifold like performing mesh convolutions for human or object reconstruction from images. \nFaust is pretty standard in the GML community but it is an easy task in terms of feature learning.\n\nThe paper contribution is elegant and significant: Gauge equivariance  is a necessity if you want an anisotropic diffusion.\n\nThe paper is unreadable without the appendix and somehow it would be better to make it self-contained and move the experiments in the appendix.\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea presented in an unnecessarily complex fashion.",
            "review": "This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The result is a Mesh-CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels. \n\nSTRENGTHS\n\n- The problem tackled here is very important and well motivated. The authors identify the issues related to existing networks and devise a sensible approach.\n- The approach is detailed and carefully patches the problems in mesh convolutions.\n- Introducing non-linearity to such networks is far from being trivial. I thank the authors for conveying an analysis on this front. \n\nWEAKNESSES\n\n- I believe that this paper (as well as many other prior works) are missing an important connection to the rich literature of 3D vision. From Ajmal Mian's work to Spin images, from SHOT to the recent deeply learned PPF-FoldNet (and point pair features thereon), there are just too many works that compute local reference frames to canonically orient the local geometry. Many of these works compute some form of a point cloud normal (thus a tangent plane) and choose a direction orthogonal to it (hence fix a gauge). Traditionally, this is known as 'local reference frame' and have set an important milestone in 3D descriptors, learned or not. Simply changing the terminology and rephrasing the established results (e.g. connection of local frames via a rotation) through the lense of Riemannian geometry should not distinguish this paper. I like this principled explanation (only a personal preference) but the ties to all those works should be made concrete. For example, recently, Zhao et al. have used local reference frames on meshes and point clouds to design equivariant point cloud networks: \n* Zhao, Y., Birdal, T., Lenssen, J. E., Menegatti, E., Guibas, L., & Tombari, F. (2020). Quaternion Equivariant Capsule Networks for 3D Point Clouds. European Conference on Computer Vision (ECCV)\n\n- It would then be nice to mention clearly that one would like to omit certain local reference frame choices and directly convolve as there is an ambiguity in the tangent plane.\n\n- There are now many works which can exploit Riemannian geometry to craft convolution operators. Technically, there are subtle differences which makes the contributions of the paper less clearer. I would strongly suggest that the paper is revised such that the contributions are stated very clearly. Otherwise, it is hard to figure out which parts already existed and which are novel.\n\n- I consider Sec. 6.1 to be a synthetic dataset because changing the underlying lattice for MNIST does not have practical use. And the results presented in Tab. 2 have some interesting accuracies such as 1.40. Why is that so low? Can we just fix this by picking a naive baseline: Choose an LRF (see above, use SHOT's frame for instance), canonicalize the points locally and apply convolution. How would this simple approach perform? I think some similar idea is already mentioned in:\n* Yang, Z., Litany, O., Birdal, T., Sridhar, S., & Guibas, L. (2020). Continuous Geodesic Convolutions for Learning on 3D Shapes. arXiv preprint arXiv:2002.02506.\n\n- Would be possible to briefly summarize Mesh-CNN? This paper gives an improvement over that so it could be beneficial to see (even in a supplementary) how it is improved. Or is Mesh-CNN is used just to refer to the category of works? \n\n- There is a form of 'discretization' of the parallel transport going on. Can't we compute the vector heat to do a better and faster discretization:\n* Sharp, Nicholas, Yousuf Soliman, and Keenan Crane. \"The vector heat method.\" ACM Transactions on Graphics (TOG) 38.3 (2019): 1-19.\nIn general it would be great to have an understanding of the proposed transport operator. \n\n- Some more focused explanation of the representation theory could be great.\n\nMy recommendation is positive because the community needs principled ways of convolution on non-Euclidean domains, and this paper seems to make an incremental contribution towards that direction. However, lack of thorough evaluations, the missing links to the literature and the rather inaccessible presentation of the material should definitely be improved.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel and interesting method with weak experiments",
            "review": "The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. It achieves that by parallel transporting features along edges and spanning a space of gauge equivariant kernels. Further, a DFT-based non-linearity is proposed, which preserves the equivariance in the limit of sampling density. The method is evaluated on an MNIST toy experiment and the Faust shape correspondence task.\n\nStrengths:\n- The method is very elegant and novel and seems to be one of the most sophisticated mesh GNN operator so far.\n- At the same time it looks like that the operator stays reasonably efficient, still keeping linear time complexity in number of edges. I would welcome, though, if computational efficiency would be analyzed in the work.\n- It bridges the more theoretical equivariant convolutions with graph neural networks for mesh processing, which are more commonly used in practice.\n- An equivariant non-linearity based on the discrete Fourier transform is presented.\n- The work seems to be technically and mathematically sound.\n- The paper is well written.\n- The figures complement the text well and are helpful for understanding.\n\nWeaknesses:\n- Neither the MNIST nor the FAUST experiment verify the gauge equivariance. I think a toy experiment verifying it would be necessary, especially since equivariance of the non-linearity is only approximated (maybe by showing appropriate feature histograms after the non-linearity for changing reference points).\n- In addition to a missing verification experiment, it is also hard to follow the theoretical reasoning that the whole approach is equivariant. The line of reasoning needs to be gathered from the main text, appendix, and referenced related work. I think with more concrete theorems, clarity could be improved. \n- The Faust shape correspondence task is not sufficient to evaluate a novel mesh operator. From personal experience I know that at least SpiralNet++ (anisotropic, intrinsic, fixed topology) and SplineCNN (anisotropic, extrinsic, arbitrary/varying topology) can be tuned to reach near perfect accuracy on this task. Such small differences in performance can come down to architecture design and might not come from more principled differences in the method. Therefore, I would highly welcome one additional comparison on a different mesh task.\n- Since the experiments are scarse, I wonder if the operator is hard to apply to larger tasks or if the kernel restrictions weaken the approach on other tasks.\n\n\nQuestions and comments:\n- As usual in this area, the kernels are restricted in trade for equivariance. I wonder how expressive the kernels still are. Is there a way to compare the expressiveness (visually or quantitatively) to lets say an MLP kernel function mapping polar or Cartesian coordinates to a full C x C matrix? Is it the/a minimal restriction that ensures equivariance or is it more restrictive?\n- Would it be possible to come up with a space of two-dimensional kernels K_neigh (dependent on the full polar coordinates, including r) while keeping the gauge equivariance?\n- How efficient is the non-linearity based on DFT? I would be interested in a execution time breakdown for the whole method, showing the bottlenecks.\n- In the MNIST experiment, pooling is applied (appendix D.1). How does this interfere with the equivariance property? \n\nMinor/Typo:\n- Appendix page 13, equation above eq 8, parenthesis missing\n\nOverall, I like the paper, it is nice to read and the method is interesting, building on mathematically elegant concepts.  I actually would like to give an accept score. However, in my opinion there are experiments missing: (1, crucial) verifying the gauge equivariance and (2) an additional comparison on a more complex task. Without those experiments (especially 1), I see the paper in a borderline state. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Elegant way to incorporate gauge symmetry on meshes; RegularNonlinearity addresses an important issue; prefer more interesting experiments",
            "review": "Although a mesh embedded in 3D space may be treated as a graph, a graph convolution network uses the same weights for each neighbor and is thus permutation invariant, which is the incorrect inductive bias for a mesh: the neighbors of a node are spatially related and may not be arbitrarily permuted.  CNNs, GCNs, and G-CNNs demonstrate the value of a weight sharing scheme which correctly reflects the symmetry of the underlying space of the data.  The authors argue convincingly that for a signal on a mesh, the appropriate bias is symmetry to local change-of-gauge.  In short, the weights should depend on the relative orientation of a node’s neighbors.  They design a network GEM-CNN which is equivariant to change of gauge.  The design is similar to a GCN but incorporates parallel transport to account for underlying geometry and uses kernels similar to those of $SO(2)$-equivariant $E(2)$-CNN (Weiler & Cesa 2019).  The experiments show the network is able to adapt to different mesh geometries and obtain very high accuracy in the shape correspondence task. \n\nI suggest accepting this paper.  The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets.  Though I would prefer more interesting experiments, they are sufficient to validate the design. \n\n**Specific Strength, Weaknesses, Points, and Questions:**\n- The symmetry of a graph convolutional network can be broken by including spatial coordinates as features.  If $x_p$ and $x_q$ are inputs, then a function $F(f_q,x_q,x_p)$ can process data in an orientation aware way even though $F$ is isotropic.  How would this compare to the proposed method in the paper?\n-Page 2: One strength of the paper is the argument for why gauge symmetry is necessary in the first place.  Features defined on a mesh may be vector-valued and defined with respect to a frame of reference at each point on the mesh.  However, since the geometry is curved, there is no consistent way to assign a frame of reference.  Thus an arbitrary choice must be made when recording data.  Since this choice is arbitrary, the output of the network should clearly be independent of it.  Gauge equivariance encodes this symmetry.   \n-The argument for encoding the geometry of the mesh is reasonable.  But then why only parameterize $K$ by $\\theta_q$; why not also include the distance $r_q$?  \n-Page 6: **RegularNonlinearity is an important contribution** of the paper.  The authors are correct that non-linearities have been a bottleneck for using equivariant neural networks with representations other than the regular representation.  Transforming to sample space (i.e. embedding in the regular representation) to apply a pointwise non-linearity and then transforming back is a nice idea for addressing this.  Furthermore, Theorem E.1 provides a nice theoretical analysis of the asymptotic error.  It would be nice to include practical non-asymptotic error bounds as well. \n-The experiments are okay, but are a weaker part of the paper.  The argument for a geometry-aware NN is that it processes signals on the geometry better.  It is not clear that embedding MNIST on a mesh illustrates data which is best understood in terms of the underlying mesh.  Far better would be to develop a signal natively on the mesh, for example by solving a PDE directly on the mesh.  Arguably the FAUST shape correspondence data addresses this issue better since the signal is inherently linked to the geometry.  \n-Both experiments also treat only scalar data of type $\\rho_0$.  While it is plausible and reasonable to model such data using vector features of type $\\rho_i$ in the hidden layers, the argument for the necessity of gauge equivariance would be even stronger if the input and/or output signal was itself vector valued, for example a velocity or gradient on the mesh. \n-Changing roughness to the embedded MNIST distorts the signal in the geometry of the manifold (changing distances and angles), so why should we expect generalization across different roughnesses? \n-Page 7: I don’t understand the argument for the value of symmetry breaking.  The gauge equivariant network can be orientation aware by encoding $\\rho_1$ features.  Why is it desirable to be dependent on arbitrary gauge choices?  What breaks down about the original argument for incorporating equivariance in this case?  Is it possible the improvement is due to a different trade off between bias and expressivity at lower layers?\n-Page 7, Para 4: The paper argues other high performing methods in shape correspondence use complicated pipelines.  It is not clear to me (probably from lack of familiarity) which is most complicated.  It seems both this method and the other method contain different complexities and subtleties.        \n\n**Minor Points:**\n- All of the citations in the paper use \\citet, but it would be more readable to use \\citep.\n- Page 3: Should not $\\rho(g_{q\\to p})$ be invertible in $\\mathbb{R}^{c_{in} \\times c_{in}}$?\n- Page 5: The notation $k \\rho_l$ is non-standard, compared to $\\rho_l^k$, but it is more readable.\n- Page 13, $K_{neigh} \\theta_{pq}-g)$ is missing a parenthesis\n- Page 13, “which is true for any features, if”.  if could be if and only if, correct? \n\n**Update From Author Reply**\nI am grateful for the author's replies, edits, and additional evaluation, all performed within limited time. This helps me feel confident in my accept (7) recommendation. My reason for not giving a higher score remains the limited experiments (which is likely not something to be addressed in two weeks), but even so I think the work is quite worthy of being accepted. The methods are a significant contribution and the experiments are sufficient to demonstrate they work.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}