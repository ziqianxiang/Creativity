{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents a density ratio estimation approach to make the early decision for sequential data. The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7). However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating. Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward. Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle). Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm---making early predictions on sequential data---because the method requires \"plotting the speed-accuracy tradeoff curve on the test dataset.\" This response implies that it at least requires a withheld dataset. Although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria.\n\nConsidering all these facts--high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation.\n"
    },
    "Reviews": [
        {
            "title": "SPRT-TANDEM additional review ",
            "review": "# Summary\nThis work introduces SPRT-TANDEM an algorithm to train a sequential probability ratio test (SPRT) as a neural network. This network is then used to discriminate between two hypotheses as fast as possible (seeing the smallest number of observations in a sequence) while maintaining a certain level of accuracy. The main contribution of this work is to enable Wald's SPRT without actual knowledge of the ratio, learning a neural network to model it.   \n\n# Major comments\n## Pros:\nThe paper does a good job of introducing the problem statement, that is the \"fast\" classification of sequential data. The algorithm introduced is well motivated and bridges the gap between \"classic statistical\" methods and machine learning approaches for sample-efficient time series classification. The experimental, results though not outstanding, show that SPRT-TANDEM outperforms other deep learning methods.  These experiments are insightful by the fact that they compare the performance of the different methods for different mean hitting time. Overall the paper is pleasant to read and introduce a new method that could be helpful for some practitioners.\n\n## Cons:\n1) The related work is quite superficial (even taking into account app B). In particular, I would have liked a deeper comparison with LSTM-s/m and EARLIEST, discussing the drawback/advantages of these methods with respect to SPRT-TANDEM.\n2) In the proof of 4, just before eq 70 you say: \"Let us assume that the process {x(s)}ts=1 is i.i.d., namely -> eq 70\". This seems wrong to me. The assumption you're making there is that the process has independent component conditionally to the class y (e.g for t = 2, the Bayesian network: x_1 <- y -> x_2). This is still a reasonable hypothesis however this is not equivalent to simply assuming the process is iid (which then would mean for t = 2, the Bayesian network: x_1 -> y <- x_2 and would not make eq 70 correct).\n3) The three tasks on which you test the models seem to be quite well solved after a few samples on average for all models. I think it would be worth testing the models on tasks that require more samples for reaching good performance and maybe where the temporality required (hyperparameter N) is larger.\n4) It is not very clear to me what are the respective roles of LLLR and MCEL. I do not understand why both are useful, I would have thought that CE in itself would be sufficient. The ablation study you did is interesting but I would have liked to get further insights about what is happening there.\n5) I believe that testing your method on a toy problem for which the correct ratio is known would be insightful about the \"optimality\" of your method.\n\n# Minor comments\n- Page 2 \"As an .. orDEr\"\n- Loss written with capital everywhere.\n- How to choose N: You say that training the features extractor is faster than the integrator's however it is not clear to me how you can train these two parts independently from each other.\n- For comparison on NMNIST it could be interesting to see the performance of a simple classifier on the 19th image alone.\n- You're talking about Optuna for hyperparameter optimization, this is unknown to me. A word about how it is working could be nice.\n- Why are the number of trials different?\n- The purpose of SPRT-TANDEM is to be as fast as possible, it could be interesting to clearly state somewhere how comparable are the different methods in term of computing times even if they are very close to each other.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very well written paper proposing an algorithm minimizing the divergence between estimated and true Log-Likelihood Ratios of SPRT and making it thereby Bayes optimal for various real-world applications. ",
            "review": "The paper proposes a novel SPRT-TANDEM algorithm minimizing the divergence between estimated and true Log-Likelihood Ratios of SPRT and making it thereby Bayes optimal for various real-world applications. The paper is very well written, clear and scientifically sound and provides  extensive contributions, e.g. a database in addition to the algorithm. Performance of the algorithm is demonstrated via three experiments.  \n\nPrevious research is given sufficient credit. The only thing I would still like to see more is the discussion at the conclusions. Why does this seemingly simple modification to the existing SPRT method provide so superior performance.\n\nThe appendices are referred a lot in the text but they are missing from the paper?\n\nA very minor comment: The following sentence is a bit vaguely written:\nLong short-term memory (LSTM)-s/LSTM-m impose monotonicity on classification ...\nI guess it should be : Long short-term memory (LSTM) variants LSTM-S and LSTM-M impose monotonicity on classification ...\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An exceptionally documented and motivated piece of work!",
            "review": "SUMMARY:\nThis work describes a new algorithm, SPRT-TANDEM, for classifying sequential data as early as possible. It builds upon several previous works (SPRT, KLIEP, and deep neural networks) to propose a well-engineered and well-motivated solution for the considered problem.\n\nSTRENGTHS:\n- This work is exceptionally documented, on all accounts: related work is multidisciplinary, broad and thorough; all losses and algorithms are derived from first principles; experiments are varied and include every last details regarding their setups, evaluation metrics or outcomes.\n- Albeit only briefly mentioned in the main, the method has strong theoretical foundations, as documented in Appendix A. \n- Experiments show stronger benchmark results than the considered baselines (LSTM-m/s, EARLIEST and 3DResNet), on a quite diverse set of experiments (images, videos).\n- Although the classification of sequential data is not among the most popular topics, I believe this contribution to be significant for the field. \n\nWEAKNESSES:\n- I believe the 8-page limit of ICLR does make this work justice, given the extensive documentation that comes along in the supplementary materials. The short format of the main paper makes it difficult at places to fully follow or appreciate the contributions presented in this work. (Should this paper be rejected, I would recommend it to be submitted to JMLR, which format is certainly a better fit.)",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper involves a certain interesting property connecting a flexible neural network function-approximator to conventional SPRT setting. ",
            "review": "The authors propose how to use the neural networks to estimate the posteriors for each label y for the log likelihood ratio (LLR) estimation. The LLR estimation is performed using the multivariate inputs from the windows of time series, and it is used for the SPRT criterion without conventional iid assumption. \n\nThe paper contains an interesting idea of using the neural networks for the prediction of likelihoods and accumulating the information for the conventional sequantial probability ratio test (SPRT), which is well known for explaining the speed-accuracy tradeoff for decision making. The experimental results using various datasets show the relevance of the algorithm in terms of improving the speed-accuaracy tradeoff. \n\nThe authors presented a reasonable combination of two different objective functions: LLLR and L_multiplet. Though the authors did not mention explicitly, one is the objective for LLR which is ill-posed because pairs of high-biased neural network outputs can result in a small LLR by preserving only the ratio of the outputs correct. The other is the posterior objective (L_multiplet) which will alleviate the ill-posedness of the first objective by making the outputs of neural networks as close as possible to the correct one though they do not use those posteriors once it can estimate the LLR correctly.\n\nMy question about this paper is the learning procedure. For LLLR training, there is no need for the sync of x^(t) for different ys. I don’t see the explanation about choosing the index t in LLLR.\n\nIn the paragraph below Eq. (6), I don't understand what it means by the KL divergence between two ratios.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Elegant combination of SPRT and density ratio estimation, though optimality claims are overstated",
            "review": "[EDITED AFTER DISCUSSION: My concerns are largely addressed and the paper is now stronger. I very much hope to see it at the conference, and have updated my review and rating accordingly.]\n\nThe paper proposes a new algorithm for early classification of sequential data, exploiting approaches to density ratio estimation to enable applying a sequential probability ratio test-type algorithm on perceptually rich data with no explicit likelihood. The algorithm is trained using a novel density ratio estimation loss alongside more standard cross-entropy, and shows strong performance on a number of provided benchmarks, including on a new variant of sequential MNIST. The paper claims ability to control speed-accuracy tradeoff in early classification, and applying the Wald SPRT on arbitrary sequential data. \n\nI enjoyed reading this paper: it combines two ideas (sequential likelihood ratio testing and density ratio estimation) in a way that appears obvious after the fact, but as often with these sort of post-hoc obvious ideas, is novel to my knowledge, and elegant. Trying to get something like the SPRT working beyond pairs of simple hypotheses has been under investigation for over half a century, and this paper is a worthwhile attempt. The empirical results look pretty good as well. \n\nAt the same time, I think the paper does overstate the benefit of the theoretical connection to the classical SPRT and previous non-i.i.d. extensions, and doesn't engage sufficiently with the challenges of stopping rules. I discuss these issues in more detail next, and conclude with some minor points about presentation, background work, and analysis. \n\n# Connections to the classical SPRT # \nThe paper claims to extend Wald's SPRT to non-i.i.d. data, and in particular its optimality properties. This includes claims in section 1 (on approaching Bayes-optimality and extending the Wald SPRT to arbitrary sequential data), at the beginning of section 4 (on how the LLLR enables performing the SPRT, which is provably optimal), and in section 5 (on approaching if not reaching Bayes-optimality). As far as I can tell, the paper does not enable performing the Wald SPRT on arbitrary data, and does not provide a provably optimal sequential test. The claim that it approaches optimality in any formal sense is likewise not supported as far as I can tell. Broadly, a sequential test consists of an update rule and a decision rule -- for the SPRT the update rule and decision rule are both optimal. For most extensions (non-i.i.d., multi-hypothesis, deadlines, etc), the update still follows Bayes' rule, and the optimal decision rule can only be found numerically (if at all), so some heuristic is given. This heuristic is often a fixed threshold, with asymptotic optimality guarantees). SPRT-TANDEM seems to be in the family of such extensions: it still applies Bayes' rule sequentially, and the stopping is given based on a fixed threshold. Thus, its optimality is asymptotic at best, and stronger claims are not supported. Furthermore, the paper doesn't make it clear whether the standard conditions for asymptotic optimality apply to the SPRT-TANDEM either: in my rough understanding, the standard asymptotic result is as risk goes to 0 (or equivalently, the LLR goes to infinity, and the threshold goes to infinity). I'm not sure that we know the SPRT-TANDEM LLR to grow in this way, and empirically, it seems like the LLR saturates to some fixed value, especially with high-order N, which means high threshold values are not achievable and risk cannot go to 0. We should also expect the SPRT-TANDEM to depart further from optimality as the model approaches the end of the video (since the optimal thing to do there is to gradually collapse the decision boundary, as the appendix reminds us). I recommend moderating these claims regarding optimality, and / or strengthening the results if possible. \n\n# Stopping rules # \nThe paper does not provide guidance on stopping rules, which limits practical use, and does not report on the thresholds used to generate the speed-accuracy tradeoff figures. Presumably, the simplest thing is to set the threshold to the desired accuracy (which I think will do the right thing in the no-overshoot case?). Does this work for SPRT-TANDEM to achieve a given accuracy? If not, is there another heuristic that applies? I recommend addressing this question in more detail. Relatedly, the paper criticizes Mori et al. 2018 and Hartvigsen et al. 2020 for using a separate objective for determining stopping and accuracy, but in fact SPRT-TANDEM would likewise need some dynamic programming or RL solver to have an optimal stopping policy, similarly to that prior work. I recommend providing explicit guidance about stopping rules, and moderating the claims relative to prior work. Solving for an optimal stopping policy would also strengthen the paper. \n\n# Presentation issues #\nThe paper tries to cram a lot into the short ICLR format, supported by an extensive appendix. I appreciate the inclusion of classical SPRT results in the appendix, which may be unfamiliar to the ICLR audience. At the same time, the main text does not provide much intuition about the novel LLLR loss, which is given very little explanation considering it is presented as one of the paper's major contributions. The relationships and improvements relative to KLIEP are presented too tersely, and a reader not familiar with that precise method will not know what to make of them. The paper would do better to provide more exposition there, perhaps in favor of moving the results tables to the appendix (since they show the same information as figure 3 as far as I can tell).  \nIn addition, the SAT curves are too busy, small, and hard to read. For the main document, I would recommend increasing font and symbol sizes, and presenting fewer orders of SPRT-TANDEM models (e.g. just order-1 and best-order), and fewer hitting times per model (e.g. there is no need to present the accuracy at every one of the last 10 frames if the accuracies are all the same there). Finally, the LLR trajectory figures can use partial transparency to make the individual traces easier to see. \n\n# Additional/minor points #\n- **Background work**. I appreciated the fairly detailed review of past work related to the SPRT. A few notable missing pieces related to neuroscience are work predating Kira et al. 2015 in applying the SPRT to neural data (e.g. Gold & Shadlen 2002) and to human decision making more broadly (e.g. Stone, 1960; Edwards, 1965, Ashby 1983, and others). Missing work related to practical applications of the SPRT includes Johari et al. 2017, Ju et al. 2019, and others from the domain of internet experimentation. None of these are critical omissions, I only bring them up considering the already-broad review. \n- **Statistical analysis**.  Given that the paper has a clear hypothesis (that SPRT-TANDEM outperforms competitors), it seems more sensible to perform repeated measures regression with planned contrasts rather than post-hoc testing for significance. This is a minor issue. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}