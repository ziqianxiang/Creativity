{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The reviewers have supported the acceptance of this paper (R3 and R5 were particularly excited) so I recommend to accept this paper."
    },
    "Reviews": [
        {
            "title": "Compelling approach to bottom-up program synthesis",
            "review": "# Summary\n\nThe paper proposes an approach to program synthesis which is done in a bottom-up fashion.\nIn order to guide the search more effectively the bottom-up search algorithm is accompanied with a model predicting whether particular sub-expressions of a program are promising directions.\nAs the system for program synthesis needs to be real-time capable, the proposed approach heavily relies on property signatures as introduced in (Odena & Sutton, 2020) for featurizing program inputs.\nThe here proposed system called BUSTLE is shown to perform favorably to a set of baselines including state-of-the-art approaches to program synthesis.\n\n# Resons for Score\nGenerally speaking, I really enjoyed the reading of the paper. The paper is well structured and written as well as easy to follow.\nEverything is explained in sufficient detail, i.e., thorough but concise.\nThe contribution of the paper is significant and the approach is technically sound.\n\n# Pros\n\nAlthough the methods might not be feasible to be used in practice yet, it performs reasonably well with only a few examples for describing the desired behavior of the programs to be synthesized.\nHowever, the programs that are synthesized are still quite smallish. It would be interesting if the authors could also highlight some future directions in order to make a step towards more complex scenarios.\n\nThe paper provides compelling experiments which are set up in a thoughtful and rational manner rather than throwing various methods on some benchmarks.\nThe experiments are conducted in a very systematic way, facilitating insights into the performance of the proposed method as well as how it compares to the baselines.\nHowever, a taxonomy of problems (e.g. how to quantify problem severity etc.) would maybe help to better spot differences in performance.\n\nThe method itself is quite simple. The authors almost seem to apologize or vindicate. I rather consider it an advantage.\n\nThe proposed approach is well placed in the context of the existing literature.\n\n# Cons\n\nSome information about the experiment setup is missing: Are all benchmark tasks of one set (either on the proposed set or SyGuS) run in a row?\nIs the order kept consistent? What hardware is used for conducting the experiments?\n\n# Questions during rebuttal period\n\n- Out of curiosity: If the order is the same for all the benchmarks for all methods, it seems to be the case the method being fastest to find a solution for a benchmark task varies from task to task,\nand the here proposed method does not always seem to be the fastest for any instance. Given the demanding real-time setting, do you think an algorithm selection approach would work here, choosing a method\non a per task basis?\n\n\n\n# Typos\n- p.3: \"during the search, [...]\" => During the search\n- p.6: \"fixed length representation\" => fixed-length representation (this is occurring at least twice)\n- p.8: \"This provides further evidence the our model [...]\" => This provides further evidence that our model [...] (occurs also in the appendix)\n- p.8 last but one paragraph in Section 5: \"[...], while like BUSTLE, uses values produced by intermediate programs [...]\" => The comma after BUSTLE is kind of irritating. Please, consider removing it or adding another comma before \"like\".\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A well written and evaluated paper combining synthesis techniques with machine learning.",
            "review": "The paper proposes to combine bottom-up program synthesis from input/output examples with a machine learning model. The machine learning model determines for each candidate intermediate value (coming bottom-up from the inputs) if it may be considered at the next round of candidate expression generation or it will be deferred for at least K rounds (with K up to 5 in the evaluation).\n\nPros:\n\n- The paper is evaluated on a good range of string manipulating programs. The presentation and the evaluation is showing the advantages of the reweighting for the number of solved programs.\n\n- The advantages of the method are not only in the number of candidates explored, but in actual wall-time. This is a relatively rare result - many prior synthesis with machine learning tasks completely actual running time.\n\n- The paper and its implementation address the engineering side of the work - batching the requests for effective machine learning and interestingly, the results are good even when the machine learning model was trained on random data.\n\nCons:\n\n- It makes it difficult to compare based on the numbers, but it looks like the result is not competitive with existing solvers for the CyGuS PBE competitions. The paper should mention where it stands here.\n\n- The actual contribution of the work is mostly in the implementation and combining known techniques.\n\nIn terms of writing, there are some improvements that are possible:\n\n- The algorithm description is not self-contained. It is not completely clear what ExtractConstants does. The inputs I and outputs O should be vectors, but they are used as sets. The algorithm actually is unclear here. Does the set E[1] include the entire input vector for input examples as an element and each constant as a vector of the constants with this length (In this case it should be E[1] = {I} \\cup C)?\n- There is also no intuition for what concrete property signatures make sense.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of BUSTLE - a program synthesis technique using program signatures (fusing formal methods with machine learning)",
            "review": "Note about NeurIPS ’20 version: \n\nI was on the NeurIPS ’20 program committee and I was assigned an earlier version of this paper. As such, I’m deeply familiar with it. In its NeurIPS form, I felt it wasn’t ready for tier-1 publication. However, my concerns were principally around the lack of experimental results. I strongly supported of the ideas presented in the paper, but I fought to ensure it wasn’t accepted to NeurIPS because I felt it would have been a disservice to both NeurIPS and the authors, resulting in a rather mediocre paper that could have been exceptional if the proper experiments were run. I attempted to make this clear in my review and encouraged the authors to address this weakness.\n\nIt appears the authors have addressed my primary concern. This version of the paper resolves my critical reservation of weak empirical results – the results, I believe, are now satisfactory of tier-1 publication, which include four different synthesis systems and over 100 different synthesized programs of two different domains. As such, I now support its acceptance at ICLR.\n\nSummary:\n\nThis paper appears to be the first work to use program signatures (ICLR ’20, Odena & Sutton) for program synthesis. The high-level concepts the authors present, as I understand them, is that by using program signatures for program synthesis, they can more closely replicate the process of program synthesis the way programmers develop programs. That is, by breaking one large program into many small sub-programs. Once enough of these small sub-programs have been generated, they can be composed together to solve the larger program – the actual goal.\n\nAssuming this hypothesis holds, the end result might be that such an approach would result in a program synthesizer that could generate both (i) more correct programs, (ii) faster than prior systems. For their experimental evaluation, this hypothesis seems to hold and BUSTLE does in fact generate more correct program, more quickly than prior state-of-the-art. The authors compare BUSTLE to three other variant systems: a baseline (less sophisticated BUSTLE system), RobustFill (ICML 2017), and DeepCoder (ICLR 2017). RobustFill and DeepCoder have demonstrated state-of-the-art performance, historically, so I believe these comparisons are sufficient for this paper’s acceptance.\n\nOverall, I think the paper provides a truly novel approach to program synthesis with its fusion of program signatures. I am admittedly biased in favor of program signatures, because I believe the future of machine programming / software 2.0 / neural programming / program synthesis with both stochastic and deterministic approaches (whatever we want to call it) is going to be heavily reliant on our ability to lift concepts from code (the “what”) which is notably more challenging than lifting the implementation (the “how”). This is because the “what” tends to not necessarily be obvious from the code, whereas the “how” almost always is – it’s the implementation. With that in mind, this paper presents what I believe is the first demonstrable evidence that program signatures can be used in this fashion. I suspect this is just the beginning of exploration with program signatures – I expect a flurry of follow-up research to emerge that uses them.\n\nWhen taken holistically, I strongly support accepting this paper, but I do have some minor nits I’d like the authors to address.\n\nMinor suggestions:\n\n1. There appears to be no system diagram of BUSTLE. While an expert in the space of program synthesis and property signatures can likely understand what is going on, non-experts I think will really struggle without some kind of visual diagram showing how BUSTLE works. It should be relatively easy to add this diagram to the paper and I believe it will make the paper more widely accessible. If only one of my recommendations is addressed by the authors, I would request it be this one.\n\n2. There appears to be multiple locations where the authors seem to deem neural network inference is “too slow” without qualification. I think this is a mistake and is a bit of a turn off and it’s a bit of a confusing one given that BUSTLE uses neural network inference. Yes, I do agree with the authors that inference with large neural networks could make the problem of program synthesis slower, but I don’t believe this is a universal truth. I think it’s proportional to the computational complexity of the neural network. I would request the authors find all such “inference is too slow” cases in the paper and properly qualify them. I suspect this will encourage future work to consider other neural network architectures that may be competitive or even outperform BUSTLE.\n\n3. Can you label the different variants of BUSTLE from something like “Using model and heuristics” to just BUSTLE, “Using model only” to BUSTLE (model only), etc. Right now it’s a bit confusing at first glance on Figure 2 to see which is the full BUSTLE system because there isn’t actually any legend item that is called “BUSTLE”. Should be an easy fix and will likely make the figure easier for the audience to understand.\n\n4. Can you please drop the word “very” from the paper everywhere it appears? I do not have a mathematical representation of what that word means (nor does anybody I think) and, as such, I believe it introduces unnecessary ambiguity and also wastes paper space.\n\n5. I couldn’t tell how the BUSTLE training time was factored into the analysis. Can you find a way to explain that more clearly? I realize that it’s a potential one-time only penalty, but it doesn’t come for free (to my understanding) while some traditional program synthesis systems using formal methods can simply generate programs without any learning overhead. I think this needs to be captured somewhere so people don’t forget about this cost.\n\n6. I got a little confused by the comments about removing restrictions of Concat() in the second paragraph in section 2.2. Can you try to explain that more clearly?\n\n7. Can you provide some intuition on the rationale behind keeping “100 positive and 100 negative values” as explained in the last sentence in section 3.1?\n\n8. Can you double check to ensure all of your acronyms are fully spelled out first? I’m familiar with all of them, but others might not. I don’t think I saw the spelled out versions of DSL, AST, JVM, etc. While these terms are generally widely known in the programming languages community, I’m not sure if the machine learning community is as deeply aware of them. Regardless, it seems to me that it’s usually a good idea to spell out all acronyms first.\n\n\nFuture work:\n\nDo you really think an abstract syntax tree (AST) representation is the right representation for this approach? \n\nI’m not so sure. I recommend taking a look at the Aroma’s simplified parse tree in the paper by FAIR, Berkeley, and Riverside (OOPSLA ’19) and, more comprehensively, MISIM’s context-aware semantics structure from Intel, Georgia Tech, MIT (arxiv). I suspect a next iteration of BUSTLE using either of these structures might result in even better performance than what you’ve currently achieved with an AST representation. But, that’s just a guess. :)\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well written paper but questions about contribution significance. Possible reject.",
            "review": "The paper proposes to add machine learning component to the bottom-up program synthesis algorithm. Machine learning component uses information from property signatures to prioritize the candidate expressions for further expression space exploration. Authors show that their algorithm outperforms other approaches on the benchmark sets.\n\nPositives:\n- The paper presents incremental improvement to the bottom-up program synthesis algorithm.\n- Authors have considered the efficiency issues of adding machine learning model into the bottom-up search and came up with optimizations that result in model cost being lower than efficiency gains.\n- Authors figured out how to train the NN for their task absent actual training dataset.\n- Algorithm outperforms baseline approaches.\n- All sections of work are well presented, understandable, and easy to follow. The paper was a pleasure to read.\n\nConcerns:\n- My concerns can be summarized in short question: are contributions significant enough?\n- In experiments, the algorithm barely outperforms a baseline with heuristics only. This is the case for both authors' benchmark set and SyGuS benchmark set.\n- Using property signatures is not new. The novel part is adding NN to prioritize search based on property signatures. I would say that the idea to use NN for this is not significantly novel. There is some novelty in setting up training dataset as I observed above. \n\nPossible mitigation of concerns:\n- Choose more complicated problem (benchmark) set. This may show where baseline approaches do not work well. Since authors created first benchmark set themselves, I am surprised this was not done already.\n- Another variation would be a useful problem set that is not more complicated, but where for some reason baseline(s) do not work.\n- Provide results for a baseline that uses property signatures without NN. This is a minor mitigation: it is possible that property signatures without NN will not outperform the baseline with heuristics. It still would be interesting to see the comparison.\n\nIn summary: I am not convinced that contribution is significant enough for acceptance.\n\nReproducibility: It would be nice if authors presented actual NN architecture used for ease of reproducibility.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}