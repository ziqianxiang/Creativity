{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors' response. Unfortunately, mn55 did not follow up to express how convinced they are with author's reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful, and the presentation of the theory through concrete examples very compelling."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper analyzes the empirical risk of mixup-style training methods. First, the authors provide an example where mixup training cannot minimize the empirical risk on the original data. Second, the authors provide sufficient conditions for mixup to minimize the empirical risk.\n",
            "main_review": "I like the topic of this paper, but I have several concerns as below.\n\n1. Focus on extremely high \\alpha values\n    1. In the illustration of mixup examples, this paper considers \\alpha=32, 128, 1024, which is not even considered in the original mixup paper (standard one is \\alpha=1, and maybe variant the authors considered is \\alpha=2). My personal feeling is, no one will use such large alpha in real application of mixup; they will just use default value \\alpha=1. I think the authors are setting extremely large \\alpha values to exaggerate the effect they want to show. \n2. Theory results are limited to simple settings\n    1. Proposition 2.5 and Theorem 2.7 holds for only a certain dataset. I guess both are not deserved for proposition & theorem, just some “example”.\n    2. Results on Sec.3.2 are applicable for linear model with Gaussian data\n3. We cannot know whether the assumptions are realistic\n    1. Assumption 2.9 and 3.1 -> How can we know that this assumption holds in real datasets?\n4. Conclusion of Sec.2.5 is unclear\n    1. It seems like the Sec.2.5 should be about analyzing the convergence rate of original empirical risk when we use mixup training. Is Theorem 2.11 sufficient for showing that theoretically? I guess not. Moreover, the “midpoints” discussed here is mixing with \\lambda=0.5, which maps to an extremely large \\alpha value case, which is unrealistic.\n",
            "summary_of_the_review": "This is an interesting paper trying to answer “why” and “when” mixup works. But it is neither a theory paper providing some meaningful theoretical results (that can be applicable for general scenarios), nor an empirical paper suggesting new scheme and providing extensive experimental results. I would say this is a toy theoretical attempt, which is not suitable for ICLR acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides interesting theoretical tools that help us study the relationship between empirical risk minimization/ empirical cross-entropy minimzation and Mixup minimization. The contributions of the paper consist of the following:\n\n1. Constructing a dataset on which Mixup training fails to obtain the empirical risk minimization.\n2. Providing sufficient conditions for minimizing the original risk and estimates of these sufficient condition measures on natural datasets.\n3. An observation that the rate of empirical risk minimization using Mixup and a theorem about why this should happen.\n4. A study on when the Mixup optimial classifier fails to generalize and a demonstration on a toy dataset.\n5. An example for when Mixup training leads to the same classifier, obtained from standard training.\n\nThe main difference with related works is that the paper attempts to understand *why* Mixup works by studying *when* Mixup works with concrete examples.\n\n",
            "main_review": "# Strengths:\n\n* The approach the paper takes in their study is a meaningful one, and the theory and experiments are well motivated.\n\n* The paper goes beyond the observations of related works to provide concrete conditions on which Mixup training can fail, and demonstrate it by constructing synthetic datasets and measures that could be estimated for natural datasets. This could be useful to practitioners as well for when they design their experiments on natural data.\n\n* The paper takes a complementary approach to existing works for understanding the generalization and robustness of the Mixup-optimal classifer.\n\n* Some interesting future work directions are underscored by the paper, such as the rate of convergence of Mixup training (Section 2.5).\n\n# Weaknesses:\n\n* `Mixup at our choice of $\\alpha=1024$`: could you report your experiments for better context? Furthermore, could you explain what the argument is in this paragraph? How does the observation that the test data points exhibit greater angular distance connect with your empirical observation? Do you mean to say that greater angular distance for the test set means less \"collision\" that is discussed in Guo et al. (2019)? I think clarification of this paragraph will improve the paper, because it is at the transition between studying the Mixup optimal classifier and its generalization properties.\n\n* I am curious what kind of applications of your Sufficient Conditions you envision for experiments. Could you elaborate on some interesting directions or limitations as you try to scale to larger and higher-dimensional raw data? \n\n* The Proof Sketch of Lemma 2.3. is unclear to me. Is it correct to say that you \"define\" $h_\\epsilon$ to be the argmin? Do you mean that you \"construct $h_\\epsilon$ and show it is the argmin\" instead?\n\n* The Proof Sketch of Proposition 2.8 is unclear to me. How do we know that the constant measure gives the empirical risk minimization?\n\n* Isn't $\\epsilon$ missing in your definition of $\\xi_{x,\\epsilon,\\lambda}^{i,j}$? \n\n* Not clear what you mean by \"recover\" in Theorem 2.11. Do you mean recover as a linear combination? Could you clarify it in the paper?\n\n* In Section 3.1. Paragraph \"Experiments\" exactly to what does *this* refer to in \"To illustrate this\"? Could you clarify the connection with the previous paragraph?\n\n* Not sure about the summation indices in the proof of Lemma 2.3. In the definition of $J_{mix}$ I think the summation goes only from $j=i+1$ to $k$. How does the summation (and the sign of the summation) change to indices $j=1$ to $i-1$ (and a negative sign) in the last summand in the equation at the end of page 15?  \n\n\n* Do you know of other papers that try to find examples for when the Mixup optimal classifier coincides with the empirical cross-entropy minimizer?\n\n\n## Minor.\n\n* Suggestion for the Introduction. Mixup has been used in contrastive learning settings. You could consider citing some of the works, e.g. [1,2].\n\n* Definition of J_mix on top of page 3. I would suggest to make the dependence on $g$ explicit in the RHS.\n\n* On page 3 you could mention that $A_{x,\\epsilon,\\delta}^{i,j}$ would be used later in the paper.\n\n* Lemma 2.3. Is it that $\\epsilon \\to 0$? You could clarify this.\n\n* Figure 1 and 8: ERM curves do not depend on $\\alpha$? Or am I missing something? Could you clarify this in the captions. What causes the difference between the blue (ERM) curves in Fig. 8(a) and (b)? \n\n* Figure 1's labeling is not clear. Please, enlarge the font.\n\n* Definition 8 is actually Figure 8 in Section D. Maybe a typo in the link? \n\n* The abuse of notation in Proposition 2.2. make it hard to understand the proof. \n\n* In the proof of Theorem 2.11, I am not sure what you mean by $Aw^*=PAw$ is equivalent to $[A, PA][(w^*)^\\top, w^\\top]^\\top$. Could you clarify?\n\n# References\n\n[1] Lee et al. 2021. i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning. In ICLR.\n\n[2] Verma et al. 2021. Towards Domain-Agnostic Contrastive Learning. In ICML.\n",
            "summary_of_the_review": "I recommend 8: accept, good paper. In my opinion, this paper will be useful to the community both for theory and experiments. I have listed a few questions and suggestions in the Main Review. It is possible that I have missed some of the main arguments in the related works, but to me the contributions of the paper are novel and useful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper provides a theoretical framework to enable analyzing Mixup training from a data dependency perspective, aiming to understand how the structure of the training data impacts the effectiveness of Mixup training. The paper first shows that the benefits of Mixup training, in terms of model generalization and adversarial robustness, heavily rely on the properties of the training data, and studies when Mixup training can fail to minimize the original empirical risk. In addition, the paper also theoretical characterizes the margin of a Mixup classifier, justifying why a Mixup classifier’s decision boundary is better than standard training and when such advantage disappear (i.e., in cases of training Mixup with linear models and linearly separable datasets). The paper contributes an interesting perspective for looking into Mixup training, which has been widely adopted by the community.   ",
            "main_review": "The paper provides an interesting  study, from the properties of the training data perspective, why and when Mixup help training better classifier in terms of model generalization and adversarial robustness. The papers show that both the model generalization and adversarial robustness brought by Mixup training heavily depends on the properties of the training data. Such observations have been empirically observed (such as by Guo et al. in (2019) as cited), but this paper provides a more formal theoretical analysis on that. I think this paper contribute an interesting analytical framework and useful insights regarding Mixup training to the community. Nevertheless, I have the following comments to the paper.\n\n1.\tThe paper did construct a case that Mixup failed to minimize the original empirical risk, and conjectured (at the end of section 2.3) that datasets with collinear structures amongst data points may cause Mixup to fail. I wonder if the authors could be more precise and specific on the conditions for such Mixup failure and formally prove that to some extent. \n2.\tFor figure 2, I wonder if the Mixup training errors are on the original training points or on the mixed points. It seems to me that Mixup obtained higher training error than the training without Mixup. Using these observations to support that “mixup training minimizes the empirical risk” (the first sentence of the last paragraph in Page6) is not very convincing to me. What training errors would you get if you train the models with a smaller \\alpha? \n3.\tI do not fully understand the argument in the last two sentences of Section2.4: “…This is in contrast to the fact that the test data points exhibit greater angular distance to the mixed training…”. To me, the reason for the test performance of Mixup at \\alpha =1024 is significantly worsening than ERM could be that the testing points are very different from the mixed points even though the training loss is low. Also, the manifold intrusion issue as discussed in Guo et al. (2019) seems to indicate the cases that mixed samples collide with the original training samples, not the test points. In your case, Mixup training with \\alpha = 1024 seems to not having such intrusion issue, but augmented samples generated by Mixup may have the affinity issue (i.e., far away from the training set). See “Tradeoffs in Data Augmentation: An Empirical Study” (Gontijo-Lopes et al., ICLR 2020).\n4.\tSection 2.5 is interesting to me, but also remind me of a recent paper “Midpoint Regularization: From High Uncertainty Training Labels to Conservative Classification Decisions” (Guo, ECML 2021). In that paper, mid-point samples (a special setting of Mixup) are generated for training. The empirical studies in that paper seems to be consistent to your theorem 2.11 here. In this sense, further discussion relating to the observations in that paper could be beneficial.\n5.\tThe paper could benefit from a more detailed related work section for the completeness of the paper. For example, Mixup has been applied to other settings such as natural text and graph data. Another missing work is “Mixup as Directional Adversarial Training” (Archambault et. al., 2019), which also attempts to provide a theoretically framework to understand why Mixup works, and their analysis seems to be related to section 3 in this paper. \n\n\nMinor question:\n\nThe theoretical analysis here is under the setting that the mixing ratio \\lambda is the same for mixing both the inputs and labels. Recent work on Mixup which uses different mixing ratios for the input and label mixing seems also working well (e.g., \"Nonlinear Mixup: Out-Of-Manifold Data Augmentation for Text Classification\", Guo, AAAI2020). I am curious to know if your theory in the paper still holds in that setting. \n",
            "summary_of_the_review": "The paper contributes to the understanding of Mixup training from a training data dependency perspective, which is novel and I think it would benefit the research community. On the other hand, the paper is not clear to me in some places as detailed in my reviews, and I would like the authors to comment on them.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}