{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper studies constructing text2text transformer models that are good at zero-shot task generalization via multi-task learning over a diverse set of NLP tasks. One main contribution of the work is to create prompt templates for various NLP tasks (that are of different task formats) such that all tasks can be framed into text2text learning format and that is \"natural\" to the pretrained T6 model. The paper conducts extensive experiments to demonstrate the promising zero-shot generalization ability of such multi-task learner.\n\nStrength:\n- Important problem setup that has broad applications\n- Extensive experiments to validate the claims\n- Useful resources are developed for the problem\n\nWeakness:\n- Good to study the effect of using different combination of training tasks on the downstream zero-shot generalization, which can shed some light on the usefulness of upstream tasks\n- Justification of \"true zero shot learning\" capability would require further experiments on analyzing the data overlap between MTL datasets (and also T5 pertaining task data) and the unseen task data.\n- Some more discussion on the task split and categorization will be helpful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper trains a sequence to sequence model in a large multi-task setting and tests the model's ability to generalize to unseen tasks (zero-shot). The crux of the contribution lies in the design of prompt setups, extensive experiment, and the evaluation. The conclusive of this paper supports a growing trend/consensus in the community that multi-task learning can be a good way for generalizability on unseen tasks. ",
            "main_review": "Overall, this is a conceptually simple and clean paper with extensive results.\n\nStrengths\nThe tasks considered are extensive, covering many areas of NLP tasks such as QA, NLI, sentiment classification, summarization etc. The zero-shot evaluation tasks are of different categories than in the training set, which helps support the zero-shot generalizability claim.\n\n\nWeaknesses\n- One could say that the paper lacks technical novelty, but to me, a simple paper with extensive results that work is a better than a technically rich paper that doesn't work as well.\n- In a way, we can see the multi tasks as a single giant task with many domains. If we view this as question answering, the paper is essentially training a system that learns to answer well given various domains of questions/tasks. What would also be interesting is to see/what's missing in the paper, is to carefully study the difference in domains of the training tasks, versus zero-shot evaluation tasks. Repeated experiments with different subsets of training/eval tasks can help shed some light as well. However, I do understand that it would be an empirically expensive set of experiments.\n- The conclusion of this paper is that multi-task supervised learning makes the model generalize well. However, we still rely on a vast amount of supervised data. In a way, the comparison with GPT-3 is not too fair.\n\n\nQuestions\nCan authors provide intuition about the performance of T0 vs FLAN on the datasets that T0 does better, or worse? Do it mainly have to do with auto-regressive style versus encoder-decoder (that T0 does not predict the input)? ",
            "summary_of_the_review": "This is a solid paper in terms of contribution to the language modeling line of work. It will steer the community in the right direction of doing multi-task learning as a way to meta-learn new task or adapt to unseen tasks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors demonstrate that massive multi-task learning with prompting can improve generalizability of large language models for zero-shot inference on unseen tasks. \n",
            "main_review": "Large pre-trained language models demonstrate state-of-the-art performance with large amounts on labeled training data. This has inspired a line of research to demonstrate generalizability of the models when trained with few labeled training samples. Notably there is a huge performance gap between fully supervised SOTA models like BERT, and few-shot and zero-shot learning with models like GPT-3. In this work, the authors aim to use labeled training data from a large number of tasks to demonstrate the improvement of model performance on unseen tasks. To this end, the authors leverage prompting to convert the format of all the tasks and their labels to an unified format for multi-task learning. \n\nThe idea of converting all the tasks to an unified format is not new, and has been explored in earlier works like DecaNLP, UFO-Entail, T5 and more recently in the CLUES few-shot benchmark. One of the primary contributions of the work is in aggregating resources across 12 tasks, 54 datasets and a large number of prompts for converting these tasks to the unified format. This is a quite valuable resource for the community.\n\nNow, coming back to the experiments, the authors partition the task types into training and hold-out a subset of the tasks, including all the datasets therein, for evaluation. The authors perform an important study to verify that data for those tasks is not leaked through the pretraining corpus which addresses memorization concerns. However, this analysis is missing for the training tasks. Since the objective of this work, is to evaluate \"true\" zero-shot performance of the models, it is necessary to perform this analysis to ensure that any text for the \"unseen\" tasks and datasets is not leaked from the training corpus. This is important since it is often difficult to explicitly partition tasks into disjoint sets. For instance, the authors evaluate their models on held-out NLI tasks, but train their models on paraphrase identification tasks that bear some similarity with the entailment tasks. Can the authors report the multi-task model performance on NLI tasks without using the paraphrase identification tasks?\n\nThe authors compare the performance of their model and demonstrate the multi-task version (T0) to perform better than the single-task version (T5-LM) and outperform GPT-3 in primarily NLI tasks. The performance in WiC seems a bit strange given that random choice should give an accuracy of 50%.\n\nFinally, some analysis on transferability of the tasks, to study the impact of different source tasks on target tasks, will be an interesting contribution for the multi-task setting.\n",
            "summary_of_the_review": "The authors demonstrate that massive multi-task learning with prompting can improve generalizability of large language models for zero-shot inference on unseen tasks. The authors develop a very useful resource for the community aggregating prompts across several tasks and datasets. The claim of \"true\" zero-shot generalization needs to be further supported by two more experiments as highlighted in the main review, namely, (i) analyzing the overlap in text from the MTL setup with that of the unseen tasks, and (ii) ablating paraphrase identification tasks to find the impact on NLI tasks.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new method for zero-shot generalization of NLP models. Since zero-shot generalization of the unsupervised pretrained language models are understood as an ability which is captured **implicitly** during unsupervised training on large natural text, this paper suggested a supervised learning method to understand zero-shot generalization through **explicit multi-task learning** in NLP models. To facilitate the multi-task supervised learning, each task is converted to multiple prompts, and an encoder-decoder model is trained (T5 model), to generate the answer for each tasks. The paper also provided a collection of crowdsourced prompt formats for each dataset, called P3.\nThe results indicate that a model which is trained with multiple prompts per dataset is less sensitive to the wording choice of prompts. The results also show that using encoder-decoder model which is trained to only generates the target, is more computationally efficient than standard language modeling training. It is also shown that the model is ~16x smaller than GPT3 and can attain similar performance in zero-shot learning. ",
            "main_review": "There are some major comments which are presented here, \n\n1- why MNLI and QNLI are not considered in NLI tasks?\n\n2- Why yellow-colored tasks in figure 2 are not considered as unseen tasks too? In other words, why different held-out sets are not considered for unseen tasks? \n\n3- The authors mentions that humans are not explicitly trained on NLI tasks, story completion, coreference resolution, or word sense disambiguation, and for that reason, these tasks are selected as held-out to test zero-shot generalization. For the same reason, question answering or paraphrasing can be selected, since humans are not explicitly trained on these tasks!\n\n4- Section 5: the author mentioned training encoder-decoder model is computationally efficient since it is  trained to only generate the target as opposed to GPT3. However, isn't the  model use more supervision to generate the input as well, which can benefit the zero-shot performance? later, the author mentioned that since T5 is pretrained with masked language modeling, which is different than conditional text generation, they used T5LM model, the language model adapted T5. Isn't this contradict their previous statement about benefits of target-only generation?\n\n5- how about T5 performance with different sizes?\n\n6- it is mentioned that the best checkpoint was selected from step 12200 since it yields best validation metric. However, the training set of T0, T0+ and T0++ are different. are they using the same validation with same tasks?\n\n7- Figure 4: why T0LM is not trained on the training mixture dataset? it is mostly underperforms T0\n\n8- author mentions that they did not do prompt selection using best evaluation performance. Since zero-shot tasks are different than training, don't they have a completely different prompt template?\n\n9- Since T0 model is the finetuned T5 on multi-tasked prompted dataset, I think the author should also add a finetuned encoder-decoder model which is not using T5 pretrained weights as initialization. This will helps to understand the benefits of encoder-decoder training for zero-shot learning. This way, the T0 can be evaluated on more BigBench tasks which has not in-vocabulary T5 tokens. Moreover, using pretrained T5 model for T0 means using implicit multi-tasking trained during T5 pretraining.\n\n10- it would be interesting to see the T0 performance which trained with different number of mixed training tasks. In other words, what is the impact of each training task, to zero-shot learning\n\n11- T0 outperforms 7 out of 11 zero-shot datasets, but it outperforms on 2 out of 4 zero-shot tasks. It underperforms on all datasets of  story completion and coreference resolution. what is the explanation for this?\n\n12- what is the purpose of evaluating T5LM without training on multi-tasked training dataset in Figure 4 ? it underpreforms Gpt3 (6.7B) on all tasks except WiC!\n\n13- the contribution is not clear. if it is about the benefit of encoder-decoder  model to decoder-only LM models for zero-shot learning, then a decoder-only LM model should be trained on the multi-tasked prompted training set. if the contribution is about the proposed multi-tasked prompted dataset\n\n14- Figure 5: there is no constant improvement between T0, to T0+ and T0++. In other words, adding more dataset to multi-tasked trained does not always improve zero-shot learning on BigBench. what is the justification for this behaviour of encoder-decoder model? if using more training data sometimes reduce performance, e.g. Known Unknown and Logical Deduction tasks, perhaps due to model-scale constraint, it would be helpful to evaluate with a larger T0. In other words, what is the relation between T0 model scale and training size to the zero-shot performance\n\n15- Section 6.2: the author compared T0 with GPT3 on RTE tasks with different prompts. what is the reason for just choosing RTE for this comparison only? shouldn't this comparison be done on all zero-shot tasks of Figure 4, to evaluate the robustness of GPT3 to prompts as well? \n\n16- The performance of T0 can also be compared with FLAN model in Figure 4 for each zero-shot task, despite different training set.\n\n17- Section 7: the author mentions a key difference between T0 and FLAN model is that T0 is an encoder-decoder model which is pretrained and finetuned with different objectives. However, no clear comparison with FLAN model is presented in the paper \n\n18- Section 7: author mentioned that FLAN model with comparable size to T0 (8B) finds that increasing multi-task training reduce zero-shot performance, whereas they find the opposite. The only evaluation on the size of multi-task training dataset is presented on BigBench dataset in Figure 5, which is different than FLAN zero-shot evaluation. ",
            "summary_of_the_review": "This paper presents a new method for NLP zero-shot learning using supervised multi-task prompted training. They proposed to use encoder-decoder model which is already pretrained using masked language modeling to attain better zero-shot performance with smaller model compared to large-scale autoregressive language model (GPT3). However, there are some shortcomings,\n\n1- The zero-shot evaluations are based on some selected tasks, including NLI, story completion, word sense disambiguation, and coreference resolution. In this setting, other tasks such as paraphrasing, QA, sentiment analysis, summarization, topic classification are not considered as zero-shot tasks, and only used for training, which makes an incomplete evaluation for zero-shot learning.\n\n2- The author mentioned the benefits of encoder-decoder pretraining objective (target-only generation) to auto-regressive objective (input generation) as a key factor. However, there are no comparison with auto-regressive model with the same pretraining and zero-shot task evaluation. The only comparison is on the BigBench dataset, where Transformer-based language models are pretrained with different dataset, and also performance comparison on NLI, story completion, word sense disambiguation, and coreference resolution.\n\n3- The paper is centered around understanding the multi-task pretraining, using explicit supervised learning of model. However, the evaluated model (T0) is using pretrained weights of T5 for training, which implicitly learned multi-tasking during unsupervised pretraining of T5 model. This makes the evaluation biased and an unfair comparison to GPT3 model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Today, most NLP applications use language models. These language models can be split into two different categories. The first is the GPT-3-style models, that are only trained on the unsupervised language modeling objective. In the second category, we have the models that use the finetuning method that was popularized by ELMO and BERT, where the model is first trained on an unsupervised objective and is then trained on supervised tasks. \n\nThis paper asks two questions about models in the second category. Specifically, the authors take the T5 model and test if it improves performance on unseen tasks when finetuned on multiple other tasks. The second question is whether training on more prompts improves performance. \n\nThe authors show that their model does indeed improve performance on multiple held-out datasets, when doing multitask training. In addition, the authors show that the median performance improves when training on multiple different prompts (and testing on unseen prompts). ",
            "main_review": "Strengths:\n\n1. This paper asks two very important questions about applying LMs, and since LMs are so widely used right now, I believe investigating these questions is super important to the community right now.\n2. The authors ran a lot of experiments, and while I did point out some missing potential experiments in the weaknesses section, I would like to state that overall the set of experiments ran by the authors here is very strong.\n3. The paper is well-written and easy to follow. \n4. The authors shine a light on the importance of creative and diverse prompts, some of which may seem too verbose or even silly at first. I really like this direction and hope this paper leads to this research direction being further explored!\n\nWeaknesses:\n\n1. I feel like your main baseline is the T5+LM model (T5 finetuned on the causal LMing task). Therefore I think it would make the paper much stronger if you were to show how that baseline performs on the tasks in Figure 5.  I understand that the baselines that you use in Figure 5 were built and trained by an external third party, so I trust that these baselines aren’t too weak or unoptimzed somehow, but it would still be very important to show that you also improve over T5 in this setting.  It would also be very interesting to see how well GPT-3 performs on those tasks, but I did not and will not penalize you for not having those numbers, as the GPT-3 API is not fully open and free.\n2. This paper seems like an extension of ideas discussed in iPET/PET (“It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners”, Schick & Schutze), and so to me it feels like it would be quite important to compare to that model in at least some of the experiments that you run, for example, if Figure 4 compared to iPET I feel the paper would be much stronger. What do you think? Are T0 and iPET comparable? \n\nQuestions/comments:\n\n1. Could you explain and/or cite relevant source “GPT-3 is behaviorally capable of zero-shot generalization to new tasks” (section 2) in the next draft?. I know that later on in the paper you do talk more about this, but I think it might also be important to explain this claim in section 2. I think that many people believe that while GPT-3 wasn’t explicitly trained on tasks such as translation or adding numbers,  GPT-3 can only do well on “zero-shot” tasks that it has implicitly observed during training\n2. I’m using the chrome PDF reader and do not see an actual figure above the caption for figure 3. \n3. “Unlike decoder-only language models such as GPT-3, is not trained to generate the input. This has computational benefits since in our prompted data, the target typically is much shorter than the input”. You might not need to generate the input but you still need to encode it and so you only save the time taken on the softmax layer which is insignificant. In addition, since you use an enc-dec model you need those extra encoder-attention sublayers so it’s not so clear that there’s an efficiency advantage here. I would like to ask you to consider adding some actual experiments that show that this claim is correct, or cite a relevant paper, or just remove that sentence. \n4. “Baseline models are standard Transformer-based language models with varying size.” (Figure 5). What is a ‘standard transformer LM’? I don’t think such a thing really exists. I think it could be helpful in the paper if you would give more details, such as: position embedding, FF dimension, number of heads, head dimension, and number of layers. \n5. Make sure Figure 5 is explicitly referenced in the text in the relevant paragraph. \n6. Consider making the following statement slightly weaker: “These results suggest that T0 is more robust to prompt formulation than GPT-3”. It’s hard to draw such a strong conclusion from an experiment on one dataset. \n7. In figure 6, what is the meaning of each dot? Is it the performance for a given prompt? If so, that should be explained in the caption.\n",
            "summary_of_the_review": "The authors ask two very interesting research questions with regards to prompting and finetuning of large LMs. A wide array of experiments answers these questions, and I think this will lead to better usage of LMs in the future. I did point out that there are some baselines that the authors missed, but I don’t think that that’s a big deal. I recommend that this paper be accepted, and feel that it will be very valuable to the NLP community. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}