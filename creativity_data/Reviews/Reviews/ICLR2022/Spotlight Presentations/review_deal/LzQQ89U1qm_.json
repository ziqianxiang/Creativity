{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two-branch attention architecture for modeling the discrepancies and establishing an anomaly score.\n\nAll reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self-attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study. \n\nMeanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work, and some reviews concern the clarity of the presentation (e.g. missing some details in experimental results), and the clarity of the exposition of the training process. The authors provided effective feedback during the discussion phase, which helped clarify many of the above concerns. All reviewers agree that the revision makes a solid paper and unanimously recommend acceptance of this work. \n\nThe authors are strongly encouraged to take into account the feedback from the discussion phase to further improve the clarity concerning the technical details as well as the reproducibility of the results."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors begin by discussing the discrepancy in association between normal and anomalous points and then suggest an anomaly transformer based on an anomaly-attention mechanism that is further improved using a minimax technique. On six different datasets, empirical analysis demonstrates that the proposed method outperforms state-of-the-art anomaly detection methods.",
            "main_review": "### Strength:\n1) The paper is well written and easy to follow\n2) Figure 1 and 2 are very intuitive and easy to understand.\n3) Detailed empirical analysis.\n\n### Weakness:\n1) If I am not wrong then the SMAP and MSL dataset are from [a] but the author cite Su et al. 2019b.\n2) There are repetitive ve entries in references. For example Su et al. 2019a and Su et al. 2019b. I suggest that the authors recheck all entries in bibliography carefully.\n3) I think it is important to at least provide the reader with different types of methods used for anomaly detection. The authors have done a good job at it but in my humble opinion literature review is still missing some important papers. For example, LSTM based method and SMAP & MSL dataset [a], Dimensionality reduction & clustering method [b], Spatiotemporal method using Convolutional LSTM  [c], Tensor decomposition based method [d].\n4) I have several question/concerns/suggestions about the experiment section.\n\n    - How the threshold δ is set?\n    - Can you add more details and provide a solid reason on why r=0.5% for SMD, 0.1% for SWAT, 1% for other dataset.\n    - The author should discuss the false-positive rate in more details inside the main text as minimizing false-alarm is really important in practical scenarios.\n    - What is the reason behind selecting 3 layers for Anomaly Transformer?\n    - channel states of hidden state $d_{model}$ is set to 512. Can you provide a reason for selecting this number and also can you discuss the impact of increasing and reducing this number on performance, efficiency, memory etc.\n    - In my humble opinion, the term robustness is very loosely used in the paper. I suggest the author to tone down the sentences about robustness. \n    -  Building on the previous point, the robustness of Anomaly Transformer is not evaluated against adversarial attacks. Latest research have shown that almost all anomaly detectors such as MSCRED fails against simple FGSM and PGD attack. It would be interesting to see how the robustness of proposed method in those scenarios. \n    - Figure 5 and 6, are hard to understand. It suggest that the author add some background shading for anomaly regions and also add the threshold line so that the reader can easily understand how your method is outperforming other methods. Also, Figure 6 need more context and detail in the main text.  \n\n  5) I was unable to locate the link to the code repository. It is critical to validate the paper's claims, and one of the simplest ways to do so is to access and run the code. I believe that the authors should consider making the code for their method and empirical experiments available to the reviewers and later release it publicly.\n\n\n[a] Hundman, Kyle, et al. \"Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding.\" Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2018.\n\n[b] Yairi, Takehisa, et al. \"A data-driven health monitoring method for satellite housekeeping data based on probabilistic clustering and dimensionality reduction.\" IEEE Transactions on Aerospace and Electronic Systems 53.3 (2017): 1384-1401.\n\n[c] Tariq, Shahroz, et al. \"Detecting anomalies in space using multivariate convolutional LSTM with mixtures of probabilistic PCA.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019.\n\n[d] Shin, Youjin, et al. \"ITAD: Integrative Tensor-based Anomaly Detection System for Reducing False Positives of Satellite Systems.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.",
            "summary_of_the_review": "Well written. Good empirical Analysis. But in some places the paper lacks the justifications and proper reasoning. Experiment/results section need some better explanations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces an Anomaly Transformer for detecting anomalies in time series with association discrepancy. This paper introduces two discrepancies: the series association (e.g., period and trend) and the prior association (e.g., the local smoothness or continuity). For abnormal points, these two associations have a small discrepancy and for normal points, there is a large discrepancy between these two associations. This is because abnormal points have a strong local association while the normal points have a global association. In the model, a two-branch strategy is adopted to model the two associations separately. The model is trained by minimizing the reconstruction loss and maximizing the discrepancy. The experimental results demonstrate the effectiveness of the proposed model.",
            "main_review": "Strengths:\n1. The observation that the abnormal points have a strong local association (or prior association) and a weak global association is interesting. \n2. To better model the association discrepancy, the paper proposes a novel min-max association learning strategy to avoid the Gaussian prior reduction problem.\n3. Comprehensive evaluation on a variety of anomaly detection datasets demonstrate the effectiveness of the proposed association discrepancy learning strategy.\n\nWeaknesses:\n1. What's the convergence property of the min-max strategy? \n2. Some details are not very clear. \n- For example, in equation (2), what's the shape of $W^l$, and what does the operation $*$ mean?\n- Equation (6) is ambigous, $||\\mathcal{X}-\\hat{\\mathcal{X}}||^2_2$ is a scalar, while the AssDis score after Softmax is a N-by-d matrix.\n- In the implementation details, for the $r$, why not set $r$ similar to AR of the datasets as shown in Table 1?\n- In table 3, for the \"Recon\" of Anomaly Transformer, if you only use the reconstruction error, then there should be no optimization strategy for the association discrepancy. Why the optimization strategy is \"minmax\"? \n- In figure 5, what are the labels for the y-axis? For the reconstruction and association criteria, are they the values of loss?",
            "summary_of_the_review": "In general, the observation that abnormal points have a large discrepancy between the prior association and the series association is very interesting. A novel Transformer based model is proposed to model this discrepancy. Experiments demonstrate the effectiveness of the proposed method. However, it is unclear about the convergence property for the proposed min-max strategy. Besides, there are many ambiguous details that hinder the readability of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary:\nThis paper is looking at anomalies in time series data. In particular they define anomalies as ones that lie outside some learned Gaussian-like distribution. They propose a minimax objective function that tries to minimize and maximize the discrepancy between a Gaussian distribution with a learned variance parameter and an “empirical” one learned directly through self-attention on the data. They measure discrepancy using the symmetric KL divergence between the two distributions and also use this for their anomaly score. ",
            "main_review": "Major Comments: \n- Is this approach a generalization / extension of change point detection? That is not mentioned at all.\n\n- The prior association is learned as one that minimized the difference with the series association and the series association is one that is learned to maximize the difference with the prior association? This will promote the series association to learn as non-Gaussian-like of a distribution as possible, while the prior association is finding the closest Gaussian distribution to the series association? Will this minimax strategy not lead to a degeneration where both distributions becomes extremely wide and almost uniform like?\n\n- “The reconstruction loss will guide the series-association to find the most informative associations, such as the adjacent time points of anomalies.” Why are adjacent time points of anomalies most informative? Is this because this “window” of time can be altogether be considered anomalies as opposed to noise from a single out of distribution point?\n\n- The anomaly score just indicated that there is anomalies or not within the N time point window? In order to narrow down where the anomalies are with the time points, do you need to then test smaller windows of time? Wouldn’t smaller windows cause there to be a shortage of data samples needed for learning?\n\n- KL divergence is not very good at measuring differences in the tails of distributions (it does not put enough importance on that area). But shouldn’t the anomalies be in the tails of the distribution as they are rare? \n\nMinor Comments:\n\nSome of the sentences are grammatically strange in the abstract and introduction sections. ",
            "summary_of_the_review": "Overall the paper has goods results and seems sufficiently interesting and novel.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new anomaly detection approach based on a Transformer architecture. The main idea is to leverage self-attention to capture the temporal dependency structure of the observations in a sliding window as a measure of anomaly. Since anomalies can be defined as a sequence that is inconsistent with regular ones, the attention matrix is expected to reflect some major aspects of anomalies. \n\nThe authors propose a two-branch attention architecture to handle multi-dimensional real-valued time-series data, where prior- and series-attention matrices are computed and utilized in a certain min-max competitive learning framework. \n\nThe final anomaly score is defined as the product between the re-construction error and the KL distance between the two attention matrices.  ",
            "main_review": "Update after the discussion with the authors.\n\nThe authors have addressed all of my immediate concerns. The paper now looks very strong. I recommend acceptance. \n\n----\nThe key idea of using Tranformer's self-attention as a measure of anomalousness sounds novel. It is an excellent idea. The proposed architecture featuring a two-branch attention mechanism also looks novel. The novelty seems undisputable, to the best of my knowledge.\n\nThe problem is, however, that the paper mostly ignores almost all the existing anomaly detection approaches for **time series**. It obviously does not make sense to use point-wise anomaly detection methods such as SVDD to capture sequential anomalies. Many researchers in this domain may agree that the baseline model in the present context can be the vector autoregressive model, which naturally realizes a particular type of self-attention in the form of the lag-dependent covariance matrices. You can find many works that leverage a dependency graph for anomaly or change detection. I'd also suggest looking at the literature on time-series segmentation. \n\nAnother issue is that the paper lacks sound justifications for the proposed approach. Given much theoretical/empirical work with VAR or state-space models (or their neural extensions), we expect a much more understandable derivation of the proposed model.  Many \"functions\" such as AssDiss lack a proper definition. \n (For example, I didn't understand how the probability distributions had been defined from P^l and S^l --- just writing SoftMax or just showing Eq.(2) does not mean you have defined a distribution for a **matrix**. The Gaussian distribution in Eq.(2) is defined in the entire real domain. But probably, you are on a regular time grid. Apparently, some mathematical inconsistency exists.)\n\nThat's not a problem if this were part of API documentation of a software library. But as a technical paper submitted to a top machine learning conference, there may be different expectations. \n\nI think the main idea deserves much more careful and deep thoughts. I encourage the authors to re-do the text to perfect it. I'm sure that the new version will be a great piece of work in the community.  \n",
            "summary_of_the_review": "Updated after the discussion\n- Novel idea of using the degree of self-attention as a metric of anomalousness.\n- New approach of positional encoding based on Gaussian kernels\n- Comprehensive empirical comparison with alternative methods\n\n----- Original summary -----\n- Novel idea of using the degree of self-attention as a metric of anomalousness.\n- Unclear and unjustified descriptions.\n- Lack of the critical baseline (major issue in this domain). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}