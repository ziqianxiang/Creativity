{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle.  The parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel.  In its current form this won’t scale to large programs that require large tree depth, but is a promising first step in this direction.  The learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used RL algorithms.  During the reviewing process the authors have actively engaged in the interaction with the reviewers and addressed all the concerns, and all reviewers unanimously recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of the low efficiency of program search guided by a pre-trained oracle or on discrete and non-differentiable architecture space. To this end, the paper proposes a framework that performs program architecture search on top of a differentiable relaxation of the architecture space. This allows the program architectures and parameters to be learned via policy-gradient methods without RL oracle. The proposed method also exploits compositionality by allowing an ensemble of primitive functions that perform task-agnostic skills. The experimental results on navigation and manipulation domains show that the proposed method can reliably obtain task-solving programs and outperforms or performs competitively compared to RL baselines including SAC, PPO, TRPO. I believe this work studies an interesting and promising research direction and proposes a convincing framework to tackle this problem with solid technical contributions. Yet, I am mainly concerned with some missing baselines, relevant works, and ablations.",
            "main_review": "## Paper strengths and contributions\n\n**Motivation**\n- Exploring using programmatic policies structured in more interpretable representations to yield better interpretability compared to deep RL policies is promising.\n- The idea of conducting a program search on a differentiable architecture space with policy gradient is novel and convincing. This paper presents an effective way to implement this idea.\n\n**Technical contribution**\n- Relaxing the discrete program architecture search space to be continuous allows for optimizing the program architecture and the parameters of program modules using policy gradient methods.\n- To the best of my knowledge, leveraging pre-defined/pre-learned primitive program functions (i.e. ensemble policies) to tackle hierarchical RL problems in the field of programmatic RL is novel.\n- Using specified state transition behavior of the abstract actions in HRL training seems effective, especially when simple, task-agnostic skills is available\n\n**Clarity**\n\nThe overall writing is clear. The authors utilize figures well to illustrate the presented ideas. Figure 5 clearly shows the idea of the Program Derivation Tree and how it creates differentiable architecture spaces.\n\n**Experimental results**\n- The presentation of the experimental results is clear and the discussion is sufficient and mostly convincing.\n- The experimental results show that the proposed method outperforms or achieves competitive performance compared to RL baselines such as HIRO, TRPO, PPO.\n\n**Reproducibility**\n\nThe code is provided, which helps understand the details of the proposed framework.\n\n## Paper weaknesses and questions\n\n**DSLs without Loops**\n\nThe paper simply states that the loops are omitted in this work. Yet, describing many desired behaviors involves loops/repetitive behaviors. I would like to hear the authors' opinions on how the proposed framework can deal with such behavior.\n\n**PRL baselines**\n\nThe paper only compares with non-programmatic RL methods. \nIt would be more informative to compare the proposed method against the PRL works (such as \"[Verifiable reinforcement learning via policy extraction](https://arxiv.org/abs/1805.08328)\", \"[Synthesizing Programmatic Policies that Inductively Generalize](https://openreview.net/forum?id=S1l8oANFDH)\", etc.), even they require training RL oracles first, which should not be a critical assumption given the tasks considered in this work. With these comparisons, it would be easier to justify how well the proposed method can perform. Or, it would be even greater to show what suboptimality is induced by learning from RL oracles so that it would make this work more convincing.\n\n**Figure 8b: the ladder-shape convergence curves**\n\nDo authors have a good intuition on why the convergence curves of the proposed method presented in Figure 8b look like that?\n\n**Related work: PRL work that does not use RL oracles**\n\nWhile I am well aware of the fact that arXiv papers do not count as publications, it would be great if the authors can distinguish this paper from a recent work (\"[Learning to Synthesize Programs as Interpretable and Generalizable Policies](https://arxiv.org/abs/2108.13643)\") that also tackles learning a programmatic policy purely from rewards. (This suggestion does not affect my evaluation of this paper.)\n\n**Related work: program synthesis**\n\nDiscussing prior works in program synthesis, whose aim is to synthesize programs to fulfill some specifications, would make the related work section more comprehensive. Some relevant works include:\n- RobustFill: Neural Program Learning under Noisy I/O\n- DeepCoder: Learning to Write Programs\n- Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis\n- Execution-Guided Neural Program Synthesis\n- Neural Program Synthesis from Diverse Demonstration Videos \n- Learning to Infer and Execute 3D Shape Programs\n- Latent Programmer: Discrete Latent Codes for Program Synthesis\t\n\n**The proposed method without compositionality**\n\nThe effectiveness of compositionality is not fully justified since no experiment with the proposed method without ensemble policies is presented.\n\n**Decreased interpretability with the transformation**\n\nWhile the differentiable architecture space is mentioned as a strength of the paper, it requires affine transformation for conditions in if-else statements, which decreases the interpretability of the program in complex environments and makes it difficult to be interpreted by non-expert users.\n\n**Code**\n\nPlease include a requirement.txt specifying the versions of all the dependencies. I have encountered many issues when trying to reproduce the results.\n\n## Other metrics\n\n### Relevance and significance\nSolid contributions to a relevant problem\n\n### Novelty\nSeveral novel and surprising contributions\n\n### Technical quality\nTechnically adequate for its area, solid results\n\n### Experimental evaluation\nInsufficient or lacking evaluation in 1-2 criteria, but sufficient w.r.t. the other criteria\n\n### Clarity\nVery clear, only minor flaws.",
            "summary_of_the_review": "I believe this work studies an interesting and promising research direction and proposes a convincing framework to tackle this problem with solid technical contributions. Yet, I am mainly concerned with some missing baselines, relevant works, and ablations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a novel method for synthesizing programmatic policies. The code idea of the method is to define a relaxed and differentiable version of the domain-specific language (DSL) used to encode the programmatic policies. \n\nThe program space the DSL induces can be described as a program of the DSL itself. Since the program defines a differentiable space, one can use policy gradient methods to search in the space of programs by assigning higher probabilities to production rules that maximize the expected rewards. \n\nThe search for programmatic policies happens in two steps. The first step applies a policy gradient method to optimize the probabilities defining the program space. Then, one can extract the most likely program structure from the search space by greedily choosing the production rules with higher probabilities. The resulting program is further optimized with reinforcement learning. \n\nEmpirical results on continuous control problems show the advantages of the method. ",
            "main_review": "The idea of using a differentiable version of a DSL to allow for gradient-based optimization is clever and elegant. I understand that the authors were inspired by methods used to learn neural network architectures such as DARTs. The application to the synthesis of programmatic policies is non-trivial and interesting, thus worthy of publication. \n\nHowever, I have a two concerns with the paper.\n\nThe first concern is minor. Why is the title of the paper \"Programmatic Reinforcement Learning Without Oracles\"? In my opinion, the main contribution of the work and that makes it interesting is the differentiable approximation of the DSL that allows one to perform gradient methods to search over the originally discrete space of programs. Learning without an oracle can also be achieved without the differentiable approximation. For example, if the problem domains were discrete, one could use an enumeration procedure similar to what NDS does without the Bayesian optimization step [1]. \n\nMoreover, the need of having an oracle isn't necessarily a major hurdle. If an oracle was available how would the proposed method compare with other methods such as NDS [1] and Propel [2]? I am wondering if the main advantage of the proposed method is that it doesn't require an oracle for training or if it is able to more quickly find an effective programmatic policy in the large space induced by the DSL. \n\nThe second concern I have is regarding the computational complexity of equation defining $\\[E\\](s)$. The equation is recursive and resolving the value of the equation is equivalent to traversing the tree of programs. This computation can be prohibitive depending on the DSL and on the size of the program that needs to be synthesized. That is, we need to enumerate all programs up to some depth for each state evaluated during an episode! \n\nI understand that the complexity of $\\[E\\](s)$ doesn't involve gathering samples from the environment, but it has a non-negligible computational complexity that isn't even mentioned in the paper (I apologize if I missed a discussion about the complexity of $\\[E\\](s)$). What happens if the x-axis of the plots is given in running time? Will the method proposed in the paper be much slower than other competing methods? \n\nThe following could be better explained in the paper:\n\n1) Why encode the tree program as a Gaussian distribution? \n2) rho isn't defined in Equation 2. \n3) Explain that the action the tree program returns is given by the evaluation of the root of the tree with the [E](s) equation. \n\nReferences\n\n[1] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. Proceedings of the 35th International Conference on Machine Learning, ICML 2018.\n\n[2] Abhinav Verma, Hoang Minh Le, Yisong Yue, and Swarat Chaudhuri. Imitation-projected programmatic reinforcement learning. Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019\n",
            "summary_of_the_review": "The paper presents a novel and interesting approach to synthesizing programmatic policies that uses a differentiable approximation of a domain-specific language for policies. This differentiable DSL allows for the use of policy gradient methods to search over the space of programs. The main weakness of the method is the computational complexity required to compute the action of the policy during training. The method iterates through all programs one can synthesize in the DSL up to a given depth. The computational complexity of the policy isn't even mentioned in the paper and it can be a show-stopper depending on the DSL and on the size of the program that needs to be synthesized. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method for synthesizing programs to solve MDPs without the need for teacher oracle policies by proposing a continuous program search relaxation that allows synthesizing performant programs as policies.\n",
            "main_review": "## **Paper Strengths**\n\n- **Clarity:** The paper is pretty clear wrt the high-level topic and what the high-level intuitions are. This is good, because I think some low-level parts of the paper could be clearer (see weaknesses section). Figure 3 is also a great and clear figure, and therefore should be referenced in the \"Ensemble Policies\" subsection of section 2.\n- **Experiments:** Comparing against the HRL style of environments such as the Ant mazes and HalfCheetah hurdle is nice, and serves as a nice comparison (although not direct because the abstractions provided are very different) to existing deep RL HRL algorithms. Furthermore, the authors evaluate over 10 random seeds, and include learning curves (not always given for these types of program RL papers), which is very thorough.\n- **Method**: This continuous relaxation of the discrete program search tree is neat and intuitive, and serves as a contribution in the program synthesis for RL space.\n- **Motivation**: The motivation is nice, to generate interpretable/stable policies for RL by synthesizing programs which can act as the agent in environments. Part of the novelty here is the continuous relaxation, which makes the program search more feasible.\n- **Comprehensive Appendix:** The appendix has some solid extra analysis along with example programs for each of the tasks. \n\n## **Paper Weaknesses**\n\n- **Clarity:** I think section 2 can be made a little clearer to people not familiar with the program synthesis domain by introducing examples earlier (e.g. giving a clear example immediately after \"production rules X → \\sigma_1, \\sigma_2,....or nonterminals\"). $\\phi_{Act}$ could be better defined by first defining $dx_i$, $dy_i$, and using underbraces with text or having an extra sentence explaining it. It took me more time than it should've to parse. Furthermore, I think there should be an algorithm pseudocode box where all stages of training the method for the different versions of the method are detailed, this would make it clearer how exactly the method works (it can be hard to keep track of high-level details when reading a long method section that goes over all individual parts). Finally, your graphs don't have axis labels (Figure 8).\n- **Method:** Seems to me that the tree structure is essentially decided beforehand. Essentially one would have to decide how complex the program is a priori, and then learn via policy optimization what the weights of that program tree are. This seems somewhat limiting to me, but in one sense perhaps acts as a hyperparameter that regularizes the policy structure. Could the authors do an ablation study on this with extra analysis?\n- **Abstractions:** Having abstractions for Program Synthesis is perfectly reasonable, and even more reasonable in the case of this HRL-style of RL program synthesis. But is $\\phi_{\\text{Guard}}$ necessary? It seems like a pretty restrictive assumption to me, and would be interesting to see how well the model-based HRL version of the algorithm can work without this guard.\n- **Misc:** \"The high interpretability of π-PRL policies in turn leads to strong generalizability to novel environments.\" → The connection between interpretability and generalizability in novel environments is not well explained. Do the authors mean the inductive biases of the program structure enforced by the fact that these have to be simple, interpretable policies?\n- **Missing related work:** This is not that big of a deal as both of these papers are recent on arXiv (2021). But authors should cite [1] and [2], and compare/contrast with them in the related works and intro section. But I believe that both of these don't require RL oracles, so perhaps a bit more nuance is needed in the intro/abstract/related work describing the contribution of this paper.\n- **Experiments:** In light of the above complaint, I think that generally the experiments lack solid analysis in the main paper. Especially in the high level planning experiments, the authors should explain why they believe HIRO gets stuck where \\pi-HPRL does not, why \\pi-HPRL is more sample efficient, etc. Furthermore, I think an ablation study on tree depth would be illuminating (mentioned in method weaknesses).\n- **Baselines**: Can the authors compare to a program RL baseline? Some of the cited works in the related works should work fine, for example the cited Verma works, the cited Bastani/Silver decision tree papers, or [2]? Even if the specific DSL here won't work with them, or they require teacher policies, this type of comparison would elucidate where some of \\pi-PRL/HPRL's advantages come from (the DSL? the program structure? the search procedure? the continuous relaxation? the lack of needing a teacher oracle??).\n\n## **Questions**\n\n- Being not familiar with Composition-SAC, why Composition-SAC instead of just regular SAC?\n- \"The $\\pi^\\Tau-$PRL models exhibit less data-efficiency than the Composition-SAC policies in part due to the use of TRPO.\" Why not use off-policy RL for the method then?\n\n## Minor Issues\n\n- Page 2: \"defined for each DSL constructs\" → each DSL construct\n- Page 4: \"u_e contain more than one architecture\" → u_e contains\n- Page 7: \"primitives e.g. Ant turning around at a corner. We\" → \"primitives, e.g. Ant turning around a corner, we learn a ...\"\n- Page 7: \"Expect pusher\" → \"Except pusher\"\n- Although our DSLs are simple, the expressiveness is equivalent\nto previous interpretable RL works (Bastani et al., 2018; Verma et al., 2018; 2019) → in light of [1] this sentence may need to be reworded\n\n[1] Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J. Lim. Learning to Synthesize Programs as Interpretable and Generalizable Policies. 2021.\n\n[2] Yichen Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, Martin Rinard. Program Synthesis Guided Reinforcement Learning. 2021.\n\n",
            "summary_of_the_review": "In summary, I like the high level idea and motivations of this paper. The method is also seemingly novel and performs well. But there are some issues with detail clarity, experiments and analysis, and a comparison to similar baselines. I am currently voting for rejection, but will happily change my score and/or be responsive to authors during the review process as these concerns are addressed.\n\n\nUPDATE: Score has been revised after discussion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}