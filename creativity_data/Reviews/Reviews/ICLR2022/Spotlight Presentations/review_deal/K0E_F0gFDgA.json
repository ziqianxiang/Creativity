{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints.\n\nAll reviewers appreciate this contribution that can be expected to be used by the NLP community."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents MultiBERTs, a set of 25 model checkpoints, and the Multi-Bootstrap, a non-parametric method to estimate the model uncertainty. The experiments verified the proposed method on a case study of gender bias in coreference resolution. ",
            "main_review": "The paper presents MultiBERTs, a set of 25 model checkpoints, and the Multi-Bootstrap, a non-parametric method to estimate the model uncertainty. The experiments verified the proposed method on a case study of gender bias in coreference resolution. \n\nPaper Strengths:\n- It is great to see the models and statistical library are available online (165 checkpoints). I appreciate the authors can provide the CO2 information on model training, which is environment friendly.\n- This is an empirical analysis paper, which is useful for the community but the computational cost seems very high. Pretrained model has been widely used and show impressive performance. The method in this paper is novel and theoretically sound, which is helpful for understanding the large models.\n- The proposed method is rigorous in terms of mathematics and statistics. Even though the techniques are simple, but they can be categorized into three designs and formulated as a formal problem.\n ",
            "summary_of_the_review": "This paper is different from classical analysis paper in NLP. The authors claim to make progress on language model pre-training and it is essential to distinguish between the properties of specific model artifacts and those of the training procedures that generated them. I can buy this claim to some degree but still reserve my recommendation for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper releases MultiBERTs, a set of 25 BERT base checkpoints to facilitate studies of robustness to parameter initialization and order of training examples. It also proposes the Multi-Bootstrap method to quantify the uncertainty of experimental results based on multiple pre-training seeds.",
            "main_review": "Strengths:\n- The paper provides various pre-trained BERT models with many checkpoints, which could enable future researches on reproducibility and robustness.\n- The proposed Multi-Bootstrap procedure could give a reasonable estimate of the distribution of the error over the seeds and test instances, under both paired and unpaired scenarios.\n- The provided code for Multi-Bootstrap is easy to understand.\n\nOverall, I feel it is still expensive for common NLP tasks to adopt MultiBERTs and MultiBootStrap methods to draw conclusions, which may hinder it from a wide range of applications.",
            "summary_of_the_review": "The paper provides adequate models and a good estimation method for future researches on robustness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Many tasks in contemporary NLP begin by building off of a large language model.  This can cast the downstream task as a sort of fine-tuning experiment, whereby the results are heavily influenced by conditioning on the starting point of a single pre-trained version of an LLM.  In this work, the authors take BERT as an example, and ask how much a specific artifact as a draw from the distribution over (model weights, initialization scheme, training data, loss function) affects downstream tasks built upon it.  \n\nThe authors provide a wide variety of BERT models that are varied in their training and initalization.  They define a bootstrap procedure for the scenario where multiple instantiations of base models are available, and tie their findings together in a case study of gender bias in coreference resolution.",
            "main_review": "The authors address the question of using artifactual large language models as the basis of experimentation in NLP research.  In particular, they examine the extent to which results of finetuning or transfer learning can be attributed to the use a single best checkpoint of drawn from a distribution over training data, loss, random seed, and architecture, or  for common building blocks (here they consider BERT).\n\nStrengths:\n\n- The biggest strength of the work as I read it is the multi-bootstrap.  This is the most likely element to be reused in future work\n- I commend the openness of the authors; their code, their checkpoints, and the clarity of their writing made this paper a pleasure to review\n- The inclusion of a case study in section 4 brought the whole of the contributions together, and highlighted the value of both multiple independent samples as well as the bootstrap framework used to assess the contribution of different possible choices.  \n\nWeaknesses:\n\n- I hope that the resources of the MultiBERT models will be reused in NLP research, but fear that since LLM research is progressing at a frenetic pace, BERT is no longer as relevant as it once was even a mere 12 months ago, being supplanted on several tasks by prompt-based language models (e.g T5, GPT-3) or smaller but more versatile reimplementations (e.g GPT-J).  This is beyond the authors control however, and does not much diminish their contribution here.\n\n- On page 5, it was not clear to me why in the paired samples design that estimating $\\hat{\\delta} - \\delta$ represents the *overall error*.  It would help here to explain (in a footnote if need be) how the overall error is represented in terms of $\\delta$ or $\\hat{\\delta}$.\n\nQuestions:\n\n- This may be already established in other bootstrap results, but what is the sample efficiency of the convergence in distribution for Theorem 1?\n\n- As the authors detail in their environmental impact statement of section 2.1, the cost of producing the MultiBERT checkpoints even in favourable energy generation conditions conditions is not negilible.  Could the multi-bootstrap be adapted to probing smaller sets of models that themselves are resampled (e.g. under repeated dropout mask as in MC-Dropout) to approximate posterior distributions over the model parameters?  The samples would no longer be IID, but such a scheme would admit many more usecases for the multi-bootstrap.\n\n- In section 4, CDA-full sounds like a much more drastic intervention.  The MultiBERTS in CDA-incr are trained on much larger corpii, but CDA-full MultiBERTS are trained from initialization only on Webster et al’s data?  Won’t this introduce a confound whereby the CDA-full models are less proficient overall compared to the CDA-incr models?\n",
            "summary_of_the_review": "It's a sound paper, well overdue in the NLP literature, and reminiscent of other sober reflection papers in the vein of [Melis et al. 2017, Narang et al. 2021], though the latter concentrate mainly on testing the benefit architectural improvements.  Going beyond these papers however, the authors build and provide a bootstrap procedure to evaluate specific hypotheses.  \n\nWhile I'm not sure that ICLR is the best venue for this work, it is clearly important and deserves to be showcased.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents MultiBERTs, a set of 25 model checkpoints to support robust research on BERT, and the Multi-Bootstrap, a non-parametric statistical method to estimate the uncertainty of model comparisons across multiple training seeds. It demonstrates the utility of these resources by showing how to quantify the effect of an intervention to reduce a type of gender bias in coreference systems built on BERT. ",
            "main_review": "The paper presents an interesting approach, a novel contribution that might have some applications. From the methodological point of view, it is well-written, there is a good review of related works, a description of the MultiBERTs release, and an application to reduce a type of gender bias in coreference systems built on BERT. There are also comprehensive supplementary materials with proofs of theorems and lemmas. It seems to be a novel approach, it's hard to assess its significance but the presented application suggests there the impact might be important. If the article is accepted, there are some typos that should be fixed before publication:\n- p. 2: seeks to to analyze -> seeks to analyze\n- p. 2:  the uncertainty the test sample ->  the uncertainty of the test sample\n- p. 6: it looks that in one case, to mark a distribution, D, is used, in another case, it is P\n- p. 8: statiatically -> statistically\n\n\n\n\n",
            "summary_of_the_review": "The paper presents an interesting approach, a novel contribution that might have some applications. From the methodological point of view, it is well-written, there is a good review of related works, a description of the MultiBERTs release, and an application to reduce a type of gender bias in coreference systems built on BERT. There are also comprehensive supplementary materials with proofs of theorems and lemmas. It seems to be a novel approach, it's hard to assess its significance but the presented application suggests that the impact might be important. Therefore, from the methodological point of view, the paper is marginally above the acceptance threshold, but if it is accepted, there are some minor typos that should be fixed before publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}