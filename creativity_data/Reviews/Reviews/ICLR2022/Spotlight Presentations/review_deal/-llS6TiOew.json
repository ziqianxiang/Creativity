{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. Through extensive (controlled) experiments using 6 languages they answer as well as raise very interesting questions about the role of morphology/segmentation/vocab size which mat spawn more work in this area.\n\nAll the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. More importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. The authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly. \n\nI have no hesitation in recommending that this paper should be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper asks an interesting question, one that appeared eternally relevant for NLP: Are languages harder or easier to contextually language-model by Transformer in light of their varying morphological complexity? The claimed main finding is provocative, indicating that the debate morphological variation is not relevant in the context of the given computational machinery.",
            "main_review": "Strengths:\n- Very nice writeup and structure, the paper is for the most part a very enjoyable read.\n- Bold in tackling important questions from a fresh angle.\n\nWeaknesses (may be obsolete, see discussion):\n- The contributions are overstated in light of the experiment setup.\n- Only six resource-rich languages are used and only one respective dataset, the UN parallel corpus.",
            "summary_of_the_review": "I recommend that the paper be rejected so that it may further develop its empirical setup & verify (or reject) the findings on a larger dataset with more substantial linguistic depth (more resource variation) and breadth (more languages). In its current form, the paper's larger-than-life claims are simply not sufficiently supported with the choice of one dataset and six resource-rich languages, even if the hypothesis and findings are indeed interesting.\n\nEdit after first round of author discussion, 2021-11-16:\n\nI am concerned that there is a disconnect between the stated contributions of the paper (which are expressed as rather broad and general in its narrative) on one side, and the setup of the empirical investigation on the other side. While I may view the insight as to the (ease of) availability of broad-coverage linguistic resources sympathetically, the fact still remains that the findings are closely tied with the UN parallel corpus and its six languages. This in itself would not disqualify the paper at face value. However, text e.g. under \"Summary of findings\" 1-5 and later in \"Conclusion\" comes across as masking this fact and painting a much broader picture.\n\nI would be much more inclined to accept this (undoubtedly insightful) paper should it insert a clear Limitations section which would reflect on the preliminary nature of these findings given the dataset constraints.\n\nAs for the language resources, I would very much love to hear which of those e.g. in OPUS (https://opus.nlpl.eu/index.php) were not fitted to the experiment while only the UN corpus passed the filter. This would be an insightful addition to the limitations section, as the author response now seems to indicate massive limitations in all the other available datasets. Do share with the community!\n\nEdit after 2021-11-18:\n\nUpvoting the overall score after insight re: multi-parallel datasets available (see discussion).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, authors aim to untangle the disparities in performance between morphologically rich languages (e.g. RU) and morphologically poor languages (e.g. ZH). In order to untangle such disparities, the authors propose a highly controlled experiment setup, where they control for a number of potentially confounding factors (e.g. language mappings, data size, text encoding, etc.). After the controlled experiments, the main take-away of the paper is that vocab size seems to be the biggest indicator of how well or poorly a CLM will perform — since there can be several different granularities for the concept of a “word” for morphologically rich languages, and this can explain why modelling morphologcially rich languages can be more difficult.  They perform additional experiments with different representations of ZH, and were able to mimic the struggles of morphologically rich languages with ZH, and use this as further evidence to support the claim that the debate within NLP about the best segmentation is irrelevant.\n\nOverall, this work is exciting, has has a lot of potential to help inform future language modelling for morphologically rich languages.  The very controlled nature of the experiment in the paper is what makes it most convincing, and this is the paper’s largest contribution. ",
            "main_review": "Pros: \n— Very interesting and engaging work, with \n--- Experiments are very controlled, from data, number of lines, language mappings, and segmentations. \n— Experiments seem to be reproducible just from the description in the Experimental Setup. Very clear. \n— Thoughtful connections from human language processing and linguistics to try to explain the Char/Byte/Word performance disparities. \n--- Really interesting follow up experiments on ZH!\n\nCons: \n— The “A0, B0, C0, A1 & A2, A3-7”, etc. nomenclature for experiments is unhelpful to readers. Can the experiment names be changed to be something more intuitive, so that readers don’t need to memorize “oh, A0 is one of the controlled experiments for data size”?\n-- Figures in this paper are largely unreadable and do not help as visual aids. I have to zoom in to 310% (at minimum!!!), and even then my eyes struggle to distinguish the colours and line styles from each other. Also, the subfigures within subfigures are not readable. This is a big problem, because the figures are used to support claims that the authors are trying to make, and I think many readers would be unable to draw the same conclusions from these figures themselves. \n \n\nFormatting, grammar, etc.:\n1. Abstract: “Performance disparity is not a necessary condition” is worded strangely to me. \n2. Introduction: Formatting at bottom of 1.1 is weird. Use of bullet points makes no sense here, they just split up the sentence. \n3. S2, “Experimental setup“: There’s a double parenthesis )) with the (Jurafsky & Martin, 2009) citation\n4. S5, your tables do not follow the same typesetting conventions as your Figures (i.e., the description is on top)\n",
            "summary_of_the_review": "Reasons for score:  \nFrankly, if the figures and tables in this paper were significantly improved, this would be an 8 or higher. The main motivation of the paper is exciting, the methods are well controlled, and the findings are interesting. The core of the problem (as I discuss in \"Cons\") is that the claims made in the paper rely on the figures/tables for illustration as evidence, and the figures/tables are entirely unhelpful. In otherwords, the tables and figures detract from your arguments because they are unreadable. Once this can be fixed, I think its great work, and it would be interesting and directly useful to many people in NLP. \n\nEDIT - Wednesday, 17 November, 2021: In light of the authors rebuttal to my review, and their changes to v2 of the document, I am bumping my score up from 5 (marginally below threshold) to 8 (good paper, accept). My biggest problem with the paper before, were the figures, but I think that the authors addressed my comments to the best of their abilities by adding a lot more clarity to the textual descriptions of the images, in the paper and in the captions. The figures are still not great, but with their new textual descriptions, you can look at the general shapes of things, and visually understand the point, and before v2 I did not have this same level of visual understanding.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "At its core, this paper focuses on testing whether languages with different morphological complexity equally challenging to language-model in a bilingual / MT-like setting using a 6 layer Transformer architecture.  Specifically, it tests the assumption whether the previous observations about languages with higher morphological complexity are difficult to language-model, using a controlled experiment with a n-way parallel corpus across 6 languages of interest. The key finding of this paper is a simple one : that the choice of input representation is highly important, given everything else is kept constant, and that differences in language-model perplexities can be normalized and made to disappear by choosing the same representation (word/character/byte)",
            "main_review": "This paper contains a series of sound, controlled experiments to study the role of input representations in language-modeling. I think the experiments are well designed, and with a clear focus to answer the research questions laid out in 1.2. Overall, I do think that this paper will benefit the multilingual NLP community in thinking carefully about the choice of input representations, rather than advocating for a one-choice fits all solution. However, I do have a few concerns that prevent me from giving a higher score. \n\n1. The claim that this paper \"renders a decade-long debate on morphology irrelevant\" is made multiple times in this paper, and in my opinion, is too strong a claim to be made on the basis of experiments on a single task (conditional LM), on a handful of languages (however typologically diverse they may be), and using a single architecture (6-layer Transformer). I would recommend scoping this claim down to something that can be clearly supported from the results of this paper.\n\n2. Second, how does the practical recommendation in Section 3 (\"A practical takeaway from this set of experiments: in order to obtain more robust training results, use bytes for ZH and characters for AR and RU\") square with the multilingual nature of recent models which typically have a shared input representation for many languages? I think more generally this point is one of several that seems slightly disconnected from downstream applications (the claim about morphological complexity being irrelevant would also fall in this bucket.).\n\n3. There are a few citations from prior work in NLP that are worth contextualizing here. Specifically, the work of Gillick at al. on byte level processing for sequence tagging [1], that of Wang et al. on byte-level Neural MT, and more recently, the work by Clark et al. on building a tokenization-free encoder seem particularly relevant [3]. In fact, the whole body of literature on character-level neural models are particularly relevant and the omission of these is critical, especially given the stance of the paper in advocating moving beyond \"word\" level models (e.g. Footnote 8)\n\n[1] - https://arxiv.org/pdf/1512.00103\n[2] - https://arxiv.org/abs/1909.03341\n[3] - https://arxiv.org/abs/2103.06874",
            "summary_of_the_review": "This paper contains sound and controlled experiments that test the role of input representation in bringing disparity to language modeling results, which I believe will be useful for the larger community. At the same time, several claims in the paper are too strong or not grounded in practical use-cases, and some relevant literature is not referred to, leading me to be marginally inclined to accept this paper.\n\n### Edit - 22 November, 2021\nI spent some time reading the discussion between the authors and the other reviewers, as well as revisiting v0.2 of the paper. In light of these discussions, I think I am fairly comfortable accepting this paper as my main concerns have been addressed by the review and the comments of the authors. Hence, I revised my overall recommendation to 8, while adjusting a few other scoring parameters too.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper asks: Are languages which have been traditionally considered\nmorphologically rich (Arabic and Russian) and poor (Chinese) equally\nhard to learn by transformer-based conditional language model (CLM).\n\nTo quantify how hard it is for CLM to model a language $l_1$, this paper\nproposes to train a transformer-based encoder-decoder model to translate\n$l_1$ to another language $l_2$, and uses the \\\"unnormalized perplexity\nscore\\\" (the number of bits needed to encode the same development set\nfor the target language) as an indicator for the \\\"hardness\\\" for CLM to\nlearn the $l_2$. For two languages $l_1$ and $l_2$, there exists\n**disparity** if their perplexity distributions are significantly\ndifferent. This paper includes extensive experiments across 30 language\npairs using three different representation level (including character,\nbyte, and word) for different dataset sizes.\n\nThey have the following findings:\n\n1.  They find that the performance disparity between morphologically\n    rich and poor languages cannot be justified due to morphological\n    complexity. Representation level matters more.\n\n2.  They find that the hardness for CLM to model a language is related\n    to the level of representation used to encode the language, the\n    vocabulary size, and/or the sequence length.\n\n3.  They find that it is equally hard to learn a certain target language\n    (that are tested in their experiment) using transformer-based\n    conditional language models, independent of the source language.",
            "main_review": "I want to thank the authors for the interesting paper. Overall, the\npaper contains a substantial amount of experiment results with some\ninteresting findings, but the paper is not easy to read and not easy to\nfollow. I am also not sure if this paper reveals substantially new\nthings to the readers.\n\nThe strength of the paper is as follows:\n\n-   This paper proposes to use conditional language modeling (machine\n    translation) to evaluate how hard it is for transformer models to\n    model a language. This is a clever setting that allows one to fix a\n    source language and compare the difficulty of modeling different\n    target languages.\n\n-   The paper uses statistical tests to verify if there exists\n    performance disparity between languages of different morphological\n    complexity.\n\n-   This paper shows that transformer conditional language models are\n    equally capable of modeling the six languages evaluated in the paper\n    when the suitable representation level is chosen. This indicates\n    that transformer models are **fair** and are not biased toward any\n    the language they tested, and morphological complexity is not indicative\n    of how hard it is for transformer-based conditional language models\n    to learn.\n\nThe weakness of this paper and questions about the paper is listed as\nfollows:\n\n### Not enough novelty compared to prior works\n\nAn important prior work of this paper is Mielke et al. (2019). I find\nthis paper highly resembles the work of Mielke’s from some aspects. I\nwill summarize the work of Mielke et al. (2019) as follows first:\n\n1.  Research Question: Do LSTM-based language models serve all\n    languages? What are the characteristics of languages that are easier\n    for LSTM-based language models to model?\n\n2.  Experimental Setup: Use parallel corpora containing 21 languages.\n    Train a mono-lingual character/BPE LSTM language model on each\n    language, and evaluate the development set perplexity for each\n    language.\n\n3.  Results and Findings: Language difficulties are not significantly\n    predicted by linguistic factors (including morphological\n    complexity), no matter the representation is character or BPE. For\n    LSTM language model with character-based representation, the\n    difficulty for LSTM to model correlates with the raw length in\n    characters. For LSTM language model with BPE-based representation,\n    the difficulty for LSTM to model correlates with the vocabulary\n    size.\n\nThe main difference between this work and Mielke et al. (2019) are as\nfollows:\n1.  The models used in this work are transformer-based conditional\n    language models, while Mielke et al. (2019) used RNN-based\n    (unconditioned) language models.\n\n2.  The indicator of performance in this paper is unnormalized\n    perplexity score, and Mielke et al. (2019) used the normalized\n    version.\n\n3.  This paper uses statistical tests to compare if there exists\n    performance *disparity* between language pairs, which was not\n    presented in Mielke et al. (2019) (since Mielke et al. (2019) did\n    not use conditional language modeling).\n\nThe main findings of Mielke et al. (2019) that I listed above are also\nobserved in this work. And this work also has the following new findings\nthat are not presented Mielke et al. (2019):\n\n1.  They find that for transformer conditional language models, the\n    difficulty of modeling a certain target language is independent of\n    the source language.\n\nWhile the experiment settings are slightly different, from the aspects\nof the model architecture and evaluation metrics, the core of the experiment\nand the main takeaway does not make me learn substantially new things. I\ndo recognize that it is important to empirically verify the results of\nMielke et al. (2019) on transformer-based conditional language models,\nbut I have reservations about whether the contribution of this work is\nample enough.\n\nAdditionally, I think this paper will definitely benefit from a more\nthorough discussion of their difference with the previous works.\n\n### Experiment settings are not clear enough to follow\n\n-   In the \"Disparity/Inequality\" part in Section 2, the paper states\n    \"Each $l_{src}$ or each $l_{tgt}$\n    consists of scores from all models trained across various sizes and\n    directions.\" I have difficulty understanding how the comparison\n    tests can be done. In my opinion, this is the most novel setting in\n    this work, and I would like to see more elaboration! Since for each\n    source/target language, there are scores for different directions,\n    dataset sizes, random seeds, and datasets, how is the comparison\n    between the \"distribution of the scores\" done? As far as I\n    understand, the \"distribution of the scores\" of a language will\n    consist of scores obtained under different settings, and I am not\n    sure if comparing the distribution as a whole is rational. Since the\n    score distribution under certain factor fixed (for example, when the\n    dataset size is fixed) may be statistically significantly different,\n    but when all the factors change together, we cannot tell the\n    influence of each factor.\n\n-   Is there any reason why not running the $10^6$ for setting\n    A3-7, B1 & C1.\n\n-   I don’t understand how the random seeds are controlled in their\n    experiments. Is the random seed only related to how the model is\n    initialized? Or the random seed will also affect how dataset B and C\n    are sampled? Why is there not a C(0) model (in appendix A). I think\n    for different data sizes and datasets, they should have a common\n    random seed, and the experiments should be run using multiple\n    different common random seeds.\n\n-   I have difficulty understanding what \"transformer-preprocess d;\n    transformer-prostprocess drn\" in appendix B is. I have never seen\n    this parameter in the context of transformer-based language models.\n\n### Some words or statements are ambiguous\n\n-   In Section 1.1, \"favoring of certain outcomes\". I do not understand\n    what \"certain outcomes\" are in this paper after I read the whole\n    paper. Do the authors mean \"better at modeling certain languages\"?\n\n-   In Section 1.2, \"bounded by representation level\". I am not sure\n    what \"bounded by\" means here , and I am skeptical about the phrase\n    \"bounded by\". Since the experiments only use a set of\n    hyperparameter, the claim that \"representation bounds hardness\" may\n    be too strong. May be the hyperparameter just don’t work for certain\n    representation.\n\n-   Section 2, in **Conditional language modeling**: \"without performing\n    any downstream translation\". What is \"downstream translation\"? This\n    seems not to be a common phrase in the context of machine\n    translation, and may need some explanation. Do the authors try to\n    refer to something like beam search?\n\n-   Section 2, in **Conditional language modeling**: \"To explicitly\n    focus on modeling the complexities that may or may not be intrinsic\n    to the languages\". I don’t understand why this paper is able to\n    explicitly focus on \"modeling the complexities\" under their setting.\n    I also don’t quite get what is intrinsic to the languages and what\n    is not.\n\n-   Section 2, in **Controlled experiments as basic research for\n    scientific understandings**: \"that make language data different from\n    other data types\". I fail to understand what \"other data types\" this\n    paper is trying to refer to. I also don’t see the connection between\n    the relationship between the quoted line above and \"data-centric\n    approach\".\n\n-   Section 5, \"disparity is not a necessary condition\". What is the\n    necessary condition for?\n\n### Some claims in the paper are not well supported by the experiments results\n\nFor example, the paper writes in page 6 that \\\"on the byte level,\n$AR_{trg}$ and $RU_{trg}$ display non-monotonic and unstable\nbehaviors\\\". I am not sure what the \\\"non-monotonic behaviors\\\" the\npaper is referring to, but I think in Figure 1(e), not only AR and RU\nare non-monotonic, all results in Figure 1(e) are not monotonic. All of\nthem decay from $1e-02$, reach the lowest perplexity at $1e-04$, and\nrise afterward.\n\n### Miscellaneous concerns\n\n-   I am concern about the title \\\"Fairness in Representation\\\". First,\n    since the word fairness is mostly used to refer to societal biases\n    in NLP community, I am not sure if the word 'fairness' may be\n    misleading at the first sight. Maybe statistical bias will be a more\n    better choice. Second, I am not sure what \\\"Fairness in\n    Representation\\\" means; it seems to refer to the performance\n    disparity between different representation levels. But since the core\n    problem in the paper is the disparity between different languages of\n    different morphological complexity, I guess the whole paper is more\n    like \\\"Transformer's Fairness for Different Languages\\\" instead of\n    \\\"Fairness in Representation\\\".\n\n-   Some passages seem to be not highly related to the experiment\n    results. For example, in page 6, I don't see what the passages \\\"But\n    this is often \\... and accepted in NLP.\\\" has to do with the\n    experiment result in this section.\n\n-   I am concerned about using Pinyin in Chinese. Will single Pinyin\n    correspond to multiple characters in Chinese? If so, then changing\n    the target from character to Pinyin does not merely change the\n    vocabulary size and makes the whole comparison unclear. Whether\n    Pinyin can be an alternative for character needs to be carefully\n    discussed.\n\n-   I am concerned about the paper's only using a set of hyperparameter. I\n    understand that using the same set of hyperparamater is for\n    \\\"controlled experiments as basic research for scientific\n    understanding\\\", but sometimes the paper attributes the performance\n    fluctuation to the fact that they did not search for better\n    hyperparameter. This seems contradicting, and one can question that\n    the results in the paper will not hold if we had searched the\n    optimal parameters for different language pairs, dataset, data\n    sizes.\n\n-   If the goal is to compare morphologically rich languages (AR, RU)\n    and frugal language (ZH), then why does the paper also compare the\n    other three languages in United Nations Parallel Corpus? What do\n    their results tell us? What is the morphological complexity of the\n    remaining three languages?\n\n-   The languages used in this paper is not too many. The conclusion of\n    Mielke et al. (2019) are based on 21 languages, I am not sure\n    whether the conclusion of this paper will still hold if more\n    languages are included.\n\n-   If in 2(b,d), only ZH is changed, I think it is better to only show\n    the result of ZH as a target since all remaining 5 subsubfigures are\n    the same as in 1(d).\n\n### Presentation problems\n\nThe figures are not easy to read. Figure 1, 2, 3 are too small and are\nunreadable on A4 paper. The problem of the figure hinders my\nunderstanding to some extent. For example, in the second last line in\npage 7, it is said that \\\"AR/RU \\> ES/FR \\> EN/ZH\\\", I cannot arrive at\nthe conclusion since Figure 1(c) is too small to read. While the enlarged\nversions of figures are provided in the appendices, I still find them not\nso useful for some cases; for example, the standard error bars in Figure\n4 EN are overlapped with the lines.",
            "summary_of_the_review": "This paper studies the fairness of transformer models to model languages\nof different morphological complexity. This work extends the previous\nobservation from Mielke et al. (2019) and show that similar results hold\nfor transformer-based conditional language models: morphological\ncomplexity is not indicative of the difficulty for transformer to conditional\nlanguage model, and representation level matters more. This indicates\nthat transformers are **fair** towards the languages tested in this\nwork, in terms of conditional language modeling. They propose to use\nconditional language models to make it easier to compare the difficulty\nof modeling a specific target language, and the usage of statistical\ntests for testing performance disparity among languages is a novel act.\nHowever, experiment settings and findings/results resemble the work of\nMielke et al. (2019) to some degree. The part on statistical testings is\nsomewhat baffling and some of the statements are ambiguous. The figures\nfor the main experiments are not very easy to read. The title is also\nsomewhat not quite indicative of the content (while it does seem\nrelevant), mainly because of the term \\\"fairness in representation\\\".\n\n## Update on Nov 22\nAfter a thorough and in-depth discussion with the authors, I upvote the score from 3 (reject, not good enough) to 5 (marginally below the acceptance threshold), meaning that I am still inclined to reject.\nI change my score mainly because the authors have addressed many of my initial concerns, including the experiment settings, some odd wordings or claims, and the presentation problems.\nBut I still do not find the paper is good enough to be accepted. \nI am still not convinced by the authors' claim of their novelty; the novelties listed by the authors seem to be more of \"something that was not being done in prior works\", but I am not sure how those \"something\" can much advance our understanding on this topic.\nOverall, I think this paper presents some nice and well-controlled experiments, but the interpretation of the results tends to be too high-level and might be overstating; I often have difficulty linking the quantitative results and qualitative interpretations.\nI am not sure what I have learned from this paper.\n\n\n## Edit again on Nov 22\nI will upvote my score to 6. \nAfter going over the paper several times, I think there indeed are certain empirical contributions of the paper. \nI think varying the data size is an important experiment that has yet to be presented in any prior works and is valuable to be empirically verified, I do agree with the authors on this.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper explores the performance of conditional language models for\ndifferent languages with the aim of establishing \"fairness\" in NLP\nsystems across languages. In particular, the paper focuses on\nperceived difficulty of \"morphologically rich\" languages for NLP.  The\npaper presents a series of experiments with 6 languages in the UN\nparallel corpus, using different units (words, sub-words, characters,\nbytes) for training the conditional language models. The paper\nconcludes with the general observation that the use of characters or\nbytes as units instead of words removes (or reduces) the performance\ndisparity between different languages.\n",
            "main_review": "Existence of \"inherent\" difference in difficulty of NLP for different\nlanguages is interesting for theoretical reasons as well as the\napplied NLP. The paper also presents a large number of experiments in\na controlled way for this purpose.\n\nAlthough I agree that the topic is important, and the study covers a\nfair amount of experiments with sound methodology, I do not think we\nlearn a lot from the present paper: (1) the effectiveness of sub-word\nunits for morphologically rich languages is expected, and (2) the\ndomain of application (conditional LMs) is rather narrow. Given the\nissue is studies using (non-conditional) LMs earlier (as the paper\ncites), the benefits of narrowing it further down to conditional LMs\nis not clear to me (after reading the paper). The issue with word\nsegmentation in Chinese is also studied before, and I do not see any\nadditional information the present paper provides.\nBesides the main above point, which is the basis of my main evaluation \nof the paper, there are a few minor issues I list below.\n\n- The paper is written in a mainly error-free English, but it is\n  rather difficult to read due to the organization and the flow of the\n  concepts/text. Although I do not have many concrete suggestions, the\n  this prevented me from understanding some simpler issues at first\n  reading. Examples include:\n    - There are some unclear statements like \"To which extent is\n      morphological complexity necessary in computing?\" (p2): this\n      probably means \"handling/modeling morphological complexity\" but\n      literally, \"need for complexity\" is not something anyone would\n      claim to be necessary.\n    - There are divergences from the main topic of discussion, for \n      example \"Our conventional way of referring to “language” (as a\n      socio-cultural product or with traditional word-based\n      approaches, or even for most multilingual tasks and\n      competitions) is too coarse-grained ...\" (p5) is just to general\n      to substantiate here while defining the training set size in the\n      previous context. The same goes for the discussion of ZH as\n      \"high-resource\" language on page 6: it is not clear to me why\n      this is relevant here, and what is the relation to the overall\n      aim of the paper.\n\n- Overall, the paper feels too dense. In my opinion, paper could\n  benefit from presenting less, explaining the main matters more\n  clearly. For example, I do not fully see the utility of the\n  byte-based models, especially that the paper goes into resolving\n  practical issues due to code-set differences among languages. The\n  additional data may be of interest for a small subset of the\n  audience, but for the most, this is not relevant for the main\n  question tackled. Leaving some of these data/exposition to appendix\n  may allow allow clearer exposition of the main points.\n\n- The figures, even the enlarged ones in the appendix, are rather\n  difficult to read: too many lines/patterns and too small fonts. No\n  concrete suggestions, but a clearer display of the results would\n  help the reader a lot.\n\n- \"Summary of findings and insights\" is too detailed at this point. A\n  more concise, general version would be much better. Many of the\n  references/notation here is not possible digest before reading the\n  rest of paper.\n\n- Although the data used is a standard data set, it would be useful\n  for most readers to have more detailed description of the data\n  (e.g., the type of documents, size of the overall data that the\n  parts used are sampled, ...). It would also be good to indicate what\n  a \"line\" corresponds to (is it a sentence, a paragraph, or some\n  other unit?).\n\n- Footnote marks should be place after punctuation.\n\n",
            "summary_of_the_review": "Although I agree that the topic is important, and the study covers a\nfair amount of experiments with sound methodology, I do not think we\nlearn a lot from the present paper: (1) the effectiveness of sub-word\nunits for morphologically rich languages is expected, and (2) the\ndomain of application (conditional LMs) is rather narrow. Given the\nissue is studies using (non-conditional) LMs earlier (as the paper\ncites), the benefits of narrowing it further down to conditional LMs\nis not clear to me (after reading the paper). The issue with word\nsegmentation in Chinese is also studied before, and I do not see any\nadditional information the present paper provides.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}