{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents a novel framing of what's at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small-scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides an argument for reconsidering how we construct a single training example during LM (pre)training. It gives a formal proof that dependencies between two texts are vanishingly weak when the texts are presented in separate training examples (but not when they are in the same training example). Intuitively, in order to make cross-textual inferences between texts in different examples, a model must be able to access information stored in its weights, but commonly used learning rates and model architectures ensure that the updates to the weights are too small to recover this information.\n\nThe paper suggests remedying this problem by changing the way in which we construct training examples. Rather than obtaining examples by separating texts into contiguous chunks, we can construct examples from multiple semantically related chunks found by computing the similarity of text embeddings in the training corpus. Empirical results are presented which show that task-adaptive (i.e. post-pretraining) training on such data leads to greatly improved performance on zero-shot semantic similarity. Further results show that doing pretraining with these kinds of examples improves zero-shot QA performance by a small margin (from an extremely low baseline).",
            "main_review": "The main results of this paper are of broad interest. Questions about how the nature of the input to LMs affects learning are important and poorly understood. The claim that the within- and between-example is crucial is novel and thought provoking.\n\nThe arguments are highly technical and require a lot of expertise, (including familiarity in particular with Levine, 2020, on which the proof builds substantially) to understand. This is not necessarily a strength or a weakness---there is great value to developing techniques for making this kind of formal argument. However, I find it difficult to follow even the thread of the argument at an intuitive level, which is a shame because it's interesting to me. I suspect I'm not the only potential reader who will have this problem. The paper could reach a much wider audience (which I think would be an improvement) if it provided more intuitive paraphrases of the main results in Section 2.3. \n\nThere are a few unaddressed issues with the authors' main suggestion: that we can improve LMs’ abilities to make cross-example connections by constructing examples out of interleaved texts. Assuming a fixed example length, this means portions of text from the same source that otherwise would have been in the same example in the standard chunking scheme would now be separated. However, it seems that, in general, dependencies within a text are more important than dependencies between random semantically related sentences from different texts. Another problem is that a language model trained on this kind of interleaved text would presumably also generate less coherent texts.\n\nI also have some doubts about the empirical results. The results about kNN-task adaptive pretraining seem to have a confound: The method for retrieving data for this intermediate training step involves finding sentences with high similarity, but the task on which this method is shown to lead to improvements is itself a semantic similarity task. The claim is that the method should be helpful for many kinds of tasks, but my intuition is that this \"coincidental\" alignment of the data collection and the evaluation task makes the result less generalizable.\n\nThe empirical results about zero-shot QA don't help much because the task is clearly too difficult. The improvement on the models trained with the new method is from F1 of ~0.001 to ~0.01. Do larger LMs succeed at this task? If so, then it seems that models tested in this paper are just too small or ineffective for the task. Is there an easier task where this method leads to strong performance but ordinary pretraining does not?\n\n",
            "summary_of_the_review": "This paper makes an intriguing claim about the effect of the input on LM learning, which is backed up by a formal argument. These kinds of arguments are valuable. However, the argument is difficult to follow for a non-specialist audience. Also, the empirical results the authors report have some confounds due to questionable task selection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses the bias induced by the inconsistency between the pretraining examples and downstream examples. Specifically, the text is continuous during pretraining while could be non-neighboring in downstream tasks. The authors proposed two methods: kNN-TAPT and kNN-pre-training in Task Adaptive Pre-training (TAPT) step or general Pre-training step respectively, where each training sample is composed of a sentence and its semantically closed neighbors (by kNN search). The method is effective in sentence similarity tasks (in kNN-TAMP scenario) and closed book open domain QA tasks (in kNN-Pre-training scenario).  The paper also gives a theoretical analysis of the so-called in-context bias,  by quantifying the NLM’s ability to model dependencies between two sentences that appear in the same training example (the in-context representation) and in different training examples (the sequential representation).",
            "main_review": "Strength:\n1.\tThe authors analyze the in-context bias of the self-attention model, which could inspire some research works on designing training examples.\n2.\tThe authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks.\n3.\tA theoretical analysis of the in-context bias.\n\nWeakness:\n1.\tThe introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning.  The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.”  Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.\n2.\tThe theory is a bit complicated and not easy to follow. \n3.\tThe experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.\n",
            "summary_of_the_review": "The paper addresses the inductive bias of in-context learning. The studied problem is meaningful and could be a potential breakthrough for pretrained language model. However, the written of the paper is a bit hard to follow and the experiments is somewhat limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors show empirically and theoretically that transformer based language models cannot effectively exploit dependencies between sentences in different inputs. For the theory part, they introduce a new measure called \\eps-sep rank. A function is said to have low \\eps-sep rank if it can be approximated up to error \\eps by a function with low separation rank. Bounding the measure shows that if learning rate is low, the information from different training instances cannot be well integrated. Empirically, they present experiments where the task specific pre training was altered to include non-task specific similar sentences in the context of the LM. They show that augmenting context of pre training inputs with related similar sentences from Wikipedia helped improve end performance on similarity and zero shot open domain QA tasks. ",
            "main_review": "Strengths: 1. Some experimental results are interesting. Simply adding related sentences in the pre training input context helps end performance.\n\nWeaknesses: 1. I think the presentation of the paper needs to be improved. For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper. I felt similar conclusions can be drawn from the results of that paper as well. It will be good to rewrite highlighting the contributions. I also noticed a lot of repeated text in section 2 from Levine 2020 paper; will be good to modify.\n\n2. The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training. I felt this was quite separate from the theoretical analysis. It does not follow from the theoretical results that adding similar sentences will be a good thing. As a result, having these in the same paper looked incoherent to me.\n\n3. The experimental results are weak. The gains are not super high. I will be more convinced if evaluation is done on a wider range of tasks.    ",
            "summary_of_the_review": "I was not clear about the impact of the theoretical analysis presented, compared to past work. It will be good to rewrite the section clarifying the contributions and novel conclusions from the analysis. The empirical section sounded disparate from the theoretical part of the paper, and the results are weak. I suggest evaluating on a wider variety of tasks. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a theoretical analysis of the strengths of including similar input examples S1 and S2 into the same actual transformer input. The analysis contrasts using examples of concatenated S1;S2 with running the two in separate inputs. The paper then proves empirically that including related examples in the same input (via a method the article calls KNN-pretraining) allows transformers to learn much faster in cases when cross-document dependency is relevant.",
            "main_review": "Strengths:\n * Clearly written paper\n * The theoretical contributions will have a high impact on training transformer-based models\n * The theoretical analysis is supplemented by experimental analysis\n\n Weaknesses:\n  * No major weaknesses\n\n  \nSome minor aspects:\n * \"correct on less than 50 questions out of 20, 000 in the evaluation set\" – Separating thousands by commas (American system) is ok. Separating them by space is also ok. Doing both is not.\n * \"correct on roughly than 250 questions in the evaluation set\" – \"than\" shouldn't be there\n * Equation 4 would have been easier to read if the right-hand side was expanded to S1 and S2's `{w_2^j}` notation from the previous line.",
            "summary_of_the_review": "Despite its theoretical focus, the paper is easy to read and to follow. The theoretical analysis is supplemented by empirical results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper formalizes the notion of an in-context bias---the pre-trained neural language models are better at modeling dependencies that appear within one contiguous pre-training text chunk, rather than those that appear in different pre-training text chunks. The authors perform a theoretical analysis of this phenomenon and link the inability to model dependencies between different examples with the models' low-pretraining learning rate. The authors then propose two methods for improving pre-training example design by adding sentences that are similar (KNN), but from different documents. They empirically show that this works better than reasonable baselines on sentence similarity tasks (e.g., STS and SICK), as well as NaturalQuestions.",
            "main_review": "I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand what is going on with pre-trained language models, why they work well, and what might we be able to improve from theses insights.\n\nStrengths:\n- Thorough theoretical analysis that reveals the connection between (practically-necessary) small learning rates and inability to use dependencies across text chunks\n- Useful framing and discussion of the \"in-context bias\", where models are more likely to learn dependencies within text chunks seen during pre-training.\n- reasonable initial experimental results demonstrating some ways to help models better use cross-text-chunk dependencies (put them into a contiguous text chunk), providing some hope that these results could make models better.\n\nWeaknesses:\n- I felt like the empirical validation could have been stronger. The only two tasks examined are sentence similarity tasks (which seem a bit more like a sanity check), and NaturalQuestions. It's particularly surprising to me that this works so well on NQ, and I wish the authors had dug a bit deeper into this, but I also recognize that page limits exist.\n- Where else do you think the in-context bias could be useful? It wasn't intuitive for me that it'd be useful for NQ.\n- Do you have any initial experiments on the \"self-improving\" aspect of this technique? This is mentioned several times, but there are no initial results or anything suggesting that it might be a promising direction to pursue.",
            "summary_of_the_review": "I thought this paper was interesting and brings a new perspective to practical design decisions used in pre-training language models. I think this paper is relevant to the ICLR audience, and that said audience would be excited to know about it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}