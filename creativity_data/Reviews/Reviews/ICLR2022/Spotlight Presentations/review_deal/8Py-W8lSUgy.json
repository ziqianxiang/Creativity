{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper describes a novel learning scenario where there are many related tasks, some seen at test time, and some seen only at training time, where additionally the task labels can be hidden or present.  This approach generalizes both a \"relational setting\" (where auxiliary task labels could be used as features) and a \"meta setting\" (where new tasks need to be solved in a zero-shot setting using data from related tasks only).  The idea behind the method is to do MTL with a common representation and a set of task-specific heads, and build a graph where (1) tasks are nodes associated with the parameters of their task-specific \"heads\" and (2) edges link examples to tasks with known labels.  A GNN method is then used to find regularities in the graph.\n\nPros\n - The setting is innovative and the approach is novel\n - The experimental results are strong\n\nCons\n - Some of the terminology seems awkward and/or strained (eg \"knowledge graph\" for the task-example graph)"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a relational multi-task learning setting and design a MetaLink model.",
            "main_review": "According to the setting introduced in Section 2, it seems that the proposed setting is just a meta learning setting but not a multi-task learning setting. The name of relational multi-task learning seems a bit misleading.\n\nIn the first paragraph of Section 3.1, the claim is not general, since in many cases, the task head consists of multiple layers instead of only one layer. In the multi-layer case, based on the second paragraph of Section 3.2, the proposed MetaLink model seems not applicable.\n\nThe knowledge graph stated in this paper is just a graph. This is different from the actual ‘knowledge graph’ and this seems misleading.\n\nThe heterogeneous GNN used in Section 3.2 seems no difference with the conventional GNN. I did not see significant novelty in the proposed MetaLink model.\n\nIn experiments, only multi-label datasets are used. Authors should experiment on multi-task benchmark datasets such as CityScape, NYUv2, Pascal-Context, Taskonomy, Office-31, Office-Home.",
            "summary_of_the_review": "The proposed setting seems a bit misleading. The proposed model has no significant novelty. More standard benckmark datasets should be used.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a novel multi-task learning framework called MetaLink that takes the opportunity to utilize the auxiliary task labels by constructing a knowledge base with the tasks and data(instances) as nodes and auxiliary labels as the edge labels between them. This method shows significant empirical success ",
            "main_review": "Strengths:\n1. MetaLink is able to leverage the correlations among tasks successfully.\n2. I find the idea of creating the knowledge graph for harnessing the relational information about the data and tasks very intreging. \n3. The usage of the labels of the auxiliary task is well motivated.\n\nWeakness: \n1. The writing could have been more precise in some parts of the paper.",
            "summary_of_the_review": "Concerns:\n1. The introduction could have been more concise, I feel the authors could have avoided explaining the technicalities of the knowledge graph in the introduction. Rather, it would help the reader to be better situated in your research space if you discuss the fundamental difference between some of the related models and yours at an early stage of the paper. For example, the authors could have mentioned Wang et al. 2016 and ML-GCN  (Chen et al.) in the introduction itself. and explained why their work is different from the others. \n\n2. Are the comparisons fair? The GraphConv( ) step uses additional parameters, especially when the number of layers (iteration) increases. Could you include the number of parameters for all the baselines and your method?\n\n3. The tasks are very specific to multilabel classification. What happens when objectives are different for each task? How would MetalLink adapt to that?\n\nminor: In the paragraph: \"Results on biochemical datasets.\", Table 5 ---> Table 1.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper exploits the multi-task modeling using a heterogenous Graph Neural Network. The key contribution of the work is having both data (sample) and task nodes in the same graph, focusing on data-task relation, accommodating for sparse task labels by design.\n\nExperimental results show relevant improvements with the proposed relational model on biomedical datasets and Imagenet splits for few-shot.\n",
            "main_review": "Strengths\n- Innovative formulation\n-- a wide “range of heterogeneity” in the graph: a node for each sample in the support set a node for each task - backed up by relevant experimental improvements\n-- dynamical graph structure based on the existing labels for the current entry\n- Experimental results are strong for the few-shot setup and medical datasets\n- Ablations are very informative: higher improvement in more correlated tasks and using more auxiliary tasks come with higher performance\n\nWeaknesses\n- Table 1. Why are there so many variations of the KG layer (both inside the same dataset and per metric)? Is there some consistent explanation for this hyperparameter?\n- Even though it is quite simplistic, Figure 2 is confusing. The authors should highlight somehow that the edges show the conceptual connections between nodes (and not the real link in the algorithm flow). Such an example: in c) and d) why are data nodes (the ones from the triangle structures) useful if they are already connected via known label edges?\n- Ablations on the graph structure decision would be useful (the ones from eq. 2 and 3, sustaining the “special GNN design”)\n- Prior work is focused on GNN approaches (even in the multi-task paragraph), comparison from the design point of view with prior-art approaches in relational multi-task graphs could offer a more complete view (eg. Robust Learning Through Cross-Task Consistency - https://arxiv.org/abs/2006.04096, Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift - https://arxiv.org/abs/2103.14417)\n\nQuestions\n- How many parameters does MetaLink learn?\n- How could the authors adapt the proposed MetaLink to work with dense predictions rather than 1-D tasks? What would be the implications?\n\nOthers (minor, eg. typos)\n- KG layer (knowledge graph) is never expanded\n- Please mention what you show in the tables (top5 performance)\n- “where T = {1, 2, ..., m} is integers between 1 and m.”\n- Eq. 1 should have an (l-1) instead of (l) - it uses only h(l)\n",
            "summary_of_the_review": "The paper is sound, introducing and validating an interesting heterogeneous graph design focused on each task and data sample as a different node. The work also offers an approach for the common labeling limitation in multi-task problems (by working with sparse labels in a dynamic graph architecture ). I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, they introduce relational multi-task learning in which they construct a graph with the data points and task and labels as edges. They represent the data points as the embedding of NN model and task as the last layer of NN model for that task. Then they solve the link label prediction problem between each node and task by using GNN and heterogeneous message passing method to predict the label of test data point on a new task at inference time.",
            "main_review": "Strengths:\n- The paper is very well-organized and well-written. Algorithm 1 is self-explanatory.\n\n- I really liked the idea of creating a knowledge graph and then solve the link label prediction problem.\n\n- The experiments are thorough, I enjoyed reading the experiments section; they cover ablation studies, and answer several question in there.  Specifically how relational ML could take advantage of more related tasks. In addition, the improvements are noticeable (Tables 1-4).\n\n\nNotes:\n- In figure 1, in the right panel, I would put 1's and 0's in a right place, e.g., 1's for the solid lines or 0's with the dashed line?!\n\n- I know that I have seen figure 2 somewhere before, but do not remember the paper, if you have seen this figure from another paper, that will be a good idea to put the reference.\n\n- I would recommend defining use the name of your model in figure 4 instead of using 'ours'",
            "summary_of_the_review": "In general, I believe the paper is innovative and descent. The backed their proposal with thorough experiments. I think this work will be useful for researchers working in the area of multi-tasking.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}