{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large-scale point clouds. \nAll reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer's scores."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The submitted paper studies 3D scene geometry reconstruction and proposed a pipeline for predicting signed distance fields from incomplete pointcloud data. The pipeline consists of two major components: an autoencoder that maps between the latent space and the voxel representations, and a Continuous Generative Cellular Automata (cGCA) which progressively and stochastically generates more complete data at each step. The proposed method is trained and evaluated on several synthetic datasets, including ShapeNet Scene, 3DFront, as well as ShapeNet, with a controlled sub-sampled pointcloud as input (with guaranteed rate of points for each object). Under these settings, both quantitative and qualitative results outperformed prior work. \n",
            "main_review": "Strengths:\n1. The proposed method is clean and simple. The key component, cGCA, is a relatively straightforward extension of the prior work GCA to include latent embeddings and to use signed distance fields, but it is still brings enough novelty and contribution.\n2. Being able to generate stochastic outputs is a significant advantage compared to prior work.\n3. Under the experimental setup, the proposed method clearly outperforms prior work\n\nWeaknesses:\n1. Lack of experiments on real-world datasets: despite authors referred to this as one of the future directions, I think it is an important experiment to include as part of this submission. The noise statistics can be quite different, which could potentially break the proposed method.\n2. My above concern is compounded by the fact that the authors were using a specific subsampling strategy to create synthetic inputs - i.e., each object has a guaranteed rate of surface points. This is significantly different from real-world pointcloud distributions. Are the prior work also re-trained specifically for this subsampled inputs? I would like to know how the method performs under uniform subsampling.\n3. [Minor] In Equation (3), is $Q$ not defined? ",
            "summary_of_the_review": "The authors' response and Appendix F & G address my concern regarding real-world experiments, so I am raising my ratings to \"8: accept, good paper\".\n\n> [Original Review]\nMy current recommendation is \"6: marginally above the acceptance threshold\", given the simplicity of the method and its results on synthetic dataset. I am concerned about its actual performance on real-world data, and I am happy to increase my rating if the method demonstrates similar improvement over prior work on real-world datasets. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Summary: The paper presents a shape completion method for large-scale 3d scenes. The method, called Continuous Generative Cellurlar Automata (cGCA)) is probabalistic in nature and produces multiple plausible completions for a single scene. The method employs leverages previous work in the form of key-buiilding blocks - Generative Cellular Automata, sparse voxel embeddings, and neural distance fields. Combining these approaches leads to impressive performance on recent 3D scene datasets - ShapeNet Scene and  3DFront. On ShapeNet the model outperforms Generative Cellular Automata (GCA) on reconstruction metrics, but not diversity metrics.\n\nThe main contribution of the paper to me is to realize that instead using a fixed grid like GCA to create voxel embeddings  leads to n^3 cost for the volume. In constrast, the authors use a sparse voxel embedding. They decode these embeddings with the method of  Chibane et al. to create a complete 3D shape. With this change, the method can scale to large scenes like from ShapeNet Scene and 3DFront, whereas GCA was limited to only small volumes(objects) of ShapeNet.\n\n",
            "main_review": "Writing: The paper is well written and complete. The abstract lays out the structure of the paper and the text matches mostly up to the expectations set in the abstract and introduction. I did not find any obvious error. The contribution are explicitly stated and thereforce can be easily verified. The mathematical notation once introduced is static and easy to parse. The figures are sufficiently high resolution and demonstrative of the improvements due to the proposed method.\n\nImplementation Details: I believe that the implementation is sufficiently detailed for a reader to be able to implement the algorithm - the previous models and their hyperparameters are described as are the datasets and their preprocessing steps.\n\nContribution: The paper extends GCA by using sparse voxel embeddings and a Distance Field method from Chibane et al. I think the contribution is pretty significant as the formulation under the new regime is vastly different from the old GCA in terms of implementation and engineering. The authors I think conclusively show that the new method is better performing on large scenes (Tables 1 and 2), while being competitive for single instances of 3D ojects (Table 3).\n\nComparisons: One could always ask from more comparisons, but I believe that the baselines are sufficient with two large scale methods - ConvOcc and IFNet, and comparison to the baseline GCA. Although it would be interesting to see how the authors managed to train GCA on such large scenes.\n\nProofs: The mathematics in the main paper was was checked and passed my review. However, I did not check the proofs in the supplementary.\n\nQuestions:\n(Infusion Traninig): I am not entirely sure how the Infusion kernel is different from the infusion chain in GCA apart from the problem of having o_c inthe generative model itself instead of being defined on a grid. I would like to hear more explanation from the authors as the final update rule (pg 5, training procedure) is the same as the training procedure of GCA (Algorithm 1).\n\n(Other methods): The authors do not make it clear how the other methods' performance was assessed - was it from the reported number or retrained with their preprocessing.\n\n(Mode Seeking): The authors do not provide a model without mode seeking. It would be interesting to see the performance from such a model.\n\n\n\n\n",
            "summary_of_the_review": "Summary of review: The paper is very well written with clear mathematics and clear delineation of contributions. The authors present a modification that makes older work scalable and show good results on multiple datasets, competing against the baseline method as well as other methods.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an approach for scene completion which allows for sampling plausible completions from a learnt probabilistic model. The idea is to learn the state transition of a Markov Chain and use this during inference. The proposed formulation is based upon the Generative Cellular Automata where the representation of the surface is a discrete occupancy grid. The extension proposed in this work is to instead of using just voxel occupancy probabilities for the voxels also learn a shape code which can be decoded into a continuous surface, which is named sparse voxel embedding. The state transition is learned using an adapted version of infusion training.",
            "main_review": "Strengths:\n- Shape completion is an important feature AR/VR, robotics and other applications \n- The method is able to produce various plausible alternatives for shape completion\n- The surfaces are continuous and quantitatively more accurate than the voxelized versions\n\n\nWeaknesses:\n- The benefit in terms of accuracy compared to the voxelized versions is coming at a cost of lesser diversity. Especially the conditioning on the initial state which helps a lot on the accuracy decreases diversity.\n- In Fig. 3 shape completions with various amounts of sparsity in the input data is shown. However, they are all in a fairly dense range. What I would be interested is how would this model behave if the input becomes really sparse. When will it break down or will it still produce something meaningful with almost no input? On the other side it would also be interesting to know what happens if the input is very dense and the completion result is not ambiguous. I feel these would be interesting evaluations to better understand the limitations and strengths of this approach.\n- From reading the submission I was not able to understand how seams between the voxels are prevented. It seems that there is a risk that such seams could be visible? Would they become visible if the input was sparser?\n- It seems like the fact that during inference 30 rounds of state transitions needs to be computed would mean this method is significantly slower than the baselines presented in Fig 3. What are the runtimes of the proposed approach?\n- One line of works I feel could be added to the related work is 3D shape representations based on octrees. I feel this would be an alternative formulation to the sparse voxel embedding to get the resolution of the shape higher than coarse voxels.",
            "summary_of_the_review": "The paper presents a method which is able to produce multiple plausible shape completions of a sparse input. The main novelty is adding a shape code to the voxel which allows for decoding continuous surfaces. The training procedure of the state transition was adjusted accordingly. The results look promising quantitatively and qualitatively. The submission has a few weaknesses but I think the approach is novel enough and shows quantitative improvement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about scene completion given partial observations. The main technical part is built upon on prior work Generative Celluar Automata.(Zhang et al, 2021). GCA can be considered by recurrently applying a uniform convolutional operation on the top of current state, similar to the diffusion process. This work augments GCA by introducing a latent code for each voxel, instead of only a single 0-1 occupancy value. This latent code is used to generate fine detail implicit surface by querying a coordinated-based MLP inside each voxel. By combining the neural implicit function representation and GCA, the proposed method, namely cGCA outperforms prior works in terms of both scalability and fidelity.\n\nAs both the initial state and final state of cGCA is voxel-based representation, while the scene is represented as a continuous function or dense point cloud, an encoder-decoder network is used to convert the scene into voxel representation and vice versa.\n\n\n========\nDongsu Zhang and Changwoon Choi and Jeonghwan Kim and Young Min Kim, \"Learning to Generate 3D Shapes with Generative Cellular Automata\", ICLR 2021",
            "main_review": "Strengths\n\n1. The main novelty in this paper is the combination of a discretized voxel representation and implicit function representation in scene completion. Although both of them has been exploited before, combining them for the task of scene completion is a very natural and elegant. The key idea of this paper is augmenting the voxel with features that represent detailed local structures.\n\n2. The proposed PointNet based encoder and implicit function decoder is new and inspiring as an autoencoder for dense point clouds or surface.\n\n3. This paper delivers proof and derivation for the continuous version of GCA. Although the main chunk is similar to that of GCA, extending it with local features is non-trivial.\n\nWeakness\n\nMy main concern is the choice of GCA as the operation for the voxel stage. As far as I know(maybe I'm wrong), GCA is still new and not  widely tested. However, I think the autoencoder proposed in this paper is irrelavant to the specific voxel-based operation, thus can be easily combined with other 3D object completion method that use also voxel-based operation, such as those baselines compared in this paper. While as the discussion of the limitation of GCA is out of the scope of this paper, this is not a big issue. But I think maybe the author can focus on the idea of augmenting voxel-based representation with local features when presenting the paper.  For example, I think Sec.3.1 should be much more importance then Sec.3.3. Most of the derivations in Sec3.3 can be moved to Appendix. Again, this is my personal idea.",
            "summary_of_the_review": "From my own perspective, this paper is interesting and strong. Especially, the scene local feature autoencoder presented in this paper is inspiring. This paper presents a way towards large-scale high-resolution scene generation/completion. Although I think the structuring of the paper could be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces continuous Generative Cellular Automata (cGCA) that is a generative model for continuous 3D reconstruction / shape completion. cGCA directly builds on top of GCA in the generation process, but instead applied the sparse voxel embeddings proposed in ConvONet and IF-Net to overcome the limitation of low resolution in GCA. Moreover, this paper adapts the infusion training strategy and also verifies the progressive generation is valid.",
            "main_review": "### Strengths\n1. The authors tackle the problem of probabilistic scene completion with partial observations as input.\n2. The authors apply the recent state-of-the-art neural implicit model for this probabilistic scene completion task, and show the ability of generating high-quality plausible shapes. The idea itself is simple and straightforward, but indeed effective.\n3. The paper is well-written and easy to follow in most parts. Need to polish Sec 3.3 to make it more accessible.\n\n### Weakness\n**Why is the task itself important?**\n\nThis paper is trying to tackle “probabilistic scene completion”. Given partial point clouds, previous works (e.g. ConvONet, IF-Net) output a single most plausible shape. If you rerun the model with a different random seed, they usually also output a slightly different shape. This is also similar to my understanding of “probabilistic scene completion”. In the introduction, you did not explain why having multiple plausible shapes is interesting or important. Moreover, justify what makes your method really different from those previous works, if all methods can output slightly different outputs. \n\n**The experimental section is convincing in general, but lacks some important experiments / explanation**\n\n1. Why cGCA can produce less blurry shapes than the deterministic models as shown in Fig. 3? Please provide some explanations more than saying “coincides with the well-known phenomena”, since I don’t understand why different losses for the your generative model should perform better.  \n2. What is the model configuration that you use for the baseline? For example, for ConvOcc, do you use 3-plane model or grid model? What is the resolution for the planes or grids? What is the number of parameters in their model? How long do you train them? Same for IF-Net and your method. These network configurations can make a big difference for the reconstruction results, because a model with a lot of parameters and train for long time usually tends to produce better results.  \n3. For the mode seeking, why T’ is set to 5? An ablation study (e.g. T’ is set from 0 to 10) is necessary to show the effectiveness of mode seeking, and why you choose 5.  \n4. In ShapeNet Scene experiments, you use T=15 transitions for GCA and cGCA? For the single object completion, you use T=30 instead. Why you have different numbers for different tasks, and it is better to provide an ablation for the choices.\n5. In Table 3, please give some explanations on 1) why (w/ cond.) does not really outperform the version w/o (2 out of 3 metrics are worse). 2) why 32^3 has really similar performance than 64^3.\n6. In Table 4 in Appendix, the memory footprint for ConvOcc does not make sense. If I remember correctly, their memory usage is very constant but only the runtime increases linearly w.r.t. the number of crops because each crop is processed individually.\n\n**What are the limitations of this paper?**\n\nYou mentioned in conclusion that the results are only evaluated with synthetic scene datasets. However, that is not really a limitation. It is always necessary to specify the real limitations of your approach. For instance, to obtain a complete shape voxel embeddings, previous methods like ConvOcc & IF-Net is pretty fast because you simply need to run encoder network containing some CNNs. In your method, you requires 15-30 transitions to obtain the final embeddings, which should be at least 15-30 slower, if I am not mistaken. \n \n\n",
            "summary_of_the_review": "In general, this paper introduces an interesting way of combining neural implicit representations and generative models. The mathematical proofs seem to be sound and correct, but in the experimental part, there are some important experiments and explanations missing. I will change my score accordingly based on the reply from the authors.\n\nMoreover, the previous work GCA has not made the code public. It would be great if the authors of cGCA can open source the code to benefit the community. This is another factor for me when making the decision.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}