{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non-linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z1 and Z2 are assumed to be conditionally independent given the target label. The paper outlines two approaches to construct orthogonal classifiers that operate only on Z2. The approach is highlighted in three applications: controlled style transfer, domain adaptation, and fair classification.\n\nThe reviewers all found the proposed method to be principled and compelling. Beyond clarification questions and some discussion on related work, the reviewers raised a few issues that were subsequently addressed: 1) Additional baselines for domain adaptation and fairness. 2) Controlled style transfer being a new task with no established baselines, and 3) The feasibility of training a proper “full classifier” that minimizes the empirical risk, and its necessity in the approach. The authors addressed these concerns and updated the paper, to the satisfaction of the reviewers. All of them unanimously recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents classifier orthogonalization technique that works for non-linear classifiers. The idea is to find a method that orthogonalizes the full classifier w.r.t. the principal classifier so that the resulting classifier is statistically orthogonal to principal classifier. The algorithm turns out to be very simple, only requiring access to the full classifier $P(Y|x)$ and the principal classifier $P(Y|z(x))$, where $z$ is a control variable you want to orthogonalize against. The paper also discusses an alternative method of classifier orthogonalization using importance sampling. \n\nThe effectiveness of the proposed classifier orthogonalization technique has been demonstrated through 3 applications: controlled style transfer, domain adaptation, and classifier fairness. For controlled style transfer, authors modified the CycleGAN's generator update step by orthogonalizing discriminator w.r.t. the controlled style variable. For domain adaptation, they modified VADA, an adversarial domain adaptation approach, by orthogonalizing discriminator based on the label-based principal classifier, so that discriminator wouldn't discriminate the domain based on the frequency count of labels from each domain. For fairness, full classifier is orthogonalized by the sensitive attribute classifier.",
            "main_review": "- Strength\n  - The paper presents rather a simple, but principled and novel way of constructing orthogonal classifier for any non-linear classifiers.\n  - The paper is written with a good flow.\n  - The effectiveness of the proposed technique has been confirmed on various applications, including style transfer, domain adaptation, and fairness. \n\n- Weakness\n  - For domain adaptation experiments, it seems authors assume access to the marginal distribution of label variables for the test set. This seems additional information the proposed method is using on top of VADA. Would you please confirm? Also, what if these marginal distributions between domains are exactly the same? Are there going to be any improvement?\n  - Is label distribution of the test set for domain adaptation experiments uniform or does it follow the distribution of the target domain used for training?\n  - For domain adaptation experiments, one baseline could be post-processing the source-only model, similarly to the procedure for constructing fair classifier in Section 6.",
            "summary_of_the_review": "Overall I find that the proposed method novel and principled. As paper has demonstrated the proposed method could have great impact multiple application domains. As mentioned in the review, some information could be verified through discussion, but I am leaning towards acceptance for my initial rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Given a defined notion of orthogonality for random variables, this paper proposes a straight-forward approach to constructing classifiers orthogonal to a given classifier. Examples of mapping problems to the orthogonal classifier setting are provided for domain adaptation and fairness as well as the newly proposed problem of controlled style transfer.",
            "main_review": "Strengths:\n\n1. The paper does an excellent job of defining orthogonal classifiers and provides a clean solution to learning these models.\n\n2. The mapping of multiple different problems is extremely interesting and provides a strong example of how to apply the approach to different problem settings.\n\n3. The proposed approach appears to yield improvements for both the controlled style transfer and domain adaptation problems.\n\nWeaknesses:\n\n1. The controlled style transfer problem seems somewhat contrived. Providing a reasonable example of where this would be applicable would significantly increase the value demonstrated in this section.\n\n2. Given the controlled style transfer problem is novel, it's difficult to determine whether the empirical results are significant as the baseline compared against is designed for a non-controlled style transfer problem.\n\n3. Comparisons to a wider range of domain adaptation approaches (e.g. Cycada, HIDC, etc.) would make the results more convincing.\n\n4. Empirical comparisons to de-biasing techniques would significantly strengthen the experimental section.\n\n5. The results for fairness are somewhat uninteresting as the proposed approach simply reduces to weighting of labels for discrete sensitive attributes.",
            "summary_of_the_review": "I am overall in favor of accepting this paper. The proposed formulation and solution are well presented and the application to different problems is interesting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- The paper introduces the notion of \"orthogonal classifiers\": classifiers that rely on orthogonal variables. It starts with the simple linear case, and adds a definition that also applies to the non-linear case.\n- The paper proposes two methods to identify a classifier orthogonal to a given one.\n- It then describes 3 use cases: style transfer, domain adaptation, and fairness.\n",
            "main_review": "Strong (+) and weak (-) points\n- (+) The authors propose and define a new concept: orthogonal classifiers.\n- (+) The concept is general and justified/motivated by needs of seemingly disparate areas of research (style transfer, domain adaptation, fairness). This non-obvious connection between multiple areas is interesting in itself.\n- (+) Demonstration of empirical benefits in multiple applications.\n- (-) There are quite a few missing references to methods that also learn multiple complementary classifiers. So it's difficult to evaluate the novelty/advantage of the proposed method.\n- (-) Imprecisions in the writing (see advice/comments below). I believe these could be fixed by the authors for the final version.\n\nDetailed comments and advice for improvements (in the order of their occurrence in the paper):\n\n- In the second paragraph of the introduction:\n  - \"joint distribution\": maybe name it (P(X, Y))\n  - the standard notation P(...) = (...) might be easier to read than (...) \\sim (...)\n  - \"label-dependent\" need a hyphen\n  - \"the orthogonal classifer (...) must satisfy\": it sounds like an established concept, whereas this is something that you define for this paper. So, rather say: \"we define the classifier w2 to be orthogonal if (...)\"\n  - Given a classifier w1, there are multiple orthogonal classifiers w2, but the current writing implies there is a single w2.\n\n- The concepts of \"principal classifier\" and \"principal variables\" are used in the abstract and intro without being defined or given a reference about. Only Sec. 3 makes it clearer what they refer to. In the intro, maybe use a non-technical word like \"a GIVEN classifier\" ?\n- Fig. 1: \"principle\", typo for principal ?\n- Proposition 1: the first notation I(A,B) uses a comma, the second a semicolumn.\n- Sec. 3: \"Similar, ...\" -> \"similarLY\"\n- Sec. 3: you should define volJf right after its first use (before \"Taken together\"). The equation before \"taken together\" would also be easier to read with some spacing between the three terms.\n- The equation after \"taken together\" is not numbered. Also add spacing between terms for readability.\n- The following sentence has a grammatical issue. I don't understand its second part. What is the subject of \"ensures\" ?\n\"The validity of this procedure is subject to overlapping support of class-conditional distributions, ensures that wx;w1 remain non-zero.\"\n- How can we make sure that this condition is satisfied ? Training a \"full classifier\" does seem trivial to me. Existing work has shown that neural networks can focus on only a few predictive features (see e.g. Pitfalls of Simplicity Bias in Neural Networks, Shah et al. 2020).\n- \"would be closeD\" -> close\n- Proposition 4: \"exits\" -> \"exists\" ?\n- Missing references: Ross et al. and more recently Teney et al. also proposed methods to learn multiple classifiers that use the input data in different ways.\n  - [Ensembles of Locally Independent Prediction Models](https://ojs.aaai.org//index.php/AAAI/article/view/6004)\n  - [Learning Qualitatively Diverse and Interpretable Rules for Classification](https://arxiv.org/abs/1806.08716)\n  - In this one, the authors train multiple classifiers sequentially to focus each on different input features.\n[Right for the right reasons: Training differentiable models by constraining their explanations](https://arxiv.org/abs/1703.03717)\n  - [Evading the Simplicity Bias](https://arxiv.org/abs/2105.05612)\n  - The multiverse loss may also be related, although admittedly more distantly. It serves to increase the number of distinct discriminative directions of a learned representation by duplicating a cross-entropy loss over multiple linear classifiers with an orthogonality constraint on their weights.\n  - [The multiverse loss for robust transfer learning](https://arxiv.org/abs/1511.09033)\n  - [Maximal multiverse learning for promoting cross-task generalization of fine-tuned language models](https://aclanthology.org/2021.eacl-main.14/)\n- The first key difference with disentanglement: \"most disentanglement definitions do not take the diffeomorphism (...) into account\". I'm not sure this is correct. At least colloquially, disentanglement is about \"breaking down\" observations into independent factors that drive the generative process, hence collectively give rise the observations. In other words, the space of the union of these disentangled factors do map, via a diffeomorphism, to the observation space. What do the authors think ? Is there really a \"key difference\" here ?\n- \"Several works aim to learn the disentangled representations\": no need for \"the\"\n- Missing word: \"Density ratio estimation using A classifier\"\n- Missing word: \"Using A binary classifier (...)\"\n- Conclusion: \"the orthogonal classifier makes accurate predictions on the orthogonal aspects of the principal classifier.\" What does \"aspect\" mean ? I feel the sentence is not very useful as a conclusion because of using such a vague term.",
            "summary_of_the_review": "Preliminary recommendation: unsure, because of the missing references to existing methods that achieve something comparable.\n\nQuestion to the authors:\n- Can you comment on differences/advantages of their approach compared to existing ones ?\n- Can you discuss the necessity/feasibility of learning a \"full classifier\" ?\n\n**Edit after authors' response: I am satisfied with the precisions added by the authors to the manuscript and I strongly recommend it for acceptance.**",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}