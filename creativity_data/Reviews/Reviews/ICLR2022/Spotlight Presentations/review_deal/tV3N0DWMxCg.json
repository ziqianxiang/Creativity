{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e.g. deep learning) model to an exponential family distribution.  This is achieved by using the model to map from the inputs to a low-dimensional latent space and then using a normalizing flow to map to the parameters of the distribution.  The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks.  This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8).  The reviewers in general found the method novel compared to existing literature, compelling and the results strong.  Multiple reviewers asked for experiments with higher dimensional output distributions (e.g. CIFAR 100) and had concerns regarding the \"entropy regularization\" term (akin to the beta term in a beta VAE, this is a constant applied to the entropy term).  The reviewers seemed satisfied with the author response, however, and the concensus decision is to accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a single-pass uncertainty estimation method for neural networks by predicting the update to the natural parameters of the conjugate prior to the likelihood of the data distribution and optimizing a corresponding ‘Bayesian loss’. In that it generalizes PostNet (Charpentier et al., 2020) beyond classification. The paper reports competitive or improved performance across a range of datasets and against both standard and recent baselines.",
            "main_review": "**Relevance**:\nUncertainty estimation is a highly active research area. Methods that don’t require calculating an average over multiple forward passes are arguably particularly relevant for practical applications. While most machine learning problems, at least in a research setting, are typically classification or regression problems, generalizations beyond those settings are of interest for the community.\n\n**Novelty**:\nWhile the paper builds off PostNet (Charpentier et al., 2020), which is clearly referenced, the generalization to non-classification tasks is novel. The paper further proposes an alternative scheme of using a single normalizing flow for density estimation on the features, which is computationally more scalable for a higher number of classes. The overall contribution seems solid in terms of novelty, although there might be related work that I’m not aware of, as I have been following this branch of the literature only superficially.\n\n**Clarity**:\nOverall the paper is clear regarding the proposed method, although I found it helpful to also read the PostNet paper. However, the typesetting is too dense, presumably due to the space constraints. In particular in section 3 I would suggest using subsection and subsubsections to structure the text instead of paragraphs. Similarly pages 7 and 8 could benefit from more spacing, the use of bold font inside of paragraphs does not seem ideal to me (in particular when some terms also get underlined, visually I find this page distracting). I’m not sure if an additional page will be available during the discussion period, otherwise I would suggest moving most of the detailed discussion of the setup into the appendix, most of the datasets and metrics are fairly standard.\n\n**Empirical evaluation**:\nThe classification experiments and metrics are mostly standard and the regression problems appear to overlap with (Amini et al., 2020). I would have ideally liked to see the paper push a little bit further in the direction of problems that are not classification or regression, but it is good to see that there is a count prediction problem and that OOD detection is improved by a Poisson observation model. \n\nIn an orthogonal direction, I would also be interested in seeing results for a classification problem with a higher number of classes, as this is the setting where the difference to PostNet of using a single normalizing flow instead of one per class becomes more relevant. I would be interested in seeing if either approach runs into limitations in terms of fitting the data. Ideally this would be shown for ImageNet (if PostNet can be scaled to 1000 classes), but I understand that this may be too much to ask over the discussion period and would be happy with Cifar100 results -- I assume this should be runnable with negligible overhead based off the Cifar10 experiments.\n\n**Other notes/questions**:\n* How sensitive is the method to the value of $\\lambda$ (the entropy hyperparameter)? An ablation study would be useful, I would ideally like to see a plot showing how the different metrics vary across values of $\\lambda$ with some dataset for each likelihood model (normal/categorical/poisson).\n* Please place table captions above the tables as per the formatting instructions.\n* Page 7, final line: missing transition between sentences?\n* There are some minor inconsistencies in the reference formatting (full first names vs only initials, title capitalization).",
            "summary_of_the_review": "The paper presents a solid and well-motivated generalization of prior work. There is some room for improvement in terms of the writing as well as some additional experiments, however, I would already consider this a solid submission as it is. Therefore I am **leaning towards an accept**",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes Natural Posterior Networks (NatPNs), a technique to provide uncertainty-estimation to tasks where the likelihood is an exponential family, e.g. classification or regression. NatPNs leverage the properties of exponential families, i.e. conjugacy and closed-form posterior predictive for fast and elegant Bayesian Inference. This is combined with a Bayesian treatment of the loss function that allows for end-to-end training. Furthermore, it uses a normalizing flow to account for the epistemic uncertainty. The normalizing flow density is supposed to increase the evidence on the training data and thereby decrease it everywhere else. The posterior distribution is on the outputs not on the weights of the NN which makes it easier to train. The paper provides one theorem for why the posterior uncertainty is meaningful and multiple experiments showing practical improvements for multiple exponential families and datasets.  \n\nNatPNs are an extension of Posterior Networks. However, the differences are significant and well explained in the paper. From my perspective, the contributions are clearly novel enough to justify this new paper. \n",
            "main_review": "**Update:** changed my score (see comment) from 6 to 8.\n\n**Strengths**:\n\nThe paper makes three claims. \nFirstly, NatPN can be applied to all problems with an exponential family likelihood distribution. The paper provides strong evidence that this is the case. They perform multiple experiments on classification tasks and two experiments on regression and count prediction. The experiments are thorough and tested on multiple datasets each. \nSecondly, they claim that empirically NatPNs outperform other state-of-the-art methods in different uncertainty-related metrics such as calibration and OOD detection. \nAcross all experiments, their method beats most competitors or provides competitive estimates across multiple metrics. Their experiments provide more than enough evidence for this claim. \nThirdly, they provide a theoretical statement on the OOD behaviour of NatPNs in the limit. Their theorom shows that under mild assumptions NatPNs recover the prior far away from the training data. \n\nOverall, the experiments convince me of the main goal of the paper, namely that NatPN provides well-calibrated uncertainty estimates for multiple data types. \n\nFurthermore, there are other miscellaneous strengths. The paper is well written and the figures are (mostly) of high quality. The paper doesn’t shy away from talking about its limitations. The method is fast and easy to set up which is especially beneficial for practitioners. All of these are important strengths of the paper that are often undervalued.\n\nAll in all, the paper provides a significant contribution to an important problem. \n\n**Weaknesses:**\n\nThere are two main (and easily fixable) weaknesses.\n\na) I think the role of the normalizing flow is underexplained. It is stated multiple times that the normalizing flow provides the evidence updates and its purpose is to estimate epistemic uncertainty. The remaining questions for me are 1. From which space to which does the NF map the latent variable z? 2. Why is the arrow in Figure 2 from a Gaussian space into the latent space, rather than from the latent space to n^(i)? I thought the main purpose was to influence n^(i)? 3. Which experiments show that the normalizing flow contributes meaningfully to the epistemic uncertainty (see b))?\n\nb) Figure 1 does a good job of showing the intuition behind NatPNs but it lacks some components and a discussion in the text. The authors choose to show aleatoric (un-)certainty and predictive certainty respectively but don’t show epistemic (un-)certainty. Technically, you could deduce epistemic uncertainty from aleatoric and predictive uncertainty but it would be easier to compare and follow your argument if it was made explicit. Furthermore, I would like to see an explicit discussion of the results. Why, for example, is the difference between aleatoric and predictive uncertainty so low? Is there no or little epistemic uncertainty in this setting? There are two things that would convince me more regarding this problem: a) an additional toy experiment similar to Figure 1 which includes more epistemic uncertainty, e.g. with fewer data points. This could show that the epistemic uncertainty is well-calibrated. b) An argument for why the epistemic uncertainty is (presumably) so low in your setting. a) and b) are not mutually exclusive, doing both would convince me more. \n\n**There are a couple of minor improvements:**\nFigure 1 is not referenced in the main text.\nI find it hard to spot the difference w.r.t the symbols in Figure 1. Maybe just making it less crowded would already improve visibility.\nIn the last paragraph of 3.1, you mention “warm-up” and “fine-tuning”. It would be helpful to explain these concepts briefly in one additional sentence or provide references.\n\n**What would raise my score?**\n\nI would raise my score by 1 or 2 points if my main weaknesses are well addressed or if evidence is provided that my criticism is the consequence of a misunderstanding. \n\nI would raise my score even further if I’m convinced of the high significance of this work. This will be mostly dependent on the estimate of more expert reviewers but I’m also open to arguments by the authors. \n\n",
            "summary_of_the_review": "**Recommendation:**\n\n6: marginally above the acceptance threshold; Tendency towards 8, if concerns addressed. \n\n**Summary:**\n\nThe paper presents a novel method and provides empirical and theoretical evidence for its effectiveness. There are two parts of the paper that are currently insufficiently explained. Firstly, the technical setup of the normalizing flow and how it is used to create epistemic uncertainty is underexplained IMO. Secondly, the toy experiments in Figure 1 should be explained and discussed in more detail. I'm specifically currently uncertain why the epistemic uncertainty is not shown and presumably quite low in this setting. \n\nI think these concerns can be easily addressed by the authors and would be willing to increase my score to an acceptance (8) if they are addressed well. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a new method Natural Posterior Network (NatPN), an extension to the Posterior Network (Charpentier et al., 2020). NatPN aims to tackle the problem of enabling calibrated uncertainty estimates for in and out-of-distribution inputs for exponential family likelihoods parameterized by deep neural networks. In particular NatPN maps input samples into a latent space on which a normalizing flow is defined. A mapping from this latent space and its corresponding likelihood under the normalizing flow to the parameters of exponential family update rule is defined. The exponential family posterior tends towards the prior parameters for data points far from the training data and prior has little influence for data points within the training distribution.\n\nNatPN makes two significant changes to the Posterior Network 1) it generalizes the method to the exponentially family, allowing for regression and counting tasks beyond just classification as per the Posterior Network and 2) it uses a single normalizing flow rather than class specific normalizing flows as per the Posterior Network, in theory this aids the scalability of the method to datasets with many classes. In addition a minor change is made to the update rule of the posterior parameters.\n\nThe paper presents extensive empirical results on classification, regression and counting tasks across several relatively small scale datasets. NatPN is shown to perform on par or slightly better than similar methods from the literature.",
            "main_review": "Strengths:\n\n- The paper is well written and clear.\n- The theoretical contribution, Theorem 1, is a nice addition to the empirical results.\n- NatPN performs on par or better than similar prior methods from the literature.\n- NatPN extends the Posterior Network to the exponential family distributions which encompasses important tasks such as regression with Gaussian likelihood and counting with Poisson likelihood.\n- NatPN uses a single normalizing flow for classification rather than one flow per class as per Posterior Networks, which in theory enables scaling to datasets with many classes.\n\nWeaknesses:\n\n- NatPN has the theoretical advantage of being able to scale to many classes given that it uses a single normalizing flow. This is a significant potential advantage over Posterior Networks. However the paper only evaluates on classifiation tasks with 10 classes: CIFAR-10, MNIST, FMNIST. It would be very useful to verify the scalability of the method to > 10 classes on datasets such as CIFAR-100 and Imagenet which have well defined associated OOD tasks e.g. CIFAR-100 vs. SVHN, CIFAR-100 corrupted, Imagenet-C/A/R/V2 and are standard benchmarks in the OOD literature.\n- Please also include OOD performance e.g. accuracy, RMSE, etc. for the near OOD datasets e.g. CIFAR-10 corrupted. Methods which are can provide good OOD detection should also be capable of generalizing to near OOD data points.\n- The work is nice, but somewhat incremental vs. the Posterior Network. The extension to exponential families is natural. The use of a single normalizing flow for classification seems more significant to me, but it is unclear if this causes new training difficulties (see questions below).\n-  The empirical results do not show a clear gain from NatPN vs. similar competing methods.\n\nAdditional suggested improvements:\n\n- Comparing against other deterministic/single-pass methods from the literature that do not require normalizing flows would be a significant improvement to the experiments, in particular SNGP (Liu et al., 2020) and DUQ (van Amersfoort et al., 2020).\n- When comparing NatPN to the Posterior Network it is hard to disentangle the performance changes for classification coming from the use of a single normalizing flow rather than C normalizing flows vs. the new update rules. Could you add an ablation to disentangle these contributions? For example, could you maintain the old Posterior Network update rules but use a single normalizing flow or could you use C normalizing flows but use the NatPN update rules per flow? This would help assessing the relative importance of the contributions of this paper for classification tasks.\n\nQuestions:\n\n- Could the authors comment on the disadvantages of not scaling the number of normalizing flows with the number of classes as per PostNet?\n- The second sentence quoted is not clear to me: \"On the other hand, a single normalized density is trained to output the evidence update n (i) = NH P(z (i) | ω) accounting for the epistemic uncertainty. The intuition is that increasing the evidence on training data during training forces the evidence everywhere else (incl. far from training data) to decrease thanks to the density normalization constraint.\" Could the authors clarify?\n- It is not clear to me where the Bayesian NatPN Ensemble posterior update for n^{post, (i)} comes from? How is this not effectively (m - 1) times overcounting the effective training samples given that each ensemble member is presumably trained on the same dataset from a different random initialization?\n- Please comment further or point to the point in the appendix where this is clarified: “Additionally, we observed that “warm-up” training and “fine-tuning” of the density helped to improve uncertainty estimation for more complex flows and datasets. Thus, we trained the normalizing flow density to maximize the likelihood of the latent representations before and after the joint optimization while keeping all other parameters fixed.” If a significant change in the training procedure for NatPN vs. Posterior Networks and other baselines is required then this is a significant disadvantage of the method.\n- What is the definition of the “predicted evidence” for all models? In particular for baseline models: dropout and deep ensembles?    \n\nNits:\n\n- For comparison purposes it would be useful to include results for a standard/deterministic network trained without dropout (with standard MAP parameter estimation).\n- NatPE is never defined. Assumed it is the Bayesian NatPN Ensemble.\n- NatPE should be included in the batched inference time table. Also nit: this should be a Table {number} not Figure 4.\n- I am surprised the Dropout baseline inference time is so slow in Figure 4. It seems reasonable to compute the inference time assuming that the sampling of the dropout masks can be parallelized along the batch dimension. From the numbers it looks like what is being measured is the inference time if the 5 dropout masks are sampled serially. In addition, given this efficient implementation, 5 dropout masks seems low for a fair comparison to a deep ensemble, a useful additional result would be to include a Dropout baseline with num_samples=128 or similar.",
            "summary_of_the_review": "The paper is well written and seems technically correct. The proposed method NatPN extends the Posterior Network to the exponential family distributions and has the advantage of using a single normalizing flow rather than C flows for classification tasks. The empirical results are backed up with an interesting theoretical guarantee under what seem to be reasonable conditions.\n\nA number of aspects of the paper are unclear to me, I have listed these questions above.\n\nGiven the slightly incremental contribution of the extension of Posterior Networks to the exponential family distributions the paper would benefit from additional empirical validation e.g. with the above suggested ablation, the addition of SNGP and DUQ baselines and the inclusion of OOD performance metrics. Additionally, given that I see the use of a single normalizing flow as a main contribution of this paper I would like to have the question regarding the modified training procedure clarified and I would like to see empirical validation of the scaling to CIFAR-100 and/or Imagenet.\n\nNonetheless, the paper is a solid contribution and I believe worthy of acceptance given the information currently available.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose NatPN, a model which can estimate predictive uncertainty for classification and regression tasks. NatPN predicts the parameters of the posterior distribution which belongs to the exponential family. Contrary to other algorithms, NatPN requires no OOD data for training, and is able to evaluate the uncertainty in a single forward pass utilizing normalizing flows for density estimation.",
            "main_review": "# Pros \n\n- Utilizing normalizing flows for density estimation for calibration and OOD detection is intuitive \n- NatPN can be used for both regression and classification tasks. The majority of previous works neglect regression tasks and focus solely on classification. \n- NatPN maintains a fast inference time as compared to other sampling based BNN’s\n- NatPN achieves high aleatoric uncertainty on OOD datasets, which performs well against baselines compared against. \n\n# Cons\n\n- Section 2: The authors state that modelling distributions over the weights results in ‘pathological’ behavior. This is vague and hard to understand, what is the pathology?\n- Section 2 (sampling free methods): parameterizing conjugate prior distributions which $\\rightarrow$ parameterizing conjugate prior distributions, which (even though I believe this fixes the problem with the sentence, it likely needs to be rewritten because the long list of references breaks the flow over three lines…)\n- It is not immediately clear to me why the 2nd term of the loss in equation 5 is necessary. If each component of the model behaves as expected, wont $p( x | \\omega )$ remove the need for entropy regularization?\n- There are recent advances in single pass uncertainty such as SNGP [1] which are nor compared to. At least for classification tasks, I would be interested to see the performance difference between the proposed method and SNGP. \n- Table 5 shows results for models which are trained on the Kin8nm and Concrete UCI datasets, and evaluated in terms of epistemic uncertainty on other UCI datasets. These datasets all have different input dimensions, so how does a model trained with a specific input dimension evaluate a dataset with another dimension? \n\n# Minor\n\n- section 4.2 or training OOD $\\rightarrow$ or training on OOD\n\n\n# References\n\n[1] Liu, J. Z., Lin, Z., Padhy, S., Tran, D., Bedrax-Weiss, T., & Lakshminarayanan, B. (2020). Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. arXiv preprint arXiv:2006.10108.",
            "summary_of_the_review": "There are a few concerns which are highlighted in the sections above, but overall the paper is a strong submission which solves a relevant problem in a novel and intuitive way. My score reflects everything stated in my review and is likely to be updated based on the authors response to my questions above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper targets the task of getting useful predictive uncertainty, as measured by in-distribution calibration and out-of-distribution detection capability. Towards this, the authors focus on the distributions from the exponential family, which allow for a closed-form posterior form whose parameters are predicted via a neural net, together with a flexible density provided by a normalizing-flow net for OOD detection. \n",
            "main_review": "_Edit: Postrebuttal Update: The authors clarify most points in the rebuttal/include them in the new version. I increase my score from 5 $\\to$ 6._\n_________________\n\n## Strengths\n- While prior work in this direction of parameterizing a _'prior'_ distribution as part of the model focused mostly on the classification task offering the clear Categorical+Dirichler pair, this paper's focus on the exponential family allows for a clear generalization. \n\n## Weaknesses\n- While the paper title is phrased as targeting the predictive uncertainty, the paper's discussion and experiments solely focus on the epistemic/aleatoric decomposed setup (apart from some in-distribution calibration results). While the distinction between reducible and irreducible uncertainty is nice within a model, to better understand its performance or as a guiding signal for an active learning task, the paper explores no direction where decomposition of the predictive uncertainty actually is relevant.\n- Related to the last point, the experimental comparisons are also all against methods that allow for a clean distinction instead of focusing on a broader class of models that target a predictive uncertainty independent of whether they allow for a theoretical/practical decomposition of such a term (e.g. a simple deterministic net with temperature scaling for calibration). \n- The exponential family offers an extension away from the standard classification case. The paper has experiments with regression and count data, but as the authors acknowledge, the Poisson likelihood seems to perform poorly in the chosen data set. Having an experiment where the explicit modeling of the counts as counts was necessary would be a lot stronger.\n- The paper is closely related to prior work by Charpentier et al. (2020), down to using the same input examples in Figure 2. \n- ~~The related work discusses BNNs in the sampling-based paragraph but lacks a similar discussion of BNNs in the sampling-free methods~~ _Edit: Wrong claim from my side. I overlooked some references in the sampling-free paragraph as the keyword BNN was missing._ \n\n\n## Questions and minor comments\n- Q1: In the contribution, the authors mention (point (3)) that the density is added \"to the last predictor layer\", a term that is never mentioned again. Can the authors comment on what they mean by that?\n- Q2: As mentioned above on the epistemic/aleatoric point, a stated goal is to aim  \"at accurately modelling both aleatoric and epistemic uncertainty\". Can the authors comment on that goal and why they target it as the paper lacks discussions/explorations on the usefulness of the distinction between the two?\n- The experiments compare against Amini et al. (2020) in the regression case but lack a comparison in the classification against Sensoy et al. (2018) Amini et al.'s closest relation and predecessor. That work also has the benefit of not OOD data during training as the PriorNets do.\n- The BNN discussion comments on the huge computational budget of Izmailov et al. (2021) as a downside. This comment ignores that this cost was due to their goal of inferring the true posterior as optimally as possible via a costly HMC approach. However, that work includes many cheaper methods (be they variational inference-based or MCMC-based, such as SGLD, SGHMC) that demonstrate good performance for a much cheaper computational cost.\n- Table 1 contains **(1),(2)** whose meaning is only resolved several pages later. At the same time, the page includes three different sets of (1),(2) (twice in two separate lists, and once each as equation numbers). A restructuring might be helpful to guide the reader towards the desired meaning in table 1. \n- While the term is only taken from prior work, I would strongly encourage the authors to rename their objective to something other than \"the Bayesian loss\". It is an objective whose optimum is given by the posterior, which is nice, but apart from that, there is no such thing as _THE Bayesian loss_ or even \"the principled Bayesian loss\"..\n- NatPN Ensembles are discussed, but the NatPE abbreviation is missing from the paragraph and never formally introduced\n- In equation (6), as soon as $\\lambda \\neq 1$, L is not actually proportional to the right-hand side anymore.\n\n### Minor Appendix comments\n- App B: The ELBO loss refers again to the appendix \n- App B: There appears a discussion on a prior over $y$, while talking about distributions over $\\theta$\n- In the appendix $P(\\theta)$ and $P(y|\\theta)$ seem to be used interchangeably \n- The appendix states that the entropy of the posterior is used to estimate the predictive uncertainty, but the posterior predictive uncertainty should actually be computed after marginalization over the posterior.\n\n### Typos\n- Throughout the paper, please follow the ICLR style guide properly. E.g. captions belong above tables and below figures.\n- Table 1 refers to eq (7) which is in the appendix. It should probably mean (5), i.e. the same loss just in the main paper\n- End of page 3: Bishop (2006) -> (Bishop, 2006).\n- page 4: predictive posterior distribution -> posterior predictive distribution\n- page 5: using each NatPN member ~~all~~ separately\n- The abbreviation OOD is introduced, but ID is not and needs to be guessed from the context\n\n\n__________\nSensoy et al., Evidential Deep Learning to Quantify Classification Uncertainty, NeurIPS 2018\n",
            "summary_of_the_review": "While the paper provides a useful extension and next step to the prior work by Charpentier et al. (2020), the theoretical contribution seems minor and without great empirical improvements. Explorations in areas where the proposed setup with the uncertainty decomposition becomes relevant could strengthen it a lot.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}