{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known -- obviously very important in practice. The approach is quite clever: they exploit the fact that for this non-convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments.\n\nMost reviewers were positive about the paper. The most critical review, by 1qf1, still acknowledged that this paper has a lot of potential, but in their opinion the paper was not in a state ready for acceptance, especially regarding the formulation of the main result, Theorem 7.  The other reviewers were OK with the state of the paper, and the authors made changes in the rebuttal. Hence, while acknowledging the paper could possibly still be improved (what paper couldn't be!), I think the paper is in a good enough state to accept it for ICLR. I don't think there would be enough benefit to the community (authors, readers and reviewers) to ask for this to go through one more round of submission/revision."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies robust sparse recovery when the dimension of the subspace is not known. It finds a basis of the orthogonal complement of the subspace via dual principal component pursuit (DPCP), but without explicitly imposing orthogonality between the columns. The authors propose a projected sub-gradient method with random initialization and overestimate of the dimension, and show theoretical results. Simulation experiments are provided.",
            "main_review": "The main result of this paper is rather surprising-- that random initialization can replace the need for explicit orthogonality constraint in DPCP. These proofs and theoretical results need more careful review, but if this is accurate, I believe it will have meaningful impact on many other problems in signal processing and machine learning.\n\nThis paper could be improved by addressing the following points:\n- Although the phrase \"implicit bias\" is in the title and repeated throughout the paper, it is not very well-defined. There should also be a mention on how this result connects to existing works on implicit bias, often mentioned in the context of non-convex optimization.\n- Figure 1 needs a lot more explanation in the text and/or the caption. What are the definition of these terms and what is the difference we're supposed to notice between them?\n- Sections 3 and 4 are very dense. Is there a way to provide more high level explanations or proof sketches?\n- Algorithm 1 is lacking, given the importance of initialization. There should be more explicit instructions on how to find $\\hat{B}_0$ inside the algorithm box.\n- Are there any computational analysis of the algorithm?\n\n\nMore minor points:\n- Should $\\tilde{X}$ equation (3) be transposed?\n- Notation in equation (6) is confusing. Is the left side claiming that $c_d$ evaluates to $2/\\pi$ for even $d$?\n- Broken link (??) in page 6 \n- Figure 2 needs a colorbar. Is this in log or linear scale?",
            "summary_of_the_review": "The take-home message of the main theoretical result is very interesting, and could open a new way to think about many other optimization problems with explicit orthogonality contrainsts. However, I have only skimmed the proofs and have not carefully checked them.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is concerned with the Robust Subspace Recovery (RSR) problem. That is, one tries to estimate an unknown subspace given some data, where some of the entries are corrupted. This paper studies the setting, where the subspace to be recovered has large dimension, i.e. the co-dimension of this subspace is small. \n\nWhereas previous work tackles this question, it requires that the dimension of the subspace to be recovered is known a-priori. This work proposes a new algorithm, which is able to recover the subspace without knowing this dimension.\n\nThe contributions of this paper are threefold.\n1. It proposes a new algorithm, which does not need a-priori knowledge of the unknown subspace.\n2. Some theory for the new algorithm is developed.\n3. The algorithm is tested on both synthetic and real data.",
            "main_review": "The paper introduces a new algorithm for a significant problem, which is able to circumvent an important issue with previous work (no knowledge of the subspace needed). For this reason, I think that the paper has a lot of potential. However, I think that in its current state the paper is not yet ready for acceptance.\n\nMost importantly, I feel that Theorem 7, which is the main result of this paper, needs to be formulated much more cleanly. For example, it is totally unclear to me what is $\\beta$. What is $K_{\\star}$ and $K_{0}$? Has this been defined before? I feel that the authors should work on a version which is more digestible and more self-contained. Furthermore, I am not able to assess the strength of these results. It might be useful to derive some implications of this theorem in some simple settings to see how sharp the result is.\n\nFurthermore, I think that the paper needs to be really polished and proofread, before it gets submitted again to another conference (see comments below).\n\nFurther comments:\n\n1. Figure 1: I think it should be \"as opposed to b)\" instead of \"as opposed to a)\" \n2. Figure 3: In the caption below the Figure, it is written that both (a) and (b) use r=0.6. I think for (b) it should be r=0.7\n3. Is in equation (3) a transpose missing? Should it not be $Xtilde^T$ instead of $Xtilde$?!\n4. In equation (7) is $c_{O,min}$ correctly defined? Should it not be $O^T b$ rather than $X^Tb$?\n5. This might be a bit nitpicky, but in the related work section on subspace recovery, the authors refer to Joliffe&Cadima and Vidal et al. for PCA, respectively SVD. I feel that if the authors of the paper are doing this they should write \"see, e.g., Vidal et. al\" instead of just \"Vidal et. al\". (SVD is not due to Vidal...)\n6. I feel that in the related work section on robust subspace recovery some important work is missing. For example Candes, Wright et al. on Robust PCA at the end of page 6: \"outliers and inliers terms of (??)\n7. In the formulation of Theorem 1 it would be helpful for the reader if the authors would refer to (1). I think that this would clarify that this is the problem which gets solved.\n8. Typo in the formulation of Lemma 6: \"then (the) matrix\"\n9. Why do M and N not matter interested formulation of Theorem? Clarifying this might be helpful for the reader.",
            "summary_of_the_review": "Due to the aforementioned reasons, I think that the paper is not yet ready for acceptance. However, I feel that the paper has a lot of potential and I hope that the authors keep working on the issues which have been raised in this report.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work proposed a simple framework that allows performing robust subspace recovery without requiring a priori knowledge of the subspace codimension. The proposed approach is based on Dual Principal Component Pursuit (DPCP), which is amenable to handling subspaces of high relative dimensions. Empirical results corroborate the developed theory and showcase the merits of the proposed optimization methods.",
            "main_review": "The work proposed a new analysis framework for justifying the robust subspace recovery under the DPCP formulation without requiring apriori knowledge of the subspace co-dimension. The work analyzed both the gradient flow and its projected subgradient counterpart, showing convergence to the target solutions.\nThe paper is well-organized, and well-presented overall. The work tackles an important challenge to perform robust subspace recovery in a high relative subspace dimension regime without requiring a priori knowledge of the true subspace dimension. \nNonetheless, the reviewer is a little concerned over the novelty of the work. It seems to be borrowing/combining ideas from previous work on DPCP, and implicit bias for low-rank matrix factorization. \n",
            "summary_of_the_review": "The work is well-presented, and tackles a challenge of unknown true subspace dimension via overparametrization and implicit bias of optimization method. The work combines ideas from previous work on DPCP and implicit bias for low-rank matrix recovery. The overall result is novel, while the approaches seem to be a combination of previous methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discusses solving robust subspace recovery by solving parallel versions of $\\min_b$ $\\|X^Tb\\|_1$ st $\\| b\\|_2=1$, for random initializations of $b$, using projected subgradient method. The argument is that using this  method, in the limit where the number of inliers greatly outnumber the outliers, that if the inlier dimension is c, then c random initializations will converge on identifying this dimension with probability 1. Experimental results and proofs are given to prove this idea. \n\n",
            "main_review": "The idea is interesting, and seems to be more of a statistical result than of optimization.  The numerical results are encouraging, and I can't directly find any issues with the proofs (though for time reasons, I did not look too closely).  \n\nThere are two assumptions I think should be elaborated, at least qualitatively: first, the condition (9) on the initialization of b's, and second, the overestimate of c' over c. These relationships would really give better intuition as to how powerful the random initialization is. Additionally, I am curious as to how much it matters that the data itself has isometry principles. I would suspect if the inlier dimensions have a very uneven contribution, this method should not work that well; how is this captured? ",
            "summary_of_the_review": "Overall, I think the paper's idea is nice and looks sound; however, I am not an expert in this area. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}