{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper introduces a procedure to control the churn (i.e. differences in the predictive model due o retraining) using distillation.\n\nThis is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb. \n\nReviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing.\n\nReviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately.\n\nAll in all, this paper is ready for and deserving of acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper examines an interesting problem, so-called churn prediction provides some nice proofs, builds distillation-based algorithms, and demonstrated the performance to multi-class problems in experiments.\n",
            "main_review": "================================================================= \n\n[Main Strengths] \n\nThis paper's main strength is that the authors' lengthy proof of the performance guarantee (not sure how tight the bound is, though), and the key implementation codes are provided in the Appendix.\n\n================================================================= \n\n[Main Weaknesses] \n\nThe primary limitation of this study is that, while the underlying problem framework is intriguing, it does not appear to make a significant impact, much like the various flavors of domain adaptations. I encourage authors to make additional improvements, particularly for the experiments described in Section 5, which is to show that how the distillation handles the distribution (or domain) shift issues, which will be highly intriguing and will definitely necessitate more detailed experiment settings.\n\n================================================================= \n\n[Technical Comments] \n\n1) The title is a little misleading, as churn prediction often means “predicting which clients are most likely to cancel a subscription\".\n2) Another essential related works that I recommend the authors cite is unsupervised domain adaptation with practical application (for example, https://arxiv.org/abs/1905.02530).\n\n================================================================= ",
            "summary_of_the_review": "The use of distillation for churn prediction is intriguing, but it does not appear to make a major difference, similar to the many flavors of domain adaptation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In real-life applications of predictive models, predictions often form a step of the process. Changes in the predictive model often need to be validated end to end using methods such as A/B tests before they can be used in production. Thus for certain class of problems, there is a need to control the churn or the difference in the predictive model due to retraining of the model towards a more robust pipeline. The authors present an approach to control this churn via a simple distillation method. They also show that their distillation method, under certain assumptions, is equivalent to a constrained optimization problem that explicitly constrains the \"churn\" without the added complexity of constrained optimization. They validated their method by conducting experiments on a number of baselines on a wide range of datasets and model architectures. ",
            "main_review": "Some of the strongest aspect of the paper is as follows:\n\n- Distillation is a training process that has been applied to many different settings with remarkable success. The current paper introduces this technique as a simpler solution for a problem of interest, especially in production settings. \n- The results seem to support the importance of their method. The authors also conducted detailed analysis of multiple settings that makes the claims stronger\n- The theoretical underpinnings presented in this paper by essentially casting distillation as a simpler dual problem for the more computationally complex constrained optimization primal is well appreciated. The following propositions are also of interest and the bounds can lead to newer theories and/or better usage of such methods in practice\n- Overall the paper is well presented, especially the methods section. \n\nThe paper can be improved upon by addressing a few points as below:\n- The authors presented the claim that their process corresponds to `a`  value of $\\lambda$ that constrains the churn in the primal setup. Algorithm 1 also points to a last step where the corresponding primal solution is found from the dual solutions. However, while possibly trivial, the methods to use this distillation process for an ``acceptable churn`` is missing. I.e how would one go about using this method for re-training if they a certain amount of churn can be tolerated. From the introduction and motivation presented in the paper, it seems such a use case maybe of interest and the paper can perhaps include this setting\n- From a presentation perspective, the confidence bounds in the appendix could be brought up in the main paper\n- Another minor aspect of the presentation, the problem of churn and its impact on problem settings should be presented with citations in the introduction section. There is some citation on these in the related work however the problem introduction also needs some justification e.g. literature on churn being hurtful in production settings etc. \n\nSome minor points that can be addressed: \n- The traces to arrive at Proposition 1 may be included in the main paper. Its one of the most interesting and foundational aspects of the paper and could be better presented to make the main paper self sufficient\n- In section 3, $y$ is used without definition. Also, `low predictive churn` is used without defining what does low mean",
            "summary_of_the_review": "This is a very interesting paper that adds both empirical and theoretical findings to the body of literature. The theoretical underpinnings of distillation as a constrained optimization applicable for churn reduction sheds new light on a very popular method and can lead to new applications. The presented results also support the efficacy of the methods. Overall this paper is a good example of an ICLR paper. I would encourage the authors to address the presentation points mentioned in the main review for a more impactful submission. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "While I don't see an ethical concern for this work on its own, it may be of interest to the authors if they can think about how their distillation process can be adjusted if the base model was somehow unfair",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores the relationship between the low-churn problem and distillation. The authors show that there is an equivalence between those two methods and that distillation performs particularly well on low-churn dataset tasks. The authors propose a novel churn reduction algorithm based on distillation which involves the training of a classifier by minimizing a distilled loss and solving a convex program. The authors  provide theoretical guarantees for the proposed algorithm and explain the advantages of the proposed approach compared to the anchor loss, another churn-reduction method. \n\nThe authors validate empirically their approach on 12 OpenML datasets, 10 MNIST variants, CIFAR10, CIFAR100 and IMDB.  \n",
            "main_review": "Strengths\n- The authors provide theoretical guarantees to justify the use of their algorithm compared to comparable approaches\n- The authors provide empirical justifications of the proposed algorithm using many datasets and baselines \n\nWeaknesses\n-\tI was not able to find a major weakness in this paper\n\nTypos\n-\t5.2 : « performs the best » or « performs well » probably\n",
            "summary_of_the_review": "This paper provides strong theoretical and empirical results regarding the effectiveness of the proposed distillation based algorithm for the churn reduction problem. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}