{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This work considers one-shot pruning in deep neural networks. The main departure from previous work is to consider stochastic Frank-Wolfe. The reported results are convincing although a number of baselines were missing from the initial submission. The authors provide a balanced account of the strengths and weaknesses of the proposed approach.\n\nThe authors adequately addressed the concerns of the reviewers. For instance they ran additional experiments to compare to missing pruning baselines. I would encourage the authors to revise the manuscript by including the missing related work, the additional clarification discussions (e.g., motivation for K-sparse constraints, follow-up analysis, and cost per iteration) and to include the additional experiments that were conducted (e.g., pruning with training)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper swaps gradient-based optimisation for training neural networks with Frank-Wolfe algorithm with a constrain that pushes less important weights towards smaller values. The resulting trained models can be pruned to different sparsity targets without needing to be retrained.",
            "main_review": "I think the proposed method here is actually quite interesting but this work fails to discuss a whole body of relevant literature that also aims to train sparse neural networks. As a result the chosen baselines for the experiments section fail to show how effective this method actually is.\n\nHere are few examples I was expecting to see discussed and/or evaluated:\n\n- Directly related to the \"one-shot no retrain\" claim of this paper:\n    - [Only Train Once: A One-Shot Neural Network Training And Pruning Framework by Chen et al](https://arxiv.org/abs/2107.07467)\n- Pruning during training:\n    - [Sparse Networks from Scratch: Faster Training without Losing Performance by Dettmers et al.](https://arxiv.org/abs/1907.04840)\n    - [The State of Sparsity in Deep Neural Networks by Gale et al.](https://arxiv.org/abs/1902.09574)\n    - [Dynamic Model Pruning with Feedback by Lin et al.](https://arxiv.org/abs/2006.07253)\n- there are also lots of works on inducing group sparsity on neural networks that's relevant to this paper. Some examples are:\n    - [Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning by Lin et al.](https://arxiv.org/abs/1901.07827)\n    - [Learning Structured Sparsity in Deep Neural Networks by Wen et al.](https://arxiv.org/abs/1608.03665)\n    - [Structured Sparsity Inducing Adaptive Optimizers for Deep Learning by Deleu et al.](https://arxiv.org/abs/2102.03869)\n    - [Neuron-level Structured Pruning using Polarization\nRegularizer by Zhuang et al.](https://papers.nips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf)\n- and of course numerous papers that traine sparse neural networks with Lasso (or directly L0) regularization\n\nResults in Figure 2:\n- while it's true that SFW maintains performance in low pruning ratios but it seems to degrade a lot above 0.9 pruning ratio which is what typical pruning methods usually aim for. So doesn't it make more sense to compare SFW against other pruning methods at _their_ \"nature sparsity\"? I guess my point here is: why should I choose SFW over another classic pruning method that can achieve better performance at higher pruning ratio? \n- In Fig2. the performance of SGD on TinyImageNet is lower than what it should be (and of course lower than Franke-Wolfe training). I was wondering if authors have an explanation for this?\n\nResults in Figure 5:\n- Section 5.4 is supposed to compare against SOTA pruning methods but chooses pruning-at-initialisation baselines such as GraSP and SynFlow. This makes very little sense to me. Why not compare against pruning-during-training methods or even pruning-after-training methods?\n\nRandom:\n- Fig1a averages over multiple architectures and multiple datasets. Why does it make sense to do so? Is the shaded area the standard deviation over these differences or over multiple runs with different seed?\n- Isn't Fig1.b exactly the same as Fig2.a? \n\n",
            "summary_of_the_review": "This paper proposes an interesting alternative to gradient descent that can be used to train models with a weight distribution suitable for one-shot pruning. I find the method section well-written and convincing however I was expecting to see many other relevant work here and the experiments section fails to compare to these relevant works, so it's difficult for me to evaluate the effectiveness of the method compared to simpler/other alternatives.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an algorithm to train a DNN in a pruning-aware manner such that the trained network can be pruned at various sparisities without any fine-tuning. The approach is based on stochastic frank-wolfe and the results are impressive for a wide-range of sparsities.",
            "main_review": "# Strengths\n\n1. The problem of pruning-aware training is interesting and has a lot of potential.\n2. The formulation and approach to optimize it using SFW is novel.\n3. In addition an initialization scheme that is suited for SFW is also presented.\n4. The results demonstrate the value of pruning-aware training using the proposed approach.\n\n# Weaknesses\n\n1. The motivation for K-sparse constraint set is not clear. More clarity is required on how the authors chose to use such a constraint set.\n\n2. If I understand correctly, the \\tau is a constant for all the weights in the network and therefore, the SFW update at any iteration is basically based on the sign of the gradient. Is this understanding correct? If yes, it further raises questions about the choice of the K-sparse constraint set. What is the need to use such a fixed \\tau? How to decide the value of \\tau? It is intriguing that even with such a restrictive update the final weights are competitive to SGD based training.\n\n3. The performance is still worse than SGD with retraining at extreme sparsities (fig. 5) but considering the cost of retraining the proposed method is useful.",
            "summary_of_the_review": "The idea is interesting and the method is technically sound. The choice of the constraint set should be motivated and clarified to improve the quality of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a Frank-Wolfe based approach for efficient pruning. \nThe highlights of this paper are:\n\n(1)  Propose a learning-based initialization scheme for SFW-based DNN training\n\n(2)  Achieves promising performance on DNN benchmark when we set different pruning ratios",
            "main_review": "The topic studied by this paper is of general interest. The authors manage to organize the paper in a good way.\n\nStrength:\n\n(1) Formulate the pruning strategy with an optimization problem and solve it through Frank-Wolfe\n\n(2) The learning-based initialization helps to achieve better performance.\n\nQuestions:\n\n(1) Which step in the proposed algorithm other than backpropagation has the largest cost per iteration? \n\n(2) Does the algorithm model-agnostic? How is the performance in various vision models\n\n(3) Does the proposed algorithms scales to larger dataset such as ImageNet?",
            "summary_of_the_review": "The paper proposes an interesting algorithm for training large DNNs. Particularly, the proposed method could handle different pruning ratios. \nThere still exist concerns regarding the scalability of the algorithm. The authors are encouraged to address it with more experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed to use stochastic Frank-Wolfe with K-sparse polytope constraints to train deep neural networks and make them pruning friendly. ",
            "main_review": "The core strength of this paper is to train a deep neural network so that it is more resilient to pruning. The author achieves this by using the K-sparse polytope constraint to enforce the model has more small weights. And they empirically show that such weight distribution can help weight pruning. They also use stochastic Frank-Wolfe for optimizing the problem. A gradient-based initialization scheme is also used to improve the trade-off between sparsity and performance.\n\nThe weakness of this paper can be summarized into the following points:\n1. The core benefit of the proposed method is not clear.  In 'Why K-Sparse Polytope Constraints?', authors argue that 'each SFW step pushes those less important weights smaller' and such weight distribution 'can more smoothly remove small values' and 'yielding competitive test accuracies'. On the other hand, in 'Learning rate $\\alpha$', authors argue that a larger learning rate can 'result in sparser parameter $\\theta_{t+1}$' and higher natural sparsity. The latter argument suggests that the benefit of K-sparse constraints is due to they can learn sparser solutions for the deep neural network. The former argument suggests that the benefit of K-sparse constraints is a weight distribution that can make pruning easier. The latter argument is a quite trivial result. The former argument is interesting but requires more analysis. In addition, it seems that the K-sparse constraint is not designed to obtain a sparse solution. The property of the Frank-Wolf algorithm ensures $\\theta$ always attains the K-sparse constraint. If the K-sparse constraint is really useful, then it is not clear why $\\alpha$ is important to the performance.\n\n2. The authors should provide a more detailed analysis of K and $\\tau$ for K-sparse polytope constraints, and how they affect the weight distributions. I also suggest the authors include a more detailed definition of K-sparse polytope constraints and their properties, to make the paper self-contain. \n\n3. Authors use the Frank-Wolfe framework, which is drastically different from regular techniques for training deep neural networks. As a result, ImageNet experiments with standard baseline models, like ResNet-50, are required, and numerical results should also be reported. The reason is that scalability is crucial to weight pruning algorithms. Moreover, on ImageNet, whether Frank-Wolfe algorithms can obtain similar performance as popular stochastic optimizers is not clear. \n\n4. In Figure.2, the advantage of the proposed method becomes much smaller when the dataset is more complex. When applying large models on small datasets, it is plausible to say small weights are not important. However, when the dataset becomes complex, small weights may also become important as suggested by previous works [1]. From this perspective, the effects of pushing more weights smaller may vanish when the dataset is large enough.\n\n[1] Ye, Jianbo, et al. \"Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers.\" ICLR, 2018.\n\n----------------------------------------------------\n\nAfter the authors' response, most of my questions were addressed, and I increased my score to 6.\n",
            "summary_of_the_review": "In summary, I think this paper is below the acceptance threshold since the main claim lacks enough support. And current results may be due to sparsity incurred by the large learning rate $\\alpha$. In addition, experimental results are not enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}