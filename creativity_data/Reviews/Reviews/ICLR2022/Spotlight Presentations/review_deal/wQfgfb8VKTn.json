{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers found that the paper offers interesting contributions for multi-agent RL and favour acceptance of the paper. The strengths of the paper are summarized below:\n- Good algorithmic contribution\n- Offers a new set of benchmark tasks for coordination in MARL settings\n- Exhaustive experiments on complex tasks with a reasonable number of agents\n- All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed.\n\nI therefore join the reviewers in the recommendation to accept the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper focuses on the problem of cooperative multi-agent Reinforcement learning and proposes a novel way to learn the dynamic (state dependent) coordination graph for the joint action selection from factorized joint value representations. The key idea here is to use the variance in payoff function estimates can be a good indicator for whether a coordination graph edge should be present or not. However, given the deep RL goal is learn these payoff functions there is a cyclic dependency between errors from payoff estimation and coordination graph estimation. The paper therefore also proposes an alternative action representation scheme that helps mitigate some of these problems. The paper evaluates the proposed approach on many classic coordination problems as well as subset of SMAC benchmark tasks.",
            "main_review": "\n**Strengths**\n\nEarlier works incorporating coordination graphs in the function approximation case which either are limited to static coordination graphs (like DCG) or were learning a \"soft\" version of fully connected coordination graph [1] (with graph neural network based action inference). In comparison, the current work provides a novel alternative for the problem of coordination graph learning. While the paper doesn't mention it, the payoff variance based edge selection method might be an interesting case for resolving some of the future work mentioned in [2]. In a sense the MCTS setting might be a better test because it would resolve the chicken and egg problem that comes from trying to learn the payoff functions and can avoid the need for action encoders with auxiliary losses to stabilize learning. The paper has an expansive set of experiments with fair number of agents. \n\n**Weaknesses**\n\nWorks like [1] should be mentioned in related work. While there are some ablations, so it's not a big weakness, but it would be useful to know for example whether the action encoder trick helps say qmix or other value factorization methods. I would expect sparsity to help even more in larger number of agent environments and it would be great if MACO allows to quickly evaluate that.\n\n[1] Navid Naderializadeh, Fan H. Hung, Sean Soleyman, Deepak Khosla. \"Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning\". arXiv:2010.04740\n\n[2] Shushman Choudhury, Jayesh K Gupta, Peter Morales, Mykel J Kochenderfer. \"Scalable Anytime Planning for Multi-Agent MDPs\". AAMAS 2021.",
            "summary_of_the_review": "Overall it's a very interesting paper and would recommend acceptance just for the new variance based metric for coordination edge selection. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study how to learn dynamic sparse coordination graphs, which is a long-standing problem in cooperative MARL. In particular, they propose a novel deep method that learns context-aware sparse coordination graphs adaptive to the dynamic coordination requirements, and evaluate the proposed method against existing methods on MACO benchmark, as well as StarCraft II micromanagement benchmark.",
            "main_review": "Overall speaking, I found the paper interesting to read and easy to follow. In comparison with existing literature on in cooperative multi-agent learning, I'm convinced that, the approach proposed this paper indeed adds valuable extensions in multiple dimensions and it would be quite beneficial to the research community in this area. As far as I know, the technique proposed in this paper is novel, and the demonstrated results of this proposed method/technique seem to be encouraging and promising. The presented MACO benchmark in this paper can also be valuable addition to the research community in this area, and the empirical evaluation of the proposed method v.s. existing SOTA methods on MACO benchmark and StarCraft II micromanagement benchmark, also provides some reasonable justification and illustration of the superior performance of the proposed new method.\n\nThe two major limitations of the proposed approach (also already pointed out by the authors in the conclusions section), especially about the first one about no guarantee of cycle-free graph and thus might select sub-optional actions, is a bit concerning, and it would be valuable if the authors could provide some more comprehensive study on this limitation and evaluate more accurately how severe this limitation is (based on the current write-up, we are not fully clear how severe this limitation would be for general scenarios) . We want to avoid the case that the good numbers and examples in the experimental section are specially hand-crafted or carefully selected in a way to minimize the impact of this limitation of no cycle-free guarantee and to favor the proposed methods to show off its advantages. If that study wold takes too much extra time and efforts, I won't object for acceptance of the paper based on the current available results, since I think these results already could be considered as significant contributions to this research area, and I'm ok for the authors to leave those study as future research work. But a comprehensive study about this limitation and some more justification/explanation about it would definitely makes the paper stronger.\n\n\n",
            "summary_of_the_review": "The paper is well-written and seems to be of significant contribution to this research area. There are some limitations that are a bit concerning and it would be significantly better if the authors could provide more comprehensive study and explanations on those main limitations, but if not possible due to time limitation, I won't object for acceptance of the paper based on the current results and leave those potential improvements as future research work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to use the variance of the payoffs functions to learn dynamic coordination graphs, i.e. coordination graphs that might change in structure at each time step of the rollout. The empirical evaluation is carried out on a newly proposed MACO benchmark as well as some levels from the Starcraft II SMAC benchmark.",
            "main_review": "### Pros\n\n- The problem is relevant;\n- MACO benchmark is a valuable contribution to the community;\n- Empirical results are good.\n- Proposition 1 is an interesting addition to the paper (I did not check the proof).\n\n### Cons\n\n- The paper talks about \"intensive and inefficient\" message passing, however, it is unclear if a sparser graph leads to wall clock time reductions due to batching using GPUs.\n- The paper hypothesises why CASEC outperforms DCG, however, it would merit a deeper analysis of the question. Personally, I think that this is the most interesting/important finding of the paper that sparsifying the graph leads to such performance improvements.\n- The paper misses relevant work: Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning, AAMAS 2021 that use attention and graph networks to learn context-dependent coordination graphs doing an evaluation on SMAC as well.\n\n### Questions/Comments\n\n- Can you provide wallclock time comparison between DCG your method? It would be interesting to see how sparsifying the graph affects the real computation time.\n- Can you provide the range of the fraction in Equation 5? I'd like to get a bit more intuition on about how loose the bound is.\n- Is the result in Figure 2 consistent on different runs? Do all the seeds demonstrate this behaviour?\n- The negative space in Figure 3 is just too large.\n- \"These results prove that our method can distinguish the most important edges\". This statement is too strong, I don't think that empirical results can prove anything.\n- The second paragraph on page 7 is an interesting analysis of DCG, this is a great find!\n- Can you provide intuition on why DCG>CASEC on Pursuit?\n- Can you include DCG in the TD error plot in Fig 5c?\n- Why do you think there is a drop in performance for threshold = 0.5 in Figure 4 for Gather?",
            "summary_of_the_review": "I reviewed this paper before, and the authors improved the manuscript and the paper reads much more nicely now. Empirical results are good, and the authors hypothesise on why this or that experiment went that way. \n\nHowever, I give it a 5 (marginally below the threshold) due to the fact that the authors decided not to include the related work that the reviewers suggested, e.g. Deep Implicit Coordination Graphs for Multi-agent RL by Li et al. I am willing to increase the score if:\n* authors provide a legitimate reason for not including relevant work into this submission, and\n* include the relevant work and compare to it / explain why such comparison is not meaningful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
            ],
            "details_of_ethics_concerns": "I reviewed this paper before, and multiple reviewers reported several very related works missing, i.e. Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning, AAMAS 2021). The authors completely ignored this criticism and did not incorporate it into a new version.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a technique called CASEC to train deep coordination graphs with sparse connectivity and a dynamic method to remove a fixed number of edges during inference. The latter removes edges in order of smallest variance of the corresponding payoff functions, which the authors motivate with a theoretical statement on the probability of changing the optimal action after removing an edge. CASEC is evaluated on a novel benchmark suit (MACO) for coordination dilemmas and StarCraft II micromanagement tasks. The proposed methods outperforms the baseline (DCG) in 2 out of 6 MACO tasks and shows in particular a clear advantage of the used action encoding method in StarCraft II.  ",
            "main_review": "**POSITIVE**\n\nThe paper is well written and the presented ideas are novel and interesting. The choice of scoring function for the edges is well motivated (although the reviewer was not able to follow the entire proof). In particular in the StarCraft II tasks the method shows a clear advantage over DCG.\n\n**NEGATIVE**\n\n(1) The analysis of Sensor is nice, but the benchmark seems to use the wrong evaluation metric. As the reward is linear in the number of agents that scan a target (as long as they are more than 2), the task does not specify whether it is better to scan few targets with many agents or many targets with few agents. All evaluation graphs (except Fig.1, middle) focus on the number of scanned targets, though, which is not optimized for. The fact that CASEC scans more targets than DCG does not mean anything, the fact that it collects more reward does. Moreover the conclusion \"the gap between return and the number of scanned targets is larger without the sparseness loss\" is somewhat weird, as adding the loss increases reward, but decreases scanned targets. The authors must address this issue. The reviewer recommends to clip the reward at 3 for 2 or more agents scanning the same target to get rid of this ambiguity and ensure all algorithms optimize the right metric.\n\n(2) The authors claim \"Intuitively, agent $i$ needs to coordinate its action selection with agent $j$ if agent $j$'s action exerts significant influence on the expected utility of agent $i$.\" However, an edge may also be important if agent $i$ needs access to the joint histories $\\tau_{ij}$ to properly evaluate an action, similar to the \"non-decentralizable task\" in the DCG paper. Ideally the authors would disentangle this dependency in an additional experiment, but the possibility should at least be mentioned. \n\n(3) The flat performance of DCG in the middle plot of Figure 1 is curious, as DCG seems to outperform CASEC (in terms of reward) for very few edges. Does this hold for no edges left as well? The authors comment this with \"only the individual utility function contributes to action selection\". Does this indicate that all payoff functions learned by DCG barely contribute, and wouldn't that mean they have almost zero variance? Shouldn't CASEC learn this kind of behavior first, as it is explicitly regularized towards it? \n\n(4) There is no incentive for CASEC to use the utility functions, as only the payoffs' variances are punished. The network could therefore absorb the utilities into the payoffs. The authors could add a second regularization term that prevents this, e.g., a norm loss on the payoffs outputs with a very small regularization constant. \n\n**DETAILS**\n\n- The space of histories must be $T \\equiv (\\Omega \\times A)^* \\times \\Omega$\n- \"and $q$ represents utility or payoff functions.\": define clearer that $q$ is the (weighted) sum of all utility and payoff functions that condition on actions $a_g$.\n- Mention where to find the proof of Proposition 1 in the main text.\n- Why not use double-Q-learning in (eq.7)?\n- Use $|\\mathcal V|$ instead of $n$ in (eq.8) for consistency. \n- You should explicitly mention that you train a fully connected graph, but perform action selection on the sparse graph.\n- \"and 2) accelerating the training of payoff function between target network updates to reduce the estimation errors.\" is unclear. This is about action representation. Say so.\n- In the Sensor description of p.5: $k \\geq 2$\n- Mention in 6.2 whether you use the sparsity constants from 6.1\n- \"DCG outperforms CASEC on Hallway\" is not significant\n- CASEC reduces the representational capacity of DCG, which can explain why it learns faster, but you cannot argue that the CASEC solution is \"beyond the representational capacity of the network\" (p.8) of DCG.\n- What is a \"utility difference function\" (p.9)?\n- Mention the potential convergence issues earlier, e.g. p.3.\n\n** POST-REBUTTAL **\nThe author's rebuttal incorporated this reviewers suggestion and the results appear much cleaner thanks to this. The discussion has convinced this reviewer that the paper should be accepted. The score has therefore be raised to 8. ",
            "summary_of_the_review": "The introduced method is novel and relevant. Although the reviewer was not able to follow the entire proof, the edge selection seems well motivated. Experiments are sometimes weirdly evaluated, but clear enough to say that in some cases using sparse graphs is an advantage. The reviewer would have liked to see more intuition when this is the case though.\n\nIn summary, this is a good paper that merits publication. The reviewer would consider increasing the score if the authors fix point (1) or convincingly explain why number of scanned targets is the correct evaluation metric. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}