{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data.  The proposed approach uses bootstrapped Q-functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo-target that is penalized by the uncertainty quantification.  The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark.  The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments.  The authors have well responded to them, and no major concerns remain."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents Pessimistic Bootstrapping for offline RL (PBRL), a model-free offline RL algorithm that purely relies on an uncertainty-driven method. Bootstrapped Q-functions are trained, and the standard deviation of their estimates is used for the uncertainty quantification. This uncertainty quantification is then used for pessimistic bootstrapping. Also, in contrast to the existing methods that only considers in-distribution target, PBRL optimizes Q-function even for out-of-distribution actions with the pseudo-target that is penalized by the uncertainty quantifier. A theoretical analysis is provided that PBRL is provably efficient in the linear MDP setting. Experimental results demonstrate that PBRL outperforms the state-of-the-art methods.\n",
            "main_review": "The paper is well written and easy to follow. The proposed PBRL is well motivated and is also backed by theoretical results in a linear MDP setting. The pessimistic value backup itself may not be new in the offline RL context (e.g. Appendix E in [1]), but it was introduced as a practical trick in the previous work, while PBRL's pessimistic bootstrapping is supported by theory, which I think is a good contribution. Also, to my knowledge, the way of exploiting the OOD samples is novel. Experiments and ablation studies are convincing and thorough. Overall, I think the paper made a solid contribution.\n\n- In contrast to $\\hat T^{in}$ in Eq (4), $\\hat T^{ood}$ in Eq (5) is not a contraction mapping. Therefore, repeatedly applying $\\hat T^{ood}$ would yield a divergence to $-\\infty$ of Q-value, which seems problematic?\n\n\n[1] Lee et al., Batch Reinforcement Learning with Hyperparameter Gradients, ICML 2020\n\n",
            "summary_of_the_review": "The paper is well written. The proposed method is well-backed by theory and the empirical results are convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed an uncertainty-aware offline reinforcement learning algorithm based on a Q-network ensemble. The proposed method penalizes Q-values on OOD actions and performs pessimistic offline Q-learning. The paper interprets the method with LCB framework. The proposed method outperforms existing offline RL algorithms on D4RL Gym and Adroit tasks. ",
            "main_review": "**Pros**\n- This paper is well-written and easy to read. \n- The paper provides the theoretic interpretation of their uncertainty penalization. \n- The proposed method outperforms various offline RL algorithms including model-based algorithms and uncertainty-aware methods. \n- The paper provides informative ablation studies comparing various regularizations and analyzing the effects of hyperparameters.  \n\n**Cons**\n- The proposed method requires additional computation costs and memory for Q-network ensemble and OOD penalization. It will be informative to compare methods in terms of computational cost and space. \n- Formulation of actor-critic objectives is relying on heuristics. For example, the critic loss term uses uncertainty penalized Q-values, whereas the policy loss term uses the minimum Q-value. Is there any reason to use these different forms?\n\n**Additional comments**\n- Does the PBRL use identical hyperparameter settings provided in Table 2 across all of the Gym and Adroit domains? If then, it is very surprising, because each task has a different dimension and complexity. As I experienced, most of the existing offline RL algorithms including CQL, MOPO, etc. are very sensitive to hyperparameters (e.g, lr, penalty coefficient) and must tune their hyperparameters to each domain. Because it is hard to validate models without online interaction, I believe the robustness to hyperparameters is a very important issue in Offline RL. \n- There is a recent paper [1] that is highly related to this submission. The proposed method seems more stable, but however, it will be informative to compare the proposed method with [1] in revision. \n\n[1] An et. al, Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble, To appear at NeurIPS 2021 (https://arxiv.org/abs/2110.01548). \n\n**Post Rebuttal**  \nI appreciate the author's efforts and clarification. The paper becomes much clearer (e.g., hyperparameter settings) and improved by adding computation cost analysis and other baselines. Thus I maintain my score. \n\n",
            "summary_of_the_review": "This paper is well motivated and clearly written with sufficient theoretical and experimental backups. Overall, I believe this is a good paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a model-free pessimistic bootstrapping approach for offline RL. Specifically, the paper considers an actor-critic approach with an ensemble of Q-functions and utilizing disagreements in their predictions (measured as standard deviation) for learning the Q-functions. The paper also presents a way to regularize the learning with out of distribution state-action pairs which according to the paper is a crucial part in obtaining improved results. \n\n---\nUpdate post author response: I have increased my score.",
            "main_review": "The paper offers theoretical support in the linear MDP setting and has compelling empirical results in the D4RL suite (with both MuJoCo and Adroit tasks). Comments:\n\n1. Can the authors spell out all assumptions for the theoretical section of the paper when describing the problem setting? \n\n2. The paper presents why the proposed bonuses can be viewed as a lower confidence bound type penalty. The paper doesn't present a result comparing the value of the learnt policy against the optimal policy under more assumptions (e.g. Bellman completeness and an appropriate coverage/relative condition number assumption). See Chapter 3 in Agarwal et al. and the work of Zanette et al. as a means to obtain such guarantees.\n\n3. The assumption on \\Lambda_ood having eigenvalues lowerbounded by lambda appears rather strong (and rather extraneous). Comments on why this is needed or how it can be relaxed? \n\n4. The theory and practical sections don't particularly relate to each other in that the theory focuses on linear/tabular MDPs. One way to bridge this divide would be through working out uncertainty measures under a Kernel Nonlinear Regulator (KNR) assumption, see [Mania et al., Kakade et al.].\n\n5. Regarding hyper-parameters:\na. Are the hyper-parameters in table 2 used in all environments considered in the paper?\nb. What were the hyper-parameters that exhibited the most sensitivity? Could you mention what ranges these values these hyper-parameters were grid searched on?\nc. For a given hyper-parameter, did you have to perform online interactions after every few gradient steps to examine the value of the policy that you have, or did you perform online interactions once per hyper-parameter (at the end of learning)?\nCould you detail answers to these questions in the appendix?\n\n4. Could you mention what Q offline and Q current policy mean in figure 10?\n\nAgarwal et al. Reinforcement Learning: Theory and Algorithms\nZanette et al. Provable Benefits of Actor-Critic Methods for offline RL\nKakade et al. Information Theoretic Regret Bounds for Online Nonlinear Control\nMania et al. Certainty equivalent control of LQR is efficient.\n",
            "summary_of_the_review": "The theory results in their current form are limited, and this can be addressed by expanding its scope to include classes of non-linear dynamics models; furthermore, the theory results are partial in their characterization since they fall short of characterizing the sub-optimality of the learnt policy as a function of properties of the offline dataset (measured in the form of coverage). That being said, the paper is definitely promising in terms of its empirical results. I'd be up for revisiting my reviews if these issues can be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper looks at the offline RL problem and claims that existing techniques penalize out-of-distribution actions too aggressively and so perform poorly at generalization and extrapolation. To remedy this, the paper proposes an offline RL algorithm in which an ensemble of critics is trained with an objective composed of (1) a TD error based on actions seen in the dataset with target value penalized by standard deviation over the ensemble of the next Q-values, and (2) a squared error on the Q-values of OOD actions regressing to those same Q-values penalized by the standard deviation over the ensemble. The proposed algorithm is paired with a theoretical analysis connecting it to recent theoretical offline RL papers. The algorithm is evaluated on the D4RL benchmarks and shows favorable performance compared to baselines.",
            "main_review": "Strengths:\n\n-- This specific algorithm is novel, although borrows some elements from existing works (which are all referenced properly, as far as I can tell).\n\n-- Experiments are setup up properly and the algorithm shows nice results compared to baselines.\n\n-- The writing is clear and easy to follow.\n\nWeaknesses:\n\n-- I feel that some of the theoretical results in the main text are misleading, especially when compared to what is actually proved in the appendix. Namely, Lemma 1 in the main text asserts that the bootstrapped uncertainties in the algorithm are equivalent to LCB penalties at the heart of recent theoretical works. In contrast, the appendix establishes a completely different equivalence, showing that when performing Bayesian linear regression, the posterior is related to the same LCB penalties used in recent works. There is no established equivalence between Bayesian linear regression and the bootstrapped uncertainties used in the proposed algorithm, and in fact, I don't think there would be any equivalence, since in the only variation/diversity introduced into the critic ensemble is via initialization and SGD, which are irrelevant in the linear closed-form setting.\n\n-- The OOD term of the critic loss seems strange to me. It is effectively regressing each ensemble member Q_i to Q_i - uncertainty(Q), so the Q_i's will be encouraged to decrease indefinitely, until uncertainty(Q) becomes zero somehow. In fact, in a tabular setting, the gradient update would decrease each Q_i by the same amount, learning_rate * uncertainty(Q), and so the uncertainty will be unchanged over SGD updates, and so the Q_i's will diverge to negative infty.\n\n-- I would be interested to see more ablations (I may have missed some of these); e.g., the same algorithm but without the OOD term in the critic loss, or perhaps replacing the OOD penalty with some other penalty (CQL-style penalty?). Perhaps also measuring the uncertainty in other ways -- previous works (BEAR, BCQ) backup a minimum over an ensemble; is this better than penalizing with stddev?",
            "summary_of_the_review": "Overall, the paper is a nice contribution, but I think some aspects of the presentation need to be changed before it is ready for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}