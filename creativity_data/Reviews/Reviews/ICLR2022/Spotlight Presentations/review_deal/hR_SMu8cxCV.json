{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This is a strong empirical paper that studies scaling laws for NMT in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. The extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep NMT. The datasets used in the study are non-public, which may make it hard to reproduce the evaluation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies scaling laws for NMT. It confirms and extends some of the existing work on scaling laws, and as such it is a valuable contribution to the field. In particular, new questions addressed include: does the encoder-decoder architecture of NMT share the same scaling law as decoder-only LMs? How does training data (backtranslated or not) affect scaling? ",
            "main_review": "Pros:\n- I really enjoyed reading this paper. The research questions are clearly stated, and the results are fascinating to go through. Well-done!\n- The number of experiments is impressive. This adds weight to the papers conclusions, which are certainly thought-provoking. For example, the conclusion that scaling law should be bivariate, treating the number of encoder and decoder layers separately, makes me think we should not train models of symmetric number of layers anymore. \n\nCons: \n- The footnote on comparison to Gordon et. al. in EMNLP2021 is too terse and should be expanded. That paper studied scaling laws for NMT as well, but in a different parameter regime. Do your conclusions affirm, reject, or extend theirs? ",
            "summary_of_the_review": "This is a strong empirical paper that extends our understanding of scaling laws in deep learning. The research questions are clear and the results are convincing. I think it will be cited often in the future. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides interesting scaling properties for autoregressive neural machine translation models, which are validated on comprehensive experiments. The authors investigate the scaling properties from three perspectives:\n1. Quantify the evolution of model quality as a function of model capacity for encoder-decoder NMT models.\n2. How the composition of training/test data affects the scaling behavior of NMT models?\n3. How generation quality evolves with the improvements in cross-entropy resulting from model scaling?",
            "main_review": "Pros:\n1. The proposed function of model capacity for encoder-decoder NMT model performance is useful for parameter allocation of NMT models training.\n2. This paper provides experiments to explore how the composition of training/test data affects the test/training loss for scaling NMT models. It's an important question to connect the other two questions.\n3. It is important to investigate how generation quality evolves with the improvements in cross-entropy resulting from model scaling. The problem is of great value for both NMT practice and research.\n\n\nCons:\n1. The authors have proposed three research questions, while the relationship among them is not clear.\n2. The scaling property of the composition bias of the train/test sets has been less studied in previous work, and is most interesting to the reviewer. This part can be more convincing if the authors can provide more experiments and analyses.",
            "summary_of_the_review": "The research questions explored in the study are important and interesting, e.g., the scaling properties of composition bias and the influence of source/target naturalness on the scaling behaviors. However, the relationship among these research questions and findings is not clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a study on the scaling power of NMT Transformer (encoder-decoder).\n\nIt shows: \n- how the scaling of encoder-decoder compares to language model\n- that NMT behaves very differently on translationese texts \n- propose to model the scaling of NMT encoder-decoder\n",
            "main_review": "\nStrengths:\n- the research questions answered in this paper are interesting and help to better understand the mechanics of NMT. As an expert in MT, I really found this paper interesting.\n- findings are very insightful\n- this paper will be impactful: it will guide NMT  (or any other kind of encoder-decoder model such a BART) practictionners to build more efficient and effective models \n- another proof that target-original test sets are not suitable for NMT evaluation (Figure 8 should go on Twitter when you publish your paper!)\n- evaluation is not only performed with BLEU (BLEURT is also used)\n\n\nWeaknesses:\n- almost the entire evaluation is not reproducible: undisclosed datasets, use of a non-standard BLEU implementation, pre-processing of datasets not detailed, etc.\n- the paper misses a related work section (more details than in the introduction). The reader may not know related work on language models for instance",
            "summary_of_the_review": "All the \"strengths\" pointed above justifies my recommendation for the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper provides scaling laws for machine translation models. They experiment on different language pairs and domains and focus on large-scale Transformer-based NMT models.\n\nThey find that the scaling behavior of cross-entropy loss can be described as a bivariate function of encoder and decoder size and make recommendations for optimal allocation of encoder/decoder capacity. They also find that “translationese” can affect the model scaling behavior. Finally, they study the correlations between cross-entropy loss and BLEU/BLEURT scores.",
            "main_review": "Strengths:\n1. They conduct extensive experiments on four language pairs and five different test domains.\n2. It is interesting that contrary to previous findings, this paper suggests that it is better to scale the decoder than the encoder, providing guidance for future research on deep NMT.\n3. The analysis on the translationese provides insights on the correlations between cross-entropy loss and generation quality, and its effect on the scaling laws.\n\nWeaknesses:\n1. The scaling laws have several test set-specific parameters, which makes it questionable if the laws have practical values. For example, if we want to get the optimal allocation of encoder/decoder capacity using their method, we have to train models of different variants and fit their proposed formula, so why don’t we just do parameter search with the same training budget?\n2. I appreciate that the authors make efforts to measure the outputs in both BLEU and BLEURT, but it would be better to include human evaluation results that can better reflect the generation quality of models.\n3. The paper mainly focuses on the relationship between cross-entropy loss and the number of Transformer layers in large-scale settings, while there are a lot of other factors that can be considered. For example, it is unclear whether the scaling laws will hold if the architecture varies or if the hidden size changes.\n",
            "summary_of_the_review": "While the paper has some minor issues, it conducts extensive experiments in large-scale settings and the results can provide insights for future research, thus I’m leaning towards an acceptance of the paper",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}