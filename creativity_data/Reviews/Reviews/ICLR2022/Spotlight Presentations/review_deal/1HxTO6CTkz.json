{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors describe a mapping from likelihood free inference to black-box sequence optimization, then use this mapping to link common algorithms in both fields. They go on to describe novel black-box sequence design algorithms induced by known LFI algorithms. Empirical results show their methods are competitive on standard datasets.",
            "main_review": "The link described in this work is interesting and the novel algorithms proposed contain significant differences to existing sequence optimization techniques. Empirical results support claims that these novel algorithms are interesting and bear consideration for future design efforts. \n\nStrengths\n- Both Iterative Scoring and Iterative Ratio lead heavily on supervised learning (outputting low-dimensional predictions) compared to many existing design algorithms. Training regression models instead of likelihood models on protein sequence space could yield useful empirical advances, making this an interesting contribution.\n- The empirical evaluations are on well-known datasets and baselines appear to be used correctly. Results align with the conclusions of the paper.\n- Drawing a distinction between \"forward modeling\" and \"backward modeling\" could provide generally useful language for the community and enable communication of ideas.\n\nWeaknesses\nThe main weakness is in presentation of the link itself. In general the link seems to be \"correct\" in the sense that it is meaningful and consistently applied to link algorithms. However its exposition does give clear intuition for what is going on. I recognize this is not easy and try to provide some useful feedback below.\n- On the one hand, the quantities (theta, x) have clear distinctions as parameters and data. On the other hand, the quantities (E, m) are a set of sequence and a sequence and do not have such a clear distinction. It is quite hard to see how the set E pops out of the mapping T described beneath Table 1. \n- The notation p(E | m) = p(m \\in E | m) is very confusing. The function p(E | m) reads like a probability distribution over subsets of sequence space, not a distribution over sequence space restricted to the subset E. I recommend finding a better notation here. This comment is based on equation 3, which I could be misunderstanding.\n",
            "summary_of_the_review": "The contribution of algorithms which heavily rely on regression / classification to guide sequence design is interesting, especially since they are the product of a general mechanism for producing sequence optimization methods. The empirical results seem sound. The exposition of the model could use work to help readers have a crisper sense of how probabilistic modeling gets linked to a problem setting where the only randomness is in experimental noise.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper draws on connections between likelihood-free inference and black-box optimization to propose new black-box optimization methods. In general, the goal here is to not find the exact optimum of the black-box objective, but to sample from a set of sequences with high-quality objective. This is akin to the problem of collecting posterior samples in a likelihood-free inference problem. \n\nThe paper provides a number of proposed methods and compares them on some benchmark sequence optimization problems that have appeared in recent literature.",
            "main_review": "Can you please comment on the relationship between your work and 'Derivative free optimization via repeated classification' https://arxiv.org/abs/1804.03761?\n\nThe paper builds up a variety of optimization methods, building on a line of work in the LFI literature. This leads to a lot of approaches to compare, and there is no clear indication as to what practitioners should use in practice. What do you actually suggest people should use?\n\nAlong these lines, the paper has far too few details about the actual optimization approaches. For example, IS-A and IS-B seem to be some of your strongest methods and these require MCMC to sample new proposed sequences. There is no discussion of how this MCMC is done. Further, there is no discussion of neural network architectures, optimization methods, etc. The paper would be stronger if it just focused on one method and provided sufficient details for practitioners to actually use it.\n\nI found this sentence in sec 4.2 very unsatisfying. Do you have any further insights about performance differences? \" This indicates that composite methodsâ€™ way of using parameterized models to replace computational procedures is not the optimal solution for small-scale tasks.\"\n\nI was surprised that the error bars were so small in all of your experiments, as I expect that the trajectory of an optimizer has high variance due, for example, to the initial set of sequences that are sampled. What do your error bars correspond to? Are they standard errors or standard deviations? Also, what sources of randomness are you accounting for when you generate multiple random trials?\n\nFor the methods that combine both a generative and discriminative models, there are two sources of approximation error. It would be very helpful if you isolated these by performing oracle experiments, where you assume that the forward model is exactly correct. This can be used to isolate the impact, for example, of using the amortized sampler q_\\phi in Alg 10. Just replace the forward model with the ground truth objective function. Can you run a quick experiment?\n",
            "summary_of_the_review": "The paper has some interesting methods, and the connection to LFI is helpful. However, it does not have a clear empirical recommendation for what algorithm readers should use going forward and the paper does not provide adequate details to understand how to go about actually using these methods in practice.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors draw direct parallels between likelihood-free inference (LFI) and black-box sequence design.  This allows that authors to draw parallels between existing methods from the LFI and black-box sequence design literatures.  In a few cases there is no direct analog in the black-box sequence design literature for a given LFI algorithm, and so the authors are able to immediately propose such an algorithm.  The authors also present a number of \"composite\" methods that combine ideas from a number of these approaches.",
            "main_review": "I found the paper to be extremely clear and well written, providing an excellent review of both the LFI and black-box sequence design literatures and drawing clear, clean parallels between the two fields.  The proposed algorithms are all sensible and seem to work well on the empirical tasks, and the connection between the two fields seems like a fruitful area for further exploration (especially using the presented framework).  I have few comments below:\n\nTypos:\n- On p. 4 it is stated that \"...to model the general posterior $p(\\theta|\\mathbf{x})$, which takes arbitrary $\\theta$ and $\\mathbf{x}$ as two inputs and outputs a distribution\", but these models only take $\\mathbf{x}$ as input and output the conditional distribution over $\\theta$ (i.e., the posterior).\n- There are a number of typos in the supplementary materials (appendix).  The meaning was always clear, but that document could use some thorough copy editing.",
            "summary_of_the_review": "The paper is clear and well-written and provides a very useful conceptual framework tying two subfields together, with sufficient empirical evidence to show real gains from this approach.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper relates likelihood free inference to methods for biological sequences design and proposes new sequence design methods based on this insight.",
            "main_review": "# Major comments\n1) The paper is missing a related works section with an overview of existing methods for sequence design and likelihood free inference, and how they relate to the methods that were introduced in this paper.\n\n2) Section 3.1: FB-VAE and DbAs are both Estimation of Distribution Algorithms (EDA) based on a generative model. EDA is closely related to expectation maximization (https://arxiv.org/pdf/1905.10474.pdf). Please describe more clearly the differences and similarities of SNP, EDA, and EM.\n\n3) Section 3.1: Please describe the differences between FB-VAE and DbAs more clearly. Both approaches update a VAE iteratively by fitting it on the top scoring sequences. What does 'top' mean? Are there differences in the 'fitting' of the VAE?\n\n4) Section 3.2: IS is a discriminative approach for sequence design (or blackbox optimization) similar to (Bayesian) model-based optimization (MBO). Please describe the similarities between IS and MBO and differences (if there are any).\n\n5) Section 3.2: Please describe what 'construct q(m) using f(m)' means. What is f(m) and q(m) in your experiments? How were they trained and which hyper-parameters were optimized?\n\n6) Section 3.2: What are the differences between IS-A and IS-B?\n\n7) Section 3.3: IR seems like a minor variation of IS that uses a classifier instead of a regressor as surrogate models, which is not new. Are there any other differences?\n\n8) Section 3.3: Please describe which models you used for IR in your experiments and how sequences were generated.\n\n9) Section 3.4: Please describe more clearly how IPS and IPR relate to (and differ from) existing design methods that combine generative and discriminative models, e.g. RL, GANs, or optimizing a surrogate model using DbAs/CbAs, for example.\n\n10) Section 3.4: Please describe the differences between IPS-A and IPS-B more clearly, including 'differ in the detailed construction of the distribution q(m)'.\n\n11) Experiments: Since the performance of algorithms can be sensitive to the batch size, I would like to see experiments with a different batch size than 100, e.g. small (1), medium (100) and large (500).\n\n12) Experiments: Please compare to Bayesian Optimization (and RL if possible) using the same surrogate model as used for IPR/IPS and tuning hyper-parameters in the same way.\n\n13) Experiments: Please describe which hyper-parameters you tuned and how they were tuned.\n\n14) Experiments: Please describe what the boolean feature \\mathcal{E} is in all or your experiments.\n\n15) Experiments: Please describe the 'Evolution' baseline more clearly. \n\n16) Experiments: Please motivate why you report the average reward of the top-10/100 sequences instead of just the maximum reward (top-1). The average can be maximized by reporting identical or very similar sequences. More important for practical applications is that the optimizer finds a diverse set of high-reward sequences as explained in Angermueller et al, who used additional diversity metrics to quantify this.\n\n17) Experiments: How did you initialize the optimization? I would like to also see experiments that are initialized with a small set of labeled sequence (e.g. one or few parent sequences/homologs), which often exist in practice.\n\n\n# Minor comments\nSection 1, 'de novo biological sequence design': Describe which kind of sequence (DNA, RNA, protein, molecules represented as strings, ...?).\n\nSection 2, 2nd paragraph: Please cite reviews (e.g. http://arxiv.org/abs/2106.05466, http://www.nature.com/articles/s41592-019-0496-6) of existing ML design methods instead of single papers (Ahn, Gottipati, ...).\n\nDenoting 'm' as sequence and 's = f(m)' as the oracle function value is confusing since 's' is the first letter of *s*equence. I strongly suggest to use 's' to denote a sequence and, for example, 'y = f(s)' to denote the function value.\n\nYour benchmark problems seem to be similar to the benchmark problems introduced in Angermueller et al (except for Flu). If this is the case, please describe that you reused the benchmark problems from Angermueller et al and describe possible differences. By referencing Angermueller et al instead of describing each benchmark problem in detail, you can also shorten the Experimental section.\n\nExperiments (Flu). You hypothesize that backward modeling techniques perform better since sequences are long. Although sequences are long, there may be only a few variable positions while most positions are conserved. Since generative models can easily fit this conservation better they may perform well in this case since the optimization problem becomes trivial as only few variable positions are mutated.",
            "summary_of_the_review": "* The outlined relation between likelihood free inference and sequence design (blackbox optimization) is interesting; I am not aware of any existing papers with this insight. However, I am not sure about the impact of this insight on how sequences are designed in practice.\n* It is not described clearly enough how the proposed methods differ from existing methods for sequence design such as Bayesian model-based optimization, GANs, or RL.\n* Important details about the proposed methods and performed experiments are missing, which makes it hard to understand and assess.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}