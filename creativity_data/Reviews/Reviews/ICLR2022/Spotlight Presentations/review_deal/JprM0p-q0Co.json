{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well-defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR-10. I would have like to see class-conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top of SoTA GANs. Overall, I recommend acceptance as a spotlight."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The diffusion models or score matching-based models have achieved great success in generating high fidelity samples with diverse modes. However, the sampling requires tremendous iterations, which hinders the practical usages. This paper proposes to break the Gaussian assumption in denoising steps by parametrizing it with a multimodal conditional GAN. The time for sampling can be 2000 times faster than the original diffusion model while keeping competitive quality and diversity.",
            "main_review": "Generally, this paper proposes a good idea to solve the trilemma in generative learning, which accelerating the sampling in denoising diffusion models by using conditional GANs. The idea is clean with the experimental support. There are several minor issues in the required addressing as listed below.\n\n1) It is desirable if the conditional GANs in the proposed model can be tested with different settings. It is currently non-saturating GANs. Does the proposed approach also work better with advanced conditional GANs? \n2) The results of FID should be specified with the evaluation dataset or both (FID (train) and FID (test)).\n3) It is desirable to make the diffusion model continuously improve as the iteration increases. However, when T is greater than 4, the performance decreases.\n4) The format of references should be consistent (e.g., with or without abbreviations).",
            "summary_of_the_review": "This paper solves an important problem in denoising diffusion models by parametrizing denoising with a multimodal conditional GAN. Experimental results show that the proposed model achieves good sample quality, mode coverage and speed without struggling in the trilemma. The minor issues are in the experiments. It is encouraged to test and discuss different setting. Therefore, I recommend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The goal of the paper is to develop a generative model which achieves three desirable goals at the same time: High-quality samples, data/mode coverage and fast sampling times. While GANs have been associated with high-quality samples and fast sampling times, they are also known for their tendency to ignore modes of the data. On the other hand, diffusion models have recently been demonstrated to achieve high-quality samples and good likelihood estimation, which can indicate good mode coverage. However, diffusion models tend to be slow to generate samples due to their iterative nature. Thus, the paper proposes to combine diffusion models with GANs to combine their complementary advantages regarding mode coverage and sampling times (together with their shared ability to produce high-quality samples).\n\nThe main reason why diffusion models require a lot of iterations is because they model each of the steps of the reverse diffusion process with a Gaussian distribution, which is only valid for very small timesteps/a large number of steps. Thus, instead of using a network to estimate parameters of a Gaussian reverse process, the authors propose to model steps of the reverse process with a GAN. In theory, a GAN could directly model the data distribution and therefore solve the task in a single step, where the approach would be equivalent to a GAN and therefore suffer again from mode collapse. The main hope is therefore that by using a few diffusion steps, such an approach is still relatively fast compared to pure diffusion models while also having a good mode coverage.\n\nExperiments evaluate sample quality, sampling speed and mode coverage. The main evaluation on CIFAR-10 indeed demonstrates a favorable combination of sample quality according to FID and IS, sampling speed and data coverage as measured by Recall. Additional experiments analyze the effect of various ablations and demonstrate an application in stroke-based image synthesis.",
            "main_review": "Presentation\n- The paper is easy to read and understand. It contains many helpful figures, qualitative examples and quantitative comparisons summarized in tables. I especially like Fig. 14 which is very useful in understanding the amount of information within each of the diffusion scales, since this is sometimes very difficult to judge from a visualization of the noisy states themselves.\n- The paper motivates the problem it is addressing well through the mental image of a trilemma.\n\nLiterature/Background\n- The related work section covers many relevant works and provides a good classification of the current work within the existing literature.\n- It contains the necessary background material on diffusion models to be sufficiently self-contained.\n\nApproach and evaluation\n- The approach is relatively simple and provides good results. Experiments convincingly demonstrate that the approach indeed strikes a good balance of the three desired goals.\n- A number of ablations analyze different aspects of the model in more detail.\n- Besides helping with mode coverage, the approach brings other advantages over a pure GAN training such as discriminator regularization helping against overfitting on small datasets, and potentially more control over the generation process (as also evidenced by the stroke-based synthesis).\n\nAdditional Comments\n- Loss weighting: For diffusion models, the weight applied to the objective from different steps of the diffusion process can be critical. In fact, a change in this weighting has been one of the break-throughs in [Ho, 2020] that lead to the current success of this class of models. The choice of this weight in the presented approach is not discussed. Is it simply the same for all timesteps? Did you experiment with this?\n- Choice of generator architecture: While it is nice to see that a similar model as used for diffusion models also works as the conditional generator in the presented setting, Tab. 2 suggests that it is actually a bad choice for an unconditional/one-step generator. Have you experimented with different choices there? Does it help to use a different architecture for the first (unconditional) step compared to the other steps?",
            "summary_of_the_review": "Diffusion models come with a lot of desired benefits such as high quality samples, data coverage and stable training. However, their slow sampling speeds limit their applicability and the problem of improving this aspect with a new generative model is very relevant. The idea of combining diffusion models with the benefits of GANs by modeling the reverse process with conditional GANs is plausible. The main question is whether such an approach can really combine the advantages of the different models. To that end, the experiments are quite convincing and I think the paper provides a positive answer to that question, which will make it significant for future research. Thus, I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces _Denoising Diffusion GANs (DDGANs)_ to solve the so-called \"generative learning trilemma\" by training a generative model that achieves high-quality sampling, fast sampling times, and at the same time, high mode coverage. The main intuition of the paper is to combine GANs, which have been shown to achieve both high-quality synthesis and very fast sampling times, and DDPMs, which provide high-quality sampling and good mode coverage. In contrast to previous work on diffusion models that rely on Gaussian transition probabilities of the reverse forward process, conditional GANs are used in this work to allow flexible, multimodal denoising distributions in the reverse process and thus much shorter sequence lengths. Experiments show that the proposed approach indeed achieves higher mode coverage than conventional GAN models while maintaining high sample quality within 4 to 8 diffusion steps. The model can also be used for stroke-based image synthesis, with significant improvements in sampling times compared to previous approaches.",
            "main_review": "**Strengths** \n\nI think this work and the proposed combination of adversarial and likelihood-based training is a natural and novel extension of DDPMs towards faster sampling times. The main idea of modeling the conditional transition probabilities with more informative models is simple (which is a good thing). Most design decisions are justified by ablation experiments (e.g., type of parameterization, number of diffusion steps, deterministic vs. stochastic generators, etc.). Experiments with the 25-Gaussian and StackedMNIST datasets show the improved mode coverage over GAN models.\n\nThe results on CIFAR-10, CelebA-HQ and LSUN Churches are competitive with previous state-of-the-art methods. Furthermore, I really like the underlying motivation of the work that interactive image processing or speech synthesis require very fast sampling, and I agree that this is a pressing issue when dealing with diffusion models. \n\n**Weaknesses**\n\n- Fig. 1 is a good motivator, but a bit imprecise: (non-reweighted) diffusion models also belong to the class of likelihood-based models, as do autoregressive generative models. The latter are slow, but cover the modes and provide high quality.\n- How important is the choice of variance schedule? If I understand correctly, the work relies exclusively on the schedule given in equation (16). How does the schedule affect controlled synthesis, e.g., stroke-based image synthesis?\n- The paper should address previous work that also attempts to solve the trilemma by reducing the number of steps required for a trained DDPM model, e.g., DDIM [1] or GGF [2], and compare with them.\n- It should also be shown that the coverage of modes on more complex, high-resolution data (such as LSUN Churches) is still as good as the results on the Toy data suggest.\n- Do the results presented in Tab. 2 (ablation studies on CIFAR-10) also hold for high-resolution (i.e. $256 \\times 256$) datasets?\n\n**Mixed Comments:**\n\n- Is there evidence that diffusion models actually provide (approximate) SOTA diversity? My impression is that they can be tuned either to very good likelihoods (as in [3]) or to high quality samples (as in [4]), but currently cannot do both at the same time. The best performing diffusion models in terms of _quality_ are based on the reweighted objective introduced in [5], which sacrifices likelihood interpretation for better sample quality.\n- Figure 9 nicely demonstrates controlled image synthesis/modification. However, compared to [6] only 4 steps provide less fine-grained control over how much content is obtained.\n- How does the model perform on other typical but more complex synthesis tasks such as LSUN Cats or class-conditional generation of ImageNet?\n- Just out of pure interest: Have you tried using the UNet as a discriminator as well (similar to [7])? How important is the exact implementation of the discriminator architecture?\n\n__References__\n\n- [1]: Song, J., Meng, C., Ermon, S.: Denoising Diffusion Implicit Models\n- [2]: Jolicoeur-Martineau, Alexia, et al. \"Gotta Go Fast When Generating Data with Score-Based Models.\"\n- [3]: Kingma, Diederik P., et al. \"Variational diffusion models.\" \n- [4]: Dhariwal, Prafulla, and Alex Nichol. \"Diffusion models beat gans on image synthesis.\" \n- [5]: Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\"\n- [6]: Meng, Chenlin, et al. \"Sdedit: Image synthesis and editing with stochastic differential equations.\" \n- [7]:  Schonfeld, Edgar, Bernt Schiele, and Anna Khoreva. \"A u-net based discriminator for generative adversarial networks.\" \n",
            "summary_of_the_review": "In summary, the paper provides a nice combination of adversarial and likelihood-based training and, in my opinion, represents a natural and novel extension of DDPMs towards faster sampling times. The experiments validate the claim that denoising diffusion GANs can indeed help overcome the \"generative learning trilemma\", and most of the ablation studies justify the design choices made in the paper. I think this is a good paper worth publishing at ICLR 2022, and I am willing to increase my score if my remaining questions and concerns are adequately answered.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use a conditional GAN as a more expressive model of the\ndenoising step in a denoising diffusion model, allowing to reverse the diffusion\n(sampling) much more efficiently, while maintaining the high quality of diffusion\nmodels.",
            "main_review": "The authors start by showing with a toy example that the denoising conditional\ndistribution can be heavily multi-modal for large diffusion steps, highlighting\nthe limitations of the typical Gaussian model, which can only be assumed for\nvery small steps, thus leading to an expensive multi-step sampling process. The\nauthors overcome this by using a conditional GAN to model the denoising\ndistribution under large steps, and experimentally show that this can indeed\nmodel very multimodal distributions. This allows the reversal of the diffusion\nprocess using much fewer steps than in competing methods, achieving efficient\nsampling speeds while maintaining the high sample quality and mode coverage of\ndiffusion models. While taking this idea to the extreme would lead to the\nstandard one-step GAN, interestingly the authors show that their method\noutperforms standard GANs, and the provided justification is that in the\nmulti-step approach the distribution mapping in intermediate steps is less\naggressive than direct Gaussian-to-data-distribution, and thus easier to learn.\n\nTo the best of my understanding I couldn't identify any serious weakness in \nthis work.",
            "summary_of_the_review": "I think the proposed idea is clever and very well executed. Experimental results\nvalidate the method in terms of the triple sample quality / mode coverage /\nsampling efficiency trade-off. Experiments demonstrate their conditional\ngenerator exhibits the strong multi-modality required for the large reverse\ndiffusion steps.  I expect this hybrid between conditional GANs and diffusion\nmodels to become an important player in generative modeling.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}