{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The authors propose a well-presented approach to likelihood-free inference. The reviewers are all in alignment in recommending this paper for acceptance. There was a healthy discussion between authors and reviewers, where the authors have already incorporated many of their recommendations. The potential for this methodology to be applied to situations with expensive simulators should be intriguing to a broad audience. As a result, I recommend for this paper to be accepted as a spotlight."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a form of variational inference (VI) approach for likelihood-free inference (LFI) named Sequential Neural Variational Inference (SNVI). Its main advantage over other VI approaches for LFI is computational efficiency rather than accuracy.\n\nThis approach has consists of three parts, a likelihood or likelihood-ratio model, a posterior model, and a sampling importance resampling (SIR) step to refine the posterior model each round. Because the variational family for the posterior model used is based on normalising flows (NFs), whose expressiveness is already very high, the authors instead focus on the variational objective of the posterior model. Here they consider three variants from previous works - (1) forward KL divergence with self-normalized importance sampling, (2) importance weighted ELBO, and (3) Rényi $\\alpha$-divergences. Method (1) prevents objective weights to be close to zero by normalising them, at the cost of introducing a bias that vanishes at rate $\\mathcal{O}(1/N)$. Method (2) aims to provide good proposals for SIR through lower bounding the ELBO, and uses the \"Sticking the Landing\" (STL) estimator to circumvent the low signal-to-noise (SNR) ratio. Method (3) allow the mass-covering or model-seeking behaviour to be tuned, and require the same STL estimator to prevent low SNR.\n\nFinally, the authors propose an SIR step from previous work which claims to enrich the variational family with minimal computational cost.\n\nIn the case of invalid data from simulations, a kernel is used to reweigh the loss to focus on specific regions in the data-space. To fix the bias, the bias factor is estimated from another model to recover the posterior up to proportionality.\n",
            "main_review": "The paper is clear from a method-recipe perspective. It has communicated well what the key aspects and elements of the approach are and how they are used together to form the overall approach.\n\nThe main weakness of the proposed approach is that it takes the form of combining multiple existing techniques into one coherent approach. There is indeed value to the community in formulating an approach that blends the strengths in multiple existing techniques together to overcome their individual weaknesses. However, both the techniques themselves and the way they are orchestrated together are not particularly novel or insightful. Specifically, each of the three parts to the SNVI approach, as well as the three variants of the variational objective used for the posterior model part of the approach, the SIR step, and the fix using calibration kernel, have already been well established by previous works, which the authors do clearly cite. Again, there is indeed value in orchestrating these techniques together, however in its current form the way this is done is neither particularly elegant or insightful.\n\nAs a minor and related weakness, it is not clear how one would choose between the three variants of the variational objective here. In terms of writing, each of these are briefly discussed with a summary and citation to the original work without much discussion on motivating or explaining them in relation to the wider proposed approach. I also find the discussion on SIR lacking and difficult to understand exactly how this enriches the variational family, although there is a citation to the original source which may have explained it. \n\nFinally, the proposed method to get around missing simulation using the calibration kernel seems arbitrary and over-convoluted. This introduces not just a new set of hyperparameters that is not part of the main approach to worry about, but also an entire third model on top of the likelihood(-ratio) and posterior models.\n\nTo change my sentiment, I would find it most helpful if the authors could provide some explanations or arguments that would incline me to believe that:\n- The choices made in this paper must be done in the particular way described for it to have the desired properties, and is not arbitrary.\n- There are alternative choices for each component of this technique that seem sensible but can be shown to be worse than this combination of techniques.\n- The method without SIR is not sufficient ensure the quality of the posterior samples despite expressiveness of NFs (as well as how the SIR step works, ideally in pseudo-code)\n- The calibration kernel fix and the extra bias-factor model is not arbitrary and its learning or selection does not bear too much additional computational hurdle to the overall technique\n\n-----\nAfter author response:\n\nThank you for your response and clarifications, and especially on acting on the feedback.\n\nThe authors has addressed my concerns in detail and communicated how they would revise the paper accordingly for the camera-ready version. I would lean towards acceptance.",
            "summary_of_the_review": "Overall, while the technique proposed seems useful in improving computational efficiencies of existing VI approaches to LFI, the technique itself is a relatively uninspiring combination of existing techniques, of which a few aspects thereof seem either lacking in explanation or over-convoluted in construction. I would tend towards a reject, but welcome the authors to challenge this sentiment by addressing the points in my main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents Sequential Neural Variational Inference (SNVI), a new simulation-based inference (SBI) technique. SNVI sequentially updates a surrogate likelihood model and an approximate posterior model in high posterior density. Finally, the approximate posterior is sampled with importance resampling. Experiments shows this technique is able to learn multimodal posterior distributions and is faster than methods relying on MCMC for generating posterior samples while being almost as accurate. ",
            "main_review": "## Pros:\n- The paper is well written.\n- Improving the efficiency of sequential SBI methods is important.\n- The paper nicely combines advanced methods and the results are good.\n## Cons:\n- It is yet not clear on which occasion it is a better idea to use a sequential method that will only be used once to do inference than an amortized method. I am sure there are good motivations for this but this is probably something that should motivate.\n- By using SIR it looks like you trust more the likelihood surrogate than the approx. posterior. I do not see any strong motivations for this in the general case where both distributions may be complex.\n- How does the method compare to simply learning an approximate posterior with a flow by minimizing some KL. It goes back to my first point, in which cases building the posterior online becomes more efficient.\n- There are now many SBI methods and I think this would make the reader's life way easier if you (at least briefly) introduce the methods you compare your algorithm to and why you do not compare to the many other methods.\n- Maybe you could give some additional context to the IW-ELBO (being explicit that it reduces the GAP between ELBO and likelihood).\n- It is not clear to me what should a user do from the 3 possible losses to optimize phi.\n- As you say that your method could be useful when we have many observations it would be interesting to show how does something that other methods cannot do. ",
            "summary_of_the_review": "I recommend weak acceptance as I think the method described in this paper may be interesting for the SBI community. I also find the paper well written and experiments convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes Sequential Neural Variational Inference (SNVI) for the posterior inference when the likelihood function is unknown. The proposed methods consist of a likelihood estimator, a posterior approximation, and sampling importance resampling. \n",
            "main_review": "Strengths:\n\n+ The idea is natural and the explanation is clear. \n\n+ It improves efficiency comparing to MCMC based SNLE and SNRE while achieves similar accuracy. \n\n+ The paper does a relatively complete ablation study. It compares the two types of KL divergence, MCMC and VI, and different variational objective in this setting. \n\n\nWeaknesses\n\n- The novelty of the paper is not very high. The major difference between the proposed method and  standard variational inference algorithm (and its variant) is that the likelihood here is not explicit.  The paper proposes to replace the unknown likelihood with an estimated likelihood, which is natural but the novelty is not very high.\n\n- The paper propose several methods to improve the inference quality, e.g., SIR, IWAE, $\\alpha$-divergence, but the reviewer concerns more about the accuracy in likelihood estimation. Can the  likelihood estimation capture the multimodality and skewness? Is there a simulation to demonstrate this accuracy?  \n\n- The calibration kernel in Sec. 3.4 is not explained clearly. How to learn $c(\\theta)$? how to draw sample from (unnormalized) posterior distribution? Does the simulations in Sec. 4.1 and 4.2 use the calibration kernel? Why the Algorithm 1 doesn’t have calibration kernel?\n\n- One question is that in the first step, instead of learning $p(x|\\theta)$ from $(x, \\theta)$ pairs, can we just learn $p(\\theta | x)$ from $(x, \\theta)$ pairs and use it to estimate $\\theta$? How does the proposed method compare to this simple approach? \n\n- The notations of the paper are sometimes confusing; here are some example:\nIn the Figure 1, the KL divergence is backward but in the method description, the KL is forward. \nOn page 4, should  $q_\\psi$ be $q_\\phi$?\nOn page 4, what are the subscripts of \\theta in the equation?\n\n",
            "summary_of_the_review": "In sum, this paper proposes a practical way for the posterior inference with unknown likelihood. The method is simple and natural, the writing is mostly clear. However, some sections and notations are not clear, and it is better to show the limitation of each component of the proposed method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents SNVI, a simulation-based inference based likelihood (ratio) estimation and variational inference. The main contribution is to sidestep the slow MCMC sampling step of SNLE (SNRE) by learning the posterior distribution with variational inference. This work is similar to SNPLA but replaces the rKL variational objective with three alternative objectives that induce a mass-covering behaviour and are, therefore, better suited when using the approximate posterior distributions as proposal distributions in the sequential regime. Experiments show convincing results.\n",
            "main_review": "Strengths:\n- This work solves a quite practical problem of SNLE and SNRE. \n- Experimental results show convincing results. Accuracy is maintained while reducing the computational cost of inference.\n- Experiments against SNPLA demonstrate that using any of three proposed variational objectives leads to better results. Modes in the posterior approximations are now correctly captured.\n- Posterior predictive checks are used to evaluate the quality of the approximate posteriors. \n- The supplementary materials are thorough and include helpful experimental details as well as further discussions.\n\nWeaknesses:\n- Figure 8 is meant to show results for SNRE and SNRVI, but the labels state SNLE and SNVI. Are the labels wrong?\n- Novelty is limited in comparison to SNPLA, but this is not a big issue as far as I am concerned since SNVI improves upon SNPLA. It now works empirically much better.\n\nQuestions:\n- The self-normalized importance sampling used to estimate fKL requires the evaluation of $w(\\theta)=p(x_o, \\theta)/\\pi(\\theta)$. How is $p(x_o, \\theta)$ computed, since we are in the likelihood-free setup? ",
            "summary_of_the_review": "This paper presents a simple and effective idea. It makes both SNLE and SNRE more computationally efficient while maintaining accuracy. Experiments demonstrate this clearly. Provided that my question above can be answered, I recommend this paper for acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}