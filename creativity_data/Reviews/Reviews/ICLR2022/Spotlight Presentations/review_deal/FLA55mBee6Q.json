{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents a new technique for constrained offline RL. The proposed method is based on reducing a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The proposed algorithm is tested against several baselines on both random grid-worlds and continuous environments. Results clearly show that the proposed algorithm outperforms baselines while keep the provided constraints satisfied. \n\nThe reviewers agree that the paper is well-written, the proposed algorithm is novel and technically sound, and the empirical evaluation clearly supports the claims of the paper. There were some concerns regarding the novelty of this idea, but these concerns were properly addressed by the authors in the discussion."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper has presented a DICE-based offline constrained RL algorithm for constrained RL. Experimental results on tabular CMDPs and continuous control tasks show that the proposed method can achieve a better trade-off between reward maximization and constraint satisfaction.\n\n1st contribution: They firstly proposed to tackle constrained offline RL by solving a single minimization problem. \n\n2nd contribution: To mitigate constraint violation in practice, they exploit the distribution correction obtained by solving the RL problem for cost upper bound estimation and then constrain the upper bound. ",
            "main_review": "Strengths:\n1) they propose to optimize the state-action stationary distribution directly, which avoids the instability caused by triple optimization problems for the actor, the critic and the cost Lagrange multiplier with three different objective functions in the existing work that manipulates both Q-function and policy.\n\n\nWeakness:\n1) important baselines (state-of-the-art offline RL with Lagrangian approach) are missing: \n- in CMDPs: besides MLE CMDP, only the variant of SPIBB, an offline RL method for tabular MDPs, is compared with the proposed method (applicable to continuous tasks). Behavior cloning is also missing in this setting to see whether the proposed method is just remembering the collected safe dataset.\n- in continuous control tasks: CRR, sota unconstrained offline RL without any modification to handle constraints, is compared with the proposed method\n2) It is always difficult to reach a confident conclusion that one method is better than others when a trade-off between rewards maximization and constraint satisfaction is needed. In both tabular CMDPs and continuous control tasks, although the proposed method shows better performance in satisfying the constraint, the constraint cost is always very close to the predefined single cost threshold. Ablation study on constraint cost threshold is required to both show the sensitivity of the proposed method to different cost threshold and consistent advantage over other baselines.\n\n\nSuggestions\n1) In tabular CMDPs, both constraint-satisfying policy and constraint-violating policy are used to collect datasets. In continuous control tasks, the online constrained RL agents are used to collect data. However, in the real world, agents normally act safely but have some unsafe attempts. It would be more interesting if a mixture dataset collected by both constraint-satisfying and constraint-violating policy is utilized for constrained offline policy learning.\n2) There is a concurrent work, constraints Penalized Q-Learning (CPQ) [1], similarly aiming at offline constrained RL. It would be great if some comparisons are studied between the proposed method and this work (either methodology aspect or empirical aspect).\n3) It would be great if there is some exploration describing how the proposed work can be adapted to hard-constraints scenarios since safety constraints in the real world are often hard constraints as well as constraints imposed by physical laws.\n\n[1] Xu, Haoran, Xianyuan Zhan, and Xiangyu Zhu. \"Constraints penalized q-learning for safe offline reinforcement learning.\" arXiv preprint arXiv:2107.09003 (2021).",
            "summary_of_the_review": "The paper proposes to optimize the state-action stationary distribution directly without considering the unstable triple intertwined optimization porblems in actor-critic-based constrained RL algorithms. However, the effectiveness of the proposed method in obtaining a trade-off between reward maximization and constraint satisfaction will be more convincing if 1) more straightforward adaptation of state-of-the-art offline policy learning algorithms are considered as baeslines; 2) more ablation study to analyze the sensitivity of the performance against different cost thresholds.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a DICE-family method for solving constrained offline reinforcement learning problems. To do this, they adapt ideas from OptiDICE and find a reduction from a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. Additionally, they draw on ideas from CoinDICE to estimate a confidence interval over the cost, which makes their method better at obeying constraints. The  authors compare their method to a number of baselines on both random grid worlds and continuous environments, and find that COptiDICE achieves both good performance and better constraint satisfaction than alternative methods. ",
            "main_review": "Strengths \n\n--The paper's approach seems to be very mathematically well-founded. Although the end result is slightly biased, the minibatch-free version simply minimizes a convex optimization problem, so I would assume this bypasses many of the convergence problems common to popular TD-based RL methods.  A line or two clarifying the theoretical guarantees would be nice though. \n\n--The experimental results also seem to demonstrate that the proposed method is better at satisfying the problem constraints than other methods. On a number of environments, this constraint satisficing also comes with little to no cost to overall performance. The authors compare against a good number of baselines on a wide range of environments. \n\nWeaknesses\n--While the result is interesting, its novelty seems a bit limited. The transformation from constrained optimization problem to unconstrained optization is of course drawn from previous DICE methods, although it is adapted here to include the cost constraint. The idea of constraining an upper bound on the cost rather than the estimated cost itself is also well-established. Ultimately though, I think the extension of an existing class of methods to a new domain they have not been applied in yet is sufficient novelty for publication. \n\n",
            "summary_of_the_review": "The paper is well-founded and feels like a \"correct\" solution to this problem, and the empirical results show the method performs well in practice. I",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studied the policy optimization problem in the offline constrained MDP setting. Compare with previous works, in which policy gradient based approaches are widely adopted, this paper solves the problem via policy visitation distribution that rooted from the primal-dual formulation of Bellman operator, which is novel. In order to guarantee the constraints are always satisfied, this paper provides a novel approach based on CoinDICE to efficiently estimate an upper bound of constraint violation. The author also provide sufficient empirical verifications to support their proposed algorithm.",
            "main_review": "This paper provide a interesting and novel algorithm to address offline safe RL problems. Most of the theoretical development and empirical looks good to me, but I have the following question for the author.\n\n(1) In order to make sure COptiDICE works in practice, do we need any special requirements for the behavioral policy (sampling distribution)? Specifically, do we required the behavioral policy (or the sampling distribution) to be feasible?\n\n(2) In the experiment of continuous setting, we can still implement CRR as a safe RL method via combining it with primal-dual approach, i.e., you can simply replace the policy optimization step in primal-dual approach with CRR. If we implment such an algorithm, can COptiDICE still outperfrom this \"safe-CRR\" algorithm?\n\n(3) Has the author considered testing the algorithm in some standard safe RL environment such as Gather, Circle, Half-Cheetah Safe, etc?",
            "summary_of_the_review": "The offline safe RL is a very challenging problem, but this paper provide a very promising approach to address several issues in this setting. Specifically, the approach proposed in this paper not only addresses the policy optimization issue in the behavioral agnostic setting but also guarantees the constraints satisfication, which are both significant contributions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the offline constrained reinforcement learning problem and formulates the problem as a CMDP. First, the paper presents the algorithm COptiDICE, which directly estimates the stationary distribution corrections of the optimal policy. Then, the paper shows that COptiDICE outperforms the baseline algorithms in terms of constraint satisfaction and return-maximization. ",
            "main_review": "Strength:\n- The paper is well-written and easy to follow. The motivation is well explained.\n- The paper considers applying DICE methods to constrained MDP problems. First, by imposing the closed-form solution of $w^*$, the paper reformulates the minimax optimization problem to a single minimization problem. Then,  the paper proposes a practical algorithm for continuous state-action space with neural networks as the function approximation. The paper contributes to the study of CMDP.\n- The paper provides an empirical study of the algorithm and shows it outperforms the baseline.\n\nWeakness:\n- The assumption that $d^D > 0$ is a strong assumption in offline RL, which assumes that the dataset uniformly covers the whole state-action space. Moreover, since $d^D$ is the empirical distribution of the dataset $D$, such assumption does not hold for continuous state-action space. The paper should provide more discussion on the restrictiveness of such an assumption.\n- The algorithm seems to be a combination of OptiDICE and CMDP. By considering the Lagrangian form of CMDP, the problem essentially reduces to an MDP problem. Thus, I expect less novelty of this paper compared with OptiDICE.\n- In the experiments, the dataset is generated in a way that enforces its exploration power, which makes the dataset covers the state-action space well. However, in real-world applications, the dataset may not be able to cover the whole state-action space, which is the main challenge of offline RL. In particular, in the case that the dataset does not cover the whole state-action space, I encourage the authors to show if the learned policy can outperform the data collecting policy.\n\n\n",
            "summary_of_the_review": "Overall, the paper is well-written and motivated. However, due to the lack of discussion on the coverage of the dataset, I think the paper is below the borderline of acceptance. I would be happy to raise my evaluation if the paper could address my concerns in the main review part.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}