{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper addresses the reward-free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal.  The proposed approach can work with any planning solver to provide an ($\\epsilon + \\epsilon_{opt}$)-optimal policy for any reward function.\n\nAfter reading the authors' feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication.\nI encourage the authors to follow the reviewers' suggestions as they will prepare the camera-ready version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper propose and analyze a model-based algorithm for the reward-free exploration setting. The analysis shows that the proposed algorithm is (nearly) minimax optimal. The proposed algorithm is agnostic of the planning algorithm that is used to solve an MDP constructed with the estimated model. This property is of independent interest because it allows the computation of the value of the policies, and the estimated model can be used for any down-stream tasks. ",
            "main_review": "# Strength\n\n- S1. The theoretical results are strong and interesting.\n- S2. The paper did a good job such that readers can grasp the high level proof idea.\n\n# Weakness\n\n- W1. There are some typos. Also, there are some arguments and proofs that I can't quite follow.\n\n# Comments and Questions\n\n- C1. In abstract, I think it would be better to replace \"interactions\" in \"taking $\\mathcal{O} (d^2 H^3 / \\varepsilon^2)$ interactions...\" with episodes. I imagine the number of steps $KH$ by \"interactions\".\n- C2. How do you solve $\\max\\_{V \\in \\mathcal{V}} \\| \\phi\\_V (s, a) \\|\\_{\\Lambda\\_{k, h}^{-1}}$ for $V$ in practice? Is it possible to obtain the optimal $V$ analytically?\n- C3. In Page 5, it is said that \"By the reduction from the tabular reward-free RL problem ... the linear mixture model setting is $\\Omega (\\frac{d^2 H^3}{\\varepsilon^2})$\". It is not unclear for me how to get $d^2$ rather than $d$. Would you explain it a bit more? Also, it would be nice if $H^3$ dependence is formally derived.\n- C4. In the equation at the bottom of Page 4, $\\beta$ is unnecessary.\n- C5. In Page 5, $\\phi\\_{t, h} (s_h, a_h)$ should be defined as argmax rather than max, I think. \n- C6. In Eqn 7, I think $P$ in $u\\_{1, k, h}^{\\pi, P}$ should be $\\widetilde{P}$.\n- C7. In Eqn 15, don't you need $B$ on RHS? Is there any assumption on $H$, $d$ etc so that $B$ disappears?\n- C8. In Eqn 21, LHS should be $W_h ( \\{ R_h \\})$.\n- C9. In Eqn 29, is $\\widetilde{V}_1^{\\pi}$ defined anywhere? I can infer its meaning, though.\n- C10. In Eqn 49, $H$ seems to be missing in $\\log$?\n- C11. In Lemma 9, \"Under event $\\Lambda_1$\" should be \"Under event $\\mathcal{E}_1$\"?\n- C12. In the definition of the noise $\\eta_k$ below Eqn 90, $\\theta_h^*$ should be $\\theta_h$?\n- C13. In Eqn 91 and 92, don't you need $B$?\n- C14. In Lemma 16, $\\theta_\\star$ above Eqn 199 should be $\\theta\\_\\*$, I think. Also in Eqn 199, $\\widehat{\\theta}\\_t$ is undefined, and $\\theta$ should be $\\theta\\_\\*$.\n- C15. There are some places where $\\star$ is used instead of $*$.\n- C16. Maybe it is nice to have a list of notations. There are too many $V$ variants, and I needed to look for their definitions.",
            "summary_of_the_review": "I think the results are strong and interesting. There are many typos in equations. While typos are not serious but needs revision. I found some parts of the proofs that I don't quite follow (lacking $B$) and undefined notations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies reward-free exploration with linear function approximation (i.e., under the setting of linear mixture MDPs). Specifically, this work introduces two algorithms: the first one has a simple form with the sample complexity $\\tilde{\\mathcal{O}}(d^2H^4 / \\varepsilon^2)$, and the second one is much complicated (i.e., with 5 estimators) but has an improved sample complexity $\\tilde{\\mathcal{O}}(d^2H^3 / \\varepsilon^2)$. To improve the dependence on $H$, the main technical challenge is that the traditional Bernstein-type analysis could not be directly applied because the variance summation of $\\{\\tilde{V}\\}_{h \\in [H]}$ is not induced from the same policy and transition function.  This paper addresses this technical challenge and makes a solid contribution to reward-free exploration.",
            "main_review": "Overall, this paper makes a fundamental contribution to the reward-free exploration field. The proposed solution to the technical challenge of the Bernstein-type analysis under this scenario is novel. Moreover, the paper is well organized. \n\nHowever, several issues/limitations preclude me from fully accepting this paper. \n\n1. The claim of “near-optimal” does not appear suitable in my opinion, provided that (Jin et al., 2020) considered the tabular MDP. Can you explain more why this is a suitable argument? In particular, why can we translate the $S^2A$ term to $d^2$? To my understanding, the reduction between tabular and linear settings is strange/unsafe.  To see this, if we let $d=S^2 A$ (i.e.,  use the one-hot feature as the linear feature), the resultant upper bound $d^2$ becomes $O(S^{4}A^2)$. This upper bound (actually this translation) is not tight, compared with the lower bound in the tabular setting. \n\nI would suggest, if the authors agree, to adjust the language to be clear of this mismatch in problem settings between this paper and (Jin et al., 2020). This would make this paper’s place in the literature more clear.\n\n2. Under the linear mixture MDPs setting, there is no concrete optimization procedure for the $\\epsilon_\\text{opt}$-optimal model-based solver (a key assumption in this paper). This assumption can be well addressed if the state space is finite (see (Jin et al., 2020)). I do not aware of efficient optimization procedures when the state space is infinite. Can you provide some algorithms that satisfy this assumption when the state space is infinite?  I have the same question for several optimization steps in the algorithm design; for example, Line 9 in Algorithm 1 and Line 7 in Algorithm 2. \n\nThis issue would not significantly detract from the contribution of this paper, but it would be better if you could provide some possible solutions or provide some discussion. \n\n3. The sample complexity in Theorem 1 and Theorem 2 does not show the dependence on $B$ (i.e., the upper bound for the weight norm of $\\theta$ in Definition 1). Indeed, we cannot treat $B$ as a absolute constant because $B$ contains the state and action space size information. For instance, considering the one-hot feature $\\phi$, we have that $d=SAS$ and $\\Vert \\phi_{V}(s, a) \\Vert_2 \\leq \\sqrt{S}$ based on the definition $\\phi_{V}(s, a) = \\sum_{s^\\prime \\in \\mathcal{S}} \\phi(s, a, s^\\prime) V(s^\\prime)$. This contradicts with the assumption that $\\Vert \\phi_{V}(s, a) \\Vert_2 \\leq 1$. To resolve this issue, we can normalize the one-hot feature by dividing $1/\\sqrt{S}$. Consequently, to ensure that $P_h(s^\\prime | s, a) = \\theta_h^{\\top} \\phi(s, a, s^\\prime)$ is a valid transition, we could set $B = \\sqrt{S^2 A}$.\n\nI realize that previous literature somehow does not show the dependence on $B$. But it’s better to discuss this point, which could make the connection between tabular and linear settings more clear. Please correct me if my understanding is wrong.\n\n4. Some important steps in proposed algorithms are not well discussed. For instance, how to find a valid transition at Line 20 in Algorithm 1. A similar issue holds for Line 5 in Algorithm 2. I would suggest the authors involve more implementation details. \n\n\nSeveral sentences are confusing/misleading in this submission. I suggest revising these statements to improve clarity.\n\n1. “Compared to our results, the standard plug-in approach (Agarwal et al., 2020; Cui & Yang, 2020) requires reward signals to guide exploration”\n \nThere is no exploration issue in Agarwal et al., 2020. \n\n2. “Based on the Berstein inequality for vector-valued martingales”\n\nThere is a typo: Berstein -> Bernstein\n\n3. “Compared with the reward-free setting studied in the previous literature ...., the main difference is that we require that the algorithm maintains an model estimation instead of all the history samples after the exploration phase”\n\nThere is a typo: an model estimation -> a model estimation.\n\n4. “Then with probability at least 1 − $\\delta$, for any given reward”\n\nIn Theorem 1 and Theorem 2, “Then with probability 1-$\\delta$” should be posted before the first claim or the authors should clarify the randomness source. \n\n5. The paragraph below Lemma 3 seems incomplete since no conclusion is given. As a result, readers must combine Lemma 3 and Lemma 1 to infer the result that the authors want to claim. \n\n6. $R_K$ is not properly defined in (4).  Maybe you mean the concatenation of all one-step “exploration rewards” in iteration $K$? \n\n\nI would like to raise my score if the above concerns are addressed in the authors’ response.\n\n---\n\nDuring the review, I noticed that there is a related paper in the Arxiv. \n\nZhang, Weitong, Dongruo Zhou, and Quanquan Gu. \"Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation.\" arXiv preprint arXiv:2110.06394 (2021).  https://arxiv.org/abs/2110.06394\n\nAccording to the review guide, authors are excused for such a paper (i.e., the Arxiv paper would not affect my decision). But, I encourage authors to incorporate the following points to further improve the quality of this paper:\n1. Compare Algorithms 1 and 2 in this work with algorithms in (Zhang et al., 2021). \n2. Point out which steps/components/analyses in this paper lead to better results.\n3. Discuss involved optimization steps as Zhang et al. do. \n\n\n===========================\n\nAfter rebuttal, the authors have addressed my concern regarding the lower bound and computational details. Thus, I raise my score from 5 to 6.\n\n\n\n\n\n",
            "summary_of_the_review": "This paper improves the statistical sample complexity (in terms of $H$) for reward-free exploration with linear function approximation. Thus, this paper deserves to be accepted and I would raise my score if the mentioned concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the reward-free exploration problem with function approximation under linear mixture MDP assumption. It proposes a pair of model-based exploration algorithms, one with a lightweight methodology and sample complexity rate $\\tilde{O}(H^4 d^2 \\epsilon^{-2})$ and the other as a refined and more convoluted version with rate $\\tilde{O}(H^3 d^2 \\epsilon^{-2})$, which is matching the reported lower bound up to logarithmic factors. Crucially, the proposed approach can work with any planning solver to provide an $(\\epsilon + \\epsilon_{opt})$-optimal policy for any reward function.",
            "main_review": "After Response Update\n\nIn their replies, the authors have clarified my concerns regarding the lower bound and the computational tractability. They are incorporating useful changes in the paper, including a comparison with a relevant concurrent work. I am now fully convinced this is a valuable paper, and I am increasing my score accordingly.\n\n\n-------\n\nI provide below some detailed comments and doubts that the authors might address in their response.\n\nCOMPUTATIONAL TRACTABILITY\n\nI think the paper is somewhat missing an assessment of the tractability of the propose methods, and especially the computational complexity of the optimization problems involved. Can the authors clarify this point?\n\nLOWER BOUND\n\nThe paper provides a lower bound $\\Omega (H^3 d^2 \\epsilon^{-2})$ by reducing a previous result from tabular MDPs, i.e., $\\Omega (H^2 S^2 A \\epsilon^{-2})$ (Jin et al., 2020a). Can the authors clarify how this reduction is carried out? I would say that tabular MDPs can be obtained from a linear mixture MDPs with $d = S^2 A$. I see why there is an additional $H$ factor w.r.t. time-homogenous settings, but where is the additional $d$ factor coming from?\n\nCOMPARING SUB-OPTIMALITY\n\nThe sub-optimality guarantee of Theorems 1, 2 includes an additional $\\epsilon_{opt}$ term with respect to previous works, which instead provide $\\epsilon$-optimality guarantees. I am wondering if this should be intended as an unavoidable cost of the generality of the proposed method, which works with any planning solver, or there is a way to make a fair comparison with the complexity rates of previous works.\n\nRELATED WORK\n\nThe work (W. Zhang et al., Reward-free model-based reinforcement learning with linear function approximation, 2021) seems to be very much related to the setting of this paper. Even if this is very recent, and thus concurrent to this work, can the authors provide a brief discussion over similarities and differences of their approaches?  \n\nADDITIONAL REFERENCES\n\nReward-free RL is a relatively novel field, which does not count a huge number of works yet, thus I would suggest the authors to try to reference all of them, even if they are not crucially relevant or comparable to the proposed setting. E.g., they could consider adding:\n- Z. Zhang et al., Near optimal reward-free reinforcement learning, 2021;\n- W. Zhang et al., Reward-free model-based reinforcement learning with linear function approximation, 2021;\n- Qiu et al., On reward-free RL with kernel and neural function approximations, 2021;\n\nas well as works in the close task-agnostic setting:\n- X. Zhang and Singla, Task-agnostic exploration in reinforcement learning, 2020;\n- Wu et al., Gap-dependent unsupervised exploration for reinforcement learning, 2021;\n\nand multi-agent settings:\n- Bai and Jin, Provable self-play algorithms for competitive reinforcement learning, 2021;\n- Liu et al., A sharp analysis of model-based reinforcement learning with self-play, 2021.",
            "summary_of_the_review": "This works looks like a solid contribution to the reward-free RL literature. To the best of my knowledge, it provides the best known sample complexity rate for reward-free exploration under linear mixture assumptions (especially, it improves over W. Zhang et al. 2021 by a factor of H), and the best rate for a reward-free method that can work with any planning solver (e.g., Jin et al. 2020a, W. Zhang et al. 2021). \n\nSince I believe these improvements are valuable, I am providing a positive evaluation for this work. A careful inspection of the novelty of the analysis (especially w.r.t. Zhou et al. 2020a), as well as additional reassurances from the authors on some questions over the computational tractability of the proposed algorithms and the sample complexity lower bound, could make me consider raising my score towards an even stronger accept.\n\nI believe that extending the proposed analysis to improve known sample complexity rates under the looser linear MDP assumption, as the authors suggest in Section 4.2, would potentially increase the value of this work from solid to great.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}