{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down-stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings. \n\n**Strengths**\nA novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed\nDetailed experimental evaluation, and baseline comparisons, show promising results\n\n**Weaknesses**\nThere were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase)\nThe related work was missing relevant work, and the proposed framework was not connected well to existing work (fixed during rebuttal)\n\n**Rebuttal**\nThe authors significantly updated the manuscript based on the feedback of the reviewers, and improved both clarity of the manuscript (method+experiments) as well as the exposition of the proposed framework with respect to related work.\n\n**Summary**\nAfter the rebuttal, all reviewers agree that this is a good paper that should be accepted. Thus my recommendation is accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a method to learn a three-leveled hierarchy of skills offline, from a dataset of demonstrations, that can then be applied to accelerate reinforcement learning. The three-level architecture is novel. It encodes a discrete selection, a continuous contextual variable (dependent on the discrete selection) and a low level policy dependant of the continuous contextual variable. The results show some improvements over baselines, especially in a sparse reward context where the presented method capitalizes from the offline learned strategies for exploration.",
            "main_review": "Strengths:\n- The paper is well-written and easy to follow. It is nice to read (except for some parts of the method that are rushed)\n- The method is novel, extending prior work\n- The results show some improvements\n\nWeaknesses:\n- Structurally, I like papers that push the related work section to the back because they first present some relevant common theoretical components to understand both the presented method and the related work. This is not the case here. I would recommend placing the related work after the introduction to fully understand and compare the method section to the previous methods.\n- The experimental evaluation is scarce. The method, although explained as very general, is only applied to one domain. Some of the results are not completely clear. The paper would gain on clarity and support for conclusions if the method would be applied to other RL domains, even simple ones. Trained policies could be used to generate expert demonstrations. In its current form, the experiments fall short.\n- The comparison to other methods, especially to NPMP, is not completely clear. The results are somewhat mixed. This is probably a consequence of the limited experimental evaluation with only one domain. Right now, apart from the sparse reward setup, it is not very clear the pros and cons of the presented method compared to previous ones.\n- Could you provide an intuition of what is the different information represented by the categorical and continuous high-level latent codes? Both represent the context. What is exactly the benefit of the two levels? Temporal consistency and commitment? Semantic information encoding? Additional experiments in other domains and with other mixtures of information at different levels would help\n- What happens if there are other observations passed to the different levels? How sensitive are the results to that?\n- What happens if during training one or more of the categorical values are not allowed? Can the system recover (find the necessary skills via exploration)? \n- I’d recommend including a small figure with the objects used in the experimental evaluation to avoid having to go back and forth to know what are “set1”, “set2”...\n- The method section is a bit rushed. I’d dedicate more time to go step by step over the derivation (not completely, that is in the appendix), explaining more of why things are done instead of just describing the mathematical equations.\n",
            "summary_of_the_review": "In summary, the paper presents a novel approach with three hierarchical levels for skill learning. The approach is novel, the results are good, the text is well-written, and the problem is relevant. The experimental evaluation is rather limited and several questions arise with respect to the benefits over previous approaches, the robustness and generality.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a latent variable controller model that allows for reusable skill learning in behaviour cloning and reinforcement learning settings. The architecture comprises three stages or levels. At the highest level, input state information (eg. proprioception, visual input, object state information) is passed through an MLP to produce a discrete latent state skill selection variable, incorporating a discrete latent transition model. This skill selection prior is then used in a mid level network operating on the same input information, to produce a continuous latent state, conditioned on this discrete latent variable. Finally, this latent state, together with the input state is used by an actor network to produce actions.\n\nThe model is trained used a heuristic process depending on the deployment setting, with different elements frozen at different times, or through the use of a mixture agent that can be initialised from scratch or using previous skills (a similar approach is explored by Florensa et al., ICLR 2017). A KL regularisation on the discrete latent variable skill selector is used (also in prior work - Burke et al., CoRL 2019). Results show that this approach is effective on a range of tasks, and that skill re-use (as expected) out performs learning from scratch. Probably the most interesting result is the comparison with a hierarchical behaviour cloning network and the experiments around skills transfer. The idea seems sound, although the presentation is in need of some improvement, and at times the paper suffers from lack of clarity (in problem formulation and presentation of results). Moreover, some important work in the admittedly vast array of work on hierarchical models for RL and behaviour cloning is missed.\n\n",
            "main_review": "Strengths:\n\nHierarchical policies are a great idea, and the proposed model seems like a sensible approach to integrate both discrete and continuous latent variables into a policy, in contrast to many existing approaches which use discrete latent indicator variables to trigger fixed policies.\n\nResults show the proposed architecture seems to work, and I was particularly interested in the comparison with a hierarchical behaviour cloning model.\n\nHowever, the most interesting part of this work lies around the RL phase of the work, where different levels of mixing are explored (completely freezing skills, vs allowing greater skill adaptation. This area is of particular interest as we move beyond \"look we discovered some skills and re-used them\" to more realistic online learning settings.\n\nWeaknesses:\n\nThe paper is in some need of smoothing, and I found this a difficult read, despite being familiar with the field. In particular, I believe the paper would benefit from a clearer problem formulation (ie. is the problem setting a two stage learning from demonstration one followed by an RL phase that makes use of the initial learned skills, or is it a tabular rasa setting with skills gradually incorporated and re-used) Initial sections suggest the former, but some experimental settings and baselines seem to indicate both. I understand that the architecture is very general, and the aim was to show it can be used in both settings with tweaks, but I think the paper would benefit greatly from making the different settings more clear in an expanded problem formulation. \n\nI would also greatly appreciate a more structured experiments setting, more clearly delineating the tasks and with a much more focused aim. As an example, the baselines are all designed to solve slightly different problems, and the broad set of experiments are not necessarily presented to make a cohesive argument, rather they seem to aim to answer many separate questions. A potential option would be to restructure this section around the question of how much prior do we need and how quickly we can allow skill adaptation, which is a very interesting research question.\n\nQuestions/ Comments:\n\nThere is a wealth of literature in hierachical and resuable skills discovery for RL/ continuous control that is missing from this work, in particular around switching nonlinear dynamical systems. See below for a small selection of work deserving mention. \n\nThis paper is particularly important, as it discusses a range of architectures to embed discrete latent variables for skill discovery and later re-use: Carlos Florensa, Yan Duan, Pieter Abbeel, \"Stochastic Neural Networks for Hierarchical Reinforcement Learning\" ICLR 2017 \n\nOther work on re-usable skill discovery using discrete latent variable models:\nScott Niekum, Andrew Barto, \"Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery\", Neurips 2011\n\nP. Ranchod, B. Rosman and G. Konidaris, \"Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning,\" 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015, pp. 471-477, doi: 10.1109/IROS.2015.7353414.\n\nKipf, Thomas, et al. \"Compile: Compositional imitation learning and execution.\" International Conference on Machine Learning. PMLR, 2019.\n\nHany Abdulsamad, Jan Peters, \"Hierarchical Decomposition of Nonlinear Dynamics and Control for System Identification and Policy Distillation\", Proceedings of the 2nd Conference on Learning for Dynamics and Control, PMLR 120:904-914, 2020.\n\nD. Tanneberg, K. Ploeger, E. Rueckert and J. Peters, \"SKID RAW: Skill Discovery From Raw Trajectories,\" in IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 4696-4703, July 2021, doi: 10.1109/LRA.2021.3068891.\n\nCategorical regularisation in hierarchical models has been proposed previously in:\nMichael Burke, Yordan Hristov, Subramanian Ramamoorthy, \"Hybrid system identification using switching density networks\", Proceedings of the Conference on Robot Learning, PMLR 100:172-181, 2020.\n\nSee also:\nZhe Dong, Bryan Seybold, Kevin Murphy, Hung Bui, \"Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems\", Proceedings of the 37th International Conference on Machine Learning, PMLR 119:2638-2647, 2020.\n\nMinor queries/ comments:\nI assume that the number of skills needs to be pre-specified and remains fixed?\n\nSection 2.3: \"Following most previous work, we ...\" - citation needed.\n ",
            "summary_of_the_review": "I think this work has some very interesting experiments and results, particularly around the question of how we should trade-off leveraging existing skills against learning new ones, but these are currently not highlighted very prominently, due to a lack of clarity in problem formulation and experimental results.\n\nThere is a wealth of research around hierarchical mixture latent variable policies and skill discovery which is missing from this work, and reduces the strength of the architecture as a contribution. Refocusing on the fact that the architecture makes it easier to develop methods and strategies for transferring to new settings would help to distinguish this paper from this related work.\n\n====== Post rebuttal comments ======\n\nThank you for your response and for the hard work in addressing the feedback provided. I feel the paper is much stronger for this, so have updated my score.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a three-level hierarchy for learning motor skills. At the bottom, there is a low-level controller for action generation and at the top a high-level controller that generates sequences of skills, with continuous control signals generated by skills at the mid-level. When transferring motor skills from one task to another using reinforcement learning, it is possible to either retrain only the high-level controller or the mid-level one as well. In experiments, it is shown that for transfers to more complex manipulation tasks with sparse rewards, you are more likely to get convergence in training with positive rewards, if only retraining the high-level controller. Thus you are given more flexibility and are possibly able to learn more complex tasks, with the proposed hierarchy compared to earlier methods based on only two levels. ",
            "main_review": "The most interesting aspect of the proposed system is the fact that transfer by retraining only at the discrete level can be so advantageous. This is true at least as long as you stick to manipulation for which skills like moving the end-effector and opening/closing the gripper are quite generally applicable. However, since different levels may work at different dimensions of the state space, it might be possible to retrain the mid-level controller from one modality to another, something that is illustrated in the experiments. \n\nThe experimental section is quite extensive and includes comparisons to other alternative methods, as well as ablation studies. Two variants are tested for transfer learning; one that only retrains the high-level controller (HeLMS-cat) and one that also retrains the mid-level (HeLMS-mix), while the low-level controller is always kept fixed. The first option seems to be more beneficial in complex cases with sparse rewards. In fact, it is a bit surprising that HeLMS-mix rarely seems to surpass HeLMS-cat. It is easy to assume that this is due to the similarity between manipulation tasks when it comes to what skill sets are required.\n\nCompared to the other alternative methods tested, HeLMS seems to cover a larger state space during exploration, which in turn leads to higher rewards, when rewards are sparse. With the three-level hierarchy, exploration can be done at a higher level of abstraction while exploiting skills already learned.\n\nThere are some questions worth asking regarding the experimental results. It is shown that for more complicated tasks, HeLMS-mix might come to the point where the average reward starts to decrease. It would seem more reasonable if the reward had just flattened out at a lower level, which happens in most other cases. An explanation given in the paper is that in such cases spurious reward correlations might cause the skills to drift. Does this instability come as a result of including an additional hierarchical level, since none of the other tested methods seems to show anything similar? Another question is why NPMP performs better on object set 2 specifically? Is there any reasonable explanation or is it just a coincidence? \n\nWhen referring to HeLMS in the experiments, it would be good if the paper mentioned either HeLMS-cat or HeLMS-mix, unless both variants are intended. Otherwise, the reader has to go into the text to see which variant was actually used. For example, which variant is used in Table I? By the way, in this table, what is going on with the space coverage in the dimension of grasping? Since it is binary it looks as if none of the other methods really tries to explore grasping, which is odd given the nature of the task given. \n\nFigures 7 c)-d) use the wrong notation for the regularization weight, at least compared to what is written in the text. Also, instead of HeLMS-cat the label says just HeLM.\n\nOn page three, “ELBO <= p(a|x)” is written in the wrong order. It is correct in appendix B. Other than that, the paper is very easy to read and understand, with clarity in both language and notations.\n",
            "summary_of_the_review": "The proposed three-level hierarchy for learning motor skills might not be groundbreaking in itself, but it brings some interesting advantages and flexibility that is illustrated in the paper. It is interesting to see how far you can go in complicated cases by reusing an already learned skill set. At the same time, you might be able to achieve higher rewards in the end if skills are also refined. For that reason, this paper is definitely worth reading. It is possible to imagine training schemes in which you gradually reduce the regularization of the mid-level skills to make them more specific at the end of training, and thus possibly get the best of both worlds.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a hierarchical model for skills extraction from an online dataset. In particular, the model can be seen as a 3-level hierarchy. The highest level of the hierarchy selects a discrete variable that corresponds to the index of a skill. The mid-level selects the continuous parameter that defines the skill. The lower level executes the skill.\n\nThe considered problem is crucial in reinforcement learning. In particular, the definition of skills to be reused across different tasks is essential to achieve better sample efficiency, and crucial to allow reinforcement learning to be applied directly on challenging real robotic tasks. \n\nWhen dealing with an off-line dataset, one wants potentially to first extract a set of skills (say, low-level policies) and then to learn via RL the selector of such policies. In this way, one both simplifies the problem (increasing sample efficiency) and allows the skills to be reused in different tasks.\n\nThe authors propose a graphical model that consists, per time step, of four variables: $x_t$, $y_t$ $z_t$ and $a_t$. $x_t$ represents the MDP's state, $y_t$ is the discrete selector of the skill, $z_t$ the (continuous) parameters of the skill, and $a_t$ is the output at time $t$ of the selected skill (and can represent, for example, the joint velocities of the robot).\n\nAfter learning via variational inference the probabilistic model, the authors propose to refine the learned behavior via MPO, learning the skill selector $p(y_t | y_{t-1},x_t)$, the skill parameters $p(z_t | y_t, x_t)$, and the skill p(a_t | z_t, y_t).\n\nFurther, the authors propose 1) to use an asymmetric information approach, where each level of the described hierarchy sees a different definition of the state. In particular, the lower layer sees the robotic state (proprioception), the mid-layer sees the position of objects in the scene, and the highest level sees both the information 2) the models use \"gated\"-heads: the categorical variable $y_t$ is fed to the last part of the neural network. \n\nOnce the hierarchy has been learned, one can tune it on the same or different tasks. In particular, the lowest layer is kept fixed, while only the highest, or the highest, and the mid-layer are learned. \n\nThe authors propose to use MPO as a reinforcement learning algorithm. They perform a few ablation studies and experiments to empirically prove the efficacy of their method. ",
            "main_review": "STRENGTH:\n\nS1: The paper considers a very important problem. \n\nI think that skill extrapolation and reuse it is at the core of providing more sample efficient reinforcement learning and to allow its application to real-world tasks.\n\nS2.1: I think that the proposed method is sound.\n\nThe idea of breaking the three-level of hierarchy makes a lot of sense. Many existing approaches in fact propose a similar structure, which, in higher detail, can be described as a mixture of parametric skills. \n\nThe idea of incorporating information asymmetries, as also noted by the authors, was empirically shown to be also effective. \n\nS2.2: Many works I am aware of, have limited applicability, since they do not make use of neural networks, and they do not scale well with high-dimensional input. This work does not suffer from this problem. Furthermore, the extracted skills can be easily used in many classic RL algorithms.\n\nS3. The experimental section consistently supports the statement made, showing the efficacy of the proposed method. I especially enjoyed the ablation study.\n\nWEAKNESSES\n\nW1: The readability of this paper can, in my opinion, be improved. \n\nIn particular:\n\nW1.1: the paragraph \"Training via the Evidence Lower bound\" is a bit messy. The quantities $p_{\\psi_{y,a}}$, $q_{\\phi_{y,z}}$, etc, have not been defined before. I think that these models should have been defined at the beginning of  2.1. \nFurthermore, the left-hand side of Equation 3 could have been defined explicitly, even though that the ELBO is a widely known concept. \n\nW1.2: Equations 4 and 5 present just RL objectives. I think that the authors could have spent some lines to write how MPO optimizes these objectives. Writing down the optimization process will explicitly clarify which parameters will be updated. The current form of Section 2.3. is, in my opinion, too fuzzy. \n\nW1.3: I think that many equations contain some typos. For example,\n\nEquation 1: on the right side we have p(y_0), but why on the right-hand-side $p(y_{1:T}, Z_{1:T})$ does not take in account of $p(y_0)$?\nThe same questions arise also in Equation 2. \n\nThe summations $\\sum_{y_{t-1}}$ in the derivation of Equation 3 should be better defined. \n\nIn Equations 4 and 5, $x_t$ is outside the expectation. This is wrong. Question: shouldn't the KL also be discounted with \\gamma^t? If the KL is summed for each $t$ term from $0$ to $\\infty$, then the summation will be unbounded (and, therefore, impossible to optimize).\n\nW2. \n\nThe proposed method, in my understanding, can be very much regarded as \"skills segmentation\" \"movement primitive segmentation\", [1-4]...\n\nFurthermore, the mixture of skills (or movement primitives) has been also discussed widely [5, 6, 7, 9, 10]. Notice that the probabilistic model of [6, 10] resembles closely your approach (discrete selection of parametric skills).\nThe mixture of movement primitives has also been utilized in reinforcement learning, leading to a very similar probabilistic model (with a discrete variable selecting the skill, and the continuous variable selecting the skill's parameters) [7-10].\n\nIf you dig a bit, you will find many more connections to your current work. \n\nI am also aware of this very recent work, which also uses high-level skills to perform RL tasks [11]. Their motivation and result are, in my opinion, strongly supporting the evidence of your result, although (in my opinion) your work is more complete. \n\nW3: as described above, and also as partially acknowledged in your paper, people have been working on this topic. I like how you have put things together, and I think your work is promising. I think that the contribution is enough, but not particularly strong. \n\nTo summarize, I think that the idea behind your method is good. I find it a pity that it misses a better related-work contextualization, mathematical precision, and exposition clarity. \n\nQUESTIONS:\n\n- Can the authors clarify my doubts about the equations? \n- Can the author draw a link between their work and HMM? \n\n[1] Scott Niekum, Sarah Osentoski, George Konidaris, Andrew G. Barto. Learning and Generalization of Complex Tasks\nfrom Unstructured Demonstrations. IROS 2012.\n[2] Rudolf Lioutikov, Gerhard Neumann, Guilherme Maeda, Jan Peters. Probabilistic Segmentation Applied to an Assembly Task. Humanoids 2015.\n[3] Rudolf Lioutikov, Gerhard Neumann, Guilherme Maeda, Jan Peters., Learning movement primitive libraries\nthrough probabilistic segmentation, IJRR 2017.\n[4] Bjorn Kruger, Anna Vogele, Tobias Willig, Angela Yao, Reinhard Klein, Efficient Unsupervised Temporal\nSegmentation of Motion Data. \n[5]  Affan Pervez,  Dongheui Lee. Learning task-parameterized dynamic movement primitives using a mixture of GMMs.\n[6] Elmar Rueckert, Jan Mundo, Alexandros Paraschos, Jan Peters, and Gerhard Neumann. Extracting Low-Dimensional Control Variables for Movement Primitives. ICRA 2015. \n[7] Katharina Muelling, Jens Kober, Jan Peters. Learning Table Tennis with a Mixture of Motor Primitives. Humanoids 2010\n[8] Katharina Mülling, Jens Kober, Oliver Kroemer and Jan Peters. Learning to select and generalize striking movements in robot table tennis. IJRR\n[9] Adria Colome, and Carme Torras, Dimensionality Reduction in Learning Gaussian Mixture Models of Movement Primitives for Contextualized Action Selection and Adaptation. RAL 2018. \n[10] Samuele Tosatto, Georgia Chalvatzaki, and Jan Peters. Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills. ICRA 2021.\n[11] Dalal Murtaza, Pathak Deepak and Salakhutdinov Ruslan. Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives. NeurIPS 2021.\n",
            "summary_of_the_review": "STRENGTH\n\n1. The paper considers a very important problem (skills extraction and reuse).\n2. The proposed method is sound. Furthermore, the employment of variational inference with deep NN allows the method to be scalable and usable on high-dimensional tasks.\n3. The experiment section is well structured and supports nicely the statements made.\n\nWEAKNESSES\n\n1. I think the paper is not very clear. In particular, some passages are missing, and some equations seem not to be correct.\n2. I think that a large part of related work is missing, especially work prior to 2017.\n\nI would give 8 (Accept, Good Paper), but I really think that this paper should be organized better, and the equations are not clear. I have an issue with every single equation in the paper. May I ask the authors to clarify my doubts?\n\n\n\n================ SUMMARY OF CHANGES ======================\n\nMy main concerns were mainly about the clarity of the paper and the lack of a large body of related work. \nI think that the authors addressed both the issues in their new version of the paper.\n\nThe new paper is much clearer, and the proper discussion of related work helps in defining more rigorously what was the contribution of this paper. Therefore, I increased my score from 6 to 8.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}