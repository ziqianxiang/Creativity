{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper provides an overview of evaluating graph generative models (GGMs). It systematically evaluates one of the more popular metrics, maximum mean discrepancy (MMD). It highlights some challenges and pitfalls for practitioners and suggests some ways to mitigate them. The reviewers found the paper practically relevant and several reviewers upgraded their scores through the discussion process. The authors acknowledged there are still some remaining issues regarding (i) considering other metrics & descriptor functions; ii) evaluating node/edge attributes and iii) addressing molecule generation. I am satisfied that these areas are beyond the scope of the current work and that the clarification improvements in the paper are adequate. It stands well enough on its own to accept in its present form."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes desiderata (expressivity, robustness, and efficiency) for metrics for comparing graph generative models and details the various ways that recent work has used maximum mean discrepancy (MMD) to evaluate graph generative models.  Several limitations of MMD are described, as well as the consequences of the various choices in using MMD (kernels, descriptor functions, and hyper-parameters) and how these choices have impacted experiments in recent work.  The paper concludes with recommendations to help ameliorate several of these limitations and impacts.",
            "main_review": "This paper is generally clear and well written.  \n\nIt could be helpful to add citations in a few places to help the reader who is not already familiar enough with the graph generative models community to recognize what is considered most popular and status quo (see minor comments below for some examples). \n\nOne limitation to the robustness experiments is that there is an assumption that a greatly perturbed graph should have a much different probability than the source graph, however it is not clear why this is necessarily the case (e.g, if the graph generating distribution is uniform).\n\nThe paper also notes and makes a point in the experiments how three popular works all use different kernels, descriptors, hyperparameters, however one of the works (Niu et al., 2020) doesn’t seem to describe the kernel and hyperparameters, instead citing one of the other of the other two works (You et al., 2018) and claiming to use the same evaluation (so presumably the same kernel and hyperparameters).  In addition, all three works appear to use degree histogram, clustering coefficient histogram, as well as the number of orbits with 4 nodes (which was left out from this paper); (Liao et al, 2019) appears to uses the Lapacian spectra in addition to these.  It might be worth noting why the number of orbits was omitted in this study.\n\nMinor comments:\n- “the community has largely gravitated towards a single comparison metric, the maximum mean discrepancy (MMD)” consider adding some citations (and if there are notable exceptions)\n- “Specifically, d(G, G′) should be monotonically increasing the further apart G and G′ are from one another.” Further apart according to what measure? Presumably there are several ways to specify this and, if taken precisely, might beg the question of how to define d.\n- “if a distribution G is subject to a perturbation, a suitable metric should be robust to small perturbations” is it correct to understand this as the metric being well-conditioned?\n- rather than perturbing graphs (since the resultant graphs might still be in the set of graphs or have high probability), why not generate sets of graphs from increasing different distributions directly? \n- “depicts an overview of this approach as it is being done today.” Consider adding citation(s).\n-  \"Equation 1 is often treated as a metric as well” consider adding citations\n- “even though it would be more correct to only use it for hypothesis/two-sample testing” the original (and several subsequent) works seem to describe MMD in the hypothesis/two-sample testing context, but it might still be worth adding a brief note why treating it as a metric is \"less correct\".\n- It seems as though there’s a typo in (2)?  The numerator appears to be 2 * deg(v), when I believe it should be a count of the edges between neighbors of v (instead of v's degree)?\n- “and only require further analysis of their expressivity and robustness” given this work's nature and scope, this would be nice to address in this work\n-  The paper notes how descriptor functions are strictly necessary for MMD, and when they are used, the importance of descriptor function choice.  In the recommendations section, the suggestion appears to be to use any of the descriptors (as opposed to how to select an/the appropriate one).\n-“clustering coefficient (CC), is even negatively correlated with the degree“, the scale in Fig 4 (correlation) appears to be 0 to 1 instead of -1 to 1?",
            "summary_of_the_review": "The paper is generally clear and well written and makes several interesting and worthwhile observations about MMD and its use in recent work.  However, there is an opportunity to better support some of the claims--like what is considered status quo (through citations) and about recent experiments (which paper used which settings and, in the case of (Niu et al., 2020), where those setting can be found--and to clarify the perturbation based desiderata and experiments/analysis.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper criticizes Maximum Mean Discrepancy (MMD) an evaluation metric that has been used lately to evaluate Generative Graph Models (GGMs). The main problems of the MMD are: It does not capture difference upon perturbations; It does not have a scale; It requires the selection of a kernel and parameters that could lead to different results (and a high time complexity according to the kernel). Finally, the paper describes some procedures based on MMD to avoid some of these problems. While the pitfalls are clearly explained, the solutions are vague.",
            "main_review": "The paper has several strengths. \n\n1) Presentation. The paper is well written even for people that are not related to this topic. The paper focuses on the main pitfalls of this measure.\n\n2) Reproducibility: Most of the experiments can be easily replicated. However, there is some lack of explanation in the main paper (9 pages) that could be addressed. \n\n3) Impact of ideas: The criticism of this measure could have a great impact on the current evaluation process, changing the way that GGMs must be evaluated. \n\n4) Technical quality: Most pitfalls are corroborated empirically, while others are not even necessary to check.\n\nEven though the main strengths of the paper, there are some issues that reduce the recommendation score. unfortunately, the solutions are vaguely explained, and not necessarily new. \n\n1) Perturbations are not defined in the paper (9 pages). This makes it difficult to understand some of the experiments and the effects in the final evaluation. Please include part of the appendix in the main part of the paper.\n\n2) The difference in the distributions versus perturbations is not necessarily a problem of the metric, it could be the descriptor function. The paper uses the clustering coefficient to show this problem; however, depending on the graph, the clustering coefficient does not change considerably with some type of perturbations. For example, imagine a graph with a low clustering coefficient (as many real-world networks). If we pick a node with cc=0 and we remove all of its edges, its cc will not change at all; even though it was highly perturbed. Please, change the example, using a measure affected by this issue or define that this problem is given to some descriptor functions.\n\n3) Provide a sense of scale: This idea is vaguely explained, and it does not define a procedure to follow. Please, define a procedure.\n\n4) Utilize meaningful descriptor functions: This does not show a new solution. There are several meaningful descriptor functions that are used in the evaluation of GGM that are not even included in this paper. For example, the geodesic distance. Expand the use over other meaningful descriptor functions. \n\n5) One of the main problems, interpretability, is not even solved. \n\n6) There is no comparison against other measures. For example, the multivariate Kolmogorov-Smirnov, that has been used in a previous evaluation of GGMs (A multivariate Kolmogorov-Smirnov test of goodness of fit and Tied Kronecker Product Graph Models to Capture Variance in Network Populations)",
            "summary_of_the_review": "The paper mainly focuses on the negative problems of the MMD, which are explained and in several cases empirically proved. This makes the paper strong and it should be accepted. However, the solutions are not clearly explained, decreasing the score for this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work, the authors aim to provide a principled way to evaluate and compare graph generative models. The authors initially list desirable criteria an evaluation metric should possess and subsequently discuss the usage of maximum mean discrepancy (MMD) for model comparison. Subsequently they highlight issues with the usage of the MMD metric for model comparison purposes and via empirical study and analysis, list a set of practical recommendations for researchers to follow when evaluating graph based generative models.",
            "main_review": "Strengths :-\n1) The problem the authors focus on in this work is significant in graph based generative modeling community. By highlighting the potential issues with usage of the MMD metric, the authors provide valuable insight and learning which would hopefully advance the state of research. \n2) The notation and description of the paper is unambiguous and clear and the authors via rigorous empirical study highlight different facets of the problem effectively. By not deciding to include their own approach, the authors add to the clarity of the paper for which I would like to commend their work. Additionally most of the experiments are reproducible which is another reason to recommend this work.\n3) The recommendations from the authors is insightful and would be useful to researchers working in the field of graph based generative models. \n\nWeaknesses :-\n1) The current scope and focus of the paper is a bit limited. There are graph based generative models which focus on edge prediction and node classification tasks for example which focus on graph based convolutions using node and edge based features and also use common metrics like logistic loss etc. There are also other forms of graph based generative models which try to learn node and edge based embeddings via deep neural networks etc. \n2) The authors could have tried to include other metrics in common usage apart from the MMD metric which would have made this work even more beneficial. Same goes for other prevalent descriptor functions.\n3) Rather than working with correlation which focusses on linear relationships, the authors could have worked with non-linear variants such as mutual information which would potentially have been more apt.",
            "summary_of_the_review": "The authors focus on an important problem in the graph based generative modeling domain and provide valuable insight and learnings via carefully designed empirical results and clear and unambiguous explanations and intuitions. The recommendations from the authors is insightful and would be useful to researchers working in the field of graph based generative models. However the current scope and focus of the paper is a bit limited. The authors could have expanded this work via considering i) other metrics apart from MMD, ii) other descriptor functions as well as iii) working with non-linear variants for correlation, i.e., mutual information. I would like to congratulate the authors for this work and ideally would like to see this paper accepted. \n\n----UPDATE----\n\nAfter revisiting the updates made by the authors, I am updating my score to Accept. I would like to congratulate the authors for their excellent work which would benefit researchers working in the field of graph based generative models.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper identifies limitations and common pitfalls that practitioners encounter when using the MMD metric to evaluate graph generative models. Moreover, it provides guidance on how to ensure that the metric is properly calibrated to avoid nonsensical results.",
            "main_review": "Strengths:\n- this is a much-awaited paper that clarifies many aspects related to MMD for evaluating graph generative models. It was really interesting to see how much results may vary whenever the parameters of MMD are not calibrated properly.\n- the recommendations on how to tune the MMD parameters to obtain meaningful results are really useful, and if properly incorporated into the graph generative modeling standard practices might increase the quality of the research and help establish more powerful benchmarks to evaluate graph generative models.\n\nWeaknesses:\n- while the content is extremely useful, I find the scope of the paper a bit limited. For example, node and edge features are not taken into consideration in this paper. In its current form, the paper only discusses how to use MMD to compare the structure of graphs, while in practical scenarios (e.g. for molecular generation), nodes and edge features play an equally important role. \n\nQuestions:\n- commonly, MMD is also computed using the 4-orbit count descriptor. Why wasn't this taken into account in your analysis?\n- I think a runtime comparison of the various kernels (at least between linear, EMD and RBF) would be something useful to the practitioner. Otherwise, claims such as \"the EMD kernel is slow\" remain a bit vacuous (how much slow is \"slow\"? At which point using EMD becomes to be intractable?)",
            "summary_of_the_review": "I am inclined towards accepting the paper for its much-needed practical guidance on the use of MMD in the context of graph generation, which could have very positive implications in this research field. However, I acknowledge (and the authors should, too) that what is discussed in this paper is only one of the many aspects to consider when evaluating graph generators.\n\n----EDIT----\nI find the response of the authors to my inquiries satisfactory. For this reason, I am raising my score from 6 to 8 and vote for its acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None identified",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}