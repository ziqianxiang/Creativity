{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations.  The proposed approach is highly motivating in biological and medical applications.  Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments.  The authors adequately addressed major concerns raised by reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work aims to use self-supervision to identify the most useful features for downstream tasks particularly in the context of correlated features. This is an important aspect that is lacking from many feature selection approaches commonly used within the highly correlated datasets of healthcare. This approach - using both feature vector reconstruction and gate vector estimation appears to provide better results for both true structure estimation in a  dataset with known truth as well as downstream predictive performance in real world data sets. ",
            "main_review": "Comments: \nI think the dataset descriptions could have been earlier, itâ€™s helpful to read them prior to some of the methods descriptions to better understand the approach. \n\nGiven the focus on genetic data in the introduction it may be helpful to cite RELIEF-based approaches. \n\nThe introduction mentions WGS and the high dimensionality inherent to it but the datasets used are several orders of magnitude less features. Either the introduction should be updated to better reflect the datasets used (e.g., not full sequence data) or the potential application to genotypes (~2M features) or sequencing should be explored. Importantly, it seems this would require a streaming approach or other algorithmic changes if the data did not fit into GPU memory. In any event, It would be helpful to see the authors thoughts of about whether these methods could be combined using dimensionality reduction as pre- or post-processing in addition to, instead of as a replacement for their approach (as in Table 1). \n\nThe name - SEFS (no SS) isn't consistent from table 1. Why isn't SEFS (no SS) shown in table 1? My understanding is that it is neither the autoencoder or independent Bernoulli approach, but I was confused about why it couldn't be included in the initial experiment. \n\nI was confused about why Table 1 does not include any of the benchmark approaches from Table S4? At first I thought the description of performance comparison in terms of both TPR and downstream predictive performance was incorrect because I did not see the supplemental table. The discussion about traditional methods performing poorly on feature 1 seems important enough to be included in the core manuscript as it provides intuition about the advantages of the author's approach. \n\nMinor comments:\nFigure 4's legend is a little confusing - lower the better, but the rank is lowest at the top of the figure. I believe this could be easily clarified. \n\nLanguage should be clarified / copy-edited prior to publication \nJust as a single example - Changing via to by simplifies this sentence in abstract: \nFirst, we pre-train the network using unlabeled samples within a self-supervised learning framework via solving pretext tasks that require the network to learn informative representations from partial feature sets. \n\nSupplement - SEFS outperforms all benchmarks - benchmarks is misspelled\n",
            "summary_of_the_review": "I found this work to be focused on an important problem as well as technically interesting and correct. I believe the authors could substantially improve the presentation of the work (e.g., condensing intro sections, reworking the order and going deeper into results - both quantitative presentation as well as the interpretation in the discussion). I believe the authors could do this during the camera-ready phase and therefore recommend this work be accepted with this feedback prior to publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a novel feature selection method for tabular data prediction. It tackles two challenges by respective novel designs, i.e. labeled data scarcity issue by an unsupervised self-supervision phase, and feature correlation issue by multivariate Bernoulli gate vector with learnable correlation matrix. The experiments on one synthetic and two medicine/biology datasets demonstrate effectiveness.",
            "main_review": "The paper is well motivated and the method is clearly presented with all its technique points delivered. Several concerns during my reading are well addressed: the Bernoulli distribution reparameterization, the lasso regularizer term's relaxation, the ablation study on SS phase, the performance gain over other baselines etc.\n\n\nIn section 3, the formulation around Eq1, Eq2 may have contradicting ideas on how to set values for features not selected. It reads around Eq 1, features not selected will take an out-of-distribution value \"*\", while in Eq 2, features not selected will take their mean values. Do authors have any comments or corrections here? Readers may be confused by the two formulations. \n\n\nAnother similar tabular data prediction task is CTR prediction for ads recommendation, this method may have potential to be applied there.",
            "summary_of_the_review": "This paper is impressive. Its technique is novel and clear. Experiment is convincing and solid. A clear accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work presents some additional tweaks and tricks in using neural networks for feature selection. The main contributions include an inclusion of a parameterized masking function that selects the feature and a self-supervised component for pretraining. The overall proposal makes sense and there are strong experimental results that justifies the merits. ",
            "main_review": "1. Equation (6) was borrowed from previous work, which makes me wonder which part of Section 4.1 is the novel contribution from this work. In addition, the last paragraph of Section 4.1 where two advantages are listed is quite hard to follow.\n\n2. Perhaps I missed this, how does one compute the correlation structure R based on the the inputs? It is stated as `pre-specified`, is it dataset or problem dependent?\n\n3. Regarding the experiments, most of the baselines are rather weak models. Could you elaborate among previous work which are those that also used neural networks? \n\n4. Overall what will the model behave if none of the features are dispensable? How does it scale with the dimensionality of the data?",
            "summary_of_the_review": "The experiments are rather comprehensive and illustrative. I look forward to get some feedback from the authors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}