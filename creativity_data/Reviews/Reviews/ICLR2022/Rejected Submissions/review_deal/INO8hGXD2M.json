{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies the general problem of out-of-distribution (OOD) detection, where the goal is to detect outliers (i.e., points not in the distribution of training data) in the sample. The paper introduces a methodology for measuring robustness by using adversarial search/distributions. Experimental evaluation indicates that traditional metrics fail to fully capture OOD detection. The reviewers' evaluations of this work were mixed. Overall, there was consensus about the importance of the problem. Moreover, some of the reviewers argued that the submission contains some interesting new ideas. On the other hand, concerns were raised regarding lacking comparison to prior work, potential overselling of the contributions, and several aspects of the experimental evaluation. At the end, there was not sufficient support for acceptance. In its current form, the work appears to be slightly below the acceptance threshold."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to evaluate out-of-distribution (OOD) detection methods on adversarial distribution to detect unexplored space of outliers.",
            "main_review": "- The proposed benchmark is reasonable in a different way, so I recommend to redirect the goal and rewrite the paper.\n\n- To my knowledge, OOD detection is not supposed to be robust against adversarial attacks, though some OOD detection works showed that their method is robust against adversarial attacks, e.g., [Lee et al., 2018]. Rather, the OOD detection task aims at detecting natural but different distributions, while adversarial attacks are mostly artificial. The proposed benchmark evaluates if OOD detection methods are robust against adversarial attacks, so the purpose is different from OOD detection works.\n\n- The Mahalanobis distance method (MD) [Lee et al., 2018] leveraged adversarial attacks for validation in some of their experiments, which is worth to note as a prior work with a similar idea. I think MD should exhibit a good performance, if the authors followed this validation strategy.\n\n## Post rebuttal\n\nI do not change my rating, as I still think the purpose of the proposed benchmark should be described in a different way.\n\nI think the answer in **1. OOD Detection and Adversarial Robustness** cannot justify if evaluating adversarial robustness on OOD samples can be generalized to unexplored space of (natural) outliers. By adding adversarial perturbations to the evaluation framework, the objective becomes to evaluate the adversarial robustness, not the OOD detection performance. To my understanding, related works are about mathematically certifiable or provable adversarial robustness on OOD detection problem, which is not really aligned with this work. As Reviewer XMWK concerned, the authors could clarify the relationship between this work and related works. It is true that there are \"adversarially perturbed outliers which can be confirmed as OOD from human eyes,\" but I think detecting such artificially generated samples is a different problem from the general OOD detection problem.\n\nHere I summarize the logical flow in the abstract below, where the transition from 2 to 3 is not convincing to me.\n1. Current evaluation protocols cover only a small fraction of all possible outliers.\n2. We want to test a detector over a larger, unexplored space of outliers.\n3. To this end, we evaluate with samples from its adversarial distribution.",
            "summary_of_the_review": "The proposed benchmark would be worth in a different way: not for a universal OOD detection, but adversarially robust OOD detection.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Just to note, as the authors stated, the outlier exposure method (OE) is trained on the 80M tinyimages dataset, which is no longer available due to an ethical issue. However, this work just borrowed the pre-trained model and has never accessed the 80M tinyimages dataset, so I think it is okay. They are just reporting the performance of the pre-trained model.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a new method for measuring an image classifier's robustness against out of distribution data by using adversarial search/distributions. The paper conducts experiments on various recent SOTA OOD models, and finds traditional metrics don't capture the whole picture when it comes to OOD detection.",
            "main_review": "Strengths:\n - well written\n - Good job at describing the general problem and setup\n - Reasonable methodology for generating the adversarial distribution\n - Strong evaluation\n\nWeaknesses:\n - I find the use of a normal auto-encoder to be a little strange, as sampling from that latent space has issues [1]. I would much prefer a VAE or GAN that enforces a prior distribution. The authors could comment on these draw-backs in the main text, but I find the example generated images to be convincing of sufficient \"OODness\"\n - The connection between adversarial search and distribution is unclear. The sentence \"sampling from an adversarial\ndistribution becomes equivalent to the adversarial search...\" is somewhat buried in the text. The full discussion of adversarial search/optimization is, in my view, tangential to the main arguments of the paper. The paper's clarity would be greatly enhanced if the term adversarial was used less frequently and the generative aspect of the work was focused on.\n\nOther:\n - Do the authors have a link to source code / website for the proposed leaderboards? If so, they should be linked. If not, leaderboard should be move to discussion section as future work.\n - section 6 & 7 could be combined \n - table 1 bolding is a little non-intuitive (typically reserved for best performing methods), perhaps italicized would convey the same message?\n - How many instances were generated for the adversarial distribution? 1000? This could be clarified in the main text.\n\n\n[1] Bengio et al. \"Representation learning: A review and new perspectives\" 2013\n\nPost rebuttal: I am generally okay with the current paper as is. I would prefer a better generative model and a bit a of a rewrite for less focus on the adversarials; for these reasons, I will be keeping my score.",
            "summary_of_the_review": "This paper provides an interesting angle for investigating/evaluating OOD detectors. The paper is well-justified, with a mostly reasonable setup, and would be of interest for broader OOD community as a new metric.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel evaluation framework for out-of-distribution (OOD) detection under worst-case scenarios. While existing benchmarks use real samples from datasets outside the training distribution, the authors propose instead to learn an adversarial outlier distribution against OOD detectors using an autoencoder model, with an auxiliary binary classifier that filters out inlier samples. Empirical experiments on CIFAR-10 (inlier) and SVHN/CelebA (outlier) datasets show that standard OOD benchmarks tend to produce overoptimistic results, and that prior methods with similar scores on standard benchmarks have diverging performance outside the predefined OOD test sets. ",
            "main_review": "## Pros\n- This work studies an important problem with OOD detection, namely that the arbitrary choices of test datasets may lead to biased/overoptimistic estimates of detection quality. The paper is well written and easy to follow. \n- The idea of extending search space of outliers by using a learned autoencoder makes intuitive sense, and the proposed solution appears to address most issues with existing evaluation protocol.\n- Experiments are conducted with a large number of OOD detection methods, with very interesting results revealing the vulnerability of state-of-the-art detectors whose performance saturate on standard benchmarks.\n\n## Cons / Questions\n- As the authors have acknowledged in the paper, the proposed evaluation method does not fully eliminate the need of selecting OOD test sets, which significantly affects evaluation outcome (ranking of OOD detectors). That is, while the autoencoder learns to *interpolate* between outlier samples, it does not *extrapolate* beyond the chosen dataset, and therefore cannot provide full coverage of outlier distribution. Therefore, the results would be stronger if 1) repeated with more pairs of inlier-outlier datasets, and 2) trained on the concatenation of multiple outlier datasets.\n- If the major concern is the coverage of OOD data used in existing evaluation protocols, can using a larger-scale test set (e.g. ImageNet) address the problem?\n- As seen in qualitative examples of Figure 3, 4, 7-12, the majority of samples drawn from the adversarial distribution diverge from the space of natural images (especially when learned on SVHN). This makes the proposed approach closer to a benchmark of adversarial robustness, where images can be manipulated adversarially to fool a target classifier, especially when the latent dimension $D_z$ of the autoencoder is large. Does this make the benchmark unnecessarily difficult, when OOD detectors are expected to differentiate between *real* images? Can the authors further comment on the difference between the proposed task and adversarial robustness? Should one expect training OOD detectors with adversarial defense methods to improve performance on the proposed benchmark?\n\n## Minor comments\n- It would help to highlight best performing OOD detectors under each criterion in table 1.",
            "summary_of_the_review": "This paper presents a very interesting approach towards an objective and unbiased evaluation of OOD detection. While the proposed approach does not fully address the problems of existing evaluation protocols it identifies, I find the solution to be novel and intuitive, and the empirical results to be worthy for the community. Overall I am leaning towards accepting the paper, and would appreciate if the authors can address the questions enumerated in the Cons section above during rebuttal period.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of constructing adversarial distributions to fool \"naturally trained\" OOD detectors. A optimization-based and a sampling-based method are proposed and evaluated respectively.",
            "main_review": "This paper contains some interesting ideas, and in general I like the fact the derivations and experiments are clearly presented.\n\nHowever, a major problem I feel is that it has an unclear relationship with previous work. This is far from the first work that consider constructing a distribution that is \"adversarial\" to the OOD. For example, Hein's group has produced a line of work in this direction, and also recently there have been many other works (e.g., https://arxiv.org/abs/2006.15207). For a big part of the paper, these related works are not discussed at all, and in fact Bitterwolf's work is discussed only until page 7. This does not seem to be the right way of handling related work.\n\nI guess this work may be arguing that, even though the previous works have considered \"adversarially constructed distributions against OOD\", this work is more explicit and systematic about this direction. To that end I remain a bit skeptical. It is not the case that, just by considering larger and larger space, it \"automatically\" makes an attacking-OOD paper more interesting. In fact, a small space of perturbation that leads to failure of OOD is even more interesting, than a very large space (clearly, NN will likely be poorly performing on far-away inputs, given the state-of-the-art technology). \n\nI am also worried about the very strong claims made in the paper, such as \"In this paper, we have addressed the limitations of the current evaluation protocol for OOD detection and proposed a novel framework, adversarial distributions, that can be used to investigate failure\nmodes of OOD detectors.\" Do we really have addressed the problem (so we cannot imagine any further work to improve the evaluation, and so this work has exhausted the entire space?)\n\nNit on the naming. Adversarial search induces an adversarial distribution from the original \"natural distribution\", so I don't really think it is a good name to call \"adversarial search\" vs. \"adversarial distribution\" in the definitions. I don't really have a good suggestion, but how about \"adversarial sampling\"?",
            "summary_of_the_review": "I am worried about the handling of previous work in this paper, and it seems to me to be overselling its contributions. For these reasons, I would vote for rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}