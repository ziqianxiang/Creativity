{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper suggests that robust overfitting could be viewed as the early-part of a double descent phenomenon for adversarial training. The authors identify implicit label noise, i.e. the label distribution mismatch between the true example and the generated adversarial example as a possible explanation for this phenomenon in adversarial training. This claim is empirically supported by experiments using static adversarial examples. The authors propose a method using temperature scaling and interpolation to mitigate the effects caused by implicit label noise for robust overfitting. This method is evaluated on CIFAR 10/100 and tiny-Imagenet. Concerns have been raised in the reviews about sufficient justification for the claim that implicit label noise leads to adversarial overfitting. The rebuttal answers this question to some extent. Concerns have also be raised about the writing and whether sufficient details of the experimental setup are present in the main paper. While I acknowledge the difficulty of fitting all details within page limits, I would think that these details are crucial given that primary support for the claims made are from empirical observations."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this work, the authors aim to show that double descent in adversarial training might be caused by implicit label noise, that is, the distribution mismatch between the true label distribution and the assigned label distribution of the adversarial examples. They empirically support this claim by showing that training with static adversarial examples would also encounter the double descent phenomenon. To further solve this problem, they propose to apply temperature scaling and interpolation to create a soft label for each adversarial sample in adversarial training. Experiments on CIFAR10/100 and Tiny-imagenet are conducted to validate the efficacy of the proposed method.",
            "main_review": "Strengths :  \n1. This work provides a novel and interesting analysis for explaining the robust overfitting problem of adversarial training. In this work, they make a connection between the double descent in learning with noisy labels and the robust overfitting in adversarial training by introducing “implicit noisy labels”. I believe the novel analysis will provide a new perspective for the community to explain some other phenomenons.\n2. This paper is well-written and easy-to-follow. The authors give a clear analysis from the perspective of implicit noisy labels and further propose a simple method based on the analysis.\n3. The evaluation of the proposed method is conducted with Autoattack, which makes this work more convincing.\n\nWeaknesses :\nAlthough the analysis sounds pretty interesting, the support for the explanation is somewhat weak. In particular, the authors claim that double descent in adversarial training is caused by implicit noisy labels and empirically verify it by showing that training with static adversarial examples would also encounter the double descent phenomenon. In my opinion, the causal relationship between double descent and implicit noisy labels is not clearly supported.",
            "summary_of_the_review": "Overall, the analysis in this work is novel and interesting, although there exists a weakness in supporting the claim. In this concern, I recommend a score marginally above the acceptance threshold",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes identifying the robust overfitting as the early part of epoch-wise double-descent, which is caused by implicit label noise derived from the mismatch between the true label and assigned label distributions on the adversarial examples. The authors further provide a method that combines temperature scaling and interpolation to mitigate robust overfitting. Both theoretical and experiments are included to show the effectiveness of the proposed method on realistic datasets.\n",
            "main_review": "\nStrength:\n1. as far as I am concerned, the idea of implicit label noise is interesting and can potentially encourage further investigations that connect the fields of adversarial robustness and learning with noisy data.\n2. the author proposed a method using temperature scaling and interpolation to reduce the implicit label noise, hence, mitigating the robust overfitting.\n3. Both theoretical investigations and numerical experiments are provided, showing the proposed method can help mitigate robust overfitting on realistic datasets.\n\n\nWeakness/Concerns:\n\n1. From my viewpoint, claims like \"label noise is often essential to explain double descent in standard learning for modern neural architectures\" are overstated and misleading. \n\n   In fact, the corresponding citations in the paper (Nakkiran et al., 2020; Yang et al., 2020b) do NOT explain double descent as a result of label noise. Instead, Nakkiran et al., 2020 explain double descent in terms of model misspecification, and Yang et al., 2020b explain double descent in the context of the bias-variance trade-off together with the unimodal-behavior of variance. \n\n   Another claim of such kind is at the beginning of section 3.5, where the authors state, \"the effect of label noise on double descent has been rigorously studied based upon both analytical settings ...\" After briefly checking the citations, none of them recognize label noise as the cause of double descent (correct me if I'm wrong; I could make a mistake here since I only quickly go through those papers.).\n\n   While adding label noise makes the double descent phenomenon more evident, the key concept is still the interplay between the complexity of the model and data. The author should rephrase relevant statements to make them more rigorous.\n\n2. The way of presenting definition 3.2 could potentially cause confusion, and I was confused when I read this part first time. The terminology true label and assigned label reminded me $\\arg\\max_jP(Y_\\delta^*=j|x_\\delta)$ and $\\arg\\max_jP(Y^*=j|x)$, which, according to section 3.2, should be the same, thus, making me wonder why there is a need to consider $P(\\tilde{y}_\\delta \\ne y^*_\\delta|x_\\delta)$. On the other hand, if I understand correctly, the phrase \"its assigned label is different from its true label\" in definition 3.2 refers to a \"potential difference\" that resulted from the mismatch between the true label and assigned label distributions.\n\n\n\n\n\n",
            "summary_of_the_review": "This paper studied the cause of robust overfitting and proposed a method to mitigate it. Both theoretical investigation and experiments on realistic datasets are included for demonstrating the effectiveness of the method. The paper is overall well written. However, the presentation of key concepts needs to be modified, and overstated claims should be rephrased. The key idea of implicit label noise is novel and insightful and could inspire works that connect the fields of adversarial robustness and learning with noisy data. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the double descent phenomenon in the adversarial training, with an aim of explaining the wide observation of robust overfitting. The authors made the connection by showing that with a large enough model, the robust overfitting can be seen as the early stage of an epoch-wise double descent. This observation is confirmed with extensive experiments and analyzed by a proposed metric called the implicit label noise. A method is proposed to mitigate such noise and the consequent overfitting.",
            "main_review": "### Strength\n\n* The connection between robust overfitting and model-wise double descent was already observed in the deep double descent paper [1]. But the discussion in the original paper was brief and not intended to make a complete point. In this paper, the authors further connect the epoch-wise double descent to robust overfitting, which is relatively novel.\n\n* The experiments conducted in this paper are extensive. The experiments show that the epoch-wise double descent prevails across different hyperparameters including learning rates, optimizers, architectures and, so on.  \n\n* The implicit label noise hypothesis is interesting. The experiments on the perturbation radius, gaussian noise, and data quality help understand the theoretical results. The method motivated to mitigate implicit label noise is sound. \n\n### Weakness\n\n* On the theoretical results: the authors attribute the double descent to the implicit label noise - a distribution shift of the ground truth labels between the adversarial and clean samples. Given that humans cannot distinguish adversarial examples, it is questionable how substantial such a shift can impact. Comparing Figure 1 against [1], the test loss trend of adversarial training with $l_\\infty$ norm attack of perturbation $8/255$ is similar to the trend of ERM training with 5-10% label noise, which is quite surprising. Does the implicit noise label really have the same impact with such a level of label noise or is there other fundamental differences that makes the adversarial training more challenging? It would be useful to somehow quantify the implicit label noise and understand how severe it has caused.\n\n* On the experimental results: apart from the vanilla adversarial training, the authors mainly compare their proposed method against the baseline proposed in [2]. However, it seems that the authors didn't compare with a complete version of the method as the stochastic weight averaging seems not to be used in their experiment. Since this is not explicitly mentioned in the paper, please correct me if I miss anything. \n\n* Minor: $J$ is not defined in eq. (3). \n\n[1] Nakkiran, Preetum, et al. \"Deep Double Descent: Where Bigger Models and More Data Hurt.\" International Conference on Learning Representations. 2019.\n\n[2] Chen, Tianlong, et al. \"Robust overfitting may be mitigated by properly learned smoothening.\" International Conference on Learning Representations. 2020.",
            "summary_of_the_review": "This paper offers some novel understanding about robust overfitting, but the findings are not surprising as similar observations have been made in the previous literature on the model-wise double descent. I'm also not fully convinced by the equivalent impact of label noise and implicit label noise on the overfitting. The proposed method seems also not significant and the comparison is not very clearly stated in the paper. For these reasons, I'm currently on the negative side.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper demonstrates that \"robust overfitting\" during adversarial training is an early part of an epoch-wise double descent phenomenon for relatively large models. The authors also demonstrate that the overfitting behavior is observed as we select larger perturbations during training.\n\nThe authors explained this \"double descent\" phenomenon using \"implicit label noise\" and finally demonstrated that by temperature scaling and knowledge distillation, we can significantly mitigate the overfitting behavior for adversarial training.\n\n================= Post Rebuttal =================\nThe authors have answered most of my concerns. Hence, I have increased my score to 6.\nPlease find additional comments to improve the organization of the paper and experimental results in the following.",
            "main_review": "The paper is well written and the proposed technique to mitigate the overfitting behavior for adversarial training is clearly motivated in sections 3 and 4. However, I have a few concerns regarding the experimental studies and comparative methods:-\n\n1. Writing: The experimental setup and in general the comparative model is not very well written. \n\n\n2. Comparative methods: \nChen et al. (2021) [1] proposed to apply a self-training smoothed loss function using both robust-/standard-trained self-teachers (in Eq. 1 of [1]). That required two hyperparameters i.e., $\\lambda_1$ and $\\lambda_2$. Next, they apply stochastic weight averaging (SWA). \n\n--- However, the authors compared with \"PGD training combined with self-distillation (KD-AT) with fixed temperature $T = 2$ and interpolation ratio $\\lambda$ = 0.5 as suggested by Chen et al. (2021).\" To the best of my understanding, this is a variant of their proposed technique and not the technique proposed in [1]. Can you please comment on this?\n\n\n3. Comparative results in Table 1:\nChen et al. (2021) already demonstrated to reduce the best vs last robust accuracy to almost close to 0\\% (using Auto-attack and CW attack in Table 4 of Chen et al. [1]). However, these results are significantly better than the reported results for comparative models in Table 1.\n\nA few minor comments:-\n\n1. (Section 3.3) \"It has been widely observed that adversarial examples transfer between different classifiers even with distinct architectures \"\n\n-- I don't think that this comment is true for adversarially trained robust models. It may be better to remove this line.\n\n2. Theorem 4.1 and 4.2: Please mention that the proofs are provided in the appendix.\n\n3. Figure 5(c): why didn't you use the same number of training epochs for TinyImagenet and CIFAR-10/CIFAR-100 datasets?\n\n4. Appendix should have been submitted along with the main paper, after reference.\n\n[1] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting may be mitigated by properly learned smoothening. In International Conference on Learning Representations, volume 1, 2021",
            "summary_of_the_review": "Pros:- This paper demonstrates that \"robust overfitting\" during adversarial training is an early part of an epoch-wise double descent phenomenon for relatively large models. Proposed a novel technique motivated by this analysis.\n\nCons:- Comparative results are not convincing enough. \n\nOverall, I liked the paper. I am giving an initial score of 5 due to my concerns about comparative models in experiments.  If the authors can clarify my concerns, I shall increase my score towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}