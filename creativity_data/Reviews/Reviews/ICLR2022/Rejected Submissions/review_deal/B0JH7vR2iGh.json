{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper includes an interesting idea of pushing towards good, and away from bad, trajectories, in a natural clean way.\n\nThe main problem of the paper is one of clarity.  The paper could be written to be more concise and clear, which would allow, for instance, for sufficient space for the figures (which are currently sometimes rather tiny) as well as not having to fiddle with the margins and spaces quite as much as the current submission seems to do (which would be strictly disallowed at most conferences).  The issue of clarity was also clear during discussion, where sometimes multiple rounds of clarifications were needed to allow the reviewers to correctly interpret parts.\n\nFor these reasons, I recommend that the authors resubmit a new, cleaned-up, version of the work, with all the changes neatly incorporated.  Then I think this could make for a nice addition to the literature.\n\nI appreciate this will be a disappointment to the authors, but I think ultimately it will make their work more impactful, and longer-lasting."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new exploration scheme for general multi-agent reinforcement learning called PMIC. PMIC maintains two separate buffers by keeping trajectories with high rewards and trajectories with less satisfying rewards. PMIC additionally computes a lower bound of mutual information between state and policy over the positive buffer and an upper bound of the negative buffer. Finally, an intrinsic reward in the form of the lower bound minus the upper bound is introduced as the exploration bonus, which improves a collection of different MARL algorithms on a few benchmark testbeds. ",
            "main_review": "## Post-Rebuttal\nI now fully understand the algorithm. Thank the authors for the detailed response. I misunderstood the motivation of using the two bounds. Now the algorithm design looks pretty clear and intuitive to me, which is simply reducing MI on poor trajectories while increasing MI on good trajectories.\n\n**some suggestion on writing**\n1. I think the position of Sec. 3.2 makes it a bit misleading since it is presented after the motivating example (Sec. 3.1).  Sec. 3.1 has already assumed the optimizing objective for cooperation is MI and presents the motivating example based on this assumption. Note Sec 3.2 simply re-stated why MI can be a good optimization objective for learning, which is, in fact, the assumption in the previous subsection! Wouldn't it be more appropriate to justify the assumption before assuming it? This cyclic statement would make the presentation less consistent for the readers (including me) to follow. From the best readability, I would suggest the authors have a separate preliminary/motivation section, which contains the current content of Sec 3.2 first (to first show why optimizing MI is good) and then the content of Sec 3.1 (to show the issues of directly optimizing MI when having multiple patterns).\n\n1. I would still encourage the authors to make the best efforts to add the related work section back to the main paper. This paper is related to a lot of good ideas in existing works. It would be critical to mention them directly. The authors may possibly make the experiment sections a bit more concise and shrink the conclusion section to make some space. \n\n\n=======================================================================\n\n### Strong Points\n1. The proposed algorithm is, to the best of my knowledge, novel and it is indeed interesting to see such a separation of buffer with mutual information computation significantly boosts the performance of a collection of MARL algorithms, **with the assumption that the derivation is correct**.\n2. There are a lot of ablation studies conducted in the experiment section as well as the appendix, which is appreciated. The experiment section is clear and easy to follow.\n\n### Weak Points\nIn general, before moving into more detailed comments, I would strongly encourage the authors to substantially re-organize the paper to make it easier for the readers to follow. Also, most of the arguments and motivations in the paper are very hand-wavy, which makes me very concerned about the correctness of the proposed algorithm. It could be because of the writing issue, but, at least from the current statements in the paper, the correctness cannot be rigorously justified. More comments follow.\n\n1. A related work section would be encouraged.\n\n    Sec. 3.1 and the introduction section have a substantial overlap describing the existing literature, which should be put into a separate related work section. Fig.1 also looks strange as it appears in a motivation section. Fig.1 seems to only serve as a role of toy example showing that PMIC works better than some baselines. This is an experiment result rather than a motivation example! It doesn't make sense to me to motivate an algorithm by simply saying hey it works well. As a research paper, a reader would expect insights before moving into empirical evidence of your algorithm being good. I would appreciate it if the authors could rewrite Sec. 3.1 such that some more concrete examples of how the algorithm is working, for example, intuitively what kind of behavior leads to high MI; what characteristics are those sub-optimal strategies intuitively so we have a sense of what kind of policies we want to avoid; and so on. The current Sec.3.1 is simply a replication of the introduction section. \n\n2. I couldn't find how sec. 3.2 and sec 3.3 are related\n\n    > However, simply maximizing this MI can hinder algorithm learning as discussed above.\n\n    Eq.(1) definitely makes a lot of sense to me and there are existing works that consider similar objectives for MARL learning (not exactly the same objective but somewhat similar, like this one for example https://arxiv.org/pdf/2106.02195.pdf ). But why does this objective hinder learning? Is this really discussed? I have made my best efforts on understanding the discussions in Sec.3.1 but I could find any convincing arguments of how this objective hinders learning. \n\n   > Therefore, we propose a novel framework PMIC to solve the problem\n\n   Okay, even if this objective does have hindered learning, then why does the proposed PMIC algorithm solves tackles the issue in this objective? What is the relationship between Eq(1) and PMIC? To be concrete, for example, Sec.3.1 argues that the policy should be diverse (well, the terminology isn't really appropriate but let's use it for now) as suggested by using the entropy bonus but I couldn't find any corresponding term in PMIC for doing so. So what problem does PMIC solve? \n\n   There are so many hand-wavy arguments in this paper like the two above.\n\n\n3. There are no rigorous justifications on the relationship between high-reward trajectories and MINE/CLUB\n\n    > Intuitively, only the behaviors that obey superior collaboration patterns or inferior collaboration patterns have large MI estimates calculated by MINE or CLUB since $\\mathcal{L}(\\omega_1)$ and $\\mathcal{L}(\\omega_2)$ are optimized.\n\n    - *I think this statement is the foundation of the correctness of the method*. If this statement is correct, the whole algorithm becomes neat. Yes, I do have witnessed empirical analysis supporting this argument in Fig 9 and Fig 15. But, I want to emphasize that those two figures are empirical evidence rather than rigorous derivation. Typically, as a research paper, the conventional way of algorithm development is to first provide rigorous deviation (e.g., mathematical proof; equation analysis and approximation; or explanation and discussion on concrete toy examples), then develop an algorithm that solves the formulated problem and finally present empirical supports. The authors simply present this key argument by using the term ''__intuitively__'', which I failed to catch with my best efforts. This argument is the most critical point of the method and if this argument is wrong, the whole paper becomes problematic. Hence, I strongly encourage the authors to provide formal and rigorous justification or proof to this statement. \n\n    - Here is what confuses me a lot. Note that the two buffers are categorized according to their _rewards_ while both MINE and CLUB are bounds for _mutual information (MI)_. So what is the mathematical connection between rewards and MI? Why should superior strategies have higher MINE value? Can you mathematically prove or justify this? To the best of my knowledge, I cannot find any related proof in the existing literature.\n\n    - MINE is a generic lower bound for MI. So wouldn't optimizing MINE with the entire replay buffer strictly lead to a higher lower bound than only using the positive buffer (you have more data!)? Similarly, why not optimize CLUB using the entire buffer as well? Fig 9 does not include such ablation either. Assuming the correctness of all the arguments of MINE and CLUB, using the entire replay buffer should lead to at least the same or probably even better behavior compared with PMIC since you have both tighter lower bound and upper bound? \n\n### Other Issues\n1. > the joint policy should be diverse enough to avoid being stuck into sub-optimal collaboration.\n\n    Typically the term \"_diverse_\" refers to a set of (or a distribution of) different policies rather than a single joint policy. Also, according to the phrase \"_avoid being stuck into sub-optimal_\", it seems that diverse in fact means that the policy maintains high entropy _throughout the training process_ rather than describing the characteristics of the _final policy_ (the final policy can have low entropy if it converges but during training, it would better have high entropy for the purpose of exploration). So I would suggest the author direct state the purpose of avoiding poor local optimum rather than using the term \"_diverse_\" (you may be encouraged to refer to the literature of learning diverse skills/strategies in deep/MA RL literature)\n\n2. > 3) Meanwhile, given an agent i’s policy on the global state, the uncertainty on other agents’ policies also should be low, thus more easily to achieve better collaboration\n\n    I don't get why this point is necessary when we have 2) already. I couldn't find any corresponding objective that minimizes the intra-agent policy uncertainty in Sec 3.4. I don't know why this point is presented here. \n\n4. minor issues\n    - The predator-prey environment is not a cooperative one, for which the one-sided reward wouldn't become a convincing metric. Are you referring to the cooperative version of predator-prey? I didn't find any content confirming this. \n\n    - I would encourage the authors to have a sub-index for each plot in Fig 8 rather than saying sth like \"the third plot in fig.8\".\n\n    - A possibly related paper also using mutual information formulation for cooperative MARL: https://arxiv.org/abs/1910.05512\n \n    - Fig 15 is okay but it is not really super convincing. I can still observe some color differences in the right plot (I guess the values range from 0.6 to 1.0 maybe?) So probably you just need some tuning of beta or value normalization to also make VM3-AC work. \n",
            "summary_of_the_review": "Post-rebuttal: \nAfter carefully re-checking the paper with the feedback from the authors, I have fully convinced by the methods and therefore changed my recommendation to acceptance. \n\n==================================\n\nThe paper proposes a seemingly neat algorithm and presents a good amount of experiments. However, some critical correctness flaws cannot be rigorously justified based on the current content of the paper due to too many hand-wavy arguments. The readability of the paper should be improved as well.\n\nI cannot recommend acceptance based on the current form of the paper. But, I would still believe in the potential of the algorithm based on the experiment results. That being said, the paper still has the potential to become a really strong one if the algorithm can be rigorously (or mathematically) justified.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to maximize the MI between the global state and joint policy for promoting collaboration. To achieve better collaboration, a Dual Progressive Collaboration Buffer (DPCB) which stores superior and inferior samples separately is introduced, and then the proposed method maximizes the lower bound on MI using the samples in superior buffer and minimizes the upper bound on MI using the samples in inferior samples. ",
            "main_review": "Strengths\n\n- This paper addresses an important issue of the MI-based MARL framework where the joint policy converges to sub-optimal.\n- The proposed idea, which maximizes MI based on superior samples and minimizes MI based on inferior samples to avoid the joint policy falling into the sub-optimal, is novel.\n- The authors provide various ablation studies for better understanding.\n\n\nWeakness and Questions\n\n-There is a lack of explanation for the collaboration criterion (section 3.2), e.g, as I understand, $s$ is a state variable whose distribution is conditioned on $\\pi$. \n-For the interpretation of the proposed MI form, the authors decompose the proposed MI into the entropy of join policy ($H(\\pi(\\cdot|s))$, the negative entropy of an agent's policy ($-H(\\pi_i|s)$) and conditional negative entropy of the agent's policy given other agents' policies ($-H(\\pi_{-i}|, \\pi_i, s)$). The authors claim that maximizing $-H(\\pi_{-i}|, \\pi_i, s)$ minimizes uncertainty on other agents' policies. However, $H(\\pi_{-i}|, \\pi_i, s)$ reduces to $H(\\pi_{-i}|s)$ since $\\pi_{-i}$ and $\\pi_i$ are independent given the state. Thus, it seems that the interpretation of the proposed MI form, $I(s;\\pi(\\cdot|s))$, is not proper. In this regard, can we say that the proposed MI form causes the collaboration?\n-The authors minimize $I(s;\\pi(\\cdot|s))$ for experiences which have low returns, so it encourages the joint action to explore for inferior experiences. This is because the joint action should be uniform to minimize $I(s;\\pi(\\cdot|s))$. On the other hand, maximizing $I(s;\\pi(\\cdot|s))$ for experiences that have high returns encourages the joint action to exploit for superior experiences. Thus, in my opinion, the effectiveness of the proposed method comes from the further exploration and exploitation strategy rather than the collaboration strategy. To see where the performance gain comes from, it would be great if the authors compare the proposed method with MA-SAC+DPCB and VM3-AC+DPCB.",
            "summary_of_the_review": "-The proposed idea is novel, but the author should provide further explanation on collaboration criterion and further experiments to see where the performance gain comes from. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes PMIC a MARL framework for improving multi-agent collaboration through mutual information. PMIC uses two separate buffers to store the superior and inferior trajectories, and uses them to train two estimators for the upper and lower bounds for the mutual information between global state and joint policy in two sets of trajectories. The estimated mutual information bounds are then used as additional rewards for training individual policies. The experiments presented in the paper show the advantage over several baselines on several benchmark environments.",
            "main_review": "Strengths:\nThe proposed method is novel and shows good performance.\n\nWeaknesses:\n1. While the results generally look good, some reward curves in Figure 5, 6, 7, 8, and 9 are not converged. It would be better to compare the converged performance between baselines. \n2. I am not quite following why minimizing the MI in inferior trajectories is helping improve the corporation. Generally, I would suggest more explanations on the motivation of the PMIC.",
            "summary_of_the_review": "I think the submission overall is good for its novelty and experiment results. I only have some minor concerns about the theoretical backup of the proposed method. Thus, I suggest accepting the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to solve the collaboration among agents (that is usually called forced coordination in other literatures). The common method to enhance the correlations between agents by mutual information (MI) could lead to the sub-optimal collaboration. To address this issue, the author propose Progressive Mutual Information Collaboration (PMIC). This new MARL framework is composed of two components: (1) two replay buffers that respectively collect superior trajectories and inferior trajectories; (2) two mutual information estimators that respectively estimate the upper bound and the lower bound of MI that are trained with the data stored in the two replay buffers introduced above. These two estimators (MINE and CLUB) are from the prior works. The novelties of this work are applying these two estimators to collaboration problem for MARL and learning the mutual information that evaluates the correlation among agents constrained within these two bounds. In my view, the idea of appropriately modelling these MI bound estimators to the actual phenomenon in the MARL problem is novel. Seen from the algorithmic framework type, this algorithm add the learned MI term to the reward function that plays the role of auxiliary reward. From the perspective of optimization, this is a kind of implementation of Lagrangian method, to transform the constraints stated in Equation (2) and (3) to a unconstrained terms involved in the objective function. Similar to many prior works, the tuning of the multipliers could be a potential issue. The authors also conduct the ablation studies on these two multipliers and the results seem like it would not affect the performance in the case of Cooperation Navigation.",
            "main_review": "## Strengths\n\n1. The writing of this paper is generally clear.\n2. The motivation of this paper is clear that aims to solves the problem that is observed from the existing state-of-the-art algorithms.\n3. The experiments are sufficient, including the discrete/continuous action cases, different environments/tasks and multiple ablation studies to verify the effectiveness of the proposed framework.\n4. The proposed framework can be incorporated into any MARL algorithms, so it is a general method.\n\n## Weaknesses\n\nThis paper does not have obvious weaknesses, but I have some concerns that the authors need to clarify.\n1. Can the authors give more details about how $\\mathbb{P}\\_{ \\mathcal{S} \\mathcal{U} }$, $\\mathbb{P}\\_{\\mathcal{U}}$ and $\\mathbb{P}\\_{\\mathcal{S}}$ implemented?\n2. In experiments, the authors claim that the results from the first two graphs in Figure 8 shows that the proposed maximizing and minimizing MI can improve the stability. However, the stability of the proposed method is not well, i.e., the variance of the proposed method is much larger than other competitors. Can the authors give more explanations on this?\n3. For the abalation study in Figure 9, the authors did not show the case that PMIC-MADDPG-normal\\_buffer that can directly show the effectiveness of the proposed replay buffer. Can the authors show the result for it?\n4. If I understand correctly, $T\\_{\\omega\\_{1}}$ is designed with the state and action encoders while $T\\_{\\omega\\_{2}}$ is without. Can the authors explain the reason for the inconsistent design for these two models? It is better that the authors can provide the results for MINE without state/action encoders to get rid of the suspect that the performance improvement relies on the encoders rather than the main contribution claimed in the paper.\n\n## Minors\n\nThere are multiple writing typos in the paper.\n\n1. \"Similarly, CLUB approximates the lower bound ...\" where lower bound should be upper bound. \n2. $I(s; \\pi(u|s)$ appearing in multiple places should be $I(s; \\pi(u|s) )$.\n3. Above Equation (3), $D\\_{a}(\\cdot)$ should be $D\\_{u}(\\cdot)$.\n\nThe above are just the ones I found. I urge the authors to check the paper writing again, and revise these writing typos in the discussion stage.",
            "summary_of_the_review": "In summary, this paper has some novolty but stays at the application level, though this is not my concern. My main concern is the effectiveness of the main contribution claimed in the paper, i.e. the maximization-minimization MI. I concern that whether the performance improvement mainly comes from the specific architecture $T$ used in the framework.\n\nFor the current version of paper, I can only recommend reject (but marginally below the threshold) due to the weakneses and concerns above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}