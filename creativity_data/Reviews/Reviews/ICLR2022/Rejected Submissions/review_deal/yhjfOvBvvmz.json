{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors present a method for learning disentangled skill representation that uses weak supervision. The reviewers mentioned that the paper tackles an important problem, delivers interesting and novel visualizations of the learned skills, and positions the paper well in the context of related work. The reviewers also point out several points of criticism: the complexity of the method, lack of convincing comparisons to baselines that utilize the same amount of data and the quality of writing, among others. I encourage the authors to address these points in the future version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method that learns disentangled skill representations, and shows qualitative & quantitative results on Mujoco Ant navigation. (1) They first synthetically generate a trajectory dataset by the combination of different factors. (2) They train a trajectory VAE (Co-Reyes ‘18) that enforces the learned trajectory representations to be disentangled using weak labels (Margonis ‘20). (3) Then, keeping the pre-trained trajectory VAE fixed, they learn a skill-based policy to generate similar trajectories to the learned decoder of the trajectory VAE by minimizing the KL divergence between the trajectory distributions.",
            "main_review": "Strengths:\n\n* Interesting visualizations of learned interpretable skills on the Ant navigation task. Additionally, the paper showed that combining interpretable skills with model-based planning (MPC) for long-horizon tasks can result in better performance on maze navigation.\n\nWeaknesses:\n\n(1) The data requirements for the proposed method seem impractical for most real applications. The method requires a synthetically generated dataset (in their experiment, they used 100K trajectories), which are generated by varying the generative factors. If I understood the dataset generation procedure correctly, generating this dataset (of trajectories with varying speeds/directions/acceleration) requires that you already have expert policies that can run at various speeds/directions/accelerations.\n\n1a) How does the dataset size & quality affect the quality of the learned disentanglement, and performance of the learned policy?\n\n1b) (Clarification) Does the 100K trajectory dataset consist of the full observations from Ant/Half-Cheetah (e.g., including joint velocities)? If so, were expert policies and a simulation environment needed to synthetically generate this dataset?\n\n1c) (Clarification) Did the other methods (SeCTAR, DADS) make use of the same 100K trajectory dataset (for fairer comparison), and if so, how?\n\n(2) The idea of using weakly-supervised disentangled representations in order to learn an “interpretable” policy behavior, and comparing against unsupervised representation learning methods such as VAE, has also already been studied in a prior work [1], which was not discussed/cited in the paper. The method in [1] also learns disentangled latent encodings Z, which is used to condition the policy, leading to “interpretable” skills. \n\n[1] “Weakly-Supervised RL for Controllable Behavior”, NeurIPS 2020 https://arxiv.org/abs/2004.02860\n\n(3) In general, I felt that the writing & presentation could be improved.\n\n3a) I had trouble understanding what “latent traversals of trajectories of the decoder” mean (in Figures 4, 5, 10, 11, 13, 15, 16). For example, in Figure 4, could you clarify what the X- and Y-axis labels are, and what each line represents? I am guessing that each line corresponds to a different latent variable (by varying one dimension, while keeping the rest fixed), and the X/Y axes are positions of the ant?\n\n3b) Could you elaborate on *why* the disentanglement helps with MPC planning & result in higher performance (Fig 7)? Could it possibly be because the learned skills are more distinguishable from one another? Can this be verified empirically somehow?\n\nMinor typos:\nPage 3: “decesion\" -> “decision”\nPage 3: “We will focus on the aspect of the generative model of the decoder”: I wasn’t sure what this sentence means. Did you perhaps mean: “we will focus on the generative modelling aspect of the decoder”?\nPage 3: “which learns the representations in the unsupervised manner” ->  “which learns the representations in an unsupervised manner”\nPage 5: “in the Equation 6” -> “in Equation 6”\nPage 7: “based on the each factor” -> “based on each factor”\nFigure 3: “solving the long-horizon problems” -> “solving long-horizon problems”\nFigure 15: “the direction ranging the full circle angles” could be better worded.\nFigure 16: “half-cheetah” -> “Half-Cheetah”\n",
            "summary_of_the_review": "My main concern is regarding the practicality of the proposed method (see Weakness #1 above). I also feel that discussion of prior work and overall paper presentation could be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach for learning interpretable skill embeddings by leveraging an automatically generated dataset of trajectories annotated with factors of variation like direction, speed and curvature which should be used to structure the learned skill space. It shows that these factors can indeed be used to learn a latent space with disentangled factors of variation which can be used via an MPC-based planning method for solving downstream tasks in a OpenAI Gym ant environment.",
            "main_review": "## Strengths\n- The related work section provides a comprehensive overview of relevant prior works.\n\n- The provided planning visualizations clearly show the differences between the learned skill spaces and how they influence downstream learning.\n\n- The appendix provides detailed instructions for generating the used dataset and reproducing the policy training.\n\n\n## Weaknesses\nI have a number of major concerns with respect to the proposed problem formulation and approach:\n\n1. The proposed approach is compared to unsupervised skill learning approaches (like DIAYN, SeCTAR, DADS), but it assumes access to an automatically pre-generated dataset of training trajectories. This directly violates the premise of unsupervised skill-*discovery* approaches: if we were able to generate meaningful trajectories in the first place, we would not need to train an agent to discover them. This becomes more apparent if we move from simple 2D ant navigation problems as used in the paper's evaluation to more complex problems like kitchen manipulation with a robot arm: there it would be very challenging to pre-define object interaction primitives etc, which is why unsupervised skill discovery approaches were proposed in the first place. Thus, I believe the problem formulation of the paper is fundamentally flawed.\n\n2. The proposed approach does not learn disentangled skills with \"weak supervision\" but assumes fully annotated training trajectories with all relevant factors of variation given for every time step. Thus I see the proposed method as a fully supervised approach to learning a disentangled latent space instead with strong assumptions on the available supervision. Defining this set of \"interesting factors of variation\" in the first place requires considerable human effort and can be non-trivial in harder problems.\n\n3. The paper does not motivate clearly *why* it would be beneficial to train a latent space with disentanglement of the given factors of variation. This should be much clearer motivated in the introduction. Currently it only vaguely mentions that a lack of disentanglement and interpretability of skills \"limits their applicability\".\n\n4. The explanation of the model is in part unclear. I am specifically confused about section 4.2.2 (see questions below). This also makes understanding the downstream usage of the model challenging.\n\n5. Section 5 introduces an approach which the paper calls \"trajectory scaling\". From my understanding it simply describes the common RL practice of \"action repeat\" which is eg used by default in many Atari games. Although it is less common in MPC-based planning approaches this connection should be made in the paper.\n\n6. The experimental comparisons to *truly* unsupervised approaches like SeCTAR and DADS are not fair in my understanding: these approaches do not have access to a pre-collected dataset of demonstrations and thus need to learn to explore the environment while learning the skill policy. This is a much harder task, but this distinction is never acknowledged in the experimental section.\n\n7. The writing of the paper in part impairs its clarity. Many formulations stay vague or are not put in appropriate context, eg from the abstract without any further explanation of \"single-step transitions\" or \"trajectory-level behaviors\": \"Additionally, we propose to train a policy network with single- step transitions and perform the trajectory-level behaviors at test time with the knowledge on the skills, which simplifies the exploration problem in the training.\" In other instances the writing does not seem to explain all the details necessary to understand the introduced concepts (see point (4) above).\n\n\n## Questions\n- I am confused why we need to train single-step transition models for reward computation instead of training with the pre-trained trajectory-level models as reward-computers. Is this to get a denser reward function? How does the model in Figure 1 change when we split the latent variable into z_single and z_multi? How does \"heuristically\" finding the relation functions between z_single and z_multi work exactly?\n",
            "summary_of_the_review": "In my opinion the assumptions made in the paper contradict the premise of the unsupervised skill discovery problem. Further, the approach is not clearly described, the experimental evaluation is not fair due to differing assumptions and the overall writing of the paper needs improvement. Thus, I do not recommend acceptance of the submission.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper at hand proposes a new framework for pre-training skill policies (WEDIS), and use them for control in a hierarchical setup with MPC. The main idea is that skill policies should follow a set of generated trajectories based on some salient factors. For this, the authors train a VAE that will then provide both the control variables (latent encoding) and a predictive model of the trajectory (decoder). The skill policy is trained to match the trajectories of the predictive model, and the predictive model is then used for MPC.",
            "main_review": "Overall, I found the method introduced in the paper to be somewhat complex, and the experimental section to not justifying this complexity. The overall ingredients (VAE on trajectories, train skill policies on a resulting reward signal) are among the current techniques used in skill discovery (e.g., OPAL (https://arxiv.org/abs/2010.13611), EDL (http://arxiv.org/abs/2002.03647)). While the authors show gains over purely unsupervised skill learning methods (DADS, SeCTAR) on the downstream navigation tasks, my main concern is the way that supervision is introduced in WEDIS. I have trouble understanding the motivation for an auto-encoder setup when reference trajectories can be generated beforehand. Why not simply pre-train a policy with a dense reward to explicitly follow these trajectories, conditioned explicitly on the salient factors (direction, speed, curvature)? This should at least be a baseline in the paper. I can see how the training setup in WEDIS could, in principle, allow for more generality, but then again it requires the ability to generate reference trajectories (which in turn would make it possible to train policies to follow them directly). The experiments also lack a comparison to plain Soft Actor-Critic.\n\nFinally, I don't really agree with the premise that current skill learning methods are fundamentally hindered by a lack interpretability. While it's true that the availability of a model enables the usage of standard planning methods like MPC, several works also showed effective hierarchical control with RL, where explicit semantics are not strictly required (e.g., DIAYN, OPAL (https://arxiv.org/abs/2010.13611) or NPMP (https://arxiv.org/abs/1811.11711)).\n\nThe writing would benefit from an overhaul wrt grammar.",
            "summary_of_the_review": "In my view, the experiments in the paper fail to justify the complexity of the method that was introduced. With the amount of supervision required, simpler and potentially more effective approaches come to mind but are missing in the comparison.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes to learn representations of artificially generated trajectories using a VAE-based approach that relies on weak labels to improve disentanglement. Then, a skill-based policy (i.e. a policy conditioned on the learned sequence representation) is trained to imitate the learned model via soft actor-critic. Moreover, the paper introduces a hierarchical planning strategy that explores at the “aggregated sequence level”, i.e. the representation space, and allows for “trajectory rescaling”. The authors show empirically both the disentanglement of WET-VAE and the advantage of their hierarchical approach.  ",
            "main_review": "My main concerns with the work are its lack of clarity and how many decisions related to the design of the algorithm are not well justified.\n\n- As a non-expert, I found the paper hard to digest. In the process of reviewing this paper, I read some other already published works and found them easier to follow and better motivated. For instance, a clean definition of what is meant by \"skill\" would have been helpful.\n- The weak-labels used to improve disentanglement are computed from the synthetic trajectories. From what I understand, they contain a lot of information about the direction, speed and curvature. If the goal of the WET-VAE is to extract direction, curvature and speed from the trajectories, why not using supervised learning to do that? For instance, one could train a neural network to output direction, speed and curvature using the supervision of the computed labels y. Then, one could use this encoder trained with supervision to train a generative model conditioned on these.\n- The advantage of disentanglement is not obvious here (apart from interpretation). It seems to be used only to separate z into z^single and z^multi. But it is not clear if this is useful or why it should be useful. This connects to the following points related to the MPC approach.\n- In Algorithm 2 when executing the policy, I did not understand why it was important to exclude z^{multi} from the latent representation fed to the policy. An ablation study would be convincing. \n- In Algorithm 2, the role of the hand-crafted functions f is unclear to me. My understanding is the following: when executing a rescaled plan, it is used to update skill z from one rescale step to another. What is the advantage of doing so instead of just using z*_1 across the subtrajectory?\n- In Algorithm 2, at each step, planning in the latent space is performed to pick the best action. What is the distribution from which the potential trajectories z_1, …, z_{H_p} are sampled? Is it uniform?\n- In some sense, both the artificially generated sequences and the scaling factor H_z must be chosen by the user. It seems H_z is thus redundant since the appropriate sequence length could have been chosen  prior to the sequence generation, no? This question also makes me doubt the usefulness of the hand-crafted functions f, which I suspect are useful only because of the rescaling.\n\n",
            "summary_of_the_review": "Given the lack of clarity and the multiple algorithm design decisions that are not sufficiently justified, I cannot recommend acceptation. I cannot comment on the novelty of the contribution as I am not well versed into hierarchical reinforcement learning.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}