{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents an approach for online continual learning where only a single pass over each task's data is allowed. Instead of the oft-used softmax classification setting in continual learning, the paper proposes to use the generative setting based on the nearest class mean (NCM). The paper claims that it avoids the logits bias problem in the softmax classifier and helps combat catastrophic forgetting.\n\nWhile the reviewers found the basic idea interesting, there were concerns about novelty and lack of clarity regarding the reasons for improved performance. In particular, there are several aspects from existing work that are leveraged in this paper (e.g, replay, metric learning loss, combination of generative and discriminative classification, etc) but the paper lacks in establishing which of these components affect the performance and in what ways.\n\nThe authors and reviewers engaged in detailed discussions; however, the reviewers were still unsatisfied and did not change their assessment. Based on my own reading of the paper as well as going through the reviews and discussions, I too concur with their assessment. It would be a stronger paper if the paper could shed more light on the above aspects as well as address the other concerns raised by the reviewers. However, in the current shape, it is not ready for publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors attempt to improve online continual learning (CL) performance by eliminating the logit bias of the classifier used. They use a nearest-class-mean (NCM) classifier and a multi-similarity metric learning loss coupled with an auxiliary loss to achieve a good plasticity-stability balance.",
            "main_review": "The paper is somewhat original and the text is relatively clear.\n\nThe authors describe the proposed approach in detail, and they present plenty of experimental results. The experimental section in general has convinced me that the proposed approach does indeed outperform the state of the art.\n\nThe authors argue that their method outperforms the state of the art because it optimizes the feature space in a generative way and thus enjoys the benefits of using a generative classifier. In combination with the NCM classifier, the authors propose a hybrid generative-discriminative loss. \n\nThe authors show that the NCM classifier can be interpreted as a very rudimentary generative model (i.e., an isometric Gaussian). This makes sense but it does not justify using a NCM classifier instead of a softmax. Moreover, the authors are never in this paper involved in actually generating latent-space data. Finally, the NCM classifier can also be interpreted as a discriminative model as it divides the latent space into Voronoi cells, hence modelling the decision boundaries between the different classes.\n\nRegarding the experimental results, the authors should probably compare their approach to the SCR method as well (see [1]).\n\nIn my view, the major downside of this paper is that it looks like an exercise in achieving better numbers than the baselines, instead of an attempt to document and transmit the knowledge gained from the underlying study. Having read the paper twice, I cannot say that I learned something new, apart from the fact that the proposed method outperforms the state of the art. In my opinion, the explanation provided as to why that is is only speculation. A paper like this needs to describe in detail assumptions about the specific reasons leading to superior performance and test whether these assumptions hold water. In short, I think this paper will benefit substantially by attempting to discover the underlying factors behind the improved performance.\n\n\\\nQuestions\n- If deep-metric-learning losses and the cross-entropy both maximize the mutual information between features and labels why would you expect different results?\n- Can you explain what do you mean by stating that the MS loss optimizes the feature space in a generative way?\n- What is the motivation behind selecting only 5k training instance for split MNIST? The paper you referenced cites two papers none of which uses the split MNIST benchmark.\n- By comparing rows 3 and 6 of Table 4 it seems to me that using the hybrid loss instead of only the MS loss is only beneficial in the case of CIFAR-100. Why is that?\n\n\\\n[1] Mai, Z., Li, R., Kim, H., & Sanner, S. (2021). Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning. In Workshop on Continual Learning in Computer Vision at Conference on Computer Vision and Pattern Recognition (CVPR), 2021",
            "summary_of_the_review": "In my view, the paper fails to adequately explain why the proposed method achieves the results it does, and there is a relevant baseline missing. Hence, I recommend rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a hybrid loss of generative classification by NCM as a form of Multi-Similarity (MS) loss (Wang et al., 2019) and discriminative classification by Proxy-NCA (Movshovits-Attias et al., 2017). The paper contains a theoretical argument that the MS loss upper-bounds the 'Gen-Bin loss', which is a generative classification model. The hybrid loss shows decent gains in 'average accuracy' (Table 1) and large gain in 'forgetting' (Table 2) in the experiments with multiple datasets including MNIST, CIFAR-10/100, miniImageNet with smaller computational complexity than the prior arts (Fig. 4).",
            "main_review": "**Strengths**\n- S1: Providing a theoretical argument that the minimizing MS loss is optimizing a generative model with the assumption of uniform distribution of $p(y)$.\n- S2: Computational overhead due to the proposed method is negligible thus practically useful.\n- S3: Pretty compelling accuracy gain in CIFAR10 (5-task), CIFAR-100 (20-task), miniImageNet (10/20-task).\n\n**Weaknesses**\n- W1: The proposed method is a combination of existing loss. The novelty of technical contribution is not very strong.\n- W2: The proposed hybrid loss is argued that it is beneficial as the Proxy-NCA loss will promotes learning new knowledge better (first paragraph in Sec. 3.4), rather than less catastrophic forgetting. But the empirical results show that the proposed method exhibits much less forgetting than the prior arts (Table. 2). The argument and the empirical results are not well aligned.\n\t- Also, as the proposed method seems promoting learning new knowledge, it is suggested to empirically validate the benefit of the proposed approach by a measure to evaluate the ability to learn new knowledge (e.g., intransigence (Chaudhry et al., 2018)).\n- W3: Missing important comparison to Ahn et al.'s method in Table 3 (and corresponding section, titled \"comparison with Logits Bias Solutions for Conventional CIL setting\").\n- W4: Missing analyses of ablated models (Table 4). The proposed hybrid loss exhibits meaningful empirical gains only in CIFAR100 (and marginal gain in CIFAR10), comparing \"MS loss with NCM (Gen)\" and \"Hybrid loss with NCM (Gen)\". But there is no descriptive analysis for it.\n- W5: Lack of details of *Smooth* datasets in Sec. 4.3.\n- W6: Missing some citation (or comparison) using logit bias correction in addition to Wu et al., 2019 and Anh et al., 2020\n\t- Kang et al., 2020: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9133417\n\t- Mittal et al., 2021: https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mittal_Essentials_for_Class_Incremental_Learning_CVPRW_2021_paper.pdf\n- W7: Unclear arguments or arguments lack of supporting facts\n\t- 4th para in Sec.1\n\t\t- 'It should be noticed that in online CIL setting the data is seen only once, not fully trained, so it is analogous to the low data regime in which the generative classifier is preferable.'\n\t\t\t- Why?\n\t- 5th line of 2nd para in Sec. 3.1.3\n\t\t- 'This problem becomes more severe as the the number of classes increases.'\n\t\t\t- Lack of supporting facts\n- W8: Some mistakes in text (see details in notes below) and unclear presentations\n\n**Note**\n- Mistakes in text\n\t- End of 1st para in Sec.1: intelligence agents -> intelligent agents\n\t- 3th line of 1 para in Sec. 3.2: we can inference with -> we can conduct inference with\n\t- 1st line of 1st para in Sec. 3.3\n\t\t- we choose MS loss as training... -> we choose the MS loss as a training...\n\t\t- MS loss is a state-of... -> MS loss is the state-of...\n\t- 6th line of Proposition 1: 'up to an additive constant and L' -> 'up to an additive constant c and L'\n- Improvement ideas in presentations\n\t- Fig. 1: texts in legends and axis labels should be larger\n\t- At the beginning of page 6: Proposition (1) -> Proposition 1. --> (1) is confused with Equation 1.\n\t- Captions and legend's font should be larger (similar to text size) in Fig. 2 and 3.",
            "summary_of_the_review": "Despite the benefits of the theoretical arguments and somewhat convincing results, lack of novelty and missing details prevents this paper from being a quality to be accepted now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "To address the problem of logits bias in class-incremental learning with deep neural networks, this paper proposes to replace the typically used softmax output layer by a nearest-mean classifier (on the feature space), which can be interpreted as a generative classifier. To train the feature attractor in an online setting (i.e., without reliance on task boundaries), the paper proposes a hybrid loss consisting of a multi-similarity loss (which can be interpreted as an approximate generative objective) and an auxiliary “proxy” loss. The paper reports substantial performance improvements on a number of computer vision benchmark (including ones with natural images) over a selection of existing methods.\n\nAdditionally, the paper proposes a new “smooth dataset” benchmark for online class-incremental learning, whereby the transitions between classes are smooth rather than based on abrupt task transitions.\n",
            "main_review": "Strengths:\n-\tIdentifying generative classification as a promising alternative to softmax-based discriminative classification, and demonstrating that nearest-class mean classification (NCM) can be interpreted as generative classification.\n-\tProposing a task-free solution for updating a feature extractor for NCM classification.\n-\tProposing an interesting new benchmark for online continual learning that might be of interest to others working on this problem (i.e., the “smooth datasets”).\n-\tThe reported performance of the proposed method is substantially better than the existing methods that are compared against.\n\n\nWeaknesses:\n-\tMy main issue with this paper is that its two main contributions (i.e., [1] proposing generative classification for class-incremental learning and [2] proposing an online variant of NCM classification for class-incremental learning) have been recently proposed by two other papers, but this paper does not discuss, compare against or cite either of them. Specifically, Van De Ven et al. (2021, CVPR-W; https://arxiv.org/abs/2104.10093) argued for addressing class-incremental learning using generative classification, and De Lange & Tuytelaars (2021, ICCV; https://arxiv.org/abs/2009.00919) proposed an online version of an NCM-based class-incremental learning method.\n-\tAnother relevant paper that would have been good to discuss (and compare against) is Lomonaco et al. (2020, CVPR-W; https://arxiv.org/abs/1907.03799), which also proposes an online class-incremental learning method that addresses the logits bias problem.\n-\tIt is unclear to me exactly what the authors mean by “online continual learning”. Does it mean that there are no clear task boundaries, or does it mean that task boundary information is not provided to the network? At the top of section 4.1, it is stated that “task boundaries are not informed during training”. However, it seems to me that because class labels are provided, task boundaries can trivially be derived? It is also unclear to me why task-incremental learning cannot be performed in the online setting? (As claimed in the introduction.)\n-\tI’m not sure how relevant the motivation of generative classification as a more efficient method in the low data regime is for this paper. My feeling is that for all problems considered in this paper, if the data is provided to the algorithm in an i.i.d. stream, the discriminative classifier would perform better than the generative classifier.\n\n\nMinor comments:\n-\tAn accuracy of 15% on CIFAR-100 or miniImageNet, although perhaps better than the compared methods, is not of any practical value for real-world applications. Perhaps this performance improvement is of interest from an academic perspective, but it does not seem to be of value for any practical applications. I think it would be good to discuss this.\n-\tWhat is meant by “virtual catastrophic forgetting”? It would be good to expand a bit on that.\n",
            "summary_of_the_review": "As explained above, my main issue with this paper is that its two main contributions are not as novel as suggested in the paper, as similar contributions were recently made by three other papers (https://arxiv.org/abs/2009.00919 and https://arxiv.org/abs/2104.10093 and https://arxiv.org/abs/1907.03799). There are some differences with these papers, but I think it is necessary that this paper clearly discusses what it contributes on top of these papers. When that is done, it should be better possible to judge the additional contribution of this paper.\n\nMy current scores are my “expectations” for this paper after a discussion of the above papers (and corresponding moderation of some novelty claims) has been incorporated. If the authors can make a more convincing case that their paper makes important contributions on top of those made by these previous papers, I’d be happy to increase my score. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work develops a novel generative framework to bypass logits bias in online continual learning. They utilized a multi-similarity loss to weight feature pairs better during training time. At test time, they leveraged reserved samples to generate a generative classifier instead of a softmax classifier. Furthermore, they introduce an auxiliary loss to improve the ability of multi-similarity loss to learn from new data.",
            "main_review": "Pros:\n•\tThe paper addresses an important and challenging problem, i.e., online continual learning.\n•\tThe motivation is clear.\n•\tThey conducted extensive experiments to show the consistent superiority of proposed method compared to existing state-of-the-art methods.\nCons:\n•\tThe hypothesis of Proposition 1 is derived in the case of supervised learning and seems to have few connections with continual learning. Why the multi-similarity loss alleviates catastrophic forgetting in continual learning?\n•\tThe hypothesis of this paper is based on that the feature is well discriminative. However, the feature extractor is usually underfitting in online setting. It is difficult to disentangle the effects of catastrophic forgetting from underfitting.\n•\tIf the memory size is very small, the reserved samples of old classes are apt to be lost, how to obtain a generative classifier?\n•\tIt is not suitable to train the model 5 epochs on i.i.d offline setting for almost datasets.\n",
            "summary_of_the_review": "The motivition is clear, and experiments are comprehensive. \nHowever, some unclear explanations and analyses are existed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}