{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper studies the join-Q value decomposition problem in MARL. Some of the results are interesting, e.g., the True-Global-Max condition and several experiments. However, the majority of the reviews are negative due to the current presentation of the paper. We encourage the authors address all the reviewers' comments and submit a new version to the next conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the problem of value decomposition, which decomposes the joint-Q function into some linear or monotonic transformations of individual factored Q functions for each of the agents. The paper identifies limitations with previous linear/monotonic forms of the decomposition, in particular that the joint greedy action (which matches the greedy joint action for these decompositions) might not match the maximum true Q value. This condition is termed \"True-Global-Max\", and the paper introduces two techniques (inferior target shaping, superior experience replay) to satisfy this condition and improve upon previous suggested Q-value decompositions in literature. The paper walks through a toy matrix game example, a predator-prey experiment, and the Starcraft Multi-Agent challenge environment.",
            "main_review": "Strengths:\nThe problem that is being studied is of high importance, given the popularity of the value decomposition baselines (VDN and QMIX). The True-Global-Max condition is interesting, and the paper proposes relatively simple techniques to satisfy the condition, which is a plus. The experiments are also interesting and the ablation studies in the supplemental are informative. I found the derivations and the writing easy to read, and overall I think the paper can be a good contribution. \n\nSome questions I have:\n- Def 3 and Tab1: the condition says that the joint Q function has converged to the true Q function under policy \\pi. But in Table 1 it seems that none of the Q functions have converged to the true Q function (Tab 1a)? Could you please clarify Def 3 (I'm not sure why we can't just write Eq5 in terms of argmax Q^\\pi = argmax \\pi\n- Is the joint interactive policy \\pi fixed? Or is it updating as the estimated Q function updates? I assume it is updating, but that also seems to mean Eq5 will always hold?\n- For inferior target shaping (ITS), we are penalizing non-optimal actions. The high-level idea is to have a large enough epsilon\\*penalty so that the agents can realize that there is a better alternative action, right? But it is unclear to me why we don't just need that epsilon\\*penalty > 0, it seems that if there is just a little improvement then over time the value iterations will converge to the better alternative?",
            "summary_of_the_review": "Overall I found the problem important, and the paper interesting (both technical and experimental parts) and easy to read. I do have some questions about the framework, and I would be more certain of my recommendation if the authors can help me clarify my confusion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses the problem of monotonic value representations for non-monotonic true values in MARL. The authors perform a theoretical analysis of the conditions under which the greedy decentralized policy coincides with the optimal joint policy and derive a novel update scheme to destabilize incorrect fix-points. They also introduce a priority replay buffer that selects transitions with optimal actions more often and thereby stabilizing learning further. The new algorithm GVR is tested on a matrix game, a predator-prey task and StarCraft II micromanagement tasks. In the latter two GVR appears to significantly outperform decentralized baselines.  \n",
            "main_review": "The authors seem to attempt a deep theoretical analysis of the underlying problem and the good results on predator-prey and StarCraft II indicate that they have found an algorithm that should be definitely published. However, although the reviewer has published on this field, he/she was largely unable to follow the text and the derivations. While the text is technically well written, the underlying concepts are very unclear to the reviewer and the formulas are swapping notations. Finally the developed algorithm remained unclear to the reviewer, who is not sure how statements like (eq.6) can actually be implemented (the pseudo-code in the appendix did not help). In detail:\n\n(1) As far as the reviewer understands the field, the problem is that monotonic value function functions cannot represent non-monotonic true values. Rashid et al. (2020) have shown that putting more importance on optimal actions can resolve this issue and Gupta et al. (2021) have linked this to relative overgeneralization, showing that sampling optimal actions more often allows to learn the optimal policy. Both papers use matrix-games similar to Table 1 to motivate their approach. However, the reviewer was not able to connect these insights with the notion of a \"stable greedy action\". Which are those? Definition 3 uses the formulation \"a stable joint Q value function $Q(s,u)$ which has converged to the true Q value $\\mathcal Q^\\pi(s, u)$ under $\\pi(s, u)$\". Does this mean that $Q(s,u) = \\mathcal Q^\\pi(s, u)$? Is $Q$ factored? How are stability and factoredness related? This seems to be the central concept of GVR, but the reviewer has no idea how to connect it to the presented problem or the proposed solution.\n\n(2) The core of the ITS method seems to be (eq.6). How is it computed in practice, when the agent does not know $\\mathcal Q(s, u)$, which the reviewer assumes corresponds to $\\mathcal Q^\\pi(s, u)$? For the current action this could be approximated with the Bellman operator, but (eq.6) requires  the evaluation of $\\mathcal Q(s, u^*)$ as well. It would be generally helpful to differntiate between $u^*$ and $u_{opt}$ throughout the text, but in particularly here, where the agent does not know $u_{opt}$, but uses $u^*$ in (eq.6), which is confusingly defined as \"optimal\".\n\n(3) The formal derivation is at least sloppy. The authors introduce the Dec-POMDP framework where the policy $\\pi(u|\\tau)$ conditions on the action-observation histories, but then define the utilities $\\mathcal U^a(u^a, o^a)$ as conditioning only on the observations $o^a$, and the values $Q(u, s)$ as conditioning on the state. This does not work (see Oliehoek and Amato, 2016). Either work in a Dec-MDP, which ignores many interesting issues, or consistently condition on the histories $\\tau^a$ and not the state or observation.\n\n(4) The paper uses the term \"reward shaping\", but it seems the authors mean \"reward function\" (as in the matrix game experiments). This is very confusing, as GVR \"shapes\" the value-targets, which one could interpret as actual reward shaping, i.e., (here indirectly) changing the given reward function to improve performance.\n\n(5) (eq.8) insinuates that higher exploration noise will \"destabilize\" non-optimal stable greedy policies. How does this fit with the relative overgeneralization example in Gupta et al. (2021)?\n\n(6) The reviewer did not understand the conclusions the authors drew from the matrix game experiment. Both the setup (why are the rewards for most actions randomized? how is this a fair comparison?) and the results in Figure 2 (are there siginificant differences in return?). While Figure 1b seems to support the theoretical statement, it is unclear why the setup justifies this (e.g. what would happen if your random rewards are from another range than (-20,6)?).",
            "summary_of_the_review": "The paper seems to be onto something that would be of great interest to the community, and the good results indicate that the authors know what they are doing, but the paper is currently almost incomprehensible to this reviewer. Although the authors are welcome to refute or explain the above criticisms, the fact that the reviewer was not able to understand the core idea of the paper during the first reading makes it unlikely that he/she will recommend to publish it in the current form. The reviewer would like to encourage to the authors to rewrite and resubmit the paper, though, as the content seems to be truly significant! \n\n\n**POST-REBUTTAL**\n\nThanks to the authors for their clarifications. However, it was not enough to clarify the paper's main message to the reviewer. The reviewer will therefore not change the score.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduced the optimal consistency for value decomposition methods in multi-agent reinforcement learning (MARL) and suggests True-Global-Max (TGM) condition along with Individual-Global-Max (IGM) proposed by QTRAN to achieve the optimal consistency. Then the authors suggested the greedy-based value representation (GVR) through the inferior target shaping and superior experience replay to ensure the TGM condition.\n",
            "main_review": "Strength\n    The paper introduces new condition TGM for value deposition MARL,and the reviewer thinks that this condition should continue to be discussed in the value deposition method. The experiment for ablation study shows their claim well, and the performance of the proposed method GVR is shown to be outperformed than the state-of-the-art value decomposition methods in various experimental environments.\n\n    Weaknesses\n    1. In the paper, the authors used true $Q$ function and joint $Q$ value function, but reviewer doesn't find the definition of joint $Q$ value function. What is the difference between joint $Q$ value function and true $Q$ function?.\n    \n    2. In the equation (3), the authors expressed the utility function as true $Q$ function. If joint $Q$-value function is used for equation (3) instead of true $Q$ function, then the equation is true. However, the author use true $Q$ function and reviewer doesn't understand how to derive the equation (3). Some more explanations are needed\n    \n    3. In Appendix D and E, the authors prove their claim, but I couldn't follow the process of proof. How can the author express the utility function as equation (13) in appendix D? The reviewer doesn't understand how mapping $f$ occurs. Therefore, more explanation is needed to understand equations 13 and 18\n    \n    4. Finally, the authors madeThe author proposes a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the author can solve the above mentioned questions, the reviewer will raise the score. target of critic $V(s)$ as equation (9), but there is no explanation of why that can happen. You need explanation of reason for target of critic.",
            "summary_of_the_review": "The authors proposed a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the authors can solve the above mentioned questions, the reviewer will raise the score.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to improve value-factorization in cooperative multi-agent reinforcement learning settings under centralized training and decentralized execution (CTDE) framework. The proposed method (GVR) attempts to ensure both Individual-Global-Max(IGM) and True-Global-Max(TGM) conditions without learning a completely expressive (CEC) value function (which can represent all joint- actions). IGM ensures the consistency between joint action selection and local greedy action selection (Individual-Global-Max, IGM), and TGM ensures that the joint-action value function correctly represents optimal values. The GVR method proposed by the authors claims to ensure both IGM and TGM conditions without CEC.\n\nThe main contribution is greedy-value based representation (GVR) which consists of two parts. First, Inferior Target Sampling (ITS) ensures that the greedy joint action is stable such that if the greedy action corresponds to the optimal joint action, then for other joint actions, the gradient will be negative, thereby selectively focusing on representing the unique optimal joint action. If the greedy action does not correspond to the unique optimal joint action, then higher exploration is required in order to destabilize it. However, the lower bound for exploration can become quite high (close to 1) with increasing agents/joint action space, therefore the authors propose a prioritized experience replay buffer. The prioritized buffer assigns higher priority to non-greedy actions which have higher values than a state-based critic.\n\nExperimental results on matrix game, predator-prey and hard/super-hard maps from Starcraft II benchmark show improvements over baseline methods.\n",
            "main_review": "### Strengths:\n\n* The ideas in the paper are novel, and experimental results validate the equations as well. Moreover, the ITS method is based on a proof and can guarantee the stability of greedy actions.  \n* The experimental results on some super hard SMAC maps like 6h_vs_8z (known to be a hard exploration map) look impressive compared to other state of the art methods like QMIX, QPLEX.\n* The paper is well motivated as both IGM and TGM conditions are essential for stability and learning coordinated policies, and can be especially helpful for non-monotonic tasks. \n\n### Weaknesses:\n\n* ITS can keep the greedy action stable which works great if the greedy action is already optimal. However, the authors rely on superior experience replay to destabilize the non-optimal greedy actions. I have some major concerns about this and would request authors to clarify the following:\n\n\t- Looking at Algorithm 1 in appendix, it is unclear how the agent loss is computed. If the Q value of an action $u$ is greater than the greedy action $u^\\star$ i.e. $Q(s,u) > Q(s,u^\\star)$, then it seems like the error would be zero as $Q^{its}(s,u) = Q(s,u)$. Does this rely on the greedy action always being the optimal joint action through higher exploration rate or superior experience replay? \n\n\t- Similarly, the update for the critic is a bit unclear as well. Why is the critic loss only updated for superior samples? It makes sense to consider the samples with $Q(s,u) > V(s)$ or $u = u^\\star$ as superior samples, but in the beginning of the training, the $Q(s,u) < V(s)$ can happen for samples with true optimal joint actions as well, but the critic loss will not be updated for them. \n \n* The paper is a bit hard to follow in certain areas, especially Section 5.1. It would be great if the authors could take some time to add more details to the paper as follows.\n\n\t- In Section 5.1, the authors can make it easier to understand what VDN+ITS variant refers to. \n\t- The paper sometimes refer to $u^\\star$ as optimal greedy  joint action (in Section 4.1), and then refer to $u^\\star$ as just greedy joint action in section 5.1 which is a bit confusing.\n\t- Should Equation (3) have $Q_{ik}$ and $Q_{kj}$ instead of $Q_{ki}$ and $Q_{jk}$?\n\n* One weakness of this paper is that the experimental section is missing  some important baselines. The destabilization of the non-optimal greedy actions through the superior replay buffer is interesting, but the authors should empirically compare against other efficient exploration based MARL approaches such as [1,2,3,4]. Although these approaches also cannot guarantee both IGM and TGM, they can potentially also alleviate the destability associated with stable non-optimal greedy actions via different exploration mechanisms. \n\n* I have a major concern about the performance of WQMIX reported on the StarCraft II benchmark map 6h_vs_8z. The reported performance is not consistent with that reported in the WQMIX paper (see Figure 4 in [5]). WQMIX paper also uses the same exploration schedule as this paper i.e. $\\epsilon$ annealed over 1e6 time steps. The original paper reports a median test win rate of around 80% with optimistic weighting but this paper only reports around 20%. Also, there seems to be only a single seed used for evaluating GVR on 6h_vs_8z. \n\n* The authors seem to have skipped a few super hard SMAC maps such as corridor, 27m_vs_30m and 3s5z_vs_3s6z which are important to determine the scalability of their approach with more agents.\n\n* The code is missing from the submission for reproducibility.\n\n### References:\n\n[1] Wang, Tonghan, et al. \"Influence-based multi-agent exploration.\" arXiv preprint arXiv:1910.05512 (2019).\n\n[2] Liu, Iou-Jen, et al. \"Cooperative exploration for multi-agent deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Mahajan, Anuj, et al. \"Maven: Multi-agent variational exploration.\" arXiv preprint arXiv:1910.07483 (2019).\n\n[4] Gupta, Tarun, et al. \"Uneven: Universal value exploration for multi-agent reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[5] Rashid, Tabish, et al. \"Weighted qmix: Expanding monotonic value function factorisation.\" arXiv e-prints (2020)",
            "summary_of_the_review": "The paper is well motivated so as to ensure both IGM and TGM conditions, and therefore stable convergence in MARL. The ideas in the paper are novel, and ITS is based on a proof and can guarantee the stability of greedy actions.  However, I am not convinced that the proposed superior experience replay can destabilize the non-optimal greedy actions.  I have asked a clarification question about this and will improve my score based on a satisfactory response. Other than this, although the empirical results are good, the authors have missing baselines from the experimental evaluations, and missing experiments with respect to proving the scalability of GVR with more agents. Finally, I have concerns regarding evaluations reported on 6h_vs_8z. Therefore, I donâ€™t think the paper is ready for publication as is.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}