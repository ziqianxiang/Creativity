{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a technique to perform data imputation with normalizing flow defining a joint density between observed and unobserved variables. This is achieved by introducing a variational posterior over the missing variables which is parametrized in terms of the original model by using the Schur complement of the model's Jacobian over the hidden variables.\nThe idea is interesting, but the proposed setup is quite complex and the experimental results are not conclusive. The quality of the results shown can likely be matched or surpassed with much simpler techniques. The paper would substantially benefit from more detailed experiments."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a variant of flow, VISCOS flow, utilizing the Schur complement. This flow model is applied to the in-painting task, which estimates the unobserved feature variables given the observed features. Given this in-painting task, the inference requires the modeling on the latent variable, which results in the ELBO derivation on such latent variables as variational variables.",
            "main_review": "1. \nThis paper provides an interesting utilization of the Schur complement to model the dimensions of observed and unobserved features.\nI cannot understand the notation of $G^{HH}(x)$ in Eq 2 because $G$ is defined on $y$. \n\n2.\nThis paper derives the gradient of the suggested ELBO formulation. This derivation results in some theoretic suggestions, such as the corrected gradient estimators for the log-absolute determinants, i.e. CLADE.\n\n3.\nIt would have been better to show an illustrative figure on the suggested Flow structure. Particularly, the structure is decomposed into the observed and the unobserved variables; and this decomposition leads to the link to the Schur decomposition. It is confusing which Jacobian sub-matrix is related to which variable.\n\n4.\nThe in-painting task is compared from the perspective of classification accuracy, FID, and RMSE. There are some other performance standards, i.e. SSIM, PSNR. I wonder whether these metrics could be used or not.[1]\n\n5.\nCurrently, only MNIST is being used for the test. Previous works utilize more diverse datasets, such as places2, celeba, streetview, etc.[2]\n\n6.\nBenchmark baseline models are too few. if you search inpainting flow models, there will be plenty of recent flow models for the same task.\n\n\n[1] Xu, Rui, et al. \"Deep flow-guided video inpainting.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Ren, Yurui, et al. \"Structureflow: Image inpainting via structure-aware appearance flow.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\n\n\n",
            "summary_of_the_review": "This paper provides an interesting utilization of Schur complement in the in-painting task. However, the evaluation is very weak. \n\n////////////////////\n\nI read the rebuttal from the authors, and I maintain my evaluation. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This is not applicable to the ethical considerations",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to sample from conditional distributions using a pre-trained flow based method for applications in problems that have missing features. The proposed method utilizes a lower bound on the log-probability of the conditional distribution (conditioned on observed features) using the Schur complement. ",
            "main_review": "The paper is relatively well written although I believe that parts of writing in the paper can be made more precise that will help the overall quality of the paper. The problem formulation is clear and the proposed solution is interesting as well as explored in detail. \n\nHowever, the paper suffers from several weaknesses:\n\n1) Pre-trained flows: The paper does not train a normalizing flow for data that may itself have missing features/data but assumes access to a pre-trained flow and uses it for sampling when some features are missing. I find this set-up a bit unnatural since I'd expect that for application where such a conditional sampling might be required, the training data for learning any probability model will itself have missing features.\n\n2) Restricted flow architectures: Secondly, the proposed solution works for only a restricted class of flow architectures ie those that have residual flow (or iResNet) like structures. I think this is a major limitation of the method and there is no clear discussion or solution of how the proposed method can be extended for a general class of flow models.\n\n3) Comparison to ACFlow and MCFlow: The authors in the introduction and as part of the motivation highlight why these models are not well suited for the problem they consider. They mainly highlighted computational cost etc as a major roadblock to deploying such models. However, unfortunately, this claim has not been substantiated with any justification. I believe clear empirical analysis is required to include such a claim and to highlight why VISCOS flows a re a better alrenative to the already proposed models in the literature.\n\n4) Experiments: I also believe that the present experiments are insufficient to evaluate the performance of VISCOS flows. Particularly, condiering the restrictive nature of the flow architectures that VISCOS flows use, I think the present experiments do not fully reveal the advantages of different methods. Precisely, since the experimental results obtained for methods like PL-MCMC, MFlow use a different flow architecture than for the results for VISCOS flows, I believe the experimental comparisons are not fair and sufficient enough to draw any meaningful conclusions. Furthermore, the results themselves are mostly mixed and due to the lack of any error bars for these results it is hard to conclude that nay method may be better than the others. I'd recommend the authors to atleast perform experiments that uses the same flow architecture to make the comparisons fair and if possible include error bars for the results.",
            "summary_of_the_review": "The paper studies an interesting problem for flows and proposes a nice solution. However, in the current form, the paper suffers from many weaknesses surrounding the experimental setup and being valid for only a restrictive class of flow models.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a technique for performing data imputation using pre-trained normalizing flows. They do so by fitting a variational distribution over a subset of the 'base' variables of the normalizing flow. Along with the observations, a sample from this variational distribution is sufficient to fully specify a sample from the normalizing flow model. The authors present techniques for constructing samples in this way, and computing the derivatives required to optimise the variational distribution.",
            "main_review": "### Strengths:\n- AFAIK this is a novel approach to parameterising a variational distribution for inference within a normalising flow.\n- I think that parameterising variational distributions for normalizing flows in this way is an interesting avenue to explore, and the derivation of this method and its gradient estimators is clearly non-trivial.\n\n### Weaknesses:\n- More comparisons to (or at least discussion of) related work should be included. [1] also perform imputation using variational inference in normalizing flows (although with a relaxed likelihood), and present (at least visually) better results than this paper. [2] present a conditional VAE that can be trained with incomplete training data.\n- This approach seems to be fundamentally difficult to scale to higher-dimensional datasets (due to the cost of solving the equality constraint and estimating the gradient). Even for the moderately high-dimensional CIFAR10 and MNIST datasets experimented on, I suspect that alternative techniques (like [1,2]) work better. A much better case needs to be made for why this method should be preferred.\n\n### Minor\n- last 3 lines of first paragraph of Section 2.4 - This sentence is confusing. An ELBO of zero is not meaningful for continuous data, since the likelihood may be above or below zero. And it is not clear what is meant by \"whenever $x^O$ is independent of $x^H$\": should they be independent under the prior, or under the posterior conditioned on observations?\n- I think $\\widetilde{\\mathbf{y}^H}$ is used before being defined in Algorithm 1: it is used on the RHS of line 8 but has no assigned value until after line 8 is executed.\n\n[1] Whang, Jay, Erik Lindgren, and Alex Dimakis. \"Composing Normalizing Flows for Inverse Problems.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2] Ma, Chao, et al. \"Eddi: Efficient dynamic discovery of high-value information with partial vae.\" arXiv preprint arXiv:1809.11142 (2018).",
            "summary_of_the_review": "While I appreciate the novel idea  and derivations, it is not clear to me that there are any use cases where this method should be preferred over the alternatives. Given the lack of a convincing case for its usefulness, I am recommending reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}