{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper performs an empirical evaluation of deterministic methods for the quantification of epistemic uncertainty.  There is no new algorithm.  The main contribution is the empirical evaluation.  This empirical evaluation will be useful for the community. It is an independent evaluation that casts some doubts on the calibration of several existing deterministic techniques, which will spur additional research. However, the paper is not well written. As pointed out by the reviewers, the paper does not provide much background. It refers to many  concepts without defining them. The concepts are not new (references are provided for each concept), but since the paper does not describe any new technique it should do a good job at explaining those concepts. The authors added some explanations in the supplementary material, but some of those explanations should really be in the main paper.  The most important issue with the paper is that it does not explain why the deterministic techniques do not seem to be well calibrated. The authors added a \"theoretical justification\" in section 6.1, but it amounts to saying that deterministic methods make a point estimate, which is too general to explain much. An important factor for proper generalization and calibration is the inductive bias of the model. At the end of the day, if we generate data from a model, then that model will be better calibrated than the other models. So a discussion of the inductive bias of each model and how this inductive bias relates to the properties of each dataset would have been much more insightful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper analyzes deterministic uncertainty estimation methods, in terms of their calibration under distributional shift. They provide a literature review, propose a quantification metric, and compare performance of several uncertainty estimation methods.",
            "main_review": "Title of the paper seems too broad and not representative of the content. “Practicality” is a very general term. The analysis and findings seem rather specific. It mainly seems to be that ”DUMs are not well calibrated under continuous distributional shifts”.\n\nA comprehensive review of DUMs could be of great benefit, if done correctly. The area is a bit non-structured and the evaluations are not standardized. However, the current version of this paper falls short in helping in both aspects.   \n\nLiterature review: \nThe explanations are very surface level and non-comprehensive. As an example, the literature review on the “discriminative methods” summarizes every approach in one very vague sentence, requiring the reader to refer to the papers to actually be able to understand. Given that the work is mostly a review of other methods, it should at least serve as a good survey, for a non expert reader. The current version of the manuscript falls short as is.\n\nBesides that, the literature review of uncertainty estimation is incomplete (see [1, 2, 3, 4, 5] as examples). MIMO approaches [2, 3] are similar to deep ensembles but without the computational and memory requirements. One may argue that these approaches fall much closer to DUMs as opposed to Ensemble methods in terms of efficiency (memory & computation). Especially given that they are mostly image based, they are directly applicable here. They’re not even mentioned in the literature review.\n\nThe terms out-of-distribution detection, epistemic uncertainty estimation, aleatoric uncertainty estimation, and calibration. are not very well defined in the community when it comes to their quantification. Especially OOD detection (maybe the most well defined one) and epistemic uncertainty estimation, are often used interchangeably in many papers (debatable if correctly or incorrectly).  \nWhen it comes to adding synthetic corruption to the image (input), how is that not aleatoric uncertainty? One may argue that OOD detection is a better proxy for epistemic (although not perfect), and calibration on corruptions (noise in input) is better for aleatoric uncertainty. One example for each: [1] for using input variation / corruption for aleatoric uncertainty evaluation. [4] for using OOD detection as a proxy for epistemic uncertainty estimation. If that is the case, why should models designed for epistemic uncertainty estimation, be the best candidates for aleatoric uncertainty estimation anyway. \n\nIf the authors disagree with these conventions they should at least provide a comprehensive justification on why they think things should be different, and they should propose clear evaluation protocols for each type of uncertainty estimation. \n\n\n\n[1] Modeling Uncertainty With Hedged Instance Embedding\n\n[2] MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks\n\n[3] Training Independent Subnetworks For Robust Prediction\n\n[4] Sketching Curvature for Efficient Out-of-Distribution Detection for Deep Neural Networks\n\n[5] Epistemic Neural Networks\n",
            "summary_of_the_review": "Overall I think the experiments of the paper are insightful and have merit, but the presentation, positioning, and claims could be significantly improved. Maybe if the paper’s focus, description, and claims were concentrated around “evaluating calibration of deterministic epistemic uncertainty models under distribution shift”, it would be more representative of the experiments, and it could become a more conclusive work. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is an analysis and benchmark comparison of deterministic uncertainty quantification models under dataset shift. The authors evaluate DUM methods, and show that while they work well in academic datasets (mostly CIFAR10/100), they seem to fail in out of distribution detection on a more realistic setting (Semantic segmentation on CityScapes and CARLA simulated images) and in distribution shift settings (corrupting input images), particularly in terms of lower quality uncertainty calibration and out of distribution detection performance, which has practical implications for the use of these algorithms.\n",
            "main_review": "Strengths\n\n- The motivation of the paper is strong, these new uncertainty quantification methods (DUMs) can raise some doubts as they do not do the usual bayesian model averaging (like ensembles or dropout/dropconnect), and as far as I know, there is no previous comparison of all these methods, particularly in more realistic settings. There is the important question if these methods work well in more complex settings.\n- The authors propose a taxonomy on DUM method which I find useful for the future and very positive. The survey and description in the first pages of the paper is also very useful for the community.\n- The evaluation seems to be correct, the dataset selection for image classification and segmentation experiments is excellent, and the metric selection is good, except for the rAULC metric which is not that common, but seems appropriate as not all evaluated methods produce probabilities. The authors argue why standard calibration errors cannot be used, but I think it could be an improvement to normalize confidence scores produced by DUMs to be able to use calibration errors, I do not see this as a big issue more than using a standard metric. For example gradient uncertainty can be normalized to produce scores in [0, 1] and ECE can be computed for these predictions.\n- The supplementary material contains many additional results of interest, like variations of DUM hyper-parameter effects, justification about issues with intermediate activations, full results for corrupted versions of all datasets, and OOD detection without dataset shift (standard OOD detection benchmarks), showing that some DUMs work better in some datasets, and ensembles on SVHN.\n- The additional segmentation results in the supplementary are very interesting, in particular Fig 7 and Fig 8, as it shows how MIR produces radically different uncertainty as seen in the heatmaps of uncertainty (last column), in comparison with other methods with mostly produce high uncertainty in the borders between regions, acting as a kind of edge detector, and this could lead to additional insights on why some methods fail.\n- There are clear conclusions from this paper, that DUM methods do not perform OOD at acceptable levels for both classification and segmentation (except for DCU), and their calibration also suffers considerably. All of this in distribution shift settings.\n- Figure 1 shows that DUMs suffer from the same issue with dataset distribution shift, where OOD/calibration performance suffers when corrupting input images, same as Ovadia et al. 2019.\n- The paper is overall well written, and I had no trouble reading it. \n- I find that the analysis in this paper is original and novel, there is small epsilon contribution to the state of the art, which should enable future research on DUMs.\n\nWeaknesses\n\n- A strong theoretical foundation on why these methods fail would strengthen the paper. The authors perform an analysis of regularization strength on MNIST (Easy but not many conclusions can be drawn from this dataset) and about distributions of intermediate features, but I think a full theoretical paper on this topic would be required.\n\nMinor Issues\n\n- It is possible that the paper would benefit from a \"page 2 figure\" to introduce the reader. There is no space, but I think putting Table 1 on top of the second page would serve this purpose.\n- In Figure 1, I think axis tick labels can be reduced a bit in size or even factored across different plots (putting the tick labels in the first plots on the right and bottom rows) to save space and make the plots a bit bigger, for readability.\n- One DUM that I think was left out is gradient-based uncertainty, for this please see \" Classification uncertainty of deep neural networks based on gradient information\" Oberdiek et al. 2018. I am not sure if adding it would change the conclusions, but maybe it can be included in the taxonomy.\n- I totally agree with the authors that DUQ could diverge on CIFAR100-C, I have experienced this using DUQ on small datasets. I suggest that the authors remove the DUQ centroid learning algorithm (the running average over input features) and train the centroids using gradient descent. This will probably solve the issue.\n- For readability, in Figure 6 of the supplementary, it is better to include the metric in the label of the y axis. Same for Figures 11 to 20, the reader will be pleased to find metrics and information in the x/y axis labels.",
            "summary_of_the_review": "I believe that this is a good paper, with important conclusions and insights for the uncertainty quantification community, it is correctly evaluated, and should be accepted. There are no major issues to be dealt with.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper mainly summarizes and evaluates the existing methods for estimating epistemic uncertainty through a single pass of the neural networks. The authors categorize deterministic uncertainty methods (DUMs) based on how latent representation is learned under regularization and how uncertainty is quantified. For evaluating DUMs, they provide some analyses on the uncertainty calibration performance under different-level distribution shifts for image classification and semantic segmentation. Specifically, they demonstrate that DUMs cannot generate well-calibrated uncertainty under distribution shifts compared to the MC-dropout and Ensemble methods.",
            "main_review": "Strengths: \n\n(1)\tThe paper is well-organized. It first introduces a summary of DUMs and then provides experiments on both image classification tasks and real-world applications, i.e., semantic segmentation.\n\n(2)\tThe experiments are systematic, including all the necessary evaluation metrics and different levels of distribution shifts.\n\nWeaknesses:\n\nFor the method:\n\n(1)\tSome important deterministic uncertainty methods such as the Dirichlet-based methods [1,2,3] are missing in the paper.  Moreover, the authors cite the Posterior network [2] work as one of the DUMs but did not evaluate it.  In addition, [4], that performed the evaluation of the Dirichlet-based methods , also considered the uncertainty robustness under distribution shifts. The authors should provide a discussion and comparison of their work with [4].\n\n(2) The authors should discuss in the body of the papers the about underlying reasons for the issues associated with the DUM methods and even propose possible solutions to address or avoid these problems.\n\n(3)\tThe authors should discuss each deterministic uncertainty method in terms of the strengths/weaknesses of each method and the possible assumptions. Some necessary equations should also be provided. Then, we can have a clear mind about which method to choose for experiments\n\n(4)\tThe authors mainly evaluate the uncertainty robustness under distributional shifts. However, the author should also provide some discussion about the OOD detection results. Could DUMs perform as well as Ensemble and MC-dropout when there are no distributional shifts?\n\nFor the experiments\n\n(1)\tThe experiment results lack the necessary analysis. For example, why do most DUMs have much worse calibration performance than softmax? \n\n(2) 5 DUMs are considered for image classification while only 3 DUMs are considered for semantic segmentation. The performance of DUMs may not be representative of semantic segmentation.\n\n(3)\tThe experiments did not consider the regression tasks.\n\n(4)\tFigures 1,2,3,4 are very confusing. It is hard to distinguish different colors from figures. The authors may use different curves when plotting the results to distinguish DUMs with Ensemble and MC-dropout.\n\n[1] Malinin, Andrey, and Mark Gales. \"Predictive uncertainty estimation via prior networks.\" arXiv preprint arXiv:1802.10501 (2018).\n\n[2] Charpentier, Bertrand, Daniel Zügner, and Stephan Günnemann. \"Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts.\" arXiv preprint arXiv:2006.09239 (2020).\n\n[3] Malinin, Andrey, Bruno Mlodozeniec, and Mark Gales. \"Ensemble distribution distillation.\" arXiv preprint arXiv:1905.00076 (2019).\n\n[4] Kopetzki, Anna-Kathrin, et al. \"Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?.\" International Conference on Machine Learning. PMLR, 2021.\n",
            "summary_of_the_review": "Overall, this paper provides the evaluation for some determinist uncertainty estimation methods focusing on the robustness of uncertainty under distributional shifts. However, the paper should include more related methods in comparison with a discussion about the pros/cons and possible assumptions for each method. The experiment results should also be analyzed in detail. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThis paper reviews and compares a class of epistemic uncertainty estimation methods that avoid sampling (hence called deterministic) as well as multiple models (like ensembles) because of their memory footprint.",
            "main_review": "The reasons invoked to include some of the cited DUM approaches are not very convincing, since the point of a benchmark is to evaluate all the relevant approaches according to the same yardstick. Since there is no methodological novelty in this paper, I would suggest to open up the comparison to a slightly larger set of methods than what was done here.\n\nNonetheless, the provided comparisons can be useful for future research aiming at proposing novel approaches and possibly avoiding all the possible comparisons, focusing instead on the methods which seem to perform well in this paper. These comparisons also point to DUM approaches being generally insufficiently well calibrated (especially those relying on the distribution of hidden representations).\n\nThere is another deterministic estimator of epistemic uncertainty that could have been considered, of a very different nature (but only applicable with one can have an estimator of aleatoric uncertainty, e.g. for pairs of examples with the same x but different y): DEUP (Lahlou et al, 2021). They train a 2nd network to predict the per-example out-of-sample error of the main predictor.",
            "summary_of_the_review": "\nThis paper compares experimentally a number of epistemic uncertainty estimators within a fairly limited but interesting class of methods. This could be useful to practitioners of the field. A broader set of comparisons could improve the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}