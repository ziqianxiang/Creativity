{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a VAE-based generative model of 3D point-clouds inspired by SPAIR that can do unsupervised segmentation, named SPAIR3D. The model uses both global and local latent variables to encode global scene structure as well as individual objects.\n\nThe proposed model is relatively complex, but the presentation is overall clear. \n\nExperimental results on simple synthetic datasets look promising. However, one might argue that for these simple tasks a direct application of a simpler mixture of VAEs (such as IODINE) might be sufficient, so it would be informative to make a direct comparison between these methods and/or show results on a problem clearly out of the scope of these simpler methods (e.g. with high imbalance in the point clouds)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is a pioneering work that extends the success of unsupervised object-centric learning in 2D images to 3D scene point clouds. The framework could detect and segment multiple 3D objects from a 3D scene point cloud through a decompose-by-reconstruct strategy in an unsupervised manner. \n\nTheir variational training pipeline utilizes a VAE-based generative model that jointly considers the global 3D background and local individual objects, and a novel chamfer mixture loss is proposed for irregular 3D point clouds.  The method is validated on two synthetic datasets, and the performance is demonstrated to be close to a supervised SOTA method.\n",
            "main_review": "\nStrengths\n[novelty]  \n1. According to my knowledge, this is the first work that explores variational generative methods for scene-level 3D point cloud understanding; the learnt representation could be used for unsupervised 3D  instance segmentation, or scene decomposition, while existing methods like PointGroup[Jiang et al.] or 3D-BoNet[Yang et al.] requires ground truth labels to cluster neighborhood points or refine from the detected bounding boxes; \n\n2. The proposed mixture chamfer loss is novel, which treats each point as a Gaussian distribution and turns the original Chamfer distance better for variational inference. It also utilizes a weighting mechanism when deciding each point’s corresponding glimpse, this formulation is critical for closed loop mask prediction;\n\n[effectiveness] \n1. The detection-inspired local voxel attention mechanism makes it possible to handle arbitrary number of objects in the 3D scene, handling well with 2-12 objects from their experiments;\n\n2. The proposed method achieves performance that is on a par with supervised SOTA method on two synthetic scene point cloud datasets, including challenging cases like almost touch surfaces;\n\n[completeness]\nThe author provides both quantitative and qualitative results, with ablation studies on reconstruction point number, voxel size, etc. They also mentioned its drawbacks like handling large objects that are much bigger than the voxel size, mixed objects inside one voxel. \n\n\nWeakness\nThough this work is sound overall, after going through the details carefully in the paper, here I’d like to point out several concerns/issues below. I have examined the key claims over the technical contributions, together with the experimental settings.\n\n1. The chamfer mixture distance loss seems to be critical for the variational model training,  but the additional per-point probability modeling and bidirectional closest point finding, plus multiple glimpse proposals seem to make the loss computation very expensive, does the author have rough estimation of the increased computation compared to the original Chamfer distance loss? One quick related question, is it possible for the reconstruction to come in the form of regular voxels? In this way, regular one-to-one correspondence could be established naturally, without the expensive argmax operation, or searching for the closest pairs;\n\n2. Current experiments are performed on synthetic complete scene point clouds, which is still quite a toy setting. On these datasets, there is strong bias that a flat and clean desk/room background is presented, would it be possible that the model simply learns to discard the flat plane when segmenting the object instead of learning good object-centric representations? A simple way to verify that is trying to tilt the point cloud data, so the desk plane would not be flat. Also I guess the proposed method might be limited when handling partial 3D point clouds, especially for the point cloud reconstruction supervision, is that expected?\n\n3. Following the previous question, is it possible for the proposed method to test on S3DIS dataset as well? This is a common benchmark for 3D instance segmentation and doesn’t have quite challenging partial data. Even though there might be challenging cases due to various object scales, the author should be able to find a reasonable voxel size for glimpse generation. \n\n4. The author is expected to provide more training details, like each glimpse may contain a different number of points, in the glimpse encoder and decoder, are different glimpses processed in batch or sequentially? Is there any difference for loss weights when training global VAE and per-glimpse VAE?\n\n5. The author needs to be aware of some recent works in this direction, like this ICCV 2021 paper: Unsupervised Point Cloud Object Co-Segmentation by Co-Contrastive Learning and Mutual Attention Sampling[Yang et al.]\n\n6. Could the author leverage more intuition about why this method could learn to develop a concept of visual entities in an unsupervised way? It is still amazing the proposed pipeline achieves such a good performance even on the toy datasets. But if the per-glimpse reconstruction generates points containing background points, the loss would not change, but then there is no way that  model could tell the object mask. And why do we need this global VAE for background when each glimpse module already handles the target  foreground objects?\n\nMinor comments about weakness:\nWhat is the foreground alpha in figure 2 (i) and (j). Besides the glimpse alpha and scene layout reconstruction, Could the author also show more reconstruction visualization in individual glimpses?\nIn the title, there is a word ‘Invariant', which I found quite interesting. What does this ‘invariant’ mean exactly?\nPage 2. Section 3, 2nd paragraph should be ‘However, point cloud data is ...’\n\n",
            "summary_of_the_review": "This paper is a valuable pioneering work on unsupervised 3D scene segmentation, with a full variational training pipeline built up for irregular 3D point cloud data. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of unsupervised object-centric learning in the context of point clouds. Specifically, the goal is to learn a generative model of 3D objects from a collection of untextured point clouds of multi-object scenes, and this is achieved via a VAE-based generative framework.\n\nThis task extends the setup of prior work on image-based 2D object-centric learning to 3D. To be able to handle irregular structures of point clouds, a novel Chamfer Mixture Loss is proposed as reconstruction loss, which essentially extends the Chamfer distance with probability mixture modeling. The resulting model seems to work well on simple scenes with small isolated objects on a plane.",
            "main_review": "### Strengths\n#### S1 - Important problem\n- Unsupervised 3D object learning is an important important problem. This paper studies a specific case of it, which is detecting 3D objects in point clouds.\n\n#### S2 - Carefully designed framework\n- The proposed method is carefully designed for handling the irregular structure of point clouds in a VAE framework. Specifically, it adopts a likelihood-based extension of Chamfer distance as a reconstruction loss for point clouds, and incorporates several recent architectures, including pointCNN and pointGNN.\n\n### Weaknesses\n#### W1 - Over-complicated method on simplistic datasets\n- A major concern with this paper is the simplicity of the evaluation experiments. The datasets used in this paper appear extremely simple, with almost perfect shapes, uncluttered scenes and clean, planar background, which makes task of foreground object detection seem rather trivial, and hence the learning of 3D object representations. Moreover, compared to the previous studies in 2D images, the 3D scenes here intrinsically alleviate challenges like occlusion and illumination effects. In contrast to this seemingly trivial task, the proposed method appears overly complicated.\n- Also, the failure examples shown in appendix (Fig. 15 and Fig. 16) suggests the model is still unable to handle scenes where objects are closer.\n- I would suggest increasing the diversity of the 3D scenes to verify the effectiveness of the proposed method, eg, using more complicated geometries for the background, varying the sizes and point densities of the objects, introducing noise to the point clouds etc.\n",
            "summary_of_the_review": "Overall, I think the problem is interesting, and the proposed framework is technical sound, but the results presented on the simplistic datasets cannot sufficiently verify the effectiveness of the method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a framework, SPAIR3D, to decompose a 3D point cloud into several objects (point cloud segmentation). The framework is unsupervised, driven by a Chamfer Mixture Loss. Two training datasets, UOR and UOT are rendered by Unity to train and evaluate the proposed method. In experiments, the method is compared with a supervised method PointGroup, where PointGroup outperforms SPAIR3D, but the performances are comparable. ",
            "main_review": "The paper proposes an unsupervised object-level segmentation framework from point cloud input. The framework is unsupervised, driven by a Chamfer Mixture Loss. Two training datasets, UOR and UOT are rendered by Unity to train and evaluate the proposed method. In experiments, the method is compared with a supervised method PointGroup, where PointGroup outperforms SPAIR3D, but the performances are comparable. \nThe unsupervised method and the new loss are good and novel, but I have a few confusions below:\n1. Firstly the problem setting is a little confusing, since the framework is unsupervised, I thought that the training data would be captured point clouds or those from other various datasets, since GT is not needed. However, the authors still make a lot of efforts to render UOR and UOT. If so, the GT is easy to get, why do we need an unsupervised framework for these synthetic datasets? On the other hand, the synthesized datasets look relatively simple and objects are placed sparsely and at similar sizes. I would like to see results on more challenging data, and I would like to hear the reason why the training is not performed on real point clouds or existing datasets. \n2. In a real scene, objects are often stacked. Would SPAIR3D work and how is it work? \n3. Typos: title of 3.1 \"Local\". \n",
            "summary_of_the_review": "The idea of unsupervised point cloud segmentation is cool since point clouds are hard to annotate, and they are easy to capture or collect. However, the dataset and evaluations are not quite reasonable to me, as described above. Thus I give a borderline reject for now. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "Evaluating only one dataset (the synthetic dataset rendered by this paper) is not fair enough. The evaluation dataset would better be different from the training dataset. Comparison with only one prior work is also not enough. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles unsupervised 3D scene decomposition/object discovery from point clouds. It proposes SPAIR3D, inspired by the 2D counterpart SPAIR, to factorize a 3D point cloud into a spatial object-centric mixture model. It derives the Chamfer Mixture Loss to fit 3D reconstruction into the variational training pipeline. It introduces two customized point-cloud datasets, and compares with a supervised baseline PointGroup.",
            "main_review": "# Strengths\n- The overall writing is clear.\n- The paper is the first to extend 2D object-centric representation learning to 3D point clouds. The Chamfer Mixture Loss is a reasonable adaptation for variational training.\n\n# Weaknesses/Questions\n1. Lack of baselines. Although there are no direct 3D object-centric representation learning baselines, 3D object discovery is not an unexplored field. Apart from \"Unsupervised Discovery of Repetitive Objects\", a common baseline in 3D object detection is to cluster/group 3D points according to their spatial relationship. Concretely, the authors can compare with a baseline, where the background/scene layout (planes in the paper's datasets) is first detected by a RANSAC and the rest objects are separated by DBSCAN/MeanShift (hyperparameters need to be tuned/searched). Although such a baseline is not scalable to more complicated scenes, SPAIR3D is not shown to have stronger scalability currently. Besides, the authors can introduce some 3D over-segementation baselines (e.g., [1], [2])\n\n2. How important is VAE in the object-centric representation learning framework? It might be a general question (also for 2D counterparts). From some experience, I suspect that the prior of model structure, like sliding window in 2D/3D grid and relative center prediction (techniques studied in 2D/3D detection), is the most important. VAE, especially the probabilistic model, mainly provides a sampling procedure during training, which might be considered as a search process (Monto-Carlo sampling). An easy way to check is to remove KL divergence terms between the prior and posterior probability of latent codes, and only maximize posterior probabilities of observations.\n\n3. I am a little confused about the claim \"Unfortunately, the Chamfer distance does not fit into the variational-inference framework\". There is no detailed explanation about it. From my understanding, the Chamfer Mixture Loss just introduces additional variance term and changes the distance to probability, since the variance term is a hyperparameter rather than per-point prediction and the likelihood is only based on the best/maximum matched point.\n\n4. As mentioned in Sec 4.1, the point cloud is fused from 10 different views. Can SPAIR3D work on partial 3D scenes? Theoretically, current SPAIR3D may have difficulty since the Chamfer Mixture Loss ignores visibility and it becomes much harder to recognize repetitive objects.\n\n# References\n1. Toward better boundary preserved supervoxel segmentation for 3D point clouds\n2. Superpoint Network for Point Cloud Oversegmentation",
            "summary_of_the_review": "To the best of my knowledge, the paper is the first to tackle 3D object-centric scene decomposition (unsupervised object discovery). It is well written and reasonably extends SPAIR to 3D scenes with new challenges only in 3D cases. However, the baselines are not carefully chosen, and the scalability of the proposed SPAIR3D is questionable. Overall, I think the paper is slightly above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a VAE-based unsupervised generative model, to achieve 3D object-centric learning as well as 3D scene decomposition. The method divides a 3D scene into voxel grids and generates latent features to reconstruct an original point cloud. A chamfer Mixture loss is proposed to train the model. ",
            "main_review": "Strengths：\n1. This paper proposes a novel Chamfer Mixture loss formulated in a probabilistic framework. \n2. The proposed method achieves astonishing performance comparable to a supervised baseline on two simple datasets.\n\nWeakness：\n1.\tThis paper is built on the SPAIR framework and focuses on point cloud data, which is somehow incremental.\n2.\tThere is no ablation study to validate the effectiveness of the proposed components and the loss. \n3.\tIt is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples.\n4.\tIt is unclear how the method can work and decompose a scene into different objects. I did not see how Chamfer Mixture loss can achieve this goal. More explanation should go here. \n",
            "summary_of_the_review": "The idea to decompose a scene in an unsupervised way is novel. However, the proposed method is mainly based on SPAIR and the only technical contribution seems to be Chamfer Mixture loss. It is unclear what is the advantage of this loss design over traditional chamfer distance and how it can help scene decomposition. Moreover, I did not see further analysis on different proposed components of the method and inspiring conclusion for the community. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}