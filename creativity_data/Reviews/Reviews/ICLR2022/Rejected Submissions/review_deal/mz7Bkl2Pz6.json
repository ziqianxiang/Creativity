{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers the global convergence and stability of SGD for non-convex setting. The main contribution of the work seems to be to remove uniform bounded assumption on the noise, and to relax the global Holder assumption typically made. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails.  The authors establish that SGD’s iterates will either globally converge to a stationary point or diverge  and hence tehir result exclude limit cycle or oscillation. Under a more restrictive assumption on the joint behavior of the non-convexity and noise model they also show that the objective function cannot diverge, even if the iterates diverge.\n\nThe reviewers are on the fence with this paper. While they agree that the paper is interesting, they only give it a score of weak accept (subsequent to rebuttal as well). One of the qualms is that while the authors claim the result helps show success of SGD in more natural non-convex problems, they don’t provide realistic examples supporting their claim. Further, while the extension to holder smoothness assumption while is indeed interesting, unless practical significance is shown via examples, the result is not that exciting.\n\nFrom my point of view and reading, while the reviews are not extensive, i do not disagree with reviewers sentiment. Technically the paper is strong but there is a unanimous lack of strong excitement for the paper amongst reviewers. While there is this lack of more enthusiasm, given the number of strong submissions this year, I am tending towards a reject."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors consider the global convergence and stability of stochastic gradient descent in a fairly general non-convex setting. They are able to remove the often-assumed unrealistic uniform bounded assumption on the noise, and also relax the global Holder assumption in the literature. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails. Their global convergence says that under some relatively weak assumptions, SGD either diverges or the objective converges to a finite random variable and the gradient converges to zero. This excludes the bad outcomes, e.g. limit cycle or oscillation. Their stability result says that under a stronger assumption, SGD's objective converges to a finite random variable and the gradient converges to zero with probability one. ",
            "main_review": "The strength of the paper is that it studies the global convergence and stability of SGD under a fairly general non-convex setting. It is nice that unrealistic uniform boundedness noise assumption often used in the literature can be removed. One weakness is that some of the assumptions is less intuitive, and needs more explanations, discussions and justifications, especially Assumption 5. Also, the authors should perhaps provide some discussions in the contributions section what are the main technical difficulties and challenges that they overcome in order to achieve these results, and whether there is any technical novelty here. ",
            "summary_of_the_review": "(1) I'm a little bit confused with Assumption 4. Would it be enough to simply assume that $\\mathbb{E}\\Vert\\dot{f}(\\theta,X)\\Vert_{2}^{1+\\alpha}$ is finite for any $\\theta$? Because it seems that if so, one can always find a function $G(\\theta)$ to upper bound this? Or your assumption is stronger than it seems? I understand that $G(\\theta)$ appears in (2) and Assumption 5. But it is not clear to me why you need this in Assumption 4, and am also wondering in mathematics, is there some name for the smallest upper semi-continuous function that upper bounds? Another comment I have is that in Assumption 4, you already taken the expectation. So my guess is that most likely, $\\mathbb{E}\\Vert\\dot{f}(\\theta,X)\\Vert_{2}^{1+\\alpha}$ is not just upper-semi-continuous but actually continuous. Do you know any commonly used example where this is not true? If so, why don't you assume this is upper-semi-continuous and use this function instead of $G(\\theta)$ in later discussions?\n\n(2) Some discussions on the intuitions behind Assumption 5, and connecting it to some assumptions that the readers are more familiar with would be very helpful. For example, the authors mentioned that Assumption 5 is satisfied for $\\theta\\in\\mathbb{R}$ for $\\exp(\\theta^{2})$, $\\exp(\\theta)$, $\\theta^{r}$ etc. But those examples are all for one-dimensional $\\theta$. How about in high-dimensions?\n\n(3) I don't understand why the assumptions in Section 3 are called Property 1-Property 4 instead of say Assumption 1'-Assumption 4' or Assumption 6-Assumption 9. They seem to be assumptions instead of properties.\n\n(4) In Theorem 3 (and maybe in Theorem 2 as well), can you say something about the properties of $F_{\\lim}$? For example, is it $L_{p}$ for some $p$?\n\n(5) In Theorem 3, when you have stability, can your proof techniques also provide some non-asymptotic convergence guarantees instead of just some asymptotic results? That can strengthen the paper by a lot.\n\n(6) In the proof of Theorem 2, it's better to write \"imply\" instead of \"supply\" and in the first line of Section 4.2., maybe you can write \"provides\" instead of \"supplies\". \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the author studies the global convergence of SGD. Compared with previous results, the authors show that under weaker conditions, the iterates of SGD will either converge to a stationary point of the objective function or diverge. ",
            "main_review": "Strength:\n1. Analyzing the global convergence of SGD is a very important research problem in the area of machine learning and optimization. \n2. The paper is well written and very good to follow.\n3. The analysis strategies may be beneficial for the community. \n\nWeakness:\nCompared with previous results, the main difference is the weaker assumptions, i.e. \n    (1) global Holder continuity (literature) -> local Holder continuity (this work)\n    (2) bounded variance of stochastic gradient (literature) -> the stochastic gradient is upper bounded by some upper semi-continuous function (this work). \nThe author claims that their setting is more realistic. However, they don't provide any example for supporting this claim (especially for the second one). \n\n",
            "summary_of_the_review": "I found the manuscript to be clearly written and technically sound. Although it has some weakness, I think it still worth a publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the behavior of SGD under very general assumptions. The existing global convergence of SGD requires two restrictive assumptions: a global Holder continuity for gradients and unrealistic noise models for stochastic gradients. This paper relaxes the global Holder continuity assumption to a local Holder continuity assumption, and consider general noise model assumptions. The main results include global convergence and stability. By global convergence, the authors show that either the iterates converge to a stationary point or they diverge. By stability, the authors show that the objective function remains finite along any iterate sequence. In the deduction, the authors introduce novel techniques to decouple some dependencies encountered in considering general assumptions. ",
            "main_review": "The paper is clearly written and the results seem to be interesting. Here are some comments/suggestions:\n\n- Assumption 5 is a bit complicated and difficult to understand. It is not quite clear how this assumption comes. I would suggest the authors to give some discussions on its motivation and its connection to the existing assumptions in the literature.\n\n- The results in the paper are of asymptotical nature, i.e., related to the limiting behavior of SGD if the iteration number goes to infinity. In practice, we are perhaps more interested in the nonasymptotical behavior, i.e., how the convergence rate is w.r.t. the iteration number. Is it possible to provide some nonasymptotical results under the local Holder continuity  assumption and the general noise model assumption? If not, are there any essential difficulties.\n\n- The authors consider general stepsize in the form of matrices. While this stepsize is very general, in practice the most used stepsize the scalar stepsizes. If we consider scalar stepsizes, can the global convergence and stability results be further improved?\n\n- Note that $L_R$ is defined as $L(0,\\psi)$ for $\\|\\psi\\|_2=R$. However, there are many $\\psi$ with $\\|\\psi\\|_2=R$. Can these different $\\psi$ lead to the same $L(0,\\psi)$. If not, then $L_R$ would not be well defined.\n\nTypos:\n\npage 2: the the\n\npage 4: holds for functions a variety\n\npage 6: our for\n\neq (9): Does $\\\\{\\|\\theta_j\\|_2\\leq R\\\\}$ mean $\\\\{X:\\|\\theta_j\\|_2\\leq R\\\\}$?\n\npage 6: be get arbitrarily close",
            "summary_of_the_review": "The convergence and stability analysis of SGD in the general assumption seems interesting. The authors also introduce some techniques in the analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}