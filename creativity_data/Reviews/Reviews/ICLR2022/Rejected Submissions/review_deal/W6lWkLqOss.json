{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Although two reviewers have given score 6, the other reviews have clearly indicated that the paper is not good enough for ICLR. The paper does not give any new insight into the considered problem, many existing papers are ignored, and the only interesting part about evaluation of label importance is rather very shallow borrowing ideas from other fields without any deep discussion or analysis."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors advocate for class-stratified weighted macro-averaging as an appropriate scalar-valued evaluation for classification under class-imbalance. In the absence of domain expert specified weights, they also propose a weighting function that emphasizes rare classes (referred to as rarity), and a multi-importance criteria based on a normalized product of weightings. Furthermore, they point out that these weightings can be used for training via existing methods in commonly used tools (e.g., class-based instance weighting, class-based loss scaling). Finally, they show performance under different weighting for three log message classification tasks, a sentiment classification task, and a URL classification task (e.g., malware, NSFW, phishing),  — demonstrating that different weighings lead to different orderings of evaluation results and that these weights can be effectively used in training.",
            "main_review": "First of all, I agree that class-stratified weighting can be effective for evaluation and training. However, from a research perspective, this paper has very limited novelty. With respect to class-stratified evaluation (or combining them into a single score), I have seen hundreds of reports that already do this. From a training perspective, this is taught in just about every undergraduate machine learning course (both in training in evaluation). From a training perspective, as the authors point out, this is available in many toolkits (e.g., scikit-learn, SVMlight, TensorFlow, pyTorch), so people are obviously doing this.\n\nThus, the real question is how to set these weights, which generally falls under the purview of decision theory. There are multiple textbook chapters (and entire textbooks) that address this issues. Given the interest in class imbalance, I would recommend something in the medical decision making community where preference elicitation is widely-studied (e.g., standard gamble). A paper that particularly influenced my thinking is in this space is [A. J. Vickers and E. B. Elkin. Decision curve analysis: A novel method for evaluating prediction models. Medical Decision Making, 26:565–574, 2006.], [but there is ample literature in this space.\n\nSecondly, averaging together a set of point estimates to derive a scalar doesn’t necessarily have well-behaved statisical properties as it depends on what is being averaged together. Specifically, there are cases (e.g., sensitivity and specificity) that are frequently modeled as bivariate normal when aggregating measurements. Without such considerations, it isn’t clear how statistical significance would be evaluated. A paper that influenced my thinking here wrt rigor and scalar values for class imbalance settings is [Jurman, Riccadonna, and Furlanello, A Comparison of MCC and CEN Error Measures in Multi-class Prediction, PLoS One, 7(8): 2012]. Basically, without knowing the distribution properties of the average, it isn’t obvious how to perform hypothesis testing.\n\nFinally, I am not entirely convinced that the motivation regarding a scalar evaluation for multi-class under severe class skew is even that desirable. While subjective, I believe there are potentially good visualization that can be used to elicit class-specific utility — which, as the authors state, are very sensitive for some rare events (e.g., medical diagnosis, (hopefully fictional) doomsday devices). \n\nThis all being said (that this isn’t a suitable paper for ICLR), the writing itself is a bit informal, but easy to understand and some of the observations on the datasets are interesting. However, there isn’t sufficient novelty, potential for impact, or mathematical rigor to accept this paper in a top-tier machine learning conference.",
            "summary_of_the_review": "The authors advocate for class-stratified weighted macro-averaging as an appropriate scalar-valued evaluation for classification under class-imbalance — justified by a reasonable number of case studies. However, they don’t describe a procedure for setting the weights in a decision theoretic way nor describe the statistical properties of the resulting point estimate for hypothesis testing. Thus, while a reasonable heuristic approach, it lacks rigor for use in critical systems (where high-loss rare events are important). Additionally, there is very limited novelty as this is performed both for training systems and evaluating them in general ML research, application-focused ML research, and industry practice. This paper has some interesting observations, but is not ready for ICLR acceptance. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The article focuses on a significant problem related to the quality assessment of unbalanced data classifiers, or rather a problem where one deals with a problem where we may observe different misclassification cost for each class. I fully agree that the known and commonly used metrics do not meet expectations, hence their diversity, the analysis presented in the works cited by the author. ",
            "main_review": "A simple and intuitive model has been proposed that modifies the global quality metric by adding class significance weights. However, a problem that, in my opinion, is still unresolved is how to determine the importance of the classes. As a rule, as the authors point out, it should be a user's decision, but as is well known, we have a problem with obtaining this information. Note that the classic probabilistic model of the classifier is based on the minimization of loss function, which leads to the minimization of the overall risk (expected value of the loss function), i.e., one can say that the problem is solved and the proposal contained in the paper does not bring anything new. Unfortunately, we usually do not know the form of the loss function because this is information that an expert should provide. Some methods try to gain the knowledge of the misclassification cost, such as the work on Utility-based Performance Measures (https://www.dcc.fc.up.pt/~ltorgo/Papers/RT08_WEMML.pdf) or methods that try to obtain information about the cost of error using linguistic variables (such as fuzzy loss function - https://link.springer.com/content/pdf/10.1007%2F978-3-540-68168-7_53.pdf). Unfortunately, neither of these approaches fully addresses this problem.\nI find interesting comments of the authors contained in sec. 3.2, where it is proposed to calculate the weights according to factors other than those obtained directly from users.\n\n",
            "summary_of_the_review": "Recognizing that the problem is interesting, however, taking into account the above, I found that the contribution of this work is insufficient for this conference.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presented a simple yet general-purpose class-sensitive evaluation framework for imbalanced data classification. Their framework is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. They provided a modular and extensible formulation that can be easily customized to different important criteria and metrics. Experiments with three real-world use cases show the value of a metric based on our framework, Weighted Balanced Accuracy (WBA), over existing metrics – in not only evaluating the classifiers’ test results more sensitively to important criteria but also training them so.",
            "main_review": "The paper addressed a known problem by introducing weights to the rare class and explaining a new theoretical finding to better grade the models applied.\n\nThe 2 main goals of simplicity and good generality of models were addressed. The study proposed a new grading metric to an old problem\n\nThe research is reproducible and presents simple novel findings\n\nThe Weak points are trying to incorporate too many things in the model like user-defined, rarity, multiple, and partially-defined importance.\n\n\n",
            "summary_of_the_review": "Overall, I vote for acceptance as the class imbalance problem is chaotic. It gives classification suboptimal results. Especially when allied with class disjunct\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification. Specifically, the proposed evaluation metric is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. Generally speaking, the problem authors paid attention to is really existed and is important in imbalanced classification problem. Moreover, the writing in this paper is very well and is very easy to follow. ",
            "main_review": "In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification. Specifically, the proposed evaluation metric is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. Generally speaking, the problem authors paid attention to is really existed and is important in imbalanced classification problem. Moreover, the writing in this paper is very well and is very easy to follow. However, the paper still exists the following problems:\n\nThe major problem of this paper is that the importance of each class is difficult to define. In this paper, the authors propose four methods in Section 3.2, i.e., importance criteria=user-defined, importance criteria=rarity, multiple importance criteria and partially-defined importance criteria. But in my opinion, none of them is very accurate and objective to define the importance of each class because in real-world applications, even if domain experts are difficult to distinguish the subtle importance differences between some classes. Anyway, it is an interesting and valuable problem for researches to discuss the importance differences between different classes. I think if the authors can provide some other importance criteria that are more objective, this paper will be more valuable.\n\nBesides, since this is a novel evaluation metrics, I think the authors should use a collection of datasets to fully verify the effectiveness of the proposed framework, instead of just three datasets in this paper. \n\nFinally, if the number of some classes are too rare, according to the rarity importance criteria, these classes will be given a very large weight, if so, whether the accuracy of majority classes will be largely decreased?\n",
            "summary_of_the_review": "In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification. Specifically, the proposed evaluation metric is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. Generally speaking, the problem authors paid attention to is really existed and is important in imbalanced classification problem. Moreover, the writing in this paper is very well and is very easy to follow. However, the paper still exists the following problems:\n\nThe major problem of this paper is that the importance of each class is difficult to define. In this paper, the authors propose four methods in Section 3.2, i.e., importance criteria=user-defined, importance criteria=rarity, multiple importance criteria and partially-defined importance criteria. But in my opinion, none of them is very accurate and objective to define the importance of each class because in real-world applications, even if domain experts are difficult to distinguish the subtle importance differences between some classes. Anyway, it is an interesting and valuable problem for researches to discuss the importance differences between different classes. I think if the authors can provide some other importance criteria that are more objective, this paper will be more valuable.\n\nBesides, since this is a novel evaluation metrics, I think the authors should use a collection of datasets to fully verify the effectiveness of the proposed framework, instead of just three datasets in this paper. \n\nFinally, if the number of some classes are too rare, according to the rarity importance criteria, these classes will be given a very large weight, if so, whether the accuracy of majority classes will be largely decreased?",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}