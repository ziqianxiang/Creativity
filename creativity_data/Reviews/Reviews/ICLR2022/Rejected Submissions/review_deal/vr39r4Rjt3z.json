{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Unfortunately, I feel the paper is not quite ready for ICLR, even if the reviews seem in general quite positive (though of low confidence). \nAfter reading the reviews and rebuttal, and going over the paper I have to make the following comments:\n * The paper do two modification to the backbone architecture, that have an impact on the ability of these systems to continually learn; these changes are adding layer normalization and a mask\n  * The paper is mostly empirical in nature; while there are some intuitions presented clearly about these ideas, their efficiency is proved empirically, which is completely fine\n\n However:\n  * The empirical validation seems not sufficient; the main results are permuted MNIST, incremental CIFAR 10, incremental CUB200; the results on permuted MNIST in terms of final accuracy seem surprisingly low (particularly when involving CL solutions, like EWC, ER, HAT .. see table 2; e.g. FA1 < 80% seems very surprising). This seems strange to me and adds a bit of shade on the results \n  * The proposed methods are simple; There is a strong message behind them, namely that the choice of the backbone (architecture size, normalization layers) has a huge impact on learning. But being a purely empirical result, this really needs to be backed up with analysis and an attempt at understanding of what is going on. E.g. looking at the masks over time .. to they converge to be task specific? Anything that would give a bit of depth to the results. Discussing the Figures (e.g. I'm looking at Fig 3 and I was grep-ing the text to see a discussion of how one would interpret those results). Why is FashionMNIST used to produced Fig 3, and why is not something like this done for one of the CL benchmark considered. Providing additional typical measures for CL (e.g. showing learning curves). \n  * Just overall does not seem that the work provides sufficient insight, or analysis. \n\nI do think there is something really interesting in this work, and I do hope the authors will resubmit this work after some modification. And I do agree that there are many aspects of the backbone or architecture that have big impacts on CL and this is an understudied and not well understood topic. So in that sense I think the idea of this work is good. But I just feel it fails short in terms of results, analysis. I feel in the current format, the work will not have the impact it deserves."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes modifications to the highway connection networks (HCN) through the introduction of masks for selective weight updates and a layer-wise normalization to mitigate internal covariate shifts at feature level. Experiments were performed on the Permuted-MNIST, incremental CIFAR-100 and incremental CUB200 by incorporating this approach alongside EWC, ER and HAT. ",
            "main_review": "\n* The motivation for the pertained weights and the differences in the experimental setup between datasets is not clear.\n* Comparison with relevant work is missing: The idea of sparse, selective update of parameters to mitigate catastrophic forgetting and encourage task separability has been explored previously. For example, [1] with energy-based models, [2] with local synaptic plasticity and neuromodulation, [3] learns relevance maps. The UCL approach was also mentioned in the paper but not compared against. A systematic comparison with this class of approaches will help better assessing the proposed approach in terms of accuracy/computational cost.\n* Continual learning approaches are typically evaluated in both task- and class-incremental learning settings, it’s not clear from the experiments about which scenarios is considered.  \n* Classification accuracy might not be the best metrics to evaluate the continual leaning potential. Other metrics such as the backward and forward transfer are better suited [4].\n* Do the results assume a certain task ordering or have they been chosen randomly. Does the accuracy change with the change in relative ordering of the tasks? Will having consecutive having tasks with similar distributional properties have any effect on the interference with previous tasks.\n \n\n[1] Li, S., et al. \"Energy-Based Models for Continual Learning.\" arXiv preprint arXiv:2011.12216, 2020\n\n[2] Madireddy, S., et al. “Neuromodulated Neural Architectures with Local Error Signals for Memory-Constrained Online Continual Learning”, arXiv preprint arXiv:2007.08159, 2021\n\n[3] Kaushik, Prakhar, et al. \"Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping.\" arXiv preprint arXiv:2102.11343 (2021).\n\n[4] Mai, Zheda,  et al. \"Online Continual Learning in Image Classification: An Empirical Survey.\" arXiv preprint arXiv:2101.10423 (2021).\n",
            "summary_of_the_review": "The results look promising, but the novelty of this approach seems limited since the use of learnable masks, gating mechanisms, and layer-wise normalization have been previously proposed in the continual learning context. Moreover, the experiments are limited in terms of comparison with other state-of-the-are continual learning methods, evaluation metrics (such as backward and forward transfer) that characterizes the forgetting behavior.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper investigates the backbone networks that are less prone to catastrophic forgetting. Two modifications to existing backbones are found to be useful: a mask attached to the gate function of the Highway Network, and layer-norm (without tuning parameters). Experiments on top of existing popular learning algorithms designed for continual learning (EWC, ER and HAT) show that these modifications work.",
            "main_review": "The findings in the paper are useful, adding evidences to the existing literature on continual learning that we need to pay attention to the interplay between learning algorithms and backbones. The introduction of the mask on the gating function in Highway Networks is interesting. However, this seems to be specific to this particular architecture. Also I would like to see more theoretical analysis of the empirical findings.\n",
            "summary_of_the_review": "The findings are interesting, suggesting that research on continual learning should pay more attention to designing neural architectures that are natively suitable to handle catastrophic forgetting. However, the novelty seems to be limited as it is based on well-known architectures and ideas.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed two architectural improvements for networks designed for continual learning. Specifically, binary masks and layer normalization are added to highway connection classifier networks in order to prevent forgetting. The value of each of these elements is made clear from thorough experimentation.",
            "main_review": "This paper excels at making clear the importance of each of the proposed changes to HCNs. The experimentation is thorough, showing strong performance of the proposed methods on several datasets. The motivation for masking and layer normalization is made clear and is supported by empirical measurements.\n\nVery minor weaknesses include small grammatical errors and tense confusion. (Examples listed below).\n\nIn general, this paper makes a compelling case MHC and LWN. The experiments span several datasets as well as different external continual learning techniques. The improvements proposed show strong performance in every case considered. In my opinion the notation is Section 3 is a bit confusing, but I think the points made in Kuo et al. (2021b) were well summarized (so using the notation from that paper is justified). Figures 3 and 4 are a bit hard to read, and perhaps representative quantities could be made explicitly clear in the text rather than only using phrases like \"deeper colours\" (and less critically I think the fonts should be larger in those plots).\n\nThese minor points do not affect my score.\n\n(i) In Section 1, the past and present tenses are both used to describe things done in this paper -- this should be consistent. \n\n(ii) Section 1 has a sentence that starts \"Our study focuses on designing new architectures in the forward pass which indirectly but positively affects...\" I think the conjugation should be 'new architectures... affect'.\n\n(ii) In the beginning of Section 4, it says \"Overleaf in the top half of Figure 2... \" and this may be a typo. \n\n(Note that I am not an expert in continual learning and none of my own research has been in this space. With that in mind, I admit that I am not up to date or extremely familiar with existing related work.)",
            "summary_of_the_review": "I think this is a strong paper with compelling experimental results. I vote to accept it. Some small grammatical errors should be corrected.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In the paper, the authors present two modifications: Masked Highway Connection and Layer-Wise Normalisation. In Masked Highway Connection, authors add a binary mask to classical HCN (Highway Connection Networks) and slightly change the training procedure. On the other hand, Layer-Wise Normalisation is a new normalization of activation in the neural network.",
            "main_review": "The paper is hard to read and uses very strange notation. It is very hard to understand formulas since authors do not use standard notation in deep learning. The modification proposed in the paper seems small and can be understood as an effect of architecture search. \n\nThe most interesting part is the experimental section. The authors have verified six classifier networks with four continual learning setups over three datasets. Unfortunately, it is not clear why the authors use SGD instead of adam. It is unclear why we do not have results on a classical convolutional neural network with 5-10 layers. It is not clear why in  MHC, authors train models with additional parameters. It is not clear the influence of such parameter since in Tab. 1 on Perm-MNIST wins LWN in MHC (70%) and on Inc-Cifar100 and Inc-CUB200 LWN in MHC (10%). It is better to choose a small or large percent of gated unit activation. Hot the parameter shud be tune in practice?  ",
            "summary_of_the_review": "The paper is hard to read and uses strange notation. The proposed modification seems to be small and not significant. The experimental section looks nice and is convincing, but authors should use some additional architecture and show how the method works on adam optimizer.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "1. What is \\omeg_i in definition (1). And wherein the method we use such information?\n2. Why activation is denoted by \\H and natural number are marked by \\gamma instead of classical n,k, l. \n3. It is not natural to use three-level indexes A_{L_{\\Gamm}}\n4. I do not underfund notation in equation (3). The note is nonintuitive for me.\n5. Masked Highway Connection seems to be a simple modification of HCN. However, it also requires adding new parameters to neural networks. \n6. Also, Layer-Wise Normalisation seems to be classical normalization. Why does it work in CL setting?\n7. The authors use a very small neural network for EWC and other classical approaches in the experimental section.  Also, we use only a dense layer (as a trainable layer) without convolution ones. \n8. Authors use only SGD. Why not ADAM?\n9. It is not obvious that all neural networks in the experimental section have similar parameters.\n10. Authors tested MHCs with a range of different settings. Hot the parameter shud be tune in practice?  ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes modifications to the highway connection network (HCN) architecture, which exhibits some inherent resistance to catastrophic forgetting.  These modification introduce fairly simple masking rule for selection of portion of weights to update and normalisation that (presumably) further adjust the internal representation to be more selective in which weights to update for a given task.  Empirical evaluation shows that these modification are highly effective in alleviating catastrophic forgetting.",
            "main_review": "I find the general approach of \"baking\" in the ability to handle continual learning into the network architecture quite an elegant way of solving the catastrophic forgetting problem.  Though the proposed modifications to the HCN architecture are rather simple, I think this paper is worthy of note, because these changes make the network quite more robust for continual learning, and thus present a relatively straight forward, headache free (other than deciding on the MHC threshold) architecture that seems significantly less vulnerable to catastrophic forgetting.  \n\nThe paper would be stronger if there was a bit more theoretical explanation of how the internal dynamics of the masking and normalisation \"select\" weights in a way that does not destroy what the network has learned.  As it stands, the explanation is mostly based on intuitive understanding of the dynamics and empirical evidence, and quite general principles, akin to the argument that HCN is good for catastrophic forgetting, because it multiplies the derivative by a fraction.  BTW, I find it a bit counter-intuitive that HCN was designed to \"withstand gradient vanishing\" and is less vulnerable to catastrophic forgetting due to scaling down of derivatives....which, effectively, is gradient vanishing.  It seems that these two statements contradict each other.  \n\nWhile overall the paper is pretty well written, some parts could be improved for clarity.  Mathematical notation seems cumbersome (why use $L_\\gamma$  etc. subscript everywhere to indicate layer index...when just the $\\gamma$ subscript will do for index identifying the layer.  Why the neuron's output is labelled with $a$ for MLP and with $k$ HCN-based models?   It's exactly same neurons, isn't it.   Also, HCN could be explained a bit better (in Section 2).  Sure, it's always possible to go and read the references...but given that HCN is the architecture that this paper builds on, it could be explained a bit more thoroughly.  ",
            "summary_of_the_review": "It's a decent paper - it would be better if there was more theoretical explanation for why the proposed changes to the architecture help with continual learning, but the provided empirical evaluation seems pretty good, and the proposed architectural changes to HCN seems quite effective.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}