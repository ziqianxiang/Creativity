{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies the effect of importance weighting in three model classes: linear models, linearized networks, and wide fully connected networks, and show that under certain assumptions, gradient descent for training an overparameterized model converges to the same ERM interpolator regardless of the reweighting scheme. The reviewers acknowledge that this paper had good exposition and writing in general, but they were in consensus that the initial version of this paper includes many inaccurate overclaiming statements. In summary, after discussions, the reviewers would like the authors to:\n\n- revise the abstract and the introduction, specifically, adding appropriate qualifiers on the neural networks, losses, full gradient descent training, etc (Reviewers hLzT, M2hT, fZgx)\n- address the discrepancy between theory and experiments, e.g. the inconsistency of the loss chosen in theory and experiments, the requirement that the widths of the neural networks need to large (Reviewers wG4N, fZgx)\n- add experimental results for early stopping (Reviewer hLzT)\n- empirically verify that the final solutions of ERM, DRO, and IW initialized at the same \\theta^0 are very similar (Reviewer M2hT)\n- discuss the novelty compared to Sagawa et al in the paper (Reviewer wG4N)\n\nthus, this submission needs a major revision, and is not ready for acceptance in its current form. We encourage the authors to revise accordingly, and resubmit in the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors study the problem of improving the worst-group accuracy in the over-parametrized setting. They offer theoretical and empirical evidence that reweighting does not affect the final solution in this setting; in other words, the algorithm would converge to the same interpolators as that of basic ERM. The theoretical analysis is done for linear models, linearized networks, and wide fully connected networks. \n\nThe authors also argue that the solution does not change unless a considerable amount of regularization is incorporated, so much so that it affects the training error. Again, some theoretical and empirical evidence is provided.\n\n",
            "main_review": "Understanding worst-group performance is an important and interesting topic. The paper is relatively easy to read, and provides valuable insights about this problem. However, there are some issues that need to be addressed\n\n+ The abstract and introduction are quite misleading. For example, we see strong statements in the abstract such as \"we cannot hope for reweighting algorithms to converge to a different interpolator than ERM...\" or \"we prove that for regularization to work, it must be large enough to prevent the model from achieving small training error.\" Such blanket statements without any hint that these are based on important assumptions (that are somewhat scattered in the paper) are misleading for the readers (of course unless if they dig deeper)\n\n+ The first row of figure 1 is a good example of an overparametrized model, where the training accuracy of all models become 1 at some point in training. However, we still clearly see that even after than point, the test accuracies of different approaches are not the same. This is in contrast with the theoretical result by the authors that all of those methods would converge to the same result. The only explanation for this discrepancy is that the assumptions that the authors have made do not make sense for this experiment. For the second row (CelebA) the algorithms do not reach training error zero, and are effectively outside the regime of interest for the theoretical results of this paper. In conclusion, the the theoretical results do not really support the empirical results in this case; they really show different things. \n\n+ Given the discrepancy between the theoretical and empirical results, it becomes more important for the authors to make the abstract/intro/.. more accurate. For example, someone that reads the abstract will conclude that there is no way for the reweighting to help unless the average accuracy drops significantly, whereas the experiments show otherwise. \n\n+ The observation in section 3.1 that for linear models, the change in the parameters is always in the span of the data points is quite useful. I believe this is a standard observation that has been made before but I may be wrong. Can the authors comment about this? Are they the first to discover this? If not, they should at least add a citation.\n\n+ More generally, are the other proofs (e.g, for the wide networks) similar to the analyses performed in previous work? If so, can you explicitly name those papers?\n\n+ Is the theoretical analysis specific to the squared loss (e.g., in Thm 4, 5, 6)? If so, this must be clearly indicated as an extra assumption. \n\n+ Can you elaborate on assumption 1? Why is it intuitively a reasonable assumption (aside from the empirical study)? \n\n+ It looks like some of your experiments is similar to Sagawa et al. (2020a). Can you elaborate on the differences?\n\n+ In Table 1, I believe you have left out the performance of ERM for early stopping. I think it is quite important to have those to be able to compare with the other methods.\n\n+ As a side-comment, the practice of reporting 95% confidence intervals just based on running the method on a number of  random seeds is not necessarily valid. (usually you need to do cross-validation, or if not, argue that the test set is large enough...)",
            "summary_of_the_review": "The paper studies an important problem. The theoretical analysis is interesting though it has its own limitations. It will help if the authors comment about the novelty of the proof techniques compared to the previous work. \n\nMany statements of the paper are misleading and should be addressed. The empirical study and the theoretical study do not necessarily support each other and this discrepancy has not been discussed in the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "This is a paper about fairness through worst-group performance. Yet the abstract/intro are quite misleading and can have negative impact.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper theoretically proves the empirical observation that has been made in a lot of recent works regarding the role of importance reweighing in overparameterized deep networks. For squared loss and independent data points, it theoretically proves that reweighing the data points has no effect on the final model learned for linear models and linearized neural networks. This work also analyzes the role of regularization in preventing this behavior and proves that regularization has to be large enough to prevent small training error to have any effect. This work also has supporting empirical results on two datasets- celeba and waterbirds.",
            "main_review": "- The paper is studying an important problem about the effect of importance reweighing in overparameterized models which has attracted a lot of interest recently and thus, would be of interest to the community.\n- However, the paper has many serious flaws. First of all, the paper is studying squared loss for its analysis and does not include a discussion about the choice of loss function. Loss function used in the previous works cited in this paper was cross entropy loss. So, there is this discrepancy between the works/experiments that this work is replicating and the theoretical results that are included.\n - This work is missing important citations for example [1] which initiated this study of importance reweighing methods in deep learning to the best of my knowledge.  Moreover, this has already been studied theoretically for cross entropy and other general losses by [2] which has not been cited in this work. So, it is not even clear if studying this paper for the squared loss brings out any new idea or insight on top of what was already known before. The point about regularization being necessary has already been made in previous works.\n\n[1] What is the Effect of Importance Weighting in Deep Learning? Byrd et. al. \n[2]  UNDERSTANDING THE ROLE OF IMPORTANCE WEIGHTING FOR DEEP LEARNING, Xu et al.",
            "summary_of_the_review": "- However, the paper has many serious flaws. First of all, the paper is studying squared loss for its analysis and does not include a discussion about the choice of loss function. Loss function used in the previous works cited in this paper was cross entropy loss. So, there is this discrepancy between the works/experiments that this work is replicating and the theoretical results that are included.\n - This work is missing important citations for example [1] which initiated this study of importance reweighing methods in deep learning to the best of my knowledge.  Moreover, this has already been studied theoretically for cross entropy and other general losses by [2] which has not been cited in this work. So, it is not even clear if studying this paper for the squared loss brings out any new idea or insight on top of what was already known before.\n\nI would like to make recommendation for rejecting this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work studies the problem of training for worst-case subgroup performance. This has been a very prominent research avenue in the past few years and provides a natural approach towards learning fair and \"causal\" models (that do not perform too badly for any pre-specified subgroup). In practice however there have been some difficulties with existing approaches, and e.g., in the overparameterization regime, these methods do not perform well in terms of  worst-case performance on test sets. This paper provides a theoretical explanation for this fact (using linear models and linearized neural networks) and also illustrates it using numerical experiments on two datasets. Furthermore, the authors assess the role of $L_2$ regularization in training. The practical takeaway is that existing approaches based on reweighting require substantial regularization or early stopping to perform well in terms of test accuracy (in the overparamerized regime).\n",
            "main_review": "\nI enjoyed reading this paper. It is written in a very clear way and it addresses a very timely and important problem. I also enjoyed that it combined both theoretical results and simulations.\n\nBelow are some questions that I have and some suggestions that perhaps can be helpful to the authors in improving their paper.\n\n1) If I understand correctly from equation (4) and Assumption 1, the framework of the present paper only applies to training updates based on the full dataset, and not to batch or stochastic updates. If this is the case, then this should be highlighted more prominently in the text and in the abstract. Would it be possible to generalize the analysis?\n\n2) \"Each $\\mathcal{D}_i$ is a subset of $\\mathcal{X} \\times \\mathcal{Y}$\": Is this condition required for the theoretical results? If so, it should be discussed at more length. Can it be relaxed? For example, in my mind often the notion of worst-case performance relates to multiple environmentsl as in e.g, the conceptual framework of Rothenhäusler et al (2021, JRSSB) or to protected attributes that are not directly included in the model. These may not be perfectly recoverable through subsets of predictors and response variables (though of course they may be strongly correlated).\n\n3) Maybe in the high-level statement after Theorem 2 one could clarify that this statement depends on identical initialization $\\theta^{(0)}$ for all involved methods?\n\n4) Last paragraph of Section 3, \"Moreover our theoretical results can explain the surprising empirical observation\": I think this paragraph is not sufficiently justified (i.e., it does not really explain the observation, at most it hints at the plausibility of it). Can the authors make a stronger case for their argument? Otherwise my suggestion would be soften the claim (or even remove the paragraph).\n\n5) I like the experiments since they bring the main point of the paper across very clearly. One additional result I would have liked to see is an empirical illustration of whether the theoretical arguments hold, e.g., are the final solutions of ERM, DRO, and IW initialized at the same $\\theta^{(0)}$ very similar (and more similar than solutions of the same method initialized at different $\\theta^{(0)}$)? By similarity here I refer to similarity of actual model fits, rather than accuracy/performance.\n \n\n\n# Typographical errors\n\n* Equation (5), $f^{(t)}$ instead of  $f$\n* Page 8: \"trained trained\" (repeated twice)\n* Reproducibility statement: I think speculation should be \"specification\" (note: the word speculation appears twice in this sentence). Anaconda should be capitalized.\n\n",
            "summary_of_the_review": "\nI think this is an interesting contribution on the theoretical and practical study of worst-case subgroup performance and therefore I would like to see this work accepted. I also think the paper could be improved a bit further by adding experiments that more closely validate the theoretical results (point 5), and by clarifying limitations/shortcomings to this work.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper mainly focuses on the reweighting algorithms (e.g. Importance Weighting, Group DRO) for the worst-group performance. It tries to theoretically explain why overfitting problems always appears in reweighting algorithms. Specifically, the authors prove that under several conditions (e.g. assumptions for algorithms, wide fully-connected neural networks, squared loss), the worst-group test performance of the reweighting algorithm will converge to the same level as ERM.",
            "main_review": "Strengths:\n1.\tThe paper provides good proof for linear models via the intuition that the change in model parameters always lies in a low-dimensional subspace. \n2.\tThe conclusion gives new insight for designing algorithms, which shows reweighting algorithms without any repair methods may not be better than ERM for the worst-group performance.\n3.\tThe paper writing is generally of high quality.\n\n\nWeaknesses:\n1.\tMy feeling is that the conclusion is somewhat overclaimed. In both abstract and conclusion, it is emphasized that this work proves the pessimistic result that reweighting algorithms always overfit. However, the paper only proves that this conclusion might be true for some **specific** situations. For example, the reweighting algorithms need to satisfy Assumption 1 and Assumption 2, which means not all reweighting algorithms are considered. The overparameterized models need to be linear models, linearized neural networks or wide fully-connected neural networks, which are not commonly used in practice. Besides, the squared loss needs to be used to confirm the update rule is linear. All those assumptions are not quite mild for me. \n2.\tThe analysis of neural networks contributes less. With the existing NTK theorem, the extension from linear models to wide fully-connected neural networks is trivial (Section 3.2, 3.3). The work bypasses the core problem of overparametrized neural networks and only considers the easy wide fully-connected neural networks. \n3.\tThe theoretical results and experiments do not match. The theoretical proof considers wide fully-connected neural networks, while the experiments utilize a ResNet18 as the model, which is quite different.\n4.\tSome key steps are empirical, although the paper claims that it provides a theoretical backing in the abstract. For example, this paper only proves that reweighting algorithms will converge to the same level as ERM, but the conclusion that ERM has a poor worst-group test performance is summarized through observation in practice. Besides, the paper can only empirically demonstrate that commonly used algorithms satisfy Assumption 2.\n",
            "summary_of_the_review": "The actual main contribution of this paper is proving that for linear overparameterized models, the current reweighting algorithms almost always overfit. It provides a possible way of analyzing overfitting problems by reducing a family of algorithms to the commonly used ERM. However, the analysis of neural networks contributes less, and some key steps are still empirical results. Besides, there is a gap between theoretical results and experiments. In summary, the conclusion of the paper is somewhat overclaimed, and there is still a lot of room for improvement. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}