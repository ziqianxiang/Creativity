{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper investigates attacks against time series analysis methods such as GNN and DNN for anomaly and intrusion detection. Standard attacks such as FGSM and PGD are extended for the time series domain and evaluated on several datasets including automotive, aerospace and resource utilization datasets. While the authors claim to be the first to investigate such attacks, some related work was not considered in the paper, which was pointed out by reviewers. Also some other weaknesses of the proposed method, e.g., its focus on feature space perturbations were pointed out. Hence, while acknowledging the importance and the novelty of this paper's contributions, the reviewers agree that the paper must be better positioned in the context of the related work in order to be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors apply adversarial attack techniques, such as FGSM and PGD, to “fool” the SOTA DNN and GNN methods for anomaly and intrusion detection in time series. They show that small perturbations to the input time series can lead to significant deterioration in the performance of the SOTA methods. ",
            "main_review": "Pros:\n\nThe paper clearly shows that DNN and GNN based anomaly and intrusion detection methods are sensitive to adversarial attacks. The authors consider all sota methods they can find and demonstrate their findings with thorough experiments. The paper is well written. \n\n\nCons:\n1.\tThis paper is not the first paper to transfer adversarial attacks to anomaly detection models for time series. As noticed by the authors, the paper titled “Robustness of Autoencoders for Anomaly Detection Under Adversarial Impact” published at IJCAI 2020 has studied the impact of IFGSM attacks on autoencoder-based anomaly detection methods. Therefore, it is better not to highlight “first” in the abstract. \n2.\tThe major contribution, that is, the performance of DNN and GNN based anomaly and intrusion detection methods suffers from the adversarial attacks, is not surprising. The potentially more attractive perspective of this research, as mentioned by the authors in Section 6, is to propose more robust algorithms, which is not done here.\n\nMinor Comments:\n1.\tPage 9, CANTransfer Performance: please provide some insights on why CANTransfer is more robust to FGSM attacks. \n2.\tPage 5, Section 4, Datasets, the last sentence: Table 1 summarize -> summarizes\n3.\tPage 5, Section 4, Evaluation Metrics: the prediction, recall, and f1-score -> the precision, recall, and f1-score\n4.\tThe same sentence as above: from thresholding methods -> from the thresholding methods\n5.\tPage 6, Line 3: I synthetic -> (i) synthetic\n",
            "summary_of_the_review": "It is a contribution to show that current SOTA anomaly and intrusion methods are quite sensitive to adversarial attacks through a systematic set of experiments. However, no new algorithms or methods or insights are proposed in this paper, and so I lean to rank the paper as borderline. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper shows adversarial attacks to time series anomaly and IDS systems, specifically focused on CAN (Controlled Area Network) dataset. The authors devise some criteria to select datasets and algorithms, and apply PGD/FGSM based attacks and show the brittleness of the detection systems.",
            "main_review": "Strengths: \n- The problem is relevant\n- Clearly outlined criteria for choosing anomaly detection baselines\n- I strongly appreciate Section 9, in addition to the authors making all their code available\n- Authors avoid base-rate fallacy by using F1, Precision, Recall\n\nWeaknesses:\n- There is no new strong methodology/attack proposed\n- The authors claim to be the first tackling adversarial in intrusion detection, but it is not the case (e.g., [1-3] )\n- The authors only consider feature-space attack, where it could have been more interesting to consider realizable problem-space perturbations [4] which consider modifications of real world objects/traffic within CANs\n\nComments:\n\nThe paper proposes a thorough evaluation of some time series-based anomaly detectors applied on datasets in the CAN domain. While the evaluation is generally well thought, I do have some concerns with the work:\n- **Contribution**. The contribution seem rather limited: at the end of the day, you are just applying a PGD/FGSM datasets to attack time series-based anomaly detection systems that use neural networks. It is not necessarily surprising that they are weak against this scenario. Moreover, you claim to be the first tackling the problem of adversarial attacks within the context of intrusion detection. This may unwittingly be a problem of overclaiming: there has been several works in the past (e.g., [1-3]) which studied the problem of adversarial attacks in the context of network intrusion detection systems, which are a super-set of your considered CAN-based attacks. \n- **Challenges**. There seems to be no new attack proposed or no significant challenge in applying PGD/FGSM attacks to these type of detectors. Hence, related to the prior point, it is unclear how much significant are the findings of this work. \n- **Problem-space**. You seem to be working only in the feature space, whereas it would have been more interesting to explore how modifying real-world CAN traffic properties would impact the effectiveness or complexity of the adversarial attack. In particular, you could have considered a modeling based on the framework proposed by Pierazzi et al [4] about realizable problem-space attacks. \n- **Focus on CAN**. It is partially unclear why the paper narrows the focus on CAN-only datasets. It seems to be a somewhat convenient choice, whereas there would be other DDoS datasets in more general network settings. Unless there is a strong motivation to focus only on CAN, it is partially unclear why the authors restricted themselves to this domain. I am saying this because the proposed attack does not depend on the CAN domain. This may change if you consider problem-space attacks. It is also important that the focus of CAN, if preserved, is clarified since the very beginning. As the abstract/intro of the paper seem to be tackling more general IDS systems. \n\nReferences: \n- [1] Apruzzese, Giovanni, and Michele Colajanni. \"Evading botnet detectors based on flows and random forest with adversarial samples.\" 2018 IEEE 17th International Symposium on Network Computing and Applications (NCA). IEEE, 2018.\n- [2] Apruzzese, Giovanni, Michele Colajanni, and Mirco Marchetti. \"Evaluating the effectiveness of adversarial attacks against botnet detectors.\" 2019 IEEE 18th International Symposium on Network Computing and Applications (NCA). IEEE, 2019.\n- [3] Corona, Igino, Giorgio Giacinto, and Fabio Roli. \"Adversarial attacks against intrusion detection systems: Taxonomy, solutions and open issues.\" Information Sciences 239 (2013): 201-225.\n- [4] Pierazzi, Fabio, et al. \"Intriguing properties of adversarial ml attacks in the problem space.\" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n\nEDIT:\nSligthly raising my score after rebuttal discussion with authors.",
            "summary_of_the_review": "I feel the major issue is with the limited contributions and originality of the work, and the unclear focus on CAN-only attacks, when the proposed methodology does not seem to tackle this domain specifically. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of adversarial attacks against time-series-based ML applications devoted to intrusion detection. The paper is relatively simple: they use existing adversarial ML strategies (white-box attacks) to thwart a similar ML system. The main contribution is the fact that few efforts investigated adversarial attacks against time-series based ML methods, and – specifically – no paper considered “anomaly and intrusion detection” scenarios.\n\nOverall, the presentation of the paper is adequate.\nThe quality of the English text is fair.\nFigures and Tables are appropriate.\nThe topic addressed by the manuscript is relevant and within ICLR’s scope.\nThe references are not appropriate.\nThe contribution is not very significant.\n\nSTRENGTHS:\n+ It is truly the first\n+ Evaluation on multiple datasets\n\n\nWEAKNESSES\n- Poor treatment of previous work\n- Unimpressive results\n- Poor threat model\n- Inadequate problem definition\n",
            "main_review": "(this review comes from a security-focused reviewer) I liked the research direction of the paper which I believe is worthy of investigation. Specifically, the claim that the paper is the first to investigate adversarial attacks against time-series-based ML systems in the context of “anomaly and intrusion detection” is valid. I also appreciated that the attacks were carried out on multiple state-of-the-art systems trained on distinct datasets.\n\nHowever, I am not impressed by the achieved results and, moreover, I believe that the paper falls short to be of adequate “impact” for ICLR. Let me elaborate on the abovementioned weaknesses.\n\nFirst, the **poor treatment of previous work**. It is true that adversarial attacks against time-series-based ML applications are not well-investigated. Yet, within the context of Intrusion Detection, there exist a plethora of works that highlighted the vulnerability of ML-based systems to adversarial attacks. A very recent summary is [A], which reports more than 40 papers within the specific context of Network Intrusion Detection (similar reviews can also be found in [B] and [C], although they also cover Malware and Phishing detectors). Specifically, [D] in 2018 was one of the first papers to expose the weaknesses of ML-based NIDS to imperceptible adversarial perturbations. Hence, the authors’ claim that “While adversarial attacks have been extensively studied in the context of image recognition, they have not been extensively investigated for anomaly and intrusion detection systems.” (start of page 2) is wrong, and should be amended. \n\n**Unimpressive Results**. I acknowledge that the paper is “the first”; however we are now in 2021, and the fact that ML-systems can be broken via adversarial perturbations is an accepted fact. Although in the specific context of time-series analyses such papers are rare, this is simply another setting. In other words: from a purely ML-perspective, there is no difference between a “static” and “temporal” phenomenon: in a “static” problem the dimension of the feature set is smaller as each sample is denoted only by its “current” values; in a “temporal” problem the feature set is larger and more complex as it also takes into account the past history. However, aside from such “dimensionality difference”, the two problems are identical. As such, I am not surprised that a white box attacker can thwart a similar system with a FGSM and PGD attack. To aggravate the issue, the attacker has no bounds on the perturbations: I acknowledge that this is a “realistic” assumption, however the attacker essentially can do whatever they want. In these circumstances, after thousands of paper that showed that ML is susceptible to adversarial attacks, is there any surprise that a “yet another” ML system can be broken?\n\n**Poor Threat Model**. This issue is strongly related to the previous comment, but since the targeted system is specifically devoted to security tasks it is even more relevant. In other words: the attacker knows and can do everything with the target system. I could accept a similar assumption if the target system addressed a task that had little in common with security (e.g., the general computer vision problem). However, when the targeted model is meant to be part of a security system, the situation changes. Specifically, an attacker that has full control of a security system is a violation of the basic security assumptions, as any security system under attacker’s control is compromised and its security is not guaranteed. In a similar circumstance, the attacker could achieve the same (or even worse) results without relying on adversarial examples.\n\n**Inadequate problem definition**. According to the authors, the attacks targeted systems devoted to “anomaly and intrusion detection methods”. However, little is done to explain these problems. Specifically, “anomaly detection” is a very broad term. It is true that the authors attack state-of-the-art systems, yet I would have appreciated a little more context w.r.t. what are the “anomalies” that are meant to be detected. Moreover, the authors’ definition of “intrusion detection” is extremely limited, as the attack seems to be specific to CAN (controller area network) which are systems embedded in vehicles, and thus represent a very niche subfield of intrusion detection. The resulting impression is that the paper simply assumes a (unrealistic and “obvious”) threat model, and then carries out (obviously successful) attacks but without considering any real world constraints or implications. I believe that a similar research method is inadequate in 2021, and even more so for ICLR. For instance, a common issue with current adversarial strategies is that the adversarial samples are crafted without taking into account the “problem space” ([E]). After reading the paper I was not able to figure out if this is also the case for the considered attacks.\n\nEXTERNAL REFERENCES\n\n[A]: \"Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems.\" ACM Digital Threats: Research and Practice (2021).\n\n[B]: \"Adversarial machine learning applied to intrusion and malware scenarios: a systematic review.\" IEEE Access 8 (2020): 35403-35419.\n\n[C]: \"Addressing adversarial attacks against security systems based on machine learning.\" 2019 11th International Conference on Cyber Conflict (CyCon). Vol. 900. IEEE, 2019.\n\n[D]: \"Evading botnet detectors based on flows and random forest with adversarial samples.\" 2018 IEEE 17th International Symposium on Network Computing and Applications (NCA). IEEE, 2018.\n\n[E]: \"Intriguing properties of adversarial ml attacks in the problem space.\" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n",
            "summary_of_the_review": "The paper has its merit, but the poor and unrealistic threat model undermine the “impact” of the results. It is true that attacks against time-series based ML are rare, but many efforts studied the impact of “traditional” adversarial attacks in intrusion detection. The extremely powerful adversary is able to thwart state-of-the-art ML systems… but such outcome is obvious considered the attacker’s assumptions.\n\nIn the current state, I do not believe the paper passes the bar for ICLR. I believe the paper requires a more realistic threat model that clearly outlines the real capabilities and knowledge of the adversary. In its current state, despite some originality, the paper represents just a “yet another adversarial ML paper”. The complete lack of references to security-focused works is emblematic of the issues affecting this paper. I acknowledge that ICLR is not a security-focused venue, but since the claimed contribution of this paper is its \"novel\" application, then I expect a stronger security background.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}