{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies self-supervised learning for graph neural networks by proposing a framework called LaGraph. Both theoretical analysis and experimental evaluation are provided in the paper.\n\nWe acknowledge the merits of this paper, which include studying a relatively less explored topic, providing theoretical analysis and comparison with other methods, and requiring less memory than a strong baseline.\n\nOn the other hand, there are also outstanding concerns (even after the discussions) regarding the novelty and significance of the proposed method (despite the claims of the authors during the discussions), whether the performance improvement over strong baselines is significant across different datasets, and missing a more comprehensive ablation study (beyond the preliminary results provided during the discussion period), among others.\n\nIn its current form, this is certainly a borderline paper for a top conference such as ICLR. It would be a better paper if the outstanding concerns could also be addressed before publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "  In this paper, the authors propose a novel SSL method (i.e., LaGraph) for graph representation learning based on latent graph prediction. The overall presentation is good with sound theoretical analysis. There is also a theoretical comparison between LaGraph and various related work with different categories. Extensive experiments were conducted on various datasets for both node-level and graphs-level tasks, where the proposed method has competitive performance to a set of baselines. However, there are some details regarding the problem definitions, technical content, and experiment settings that need further clarification.",
            "main_review": "+Strengths\n  1. Good overall presentation of the technical content\n  2. New contributions of tackling the challenges of conditional distribution $p(G|{G_I})$ instead of simply applying the noisy data reconstruction\n  3. Sound theoretical analysis\n  4. Comprehensive comparison with related work based on various techniques, e.g., denoising autoencoders, information bottleneck principle, and local-global mutual information maximization for contrastive learning\n  \n+Weaknesses\n  1. Some problem definitions, preliminaries, and details regarding the proposed method are not very clear (see Detailed Comments 1)\n  2. Some details regarding the experiments are not given (see Detailed Comments 2)\n  3. No discussion about the inherent limitations of the proposed method and possible solutions (see Detailed Comments 3)\n\n+Details Comments\n1. Some problem definitions and technical details regarding the proposed method are unclear and need to be further clarified.\n\n  For the problem definitions in Section 2.1, the authors claim that they 'consider an undirected graph'. It seems the proposed method can only derive the representations of one single graph. However, in Section 2.4, the authors claim that they consider a mini-bath of $N$ graphs, which is inconsistent with that in Section 2.1. In the experiments, the author applied the proposed method to multiple graphs for graph classification, which is also inconsistent with that in Section 2.1 (i.e., learning representation of one single graph). It is better to formulate the graph representation learning based on a set of graphs (but not a single graph) in Section 2.1.\n\n  It is also recommended to give formal definitions regarding transductive and inductive graph representation learning. Especially for the inductive graph embedding, it is unclear that the authors consider inductive node-level task for new unseen nodes of a graph or across multiple graphs (w.r.t. what they conduced in the experiments). The authors should also  verify some constraints of inductive graph embedding, e.g., different graphs can have different node sets, all the graphs must have the same feature dimensionality, etc.\n\n  In the first paragraph of Section 2.2, the authors claim that the latent data ${\\bf{x}}_{I}$ determines the semantic of the observed data ${\\bf{x}}$. What does the 'semantic' of observed data (e.g., in terms of graphs) mean? It is better to give some simple examples to explain the 'semantic' of an observed graph.\n\n  In the 2nd paragraph in Section 2.2, the authors claim that 'the pair of graphs have matched structure and feature dimensions'. What does 'matched structure' mean? From my perspective, one may have ambiguous understandings. It may indicate that the two graphs (i.e., an observed graph and its latent graph) have the same node set (but with different topology described by different edge sets). It can also indicate that the two graphs have the same node set and edge set. Until I read Theorem 1 with the notations $G=({\\bf{A}}, {\\bf{X}})$ and $G_I=({\\bf{A}}, {\\bf{F}})$, I know that it indicate the latter case. It is better to give the formal definitions of the observed graph and its latent graph at the beginning of Section 2.2.\n\n  In Corollary 1 and Corollary 2, ${\\bf{H}}'$ denotes the embedding of 'masked graph', but what is a 'masked graph'? Is it with the same definition as 'latent graph'? It seems there is no definition of 'masked graph' before Corollary 1 and Corollary 2.\n\n  In Section 2.4, what is the definition of ${\\bf{X}}_{(i,J_i^c)}$? Is it a single masked variant of an observed graph? If so, the proposed method only generated one masked variant for each graph. It is not clear how many mask variants are generated for each single observed graph $G_i$. In fact, some details of the proposed method can be clearly presented using the pseudo-code (even in the appendix) but there is no pseudo-code to describe the overall training and inference procedures of the proposed method.\n\n  From my perspective, Fig. 1 does not accurately present the basic ideas of the proposed method. In this paper, the authors consider the representation learning of attributed graph, where each node is associated with a feature vector. However, node feature vectors are not presented in Fig. 1. For the masked graph in the 'Input graphs' subfigure, some of the nodes are crossed. What are the crossed nodes? Are they the nodes selected to be masked? The masking operation is conducted on node features but not both on topology and attributes. The current presentation of the masked graph implies that one needs to first remove the crossed nodes from the original graph (i.e., the masked graph is then with the node set $J^c$), which is inconsistent with the proposed method. Moreover, in the 'Representation' subfigures of Fig. 1, there is the topology of two graphs, which are not the derived representations. In fact, the representations are low-dimensional vectors but not graph topology as presented in the 'Representation' subfigures. Why the graph-level representations of the observed graph and latent graph are denoted as ${\\bf{Z}}$ and ${\\bf{Z}}'$ in bold capital letters in Fig. 1? Usually, bold capital letters are used to describe matrices. From my perspective, they should be presented in bold lowercase letters, i.e., ${\\bf{z}}$ and ${\\bf{z}}'$ as in Eq. (6), which indicate that they are vectors (but not matrices).\n\n  In Section 3.1, the author claim that the proposed upper bounds 'allow an encoder to access a certain level of information of the masked nodes, whose representations can be as good as ones from supervised learning'. I am curious about how the information of the masked nodes is captured by the proposed method and why the derived representations can be as good as ones from supervised learning? It is better to give some simple examples to help the readers better understand this point.\n\n2. Some details regarding the experiments are missing. Some experiment settings also need further clarification.\n\n  There are no statistic details regarding the datasets used in the experiments, e.g., number of graphs, number of nodes, number of edges, dimensionality of node attributes, etc. Details regarding the evaluation protocols for different tasks are also not given. Although the authors claim that they follow the experiment settings of prior work, some important content (e.g., experiment settings) of a paper should be self-contained and do not rely on other papers. Hence, it is recommended to briefly introduce the statistic details and evaluation protocols even in the appendix.\n\n  As I mentioned in the Detailed Comments 1, what does the inductive node-level classification indicate? Does it refer to the (i) classification for new unseen nodes in a graph, (ii) classification across multiple graphs, or (iii) both of them? It is better to give a clear definition of this task.\n  \n  In the graph classification task, how node2vec and sub2vec are used to derive the graph embedding? As I know node2vec and sub2vec can only derive the vector representations for each node and subgraph, respectively. How did the authors convert the node and subgraph embeddings to the graph embedding? How did the subgraphs sampled for sub2evc? The settings of other baselines should also be briefly introduced.\n  \n  Why did the authors use node2vec in graph classification but use DeepWalk in transductive node classification? As I know, node2vec and DeepWalk are two node embedding baselines with similar motivations. Why not use both of them in graph classification and transductive node classification?\n  \n  In Table 2, why did the authors use different evaluation metrics for transductive and inductive node classification (i.e., accuracy and Micro-average F1 score)? \n\n  In Table 1 and Table 2, why some of the results of GL (e.g., on NCI1, PROTEINS, and DD), WL (e.g., on DD and COLLAB), DGK, Node2Vec, Sub2Vec, Graph2Vec, BGRL-GCN, and BGRL-GAT are not given?\n  \n  For the empirical analysis regarding the batch size and number of sampled nodes in Fig. 2 and Table 3, besides the memory usage, what is the corresponding training time w.r.t. each setting? \n\n3. The limitations of the proposed method and possible solutions are not fully discussed in Section 5. A good paper should also comprehensively discuss its limitations in addition to its contributions.\n\n  From my perspective, the proposed method is designed for attributed graphs and relies heavily on the reconstruction of node features. How can the proposed method deal with the representation learning on graphs without any available node attributes? By reading the appendix, I know that the authors tried to apply the one-hot vector of degree for each node as the node attributes. It is not an elegant way for the inductive representation learning across multiple graphs, where the model can only be trained on some of the graphs (i.e., the training set) and be generalized to the other unseen graphs (i.e., the test set). During the training, only the information of training set is given, which indicates that the node degree of the test set is unknown. When the node degrees in the test set have a wide value range but those in the training set only have a small value range, it is possible that most of the node attributes in a test graph are zero vectors. Moreover, what about the graph embedding on a set of regularized graphs without node attributes, where all the nodes have the same degree value? In this case, all the node feature vectors are the same.\n  \n  In addition to reconstructing the node attributes, why not consider another direction of reconstructing the topology? Namely, we can derive an observed graph and its latent graph with different set of edges (but with the same node attributes). The authors can give some discussions regarding this direction.\n  \n  To scale a GNN to large graphs (in terms of the number of nodes) is a hot topic in recent research. The authors also mention that the proposed method can be applied to large graphs. However, as I can check from the descriptions of experiments, the largest graph (i.e., Reddit) used in the experiments only has 232,965 nodes, which cannot be considered as a very large graph. For the reported evaluation results in Table 2, it seems that the authors did not apply any mini-batch subgraph sampling strategies. For large graphs (e.g., with more than one million nodes), most of the GNN structures need certain mini-batch subgraph sampling strategies. How the sampling strategies can be applied to the proposed method considering both the effects of topology and attributes? The authors can give some further discussions regarding this point.\n  \n  Most of the SSL-based graph embedding methods are based on unsupervised training loss and evaluated via supervised downstream tasks (e.g., node and graph classification). I am curious about their ability to deal with some unsupervised downstream tasks (e.g., disjoint and overlapping community detection).",
            "summary_of_the_review": "This paper has good overall presentation of the technical content and sound theoretical analysis regarding the proposed method as well as related work. From my perspective, the proposed method is novel. However, there are some details regarding the problem definitions and experiment settings that need to be further verified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new self-supervised learning framework for learning latent representations of graphs. Like recent SSL methods (BGRL,  Barlow twins), LaGraph does not rely on negative samples but only compares the embeddings of the graph to the embeddings of an augmented version of the graph. To avoid representational collapse, a reconstruction term (similar to denoising autoencoders) is used. They provide a theoretical analysis of the method which provides some connections to other approaches.",
            "main_review": "Strengths:\n* Simple architecture, with good results (Table 1)\n* Theory is provided to justify their method and approach.\n* The authors do a good job at explaining the conceptual differences between LaGraph and prior work.\n* Comparisons with other SOTA methods on a number of graph-level and node-level benchmarks.\n\nWeaknesses:\n* It appears that the BGRL accuracies shown in Table 2 were reported in the original work and were not reproduced by the authors. The evaluation protocol used for LaGraph is different from the one used in BGRL: in BGRL, logistic regression is performed over the last layer of the encoder whereas in LaGraph (Appendix C), it is performed over the concatenation of the original features and the embeddings of the last layer. This is especially relevant for the transductive tasks, where the differences in accuracy are within 0.1% (which lies within the confidence interval). The authors should compare their methods using the same evaluation protocol.\n* In table 2, LaGraph is only better than BGRL on one dataset (PPI). It would be interesting to see how this method provides additional benefits beyond performance since the methods give similar performance.\n* The ablation studies are lacking as well. Given the similarity between LaGraph and BGRL, the ablations and other comparisons should include BGRL to show the performance gains. \n* The assumption that each node embedding is sampled independently from the rest is not fully justified. The theory lacks any discussion of properties of the graph or its connectivity and thus is limited in its impact.\n* While the authors describe the method as a new approach for latent graph prediction, the method is very similar to other predictive methods, especially BGRL and Graph BarlowTwins: the node representations of two views (view 1 = original graph, view 2 = graph with node dropout) are compared. The only difference is in the way the collapse is avoided is through the reconstruction term. Thus the novelty of the approach is limited in this respect. \n",
            "summary_of_the_review": "This paper introduces a new reconstruction-based loss for avoiding representation collapse and theory to justify their choice. In experiments on both graph- and node-level tasks, they show competitive performance with other SOTA methods. While the goal of providing some insights into SSL-based approaches is laudable, their theory is limited in that it doesn't provide any unique information that is graph specific and thus fails to provide insights into graph representation learning. When compared with similar SOTA methods, there is limited gain in performance; thus, it would be useful to evaluate other aspects of the algorithm and make stronger connections to the theory.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "A method called LaGraph is proposed for semi-supervised graph representation learning. In particular, a new task named latent graph prediction is introduced, and the corresponding objective requiring no negativing samples is also derived. It is impressive that the proposed method requires no negative samples and works with a small batch size. ",
            "main_review": "However, the model assumption, that the conditional distribution of the observed graph is centered at the latent graph, is too strong, which is not natural at all. Meanwhile, it seems the introduced objective is just an upper bound, whose tightness should be further discussed. \n\nMeanwhile, as claimed in Section 3, the introduced objective is highly correlated with the existing method, which is good for us the understand the method. However, the difference or superiority of the objective over the existing InforMax should also be highlighted to analyze the method deeper. \n\nAlthough the motivation of eliminating the latent feature matrix F is impressive, the method adopted in this paper highly depends on the approximation of the upper bound, i.e., the number of subgraph N and hyperparameter alpha. This should be declared in the paper and the scenario that the proposed method is not suitable can also be discussed. \n\nThe notation in this paper is not professional, especially the use of subscript. $1_J, X_J, f_J, E_J, H_J; l_n, l_g, l_r$ It hinders me from fully understanding the meaning of some terms. \n\nsome minor concerns:\n\nHow is Eq.12 derived? \n\nwhat does \"Supervised\" mean in Table 2?",
            "summary_of_the_review": "The idea of the proposed method is novel and interesting to me, but the assumption of latent graph about the generation process of graph is a bit strong. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a self-supervised learning (SSL) framework for graph neural networks by predicting an unobserved latent graph. The proposed self-supervised learning loss is derived from upper bounds to the original objective functions for predicting latent graphs. The proposed method is robust to small sample size and small batch size.",
            "main_review": "Strengths\n--\n\n* The paper studied self-supervised learning for GNNs that is less explored than other domains.\n* The authors provide theoretical analyses of the proposed method. The relation between the proposed method and previous work is discussed with theoretical analysis.\n* The memory consumption is much less than a strong baseline GraphCL and works well (e.g., relatively small degradation) with a relatively small mini-batch size.\n \nWeaknesses\n--\n* The proposed method and losses are a simple variation of existing methods. Although the authors present their works as latent graph prediction, it is indeed a simple consistency regularization with noise on randomly sampled nodes. Since the method is proposed as an unsupervised method, the authors impose the consistency regularization on hidden representations, which are the output of encoder (GNNs) rather than predictions/softmax values. \n* Although the relation between latent graph prediction and consistency regularization is interesting, the relation and analyses barely provide intuitions to improve the consistency loss.\n* It is not clear whether the performance gain is statistically significant. \n\nQuestions\n--\n* Do you use stop gradient along the branch for original graphs?\n* Is the proposed method applicable to link prediction?\n* Why do you focus on only masked nodes for the consistency of hidden representations in (5) and (6)?\n* Is the substitution of node features with random noise for randomly sampled nodes more effective than dropNode, which are used in previous work such as GRAND [1]?\n\n[1] Feng, Wenzheng, et al. \"Graph Random Neural Network for Semi-Supervised Learning on Graphs.\" NeurIPS, 2020.",
            "summary_of_the_review": "This paper studies a self-supervised learning framework to perform representation learning for graph neural networks. The authors provide theoretical discussion about latent graph prediction and related domains. Even though many theoretical analyses and derivations are provided, the resulting algorithm is too close to existing methods, and the latent graph prediction perspective makes it more difficult to understand the proposed method. It has strengths with interesting theoretical analyses, but the impact and the novelty of the resulting algorithms are limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}