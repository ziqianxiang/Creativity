{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes to substitute the gradient in the second moment estimation term with the \"momentumized\" version, arguing that it improves both optimization and generalization. Some theoretical results are shown as well as empirical results.\n\nThe paper has been widely discussed by the reviewers and several weak points have been raised. Let me list some of the most important ones.\n- The theory appears to be incremental and overall very weak. The authors themselves acknowledged that this \"is not a pure optimization theory paper\". In details, the generalization analysis is a straightforward extension of Zhou et al. [NeurIPS 2020], while the optimization analysis inherits all the known weaknesses of previous similar analysis in deep learning optimization papers. In particular, *none* of the following is correct: the use of a regret analysis for a stochastic non-convex optimization algorithm, the assumption of bounded iterates, Assumption 5, the assumption in Theorem 2 on $\\alpha_t/\\sqrt{v_t}$. The fact that similar mistakes were done in previous papers does not make them correct: The community should aspire at doing better not at reiterating known mistakes.\n- On $\\epsilon$: the reviewers correctly pointed out that moving $\\epsilon$ under the square root and not changing its value is not fair. The answers of the authors on this point were unconvincing.\n- Doubts on empirical results: it seems that not all the possible hyperparameters of the baselines were properly tuned. For example, despite being common practice, epsilon should also be tuned, see for example the experiments in Agarwal et al. 2020.\n\nI didn't consider the discussion on AdaBelief because only marginally relevant to this paper.\n\nOverall, the paper does not seem interesting from a theoretical point of view and its empirical comparison cannot be fully trusted for the presence of some weaknesses."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed AdaMomentum, which uses the EMA (exponential moving average) of the square of EMA of gradient as sthe denominator, while Adam uses the EMA of the square of the gradient. The authors conducted experiments in CNN, Transformer, LSTM and GAN to show the superior performance of AdaMomentum. The authors also tried to provide theoretical analysis.",
            "main_review": "Strengths:\nI appreciate the authors' efforts to provide both theoretical and experimental validations on the proposed method. The authors try to understand both the convergence and generalization of the algorithm, and the experiments also cover a wide range of neural network architectures and tasks.\n\nWeakness:\nDespite that I appreciate the authors' efforts, I found potential flaws in both theory and experiments, listed below.\n\nTheory:\n\n1. Fig1 as a demonstration of the idea does not make sense to me. It essentially states that AdaMomentum is easier than Adam to be trapped at the 1st local minimum it encounters. But the question is, why the local minimum you encountered early is the global optimal? Give me an example that behaves like in Fig.1, I can easily modify the loss function to the right of $\\theta_{t+1}^{(2)}$ as the global optima, and keep the function to its left the same as in the figure (so that the behavior of Adam and AdaMomentum don't change until they pass $\\theta_{t+1}^{(2)}$). In this case the right minimum is better than the left one, and Adam would jump in the global optima while AdaMomentum will be trapped in a bad local minimum.  \nIn short, which minima is the global minima can be easily changed, AdaMomentum sticks to the 1st one, but the 1st minima can either be a local or global minimum. You only pick the case that's good for AdaMomentum, but ignore the bad case.\n\n2. The generalization part mostly follows [1], however, I think your results contradict [1]. [1] stated that SGD has a **smaller** escaping time than Adam hence SGD can escape local minima and generalize **better**. Your paper stated that AdaMomentum has a **longer** escaping time than Adam but still generalizes better? There's a clear contradiction here.    \nI might miss something with the \"flat basin\" part, I think it's referring to assumption 4. After going over the proof, I think you will have $\\mathbb{E}\\tilde{v} \\geq \\mathbb{E} v$ for both sharp and flat minima, hence AdaMomentum always has a longer escaping time than Adam. At least this implies that AdaMomentum is more likely than Adam to stay at a bad local minimum if they fall into it, this would only hurt the generalization rather than improve it. \n\n3. The proposed method essentially slows down the change of denominator, how does this compare to using a larger $\\beta_2$?\n\n4. The current version of AdaMomentum scales lr up by a factor of $1/(1-\\beta_1^t)$ (see comment Experiment 1 below for an example), which contradicts findings in RAdam that warmup from a small lr to a large lr would stabilize the training. I wonder what's the authors' comment on this?  \n\n5. Discussion on $\\epsilon$. First I don't think the equation above Sec3.2 is correct, it should be $v_t^\\prime=v_t + \\epsilon/ (1-\\beta_2^t)$, as $t\\to \\infty$, $v^\\prime \\to v_t + \\epsilon$. Second the authors confuse $\\epsilon_{Adam}$ with the $\\epsilon$ here, in general $\\epsilon_{Adam}$ is outside $\\sqrt{.} $ but $\\epsilon_{AdaBelief}$ is inside $\\sqrt{.}$, that's why default is $\\epsilon_{Adam}=1e-8, \\epsilon_{AdaBelief}=1e-16$. Considering this, $\\sqrt{\\epsilon_{AdaBelief}} \\approx \\epsilon_{Adam}$, and AdaMomentum uses the same default as AdaBelief, I don't think the arguments by the authors are convincing. The effect of $\\epsilon$ is amplified because you move it outside the $\\sqrt{.}$, but this effect is corrected by using a squared value (1e-16 rather than 1e-8) as default.\n\nExperiments:\n\n1. Toy examples. It's very hard to notice until I checked the code that I found the authors only plot the behavior of the first 30 steps or so. Note that $\\mathbb{E} m_t = (1-\\beta_1^t) \\mathbb{E} g_t$ in general. This implies that, AdaMomentum scales the denominator by a factor of $1-\\beta_1^t$, and the effective stepsize would be larger than Adam by a factor of $1/(1-\\beta_1^t)$. When $t$ is small, this factor is very large (e.g. $t=1, \\beta_1=0.9$, this factor is 10). This means the fast convergence of AdaMentum with few steps is heavily influenced by the large effective learning rate, rather than the $m_t$ vs $g_t$ issue. The authors need to consider this in the statement and experiments.\n\n2. I was surprised to see the authors reported an FID of 12.06, so I ran the code twice, and got 12.68 and 12.80, which is way much worse than reported, even worse than baseline Adam. In case the authors uploaded the wrong file, I would run it again if the authors can re-upload the code (in a single zip file please, from the previous link I have to download files one by one).\n\n3. ImageNet experiments with ResNet18. This is again tricky that the authors switched to cosine learning rate, yet the results by AdaBelief is worse than reported in the original paper using a step lr schdule. I tried ResNet18 on ImageNet with a cosine learning rate, batchsize 4096, 90 epochs, initial learning rate 0.001*4096/512, weight decay 5e-2, and got 72.2 top-1 accuracy (70.45 for AdaMomentum). I'm using Jax with the AdaBelief in Flax, all default hyper-params. The settings are slightly different from this paper, but typically large-batch training is worse than small-batch training. So the ImageNet results look unconvincing to me.\n\n[1] Zhou, Pan, et al. \"Towards theoretically understanding why sgd generalizes better than adam in deep learning.\" arXiv preprint arXiv:2010.05627 (2020).",
            "summary_of_the_review": "The theoretical parts on generalization contradict the literature. The experimental results also mismatch literature, and I got a far worse result on GAN training using the authors' code (could be the wrong version, I can re-run if the authors can re-upload code into a single file). I tend to reject for now and would increase the rating if the authors can resolve my comments above.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This manuscript proposes to substitute the gradient in the second moment estimation term with the momentumized version, and show that it improves both optimization and generalization. Some theory is presented, and there are also some promising deep learning experiments on different tasks.",
            "main_review": "Strengths: the empirical results are good for language modeling, and it is comparable with SGD with momentum on vision tasks.\n\nWeaknesses: my main concern of this manuscript is limited technical novelty and minimal insight. \n\n1. Lemma 1 leverages the tool of SDE to show closed-form formula of escaping time. However, the analysis is almost the same as the Theorem 1 in [Zhou et al. NeurIPS 2020]. One only need to change some definitions in the proof of [Zhou et al. NeurIPS 2020] and then can easily prove the Lemma 1.\n\n2. Assumption 5 is the key to show the improved escaping time in the proof of Proposition 1. It is an unrealistic assumption, it is possible that the RHS is smaller than LHS when $t$ gets large. Please give more explanations. \n\n3. In Theorem 1, the authors require the iterates to be bounded. One cannot simply assume it holds without proving it. \n\n4. In Theorem 2, the authors assume $\\alpha_t/\\sqrt{v_t}\\geq \\alpha_{t+1}/\\sqrt{v_{t+1}}$ to prove the convergence in non-convex case, which is problematic. For example, [Reddi et al., ICLR 2018] provided a non-convergence analysis of Adam and they showed that this inequality does not hold at all.\n\n\n\n\n",
            "summary_of_the_review": "Overall, this manuscript presents an interesting modification of Adam to improve empirical performance. However, the technical novelty is very limited and there is no new theoretical insight. The generalization analysis using SDE comes from [Zhou et al. NeurIPS 2020], and the optimization analysis is based on a problematic assumption pointed out by [Reddi et al. ICLR 2018]. There are also some unrealistic assumptions (e.g., Assumption 5) which needs to be further justified. \n\nFor its current version, it clearly does not reach the bar of top venues such as ICLR.\n",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper study the  adaptive type gradient methods, such as ADAM which scaling gradients by square roots of the running averages of\nsquared past gradients. Nevertheless,  they discover that substituting the gradient in the second moment estimation term with the momentumized version in ADAM can well solve the weaknesses of ADAM which  generalize worse than stochastic gradient descent (SGD) and tend to be trapped in local minima at an early stage during training.Their intuition is that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than that of the raw gradient. Thereby they propose ADAMOMENTUM as a new optimizer reaching the goal of training fast while generalizing better. Convergence guarantee is provided   and   extensive experiments on a wide range of tasks and models is also conducted.",
            "main_review": "ttStrengths \n1. The paper is well written.\n2.The intuition of estimating  second moment by momentum  is interesting.\n3. Theoretical results and experimental result is provided.\n\nWeaknesses \n1. The assumption in Theorem 1 seems wired, where it requires the   distance between any Î¸t generated by AdaMomentum is bounded. I am not sure whether this assumption is a widely accepted assumption or it is a special assumption required by the proposed algorithm. If yes, it will be much better if the authors can clearly state this for clarification. \n2. The assumption in Theorem 2 seems has the same confusion, where it requires alpha_t/sqrt(v_t) > alpha_(t+1)/sqrt(v_(t+1)). Whether it can be satisfied by using the setting in Corollary 2 or it still a assumption is not clear. It will be much better if the authors can clarify this in their paper.",
            "summary_of_the_review": "This paper proposed a adaptive type gradient methods named ADAMOMENTUM  by the intuition that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than that of the raw gradient.  The intuition, theory, and experiments seems OK. Therefore, I tend to accept this paper at this time. I am not an expert at this area correct me if I am wrong, and I am willing to change my score to align with other expert's score if needed and reasonable.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors proposed an improved version of Adam by using a momentumized version of the second moment.\nThe authors justify the proposed modification and give some standard convergence analysis.\nThe authors test the proposed methods in several deep learning tasks and show the improvement over some existing methods. ",
            "main_review": "## Strengths\nThe paper is in general well-organized. The technical contribution seems to be reasonable. I do not carefully check the proofs in the appendix. The empirical evaluations show that the proposed method is robust.\n\n## Weaknesses\n\n### 1.  The presentation in Sec 3.1 can be further improved.\nIt is more clear if the authors can construct a 1D example like Figure 1 and plot the trajectory of Adam and the proposed method. Readers may not fully understand the statement made in the section Benefits of substiuting g with m.\n\n### 2. Sec 4\nThe convergence analysis looks quite standard. The authors should tell readers if the analysis gives a better bound compared to Adam or other variants.\n\n### 3. General non-convex optimization (related to Sec 5.1.1)\nAdam and many diagonal adaptive-gradient methods do not work well for general non-convex functions.\nI wonder whether the proposed method has the same issue. For example, Ackley function and  Rosenbrock function are multivariate (d>1) objective functions. In Sec 5.1.1., the authors only consider the simplest case when d=2. I wonder about the performance of the proposed method and Adam for higher-dimensional (e.g., d=100) Ackley functions and Rosenbrock functions.  \n\n\n  \n\n",
            "summary_of_the_review": "The paper is in general well-organized.  However, as pointed out by other reviewers, the empirical evaluations are not fair and the technical statement could be incorrect.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}