{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper received a majority voting of rejection. In the internal discussion, no reviewer would like to change the score according to the author response. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.\n\n**Motivation**\n\nThe motivation of this paper is not strong. In this paper, the authors claimed that the fairness level of deep clustering methods is relatively poorly compared with the traditional fair clustering methods. The traditional fair clustering methods employ the hard constraints to achieve fairness by scarifying the cluster utility. Instead, deep fair clustering methods seek the trade-off balance between fairness level and cluster utility; therefore, the deep fair clustering can be regarded to use the soft constraints. There is no necessary to compare two different fairness constraints. Even the proposed method is a trade-off balance between fairness level and cluster utility.\n\n**Self-augmented Training**\n\nThe relationship between self-augmented learning and fairness learning is unclear. I guess that the authors added this modular simply to enhance the cluster utility. However, such a loss or an operator can be also applied to other (fair) clustering algorithms. The experimental comparisons in Section 5 is unfair. No ablation study on this is provided.\n\n**Novelty**\n\nOne reviewer pointed out there existed some work that plugs integer linear programming into a probabilistic discriminative clustering model proposed in 2017. \n\n**Experiments**\n\n(1) ScFC and DFCV release their codes; no results of these two methods were reported on HAR. (2) No standard deviation. (3) The Initial ILP Results (Ours) and Ours Result in Table 1 on HAR dataset is 0.653 and 0.468, both higher than the Ground Truth (Optimal) 0.458.\n\n**Presentation**\n\n A few statements are not well-supported, or require small changes to be made correct.\n\nNo objection from reviewers was raised to again this recommendation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for fair clustering (where a clustering is considered fair when each protected group is present in every cluster in the same proportion as in the population) using deep neural networks.  The method works by training a neural network for clustering using the deep clustering with virtual adversarial training approach proposed by (Hu et al., 2017) and then refining (retraining) the network by adding a fairness term to the loss function (where the fairness term is the cross entropy loss between the current networks prediction and the fair assignments obtained using an LP solver on a constraint matrix that enforces the group fairness/balance constraints).  It also presents some experiments that demonstrate the method is more effective at finding high quality fair clusterings than existing deep clustering, fair clustering, and deep fair clustering methods. ",
            "main_review": "The paper is generally clear and well written, and the empirical results are impressive.\n\nThe paper could do a better job distinguishing itself from existing work in deep fair clustering, which also supports multiple-state PSVs (this point seems muddied at times, e.g., the second to last sentence of section two and bullet 2 in section 5 both seem to imply otherwise).  It seems the main advantage of the proposed method over existing methods (beyond better observed performance in experiments, which is motivating in itself) is that the method guarantees a fair clustering.  Unfortunately, it is not very clear how this method is able to guarantee a fair clustering, since fairness is a weighted term in the loss function, and the authors don’t seem to prove (or argue) that the method will converge to a fair clustering (I can certainly believe that it would, but perhaps it would be useful to help the reader see that this is the case), and the authors also do not address what happens when a fair clustering is not possible.  I suppose the fairness guarantee involves using the output from the ILP in the last iteration as the output, but this doesn’t seem clearly specified.\n\nOne limitation of this work seems to be that each person can only belong to one protected class, which doesn’t seem to be the case for (Bera et al., 19).\n\nThe importance of Theorem 3.2 (one of the stated contributions of the paper) is not very clear and could be emphasized.\n\nI found the description of the constraint matrix a little confusing and would imagine a little more clarification could be helpful.  For example, why are 2NK variables used (rather than NK), and doesn’t the number of variables typically equal the number of columns in the constraint matrix?\n\nThe empirical results are impressive, though counter intuitive at times and could benefit from a little additional explanation (perhaps as the result of a bit more exploration).  For example, it would seem intuitive that taking a loss function and adding a term for fairness would cause the result to be worse w.r.t. the original loss function.  While neither accuracy nor NMI are the original loss function, they (especially NMI) seem more closely related than to the fairness term.  The authors note this and claim that fairness provides “positive guidance” for clustering performance, but this does not seem to be an explanation as much as a hypothesis (or, less generously, a re-statement of the observation).  A second example that could probably benefit from slightly more explanation is the observed results when setting the beta hyper parameter.  Why does beta=4 seem to be the sweet spot for all datasets and metrics?  Any intuition for this (if any could be gained) would be interesting to report.  A third example, “we observe the ACC and NMI are decreasing with larger ε”, seems very counter intuitive  (similar to the first example, but perhaps more so) and some explanation beyond “positive guidance” would be encouraging.\n\nMinor comments:\n\n“Fairness takes two primary forms: i) group-level fairness and ii) individual-level fairness.” Please consider citing, I have seen other characterizations of fairness.\n\n“Different geographic regions place this tolerance at different levels.” Consider adding a citation.\n\n“most of these algorithms evaluate their performance on low-dimensional tabular data and mainly study the problems with binary PSV.”  Most of and mainly seem like unfortunate hedges here.  Perhaps its worth pointing out which work evaluate their performance on low-dimensional tabular data and study the problems with binary PSV, and which do not.\n\n“Finally, the base clustering model optimizes the clustering loss lC and lAug simultaneously” Doesn’t (Hu et al., 2017) optimize lC subject to constraints on lAug? (And isn’t this slightly different?)\n \n“Let the rounded version of current assignment Y as Y’ ” I’m not sure I understand what is meant here (perhaps due to a typo resulting in a grammatical error).\n\nThe choice of datasets makes sense (in that they were used in prior work).  Nevertheless, using MNIST for fairness seems somewhat contrived and the results would seem more motivating on more reasonable data.\n\nWhat is ““fnlwgt”?  It might be worth spelling out what the attributes mean/their possible values.\n\nIt seems like an overstatement to claim that by evaluating both accuracy and NMI the study is comprehensive (which means  including all or nearly all elements or aspects of something).\n\n“Note we report both the deep model’s results and the final ILP’s results.”  What is meant here does not seem to be specified anywhere and was confusing to me (and I think resulted in my confusion elsewhere).  I think a sentence or two explanation could go a long way.\n\n“ground-truth and the guaranteed fairness results which are marked with blue”. I guess I know what is meant here, but there is a whole row labeled ground truth and only some of the cells in that row are blue. \n\n“This is particularly important for practitioners who are, for instance, deploying models on the web (where individuals are reluctant to share PSV information)”. If the train data came from a source where PSV information is available and the test data came from a source where it is not (e.g., the web, as described), is it reasonable to assume other changes in the population/resultant distribution are present as well?\n \n“We hypothesize this is because the fairness objective dominates the overall objective so that the impact of clustering objective is hindered” What is meant here?  That training solely on the fairness objective does not lead to good clustering results?  Something else?\n",
            "summary_of_the_review": "The paper is generally clear and well written, and the empirical results are impressive.  Still, there are several opportunities to clarify the details of the approach as well as its advantages, to note its limitations, and to provide intuition for the empirical results (that may be surprising to some readers).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel deep fair clustering framework which addresses the problem about producing guaranteed fair predictions on clustered data with PSVs and making out-of-sample fair predictions for data without PSV. ",
            "main_review": "Strengths:\n1. This paper explores the fairness problem in deep clustering, and presents a general notion of fairness for both binary and multi-state protected status variables, which is proved to be equivalent to optimize the general balance measure for disparate impact.\n\nWeaknesses: \n1. The paper said that it can be fairer in clustering without the sensitive attributes. However, what is sensitive attributes in a data set? The definition is not clear. How do we judge whether attribute is sensitive? The authors think the clustering result is not fair, if the sensitive attributes are used in clustering process. Why do we delete these sensitive attributes and cluster the data set without sensitive attributes? Why doe we need the fairness constraints for clustering task? The authors should provide the clear explanation.\n2. In the proposed algorithm, the authors first get pseudo labels y on a data set with sensitive attributes by a deep clustering model. Furthermore, they use fairness constraints to refine the pseudo labels to get \\hat{y}. Finally, they use \\hat{y} as supervised information to train a deep model. That make me confuse: why do the authors directly add the fair constraints to the objective function or problem of the deep model? Why are the fairness constraints only used as a preprocess of pseudo labels?\n3. In the experiments, these tested datasets do not include sensitive attributes. The authors should provide an application scenario to prove the performance of the proposed algorithm.\n",
            "summary_of_the_review": "The present paper presents a framework to ensure the fairness of deep clustering algorithms. The paper is well organized and the motivation of this paper is clear. However, some definitions and roles about sensitive attributes are not clear. Besides, these tested datasets do not include sensitive attributes. The experiments are not enough. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to combine deep learning and discrete optimization to learn fair representation for clustering. To this end, a generalized notion based on the previous defined balance measure is introduced, based on which an integer linear programming problem is defined and is further added into an existing deep learner for fair deep clustering. ",
            "main_review": "The own contributions of this paper are unclear. The introduced fairness notion is simply a generalization of the previous proposed measure of fairness allowing for protected status variable (PSV) with multi-state instead of binary-state. The integer linear programming formulation is then plug into a probabilistic discriminative clustering model proposed in 2017 for deep fair discriminative clustering. The own contributions therefore remain unknown. It is a stretch to argue that the authors should not use \"we\" as the subjects when describing the base clustering model and others' methods which mystify their own contributions. \n\nRelevantly, multi-state PSV is stated as their learning target while 2/3 datasets are binary PSV. In addition, the improvements over baselines are marginal or no improvement and flexible fairness constraint is not completely supported by the experimental results notably the ACC part. The authors should consider tone down relevant claims.   \n\nThis study is not well motivated either. The drawbacks of existing studies and how these limitations are addressed by this work should have discussed to justify the necessary of this work. The rationals of selecting the base clustering model and evaluation metrics also lack of more detailed discussion. ",
            "summary_of_the_review": "Lack of own contribution, proper motivation and sufficient justification of the proposed method. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a fair clustering algorithm that uses DL models to map the data into deep representations. The authors also show the equivalence between the practical fairness measure and the balance measure. The algorithms can be concluded as two steps: \n1) find fair assignments $\\hat y$ based on $y$;\n2) tune the latent representations and $y$ according to the pseudo-label $\\hat y$. ",
            "main_review": "## Pros\n1) The proposed fair clustering algorithm shows the equivalence between the fairness measure defined in eqn 3 and the balance measure. \n2) The experimental results seem satisfactory, especially the balance measure. \n3) This paper is well-organized and easy to follow. \n \n## Cons\n1)  As Section 4.1 is simply the introduction of the backbone network, maybe it should be an individual section. In my opinion, the major contribution is to use the fair approximation as pseudo-labels. It will be more convincing to conduct experiments on various backbone networks. \n2) In P5, the authors claim that \"Let the rounded version of current assignment $Y$ as $Y'$ then the size of cluster $C_j$ ... \". But what is the *rounded version* (formally)? Does it mean the hard version of $Y$ as $Y$ is a continuous variable?\n3) In Table 1, the balance scores of DEC and IMSAT are 0. It means that some samples from USUS are not assigned to a certain cluster (on MNIST-USPS). It is abnormal since ACC and NMI are still high. Although these two models are non-fair clustering methods, it might be inappropriate to mark the balance score as 0 simply. \n4) There are some typos and inappropriate formulation:\n- P5: \"that *satisfy* our optimal fairness condition\"\n- P5, eqn 8: $*$ -> $\\times$ or $\\cdot$. What's more, the authors should emphasize that $y_i$ is a row vector. \n- P5, eqn 11: the superscript is missing. \n ",
            "summary_of_the_review": "Please see above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}