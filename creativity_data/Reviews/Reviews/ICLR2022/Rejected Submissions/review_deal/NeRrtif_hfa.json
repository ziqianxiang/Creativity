{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This well-written paper introduces an improved exploration strategy by exploiting knowledge about sequences of actions that lead to the same state. The idea is straightforward and easy to understand and apply, which makes it potentially interesting. An important downside is the limited applicability of the method, as there mainly seems to be an advantage in (mostly deterministic) grid-like MDPs. In addition, priors about action-sequence equivalences have to be available. Overall, the contribution of the paper is not deemed significant enough for publication at a top-tier conference like ICLR by the majority of the reviewers as well as myself. For these reasons, I recommend rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers a property of MDPs where multiple sequences of states end up at the same action. The main contributions of this paper are (1) a formalization of the action sequence phenomenon, (2) an algorithm to build a local dynamics graph, (3) a simple exploration procedure based on the local dynamics graph.\n",
            "main_review": "Strengths:\n\n-The paper formalizes an interesting phenomenon of equivalent action sequences that exists in some MDPs. \n\n-The method to construct a sampling policy that maximizes entropy is interesting. It uses a local dynamics graph built using action sequence equivalence knowledge  + convex optimization to  accomplish this.\n\n\nWeaknesses:\n\n-The formulation of equivalent action sequences could be simplified -- although nice, I’m not sure all of the formality is necessary here.\n\n-The method is designed for deterministic MDPs, in which case it is believable that the transitions are known. The authors also discuss an extension to the stochastic case, where equivalences are determined based on next state distributions. This seems nearly impossible in the model-free RL setting. Even in a dynamic programming setup where the stochastic transitions are fully known, it can be difficult to verify equality of distributions. Moreover, even if it were possible to verify, I cannot think of any problem where we’d expect equality of next state distributions between action sequences.\n\n-All experiments are performed on simple domains. It is unclear how applicable this method is in real problems.\n",
            "summary_of_the_review": "Overall, I am not convinced that the method is truly useful in real settings beyond the deterministic + spatial problems that the paper focuses on. The formalization of equivalent action sequences satisfied my curiosity but is perhaps slightly overkill and unnecessary. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the context of reinforcement learning, authors propose an exploration strategy based on environment-specific prior knowledge of action equivalence. An example of such equivalence is rotating 180° twice in a grid world, as the agent comes back to the same original state: the action sequence forms an identity in this case. The proposed exploration, instead of picking a random action with probability (\\epsilon)/(number of actions), builds an abstracted lookahead tree of specified depth that merges equivalent action sequences and adjusts the distribution of \\epsilon over them. Authors demonstrate this i) increases the number of unique state visitations in grid-world examples, and ii) DQN with their exploration strategy achieves a higher reward faster than the baseline. ",
            "main_review": "The paper is written very clearly and the contributions are easy to follow. The idea of using explicit equivalences is fairly simple and I found it surprising this has not been done before (or at least, I couldn't find such a reference). I was thinking about how this work could be meaningfully built upon to improve a general zero-knowledge RL algorithm. I see only a single extension, where the exploration strategies are learned: however this is already studied in prior work [1]. The paper therefore seems to fill a \"void\" in the hand-crafted case with known priors. I don't find this sufficiently significant as its own contribution. Additionally, the applicability seems to be limited to grid-like MDPs, where it is easy to hand-craft the action-equivalences. For more general environments the improvement seems marginal (Figure 7). \nA suggestion how to make the work and empirical results more interesting: perhaps the problem can be cast as a Bayesian update using the hand-crafted priors with meta-learning [1].\n\nOther notes:\n\n- \"Ideally we would like each t-step state distribution to be uniform. However, depending on the exact local-dynamics graph this may or may not be possible.\" Why? Can you give an example when this would not be possible? I believe it should be straightforward to use uniform strategy in the abstracted MDP and translate it back to the original problem. Since this is much simpler than solving the optimization problem, it would be preferrable, and a comparison to the max-entropy solution should be made.\n- Action Space Structure: there is no explanation how the last 3 references relate to the work.\n\n\n[1] Meta-Reinforcement Learning of Structured Exploration Strategies, Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine\n",
            "summary_of_the_review": "I recommend 5) marginally below the acceptance threshold. There is existing literature that tackles the problem of exploration in a more general way without handcrafting prior knowledge into the problem.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method that, from a simple encoding of sequences of actions that have equivalent outcome in an MDP, allows to compute a local policy for local high-quality exploration (it replaces the random action of $\\varepsilon$-greedy with an action that maximizes the entropy of future visited states). Full algorithmic details about how to do that are given in the paper, and experiments on a few environments show that the method is promising. The Freeway experiment is particularly interesting, as it shows that a (small) benefit can be obtained from the method even on an Atari game, with minimal domain knowledge (only a few equivalent action sequences are encoded), and using a slightly modified DQN algorithm.",
            "main_review": "The idea proposed in the paper is quite elegant, albeit its realization is quite complex. The paper does a great job at explaining all the steps that go from equivalent sequences of actions, to an actual local policy that can be queried for an exploratory action. Overall, the paper is well-written, even if its complexity is sometimes a bit difficult for the reader.\n\nI think that the paper would greatly benefit from a quick summary of what it does. In the introduction, the last paragraph gives a bit of guidance, but already a bit too technical in my opinion (\"Then, we show that priors on this type of structure can easily be exploited during offline exploration by solving a convex optimization problem\") for instance. I would put, either in the introduction or in a dedicated \"Overview\" section, an intuition of the steps that the algorithm takes:\n\n- Assume that we have sets of equivalent actions sequences for the environment. Equivalent action sequences are sequences that lead to the same state (please correct me if I'm wrong)\n- These sequences are used, from a current state $s_t$, to build a DAG, that sorts of models where the agent will end up after any sequence of actions of length $d$. Because some sequences are equivalent, several parent nodes may share a child node. This is the information that we want to extract.\n- In a Q-Learning-like algorithm, every time-step, construct the DAG above for $s_t$, then, with probability $\\varepsilon$, execute an exploratory action $a_t$ that maximizes the entropy of the future visited states.\n\nA further intuition about the fact that the resulting exploratory policy will, basically, assign low probabilities to actions that share a same future outcome, may also help the reader understand why the proposed algorithm is beneficial to exploration.\n\nSomething else that could be made clearer is when the state-space has to be discretized. It seems that the DAG nodes don't consider states, but it is not that clear to me, and I have the feeling that explicitly mentioning what happens to the states in the Freeway example may help.\n\nOther than that, I think that the paper is highly novel and proposes a complicated (but justified) algorithm. As such, I recommend accepting it.",
            "summary_of_the_review": "The paper is relatively well-written, and proposes a sound algorithm that leads to good empirical results (even if the amount of empirical results is quite low). The algorithm is general enough to be applicable to an Atari game, with minimal engineering effort. The approach is quite novel, and multi-disciplinary (RL, information theory, convex optimization). As such, I think that the paper will be very interesting to people attending ICLR, and recommend accepting it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}