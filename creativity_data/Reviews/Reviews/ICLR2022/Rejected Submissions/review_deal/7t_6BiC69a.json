{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper provides a deep learning technique aimed for tabular data via a unified view of factorization machines and other DNN approaches. The reviews are overall positive when discussing the provided technique, the motivation behind it and the writing. However, there are major concerns related to the experiments. The most dominant one is that of significance, meaning the advantage of the provided method when compared to existing literature. Other claims such as unclear details or different methods of reporting might be possible to resolve via minor edits, but this concern was not resolved in the rebuttal period. Before the paper can be published in a venue such as ICLR, it should provide a clearer comparison against previous works showing exactly where it improves upon them. At its current state, it doesn’t seem to be ready."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a general neural network framework for tabular data classification, that can recover a range of popular classification methods. The main components are per-field representation by factorized network and an activation function on aggregated representations of all fields. Experiments on general tabular classification and CTR prediction demonstrate its effectiveness, although performance gain is marginal.   ",
            "main_review": "This theoretical connection between FM-based models and field wise neural networks is impressive. The generalization next to it is convincing and solid.\n\nThe line \"M and is made symmetric\" should be typo. The equivalence of upper triangular M and symmetric M should be stated explicitly.   \n\nThe experiment shows quite marginal performance improvement. Do the authors have comments? This paper may pose new directions to improve current popular models given the theoretical connection. Any preliminary thoughts? the conclusion section doesn't include next steps. Increasing lowered rank in factorization can be explored. \n\nIn Appendix A, it said \"FM, FwFM and FmFM, learn C vectors for each feature\", should be \"FM, FwFM and FmFM, learn C vectors, one for each feature\"?",
            "summary_of_the_review": "The paper provides new insights on a unified neural network framework that can cover a range of popular tabular classification methods. The derivation is dense but solid. Although performance gain is marginal from experiments, the framework itself deserves more attention. It would be even better to pose new directions to improve models further.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a neural network architecture, inspired from factorization machines, for classification task from tabular data. In particular, the proposed factorized neural networks (F2NN) leverage the capabilities of shallow networks for learning factorized low-level representations that can approximate (the typically) high-dimensional representations of each field. This,in turn, together with different choices of activation factions, provides a generalization over different factorization approaches, and can lead to good (albeit not always better) performance on different classification tasks. Results from several (16) experiments on binary and multi-class classification tasks and two CTR tasks, comparing F2NN to different benchmark algorithms, show that it has comparable performance to some of standard (SOTA) methods. ",
            "main_review": "The paper, in general, is well written, however the structure (in particular the RW and problem setup) can be improved.  The contributions, especially on studying and investigating the variants of F2NN (wrt architecture and activation functions) are interesting and seem solid. \n\nstrengths: \n- an elegant method \n- well documented ablation study\n- well written paper\n\nweaknesses:\n- weak experimental setup\n- weak (and potentially misleading) discussion of the findings\n\n--\nThe major concerns I see, relate to the experimental setup wrt the comparisons to other methods and the reporting of the results. Namely, the authors report comparisons (on the classification problems) to several benchmark methods, typically applied to these tasks, but in reality they use results from a previous paper. It seems that none of the other methods have been tuned nor their hyper-parameters reported. Many of these benchmark methods are sensitive to their parameters, and their performance may improve significantly when properly used. On the other hand, F2NN, and one of the methods (SAINT) seem to be properly evaluated. This may lead to over-optimistic results, and misleading conclusions. \n\nAnother issue is how these results have been reported. The authors report mean performance, averaged over all 16 tasks (without reporting even std). This can also be misleading, since these are tasks with different properties, and the performance of different methods may substantially vary between them. IMO, analyzing and reporting them separately, not only will be more correct, but it also may provide other insights related to the performance of F2NN (which is given in Appendix C, but not discussed). Moreover, while I enjoyed the ablation study, the GELU adaptivity experiments seem a bit incomplete. The authors simulate the experimental setting reported by Rendle (2010), which is ok to begin with, but IMO should have investigated this further wrt other embedding-sizes and architecture designs (since one of the contributions is the GELU adaptivity).\n\nThere are several 'deep' FM/FFM approaches highlighted in the related work. On the other hand, the authors evaluate on some such methods (implemented within a public repo) but never discuss nor connect these two. Are some of the methods discussed (eg DeepFM) used in the experiments (eg. DeepFmFM)? If so, which ones and can you provide more details on them? Also for instance the one of Guo et al (2017) seems to be (partially) approximated by F2NN, but not really evaluated. How does F2NN compare to it? \n\nThe related work can also be improved in order to better serve the motivation for the work. Namely, personally, I don't see how the 'critiques of DL' paragraph fits, since this work doesn't seem to really support/nor dispute the stated claims. On a similar note, IMO the relation to Qi et al (2017) in the last paragraph, as it reads now - seems a bit of a stretch. \n\nConsidering, the 'devil is in the details' section, there is a statement regarding the benefits of the network depth for transformers. It is unclear, whether the referred experiments (that dispute this) concern F2NN or the transformer-based models, since it seems SAINT was tested with the same architecture, and TabTransformer not evaluated.",
            "summary_of_the_review": "The paper, in general, is well written, however the structure (in particular the RW and problem setup) can be improved.  The contributions, especially on studying and investigating the variants of F2NN (wrt architecture and activation functions) are interesting and seem solid. However, I have some concerns regarding the experimental set-up as well as the discussion of related work, which are reflected in my score. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- The authors proposed a novel deep learning model for tabular data classification.\n- The authors utilize the field-wise factorized networks to extract the meaningful information from both categorical and numerical features of the tabular data. Then, using those embeddings to solve the classification problem.\n- The experimental results showed that the proposed method achieved comparable performances in comparison to SOTA deep learning models for tabular data (SAINT).",
            "main_review": "1. Experimental results\n- Based on Table 1 and 8, the overall performances are very similar with SAINT.\n- Definitely, the performances are not \"statistically significant\" better than SAINT.\n- What are the main advantages of the proposed method in comparison to SAINT? It seems like in terms of the performances, the advantages seem very marginal and we cannot claim that the proposed method is better than SAINT.\n- Also, based on Table 1, it seems like the differences from XGBoost and GBM are also marginal. In that case, are there any advantages to using deep learning models instead of those traditional models?\n\n2. Standard deviations\n- It is always good to show the standard deviations of the performances.\n- We can see it in Table 1 (left) but not in Table 1 (right).\n- Based on the marginal performance improvements, it would be more important to demonstrate this standard deviation.\n\n3. Figures.\n- It would be good if the authors provide a figure that overview the proposed networks.\n- As the authors mentioned in the discussions, it would be more informative if the authors show some tSNE analyzes about the learned representations.\n\n4. Intuitions\n- The authors showed various results and ablation studies to discover the rules of thumbs for different cases.\n- However, the intuitions for those conclusions are not enough. For instance, why is wide more effective than deep?\n\n5. Extensions of the model\n- It seems like the outputs do not need to be a categorical variable.\n- In that point of view, can we extend this method to the regression problem?",
            "summary_of_the_review": "Strength:\n- The authors proposed a novel and well-grounded deep learning model for tabular data classification problem.\n- The authors provided extensive experimental results to understand the performances of the proposed method.\n\nWeakness:\n- The experimental results are not that promising. The performances are similar (not noticeably better) than SOTA. If the authors can claim the unique advantages of the proposed method in comparison to SOTA, I think I will increase my scores.\n- The intuitions of the proposed method are limited. \n\n-----------------------------------------\n\nThank you for the detailed answers from the authors.\nI carefully read the authors' responses and other reviewers' reviews.\nOverall, most reviewers are concerned on the weakness of the experimental results. This is also my main concern about this paper.\nAlthough the authors claimed that the proposed method is \"faster\" than another baseline, it does not mean that this method is \"faster\" than other alternatives (we can easily find other traditional methods that are faster than the proposed method with similar performance).\nFor XGBoost and GBM, the authors said that those are not suitable for CTR prediction due to the high cardinality, However, the authors did not show how they are working in those kinds of datasets in the classification problem. If that claim is valid, it would be much better to show some datasets with that characteristic and show the failure of XGBoost and GBM.\nBased on these, I am going to stay on my original score (5) because the performance concerns are not well resolved. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes an approach for tabular data classification.\nIn particular, authors proposed the Fieldwise Factorized neural network to tackle the problem defining a general framework in which it is possible to express other tabular classification methods.\nThe proposed approach permits to manage categorical and numerical fields and seems to outperform other methods (on average). Moreover the experiments seem to demonstrate the importance of per-field network and the improvement in the results using GELU instead of the ReLU of Quadratic activations.",
            "main_review": "The authors present an interesting framework useful for tabular classification task. \nThey have compared their method with other classifiers in a fair way, with the experiments that are well designed and well described with the right amount of details useful for the reproducibility (I hope the authors share their code). \nMoreover, the ablation studies are very interesting and seem to confirm the importance of the per-field network.\n\nThe results of the proposed approach are quite good even though it is difficult to be sure on which model is better because the results are really near each other and there isn't a difference statistically significant. \nNonetheless, the experiments have shown some interesting insights and the importance of some design choices.\n\nMy main concern is about the formalism they have used in the description of the method. I think that a revision of the math and formalism could make the work more readable and reproducible. \nIn the following some details.\n\nThe formalism introduced in Section 2 is not completely coherent and in some parts misleading. \nThere is not a different text formatting used for the scalar values and vectors (for instance bold for vectors). This good practice is instead followed in some parts of Section 3.\nJust for example (but all the section 2 and 3 should be revised carefully in order to make the paper more rigorous): \n- $x_f$ is a vector and $x_i$ is a scalar. \n- $f$ is used to indicate the categorical features but also to indicates the generic field (in definition of $S_f$ for example). Probably it is better to use F to indicate the generic field.\n\nOther comments:\n- In §2.2 insert a reference to the Appendix where $M$ is appropriately described.\n- In Equation 2 the authors introduce also $x_i$ and $x_j$ even though the previous defined factorization don't mention them. The authors should describe the motivation and the validity of this operation.\n \nSome minor typos:\n- Page 7: XGBoost Chen & Guestrin (2016) --> XGBoost (Chen & Guestrin (2016))\n- Page 8: immaterial --> do you mean not significant?\n\nSome doubts:\n- $E_f$ is the field embedding matrix, and the embeddings are the rows (as described in §2.1). Before the Equation 4 authors stated that: $E_fx_f$ is the embedding of the one hot vector $x_f$ but, if $x_f$ is column vector, it should be $E_f^T x_f$. If I'm right the authors should carefully revised all equations accordingly.\n- Equation 1 presents four Factorizazion Machine Models. If I understood well, only FmFM has been tested (probably is the best model among others). If it is correct the author should be clarify this point. Moreover, in Table 1 it is used the name FmFm but in other parts it is indicated as FmFM (also §4.1.2). \n\nMissed references:\n- [1] Yuanfei Luo et al. Network On Network for Tabular Data Classification in Real-world Applications 2020\n- [2] Zhibin Li et al., Field-wise Learning for Multi-field Categorical Data, 2020\n\nThe [1] and [2] present two approaches to tackle the same problem. Moreover [2] seems to use a similar approach using a field-wise network. Could the authors describe the differences? Moreover, the models in [1] and [2] are not tested, why didn't they tested also these two methods? Do the other tested methods outperform them?  ",
            "summary_of_the_review": "There are some interesting aspects and the insights are supported by a good experimental set. In my opinion, with a careful revision of the formalism in the description of the method, the paper can be accepted for ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}