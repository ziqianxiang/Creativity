{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "I thank the authors for their submission and active participation in the discussions. The majority of reviewers have concers with this paper, in particular, regarding the motivation of the method [dgHr], clarity [Mgm9], and theorethical support [4ENc]. I side with reviewers 4ENc, dgHr and fFaW, and recommend rejection of this paper. I want to encourage the authors to use the feedback by the reviewers to improve their paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a linear combination of reward shifting to improve the performance of value-based reinforcement learning. Using the equivalence between reward shifting and a new initialization for the Q-value function approximation, this work shows that the optimal policy will not change and the method can achieve better results compared to no reward shifting.",
            "main_review": "Advantage:\nWith the idea of reward shifting, this paper considered different reward shifting schemes for a lot of different scenarios such as offline RL, online RL, and curious-driven RL. The intuition of using pessimistic Q-value for offline RL and optimistic Q-value to encourage exploration is easy to understand.\n\nSome questions:\n1. Is constant reward shifting can be regarded as solving a different MDP or is actually a way to design reward but not a more efficient way to accelerate the Q-value convergence? \n2. It seems to minus/plus a constant reward to the Q-estimates can be seen as using pessimistic penalty in offline RL or optimistic bonus for exploration in online RL. The previous works designed different values of the bonus $b(s, a)$ corresponding to different $(s,a)$ pairs, which may be more efficient than adding a constant reward for all the Q-values? However such kinds of baselines seem not to be covered or even discussed? I'm not sure whether they are related such as [1],[2].\n3. It is claimed in Section 2.1 that this paper considers off-policy cases? So will the data be generated under a fixed behavior policy or generated conditioned on some Q-values?\n4. It is a little bit counterintuitive why changing the initialization of Q-value will bring benefits for the algorithms. Maybe the biggest change brought by reward shifting is to change the landscape of the Q-value (or say the optimal Q-values after changing the reward) but not the change of the initialization? If that landscape and all the local and global minima of a loss function does not change, changing the initialization helps just means the algorithm is not robust which depends a lot on the initialization?\n5. Will the initialization of Q-values be too conservative or optimistic which may hinder the performance of the algorithm? And how to choose the reward shifting magnitude?\n6. It seems a lot more baselines involving other kinds of pessimistic or optimistic additional terms instead of constant reward shifting will be more convincing?\n\n[1] Jin, Chi, et al. \"Is Q-learning provably efficient?.\" arXiv preprint arXiv:1807.03765 (2018).\n\n[2] Rashidinejad, Paria, et al. \"Bridging offline reinforcement learning and imitation learning: A tale of pessimism.\" arXiv preprint arXiv:2103.12021 (2021).",
            "summary_of_the_review": "In my opinion, this work is slightly below the acceptance threshold. But I'm happy to change my score if the problems are addressed well.\n\nAdvantage:\nIt formulates the addition of a pessimistic penalty or an optimistic bonus when learning Q-value estimations as a reward shifting, which is may of independent interest.\nTargeting different settings in RL, it designs different algorithms using reward shifting.\n\nDisadvantage:\nI'm not sure whether the intuition that changing the initialization of the Q-value network is the main reason why the algorithm with reward shifting outperforms other baselines.\nIf we see reward shifting as an extra term added for Q-values, there are some other ways instead of adding constant to do that, while there are no baselines about that is involved.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the effectiveness of reward shifting in value-based deep reinforcement learning. Particularly, it points out that (1) a positive reward shifting is equivalent to pessimistic initialization of Q values, thus, leading to conservative exploitation; and (2) a negative reward shifting equals to optimistic initialization and hence leads to curiosity-driven exploration. By leveraging these insights, the paper proposes to modify existing RL algorithms by simply adding reward shifting to improve their performance. Empirically, experimental results showed that reward shifting helps improve the baselines (or the vanilla algorithms) under various scenarios, including offline RL, online continuous control and online value-based curiosity-driven exploration. ",
            "main_review": "Strengths: \n+ Overall, the paper is technically sound and the major claims are well supported with experimental results. \n+ Various settings and scenarios are considered in the experimentation, including offline RL, online RL with continuous and discrete action spaces. \n+ Related work in different domains, offline and online RL, is adequately cited.    \n\nWeaknesses/Concerns: \n1.\t[Clarity] Some parts of the paper are not clear enough in my opinion. Please find the list of items below for details. \n-\t1.a. It is not super clear to me how the Humanoid example in Section 3 connects to the major points made in this paper? Turning off the alive bonus is only applicable and effective for selective states, while the reward shifting is universal across all state-action pairs. To me, it only seems that the reward function for Humanoid is kind of problematic and is not designed well. Thus, it seems a bit off as a motivating example for reward shifting. Additional elaboration could be helpful here.  \n-\t1.b. It is important to clarify the sign of parameter $b^-$ in the revision. Section 4.3 states that $b^-$ is a positive constant, while Figure 3 and Equation (6) should interpret $b^-$ as a negative constant I believe. Please make sure the definition of $b^-$ is consistent throughout the paper including Appendices. \n-\t1.c. It would be helpful to clearly state, for example at the end of the first paragraph under Section 4.3, that 4.3.1 is for continuous control and 4.3.2 is for improving curiosity-driven exploration with RND, so that the readers may be clearer that these two subsections are talking about different scenarios with their corresponding designs.    \n-\t1.d. I had an impression that all the Q-functions should be seen as function approximators with neural networks. If that is the case, the statement from Section 4.3.1., “For under-explored state-action pairs, both $Q_A$ and $Q_B$ will give near-zero predictions as a consequence of initialization”, is not true to me. Even for unseen state-action pairs, the Q-network would extrapolate based on observed data, and the Q-values would not stay near-zeros for all under-explored state-action pairs. To be more rigorous, I encourage the authors to state this as an assumption in the paper or specify that this part of analysis only applies to tabular Q-learning settings.   \n-\t1.e. The statement made in Proposition 1 is not clear enough. Appendix A definitely helps but I would still suggest elaborate a bit more in the main paper, especially for this part “combining the linear combination in Equation (6) is equivalent to using a linear combination of the constants”. Accordingly, it would be very helpful if the authors can describe more about the Random Reward Shift (RRS) in Section 4.3.1 or in Section 5.2. Again, I understand RRS much better by looking at Algorithm 1 in the Appendix, and I suggest make clearer connections between the algorithm RRS and Section 4.3.1, e.g., Equation (6) and how 4.3.1 leads to the design of RRS?  \n2.\t[Experiments/Baselines]  \n-\t2.a. How many seeds were used in the experiments Figure 4 - 6? I don’t think it is stated in the main paper? \n-\t2.b. To my understanding, RRS is training K = 3 different Q networks, each trained with reward shifting $b_k$, $k = 1, 2, 3$. Are these three shifting constants $b_1 = 0, b_2 = 0.5, b_3 = -0.5$ or they are sampled from $[0, 0.5, -0.5]$ randomly? In addition, it seems to me that RRS is similar to the ensemble of Q-networks with randomized additive priors [1]. Therefore, it is interesting to see how RRS would compare with [1] (or Ensemble TD3 + Randomized Prior Functions) when $K = 3$. It would be an interesting baseline to include in the paper. \n\nMinor comments:\n-\tRemove the line break right below Figure 2. \n-\tMake sure the sign of $b^-$ is consistent in the paper. \n-\tSection 4.3.2, the penultimate paragraph, I believe it should be $I = max_{s, a} |A – B|^2$, the square is missing.    \n\n[1] Osband, Ian, John Aslanides, and Albin Cassirer. \"Randomized prior functions for deep reinforcement learning.\" arXiv preprint arXiv:1806.03335 (2018).\n\n",
            "summary_of_the_review": "The paper provides helpful insights to better understand the effectiveness of reward shifting in deep reinforcement learning. Major claims are well supported by empirical experiments in various domains, including offline and online RL. My main concerns are about the clarity of the paper, which I believe needs to be further improved, and additional baselines (please see the above Concerns for details). Therefore, I vote for a weak reject for the current submission. I am willing to adjust my scores should my concerns be addressed in the rebuttal period.  \n\n\n=== Updated ===\n\nMy major concerns were addressed during the rebuttal period, so increased my score accordingly.      \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies how different reward shifting (to be precise, adding a bias term on the reward) affects value-based RL algorithms. It illustrates the idea that different reward shifting corresponds to different initialization of the Q networks. Though linear reward shifting does not change the optimal solution for Q, the initialization does affect the learning process and might lead to different stationary points, when equipped with function approximation. The paper further studies the idea in two settings: (1). offline RL, which shows a position bias term could assign the $(s,a)\\in\\mathcal{D}$ a large value which helps conservative exploitation while in (2) online RL, a negative bias term could facilitate exploration. Some empirical results verify the ideas.\n\nReward shifting techniques have been studied in the bandit literature, this paper studies how it affects the value-based RL algorithms with function approximations. This is an interesting and important topic, and this paper provides some intuition and empirical results to help the community systematically understand the effects of reward shifting on both online and offline settings.",
            "main_review": "Strength:\n- I do like the topic this paper studies, as I believe it is also what the deep RL community observes, that reward shifting does have an impact on the final learned policy. \n- The paper discusses the reward shifting from both online and offline settings, which is interesting to see how positive/negative bias affects exploration and exploitation. \n- Empirical studies in both online and offline setting verify the effectiveness of the reward shifting.\n\nWeakness:\n- The main argument in this paper is that reward shifting corresponds to different initialization, and pessimistic or optimistic initialization does affect the final learned policy, which is known to the community for a while. However, I think it lacks some intermediate important steps in the paper, such as how the initialization affects the learning dynamics? It seems all the arguments in the paper intuitively make sense, but lack strong theoretical support.\n- The experiments though illustrates how the positive/negative bias helps in online/offline setting, which is well-known. A more important question is how to determine an appropriate bias in different tasks and environments. As shown in Appendix B.1, the magnitude of the bias does affect the learning behavior and outcome. It would be great to discuss this in a more principle way, instead of saying something like, $r+8$ seems work. \n- Also, the author aims to study the linear transformation in the reward shift, but just ignores the scaling factor, which also seems have some significant effect in the learning process. It would be great to investigate more on this.",
            "summary_of_the_review": "I like the topic this paper studies. However, the main argument in the paper is intuitive and seems well-known to the community. Beyond some empirical validation, I do not think it gives enough insight on how different initialization affects the learning dynamics and final learned policy. Also, there are important questions left, such as: (1). How to select the optimal bias $b$ to facilitate the learning process and (2). Does the scaling factor matter, empirically? Though the topic is interesting, given the current status of the paper, I would recommend a reject and I believe it would be a valuable paper if the authors study more deeply on the underlying causes of reward shifting.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper shows a key observation that the linear transformation of reward shaping is equivalent to diversified Q-value network initialization. Based on such observation, the paper presents three scenarios where the reward shifting can benefit: the offline conservative exploitation, the online sample-efficient RL, and the curiosity-driven exploration. The algorithm is evaluated on the continuous control tasks and discrete control tasks.",
            "main_review": "Strength: \nThe strengths are summarized below:\n\n1. This paper tries to unify optimistic exploration and conservative Q learning through reward shaping, the idea by itself is very interesting.\n2. The paper provides several nice plots to help understand the algorithm.\n3. The experiments are pretty solid including various settings (e.g., offline/online, discrete/continuous) and environments.\n\nWeakness:\nI would like to raise several questions to the paper: \n\n1. In Sec.3 the motivating example, the design of the alive bonus is not just to provide the curriculum. Such design may also have the practical effect of preventing the robotic to break itself. For example, falling down can cause the robotic to break some of its components. In addition, since the reward is shifted, I am not sure how to read the curve in the figure. Is the orange curve representing the shifted reward and the blue one is the unshifted reward? If that is the case, they are not directly comparable. Thus, I think this moving example is problematic to some extent and needs more explanation. \n\n2. In Figure. 2, it seems not correct that the initialization $\\tilde{Q}_0$ is always smaller than $Q^*$. This would be interpreted as always having a pessimistic initialization of the $Q$ function. \n\n3. In the same figure, the labeling: $|Q_t - Q^*| < |Q_{t-1} - Q^*|$ seems to be very strict and cannot be achieved in practice. Even we are doing optimization over the $Q$ function, I don't think it is a practical condition.\n\n4. Same problem in Sec.4.2, the author states \"the near-zero initialization guarantees the prediction is lower than the optimal Q-value, thus\nconservative exploitation can be conducted with such a value function\". Which I think is not valid, a simple negative reward case would cause this 0 initialization to be optimistic rather than conservative.\n\n5. In the optimistic/pessimistic in face of uncertainty criterion, the algorithm design is always state-action dependent. Thus, I am having trouble understanding just adding a uniform bias can achieve such criterion, since it is not a function of state-action at all. \n\n6. In the experiments, I would particularly like to see how the authors choose the value of bias. My feeling is that it will be very sensitive to the magnitude, but such magnitude is environment-dependent. I think adding an ablation on a larger range of values will help clarify this.\n\n7. In addition, some of the other optimistic initialization/exploration designs are not sufficiently discussed. Such as https://arxiv.org/pdf/2002.12174.pdf.\n\nClarity: The paper is well-written and clear in the flow. In addition, the figure is pretty straightforward to illustrate the point.\n\nFeedbacks & Questions: Please see details in the weakness.",
            "summary_of_the_review": "Overall, I am still having some concerns about the claim of the paper. These concerns are pretty important and thus I encourage the author to engage in the discussion period and clarify these if there is any misunderstanding. I am happy to re-evaluate if the author convinces me. Based on my current evaluation, I don't recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}