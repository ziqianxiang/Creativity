{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies efficient algorithms for distributional reinforcement learning. The motivation stems from the need of risk neutrality, since other existing approaches might have one-sided risk tendencies. The algorithms proposed in this paper are based on sampling from a distributional perturbation rather than using optimism in the face of uncertainty. Both theoretical guarantees and empirical results have been provided to validate the effectiveness of the proposed algorithms. While this is certainly an important and interesting direction, I agree with the reviewers that it is unclear from the theory in this paper why distributional perturbation is helpful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel distributional RL algorithm that is neither risk-seeking (exploratory) or risk-averse (exploitatory) -- but rather risk-neutral. The paper motivates the need for risk neutrality from the family of works applying OFU (optimism in the face of uncertainty) to distributional RL -- it states that prior works in this space induce a one-sided risk tendency (risk-seeking or risk-averse) which is undesirable as it leads to “biased exploration”. An N-chain environment is initially studied where it is shown that prior works (DLTV, QRDQN)  are not able to identify the optimal action as fast as the proposed method (PQR) -- the prior works are misled by the low probability high reward side of the N-chain environment as opposed to the optimal side which provides maximum averaged reward. The paper then proposes a method for perturbing the risk-measure in risk-sensitive RL such that the resulting perturbed distributional bellman operator (PDBOO) converges to the same fixed point as the standard bellman operator (with some weak assumptions) and hence, produces a risk-neutral policy. The paper presented results in the LunarLander-v2 and Atari domain (4 Atari games selected), demonstrating that prior works with one-sided risk tendencies do not perform as well as the proposed risk-neutral method (PQR).",
            "main_review": "## Strengths\n\n\n1. The proposed method is a novel application of results from DRO (Distributionally Robust Optimization) literature to formulate a perturbed bellman optimality operator (PDBOO) that is theoretically well-behaved -- it is shown to converge under a “weak contraction” property and to the same unique fixed point as the standard bellman operator.\n\n2. The pedagogical N-chain environment setting clearly demonstrates the benefit of the proposed risk-neutral method in converging to the optimal action despite the existence of sub-optimal actions that have a low probability of higher reward. In the LunarLander and Atari environments, a similar result is obtained with baselines including DLTV, a perturbed variant of DLTV and QR-DQN.\n\n## Weaknesses\n\n1. The paper is empirically weak, demonstrating results in only 4 Atari environments. It would be good to see a full empirical analysis of all Atari environments. The paper would also benefit from diversity in the choice of environments -- given that it proposes risk-neutral exploration, what about continuous control simulation tasks such as locomotion or manipulation (in MuJoCo for example), or the b-suite family of environments (https://github.com/deepmind/bsuite)?\n\n1. The paper has issues with writing. Grammatical errors are prevalent throughout the paper and the related works, motivation and presentation of the proposed method all seem to be lacking in clarity.\n\n",
            "summary_of_the_review": "Overall, the paper provides valuable theoretical contributions in the form of their analysis of the perturbed distributional bellman optimality operator (PDBOO) and its successful application in toy environments (N-chain) as well as some non-trivial ones (LunarLander, Atari). However, the paper lacks in quality of writing and empirical analysis.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel strategy for performing exploration based on distributional reinforcement learning. They highlight the fact that existing approaches cannot distinguish between epistemic and aleatoric uncertainty. Then, they propose an alternative approach that leverages a random perturbation to the distribution, but then takes this perturbation to zero. They prove that this approach converges. Finally, they validate their approach in their experiments.\n",
            "main_review": "According to the authors, the shortcoming of existing approaches is that they do not separate epistemic and aleatoric uncertainty. This problem appears well motivated (though their example is a bit simplistic; see discussion below). However, I’m not sure I understand how the proposed approach solves the problem. As far as I can tell, their main strategy is to use sampling from a distributional perturbation rather than optimism. The main reason their approach converges appears to be the fact that they require that the size of the perturbation goes to zero over time. Wouldn’t taking the optimism to zero over time achieve the same effect?\n\nFurthermore, their example in Section 3.1 is for a toy algorithm; it is unclear if existing techniques suffer from this issue (though I suspect that they do). As discussed above, it also appears that their algorithm primarily avoids this issue by taking \\Delta_n to zero over time; in their example, it appears that you can achieve the same effect by taking c_t to zero over time.\n\nAlong the same lines, their theoretical guarantee does not appear to be very strong. For example, as far as I can tell, it applies to traditional Bellman operator (i.e., the one that does not perform any exploration); indeed, this is the case \\Delta_n=0 for all n. Thus, their theoretical analysis primarily results from the fact that they are taking \\Delta_n to be small sufficiently fast, rather than from any feature of their particular exploration strategy. I would have expected the theory to support the benefits of using their exploration strategy instead of an alternative.\n\nFinally, their experiments also appear to be fairly weak. They only compare to a limited number of baselines; for instance, why not compare to prior approaches such as Keramati et al. 2020? Furthermore, they do not appear to outperform the baselines by very much in most of their empirical results.\n\nMinor comments\n- Does Theorem 4 apply for any choice of U_\\Delta in Theorem 4? This is not clear.\n- The proposed algorithm feels very similar to Thompson sampling; I wonder if some connection can be drawn here.\n- The authors do not clearly distinguish that their goal is to use distributional RL, but eventually learn a risk-neural policy. Several of the papers they cite have the goal of learning a risk-aware policy; the concerns regarding epistemic vs. aleatoric uncertainty do not apply to these approaches. It would be helpful if the authors clarify this distinction.\n",
            "summary_of_the_review": "Pros\n- Important problem\n\nCons\n- Unclear whether their approach solves the stated problem\n- Limited theoretical guarantee\n- Weak experiments\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers distributional reinforcement learning with the goal of seeking risk-neutral optimal policy. Prior approaches often have a convergence issue due to either being just risk-seeking or risk-averse. Towards resolving the problem, the authors propose a simple, yet effective approach by randomizing the risk criterion during the training process. On the theoretical front, the authors prove that the method converges to an optimal return distribution under certain conditions. Several experimental results also confirm the validity of the method.",
            "main_review": "I think that the overall idea is intuitive and makes sense to me. Since committing to specific criterion induces a one-sided tendency on risk for the agent, it seems natural to instead randomize the risk criterion so as to reduce the bias and gradually shrink the ambiguity set to obtain converged results. The paper is to some extend \"complete\", with both theoretical analysis showing the convergence property as well as a few examples demonstrating the performance of the method.\n\nAdditional comments:\n1. The writing can be improved. There are many small typos/grammatical errors here and there which seem to indicate that the paper was rushed. Just to name a few: definition 2, line 3, missing \".\"; P8, \"let define a reweigh'; P8, N-Chain, \"We more plot the empirical\". Please carefully go through the paper and correct the mistakes. Further, the theoretical part contains complicated notation, and the authors could try to see if there is a better way to present the technical content with more intuitive explanations.\n2. How were the four Atari games selected? Were they cherry-picked? The authors mention that \"To achieve high performance, the agent should take an optimistic approach in some games and a pessimistic approach in others\". Perhaps, a more detailed explanation and comparison should be provided so that the readers can better understand the behavior of the method. More thorough experiments and ablations studies on hyper-parameters would greatly improve the paper.",
            "summary_of_the_review": "Overall, I think the main idea is clearly articulated. While there is room to improve, the theoretical and empirical contributions seem to be relevant and of interest. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}