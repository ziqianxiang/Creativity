{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper conducts a series of empirical studies to evaluate the robustness of smoothed attribution methods. Although the reviewers think this is an important direction, there are several concerns about the experimental settings, such as the sample size and the models to be tested. Also, one of the main finding that Lp based smoothing methods are non-robust to non-Lp norm perturbations is well known and is not that surprising."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents experiments for both post-hoc and ad-hoc explainers to better understand their quality and robustness.",
            "main_review": "The paper touches on a very important problem - the quality and understanding of (smoothed) explanations. It is very well written and an easy to follow. \n\nIt has many positives. I particularly liked the approaches to assess robustness as they seems very reasonable and widely applicable, for example using LPIPS and perception and a way of giving an interesting new perspective. The main to areas for improvement / clarification are as follows.\n\n- Generalization. How do the results extend beyond the specific networks chosen? It is difficult to understand how relevant the conclusions are in general and whether the example configuration in influencing the conclusions too much. \n\n- Formalization. For me, the paper lack in the formalization of the problem. After reading the paper I don't fully have a clear notion of what makes an explanation of this form high quality and the exact properties that one should be looking for to assess it. Of course, the metrics reported are proxies to it, but the actual objetive is not clear to me. \n\n",
            "summary_of_the_review": "Good paper but I expected a bit more in terms of formalizing quality. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The main contributions of this paper are a series of empirical results on smoothed attribution methods that are designed to show several smoothing techniques in the previous literature may produce worse explanations and these smoothing techniques are also not robust to non-Lp attacks. \n",
            "main_review": "### Strength\nThis paper presents sanity check, non-Lp adversarial attacks, and sparsity measurement on SmoothGrad, Uniform Gradient and $\\beta$-smoothing. The results are generally interesting to the community. \nUsing the Gini Index to measure the sparsity is an interesting idea that might be useful for the follow-up works. However, I have several questions regarding this metric. I will elaborate in the next subsection.\n\n### Weakness\n- It is not surprising that Lp-based defenses (smoothing) are not robust to non-Lp threat models in the community. The authors aim to present evidence that smoothing techniques are not robust enough because the proposed non-Lp attacks can break the explanations, which by itself seems to be an unfair comparison for the prior work. In fact, the contributions showing the proposed non-Lp attacks can break Lp defense is not a new observation.\n\n- The proposed attacks do not seem to produce very different results. In Fig 5, resulting adversarial attacks from new technologies remain very similar to Delta attack. In Fig 4, it seems that the new attacks only make more than 0.05 difference with the Delta attack on $\\beta$-smoothing in cosine distance. On other attributions, the improvement seems to be minimal.\n\n- Gini Index for evaluating sparseness. The motivation to use Gini Index seems to be fair, however, a lot of prior works have proposed several different metrics [1, 2, 3, 4] in measuring the sparseness (or the concentrations and the localizations on the relevant features) of attributions. At least some discussions and justifications of the proposed metrics should be included. The robustness-related and fidelity-related evaluations are motivated from the paper’s main idea, understanding if smoothing techniques provide faithful explanations, however, the transition to study sparsity is somewhat sudden to me and it seems to be unconnected from the previous content, only because the following reason authors provide: “To create explanations that are human-accessible, it is advantageous to have a sparse explanation map”\n\n- Some related prior work, i.e. ROAR [5] , that finds smoothing techniques do not create significant degeneration to explanations should be included and discussed, especially when the paper is trying to provide the shortcoming of the smoothing techniques.\n\n\n[1] A. Chattopadhay, A. Sarkar, P. Howlader and V. N. Balasubramanian, \"Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks,\" 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2018, pp. 839-847, doi: 10.1109/WACV.2018.00097.\n\n[2] Poppi, S., Cornia, M., Baraldi, L., & Cucchiara, R. (2021). Revisiting The Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2299-2304.\n\n[3] Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., & Hu, X. (2020). Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 111-119.\n\n[4] Fong, R., Patrick, M., & Vedaldi, A. (2019). Understanding Deep Networks via Extremal Perturbations and Smooth Masks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2950-2958.\n\n[5] Hooker, Sara et al. “A Benchmark for Interpretability Methods in Deep Neural Networks.” NeurIPS(2019).\n",
            "summary_of_the_review": "In summary this paper presents a series of interesting empirical results for smoothing techniques. However, I find the empirical results for the goodness of the smoothed attributions are not convincing to me, and the conclusions from the non-Lp attacks are not very new to the community, I remain negative for this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors empirically evaluate the quality and robustness of 3 post-hoc and\n2 ad-hoc approaches for robustification of gradient-based attributions under a\ncombination of 3 adversarial attack approaches which they use to target the\nattributions.\nThey evaluate the robustness via cosine-distance and LPIPS to a target-heatmap,\nand the quality via spearman correlation to the original under cascade\nrandomization of model parameters, the sparseness of the attribution maps using\nthe GINI index, and the explanation infidelity.",
            "main_review": "### Strengths\n- a broad set of evaluation metrics is used to measure the quality\n- hyper-parameters are well-documented, reproducibility is high\n- methods from prior work are studied thoroughly for their application\n- the paper is structured and written well\n\n\n### Weaknesses\n- the sample set of almost all experiments is very small and should be increased\n    - currently for post-hoc approaches on ImageNet:\n        - 128 samples for the similarity to the target map\n        - 100 samples for cascade randomization\n        - 20 (!) samples for the segmentation\n        - 100 samples for sparseness with the GINI index\n        - 100 samples for infidelity\n    - currently for the ad-hoc approaches on CIFAR-10:\n        - 320 samples for the similarity to the target map\n        - 100 samples for cascade randomization\n        - 100 samples for sparseness with the GINI index\n        - 100 samples for infidelity\n\n- errors should be reported for the segmentation experiment in table 1, the\n  cascade randomization (Figure 2 & 6), the GINI index (Table 2 & 4) and the\n  infidelity (Table 3 & 5)\n\n- only a single model on a single dataset is used for each experiment, which should be increased\n    - VGG16 on ImageNet for post-hoc approaches\n    - ResNet-18 on CIFAR-10 for ad-hoc approaches\n\n- only a single run of the experiments is conducted with only a single seed\n  (model parameters)\n    - to test for statistical significance, the experiments should, in addition\n      to more models and a higher sample size, be conducted multiple times with\n      multiple seeds\n\n- are the error bars in Figures 4 and 7 percentiles or the standard deviation?\n- the errors in Figures 4 and 7 are very large (\"compatible results\" most of\n  the time), but this is not discussed\n\n- in the segmentation experiment in Table 1 only the 20 pixels with highest\n  attribution values are investigated\n    - why do the authors not use all pixels' attribution values?\n    - if there is any reason behind this, it should be discussed, otherwise all\n      pixels should be used\n    - this approach measures spikey attribution maps in a different way than\n      smooth ones, since the spikey attribution maps will have more of their\n      total attribution scores measured\n\n- in Table 3 the infidelity for Smooth and Uniform are only marginally better\n  (1.43 vs. 1.42), yet in the text this is simply described as an improvement\n  (this marginal improvement will probably even become less significant with\n  errors reported)\n\n- it seems a little suspicious that Smooth and Uniform Gradient have the exact\n  same values for GINI Index and infidelity in Tables 2 & 3, though this may\n  just be by chance\n\n- for the similarity in Figures 4 & 7, it may give more insights to also\n  compute the distance to the original, ie. dist(h(x_{adv}, h(x)))\n\n- why is the segmentation experiment is not conducted for the ad-hoc methods?\n\n- minor: a label \"higher is better\" for figures 4 & 7 could improve readability\n",
            "summary_of_the_review": "While the results of this work are potentially interesting, this paper has many\nshort-comings in its empirical evaluation, which is especially problematic\nsince it is the paper's main contribution. With small sample sizes, most errors\nunreported, only a single model and dataset, and only a single run, there is a\nlot of room to improve this work. As these changes involve running many more\nexperiments, for the sake of improving this work, I recommend a score of 3:\nreject, not good enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper evaluates the quality and robustness of explanations of three post-hoc smoothing approaches (Smooth Gradient, Uniform Gradient, B-smoothing), and two ad-hoc smoothing approaches (CURE, Adv). It evaluates the quality of explanations based on the model parameter sensitivity, class sensitivity, sparseness, infidelity. It also evaluates the robustness of explanations to combinations of additive, spatial transformation, and recoloring attacks by comparing similarities between target and explanation maps. All evaluation is performed on publicly available benchmark datasets such as ImageNet and MS COCO. Based on their experimental results, the authors made several claims about the quality and robustness compared to the vanilla gradient method. For example, the authors claim that most of the smoothing methods are less sensitive to perturbation of model parameters so they may not be helpful to debug a model, and they may not be \"robust\" to non-additive types of attacks.  ",
            "main_review": "Novelty and Significance:\nThe novelty and significance of this paper are in the experimental evaluation of \"smoothing explanation approaches\" using diverse evaluation measures. The major novelty and significance of this paper can come from section 3.2 that aims to show non-robustness of the smoothing methods. However, the experimental results do not show a significant difference between the three attacks, which degrades the novelty and significance. Section 3.1. experiment and result are similar to those in prior works, but still can provide somewhat meaningful observations  (i.e. insensitivity) about the smoothing approaches if error bar is provided.\n\nTechnical soundness: The experimental evidence is not fully supporting the authors' claims. Additional experiments and in-depth discussion are needed.\n- Table1-3, 5: Please provide error bar (i.e. empirical confidence interval). If error bar is not small enough to distinguish the difference, please use more samples for evaluation.\n- Figure 4, 7. The two metrics are not consistent with each other. For example, in figure 4, the non-additive attack is shown as effective for Smooth gradient and $\\beta$-smoothing in the left figure but not in the right figure. Why do those two metrics show different results? \n- Figure 4, 7. Each smoothing approach shows no significant differences between the three attacks. It is quite hard to say that those methods are less robust to other types of attacks based on this experiment. Please provide more experimental evidence or use more samples to reduce the error bar. \n\nWriting and Clarity: Minor corrections are needed.\n- Page 1. The first sentence 'Explanation methods attribute a numerical value ... ' is not always true. There are other explanation methods not using attribution scores. Please modify this sentence. \n- Page 2. Infidelity -> infidelity\n",
            "summary_of_the_review": "The novelty and significance of this paper are in the experimental evaluation of robustness of the smoothing explanations that were known to be robust and any quality degradation trade-off. However, the experimental evidence is not fully supporting the authors' claims. The paper should provide experimental evidence showing significant robustness reduction. Also in-depth discussion about the experimental results is needed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}