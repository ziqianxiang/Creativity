{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Description of paper content:\n\nThe paper describes a technique to learn option policies using behavioral cloning and then recombine them using a high-level controller trained by RL. The underlying options are frozen. The method is tested in two published environments: a discrete grid world environment and a continuous action space robot. It is compared to three baselines.\n\nSummary of paper discussion:\n\nAll reviewers moved to reject based on a lack of novelty and a lack of significant empirical results. No rebuttals were provided."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "\nThis work targets the issues in learning from demonstration, in order to achieve the ability of skill reuse/ transferability and improve the sample efficiency. To address the issues, the authors propose an option-control network (OCN), including a high-level controller and a pool of low-level options, which is an \"imitation-finetune\" paradigm as well. Finally, the proposed method is evaluated on two domains, Craft (discrete action space) and Dial (continuous action space), with results demonstrating its effectiveness.",
            "main_review": "\nStrengths:\n- Although this work extends the paradigm of imitation-finetune, its idea is still novel by freezing the options and fine-tuning the controller.\n- The topic is timely important, which is related to transfer learning and multi-tasking. \n- The paper is well-organized and its method is sound to me. Besides, the related work is quite clear and includes the relevant literature.\n\n\nWeakness:\n- There lacks the clarity. For example, why \"positive rewards at the beginning of training\" is a problem in HRL? what are structured exploration and unstructured exploration? \"During the execution of an option, if probability $e_{i,t}$ is 0, the option will keep executing\", what if the task is really hard and $e_{i,t}$ keeps 0, will this step into an infinite loop? According to Figure 1, Figure 3(e), and Table 1, does that mean each option corresponds to a specific task? If this is the case, there lacks analysis or discussion on the scalability and generalizability.\n- This work primarily offers an empirical contribution. However, the empirical evaluation is still weak. Regarding the transferability demonstrated in the experiment, the \"newly\" task is a similar task in the same domain, which is still limited. \n\nMinor:\n- typo: \"They encoder skills\" -> \"They encode skills\"\n",
            "summary_of_the_review": "I prefer to weak reject ( marginally below the acceptance threshold) the paper.\n- The clarity is an issue that makes it a little bit hard to follow.\n- The empirical evaluation is weak so that the transferability looks limited. The scalability and generalizability should be discussed in the paper as well.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to extract options from a dataset using an options framework with a recurrent controller and multiple recurrent option networks. During pre-training (i.e. skill extraction), the actions from the framework are computed by the weighted sum over options, which can be end-to-end differentiable, and trained using behavioral cloning. Once all options are extracted from the dataset, a new controller can be trained to solve a new task with the learned and frozen options via RL. The experimental results on the discrete CRAFT environment are impressive, outperforming all baselines. The results on the continuous DIAL environment show improved results over the baseline methods but the difference is marginal.",
            "main_review": "### Strengths\n\n- The proposed approach is intuitive and easy to implement.\n- The paper is easy to follow and clearly written.\n- The experimental results in CRAFT show significant improvement over prior works.\n\n\n### Weaknesses\n\n- Comparison to prior latent skill learning approaches is required to understand the contribution of the proposed option extraction method. These approaches already have shown impressive results on complex robotic manipulation environments. It is unclear whether the proposed method can achieve better performance compared to these approaches.\n\t- Pertsch et al. Accelerating Reinforcement Learning with Learned Skill Priors, CoRL 2020\n\t- Shankar et al. Learning Robot Skills with Temporal Variational Inference, ICML 2020\n- In the approach section, \"OCN can be easily expanded to a multi-level model.\" does not seem true. Most hierarchical approaches so far have failed to learn more than two-level hierarchy, which implies challenges in extending to multi-level hierarchy.\n- When an option is continued, the hidden state of the controller is copied without any update. This results in missing task-specific information during the option execution. Given the fact that the proposed method relies heavily on recurrent models, this missing temporal information could make the agent decision not optimal.\n- There are discrepancies between pre-training and training OCN models, where the pre-training stage computes an action and termination signal from all options with soft weighting while the RL training stage chooses a single option at a time. Will this affect RL training badly? The options learn to be mixed together during skill learning without clear skill decomposition. But, when using these options, they need to work alone and have a clear skill boundary.\n- In the Dial task, the proposed method works better than the baseline methods. However, the variance in Figure 6 seems very high and the performance does not look significant. How many random seeds are used for the experiments? It definitely requires more random seeds to reach a meaningful conclusion. Another question from the learning curves is about the discrepancy between dense and sparse reward settings. OCN seems to succeed 20-30% in the sparse reward setting, but in the dense reward setting, it's not even close to solving 2 subtasks out of 3 subtasks. Moreover, this poor performance makes the scalability of the proposed method less convincing. Can OCN even learn the tasks it saw during pre-training (such as [1, 2])?\n- In the Dial task, the experiments focus on generalizing to new tasks with a longer sequence of subtasks. One interesting experiment would be learning skills from a large number of tasks (e.g. three digits pressing tasks with all numbers) and then learning a controller for a holdout task.\n\n\n\n### Minor comments\n\n- \"Option-Control Network\" is used instead of \"Option-Controller Network\" in the introduction.\n- \"option framework\" -> \"options framework\"\n- \"furthered combined\" -> \"further combined\"\n- In equation (13), LHS should be $\\hat{p}^a_{t}$, not $\\hat{p}^a_{i, t}$.\n- What happen if the chosen option outputs \"done\"?\n  ",
            "summary_of_the_review": "This paper tackles an important problem of reusable skill (option) extraction from a large multi-task dataset. The proposed method is simple and works well in the discrete task, CRAFT. However, the training process is not intuitive (soft selection during pre-training and hard selection during RL). The experimental results on the continuous domain, DIAL, are noisy and show only marginal improvement over prior work. Moreover, comparisons to continuous latent skill approaches are required to show the technical contribution of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a specific neural network architecture for option learning with recurrency on both option and high-level controller. The final action outputs are determined from a mixture of experts of all options. The approach learns options offline from demonstrations (behavioural cloning) and combines them online by learning a new high-level controller via RL (PPO) while using frozen options. The submission uses variations of 2 domains from prior work for evaluation and shows good performance.\n",
            "main_review": "The overall ideas of task decomposition and using information from demonstrations to accelerate RL are important and this paper proposes a new architecture to address these ideas. It is overall well written and easy to follow. However, the particular network is quite similar to existing implementations and has contributions focused on an aspect that could fall under implementation details in other papers. A further big challenge is the limited evaluation and a missing connection to much of related work. In its current form, the submission is well worth discussion but fits more in the context of a workshop.\n\nRegarding the evaluation it is unclear why the authors do not compare to using options from known approaches like DDO (cited in paper), Option Critic [1] or HO2 [2] (where the latter two can naturally use BC pretrained options as it just requires a change of the initial parameters). The evaluation uses variations of two domains which are not compared against methods from the papers which propose them. In addition, one of the baselines, OMPN, uses an adaptation of the Craft domain in the paper but shows a very different level of performance which suggests that the variation of the domain is different for the submitted paper. The original reason for investigating the papers was that all baselines flatline at surprisingly low performance (most prominent in the Craft domain).\n\nOn the more positive side: Section 4.1 includes an example from transferring options from multiple previous experiments which provides an interesting perspective. A future iteration could benefit from more focus on this generally under-investigated idea. But the results for baselines are again surprisingly low.\n\n\nMinor:\nSection 1: the mentioned exploration problem is more a property of specific domains rather than algorithms \n‘High level controller is updated less frequently’ is likely supposed to mean acting. Updating can be confused with learning.\n‘each option does not correspond to a meaningful subtask’ It is unclear what this is supposed to mean. What is a ‘meaningful subtask’? And why is it good for options to not correspond to one?\n\n[1] Bacon, Pierre-Luc, Jean Harb, and Doina Precup. \"The option-critic architecture.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. No. 1. 2017.\n[2] Wulfmeier, Markus, et al. \"Data-efficient hindsight off-policy option learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n",
            "summary_of_the_review": "Submission proposes a specific architecture for option learning. An aspect that in other work might fall under implementation details. The work has some interesting experiments on combining options from multiple experiments, but the evaluation and connection to previous work is very limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new model called Option-Controller Network with requisite inductive biases to model temporal hierarchy. This model is used to learn temporal abstractions and the control policy in the space of options using demonstration data via imitation learning. The performance on discrete action (Craft) and continuous action (Dial) environments are promising compared to existing baselines such as OMPN, CompILE and MoE. ",
            "main_review": "Strengths:\n\nThe model proposed is interesting and builds on the recently proposed OMPN model as well as prior work [1, 2, 3, 4, 5] on incorporating inductive biases in RNNs to model temporal hierarchy. Baseline models compared against (CompILE, OMPN, MoE) are strong, appropriate and relevant. All models have been evaluated on the benchmark datasets such as Craft and Dial as in prior work. The pretrained options learned by OCN during the imitation phase lead to good performance gains when being reused later by the controller network. Implementation details for all models (including baselines) have been described well in the Appendix. \n\n\n\nWeaknesses:\n\nAssuming I understood the generation of training datasets in experiments S1 and S2 correctly. The training datasets (AC, CD, DA) or (AB, BA) or (CD, DC) generated for the imitation learning phase in S1 & S2 are overly simplistic. The associated decomposition problem involves just finding 1 boundary point to demarcate 2 skill primitives in an episode in the datasets for S1 and S2 experiments. It would be unreasonable to assume this training setup would directly transfer to any general/complex real-world offline datasets containing longer sequences involving execution of many options. Have I missed something in my interpretation? Could the authors clarify this point and/or provide some justification as to why they would expect their model to show similar performance in the more general case?\n\nIt would be beneficial to compare and discuss the OCN model in the context of prior work on RNN variants that model temporal hierarchy (Clockwork RNN[1], HM-RNN[2]) using various adaptations to hidden-state updates. Specifically, the HM-RNN update rule uses similar operators such as copy, update (and flush) to allow for slower updates to memory cells deeper in the hierarchy. Therefore it is important to delineate the differences in the update rules employed in HM-RNN [2] and the proposed OCN model to evaluate the novelty/originality of the contribution. Further, earlier work in literature on inducing hierarchical temporal structure in RNNs [3, 4, 5, 6] has not been referenced and compared/discussed in the related work. \n\nCan the hierarchical update rule (control flow between controller and option networks) proposed in the OCN model not be implemented/computed by an appropriately sized OMPN model? Or perhaps a HM-RNN [2] model? \n\n\n\nResults:\n\nI found it rather surprising to see that the OMPN seems to struggle to learn good decompositions of primitive skills in S1 which can be composed (reused) by the controller later. Several experiments show CompILE and MoE baselines outperforming the OMPN model quite significantly (Figure 4 and Figure 6). This is very surprising to me since OMPN essentially has a strong inductive bias for modelling temporal hierarchy which is clearly beneficial for these tasks and very similar to the proposed OCN model. Could the authors offer an explanation for why this is so? \n\nIt would be interesting to quantify how redundant (i.e. number of options learned for the same underlying skill) the learned options are and amount of reuse by the controller as we use more noisier/complex demonstration datasets for the imitation learning phase. \n\nThe authors claim that OCN does not require specifying the exact number of segments in an episode and only need to specify a safe upper bound on it. What do the “extra” slots capture/model? Are they left unused or does it lead to a slightly over-segmented decomposition of the episode? \n\n\n\nWriting/Presentation:\n\nIn general, the paper contains several typos and ambiguous or unclear phrasing in several sections. The papers’ readability and clarity would greatly benefit from a significant revision in this regard. I have highlighted some of the typos/inconsistent math notation/grammatical errors below:\n\n“model each skill with separate options” -> does this imply that there are several options used to model a single latent skill? \n\n“is a hallmark in human intelligence” -> “is a hallmark of human intelligence”. Further, the authors do not provide a reference to validate the claim in this sentence. \n\nThe authors introduce It is unclear whether this is a new problem introduced by the authors or an interpretation of an existing problem. It would help to cite relevant references that define/study this “exploration  problem” in pure HRL methods. \n\n“.. limits the practical values of these approaches” -> “ … limits the practical utility of these approaches”\n\n“ popularity of neural nets” -> “popularity of neural networks”\n\n“However assuming access to an environment in the pertain phase might be infeasible in many tasks” -> missing citation to relevant environments?\n\n“and performs imitation learning” -> “and perform imitation learning” \n\n“our work focused on” -> “our work focuses on”\n\n“performs IRL on the demonstration” -> “performs IRL on the demonstration data”; what does the abbreviation IRL stand for?\n\n“extracts meaning segments” -> “extracts meaningful segments”\n\n“temporal hierarchical structure” -> “hierarchical temporal structure” \n\n“Following the fast and slow learning idea proposed in Madan et. al 2021“ Please improve the citation, some suggestions for the same [6, 7]\n\n\nSome of the math notation in the “Methodology” section is confusing, difficult (unintuitive) to follow for the reader and also not consistent throughout the text. Using boldface for functions (i.e. RNN updates, example $o_i (x_t, h_{t-1}), c (x_t, h_{t-1})$) is confusing as a small letters with bold font is being used to denote vectors as well. Typo in equation 3? Should it be $\\hat{h_{t-1}}$ and not $h_{t-1}$? In Equations (3,4,5,6) several vectors with no bold font. It is also very confusing to have both $h_t$ and $\\hat{h_t}$ in the cell network equations. Further, the indexing notation (i, t) for activations of option models (ex: $h_{i, t}^{o}$) is also very confusing and reduces the clarity and readability. Please use another indexing scheme for activations of option models like $h_t^{o_i}$ or something similar. It is also incredibly hard to mentally keep track of 3 variants of each activation vector (for ex: $\\hat{p}$, $p’$ and $p$) throughout the description of the algorithm and their roles. Please use a more intuitive alternative to make the reading experience better and improve the clarity. The objective function to minimise (equation 18) is written in a pseudocode-like manner. Please remain consistent and define the objective function using a mathematical expression to avoid ambiguity.   \n\n\n[1] Koutnik et. al, “A Clockwork RNN.”, ICML 2014.\n\n[2] Chung et. al, “Hierarchical Multiscale Recurrent Neural Networks”, ICLR 2017. \n\n[3] Schmidhuber et. al, \"Learning Complex, Extended Sequences Using the Principle of History Compression,\", Neural Computation 1992,\n\n[4] Mozer et. al,  \"Induction of multiscale temporal structure.\", NIPS 1992.\n\n[5] El Hihi et. al, “Hierarchical recurrent neural networks for long-term dependencies.”, NIPS 1995. \n\n[6] Jaynes, Edwin T. (1957). Information Theory and Statistical Mechanics. II. Physical Review 108 (2):171.\n\n[7] Boltzmann, Ludwig,  \"Studien über das Gleichgewicht der lebendigen Kraft zwischen bewegten materiellen Punkten\", Wiener Berichte. 58: 517–560, 1868.\n",
            "summary_of_the_review": "The proposed OCN model is an interesting extension to the recent OMPN. However, my main issue is that the demonstration dataset generation process has drastically reduced the full sequence decomposition task to an overly simplistic case. The writing can be improved significantly as suggested above to improve clarity of the proposed ideas. Further, some closely related prior work [2] on modelling temporal hierarchy in RNNs using similar ideas have not been compared/discussed and many others not referenced [1, 3, 4, 5].\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}