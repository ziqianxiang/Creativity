{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This is a borderline paper with some reviewers voted for acceptance and some think it is not still ready. What is clear is more efforts by the authors is needed to make the paper appealing to reviewers with different interests. Changes such as better writing, more in depth literature review, more convincing experiments can definitely improve the quality of the paper. I personally do not think regret analysis is needed for this work, but it was mentioned by a reviewer. I would suggest the authors to use the reviewers' comments, revise their work, and prepare it for future conferences."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for mean-variance trade-off optimization in RL. The method, named EQUMRL, tries to optimize the Pareto efficiency by maximizing the quadratic utility function. The proposed method mitigates the double sampling issue appeared in prior work thus simplifies the optimization procedure. The authors also provided experiments to demonstrate the benefits of the algorithm using both synthetic datasets and real world data.",
            "main_review": "Strength:\nThe method in this paper is intuitive, and it builds a connection between utility optimization, a popular topic in finance, and reinforcement learning. The proposed method can mitigate the double sampling issue that appeared in prior work, and can be combined with any existing policy-gradient-based algorithms. The experimental results show that the algorithm has better performance compared to baselines.\n\nWeakness:\nI think there are some weakness in the experimental section. In Section 6.2, the portfolio management experiment using real-world data does not seem to be an RL problem. The dataset is static, which contains the returns of each asset at each time. This return does not change when the agent's action (weight w) changes. The policy network is essentially a \"classifier\" that maps the input feature vector (returns of assets) to a probability vector. Thus, the problem is more like an online learning (or supervised laerning) problem rather than an RL problem.\n\nI agree that for this problem, it is still useful to better optimize the mean-variance trade-off, but I think the authors should clarify this better to avoid confusion. Moreover, since the authors propose this algorithm as an RL algorithm, it would be interesting to test the performance of the algorithm on a more realistic RL benchmark.\n\nMinor:\nSection 2.1: “A typical method for consider the trade-off is train a policy under some constrains”: this sentence has grammar mistake and typo.\nSection 6.2: “The policy determines e the weight.”: typo.\n",
            "summary_of_the_review": "The proposed algorithm is interesting and has solid contribution. The experiment section is not satisfactory enough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper learns MV-efficient policies that achieve Pareto efficiency in terms of MV trade-off. The authors propose an approach called expected quadratic utility maximization (EQUMRL), which trains an agent to maximize the expected quadratic utility function. The proposed EQUMRL does not suffer from the computational difficulties. Experimental results are provided to demonstrate the effectiveness of the proposed EQUMRL.",
            "main_review": "Strengths: \n\n1. The studied mean-variance problem is very interesting and well-motivated.\n2. This paper conducts extensive experimental results and demonstrate the empirical performance superiority of the proposed EQUMRL.\n3. The authors provide code, which is helpful for readers to reproduce their results.\n\nWeakness:\n\nThis paper lacks theoretical analysis. In particular, what regret/mean-covariance/Pareto efficiency guarantee can the proposed EQUMRL achieve? I expect to see formal theorems and rigorous proofs.\n\n\n----After Rebuttal----\n\nI read the authors' response. In general, I think that the weakness of this paper falls on theoretical analysis. Although the authors explained that the regret analysis in RL is an open problem, to my best knowledge, there are a number of papers that study regret bounds for (online) RL under tabular or linear MDP settings in recent years. \n\nFor example,\n\nAzar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement\nlearning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70.\nJMLR. org.\n\nJin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M. I. (2018). Is Q-learning provably efficient?\nIn Advances in Neural Information Processing Systems.\n\nJin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory.\n\nI more or less understand that it is hard to analyze a policy gradient based RL algorithm with sophisticated parametric models like neural networks. But the authors' response did not show me sufficient contributions of this paper.\n\nIn my opinion, this paper lacks solid theoretical analysis, and its experiment part looks standard (does not provide empirical evaluations under sufficiently comprehensive and challenging experimental benchmarks/setups). I did not find enough reasons to recommend accept.\nSo I plan to stick to my score 5.",
            "summary_of_the_review": "Overall, I think that the studied mean-covariance problem in this paper is very interesting. The idea behind the proposed approach looks non-trivial. Extensive experimental results are provided to show the superior empirical performance. However, this paper lacks theoretical analysis, and thus I could not judge whether the proposed approach achieves good theoretical results as well.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposed an alternative objective (expected quadratic utility) to the mean-variance objective in an episodic RL setting. The new objective does not involve the squared expectation term, thus resolving the double sampling issue for obtaining the gradient of the mean-variance objective. In addition, the new objective has a couple of nice interpretations including the Pareto-efficiency one.  ",
            "main_review": "Strengths:\n- The paper is nicely written with a clear motivation, i.e., to resolve the computational/sampling issues for traditional methods that aim to optimize the mean-variance objective.\n- The proposed expected quadratic utility objective is easy to work with. \n- The interpretation of the proposed objective is well-explained. \n\nWeaknesses/Questions:\n- Is it possible to provide some quantifiable gap between the optimal policy under the proposed objective and the optimal policy under the original mean-variance objective?\n- In the experiments, it seems that only the mean performances are shown. What are the standard deviations for the performances of the methods? \n- side points: (1) I am not sure how the traditional MVRL objectives can be rephrased as the constraint optimization problem with *equality* constraint. Could the authors provide more details on that? (2) The \\nabla_{REINFORCE} notation seems very unconventional.\n\n",
            "summary_of_the_review": "The paper was nicely written and provided a simple alternative (the expected quadratic utility function) to the mean-variance objective, which has nice interpretations and avoids the double sampling issues.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the reinforcement learning problem with mean-variance tradeoff. It investigates a quadratic formulation that aims to optimize for the mean-variance efficiency, in a sense similar to the Pareto efficiency. It establishes connections between the quadratic formulation with the standard mean-variance formulation studied in the literature, and provides various interpretations of the mean-variance efficiency. The paper also proposes novel algorithmic implementations of the quadratic formulation based on REINFORCE and actor-critic methods. Three investment-related experiments are performed for the REINFORCE-based algorithm and demonstrate its good performance compared with existing algorithms.",
            "main_review": "The paper is well written and easy to follow in general. The survey in the appendix is helpful for understanding concepts and motivations from a finance and economics perspective. The authors provide detailed discussion of the related literature and comparison of the present work with existing ones. They also present ample experimental results under various settings (related to investment) to justify the efficacy of the algorithms.\n\nQuestions:\n- pp4, line 7, E[G] + \\lambda (...) should be E[G] - \\lambda (...)?\n- pp5, line 4, \\hat{G}^k should approximate E[G]?\n- pp5, the displayed equation seems to imply that (\\hat{G}^k)^2 is an unbiased estimateor of E[G^2]. Can the authors provide intuition of why this is the case?\n- I am curious of how the actor-critic implementation would fare against the REINFORCE-based method in experiments.\n\n\n---- Post rebuttal ----\n\nI appreciate the authors' response. I have read through other reviewers' comments and authors' rebuttal. I would like to maintain my score.",
            "summary_of_the_review": "The paper considers an important problem and its proposed algorithm is justified by extensive experiment results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, authors propose a reinforcement learning approach to solve mean-variance portfolio optimization problem. The main idea of the paper relies on transforming mean variance optimization to maximizing expected quadratic utility. This allows the authors to avoid inefficient approximation of the gradient of the variance and instead use the same sample paths to approximate the gradient of the first and second moments of the reward. They implement two algorithms, first based on REINFORCE and another based on Actor-Critic method. Authors also illustrate the performance of the method on two problems, first based on synthetic dataset and another based on Fama & French dataset.",
            "main_review": "Strengths: Authors use well known results from mathematical finance to improve state-of-the-art for portfolio optimization using reinforcement learning. The paper is also well written and the flow is clear.\n\nWeakness: I have concerns on results presented for both the examples illustrated in the paper. In addition, authors need to improve the writing and update literature review to provide better context of the contribution of this work. Please see below for detailed comments:\n\n*Contribution/Results:*\n1. It is not clear why the proposed algorithm works better than the Tamar et. al. and Xie et. al. Author’s state: “We conjecture that this is because while the EQUMRL is an end-to-end optimization for obtaining an efficient agent, the other methods consist of several steps for solving the constrained optimization, where those multiple steps can be sources of the suboptimal result.”. This provides no concrete explanation or evidence on the difference.\n2. Results in Figure 2 are somewhat concerning. It is not clear why target variance is much higher than the observed variance (e.g., blue square with “var=50” has an observed variance of 33). Since returns increase with increasing variance, I would expect the constraint to be binding.\n3. For the real world dataset, it is not clear how the authors have trained the model. Since there is only one historical trajectory of each stock in the portfolio, it is not obvious how the training works. It will be helpful to provide some details on model training in this context.\n\n*Writing:*\n1. It will be helpful if authors could clearly set up the portfolio optimization problem in Section 2. The current description is very generic. As an example, they do not describe what is the action space (e.g., portfolio weights or actual dollar investments), the state space (returns or stock prices) or the rewards for the problem considered in the paper. It only starts to become somewhat clear when the reader is in Section 6. There are a lot of different ways one can set up the portfolio optimization problem, so providing clarity on it is important.\n2. Authors do not describe the sequence of steps required for implementing the algorithm. The practitioners will find the paper far more useful if at least it could provide the complete algorithm at one place.\n3. The discussion on per-step variance perspective has no corresponding results. Authors should probably consider moving it as a discussion topic at the end or provide additional results to support the setup.\n\n*Literature:*\nThere is a rich literature (over 20 years of work) on portfolio optimization in mathematical finance using stochastic control which also solves for dynamic decision problems and is closely related to the work done by the authors. Providing some context of how this work relates to that domain would be helpful.\n\n*Typos:*\n1. Section 6, first line “variacen” → “variance”\n2. Page 1, 4th last line - should “Pareto efficient policy” be “pareto efficient frontier”?\n",
            "summary_of_the_review": "My recommendation is to reject because the results require more justification on the performance of the method relative to state-of-the-art. The work does not provide any novel perspective over what is already known to the financial mathematics or reinforcement learning community. Authors need to provide more clarity in writing, In particular, in formulating the optimization problem and how the method is implemented.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}