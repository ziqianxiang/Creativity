{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents a new perspective on recommendation systems, categorizing them as linear predictors where the main difference between the various methods is the regularizer. The authors then propose an objective function that aims at optimizing the Frobenius norm while maintaining a low-rank solution, and present algorithm that have closed-form solutions based on the SVD.\n\nThe reviewers noted the novelty of the framework, but the overall assessment after the discussion was that the theoretical contribution was limited. The algorithm proposed by the authors does not provide any improvement on standard criteria (performance, computational complexity), which makes the algorithmic/experimental contribution limited as well."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper unifies under one theoretical framework the different state-of-the-art regularization approaches for linear collaborative-filtering recommendation models which leverage the user-item interaction data matrix. \n\nThe authors classify these algorithms among two families: 1) the nuclear-norm-based regularizers and 2) the Frobenius-norm-based regularizers. Both have their advantages and drawbacks. \nWhile nuclear-norm-based regularizer's solutions are low-rank (and therefore more scalable) and have a closed form, their performance is limited. \nOn the other hand, Frobenius-norm-based regularizer's solutions are full-rank or difficult to train.\nHence, focusing on generalizing Frobenius-norm-based regularizers, the authors bridge the gap between the two families by providing two new low-rank, closed-forms solutions - LR-EDLAE-1 and LR-EDLAE-2 - peforming similarly to full-rank EASE and EDLAE state-of-the-art models. \n\nTo that purpose, the authors start from the analysis work from Jin et al. (2021) about the relationship between Matrix Factorization approaches and linear autoencoders-based ones. \nBesides EASE and EDLAE algorithms, they also analyze VLAE, DLAE and LRR. They show that VLAE solves a weighted nuclear-norm regularixation problem.\nThey also generalize the result on the equivalence between dropout technique and the adding of a squared nuclear-norm regularizer to show that solution structures for ||W||^p_* are similar for all p>= 1.",
            "main_review": "Note to ICLR 2022 (not the authors): next time, it would be nice to include row number in the submission template to ease the reviewing process!\n\n== STRENGTHS == \n\nThe paper is rather well written with several emphases on the different contributions of the authors. There are solid theoretical foundations accompanied by empirical evidences on three real datasets.\nThe experiments include 1) recommendation performance (NDCG) of the several considered approaches, 2) influence of low rank value on NDCG and 3) impact of weight ordering on recommendation performance.\nMoreover, the authors plan to share the code which is highly appreciable in order to provide reproducible results.\n\n== WEAKNESSES (OR SUGGESTIONS) ==\n\nThere is a certain amount of minor typos (see some below) but also a lack of some term definitions (because of late copy-paste in Appendix to observe the page limitation rule probably). \nTable 3 is actually very useful for the understanding, I would not put it in Appendix.\n\nSome things which could be clarified:\n- Reference to deep learning methods is done for state-of-the-approaches. In the paper, it seems it is more about shallow networks (max 2 layers) that are discussed.\n- p.3, A3, Eq. 2: W1 and W2 are not defined. I guess they denote the Encoder and the Decoder network.\n- p.3, A4, eq.3: W and V not defined, same as above\n- Eq. 4: N is not defined\n- p.4, A6: M not defined.\n- Eq. 13, p.7: operation dMat not defined (too late to put it in p.2 of supplementary material), same in eq. for W* (which should be indexed)\n\nAlso to help the reader, it would be nice to start Sections 2 and 3 with a summary of what the section plans to achieve/demonstrate. In particular, this is quite difficult in Section 3 to follow the objective. \nFor instance, a sentence such as \"Prop. 2 also leads to the following Corollary.\" does not help to understand the implication of such result. \n\nUnless I missed it, differences of implication between LR-EDLAE-1 and LR-EDLAE-2 (the proposed methods) are not enough carefully detailed.\n\nIn Table 1, it would be clearer for the reader to identify the best methods for instance, 1) by putting in bold the best obtained results and 2) by underlying the 2nd best result. \nTable 1 refers to Table 3 for more details: I think this is a mistake because it does not let the paper \"self-contained\".\n\nAm I missing something or Mult-VAE/DAE are used as baselines during experiments but are not discussed before (contrarily to the other approaches)? Why?\n\nSome typos :\n- in abstract : « (surprisnig) »\n- abstract: missing \"-\" for low-rank and closed form\n- p.1 in introduction: \"the linear autoencoders [...] which encompasses\"\n- missing upper case : p.2, 2nd paragraph « . we generalize the »\n- p.2 missing \"-\", \"These models produce closed form full-rank estimators\"; \"However, no closed form solutions\"; \"ADMM based solutions\"; \"the full rank W\"\n- end of p.2 : « The weighted unclear norm »\n- beginning of p. 3 : missing lower case « Therefore, Nuclear-norm regularizers »\n- inconsistency in convention naming for equations : eq. Or Eq. / eq (1) or eq. 1 ; 5 times in p. 3, same for Table or table, Proposition or Prop.\n- p.3, A2 : « This approach useS »\n- p.3, A4 : « probabolistic »\n- p.4, first row: choose between \"hyperparameter\" (p.3 after Lemma 1) and \"hyper-parameter\" (p. 4)\n- p.4, (ii): \"choices of p [...] produces\" ; \"all entries in X is\"\n- p.4, (ii): inconsistency in naming convention: nuclear-norm-based and nuclear-norm based in the same sentence, sometimes it is nuclear norm (check everywhere in the paper)\n- Before eq.7: \"Its a closed-form solution is.\"\n- choose between \"Frobenius-norm regularizer\" p.4,5 and \"Frobenius norm regularizers\" p.5\n- before Sec.3, p.5: \"two low-rank Frobenius-norm-based modelS\"\n- p.8 in Table 1: \"Frobinius\"\n- eq.8, missing \"|\" in 3rd norm, extra space to remoce: \"equivalently ,\"\n- missing \"-\" for closed form solutions after Proposition  2 p.5, in Section 4 p.7, in Section 5 p.7, in Q1 p.8, Q.2 in p.9 twice, Q.3, in conclusion ; check for low-rank everywhere also\n- p.5 choose between \"rearrangement\" and \"re-arrangement\"\n- Corollary 1, p.6: Missing Eq. before (9)\n- Section 4, p.7: missing \"-\" for state-of-the-art\n- Section 4, p.7: after ADMM: \\cite instead of \\citep, same after closed-form and EDLAE\n- Table 1: refer TO\n- p.8 in Q1: (eq. (11))\n- p8, Q1: \"one of the most popular implicit matrix factorization algorithmS\"\n- Q2: \"most of them reaches\"\n- Q2, missing \"-\" in EDLAE based approaches\n- Q2, p.9: ADMM method performS slightly better ; none-the-less\n- p.9: \"all the models add either a nuclear-norm-based [...] or a Frobenius-norm based regularizer.\" ; check also the abstract\n- p.9 \"The Frobenius-norm models are more express\" \n- Supplementary: section A autoencoder vs auto-encoder \n-Supp, section A: \\cite instead of \\citep for 2nd paragraph\n\n\n\n\n",
            "summary_of_the_review": "This paper represents an extensive work on both theoretical and experimental sides. \nHowever, the number of typos and missing notation definitions show that the manuscript has not been re-read enough to check that the fact of having moved parts from the general paper to appendix does not hurt actually the linear reading. \nI advise the authors to do another reading pass to fix the typos and problems of definitions listed above. \nI vote for a weak reject.\n\n======  AFTER REBUTTAL ======\nI read carefully the other reviews and author's responses. First of all, thank you very much to the authors for their hard work during the rebuttal period and the significative changes brought to the structure of the paper which is improving its quality a lot. \nHowever, I tend to share the concerns of some reviewers regarding the complexity of the algorithm which is actually rather similar to the state-of-the-art baselines. Hence if the designed methods bring neither substantive improvements in terms of accuracy nor in terms of complexity, the proposed theoretical framework does not seem finally enough for an acceptance. I keep my grade \"marginally below the acceptance threshold\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors studied some linear recommendation algorithms and their relations and also proposed two new methods to benefit from both\nnuclear-norm and Frobenius-norm regularizations. The two new methods have similar performance as compared methods.",
            "main_review": "1. My main concern about this work is that the performance of the proposed methods is not better than SOTA ones like DLAE and EDLAE. Also, the authors did not analyze or compare the computational complexity. I do not see the value of proposing a method with similar performance and unclear complexity. Specifically, the two proposed methods both require SVD approximation, which has a cubic complexity and is quite expensive. It is nontrivial to analyze the complexity of the proposed two methods and other algorithms.\n\n2. The authors claim that ‘while the syntax of this problem resembles matrix completion (MC), recommendation systems and MC have different evaluation criteria so MC’s results are not directly applicable here.’ It is better and helpful to make it clear what the evaluation criteria difference is if the authors want to discuss the relations between recommender and MC. Besides, if two problems do not share the same evaluation criteria but have the same or similar objective function to be optimized, they are still quite correlated.\n\n3. Section 3 has the same name as Section 2.1, which is confusing. The overall structure should be reorganized.\n\n4. In Section Matrix Factorization via dropouts,  the relation of mu and X and lambda should be better explained.\n\nMinor:\n1. choise—> choice\n2. Please unify ‘closed form’ and ‘’close-form’\n3. which make the —> which makes the \nPlease polish the English writing and correct all grammar errors.",
            "summary_of_the_review": "Overall, the authors have done extensive works to analyze the SOTA linear recommendation models but some critical analysis like the computational complexity is missing. Also, the proposed methods do not perform better than SOTA ones.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzed the linear models for recommendation from their regularization terms, more specifically, via nuclear norm and Frobenius norm. Based on the analyzes, this paper proposed two alternatives for linear recommendation models with closed form solutions. Experiments on three large scale datasets showed that the proposed alternatives are comparable to the state-of-the-art baseline methods.",
            "main_review": "The paper is well structured and easy to follow. The analyses from the perspective of regularization terms are novel. The results in Table 3 are very interesting. Although it is unknown why not ordering the singular values will lead to significant performance drop, the results can help confirm another limitation of matrix factorization-based methods.\n\nMy main concerns about this paper are as follows:\n1)\tThis paper may help researchers to understand linear recommendation methods from a new perspective, but the theoretical contributions are limited. In addition, the proposed methods are without statistically significant improvements compared to baseline methods.\n2)\tThe authors claimed that the proposed method can have closed form solutions. But in both proposed methods, SVD is required to obtain the so-called closed form solution. I am not sure if this kind of solutions are “closed form” or not.\n3)\tThere is no efficiency analysis about the proposed method. Although the proposed methods are low-rank not full-rank, the required ranks are actually very high, e.g., 2000-12000 in Figure 1. This is rarely seen in practice and might cause storage or computation issue in large datasets.\n4)\tIt will be interesting to see more insights about why ordering of weights on matrix factorization and other regularizations are so important in recommendation models.",
            "summary_of_the_review": "Overall, I feel that this paper makes incremental contribution by connecting existing recommendation methods both theoretically and empirically.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper categorizes various linear models based on the applied regularization: nuclear norm vs Frobenius norm regularization. It is claimed that this categorization is key to understanding the prediction accuracies of the various linear model-classes. The paper also makes connections between nuclear norm regularization and Frobenius-norm regularization, and also discusses L_p norm regularization of the singular values. The paper also describes 2 methods that yield a closed-form solution of a low-rank model with Frobenius norm regularization: while it is not motivated/explained why these methods should work from a conceptual/theoretical perspective, the experiments show that they indeed work surprisingly well.\n",
            "main_review": "Overall, I think the flow of the paper could be improved. Many things are stated/claimed in Section 2, and then the justified several pages later. Essentially, this means that the reader has to memorize many things until they get explained later. For instance, in section 2.1 on nuclear-norm regularization, various Frobenius-norm regularizations are given, and the connection to the nuclear-norm is provided 2 pages later in proposition 2. Even more confusing in section 2.1 is that A3 / Eq 2 appears to be different from A1, A2, and A4  in that L_2 norm regularization of ||W_1||^2 is used in Eq 2, i.e., without the data matrix X. In contrast, A1, A2, and A4 apply L_2 norm regularization essentially to ||X * W_1||^2, or if P= X * W_1 then ||X * W_1||^2. This is eventually justified 3 pages later in corollary 1. I wonder if one could simply swap the ordering of sections 2 and 3 to improve the flow. At the moment, I feel that the reader has to read the paper twice, as section 2.1 only makes sense after reading section 3.\n\nSection 2:\n\nThe authors categorize the various linear models according to the applied regularization (nuclear-norm vs Frobenius). To this end, they also need weighted regularization, and they generalize recent results (which seems somewhat straight forward, though).\n\nIt is not clear to me why A3 / Eq 2 is called denoising linear autoencoder, given that denoising in linear models is equivalent to applying an L_2 norm regularization of the form ||W_1 * W2||^2 (i.e. product), and not ||W_1||^2+||W_2||^2 (i.e., sum) (dropping the L_2 parameters here for concise notation). A second version of a denoising linear autoencoder appears in A6, where the correct L_2 regularization induced by denoising is used. Maybe this can be described more clearly.\n\nI wonder if  the claim that Eq 5 is equivalent to MF is actually trivial, as one can simply identify P:= XA and Q:=B to connect both formulations. The equations below Eq 11 in Jin's paper already show that MF and autoencoders are equivalent models, i.e., P*Q in MF can be equivalently rewritten as an autoencoder taking the form X*A*B--hence the only difference and the key difference is the L_2 norm regularization ||P||^2 = ||XA||^2 (in MF) vs ||A||^2 (in the autoencoder), i.e., X is included vs excluded in the L_2 norm regularization. Hence, it is obvious that one could use the regularization  ||XA||^2 in the autoencoder, which would obviously be equivalent to MF. Apart from that, it is not clear to me why the weighting is introduced, as it  does not seem to add anything to the conceptual insights gained (but I can believe that it will lead to better results in practice/experiments)\n\nIn A7 (EDLAE), the statement is incorrect that the constraint diag(W)=0 is applied to low-rank EDLAE models. Instead, the diagonal is subtracted during training, i.e., || X- X * {W-diagMat(diag(W)) } ||^2. In other words, while W is not required to have a 0 diagonal, the matrix {W-diagMat(diag(W)) } obviously has a zero diagonal. If W is low rank, then the diagonal is generally not 0, as the diagonal and off-diagonal elements are not independent of each other (due to the low-rank constraint). Only when W is full-rank, all entries in W become independent of each other, so that one can afford the freedom to constrain the diagonal to 0. \n\nSection 3:\n\nThis section provides the connection between nuclear norm and Frobenius norm regularization. The key contribution seems to be that existing results on the nuclear norm are generalized to the *weighted* nuclear norm.\n\nSection 4:\n\nBelow Eq 13, it says that diag(W)=0 in EDLAE. This is only true for the full-rank EDLAE model, while it is not true for low-rank EDLAE, which is considered here. See comment above.\n\nThis section provides 2 ways of approximating the full-rank EDLAE by a low-rank model, which somewhat seems to fall out of the blue sky, i.e., it seems unrelated to the previous parts of the paper--including the section 2.2 on Frobenius norm regularization applied to various models. While it is clearly outlined how the 2 methods work, it is not discussed *why* they can be expected to work / result in accurate approximations. The reason is that the diagonal is ignored / assumed away in both proposed methods, even though the diagonal is essential in the original EDLAE model. It would be great if the authors could provide some theoretical / intuitive justification why this approximation is valid/accurate.  The experiments show that this approach indeed works surprisingly well.\n\nIt is not clear to me what it means that the best of both  nuclear norm and Frobenius norm regularization are obtained by these 2 methods. In this generality, I think this is not true.  For instance, Jin et al. 2021 derived a closed form solution of A6, which uses Frobenious norm regularization and is a low-rank model. Hence, obtaining a closed-form solution is not a property of the nuclear-norm regularization, as suggested/implied by the authors. The authors propose 2 methods for a closed form approximation to EDLAE--which uses Frobenius norm regularization--it is hence not clear to me why there is any need to make a connection to the  nuclear norm regularization in this section. The only remaining indirect justification could be that nuclear-norm regularization induces a low-rank model, but conversely, I am not sure that a low-rank model induces the need for nuclear-norm regularization. Moreover, the full-rank EDLAE model is usually  more accurate than the low-rank EDLAE--from this perspective there may not be any need for  nuclear-norm regularization or the low-rank version of EDLAE. \n\nSection 5:\n\nThe experiments show that the 2 proposed methods achieve high accuracy.\n\nAs a suggestion, Table 1 might be easier to read if it were explicitly indicated which models are full-rank, and which are low-rank (and which rank k).\n\nAdditional points:\n\n- Given that this paper makes several contributions that are not all connected to each other, I wonder if it would make sense to split this paper up, as it would allow for more space to discuss the various contributions in more depth.\n\n- The paper contains many typos and grammatical mistakes. Also note that 'non-descending' = 'ascending'.\n\n- below Eq 8, are Q' and P' swapped? I.e., I would have expected P' = XA^*, not Q'XA^*.\n\n- Lambda in A3 and A6 seem to be different matrices--this should be reflected in the notation.\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nUpdate based on authors' responses:\nI very much appreciate the detailed responses and improvements in the paper. As I expected that many of the improvements can be made easily, and factored them into the score (which is the highest among all the reviews), I would like to maintain my current score.\nAs an aside, I am not really convinced by the arguments for dropping the zero diagonal constraint for the low-rank model, as it is unclear to me how tight the bounds are, and hence it is unclear if optimizing the bound  really yields similar results as optimizing the original objective (with the zero diagonal). Moreover, when dropping the zero-diagonal constraint, the bound seems to be a lower (rather than an upper) one, which means that a lower bound is minimized.",
            "summary_of_the_review": "\npostive points:\n\n- results on unweighted matrix norms are generalized to the *weighted* nuclear norm, connecting the latter to the weighted Frobenius norm.\n- various models are categorized based on the used regularization (nuclear norm vs Frobenius norm), and it is shown that this explains the differences in prediction accuracy to a large degree.\n- the proposed 2 methods / approximations work well in the experiments (but are not motivated/justified conceptually).\n\nnegative points:\n\n- the 2 methods/approximations in section 4 are not motivated / justified theoretically (but they work well in the experiments).\n- the writing and the flow of the paper could be improved considerably.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}