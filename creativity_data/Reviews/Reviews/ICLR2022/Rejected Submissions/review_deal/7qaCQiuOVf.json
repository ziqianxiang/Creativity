{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a new perspective for understanding reinforcement learning policies based on meta-states, as an effort to improve the explainability of RL control policies. After reviewing the revised paper and reading the comments from the reviewers, here are my comments:\n \n- The paper is well-written and very concise.\n- The strategy is novel and deserves merit.\n- The utility of the explanation is not well described.\n- The main concerns of the proposal are the utility of the explanation (that is not well described) and its usage in large discrete state spaces or continuous state spaces domains. \n \nFrom the above, it is difficult to see the contribution and applicability of the paper in a clear manner."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an explanation method for deep reinforcement learning. ",
            "main_review": "Strength: (1) The paper proposes a new explanation method against deep reinforcement learning\nWeakness: (1) The utility of the explanation is vague and needs more details. (2) the evaluation is insufficient. ",
            "summary_of_the_review": "It seems the method cannot handle exponentially large state spaces. While the authors discuss the scalability issue, it is still difficult for readers like me to understand how to a bounded but exponentially large space. I would like to the authors provide more details. \n\nI would like to see the proposed method applied in more sophisticated games (e.g., Pong and Mujoco games) in which the states cannot be numerated. \n\nThe utility of the explanation is unclear. It would be very helpful if the authors could share their survey questions. That could better help us understand the utility of the explanation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes to explain an RL policy by clustering states into meta-states and presenting strategic state(s) for each meta-state. This clustering is performed based on policy rollouts, balancing likelihood of paths within a meta-state and number of paths from states within the meta-state. The authors present example explanations for three domains. A user study is performed to compare VIPER-D to the proposed method (SSX).",
            "main_review": "#### Local vs Global Confusion: \n\n-The authors state \"our focus is on local explanations,\" and use this to discount existing \"local\" works while not comparing to \"global\" works. However, in other places in the text, SSX is treated as producing global explanations. SSX is \"local\" only in the sense that, for larger domains, approximations are made to address scalability issues. Especially when discussing experiments, the authors use explanations to comment on overall (global) policy behavior (e.g., study subjects are asked about overall behavior not per-state behavior).\n\n-As a specific example, TLdR automatically finds landmark states based on transitions between states. This approach does differ from SSX but not to the degree mentioned in this work (which labels it a \"global summary\" method, unlike SSX). At the very least, TLdR is a good candidate for comparison.\n\n#### Complexity Concerns:\n\n-The authors note that directly applying SSX is not feasible, so a local neighborhood is used. The faithfulness of such a local approximation is not evaluated (e.g., run approximation for different N and non-approximate approach, then compare resulting explanations). \n\n-However, each neighborhood requires a number of samples exponential in the \"radius\" of the neighborhood. Experiments use a \"radius\" of at most 6. This is low for the use-cases used to motivate this work. To produce explanations for larger domains, even a single local neighborhood may not be feasible to explain.\n\n-The authors do not present a way to combine local neighborhoods. As neighborhood \"radius\" is decreased (to address exponential complexity) the number of neighborhoods to combine increases. It is unclear how to explain behaviors that consist of more actions than the neighborhood diameter. The consistency of explanations for adjacent explanations is not evaluated (e.g., produce Figure 1 for different neighborhoods, then look at whether states labeled by multiple neighborhoods create a consistent explanation).\n\n-The sharding process implicitly relies on access to a simulator to efficiently gather samples centered around different states. This is not acknowledged as a requirement/shortcoming of SSX.\n\n#### Positioning wrt Related Work:\n\n-Most relevant past work has been mentioned! Unfortunately, mentioning this work is insufficient to position SSX with respect to past work. SSX's methodology is indeed different from past work, but its final product (set of clusters with representative example(s) for each) is not novel. The authors note that some past work has \"the intent of efficient learning rather than interpretability\" but a different intent does not make a method sufficiently novel. Please position SSX with respect to past global methods, making the differences clear.\n\n-SSX is only compared with VIPER-D. Other works should be included in this comparison, especially works that produce similar cluster / meaningful state explanations. Relatedly, \"they require estimating Q-values ... which relies on sensitive hyperparameter tuning\" does not seem like a valid reason to exclude a work from this comparison.\n\n-The authors note that other state clustering methods may not map adjacent states to the same meta-state. However, SSX does not do always do this either, as seen in Figure 2.\n\n#### Problems with Example Explanations:\n\n-Explanations for Fig 2-3 are based on starting states within a single trajectory. I am concerned that the clusters exhibit \"common\" behavior within themselves specifically because the starting states are spaced apart. (e.g., if neighborhoods have radius 6 and clusters are m>6 steps apart, then of course each cluster will have similar states) To test this, multiple trials should be run and similarity should be measured across trials. Is SSX effectively just segmenting a trajectory for these figures? Please explicitly test this by comparing to baseline methods (such as creating a single \"cluster\" from each neighborhood around each starting state).\n\n-There appears to be manual tuning that is insufficiently explained. How were the starting states chosen for the different neighborhoods? How were clusters chosen for showcasing? (i.e., Fig 3 has odd cluster numbers, as though some were manually chosen to show)\n\n-The explanations given for Fig 2 (arguing that the clusters exhibit meaningful, distinct behaviors) do not match what is shown in the figure. There are images in later clusters despite occurring earlier in the episode. Specifically, \"Unlocked Door, second row, third column\" is several steps through the door despite being part of the \"going through the door\" cluster, despite \"third row, second column\" being an earlier state and in the \"moving toward the target\" cluster.\n\n#### Problems with Human Study:\n\n-It is great that a human study was done. The right feedback is being gathered from human subjects.\n\n-VIPER-D and SSX differ among several axes, so SSX's success cannot be attributed to the specific qualities pointed out by the authors. For example, VIPER-D uses a different output representation (distinct from both SSX and original VIPER).\n\n-In addition to not including past works in this comparison, a number of baselines are missing to determine which aspects of SSX are helpful. Please add these comparisons to show that SSX is meaningfully better due to its clustering approach. Perhaps showing states leads to subject satisfaction (should compare to randomly grouped past experiences)? Perhaps the trajectory is being segmented by clusters (should compare to \"local neighborhood around equally spaced starting states\")? \n\n-VIPER-D is unnecessarily modified and likely performing worse than original VIPER. VIPER can already handle discrete states. VIPER is designed to work with an iterative state gathering method, not simply the full state space as done here. ",
            "summary_of_the_review": "The authors present a new way (SSX) to create a certain style of explanation, but the style itself is not novel. SSX is not shown to be better than existing methods. Furthermore, its computational complexity is concerning, and the authors do not fully address how to apply their method to larger domains. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach to interpret a black-box control policy of a reinforcement learning (RL) problem such that its interpretations can be understood by a human user. For a given expert policy (i.e., a fitted deep neural network), the proposed approach uses transition probabilities induced by the expert policy to define regions of the state space that are “similar.” These policy-dependent regions are referred to as meta-states and are computed using an algorithm similar to spectral clustering. The proposed method then computes so-called strategic states for each meta-state, where strategic states of a meta-state are those states that belong to the meta-state and bridge the meta-state to other ones. In other words, the expert policy should pass through the strategic states frequently when it goes from one meta-state to another. The paper performs numerical experiments using standard RL applications, i.e., four rooms, door-key, and mini-Pacman, to show the effectiveness of the proposed approach.",
            "main_review": "As motivated by the first paragraph of the paper, interpreting RL policies such that a human user understands the reasoning behind these policies is a critical step toward using these policies in a trustworthy manner in real-world domains. Unfortunately, the paper does not show the usefulness of the proposed method on any real-world application. Studied applications are grid games that may not reflect real-world applications such as those that arise in queuing, dynamic pricing, revenue management, among many others. As explained above, the notion of meta-states and strategic states for applications beyond gird games is unclear. Particularly, how should a human user interpret strategic states in real-world applications? The interpretation of strategic states for these applications may not be as simple as doors in a grid game. Therefore, at the very least, I recommend authors provide detailed explanations on how their approach can be employed in real-world settings, i.e., in robotics and business applications. Moreover, conducting a numerical study that applies the proposed method to a real-world application can significantly add to the paper's contributions.\n\nThe authors assume that the dynamics of the expert policy π_E, denoted f_E, are known apriori, and thus Γ can be computed exactly and provided to Algorithm 1 as an input. However, such dynamics are often unknown in practice and should be estimated through policy simulation, that is, one can only estimate f_E by simulation of π_E. Authors need to pose their method and the computation of matrix Γ through policy simulation. \n\nTo build on my previous concern, computing the entire matrix Γ, as done in the current version, is expensive and seems unnecessary. In many domains, a good policy (i.e., the expert policy) only visits a small subset of states with a non-negligible probability. In this case, is not it enough to restrict the computation of Γ to such states? If so, authors can more efficiently compute Γ by only focusing on the states observed by π_E during simulation. Please also comment on the computational cost of getting the eigen decomposition of the Laplacian of Γ needed in Algorithm 1. Can this be efficiently computed for large-scale problems? Given the current manuscript, I am not convinced that the computation of Γ and its Laplacian eigen decomposition are tractable for large-scale problems. \n\nThe authors mentioned the convergence of their method multiple times in the paper without carefully explaining what their algorithm converges to.  Proposition 1 proves the convergence of Algorithm 1 without identifying what does Algorithm 1 converges to. Can authors show that if a given expert policy induces a collection of k meta-states \\{Φ_1^E,…,Φ_k^E \\}, then Algorithm 1 converges to this set of meta-states? If not, how the output of Algorithm 1 is connected to the expert policy? The current version of Proposition 1 is not insightful. Moreover, the convergence results in Algorithm 1 should be presented when Γ is estimated through policy simulation and not when Γ  is known exactly (please see my comments above). \n\nI wonder how the choice of parameter η affects the interpretability of the expert policy. Some theoretical or empirical studies are useful here. For example, the authors can explain in the extreme cases of η being zero and being a large number, how does Figure 1-a change? Is there a need for fine-tuning η to get human-understandable meta-states? Moreover, how does Figure 1-a change for different values of λ?\n\nIt is not clear if the program (3) has a unique solution or not. I think there are examples at which multiple strategic states exist, given a fixed m and meta-state. In the case of degeneracy, how should a human user choose a solution? I encourage authors to comment on this.\n\n\nMinor comments.\n\t\nConsider rewriting the sentence “… which act as intermediate goals for states belonging to a particular meta-state”. I had read this sentence multiple times to understand what a strategic state means. \n\nTerm “policy dynamics” is unclear in Section 1. Authors may need to explain what they mean by policy dynamics in that section.\n\nColoring in Figure 1 is hard-to-follow. Please consider using different colors and using different markers.\n\nThe sentence “While one could discretize the state space, it would be interesting to see if it can be symbiotically done” is misleading. In many real-world continuous-state MDPs, discretizing the state-space leads to poor approximations and is thus not a solution.\n\nFigure 2 is very confusing. I had to read the text and check the figure multiple times. Is there a way to enhance it?",
            "summary_of_the_review": "My overall impression of the paper is positive, but I have several concerns. My most critical concern is that given the theory and numerical experiments in the current manuscript, I am not sure if the proposed approach delivers human-understandable interpretations of RL policies in applications beyond grid games arising in robotics, flight control, multi-armed bandit, etc. In other words, the proposed method seems to work only for the grid games, and it is unclear how a human user should interpret meta-states and/or strategic states in applications beyond grid games. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel explanation method for DRL policies that clusters environment states into meta states and identifies strategic states from meta states as explanations. The paper empirically compares its proposed explanation method with some existing methods on three games and designs a user study to demonstrate its utility. ",
            "main_review": "This paper explains DRL policies from an interesting perspective, i.e., explaining via prediction. It draws insights into a DRL agent's policy by predicting certain steps of future moves. Although studying an important problem, this paper has the following limitations that prevent me from supporting it. \n\n1. The readability and clarity of this work can be further improved. Due to some unclear terminologies, the proposed technique is not clearly described. Specifically, I would first suggest giving a concrete example to demonstrate the difference among states, meta states, and strategy states, especially strategy states. Neither Fig. 1 nor notions clearly express that. In addition, the papers suddenly throw out many termonologies without explanations, such as state-space topology, bottleneck, strategic policy. To be best of my knowledge, these terminologies are not specific to RL and may cause confusion. I would suggest giving a clear explanation or definition before using them. \n\n2. The proposed technique is actually mainly algorithms 1 and 2, where Algorithm 1 is like a clustering algorithm, and Algorithm 2 is a simple searching algorithm. I thus am a little bit concerned about the technical contribution of this work. It may not be reaching the bar of a top-tier ML conference. Regarding Algorithm 1, I am also wondering if one can directly apply an existing clustering method to replace it, like using K-NN or K-means based on $\\Gamma$. \n\n3. The paper misses a potential baseline that also explains DRL policies through action prediction, i.e., [1]. I would suggest the authors discuss the difference between [1] and the proposed technique and even empirically compare them.  Regarding the user study, existing works utilize user study to compare the utility difference among different explanation methods in identifying good/bad policies, e.g., [2,3]. I would suggest the authors comment on their choice of user study design and why not follow the existing work to compare policy identification ability.  \n\n4. It's not clear the problem space of the proposed technique or, in other words, what types of RL tasks are suitable for the proposed technique. More specifically, I am wondering whether the proposed technique can be applied to widely used Atari and MuJoCo environments or even real-world environments, like Go and Texas hold 'em. \n\n[1] Explain via prediction: What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes, NeurIPS 20.\n\n[2] Visualizing and understanding atari agents, ICML 2018.\n\n[3] Highlights: Summarizing agent behavior to people. AAMAS 2018",
            "summary_of_the_review": "This paper studies an important problem from an interesting perspective. It explains a policy by predicting certain steps of future moves. In summary, I identify the following limitations from this work:\n\n1. The readability and clarity of this work can be further improved. Due to some unclear terminologies, the proposed technique is not clearly described.\n\n2. The technical contribution may be a little bit thin, and some design choices are not clearly justified. \n\n3. The paper misses a potential baseline that also explains DRL policies through action prediction. \n\n4. It's not clear the problem space of the proposed technique or, in other words, what types of RL tasks are suitable for the proposed technique.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper provides a discussion on ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}