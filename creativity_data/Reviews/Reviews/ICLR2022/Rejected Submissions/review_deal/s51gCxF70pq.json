{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a reinforcement learning architecture that uses an auxiliary k-step step loss in the context of continuous control from image-based states.\n\nWhile the topic is relevant and potentially impactful, several reviewers have major concerns about the manuscript. Among these, I highlight:\n- Reviewers J6YX, 38iT and Qru8 have concerns about the novelty and contribution of the approach compared to existing literature.\n- Reviewers J6YX, TKuY, 38iT and Qru8 have concerns about the experimental evaluation and the quality of comparisons to baselines.\n\nOverall, it seems that the paper would benefit from further polishing."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper shows that an auxiliary self-supervised task that enforces temporal consistency of latents improves sample efficiency in continuous control environemnts, and identifies important implementation details that make it work. ",
            "main_review": "Data-efficiency of RL algorithms is an important research area, and this paper explores how auxiliary self-supervised learning can improve data-efficiency in continuous control domains. The paper is very rich with experimental details, the implementation choices are carefully ablated and the paper is overall well written and explained. \n\n### Similarities with SPR\n------\nMy main issue with this paper is the fact that it positions the K-step latent objective as a “new representation learning method”, whereas in fact the K-step latent is exactly the representation learning method used in SPR [1]. Throughout the abstract, intro and the methods section, the paper positions KSL as a new representation method: but in practice it’s an adaptation of SPR to continuous control that requires some implementation changes. The entirety of section 3.2 and the Figure 1 is exactly SPR (see Section 2.2 and Figure 2 in the SPR paper), but the paper makes no references to it except in the related work section. The related work section again fails to acknowledge that KSL and SPR share the same representation learning objective, and not only the architecture. \n\nThe authors could very well have positioned this paper as “We applied SPR to continuous control, here’s some implementation changes we needed to make along the way to make it work”, and it would have been a much more honest and accurate description of the work.  The empirical contribution here and the detailed analyses itself would have been valuable on its own. \n\n### Questions on Experiments\n------\nThe description of the “Generalization of Encoders” experiments is very sparse. Can you specify what exactly the training tasks and evaluation tasks are in the generalization experiment? \n\nFor the invariance experiment, it would have been nicer to see invariance to real-world distractors and not artificial noise. I would recommend the Distracting Control Suite [2] for more convincing experiments around these.\n\nFor a lot of results, the performance seems to be under-reported than the original results in papers, especially for RAD and DrQ. Here’s a link to the raw performance scores for baseline methods used in [3] https://console.cloud.google.com/storage/browser/rl-benchmark-data/dm_control, and these were reportedly obtained from the corresponding authors. Can you clarify the discrepancy in the performance data? This seems to be a major issue. \n\nAdditionally, in a lot of performance curves, the standard deviation regions overlap, making it harder to establish stochastic dominance of one method over another. It would be nicer to see a better stochastic analysis using stratified CIs on multiple normalized metrics (see Figure 11 in [3]). You can do this easily via the colab: https://bit.ly/statistical_precipice_colab\n\n[1]  Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., & Bachman, P. (2020). Data-efficient reinforcement learning with self-predictive representations. ICLR 2021. https://arxiv.org/abs/2007.05929\n\n[2] Stone, A., Ramirez, O., Konolige, K., & Jonschkowski, R. (2021). The Distracting Control Suite—A Challenging Benchmark for Reinforcement Learning from Pixels. https://arxiv.org/abs/2101.02722\n\n[3] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., & Bellemare, M. G. (2021). Deep reinforcement learning at the edge of the statistical precipice. NeurIPS 2021 https://arxiv.org/abs/2108.13264",
            "summary_of_the_review": "Data-efficiency of RL algorithms is an important research area, and this paper explores how auxiliary self-supervised learning can improve data-efficiency in continuous control domains. The paper is very rich with experimental details, the implementation choices are carefully ablated and the paper is overall well written and explained. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of sample inefficiency in continuous control, by noting that standard RL methods deal with both policy optimisation and representation learning jointly with a single supervisory signal, namely the reward. Consequently, authors propose to leverage long-term temporal connections between actions in the representation learning, and introduce k-Step Latent (KSL), a representation learning module for learning temporally consistent representations of the state space. Authors show that KSL improves previous state-of-the-art methods in PlaNet benchmark suite and provide some analysis of the representations learned by KSL.",
            "main_review": "The paper addresses a very important problem in RL: data efficiency. The author's perspective on leveraging long-term temporal connection is not exactly novel (see Self-Predictive Representation [Schwarzer et al. 2021], Successor Features [Kulkarni, et al. 2016, Barreto et al. 2017]), but the specific method introduced seems to be novel, to the best of my knowledge.  \n\nOn the method:\nThe motivation is clearly stated and makes sense to me. I would have like to see discussion on how this differ/relates to successor features, when learned jointly or separately. \n\nA claimed \"that learned representations of the state space should relate to reward\" is not really justified: there are data-efficient method that disentangle the reward from the representation. It also seems to contradict another desired property: generalisation from one task to another. \n\nOn the experiment:\nThe experiments are extensive and the method is compared against sensible baselines. The results with respect to data-efficiency are promising.\n\n\nminor comments: \n- the font is very small on most figures axes\n- figures 5.2, 5.3 and 6 would make more sense with a different y axis scale.",
            "summary_of_the_review": "The main idea behind the paper is not novel, but the implementation is, and the results are promising. Some claims are not founded and somehow contradictory, and some related works are missing. But overall an interesting contribution!",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper applied \"Bootstrap your own latent\" (BYOL) to the case of RL by introducing an additional learned transition model and show that this can improve sample efficiency.",
            "main_review": "Strenghts:\n* Simple approach\n* Clearly written\n* Representation learning for better generalization or sample efficiency is an important topic in RL\n* Positive experimental results\n\nWeaknesses: \n* My main worry with this application of BYOL to RL is that the introduction of the transition model T changes the 'support' of z_m away from the support of z_o. In other words, while psi_o expects an output of T, psi_m gets the direct output of phi_m for which it was not trained and which might be entirely different from the output of T. Based on the experiments, this still seems to do something useful, but I would argue that psi_m should be seen more as random mapping than as the slow moving state-encoding. In any case, I think this out-of-distribution problem for psi_m should be addressed in the paper. \n* For Figure 3: Why not use t-SNI instead of only the first two dimensions of PCA? In particular, while it's not wrong to say that \"DrQ's projections show little sign of reward-based orgnaization by 15k steps\", that is slightly misleading as it doesn't say anything about the latent representation as we're only looking at 2 principal axes. \n\nAdditional Questions:\n* How is translation augmentation applied?\n* Nit: How would the results change when removing the sg before the policy?",
            "summary_of_the_review": "An interesting direct application of BYOL to RL. However, the necessity to include action-conditioned transition models in RL raises additional complications compared to BYOL which have not yet been addressed (or discussed) and I believe these should be included in the paper before publication. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a representation-learning method (k-step latent, or KSL) that uses a self-supervised auxiliary loss between recurrently-predicted action-conditioned representations of the state space and non-recurrently predicted target representations, in the style of BYOL. The method is trained using two separate optimizers on different parts of the models, to avoid interference in the statistics maintained by the optimizers. Results at 100k and 500k steps on 6 tasks from the DM control suite (from pixels) compared to methods using alternative self-supervised auxiliary losses show that the proposed method improves data efficiency. Analysis of the learned latent representations shows that those from KSL produce more robust encoders and are more consistent with the underlying MDP.",
            "main_review": "**Clarity.**\nFor the most part, the writing is clear. One minor concern is that the description of KSL (sec 3.2) is difficult to understand, and could be made much more clear with better choices of notation, including the pseudocode in the main text, and an explanation of the algorithm that follows the pseudocode. \n\nThat said, the main issue with the clarity and writing of the paper is that the contribution is not made clear both in the paper nor in the experimental analysis. Is the paper about using k-step latents plus an auxiliary loss tying these to observations? Or is the paper about the specific form this auxiliary loss takes? Most of the writing and the naming (KSL) imply the former but the evaluation speaks to the latter.\n\n**Novelty and significance.**\nClarity aside, the paper does not propose a sufficiently novel contribution for acceptance. There is a very large body of work in model-based reinforcement learning that uses k-step model-based predictions and corresponding losses to improve data efficiency and performance, and reduce model rollout errors. Some of this is even cited in this paper, but others that are not cited include Recurrent environment simulators by Chiappa et al 2017, TreeQN by Farquhar et al 2018, MuZero by Schrittwieser et al 2019, and Muesli by Hessel et al 2021. These methods all use k-step latents with a loss tying them to (encodings of) observations.\n\nThe specific form of the auxiliary loss here, adapted from BYOL (which is not even mentioned until the related work for some reason), is very similar to that of SPR, which is cited by the paper. There are some minor differences but these mainly seem like implementation details and since there are no empirical comparisons I have to assume this is the case.\n\nFurther, the experiments themselves are extremely limited. Evaluating only on 6 tasks from the DM control suite is not enough to show that this is a compelling and useful contribution, especially given the high overlap with prior work. The additional analysis of the learned representations are nice, but are not enough without showing the strength of the proposed approach, or else doing a much more thorough analysis. Finally, I’d like to see ablations of the components and choices made for the proposed method. Why have both \\Psi_o and P? Is using the EMA for the momentum pathway the best choice? Is a normalized L2 loss the best choice?\n",
            "summary_of_the_review": "Overall, this paper lacks sufficient novelty for acceptance. It recombines existing techniques in a slightly different way than previously and shows improvements on a very small and narrow set of environments without comparing to the most relevant related work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary:\nThe authors introduced k-Step Latent (KSL), a representation learning method for visual-based continuous control tasks. KSL utilises multi-step latent action-dependent predictive supervision for training the representation. The empirical evaluations are based on the dm-control suite benchmarks, KSL demonstrates improved sample efficiency (100k evaluation) and asymptotic performance (500k evaluation) across the six tasks presented, comparing to the baseline algorithms (mainly based on image augmentation). The authors further empirically examined the properties of the learned representations, and showed that the trained encoder quickly learns to be representative of the reward structure. The authors also argue that KSL supports more robust representation learning and stronger generalisability.\n",
            "main_review": "Pros:\n\n - The paper is well-written and easy to comprehend;\n\n - The empirical evaluation on dm-control suite indeed shows that KSL yields state-of-the-art results on the six presented tasks;\n\n - The choice of using independent optimisers for training the representation learning module given the signals from the predictive latent supervision (Eq. 4) and critic training (Eq. 1) respectively raises a good point for representation learning in RL with auxiliary tasks, from multi-task learning perspective. Such inductive bias is additionally substantiated with empirical comparisons.\n\nConcerns:\n\n - The main evaluation on the overall performance is rather limited, it might worth showing more (especially the middle/hard tasks as defined in Yarats, et al. 2021);\n\n - The KSL model combines many existing techniques in representation learning for RL, such as image augmentation (Laskin, et al. 2020) and multi-step latent predictive supervision auxiliary task (Schwarzer, et al. 2021), leading to limited novelty of the proposed KSL model.\n\n - The multi-step latent predictive supervision objective for representation learning in KSL is highly similar to the SPR model (Schwarzer, et al. 2021), it seems like the authors acknowledge the similarity hence spent a paragraph discussing the difference, but discussions are mainly based on the architectural/training differences. From this perspective, KSL appears as an adaptation of the SPR model to the continuous control tasks (despite the admirable engineering efforts). The difference argument could be made convincing given that the authors could provide further clarifications of the differences between KSL and SPR, from an algorithmic perspective, and provide further empirical comparisons between the two agents (e.g., the authors state that \"KSL’s architecture is general enough to be applied in both discrete- and continuous-action domains\", hence it would be interesting to see an adaptation of KSL to discrete control and test it on atari benchmarks, and compare with the SPR results).\n\n - The arguments of the improved representation learning of KSL in terms of \"Permutation Invariance\" and \"Temporal Coherence\" seems poorly justified. The reported figures do not show significant or consistent improvement of the KSL over the baseline methods, only by largely zooming in the y-axis could one observe some differences. I doubt if such minuscule differences are consequential in the overall learning.\n\n - I think using the momentum encoder to provide input to the target Q-network in SAC critic-training is an interesting choice, but lacks further (theoretical) justification, i.e., why would this be better than simply using the online encoder as the inputs to the target Q-network, is it possibly because of the consistent temporal lags?\n\nMinor Points:\n\n - The main empirical evaluations of the learned representations is based on the walker-walk task, which is a simple task with dense reward structure(Yarats, et al. 2021). It would be more interesting to see how the learned representations are indicative of the reward structure in the sparse reward tasks, such that Cartpole-Swingup.\n\n - KSL is motivated by the inductive bias that \"States that are nearby in time are likely to share high levels of mutual information\", another similar work that utilises multi-step action-dependent latent predictions by Whitney, et al. (2019) is based on the inductive bias that the similarities between the embeddings for the states and/or action sequences should be based on their successor outcomes (e.g., successor representations). It would be nice to see some discussions on the relationship between the two seemingly independent inductive biases.\n",
            "summary_of_the_review": "Scores:\n\nI suggest marginal rejection (5/10). KSL indeed show state-of-the-art performance on the presented tasks and I like the way the authors assessed the quality of representation learning. However, KSL seems like a combination of existing methods, but lacks comprehensive empirical evaluation in that sense. Moreover, the high similarity with SPR is concerning without further clarification and empirical justification. Some arguments of the improvement learned representation is over-stated.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}