{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes SimpleBits, for simplifying input images to remove irrelevant details but keep relevant details for classification. This idea can be applied during/after training. Authors have significantly revised the draft to address reviewer concerns, to improve the readability and clarify concerns on complexity analysis, for which reviewers have raised scores post-rebuttal. However, even with score changes, there are commonly expressed concerns, that manuscript still needs some more improvements to be ready for publication in their post-rebuttal comments: findings are not very nontrivial or significant (reviewer eYVm), still incomplete (RfmX) or optimization algorithm is yet to be found (reviewer agcx) ."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "- Draft proposes a simplification method for visual recognition task where the information less relevant for the classification task is removed from the samples. In an iterative fashion the proposed approach (with no domain knowledge) synthesizes the simplified samples. Then the draft attempts to (i) measure the effect of the simplification via evaluating the models trained on the simplified samples, (ii) understand the models' behavior via posthoc interpretations, and (iii) simplify the synthetic data learned through dataset condensation.",
            "main_review": "- Draft discusses an interesting problem. However, in the end-to-end learning framework it is likely to find task-irrelevant information in the input. While surely it is an interesting investigation to understand the amount of relevant information in the inputs, authors may better highlight practical utility of the simplified samples (or simplification).\n\n- Since the simplified images have less irrelevant information how do they affect the time taken for training the models compared to the original samples? \n\n- On the same lines, how can one leverage the trade-off between the model performance and sample simplicity? In other words, in the conventional training scenario the reader may like to know if there are any advantages/applications for sample simplification. For instance, do the simplified samples occupy lesser memory footprint than the original samples?\n\n- Proposed per-instance simplification post model training can help to explore model behavior as a posthoc interpretation. However, it looks complicated to obtain as opposed to multiple gradient based posthoc explanations. It is not clear if they are related in some way.\n\n- Shouldn't the notion of sample complexity be task specific? How is it justified to interpret the negative likelihood of the sample to be its complexity in the classification task?\n\n- Authors use bpd as the image complexity measure which treats an image with more likelihood under a generative model trained on Tiny images dataset as less complex. From Figure 2 of the draft, a complete black image has close to 0 bpd, does that mean it has very strong likelihood? However, in general such images are not common in real datasets. Authors are suggested to clarify this. \n\n- Also, how representative the computed likelihood can be, given that it is applied across different datasets (gray scale, color, etc. sometimes in different domains such as medical images) despite being learned on a specific dataset? \n\n- In the experiment with the side-by-side MNIST dataset, SimpleBits output erases major portion of the relevant information also. I wonder what is the classification accuracy on the side-by-side data and the actual MNIST test data are.",
            "summary_of_the_review": "- While the draft discusses a novel and interesting problem,  I feel it lacks in some of the aspects (listed in the Main Review). The work can be considered as a good early work in this direction. If the authors can justify the motivation for chosen complexity notion, and the effectiveness of the proposed posthoc interpretation along with other minor issues, this can become an excellent paper.\n\n- Pre-rebuttal score: 5 (marginally below)\n-----------------\npost-rebuttal\n----------------\n\n- Rebuttal partially convinces me on some of the aspects (e.g. sample complexity) so I increased the score to 6. \n- However, after reading other reviews and authors' rebuttal I feel that the manuscript needs some more improvements. \n- For instance, primary aim of the manuscript is to investigate the effect of reducing information on classification, and I feel the findings are not very nontrivial or significant. In the end-to-end framework where the sample is presented to the learning algorithm as it is, one can easily see that he/she can find irrelevant information in the sample. And the framework deployed in the manuscript is very much inspired from the meta-learning approaches used for dataset distillation, condensation works. Also, the observations have not been leveraged (or harvested) in a significant way.\n- Despite my earlier points, I feel that these investigations have potential to better understand the information that the classifier networks might be relying for learning. Hence, I score 6, but I will not fight for this manuscript in a discussion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed SimpleBits, which is a method to create simplified input images that retain the most relevant parts for classification while removing irrelevant details. The core of SimpleBits is the simplification loss, which is the negative log likelihood of seeing an image, according to an already trained generative model (as well as a related concept known as bits per dimension, which is simply the simplification loss divided by the dimension of a flattened image). The authors showed empirically that more complex images generally have higher bits per dimension/simplification loss.\n\nThe authors applied SimpleBits in three settings: (1) during training, (2) after training, and (3) dataset condensation/summarization. In the first setting, the authors trained both an image classifier and a simplifier network in tandem -- in particular, the simplifier network is trained by minimizing both the classification loss and the simplification loss. In the second setting, the authors tried to create simplified versions of input images, especially input images that are wrongly classified, to hypothesize (posthoc) about how an already trained network made predictions. In the third setting, the authors combined simplification loss with the gradient matching loss (Zhao and Bilen, 2021) to create a smaller dataset with simplified images, and showed that simplification does not have significant impact on accuracy.",
            "main_review": "The proposed method has the potential to illuminate the important regions of an input image for classification while avoiding having to manually occlude parts of images to figure out the most relevant image parts. I especially like the idea of jointly optimizing the simplifier network and the classification network -- in this way, the simplifier network can explain the prediction made by the classification network, by showing the simplified version of the input image that retains the most salient features for classification, and we know that the classification network can perform reasonably well on simplified images. The integration of the simplification loss and the gradient matching loss for dataset condensation is also a great idea, because it allows us to not only condense a dataset but also illuminate the important aspects of each class in the dataset.\n\nI have some questions/concerns regarding the use of SimpleBits for posthoc explainability analysis of an already trained model (Section 4). Is the goal to train a simplifier network to explain the prediction of an already trained image classifier (by showing a simplified version of the input image that contains the most relevant parts for classification)? I am not entirely sure from the reading of Section 4. Is Algorithm 1 (or Algorithm 2 in the supplement) trying to describe how one computes the loss function for training the simplifier network in this case? Do you then perform back-propagation to update the simplifier network? Why is scaling of parameters of the trained network needed? Also, what is the purpose/meaning of the gradients of the KL-divergence between the rescaled network's prediction and the original network's prediction? Section 4 needs a lot more explanations than what is currently written -- it needs to be rewritten for better clarity. Also, how does the proposed method compares with existing posthoc techniques (qualitatively or quantitatively)?",
            "summary_of_the_review": "The paper is for the most part well-written. Section 4 needs serious improvement in terms of clarity -- in particular, the authors should provide intuition and clearer explanations about how they performed posthoc explainability analysis using SimpleBits/a simplifier network (e.g., explanations about L_grad and L_pred in Algorithm 1 and why scaling of parameters of the trained network is needed). The authors should also consider comparing their methods of generating simplified images with existing posthoc techniques (e.g., saliency techniques). For a given input image, is the simplified image retaining regions similar to those identified as salient by an existing saliency method?\n\n******************************\npost-rebuttal:\n\nThanks for the clarification of Algorithm 1/2. After reading the response and the revised paper, I still think that the paper will benefit from a better explanation of how to synthesize a simplified image for a given input image for an already trained neural network. I understand that Algorithm 1/2 details the process of how to compute the loss function, but there is still a lack of explanation of how to optimize the loss function in the latent space. Essentially, I want to the following question be answered clearly: how do you perform the optimization in the latent space of the pretrained invertible generative model, using the loss function you computed in Algorithm 1/2? Right now, there is only an algorithm for how to compute the loss of the optimization problem, but I cannot find the optimization algorithm itself. Therefore, I am keeping my original score of 6.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to minimize the image per-instance complexity to interpret the essential ingredients for neural network learning. The proposed method, SimpleBits, works in three different settings _i)_ Simplification during training _ii)_ Simplification after training _iii)_ Simplification with dataset condensation. \n\nIn _i)_, SimpleBits is able to differentiate task-relevant information on simulated composite datasets. In _ii)_, SimpleBits is able to explain the possible reason for misclassification. In _iii)_, SimpleBits reduces the image complexity in dataset condensation even further.\n\nThe major contributions can be summarized as follows.\n\n- Simplifying images to interpret the neural models is not new, while this paper performs the novel simplification on image complexity.\n- The paper proposes a way to simplify image complexity while retaining classifier accuracy.\n- The paper shows that SimpleBits helps interpret the model learning in three different cases.",
            "main_review": "Overall, the paper provides a diverse set of experiments showing that SimpleBits is able to reduce the image complexity in terms of bpd (bits per dimension) while retaining model performance at the same time. The proposed method is fully data-driven and does not have any strong assumptions. \n\n**Strength**\n- The proposed loss terms $\\lambda_{sim} L_{sim} + L_{cls}$ are easy to implement\n- The paper provides ways to apply the simplification network in various settings\n- I like figure 4, where it is easy to see the progress of the simplification\n\n**Weakness**\n- Section 4 is hard to understand. What is the motivation for simulating forgetting? In Algo.1, what are $\\mathbf{h}$ and $\\mathbf{h}_{sim}$ in L4, 5? The gradients of the returned loss are taken w.r.t to which parameter, simplification network or $f$?\n- In Section 5, it is unclear if the motivation here is to condense the dataset further or interpret neural nets. \n- In the dataset condensation part, prior work (Zhao & Bilen, 2021) performs gradient matching in the inner loop and re-initializes the model parameters in the outer loop. Therefore, for each outer loop, a different neural network is used. It is unclear which model the proposed method is interpreting. In the rebuttal, the authors can explain the details of combining simplification and dataset condensation more.\n\n**Other**\n- For section 3 (simplification during training), can the proposed method identify spurious features learned by the neural nets?\n",
            "summary_of_the_review": "The paper proposes a novel direction of simplifying images (in terms of image complexity) to interpret neural networks. The proposed method is intuitive and easy to implement, having the potential to make broader impacts. \nThe paper applies the proposed, SimpleBits, in three different settings *i)* simplification during training *ii)* simplification after training *iii)* simplification with dataset condensation. \n\nFrom the title, the paper's main goal should be understanding and interpreting neural networks, while I found the contents sway between interpreting models and simplifying images. Without further elaboration, better simplifying images does not naturally imply better model interpretation. \n\nOverall, I think the method is novel, and the experimental results look convincing, but the explanation and motivation are unclear, especially sections 4 and 5.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to simplify images by reducing their information content, and measure the resulting effects on several aspects of learning: removing redundant image features for network training, understanding classification errors via visualization, and data condensation.",
            "main_review": "I find the visualization tool proposed in the paper very interesting but I encountered two important limitations:\n\n(1) Poorly explained methodology\n\nIt's not clear what the $unrolledtrainstep$ function (Eq.3) implements. I don't think this problem formalization is common knowledge, so it should be more thoroughly explained.\nSimilarly, the $simplifier$ function (Eq.2) is not described in the main paper.\n\n\"...adding the classification losses on the simplified data both before and after the unrolled training step improves training stability\". \nThe rationale for *why* this works is not provided.\n\n(2) Lack of baselines and comparisons\n\nThe simplifier function is not investigated, so it's impossible to know if the method is sensitive to this component. The choice of the specific simplifier used is not justified.\n\nOn the condensation experiments there's no comparison with other condensation methods in the literature (Fig.7).\nThere's also no natural baseline provided, e.g. a dataset of the same size as the original (or until performance levels off) in order to understand how good this type of condensation can be in the limit.\n\nSimilarly, the results on training with simplified images (Fig.5) do not include baselines or other competing methods.\n\n(other comments)\n\nFig.6: It would be interesting to see what the SimpleBits output is for the control image. For example, does the simplified image of the deer change considerably when the background color (outside the circle) is manually corrected? \nAre there cases where after correcting the suspected cause of error, the image is still misclassified? It would be good to show some unsuccessful/hard cases.\n\nSec.4: As a visual inspection tool, this is interesting, but the manual process of altering the images is unlikely to be viable for many applications like large datasets or streaming data). Do the authors anticipate a way of automating this process? ",
            "summary_of_the_review": "In my opinion the methodology section must be more thoroughly explained, and comparisons/baselines must be added. For these reasons I recommend that the paper is rejected in its current form. I think that the visualization tool proposed for diagnosing errors could be useful, so I would encourage the authors to improve the manuscript.\n\nScore: 3: reject, not good enough.\n\n============== AFTER REBUTTAL\n\nI have updated my score for this paper, following the authors' clarifications. However, I still think that the paper is incomplete with respect to baselines using alternative image compression methods or other techniques.\n\nUpdated Score: 5: marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}