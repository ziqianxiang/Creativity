{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper develops a new method, named Augmented Intermediate Level Attack (Aug-ILA), to improve the transferability of black-box attacks. Specifically, the proposed Aug-ILA contains three key modules: image transformations, reverse-adversarial update, and attack interpolation. \n\nOverall, the reviewers think it is an interesting paper, but are concerned that the original ablations are not enough to support the effectiveness of the proposed method, including missing strong baseline attacks and defense methods, and only one dataset is considered. During the discussion period, the authors actively provide new results. However, the Reviewer TcRw and the Reviewer 2dLg are not fully convinced by the rebuttal, especially regarding 1) in these additional experiments, no comparison is provided with other SOTA attacks beyond ILA-based approaches; 2) Table 11 shows the proposed method even degrades (rather than improves) the performance of VNI-CT-FGSM on defense models;  3) the attack rate of the proposed method is sensitive to the selection of layers, therefore, need to be carefully tuned in experiments (which could lead to unfair comparisons to other attacks). These concerns are indeed legitimate, and should be addressed carefully before publication.\n\nI encourage the authors to incorporate all the reviewers' comments and make a stronger submission next time."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed the augmented Intermediate Level Attack (ILA) algorithm to strengthen the transferability of adversarial examples. Also, it claimed that increasing the diversity of input references could improve the generalization of adversarial examples when attacking different models. Specifically, this paper performs augmentation operations, including common image data augmentations and transformations exploiting adversarial perturbation (e.g., reverse adversarial updates and attack interpolation on the reference attack), before\nmaximizing the projection of intermediate feature map discrepancies.",
            "main_review": "**Strengths**\n\n1. This paper is well organized.\n2. The proposed AUG-ILA algorithm is simple, practical, and effective. Experiments on various datasets show that it outperforms both the original ILA algorithms and other state-of-the-art methods with a large margin.\n\n**Weakness**\n1. Lack of novelty. Although the experiment results are impressive, the proposed AIG-ILA algorithm is a very practical extension. No new theories or methods about ILA are proposed.\n2. In Section 3.1, this paper claims that the reverse adversarial update is used to boost the confidence of clean image $x$. The update formulation is $x − (x^\\prime− x) = x − \\epsilon$ where $\\epsilon$ is the adversarial\nperturbation. Although $x − \\epsilon$ will decrease the loss for the original model, it doesn’t always decrease the loss of the target model. So the reverse adversarial update operation that will boost confidence might not be well supported unless the transferability of adversarial examples is proved.\n3. This paper argues that reverse update operation will help to get more useful attack information from discrepancy feature maps. It is better to show some visualization results to support this view.",
            "summary_of_the_review": "Generally, this paper proposes a simple and practical extension of the ILA algorithm. The experiments on various benchmarks are strong. However, this paper lacks the support of theory and some views might not be well supported.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces improvements over Transferable Intermediate-level Attacks (ILA), by incorporating data augmentation into the attack tuning stage. The data augmentations introduced include 3 kinds: simple transformations (cropping), reverse-adversarial update, and attack interpolation. An empirical study has been conducted on Imagenet, and 3 well-known architectures have been used as source models, and the attack transferability was evaluated onto 9 models using I-FGSM with $L_{inf}$ distance and 2 different perturbation sizes.\nAblation studies have been conducted to demonstrate the effectiveness of hyperparameter choices, and different kinds of data augmentation used, demonstrating the effectiveness of the proposed approach compared to other ILA-based blackbox attacks.\n\n",
            "main_review": "strengths:\n===========\n- Using more number of reference attacks seems to be a valid solution (as also done in ILA++), and using data augmentation techniques to achieve this goal is logical, providing a simple solution with significant improvements.\n\n- The empirical results consistently show significant improvements to other approaches, specially to closely related methods such as ILA and ILA++.\n\n- The study of hyperparameters supports the choices made\n\n- Ablation result demonstrates the effectiveness of each data augmentation\n\nweaknesses:\n===========\n\n## lack of comparison with published results:\n\n- The configurations used (e.g, perturbation size, dataset combinations, architecture type) in the reported experiments are different from those used in the published results, and it appears all results are reported from experiments carried out by the authors. This raises some concerns on whether the baselines were used justly, with best hyperparameters etc. For example ILA++ [Li et at., 2020] reports results on CIFAR100, ILA [Huang et al,. 2019] on CIFAR10 and ImageNet (but different models than used in this work). Why isn't the models/dataset combinations used in related work (e.g, in [Wang et al., 2021], [Wu et al., 2021]) aren't used? Also it is unclear which methods from [Wang et al., 2021.] are dubbed as VMI-SI-DI-TI-FGSM, and what are the related details regarding how they relate to VMI-SI-DI-TI-FGSM ? Also it seems that in  [Wang et al., 2021.] VNI-FGSM performs better than VNI-FGSM. What's the reason for choosing VMI-FGSM over VNI-FGSM? As for I-FGSM + LinBP + SGM + ILA, and LinBP from [Wu et al., 2020], it is unclear what method from [Wu et al., 2020] is I-FGSM + LinBP + SGM + ILA referring to.\n\n\n\n- The proposed method's empirical performance is only tested on one dataset, and its generalization capabilities onto other datasets with the tuned hyperparameters is not demosntrated. Providing results on other datasets (e.g, CIFAR100) would demonstrate generalisation on their approach. Especially, because the best parameters chosen, are found via the results conducted on the validation set of Imagenet, the same set as model is evaluated, and there os no held-out set to account for overfitting.\n\n- Only  $L_{inf}$  I-FGSM attacks are used, and for example, the effectiveness by using l2, single-step FGSM, was not demonstrated (as done in [Wu et al., 2020]. Also the choices of perturbation sizes are not justified. \n\n- All results are only reported on undefended models, and it is unclear how these attacks would transfer if an adversarially-robust model was used as target (as done in [Wu et al., 2021.].\n\n- results reported only on a single run, and there is no performance report on the effect of random seed. As in [Wu et al., 2020], it is recommended to report mean and std over several runs of the algorithm to demonstrate stability of the attack's convergence.\n\n\n## limitation of ablation studies:\n\n- In ablations reported in Appx. I, results when ignoring a single factor removed; and the two other augmentations are in place, are reported. Based on these, it appears that removing cropping has the largest effect. I would like to know what is the performance of ILA only with each of the augmentations (cropping, reverse update, interpolation). This way we can pinpoint the contribution of each individual part. At the moment, we can only observe a result of combined steps.\n\n\n## computational cost:\n\nPlease discuss the computational complexity of the proposed method in comparison with SOTA and baselines used. E.g, how much additional computation will these 3 data augmentations require compared to ILA?\n\n\nadditional comments:\n====================\n\n- in  $L_{inf}$  attacks, it is common to state perturbation size by $\\epsilon= x/255$. I recommend providing the perturbation size in this format, or at least clarify this point.\n\n- on the reverse adversarial update, it seems that this step will move the sample further away from the decision boundary.\n\n- connection to related work: it would be insightful to discuss connections to related work (ILA, ILA++). Can Aug-ILA be considered a generalisation of any of the previous methods? or a special case? or there is no major connection?\n\n- some theoretical analysis on the convergence/generalisation on Aug-ILA, could shed light on the functionality of such approaches, and is encouraged if it could provide insights. For example, ILA, ILA++, and Aug-ILA could be studied under a simplified setting.\n\n\nrefs:\n=====\n\n[Wu et al., 2021]: Improving the Transferability of Adversarial Samples with Adversarial Transformations\n\n[Wang et al., 2021]: Enhancing the Transferability of Adversarial Attacks through Variance Tuning\n\n[Wu et al., 2020]: Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets\n\n\n \n",
            "summary_of_the_review": "The proposed method is simple, and provides significant improvements. However, I have the following concerns:\n\n- More clarity on some of the baselines and evaluation used is required (see above).\n\n- There is possibility of overfitting on hyperparameters due to using a single dataset for evaluations. Providing additional results will help resolve this issue.\n\n- The generality of the proposed method is not sufficiently demonstrated (only one attack, only one dataset).\n\nPlease see full comments above, and address if possible.\n\n\nAfter rebuttal:\n=====\n\nAfter the rebuttal, results on 2 new datasets, as well as results on defended models, and single-step attacks are provided, comparing AugILA with other ILA variants, which partially addresses my concerns regarding these points. Additionally, some concerns regarding the use of baselines have been resolved by providing responses to my comments.\n\nThe additional results shows that the proposed method consistently and in the majority of the cases, improves over ILA and ILA++, and I am convinced that the proposed approach improves over ILA/ILA++.\nHowever, unfortunately no comparison is provided with other SOTA methods beyond ILA-based approaches, which limits the paper's contributions.\nMore specifically, my comments regarding evaluation on new datasets, single-step attacks, and defended models are not fully addressed due to the fact that the provided results are only comparisons to ILA-based approaches (ILA, ILA++), and baselines such as VMI-CT-FGSM and I-FGSM + LinBP + SGM + ILA, which were among the baseline presented in Table 1 have been missed. \nEspecially, because as reported in Table 1, VMI-CT-FGSM and I-FGSM + LinBP + SGM + ILA have better success rates than of the ILA baseline.\nThis leaves uncertainty on how the proposed method would perform on other datasets, single-step attacks, and defended models, compared to non-ILA approaches. In my opinion, this is the main drawback of the new results.\n\nIn summary, although I believe the paper has been improved after the rebuttal, due to the limitations mentioned above, I keep my score.\nNevertheless, I believe by strengthening the baseline comparisons, this work has the potential to become a valuable contribution for the community.\n\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a transfer-based attack method based on Intermediate Level Attack(ILA). Image augementation and reverse adversarial updateare applied to ILA input, to make diverse adversarial references. Moreover, the interpolation of cumlative attacks can maintain a better transfer direction.The paper is clear and well-written and experimental results and analysis support the claims.",
            "main_review": "- The overall structure of this paper is clear, and  extensive experiment results are provided to support their ideas.\n- Based on the observation from the  previous work, diversity transformation and perturbation interpolation are used to improve the transferability. The experimental results show that the augementation is simple and effective. However, I still have some doubts: how does the adversarial perturbation generated against the transformed examples correspond to the original ones? Especially random cropping with large size causes great damage to the image, making the generated perturbation is extremely incosistent with the original image in pixel. But the adversarial example shown in Figure 1 is natural.\n- The author expect the reverse adversarial update input can highly activate the intermediate layers and provide a better attack direction guidence. However, the reduction of classification loss in the Appendix A seems obvious. \nIs it possible to show the change of feature attention map after the first update?\n- Although the three basic methods proposed are effective and intuitive, there is a problem to be solved: how to choose the index of the attack layer, which seems to have no prior guidance other than referring to existing papers.\n- Beyond experiment results, the author provide explanations on the effect of augmentation in terms of the weakening effect to the perturbation strength.",
            "summary_of_the_review": "In summary, I find the experiments satisfying, although the additional experiment show can make the claims more convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed / used three techniques to enhance the transferability of ILA on undefended models. The techniques include image augmentation, reverse adversarial update on the clean example and reference attack update through interpolation with an automatic parameter selection scheme. The authors evaluated their method on nine undefended models and showed the proposed method outperforms ILA and ILA++ and other state-of-the-art methods ",
            "main_review": "Strengths\n1.\tThe paper is well written, and the content is clear, except some minor problems.  \n2.\tThe three techniques effectively enhance ILA.\n3.\tI appreciate the use of reverse adversarial update on the clean example and the automatic parameter selection scheme.\n\n\nWeaknesses (major)\n1.\tRecently, most of transferability methods published in top conferences are evaluated on both undefended and defended models. The authors only evaluated their method on undefended models. If we consider that the evaluation of defended models is a must, the paper is not ready for publish. \n2.\tSome strong transferability methods, such as LinBP, have evaluated on target attacks. However, no such experiment was provided. \n3.\tThe method is very sensitive to the selection of layers in the source model. For example, the Inception V3 in Fig. 2(c) shows that the difference of the attack success rates between the 6a and 6e layer is more than 20% and the VGG19 in Fig. 2(b) shows that the difference of the attack success rates between 9 and 12 layer is around 20%. Once non-optimal layers are selected, the attack performance would drop very significantly. Does the proposed method still outperform other state-of-the-art methods in that situation? In the real black attacks, attackers cannot select the layers, as the authors did, because they don’t have attack accuracy of target models. \n4.\tThe authors mentioned “VMI-SI-TI-DI-FDSM does not work well in terms of black-box attack, despite its remarkable performance in attacking defended models”. The statement has two problems. 1) VMI-SI-TI-DI-FDSM was examined on a black-box setting when their method was evaluated on defended model. 2) Why don’t compare the proposed method with it on the defended models? \n\n\nWeaknesses (others)\n1.\tThe authors method that “The combination of LinBP and SGM also incurs overhead as it requires 300 iterations of I-FGSM on ImageNet, which is 30 times more than the reference attack in Aug-ILA. Thus, statement is inaccurate. In setup, the authors mention that they use 10 iterations for I-FGSM and 50 iterations for Aug -ILA. In the 50 iterations, gradients are needed. Thus, this statement is inaccurate, although speed is not very important for adversarial attack. \n2.\tIn Eq. 1, better to write clearly that dot represents dot product and the outputs of Eq. 2 and 3 are vectors.\n3.\tAppendix F, in the training, do the victim models use copping with 0.9-0.97 as their augmentation? Thus, the curves have similar behaviours. \n4.\tNo experiment shows that the proposed method can work with other methods to enhance overall transferability.\n\n",
            "summary_of_the_review": "The paper is well-written and the proposed method does improve ILA. However, due to the insufficient experiments, especially evaluation on the defended models and target attacks and the sensitivity of the selected layers, this paper is not ready for publish. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}