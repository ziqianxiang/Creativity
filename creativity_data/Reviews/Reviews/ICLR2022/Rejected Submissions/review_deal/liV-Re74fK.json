{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors introduce a modification to CQL to use a weighting based on density estimates. In an idealized setting, they show that the estimate Q-values bound the true Q-values. Finally, they evaluate their proposed approach on a few benchmark offline RL tasks.\n\nGenerally, all reviewers felt that the results were too incremental. The theoretical result follows with light modifications from the CQL paper and even then, the implications of the result are unclear. The experimental results showed small improvements or comparable performance while requiring training a density estimator and introducing an additional hyperparameter. Furthermore, the set of tasks evaluated was limited and no comparisons to other methods than CQL were shown. \n\nWhile I appreciate the effort the authors took to investigate this improvement, at this time, the paper falls below the bar and I recommend rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a conservative Q learning approach based on density and uncertainty estimation, D-CQL. It argues it is a more effective regularization than previous conservative Q learning approaches. The performance of the proposed method is compared with other batch RL methods in several classical benchmarks.",
            "main_review": "Strength:\n1. The proposed method overcomes some not-so-justified designs in the original CQL. D-CQL tries to regularize the Q value in out-of-distribution actions instead of simply penalizing an arbitrary distribution that is different from behavior policy. \n2. The density estimation method might be useful for many batch RL algorithms using similar concepts/modules.\n\nWeakness\n1. The theory in this paper is meaningless. Theorem 1 shows that the fixed point solution lower bounds expected value Q^\\pi assuming no sampling error (and is tight under some conditions). But assuming no sampling error the vanilla Bellman equation's solution is tight everywhere. A big part of the motivation in conservative updates is due to high sampling error in some OOD regions. I'd rather see it removed instead of a meaningless theorem.\n2. The core algorithmic design is vague. The paper use uncertainty in the motivation and description of the algorithm choice. The specific form of uncertainty \\zeta as a function of the density is not explained/justified at all. It's not clear to me why that function can be viewed as \"uncertainty\".\n3. The empirical validation is not clear either. The performance is so close given the large variance of RL methods. It is not explained how the promised benefit in previous sections has helped or not. One of the four experiments shows a clearly decreasing curve and does not converge. The performance is only compared with CQL and there are many recently proposed batch RL algorithms.",
            "summary_of_the_review": "There is some interesting idea of regularizing OOD action's value, but the significance of both theoretical and empirical contributions is vague.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims to improve CQL by providing a better compromise between learning a good policy and staying close to the behavior policy. The authors introduce a weighting scheme into the CQL algorithm, which assigns small weights to those state-action pairs close to the dataset and large weight to the OOD actions. The effect is the gap between the underestimated values of those in-distribution state-action pairs and their true values are closer (i.e., the lower bound is tighter). The calculation of the weighting scheme requires a density estimation, and the authors propose to use normalizing flow to learn the density function. Then, the authors connect their weighted updating rule and the one by penalizing the KL divergence between learned policy and behavior policy. Experiments are presented to show the effect of their version of the CQL method.",
            "main_review": "Strength: \n\n1. CQL concerns an important problem in offline RL — distribution shift, and the paper focuses on improving CQL, so it is an important topic. \n\n2. The paper has a reasonably clear logic flow. \n\nWeaknesses:\n\nI feel the motivation of the work is confusing. I can understand the authors want to improve CQL somehow further. But it is never made clear: \n1) what existing problems are and why they matter; \nIs it the lower bound on the exiting CQL is too loose? Why is improving the bound important? \n\n2) what is the effect you want to achieve? \nIs it an offline algorithm that can learn a better policy from data generated by a poor behavior policy? \n\nThe contribution is incremental, and I doubt the significance. As the paper cited in section 2.2, Kumar et al. (2020) propose to penalize the actions not described by the dataset, which enables a general definition of \\mu. Note that the additional weighting scheme can be essentially thought of as a new type of \\mu. I don’t see a clear difference between (2) and (3). Both can be considered as the specially designed action sampling distribution. \n\nWhy is theorem 1 useful? If I understand correctly, the key thing you want to say is the additional weighting can provide a tighter bound for OOD state-action pairs or those not close to the dataset? But the first step should be figuring out the effects of having a tight/loose bound. Does it hurt optimality/convergence rate/generalization…? Even partially answering this question can better motivate the reweighting approach. I believe the proof of the theorem is a simple modification from the existing CQL work. \n\nThe choice of the weighting scheme lacks justification. An intuitive choice is the RBF function. Furthermore, according to theorem 1, the proposed weighting is useful only when the action is OOD, i.e., the weight is 1; when the action is ID, weight should be 0, but your weighting scheme does not give zero? \n\nSection 4.1, the proposed method comes out suddenly. Is there any reason to choose normalizing flows? Of course, normalizing flows is a good method enabling both efficient sampling and density evaluation. In your algorithm, you only need to evaluate the density but not to sample. There should be plenty of other choices. When testing ideas, it is more natural to start with some simple methods. \n\nWhat is the purpose of section 4.3? \n\nI expect to see how various concrete choices of the weighting scheme can affect the distance (e.g., KL divergence) between the learned policy and the behavior policy. \n\nThe experiments. 1. more random seeds should be tested (figure 1) — it is hard to distinguish algorithms from the current learning curves. Readers cannot see a clear message from them. 2. I expect more baselines to be compared and more domains to be tested. As I mentioned, the choices of the weighting and the way of learning density functions are not strongly motivated. In this case, I have to ask for stronger empirical results: baselines with other design choices and more domains. 3. The experiments in Fig 2 are incomplete. Why are there no experiments for half cheetah and walker with expert data? 4. Please provide reproducing details.  \n\nThe abstract says, “… with a strong theoretical guarantee.” I don’t think there is any strong theory in the paper.  \n\nPage 3. Last paragraph. The criticism towards using empirical dataset distribution for \\hat{pi}_\\beta does not make sense to me. When the state/action is continuous, the empirical estimation should be kernel density estimation, which is a consistent estimator for estimating empirical distributions. The kernel can be chosen as smooth, so the KDE should have support everywhere. \n\nMinor: \n\n1. there is a nontrivial number of grammar issues/typos. Please double-check. \n\n2. Many sentences are confusing or logically disconnected. \n\ne.g., in the abstract, “A compromise between enhancing …. To alleviate this issue, … ” what issue? \n\n“Improving the learning process.” In what sense? Higher sample efficiency? \n\n“Indeed, the lack of information … ” what information? Why does it provoke overestimation? \n\nI believe saying “based on the amount of information collected” is inaccurate because the paper does not really introduce any information measurement. \n",
            "summary_of_the_review": "Please see the main review. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focuses on the trade-off between extrapolation error and over-penalization for batch reinforcement learning. Unlike the CQL to penalize uniformly any actions  that do not belong to the dataset, this paper proposes to relax the strong penalization imposed on the out-of-distribution actions. Specifically, the proposed D-CQL considers the uncertainty on OOD actions, and only supresses the Q-values of actions which belong to OOD data with large probability. Thus the D-CQL is considered to have a better exploration when the dataset includes some suboptimal behaviors.",
            "main_review": "strengths:\n- the motivation and idea are intuitive and natural;\n- the proposed solution (weighting scheme according to the density estimation) also make sense;\n \nweakness:\n- the proposed method critically depends on the accuracy of the density estimator of the in-distribution trajectory datasets, however, in this aspects, the paper does not make any significant contribution and actually this is the most difficult part of the whole pipeline as density estimation in high dimensional state-action space is very challenging. I think that an ablation study should be presented to verify that the underlying estimator really works in complicated environments.\n\n- the empirical results in Fig.2 do not show a significant performance improvement than CQL, in fact, only in halfcheetah-medium-v0, the proposed D-CQL can achieve a better performance than CQL in the final;\n- About the experiments, this paper only compares with the original algorithm, CQL, however, it's necessary to include more baselines(e.g., BAIL, BCQ, etc) to verify the effectiveness of D-CQL;\n\nSome questions:\n1. About the ablation study in Figure.1, the right figure shows the D-CQL with $\\nu=1$ has a similar performance with  CQL. According to the definition in Eq.5 and Eq.7, $\\nu=1$ means the final objective will penalize all the actions from  OOD or ID, it can't constrict the leanred policy to behave similarily with $\\pi_\\beta$, so why does it achieve such a good performance?\n2. I'm confused with the explanation about Eq.9, why a high $\\pi_\\beta(a|s)$ implies a low $\\zeta(a|s)$ that in turns further augments the Q-values in this area?\n3. The D-CQL can be seen a soft penalization instead of the hard one in CQL that constrains its policy to be extremely close to the behavioural policy. So it should have a slight difference between the final Q-value, however the Fig.2 shows D-CQL often has a larger difference between ID and OOD actions. Theses results confused me.\n",
            "summary_of_the_review": "The idea of this paper is reasonable but to implement this idea, the problem of density estimation in high dimensional state-action space should be addressed first, which is unfortunately not convincing in this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an improved version of CQL (conservative q-learning), by using $\\zeta$ from estimated data density, which was $\\zeta(a|s)=1-\\frac{\\hat{\\pi}_\\beta(a|s)}{\\mu(a|s)}$ in original CQL paper. The paper shows that proposed D-CQL, which uses $ \\zeta = \\max ( \\nu , 1- \\frac { \\eta _ \\beta (s,a) } { \\eta _ \\beta ( s , a _ { \\mathcal { D } } ) } )$ with normalizing flow density estimator $\\eta_\\beta$, shows better performance than CQL in number of domains.",
            "main_review": "\nStrengths:\n- The paper is well written and easy to follow.\n- It is a good point that original CQL actually does not satisfy $\\text{supp} \\mu \\subset \\text{supp} \\hat{\\pi}_\\beta$.\n\nWeaknesses:\n- This paper criticizes various points of original CQL paper, such as:\n   - Lower bound of CQL is loosened in the common setting where $\\mu$ is a uniform distribution\n   - CQL (Theorem 3.1 of CQL paper) requires $\\text{supp} \\mu \\subset \\text{supp} \\hat{\\pi}_\\beta$, which does not hold when $\\hat{\\pi}_\\beta$ is an empirical distribution. \n   - The regularization is equally penalizing the actions that are not appearing in the dataset, no matter their closeness with the dataset\n   - I understand that by learning the density estimator can address the last point, but I am not sure whether the paper addresses the first two points.\n- Newly introduced OOD weighting scheme: $ \\zeta = \\max ( \\nu , 1- \\frac { \\eta _ \\beta (s,a) } { \\eta _ \\beta ( s , a _ { \\mathcal { D } } ) } )$ is not well rationalized in the paper. The constraints we had is that it should be state independent and to be bounded between 0 and 1. Why can't we simply estimate $\\hat{\\pi}_\\beta$ and use $\\max (\\nu, 1-\\hat{\\pi}_\\beta$)? Is there any reason on learning joint density rather than a conditional density? \n- I think the paper lacks concrete backgrounds for the suggestion of $\\zeta$, and at least needs a thorough empirical analysis of various versions of $\\zeta$, e.g. $\\zeta$ that is bounded between 0,1 vs that is not bounded.\n- Most of the contributions of the paper seems to be modified kinds of stuff from CQL paper, and overall the paper looks like a souped-up version of CQL.\n",
            "summary_of_the_review": "Overall, I believe that the contributions of the paper are only marginally significant. The arguments criticizing CQL are made but do not seem to be properly addressed by the proposed method. The results have shown that the performance has increased, but the introduced hyperparameter $\\nu$ does not have a proper way of optimizing it, which is different from $\\alpha$ of CQL that can be tuned to prevent the divergence of Q values. We usually expect to have a performance improvement from having another freedom on hyperparameter, and I cannot credit the performance improvement of this paper much.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}