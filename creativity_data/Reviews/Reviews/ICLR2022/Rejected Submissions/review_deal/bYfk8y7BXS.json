{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a new approach called pessimistic model selection (PMS)  for model selection in offline RL and tests it in 6 different environments. Under certain assumptions this allows theoretical results that the best model is recovered with high probability.\n\nSeveral points were raised by the reviewers and maintained after the rebuttal:\n- Theoretical results were considered weak as they only hold asymptotically.\n- Experimental results limited (potentially different regret scales, no sufficient comparison to other baselines).\n- Exposition of the paper that needs to be improved.\n\nGiven the strong consensus among the reviewer I recommend rejecting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the model selection problem in offline RL and proposes a new procedure called pessimistic model selection (PMS) that estimates performance lower bound for each candidate policy. Theoretical results show that under certain assumptions PMS can identify the best candidate policy with high probability. The proposed approach is compared to three common OPE methods used in Tang & Wiens 2021 and shows favorable performance on 6 RL domains. ",
            "main_review": "Introduction mentions \"AutoML\" but I don't believe the main paper focuses on AutoML or even mentions it again. There seems to be a disconnect between this motivation and the content. \n\nSaying their proposed PMS approach is “tuning free” is kind of overselling: you still have to pick the  $O$ and $\\alpha$ hyperparameters, and additionally tuning the models architecture/training details for the value estimation procedure (probably FQI) that is used in each iteration to get $\\hat{Q}$. Consider adjusting this claim; also compare to Zhang & Jiang 2021 [1] (this paper is very new so the authors are not expected to know/cite it, it is however important to illustrate what tuning free might mean). \n\nMain results seem to have been aggregated over all 6 domains considered, however it is unclear whether the regret across domains are comparable i.e. on the same scale. It would be clearer to also provide per-domain results (the experiments section says these should be in Appendix B be but they are not). \n\nWhat are $O$ and $\\alpha$ set to for the experiments? \n\nSince the method splits the data into $O$ parts, this will lead to higher variance estimates for each iteration compared to using the full dataset. Is this accounted for in the theoretical analysis? \n\n[1] Siyuan Zhang, Nan Jiang. Towards Hyperparameter-free Policy Selection for Offline Reinforcement Learning. NeurIPS 2021. https://arxiv.org/abs/2110.14000",
            "summary_of_the_review": "This paper makes an attempt at an important problem - offline RL model selection, however some experimental details are missing and the results seem to be too brief and not as informative as they could be. It also has significantly smaller page margins than the ICLR required format. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This submission presented a pessimistic model selection approach (PMS) for offline deep reinforcement learning (ORL) and tested the approach with performance comparison on six simulated environments. Under several specified conditions, the authors provided asymptotic analysis to show that PMS can lead to the best model with respect the worst performance of the derived policy. ",
            "main_review": "This submission presented a pessimistic model selection approach (PMS) for offline deep reinforcement learning (ORL) and tested the approach with performance comparison on six simulated environments. Under several specified conditions, the authors provided asymptotic analysis to show that PMS can lead to the best model with respect the worst performance of the derived policy. \n\n**Strengths:** \n\n1. The authors proposed the PMS to select ORL model based on the constructed U(l) confidence interval estimate (page 6) by splitting offline training data and sequential estimation, which has the potential to address overfitting issues. Both theoretical analysis and empirical performance have been conducted. \n\n2. Two refined approaches were presented considering potential bias, even though empirically not showing advantages in simulated environments. \n\n**Weaknesses:** \n\n1. The authors may want to better explain some steps of the proofs in the appendix, for example, the last inequality for the proof of corollary 1 on page 17. \n\n2. Regarding empirical evaluation of PMS, it appears that the theoretical analyses were mostly based on several references, including Su et al (2020). In addition to comparing to OPE estimates in Tang & Wiens (2021), can the authors compare with other competing methods? Also, why the authors did not show the corresponding regrets for other competing methods in Figure 3 to better show the advantages of PMS? In Figure 3, what is $\\pi^*$? If it is the optimal policy, shouldn't the corresponding regret be zero? Also by simply looking at the tables in the appendix, it is not really clear how the initial models were selected in different experiments. More comprehensive performance evaluation should be provided. \n\n3. The submission may have to be checked carefully. For example, it is clear that the indices do not match in the last equation on page 7 for the constructed interval: should the index to be i instead of l in the righthand side?  In the beginning paragraph of Appendix A, the inequality directions are not consistent. The starting indices for t (from page 3) and o (from page 5) have to be checked to make them correct. \n\n4. The presentation needs to be improved. There are many language problems. For example, several acronyms were never defined, including OPE, which I assume that it stands for Offline Policy Evaluation. There are many grammar errors, for example, \"such A guarantee\" on page 2; \"such A procedure\" on page 8; \"...in continuous caseS\" on page 3. There are also incomplete sentences, for example, \"...where the refinement algorithms (PMS R1/R2) have only a relative +/- 0.423 % performance ...\" on page 9. Some referred indices were not correct. For example, in the paragraph before Assumption 3 on page 6, \"Assumption 1-->2 also requires that the convergence rates.... \" The language problems are almost everywhere in the appendix. For example, \"These environments covers wide applications includes tabular learning ....\", \"... for each environments.\" \"... one experimental setup needs careful control... \" The captions of Tables 2 and 3 (\"... prioritized replAy...\"). There are just way too many such problems all over the place. Finally, the authors stated on page \"Detailed results for each environment is given in Appendix B.\" But the appendix does not contain any results. \n\n",
            "summary_of_the_review": "The authors proposed the PMS to select ORL model based on the estimated worst performance to address potential overfitting issues in ORL. Both theoretical analysis and empirical performance have been conducted. The overall impression of the submission is that the authors should carefully check the writing and present more complete and comprehensive performance evaluation. Better presentation of the research results will be also appreciated. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new algorithm for model-section in offline RL based on adding the pessimism principle on top of existing OPE algorithms. Under strong assumptions, the paper provides an asymptotic convergence result for the proposed algorithm. It also provides empirical experiments demonstrating that the proposed algorithm outperforms existing baselines.",
            "main_review": "High-level comments:\n1. The theoretical part is weak, with many strong assumptions being made, but only providing asymptotic convergence results.\n2. The exposition can be significantly improved. The proposed method is based on many existing techniques like doubly-robust estimators, for which little explanation is provided. No self-contained pseudo-code is provided. Many notations are used without definition. \n\nDetailed comments:\n\n1. Please avoid using the acronym ORL for offline reinforcement learning, as online reinforcement learning can also be abbreviated with the same acronym.\n\n2. In related works, when discussing other works on model-selection in offline RL, please elaborate on their techniques and contrast with the current paper.\n\n3. $d^{\\pi,\\nu}$ is not defined.\n\n4. Could the authors elaborate on how equation (4) comes about? The RHS doesn't depend on the MDP transition at all, whereas the LHS clearly does, so how could they be equal?\n\n5. Section 4.1, I think you mean \"an estimate of $Q^{\\hat\\pi_\\ell}$ as $\\hat Q_\\ell$\", since there is no assumption that any of the $\\pi_\\ell$ is close to the optimal policy.\n\n6. Sec 4.2 in the description of the proposed algorithm, above equation (6), what do you mean by \"applying FQI on $D_0$ to compute $\\hat Q_\\ell$\"? To my knowledge, FQI only take as input a value function class $\\mathcal{F}$ but doesn't take any policy as additional input. If you mean to evaluate a particular policy $\\pi_\\ell$, the procedure is called Fitted-Q Evaluation (FQE). Also, it is confusing what \"candidate model\" means. Does it mean candidate policies returned by L different algorithms? The phrase \"model\" is very often confusing in RL, as many constructs are referred to as models, e.g. MDP transition in model-based RL or model as in a trained neural network. Avoid it if you can and be specific.\n\n\n7. $\\hat Q^{\\star (o)}_\\ell$ in (8) is not defined.\n\n8. What is the justification for constructing the confidence interval with finite samples based on an asymptotic convergence result? In finite sample regime, (11) can be arbitrarily far off.\n\n9. In what way can we see that large $O$ increases accuracy? Solely based on equation (11), the effect of large O on the convergence rate is negligible.\n\n10. $\\alpha$ is defined multiple times, in section 4 as the confidence parameter and in section 5 assumption 2 as the convergence rate.\n\n11. For section 6, could the authors elaborate on how their refined approach automatically adapts to unknown biases $B(\\ell)$? Also if there is bias in assumption 2 for policy improvement, then there definitely should be biases in assumption 3 for policy evaluation. Then, the additional assumption in Corollary 2 should not hold.\n\n12. No discussion about the computational complexity of the proposed algorithm. In particular, the min-max problem in doubly-robust estimation seems to scale poorly.",
            "summary_of_the_review": "The theoretical result is weak, and the exposition can be significantly improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the model selection problem in offline reinforcement learning (ORL). The model selection of ORL is challenging due to insufficient observational data from offline collection. The paper proposes a novel model selection approach to automate ORL development process and to identify a well-performed model given offline data. The proposed pessimistic model selection (PMS) method leverages an uncertainty quantification on value functions and a pessimistic idea. Some experiments show the superior performance of the proposed model selection method.",
            "main_review": "**Strengths**\n- the paper tackles the important problem of model selection in ORL\n- the proposed method is based on religious guarantees, there are also two refined approaches for addressing the issue in an assumption\n- the simulation study shows the benefit of using the proposed approach\n\n**Weaknesses**\n- The claim “our approach essentially does not involve additional hyperparameter tuning” seems somewhat overselling. We should specify “O” and “\\alpha” to use PMS, and how they affect the model selection performance is unclear in the experiment\n\n- Some related papers are missing such as [a] and [b]\n\n[a] Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, and Dale Schuurmans. Offline Policy Selection under Uncertainty. https://arxiv.org/abs/2012.06919 \n\n[b] Ilja Kuzborskij, Claire Vernade, András György, and Csaba Szepesvári. Confident Off-Policy Evaluation and Selection through Self-Normalized Importance Weighting. https://arxiv.org/abs/2006.10460 \n\n- there is room for improvement in the experimental section\n    - regret is defined in an additive manner, and Figure 3 contains results across different environments. Is this valid? Using relative regret ($\\frac{\\mathcal{V}\\left(\\pi_{l^{*}}\\right)-\\mathcal{V}\\left(\\pi_{l}\\right)}{\\mathcal{V}\\left(\\pi_{l^{*}}\\right)}$) seems more reasonable.\n    - What each point in Figure 4 means seems unclear\n    - I would love to see how the choice of “O” and “\\alpha” affect the model selection of PMS \n    - Figures 1 and 5 contain the result from only one of six environments. It is possible to use Appendix to share the results from the other environments that are not presented in the paper currently\n    - Additional evaluation metrics such as top-k presion used in [b] would make the experiments more practically relevant\n\nI would consider raising my score if I see substantial improvement in the experimental section. It is also essential to clarify the contributions of the paper compared to [a] and [b].\n",
            "summary_of_the_review": "The paper studies an important problem and provide nice solutions. However, some important related papers are missing and there are number of concerns in the experiment. Thus, I would recommend weak reject at this moment. I would like to see substantial effort, in particular the improvement in the experiments to change my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}