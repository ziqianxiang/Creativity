{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The work proposed multi-view learning framework that combines diversity and consistency objectives for semi-supervised learning. While reviewers appreciated that simplicity of the proposed method, they raised concerns on the limited contribution on top of the original Bayesian Co-Training work. Although authors provided detailed rebuttals that addressed some of the reviewers' concerns, and one reviewer did raise their score, the other reviewers' scores remained unchanged. Given the work is closely based off the BCT work, I would like to see more detailed analyses on the importance of the changes brought in this work, such as changing the base learners and introduction of diversity objectives as pointed out by the authors."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "A view-consistent loss is designed, which is based on PGM. The PGM is quite simple to understand.",
            "main_review": "1. I want to know how many branches is good and why don't you use multiple heads? e.g., GAT uses multi-head.\n2. In multiview learning, I want to know how to model diversity? The consistency is easy to model, but the diversity is not well addressed. Even CCA can be regarded as a view-consistency modeling.\n3. I want to know why you design the model? for what applications?",
            "summary_of_the_review": "The view-consistency modeling does not give a supervise or somehow it is not new enough. \nThe diversity loss is not well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "the submission proposed a semi-supervised learning framework that leverages the benefits of multi-view learning with neural networks. On labelled data, besides minimising the empirical risk, the objective function encourages diversity by driving the correlation between individual functions with the mean function; on the unlabelled data, the objective forces multiple views to produce consistent features or predictions. ",
            "main_review": "1. The ablation study missed an interesting question:  would increasing the number of views with either consistency or diversity loss surpass the combined loss with fewer views?\n\nOn one hand, increasing the number of views certainly increases the chance of producing more diverse features or predictions, but on the other hand, having more views on the same data also leads to a higher probability of consistent predictions from multiple views. It would be good to see how the number of views plays in the game.\n\n2. The fundamental grounding of multi-view learning is that, the utility of all views is maximised if, given the label, all views are independent of each other. \n\nThe paper made an argument in the intro that negative correlations between or among individual views would be helpful, which doesn't make sense. Referred literature in this paper talked about decreasing correlations by including a penalty term, but they didn't mean that we would like to see negative correlations.\n\n3. In ensemble learning, at least in bagging, the improvement over individual learners is often more significant when these learners are more diverse (have larger variance), but again, the baseline model here is a trained base learner. It is highly likely that an ensemble of model A would still be worse than a single model B. There is always a tradeoff between the diversity (variance) of the base learner and its accuracy. I hope the authors could be more precise on their wording in the intro. \n\n4. I am not super positive about the derivations in Section 3. Let me elaborate a bit.\n\nProviding that $$f_m \\sim \\mathcal{N}(f_c, \\sigma_m^2), \\forall m=1,...,M$$ with the distribution of f_c unknown.\n\nThe joint distribution should be $$p(f_c, f_1, ..., f_M)=p(f_1, ..., f_M|f_c)p(f_c)=p(f_c)\\Pi_{m=1}^Mp(f_m|f_c)=p(f_c)\\Pi_{m=1}^M\\mathcal{N}(f_c, \\sigma_n^2)$$, which is different from Eq. 2.\n\nThe marginal distribution should be $$p( f_1, ..., f_M)=\\int p(f_1, ..., f_M|f_c)p(f_c) df_c=\\int p(f_c)\\Pi_{m=1}^M\\mathcal{N}(f_c, \\sigma_m^2)df_c$$, which doesn't give us Eq. 3 since we don't know  the distribution of f_c.\n\nLet's move on to the posterior distribution of the consensus function: \n\n$$p(f_c | f_1, …, f_M) = p(f_c, f_1, …, f_M) / p(f_1, …, f_M)= p(f_c) p(f_1, …, f_M | f_c) / \\int p(f_c) p(f_1, …, f_M | f_c) df_c$$\n\nwhich may not be a gaussian distribution. \n\nA quick fix is to also assume that p(f_c) is a gaussian distribution, although this assumption seems to be very strong. ",
            "summary_of_the_review": "Overall, this submission's empirical contribution is more appreciated than its unclear theoretical justification. However, the arguments made in the intro were too vague and also misleading. Therefore, I do not vote for accepting this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper concerns the semi-supervised problem. Different from SOTA deep semi-supervised methods, the proposed DiCom employs a diversity measure on the labeled multi-view data, and combines diversity with consistency based on underlying probabilistic graphical assumptions. Experiments verify the effectiveness.",
            "main_review": "1.\tThe consideration of diversity for semi-supervised learning is interesting, and the reviewer considers that it is helpful for improving performance.\n2.\tThe proposed method combines diversity with consistency based on underlying probabilistic graphical assumptions is a meaningful idea.\n\nweakness\n1.\tThe idea of this paper is direct. This paper still considers traditional semi-supervised learning, and the multi-view refers to the view generated from instance augmentation. Why only measure the diversity of labeled instances and not extend to unlabeled instances? Are there relevant experimental verifications?\n2.\tWhat is the advantage of combining diversity with consistency based on underlying probabilistic graphical assumptions? Several multi-view methods have already considered the diversity with consistency simultaneously, but the authors choose to ignore them, e.g., “Exclusivity-Consistency Regularized Multi-view Subspace Clustering”, “End-to-End Adversarial-Attention Network for Multi-Modal Clustering”. More analyses and comparisons are expected, the reviewer considers that these ideas can also be transformed into the semi-supervised framework.\n3.\tAlthough this paper considers semi-supervised regression, why can't the semi-supervised classification method extend to the regression problem by replacing the loss function? The SOTA comparison method is in 2018, more comparison methods recently are expected.\n\n",
            "summary_of_the_review": "This paper concerns the semi-supervised problem. Different from SOTA deep semi-supervised methods, the proposed DiCom employs a diversity measure on the labeled multi-view data, and combines diversity with consistency based on underlying probabilistic graphical assumptions. Experiments verify the effectiveness. However, there exist several shortcomings for this paper, so I recommend weak rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method for **semi-supervised multi-view regression** based on **data augmentation** and an **undirected graphical model**.\nThe author derived a \"**consistency**\" term for unlabeled data and a \"**diversity**\" term for labeled data based on their log-likelihood and combined them linearly with two hyperparameters.\nThe proposed method was evaluated on tabular and image data and analyzed via ablation study.",
            "main_review": "## Strengths\n\nThis paper studied the semi-supervised regression problem and tackled it with a technically sound approach.\nIt sheds light on the effects of diversity and consistency in multi-view learning.\nBesides, this paper is mostly clear written.\nIt is easy to follow the derivation and verify its correctness.\n\n---\n\n## Weaknesses\n\nMy biggest concern is its novelty.\nAs cited in the paper, most of the probabilistic graphical model definition, marginalization, and likelihood derivation is quite similar to those used in [Yu et al., 2011].\nAlthough [Yu et al., 2011] used the Gaussian process while this work mainly used neural network and gradient-based optimization, the techniques for deriving the learning objective are basically the same.\n\nConcretely, if I didn't misunderstand anything, the definition of the undirected graphical model is roughly as follows:\n\n- $p(y, f_c, f_1, \\dots, f_M, x) = p(y, f_c) \\prod_{m=1}^M p(f_c, f_m) p(f_m, x)$\n\n- $p(f_m, x) \\propto F_m(\\eta_m(x), \\theta_m)$ (multi-view neural networks)\n\n- $p(f_c, f_m) \\propto \\exp\\left(-\\frac{(f_c - f_m)^2}{2\\sigma^2_m}\\right)$ (consensus potential, \"within-view potential\" $p(f_m)$ in [Yu et al., 2011] does not appear here)\n\n- $p(y, f_c) \\propto \\exp\\left(-\\frac{(y - f_c)^2}{2\\sigma^2}\\right)$ (output potential, Gaussian noise model for regression)\n\nThen, both [Yu et al., 2011] and this work marginalize the potential function to obtain:\n\n- $p(f_1, \\dots, f_M) \\propto \\int \\prod_{m=1}^M p(f_c, f_m) \\,\\mathrm{d} f_c$ (similar to [Yu et al., 2011] Eq. (7) in Section 3.1 (Marginal 1) and Appendix A.1 (derivation) without the within-view potential)\n\nAnd the conditional probability is calculated via Bayes' rule:\n\n- $p(f_c | f_1, \\dots, f_M) = \\frac{p(f_c, f_1, \\dots, f_M)}{p(f_1, \\dots, f_M)} \\propto \\exp\\left(-\\sum_{m=1}^M \\frac{(f_c - f_m)^2}{2\\sigma^2_m}  + \\sum_{m<k} \\lambda_{m,k} (f_m - f_k)^2\\right)$\n\nIt seems that [Yu et al., 2011] can be used for semi-supervised multi-view regression as well as classification.\nI'd like to know the main challenge and unique contribution of this work compared with [Yu et al., 2011].\n\n---\n\n## Questions and comments\n\n### Regression\n\n- I might have missed something, but is there anything \"regression-specific\" in this work?\n  If we just change the form of likelihood, can we apply the same technique to classification as well?\n\n### Data augmentation\n\n- How do you perform data augmentation on tabular data?\n- How do you perform data augmentation after the backbone network in DiCoM-B?\n- The data augmentation seems non-differentiable, so the backbone is pre-trained and fixed?\n\n### Graphical model notation\n\n- Figure 2 does not seem to be standard probabilistic graphical model notation so it's a little confusing.\n  For example, edges should not be labeled and $\\sigma^2$ and $\\lambda$ should be fixed parameters.\n\n### Others\n\n- Eq. (1): Shouldn't it be $\\sigma^2_m$? The assumption of $\\sigma^2_m = \\sigma^2_v$ is not given here.\n- Eq. (2): $p(f_m, f_c)$ is not defined nor explained.\n\n---\n\n## References\n\n- Yu, Shipeng, et al. \"Bayesian co-training.\" The Journal of Machine Learning Research 12 (2011): 2649-2680.",
            "summary_of_the_review": "This paper proposed a technically sound method based on an undirected graphical model for semi-supervised regression.\nHowever, compared to an existing formulation [Yu et al., 2011], its novelty and contribution are a little questionable and need to be further highlighted.\nHence, I think it's on the borderline for now.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}