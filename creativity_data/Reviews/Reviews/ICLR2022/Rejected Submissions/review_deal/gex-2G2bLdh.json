{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "All reviewers agreed that analysis of PPO is interesting. \nDuring the discussion, however, there was an agreement that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. Meanwhile, for conventional policy gradient, recent works provided convergence rates.\nAs one reviewer pointed out - this work does not further our theoretical understanding on why PPO is better than vanilla policy gradient, as all the established results hold for policy gradient, even with less assumptions.\nI encourage the authors to strengthen their paper by relaxing Assumption 4 (perhaps based on the robust classification idea raised in the discussion), and by further providing rate results."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "\nThis paper reinterprets the theory of PPO-clip based on the hinge policy optimization. They prove the global convergence of PPO by introducing some assumptions. Besides, they generalize the algorithm to a new family of policy-based algorithms by regarding the policy as a generalized classifier.",
            "main_review": "\nStrengths:\nThe writing of the paper is clear. The presentations of the theorems are easy to understand. \nThe interesting part of the paper is to generalize PPO to other classifiers. I suggest the author focus more on it.\n\nWeaknesses:\nThe major theoretical contribution is the proof of the convergence of PPO. However, in my view, this paper contributes little to understanding PPO. I give my reasons as follows.\n- The author proves the result by introducing a very strong assumption (Assumption 3), which in fact includes two assumptions.\n\t- First, the initial state distribution is strictly positive; thus the policy is possible to sample the state-action pair over the entire space. However, this is almost impossible in practice. This implies that the result of this paper can contribute to a very limited range of tasks.\n\t- Second, the sampled training dataset are sufficiently large to cover all possible state-action pair. \n- The key of PPO is the clipping technique. This is the key to ensuring the performance is bound. In my view, the theoretical result in this paper (Theorem 1) still holds if one removes the clipping function. If I'm right, then this paper contributes little to understanding PPO.\n- The author claim that \"no additional constraints, such as the KL divergence constraint used in TRPO, are needed to ensure policy improvement\". By this claim, do you think that PPO can remove the clipping technique? How do you think the effect of policy constraint in TRPO (KL divergence constraint) and PPO (clipping technique)?\n\nOverall, in my view, the contribution of the paper is to prove the performance improvement of the objective function without clipping technique (by some very strong assumptions).\n\n",
            "summary_of_the_review": "The paper does a good job in sharing new insights of policy optimization, and in connecting the policy optimization with classification. However, the two main claimed contributions are somewhat weak.\nRethinking policy improvement (Proposition 1 and 2) is not the original idea of this paper. Reinterpreting PPO by hinge loss is also not the idea of this paper. At the same time, the improvement analysis is based on some very strong assumptions. Therefore, I vote for rejection for this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper describes a new family of methods, called Hinge Policy Optimisation (HPO), generalising over proximal policy optimisation algorithm with a clipped surrogate objective (PPO-clip).",
            "main_review": "Pros:\n\n-  the paper gives interesting theoretical insight linking PPO-clip with a large margin classification and theoretical analysis of a new group of reinforcement learning as classification methods. \n\nCons:\n\n(1) while the reviewer appreciates the paper is mostly theoretical, it might be a good idea to bring some experimental analysis into the main text to show the performance of the method on a range of benchmark problems, especially given that the experimental analysis is already presented in the appendix. While the proofs are important part of the work, are they more essential for the narrative than the experiments? There is no right or wrong answer on this question, this question is more about gaining insight into why it has been done this way. \n\n(2)restrictive assumptions of the proposed method: “Assumption 4. We assume that at each iteration t, for each state-action pair, the sign of the estimated advantage is the same as that of the true advantage. “;   \"Assumption 2. Tabular policies” . Although it makes sense to  use assumption 4 for the sake of theoretical analysis, it would be good to have some evidence whether it actually could hold in any experimental scenarios. The reviewer wonders whether it is possible to show some experimental analysis in the appendix showing whether the assumption 4 holds during the training (and how it is reflected in training dynamics)?\n\nOther comments:\n\n(1) Page 5: Entropic mirror descent: could the authors give some references?\n\n(2) In the experimental section of the appendix, it would be useful to see the grid world tasks compared with a well-known baseline the same way as the MinAtar problems are. It would be also useful to see more environments to show the advantages and limitations of the proposed idea on more scenarios. ",
            "summary_of_the_review": "Pros: novel idea and interesting analysis of reinforcement learning as classification\nCons: restrictive assumptions of the theoretical analysis; the experimental analysis could be given more space given that it actually exists in the appendix. \nStarting from 5, but happy to update the scores to acceptance if there's a convincing answer on the cons during the rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes hinge policy optimization, a new theoretical framework for interpreting policy gradient algorithms as classification problems to be solved with a hinge loss. In this perspective, the sign of the advantage function becomes the label, and the difference in action probabilities between policy after and before an update becomes the classifier's output. The paper shows the equivalence between such a formulation and the popular PPO-clip objective and provides global converge guarantees on the blueprint of (Agarwal, 2020). It also proposes a range of policy optimization algorithms, depending on the details of the classification algorithm that is used, and empirically evaluates some of them.",
            "main_review": "Please list both the strengths and weaknesses of the paper. When discussing weaknesses, please provide concrete, actionable feedback on the paper.\n\n**Originality:** To the best of my knowledge, this is the first formalization which allows a theoretical analysis of PPO-clip, albeit, as highlighted in the paper, not the first formalization of reinforcement learning as a classification problem.\n\n**Significance:** Since PPO-clip is one of the most commonly used policy gradient algorithms, a theoretical analysis of the approach is a relevant contribution to the reinforcement learning community.\n\n**Rigour:** The paper is built upon common tools and I could not find any factual errors in the proofs.\n\n**Strengths**\n\n- The paper provides a nice bridge between the existing theoretical tools for the analysis of policy gradient algorithms and the popular PPO-clip algorithm.\n- The presentation of the theoretical results is quite clear.\n\n**Major Concerns**\n\n- I do not fully understand the justification behind the use of the specific EMDA approach as a subroutine in the algorithm. Is this essential for proving convergence of the resulting algorithm? Or would an alternative, perhaps simpler, procedure yield convergence as well albeit with a different proof scheme?\n- The fact that the experimental results are relegated in Appendix is quite disappointing. Albeit they are in small experimental settings and the results are not stellar, they provide a nice additional perspective on the proposed framework. In my opinion, it could take the space now reserved to the proof of Theorem 1, which generally follows (Agarwal et al. 2020).\n- The simple tabular policy representation used in the theoretical analysis burns some of the bridges between theory and practice that the paper is trying to build, since PPO-clip is traditionally associated to neural function approximators. What worries me the most is how the optimality in Definition 1-2 transfers to the setting with function approximation, in which this kind of sorting among policies is often unfeasible.\n\n**Minor Concerns**\n\n- Before Equation (8), it is written that the hinge loss and the PPO-clip gradients differ by a constant, which is not correct, given that the advantage depends on states and actions.\n- Why is Equation (8)-(9) repeated?",
            "summary_of_the_review": "The paper mostly achieves its goal, which is to provide a theoretical justification for PPO-clip. I currently lean towards acceptance, although the paper suffers from some presentation issues and imperfect justification of some algorithmic/theoretical design choices.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper suggests a generalization to the commonly used PPO(-clip) algorithm, by analyzing the loss function from a state-wise perspective and generalizing the policy-ratio clip objective into a hinge loss formulation. The main contributions of this paper are:\n(1) They introduce a family of algorithm that includes PPO-clip\n(2) Show convergence results for these algorithms (under less strict assumptions than previous works)\n(3) They suggest an instance from the suggested family of algorithms  - HPO-AM\n(4) They provide some empirical evidence (specifically on some of the mini atar games) showing this algorithm is competitive with other commonly used algorithms.",
            "main_review": "I think this is a very interesting paper that improves a commonly used algorithm and provides both theoretical and empirical evidence that support using this modified algorithm.\nThe paper is very well-written both in terms of how the main parts of the paper are easy to follow, and the appendix section is very detailed.\n\nBesides minor corrections and clarifications below, the only \"weakness\" is that PPO is a heavily used algorithm and the empirical results here are not convincing enough (only a part of the mini atar benchmark and a really small toy problem). However, in my opinion, the theoretical arguments provide enough value for the RL community so even though a more through survey would be valuable for practitioners, the current contents of this work are already enough for a good paper.\n\nMinor issues:\n1. eq 8 = eq 9?\n2. In \"Connecting PPO-clip and Hinge Loss\" section: In practical -> In practice (?)\n3. Regarding:\n\"and |Aπ (s, a)| serves as the weight of the sample.\" \nI think you meant that A could be interpreted as the weight, but on initial read I thought you added an additional multiplicative term of A. Maybe rephrase to clarify it is not an additional modification, but an interpretation of an existing element.\n4. Eq 13: what happens if all the advantages for a given state are 0? (I suspect that the denominator should be greater or equal to 0, since at least one action is non-negative)\n5. Could you clarify the following statement in assumption 3:\n\" Strict positivity of the initial state distribution µ0 and the initial policy π (0) is a necessary condition for this assumption\"\n6. In remark 3: \n\"we leverage Assumption 3 in order to rigorously\" - assumption 4? assumptions 3 and 4? assumption 3 alone seems out of context. And you need both the data assumption and the advantage sign assumption to prove your claims, right?\n7. Eq 15: if I understand correctly, this expression is for the true advantage in the limit, which should be the empty set if the algorithm converges. You don't mention this expression in the remarks directly below. I found it confusing that you don't mention it at all, you probably meant to present it for completeness. It might be helpful to hint on its role in the text as well.",
            "summary_of_the_review": "I found this paper to be valuable to the deep-RL community as it presents an interesting generalization to a commonly used algorithm with theoretical justifications. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}