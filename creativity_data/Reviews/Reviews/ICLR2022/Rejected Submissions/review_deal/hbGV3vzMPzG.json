{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a metric to measure the difficulty of training examples. The main thesis is that hard training examples lead to bad test adversarial error. There are theoretical results on simple models establishing such claims. The paper also proposes a method to adaptively weight training examples to improve training which gives improvement for adversarial error. \n\nThe reviewers have raised a number of questions and the rebuttal period has been useful. In particularly, I agree with the reviewers that 'model-agnostic' is misleading in this context and the authors have agreed to remove this in the future. It is felt that more experiments, comparison to adversarial training, etc. is needed and I think the paper will need to go through a proper review process again before acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a metric to measure the difficulty of training examples and find that hard training examples influence the generalization of adversarial training and cause overfitting in adversarial training. The authors also provide a theoretical analysis.\nTo mitigate the issue caused by hard training examples, the authors propose to assign different weights for training examples based on loss values. The evaluation results show that the proposed training methods can improve robustness in fast adversarial training and adversarial fine-tuning with additional data.\n",
            "main_review": "Strength:\n+ The topic studied in this paper is interesting and important. It can help practitioners better understand how training data influences the overfitting of adversarial training.\n+ This paper provides a theoretical explanation for why hard examples hurt the generalization gap.\n+ The evaluation results empirically show that adaptive training examples can improve robustness.\n\nWeakness:\n- The paper is a little bit overclaimed in the abstract. By definition, the difficulties of training examples are defined with losses of corresponding models, which means that it is not model agnostic. Same training samples could have different difficulties for different models. The method to calculate the metric is model agnostic, but the metric is not model agnostic.\n\n- In Section 3, the authors define the difficulty metric for training samples. One of my recommendations is to explain more about how and why the metric is defined.\n\n- Figures 1(a) and 2(a) show average losses of models trained on data with different difficulties, but I find that models with harder training data don't reach a plateau. A potential reason could be that models need more training epochs to converge on hard training examples. With enough training epochs, it is possible for models training on hard examples to get similar average losses.\n\n- The proposed training method is only evaluated on fast adversarial training and adversarial fine-tuning with additional data, but not on standard adversarial training. From my understanding, we can also combine this method with standard adversarial training. If the evaluation results also show improvements on standard adversarial training, the paper will be more compelling.\n",
            "summary_of_the_review": "This paper studies an interesting topic. It will be great that the method is also evaluated with other SOTA standard adversarial training methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper began by analyzing the influence of hard adversarial examples. And, they found that hard adversarial examples may be the major reason leading to robust overfitting. Then, based on these observations and analysis, the authors further introduced a Fast Adversarial Training scheme, which can achieve competitive results compared to baselines.  ",
            "main_review": "Strengths:\n\nThe authors first gave experiments and theoretical analysis to illustrate the relationship between hard samples and robust over-fitting, which is an interesting observation. Then, the proposed algorithm is reasonable and makes sense.\n\nWeaknesses:\n\n1. My primary concern is about the novelty. The difficulty level of samples has been considered in the adversarial training, especially the strategy that considers the weight of examples (like MART and SAT). I understand that the author constructs a new adversarial training by observing the relationship between hard samples and robust over-fitting. However, the proposed algorithm is very similar to self-adaptive training (SAT). Even in the experiments, the author did not compare and analyze the SAT algorithm. \n\n2. It is not clear how to distinguish hard adversarial samples. The authors first said they provide a model-agnostic metric, which can measure the difﬁculty of an instance. But, it is hard to understand how to use this metric to guide adversarial training. In my opinion, section 6 does not seem to have much to do with the previous observation. \n\n3. I carefully compare and analyze the difference between the algorithm in this paper and SAT, and I think it is mainly on the final adaptive target. So, if we also add this line to SAT, what will be the result? ",
            "summary_of_the_review": "Through experiments and theoretical analysis, this paper concludes that hard adversarial samples will lead to overfitting. However, the introduced algorithm is not novel enough, which is very similar to the existing self-adaptive training. Even in the experiments, the corresponding experiments are also missing. Based on these, I recommend ''marginally below the acceptance threshold''.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Ethics Concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a metric for measuring the difficulty of instances. The authors empirically show that hard instances lead to the issue of robust overfitting. Theoretical analysis on logistic regression and general nonlinear model indicates that as the increase of instance difficulty, the adversarial vulnerability becomes higher. Finally, this paper proposes a reweight strategy and the trick of adaptive label for improving the adversarial robustness.",
            "main_review": "Strengths:\n\n1.\tThis paper shows that hard adversarial instances lead to degraded generalization performance in adversarial training based on the measurement of instance difficulty. \n\n2.\tThis paper provides extensive theoretical analysis that shows fitting hard adversarial data hurts adversarial robustness.\n\nQuestions:\n\n1.\tI am concerned about the meaning of 'model-agnostic' which is one of the property of the metric for instance difficulty.  The calculation of loss for each instance depends on the parameters and the optimization for each model. Therefore, how can you ensure for each model and each setting of training, the difficulty of a particular instance keep the same? \n\n2.\tIn Sec.4.2, this paper claims that adversarial overfitting arises from the model’s attempt to fit the hard adversarial instances. However, it is confusing that the easy instance (yellow lines in the right panel of Figure 2) has the similar behavior with the hard instance (red lines in the right panel of Figure 2) and the model fits the easy instances (yellow lines in the left panel of Figure 2) and hard instances (red lines in the left panel of Figure 2) almost simultaneously. Therefore, it seems that adversarial overfitting happens when the model’s attempt to fit the easy and hard adversarial instances.\n\n3.\tTheorem 2 and Theorem 3 both indicate that with the increase of instance difficulty, the adversarial robustness becomes lower. However, it is not clear how can those theorems show the relationship between robust overfitting and instance difficulty.\n\n4.\tAlthough the effectiveness of two tricks proposed in Sec.6 are empirically validated, the motivation for these two tricks is not clear. The reweight strategy gives lower weight for the instance whose predicted probability is lower. However, it is hard to say the instance whose predicted probability is low is the hard instance measured by the proposed metric. Therefore, the proposed method seems to be hardly related to the metric.\n",
            "summary_of_the_review": "Overall, this paper investigates the impact of hard instance on adversarial training. However, I am not convinced of the novelty and the correctness of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}