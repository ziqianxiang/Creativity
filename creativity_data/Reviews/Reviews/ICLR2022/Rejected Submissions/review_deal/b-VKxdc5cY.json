{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Perron-Frobenius operator (P) is a well-known tool which maps the density of a dynamical system at time t (p_t) to that at t+1 (p_{t+1}): p_{t+1} = P p_t. The idea has recently been extended (kernel Perron-Frobenius operator (kPF); Klus et al. 2020) to map a probability measure p_Z to p_X via covariance operators (4) associated to a reproducing kernel; this corresponds to the transformation of the kernel mean embedding of Z to that of X as it is recalled in (5). The authors use the kPF technique in generative modelling to map the known prior (p_Z) to the data-generating distribution (p_X), and illustrate the idea numerically.\n\nWhile the focus of the paper (generative modelling) is relevant, the reviewers had several severe issues with the submission:\n1) the manuscript lacks clarity of presentation at multiple points,\n2) the reviewers had concerns with the scalability of the approach (which unfortunately has not been analyzed),\n3) the submission is a straightforward application of a well-established tool (kPF) in the literature; the paper lacks novelty.\n\nSignificantly more effort is required before publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a novel (or an unusual type of) generative modeling approach that is kernel-based and non-parametric. The basic idea is using an operator that maps from the RKHS of Z to the RKHS of X, so that data generation can be done by mapping the prior distribution $p_{prior}(z)$ to the RKHS of Z, applying the operator, and project down the result in the RKHS of X to the data space. The operator is constructed as the RKHS analogy of the desired conditional distribution p(x|z) (which makes $\\int p(x|z) p_{prior}(z) dz = p_{data}(x)$), so that it transforms the RHKS embedding of $p_{prior}(z)$ to that of $p_{data}(x)$. However, to guarantee a desired $p(x|z)$, the operator is chosen by assuming $x$ and $z$ are independent. The operator can be estimated by samples of the joint distribution, which amounts to samples of $p_{data}(x)$ (i.e., training dataset) and samples from $p_{prior}(z)$ under the independent assumption, and the maps between the sample space and the RKHS can also be estimated. Experiments show the utility of the method for data generation for densely supported distributions.",
            "main_review": "I'm deeply impressed by the novelty. It is a smart idea to start with the formulation of data generation as transforming a tractable prior to the data distribution, and then implement the formulation in RKHSs using a corresponding transfer operator. Up to my knowledge I did not notice similar ideas before. The technical details seem good to me. The paper also presents a satisfying introduction to involved techniques (nevertheless the paper may need to highlight the novel technical contribution).\n\nHowever, the generative modeling approach is actually quite different from common ones, which leads to my two major concerns.\n* The presentation seems misleading.\n  First comes my understanding of the method. It is basically not of the same type of common deep generative models: it is non-parametric, it does not have a training stage, and its generation process is a manipulation on all training examples. In this sense, it is of the same type of kernel density estimation (KDE). Also, the method constructs the transfer operator by assuming the independence between $z$ and $x$. This makes $z$ not the usual latent variable: it does not hold any information of data, but would rather act as the random seed. (In analogy to KDE, this $z$ may correspond to the random noise around each data point).\n\n  What I felt most misleading is the title. There is no distribution matching: the method constructs a transfer operator based on the distributions at the two ends, instead of matching a learnable distribution to a fixed target distribution. Also, the method itself is not a deep generative model: it is non-parametric (does not involve a parametric model), so \"deep\" does not make sense (the deep AE is independently pretrained).\n\n  I may prefer presenting the method as it is and highlighting its unique utility/benefit as it is (instead of telling its benefits as a deep generative model, on which see the issues below; it could have other utility like learning-free/model-free data augmentation).\n\n* Practical usefulness as a generative model.\n  - Calculating the inverse of Gram matrix (Step 6 in Alg. 1) seems to cost a complexity cubic to the dataset size. Although Fig. 5 shows the training time comparison, I think asymptotically, increasing the dataset size may not lower the convergence time of parametric methods (as long as the data samples are iid), but the cost of the proposed method increases cubicly anyway.\n  - Following the above point, in the development of generative models people are trying to explain that the model is not just remembering the training data. This type of method instead just generates by manipulating training data, at the other extreme. Is this acceptable? Particularly, the time complexity for deployment / data generation, would then depend on the training dataset size.\n  - As mentioned, since $x$ and $z$ are taken as independent random variables, this $z$ cannot serve as a representation of data. (Posterior $p(z|x)$ is the prior $p(z)$. Also for this reason I did not understand the first limitation listed in Sec. 6. This is also what makes the approach in the same type of KDE.) This disallows the usage for embedding learning, manipulated generation, dimensionality reduction, clustering, etc.\n  - As also mentioned in the paper, the method requires both marginal distributions to be supported on the whole space but not on a lower-dimensional manifold. When the latter is the case, an auxiliary model is required for dimensionality reduction.\n\nI have some other minor questions.\n* As I feel the method is of the same type as KDE, an experimental comparison with KDE is expected.\n* I wonder how the choice of kernel and kernel bandwidths are set and how they influence the performance.\n* What's the difference between the Perron-Frobenius operator in Definition 3.1 and the push-forward operator of a distribution by the same map $f$ (unnecessarily invertible)? Also, the domain of $f$ (and $\\mathcal{P}$) seems better be $\\mathcal{Z}$ (and $L^1(\\mathcal{Z})$).\n* I did not quite understand the method in Sec. 4.1. Particularly, what is $X$ and what is $X'$? Which one is the output of the inverse map?\n* The generated data is a _linear_ transformation on training data samples. Is it flexible enough? Can it generate all possible samples following the data distribution?\n",
            "summary_of_the_review": "I appreciate the novelty and I think the idea is worth being noticed. Nevertheless the presentation (story) seems a little misleading, which may not be a proper way to present the merit of the method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to leverage the Perron-Frobenius operator to simplify the mapping from Z -> X in latent variable generative models. Specifically, this “forward operator” can be approximated by a closed-form linear operator in the RKHS. They evaluate their method on both synthetic settings and high-dimensional image datasets such as MNIST, CIFAR-10, CelebA, and FFHQ.",
            "main_review": "Strengths: \n- The idea of using kernel transfer operators for generative modeling is novel and interesting, and. The empirical results on various image datasets are also quite good.\n\nWeaknesses: \n- Notation is a bit confusing at times, as some items are not defined. For example, the authors should define the tensor product operation in Definition 2.3. Additionally, \\Delta_t in the integral right before Eq. (2) is not defined, and I’m not sure what z(.) is supposed to be when describing the underlying dynamics in Section 3. And why is the forward operator defined as f^* = (I + g^*)?\n\nQuestions:\n- Why can we just embed Z into X as mentioned in the 1st paragraph of Section 3?\n\nMiscellaneous/minor comments and typos:\n- There are numerous typos/awkward sentence phrasings that should be cleaned up.\n- “We often [approach] this” in the intro\n- Citation formatting is confusing (should use parentheses)\n- “We [demonstrate]” at the end of Section 1\n- “MDS-based” in Section 4.1\n\n",
            "summary_of_the_review": "This paper proposes to use kernel transfer operators for generative models, by leveraging a kernelized embedding of the “forward operator” (generative model). The authors experiment with various kernels and obtain favorable results on high-dimensional image generation tasks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a new type of generative model. The new scheme is based on a kernel transfer operator that leads to a cheap method for distribution matching. The authors rely on rigorous theory on RKHS and propose a framework for transferring a prior distribution linearly (in RKHS) to the data distribution. The authors demonstrate that the proposed approach leads to improved approximations of observed distributions. Specifically, the new approach can lead to the generation of new images form a given distribution and requires less training time compared to existing baselines. The paper is mostly well written, and the method relies on solid justifications. The authors demonstrate the usefulness of the method and compare it to several existing methods. ",
            "main_review": "The authors propose a new method for an important problem in machine learning. The new perspective of incorporating RKHS tools into generative models is interesting and novel. Nonetheless, I am not completely convinced that the method leads to a substantial benefit compared to existing approaches. My main comments are :\nSince the method relies on interpolation using RKHS it probably can’t extrapolate beyond the support of the given distribution. Doesn’t this limit the method in terms of its ability to generate new samples? I mean here that FID should not be the only metric for evaluating a generative model; other metrics are more meaningful in that sense, for example, in terms of the diversity of the generated samples.\n-The authors demonstrate that their method has low runtime compared to several baselines; however, some methods are missing from these evaluations—namely, Vanilla VAE and GMM.\n-On the same point, what is the inference time required by the method to generate a new sample? I think it should be longer than of a vanilla VAE.\n-Can the method be used for extremely large datasets? The kernel inverse calculation should take long and also is limited by the memory restrictions.\nMinor comments:\n-we often approaches-> we often approach\n-by via -> remove by\n-SGD mentioned before it is defined.\n-the eq. above eq. 2 ends with a comma, should be period.\n-period missing after eq. 2.\n-kPF mentioned in caption 1, before it is defined.\n-The caption in figure 1 doesn’t provide a clear explanation for the proposed scheme, this should be extended.\n-”A natural extension of Klaus et al.”-> perhaps you mean “A natural extension of the work of Klaus et al.”\n-In general some of the citations should be in brackets.\n-”There, for” this sentence is not clear.\n-”See Klaus” - why is See capitalized.\n-X’ and x’ are not well defined.\n-step 10 in algorithm 1- this is python notation; perhaps you want to use a mathematical notation?\n-NVAE in the caption of figure 3 is only defined on the next page. Please rearrange.\n-SRAE is also only defined after its first mention.\n",
            "summary_of_the_review": "To summarize, the authors address an important problem and propose a new generative model with several benefits over existing methods in the low sample size regime. The results demonstrate that the method leads to improvement in retrieval tasks. However, the technique has some limitations (namely, the inference may take a long time, and there might be memory limitations for large datasets). The experiments do not provide a complete picture of the method's capabilities, namely how diverse the new points are and whether they really differ from the original samples. To demonstrate that a generative model is useful, the authors could try to use the model to enrich imbalanced datasets and see if clustering or classification improves.\nFor these reasons, I recommend a weak rejection of the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a density modelling approach that is based on applying conditional embedding operators to RKHS embeddings of a base distribution.\nThe approach, thanks to use results from the kernel literature, is conceptually simpler and appears less computation heavy compared to VAE/flow models, while at the same time performing favorably.",
            "main_review": "Strengths:\n* A conceptually simple kernel method to model densities in latent spaces of AEs.\n* A simple linear operator in RKHS that maps the embedded base to the embedded data distribution, neat idea!\n* Results appear to be comparable to sota methods of density modelling, while an order of magnitude faster.\n\nWeak points:\n\nNovelty / Presentation\nSection 3 of the paper on first instance looks like the authors are presenting a novel concept here. However, the transfer operator is just the conditional embedding operator of Song et al, marginalized over the conditioning variable / applied to the mean embedding of Z, plus a pre-image estimation (see alos below). The equations are exactly the same as in e.g. Song et al 2013 (e.g. see Fig 5 in that paper).\nThere is a lot of equation shuffling where definitions are plugged into each other, (eq 6, eq 7, and the unnumbered eq before 6, Proposition 3.3 and its proof), that re-write definitions of the underlying operators in one form or another. I am quite skeptical about this form of presenting the method, and about the usefulness of these equations.\nIn addition, the connections to dynamical systems (all of which are reviewing external literature) do not appear to be necessary for the rest of the paper, especially considering Song's work, which is essentially the RKHS version of the standard rules of conditional probability.\n\nPreimages.\nAs the authors mention, pre-images are problematic as RKHS embeddings are not necessarily surjective. I am therefore skeptical about the deterministic pre-image estimation methods presented here, especially for sampling. What if the distribution represented by \\mu_X is multi-modal? The same point z could be mapped to two totally different x in that case, but with this deterministic approach, it won't. This should be discussed and investigated, even if performance suggests that this is not necessarily a problem.\n\nComputational costs.\nThe method needs to store, and invert a kernel matrix between the data points, which costs O(n^2) and O(n^3) respectively. This becomes prohibitive from a few thousand points. I am very surprised that this is supposedly computationally cheaper that other methods. I also think the comparison done is very vague: the kernel method is dominated by a Cholesky decomposition / matrix inversion, while other methods rely on SGD/etc. So supposedly, there is a convergence question in Fig 5. Wouldn't it be better to present the other methods on a graph time-vs-accuracy? I find it also find \"additional training time\" confusing, and it is not very clear what is exactly compared here.\nQuestion: The provided code contains code for Nystrom approximations. Are these actually used in the experiments? (Seems hard to invert a 10k kernel matrix otherwise). If so this should be described/mentioned.\n\nPre-trained AEs.\nI understand that the presented method will not work well when applied directly on high-dimensional data, so the using an AE and work in its latent space makes sense. What I do not understand however, is how this can be compared (in terms of compute and performance) to the other methods in the paper. E.g. How is the method compared to a VAE when \"all models share the same encoder/decoder\"? Are the other models applied on the original data, or on the AE latents? Does it make sense to apply e.g. GLOW to AE latents? It would be great to make this more clear. For the NVAE case, I am even more confused: do we first train and NVAE, then use the kernel method in its latent space, and then compare it to the NVAE that we trained in the first place?\n\nBrain data experiment.\nIt is a bit unclear why generating samples from this dataset would be desirable, how these samples can be \"scientifically meaningful\", and what the results show (e.g. colourbar is not annotated, etc). Some more motivation would help.\n\nFinally, comparisons to GANs would be appropriate for the way the method is evaluated (only samples, density only shown for the toy examples)\n\n\nMinor:\n* Double period p2 \"deep generative models and dynamical systems..\"\n* Inconsistent pre-trained vs pretrained.\n* Typo in Intro \"we often approaches this problem\"\n* \"Use mean embedded operator\" -> \"Use mean embedding operator\"\n* p1 \"the lower bound of likelihood\" -> \"a lower bound of the likelihood\"\n* many times citations are not properly punctuated, i.e. citep should be used\n* The tensor-product operator in eq 1 is not defined.\n* \"Let us temporarily assume Z=X\" ... this is never resolved.\n* What does point (b) mean \"The training scheme directly learns the mapping from the prior rather than approximation\" ? ",
            "summary_of_the_review": "Great idea, though there are some issues with presentation and experiments. Not very convincing as is, but a lot of potential to be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}