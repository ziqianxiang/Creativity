{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a novel explanation for the ineffectiveness of active learning (AL), namely, that AL will select unlearnable collective outliers. \n\nReviewers generally find the finding is interesting, but the paper lacks in-depth analyses. There're additional concerns on the experimental setups."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper examines how active learning works with large pre-trained models, with a novel explanation with empirical evidence. ",
            "main_review": "Strengths:\n - I could learn various empirical observations of how active learning works in these large pre-trained models on various NLP tasks. I hope to see this paper at the conference and others can learn from it as well. \n\nWeakness:\n - It would be nice to see some macro analysis of data points showing the overall tendency of each setup. \n - I hope to see more insights regarding how these two different AL methods (density-based and entropy-based) would work in a different way.",
            "summary_of_the_review": "Good empirical findings from extensive experiments, but limited to the individual setup in specific conditions without an overall picture of dataset dynamics.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work investigates Active Learning (AL) for fine-tuning large pre-trained transformer models (BERT, RoBERT) with applications to several NLP tasks.\nThey observe that depending on the task/dataset, AL actually does not improve over a random baseline.\nTheir ablation tests suggests that the instability of re-training the large models might be amplified by instances selected by AL. As a remedy, they suggest to train several times with different initializations, and then selecting the best model on the development data.",
            "main_review": "Strength:\n- Experiments on many data sets.\n- The idea of using convex hull of AUC is interesting, but it would it nice to confirm it experimentally. \n\nWeakness:\n- Q seems to be too small, which might amplify the instability issue.\n- Little novelty \n\nWhile their experiments, and findings are interesting, it leaves many questions that could (at least in principle) be answered by more experiments. \nTheir main finding seems to be that larger transformer models lead to more instability for training with the instances selected by AL, Table 4 and Table 5.\nHowever, Table 4 is only an approximation to results that could be achieved with training with different initializations. Though computationally expensive it would be very interesting to see whether the results from the approximation can be confirmed. \n\nSeveral questions:\n- What exactly is meant by the Entropy method?\nMy understanding that is that Entropy is the same as Least Confidence, since high entropy <-> low confidence.\n- Why is Entropy missing in Table 2?\n- Why is Q set to 500? This seems to be too small. It would be interesting to learning curve graphs with larger Q.",
            "summary_of_the_review": "For an experimental paper, it is interesting, and might point to some new directions for AL.\nHowever, more experiments to clarify some points (see above) should be added.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work evaluates active learning methods for pre-trained Transformer models, and diagnoses their inconsistent performance as a consequence of training examples which exacerbate variability in test-time accuracy. They propose re-training the model repeatedly to circumvent bad outcomes due to variability, and show this works particularly well on larger pre-trained models. The authors conduct a wide range of experiments, over many datasets, and also a series of ablation studies to reinforce their claim that these active learning methods are ineffective for pre-trained Transformers.",
            "main_review": "Strengths:\n \nThe authors connect active learning example selection to more severe training instability than random selection. Further, they show downside-variability (“greater instability on the losses than on the wins”) suggesting specific sets of examples are responsible for systemic failures in active learning, but not necessarily that other selections would greatly outperform the random baseline. Both of these observations are of interest to the NLP active learning community, adding depth to the characterization of the problem.\n \nThe wide range of datasets and active learning techniques they use (including BALD which prior works shows is very competitive) lends credence to the conclusions. It especially corroborates that certain tasks are innately more challenging for active learning than others. \n \nTheir thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more. For instance, the greater instability of larger Transformers to active learning bodes poorly for practitioners leveraging ever increasing model sizes for low-resource datasets. Additionally, the authors conclude that pre-training is not a significant factor in the efficacy of active learning, but their numerical results suggest active learning methods (Entropy and Coreset) narrow the gap with the random baseline significantly from BERT-Base to RoBERTa-Base! The margin of change seems even larger than some results which are discussed in the paper as significant. More discussion could bolster this finding.\n \nWeaknesses:\n \nThe primary weakness of the paper is the lack of convincing justification that the authors have discovered a phenomenon distinct from “collective outliers” (Karamcheti et al., 2021) -- or if it is distinct, how exactly is it distinct? \n\n1. For context, Karamcheti et al. (2021) introduce an \"oracle\" active learning method that prunes uninformative (outlier) examples from the unlabelled, selectable pool U, but it requires gold labels to filter these. This work tries a single pruning rate and finds it significantly degrades, rather than improves results: for both active learning and random selection. However, they only mention a single pruning rate of 50%, which could easily over-prune (all challenging as well as outlier examples) for the NLP datasets used here. Especially because of the surprising magnitude by which this pruning degrades absolute performance, it is unfortunately necessary to try more pruning rates for a fair comparison.\n\n2. Even if pruning is ineffective (at multiple pruning rates), it’s never really explained why, despite this being a core contribution of the paper. \n\nA qualitative analysis was required in Karamcheti et al., (2021) to explain “collective outliers”. A similar analysis here could greatly demystify why these sets of examples cause instability, and whether they are indeed “informative”.\n \nThere are a couple decisions related to methodology that may need some further examination. \n\n1. The first is the total unlabelled pool size. It appears to vary by orders of magnitude according to Table 10 in the Appendix. While the batch selection L and total selection Q variables are constants, fixed at 25 and 500 respectively, the pool of examples to choose from varies wildly. This is very likely a confounding factor in the efficacy of active learning and pruning techniques. It deserves some discussion or control.\n\n2. The second is the choice of L=25 examples chosen at each acquisition, and also as the seed set. I am concerned this is quite low and could yield exaggerated instability, especially for large Transformer models. If I read them correctly, Ein-Dor et al. (2020) use at least L=50 for simple classification tasks, and Karamcheti et al., (2021) use L>= 400 and larger seed sets. The authors investigate L=12 in the ablation, but in a real setting, it seems unlikely practitioners will label <50 examples before re-training.\n\nParticularly because the reported accuracy margins are so slim, either of these variables could modify the empirical conclusions. A statistical test on the Average performances would also improve confidence in the conclusions.\n\nQuestions and minor suggestions for the authors:\n\n1. Why is Entropy missing from Table 2? And BALD-MC from Tables 3 and 4? And why is BALD missing again from Table 6? Computational expense is used to justify the omission of BALD in one instance, but the others appear inconsistent?\n2. It’s very hard to compare the absolute differences Tables 1 and 2 for ourselves. Perhaps there is some visual representation that could help demonstrate the comparisons you make in the text?\n3. Margins are very small for the Average differences across all datasets as well -- have you considered confidence intervals on those as well?\n4. Minor: Only half the datasets are shown in Tables 3 and 4, but it’s unclear how/why those were chosen?\n\n",
            "summary_of_the_review": "In summary, we would encourage the authors to more rigorously examine the heart of their work -- the source of the instability they uncover: is pruning really ineffective, or are we removing too many challenging examples along with the outliers? Can we qualitatively diagnose the problem and explain what makes an example “informative” but unstable? And as accuracy margins are so small, it may be unfortunately necessary to justify or relax some hyperparameter choices, and run additional experiments to confirm these findings generalize.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper test 4 deep active learning mehods across 10 datasets (including text classification and multi-choice commonsense reasoning) based on pre-trained LMs. It conduct many ablation studies to explore whether some inherent factors of like batch size in active learning, the model size of pre-training models would affect the effectiveness of active learning with pre-trained LMs. ",
            "main_review": "Strengths: The experimental scale of this work is undoubtedly large and comprehensive, involving many data sets, different types of active learning algorithms. It is constructive for the researchers who tried to use active learning methods for reducing the labeling cost in NLP tasks. \n\nWeaknesses: \n1) compare with [1], the research questions and the corresponding conclusions of this work are not enough novel and attractive. Actually, current studies have involved: i) marginal benefits of deep active learning methods; ii) employing active learning with pre-trained LMs like Bert on regular NLP tasks; iii) employing active learning on challenging tasks like VQA. In fact, these studies have covered almost all existing research topics that worth studying and revealed the problems existing in deep active learning. For now, it is more meaningful to address these issues than to continue to discuss whether deep active learning has similar problems in some subdivided tasks.\n\n2) In the experiments, the authors only adopt 500 quota for each AL experiment, it's too small. For example, in Figure 2 (CODAH), the performance of AL methods are far from convergence, the author should adopt more quota or at least provide the performance trained on full dataset, like [2].\n--------------------------------------- Minor Issues -------------------------------------------\n\n3) Many writing problems could be found in this paper. For instance, in page 2, in contribution 2, the sentence is incompleted. In section 2.2, should add ';' or '.' at the end of each item when describing the AL algorithms.\n\n4) It is inaesthetic to plot standard error bar, like Figure 1, can use plt.fill_between instead of plt.errorbar.\n5) Should pay more attention to design the layout of tables, e.g., in Table 6, it should be Method | Dataset | AL methods.\n\n\nReferences:\n[1] Karamcheti S, Krishna R, Fei-Fei L, et al. Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering[J]. arXiv preprint arXiv:2107.02331, 2021.\n\n[2] Margatina K, Barrault L, Aletras N. Bayesian Active Learning with Pretrained Language Models[J]. arXiv preprint arXiv:2104.08320, 2021.",
            "summary_of_the_review": "This paper conducted a large amount of empirical experiments on AL with pre-trained LMs. However, the research problems they explored are somewhat covered by existing research outputs and their conclusions are less attractive compared with [1]. Additionally, the writing of this article should also be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigates the usefulness of Active Learning (AL) on a variety of NLP tasks with pre-trained models. The authors show that with pre-trained models, AL can sometimes offer inconsistent and even poorer results than random baselines. In addition to outliers, they show that training instability is a major issue. There are also various further ablation studies on this issue.\n",
            "main_review": "Strengths:\n\nThe topic investigated in this work is interesting and would be useful for further works since pre-trained models have been shown effective at learning with small amounts of annotations and AL is also a method with similar spirit.\n\nThis paper provides a good and solid empirical analysis on this topic with various different models, datasets and AL methods.\n\nThere are several detailed ablation analyses which I find interesting, such as the influence of question format and adversarial filtering on the datasets.\n\nWeakness:\n\nOne of my major concerns is on the point of training instability. I feel that this might not be the problem of AL, but at least some part of the problem is that we need a more robust way to train the model? Surely the random baseline is also treated in similar ways, so the comparisons are fair. Especially, fine-tuning might be sensitive to hyper-parameters, and it seems that there are no specific tunings? Surely I understand the cost of doing this, but I think it would be nice to select some of the datasets to carry out some hyper-parameter tuning.\n\nAs discussed in Section 3.4.4, annotation batch size could be important. I think there should be more experiments on this, for example, with larger sizes rather than smaller.\n\nOne specific question that I have but might not be fully answered in this work is that: why are the examples selected by AL more likely to cause the training instability? Some experiment results show that outliers might not tell the full story. Then why does this happen, is it related with instance diversity or even simpler label diversity (for example, maybe in some iterations, AL only chooses instances with the same label)? I'm quite interested to see more analysis on this.\n\nOther comments:\n\nFor the hyper-parameter issue above, what are the differences of multiple runs in the multi-training (in Section 3.3), only changing seed or changing hyper-parameters? If only seed, I suggest doing similar experiments with some degree of tuning of hyper-parameters.\n\nIt would be nice to have results where all the datasets are available as in standard supervised settings as upper-bounds.\n\nIt would be interesting to explore tasks beyond simple classification.\n",
            "summary_of_the_review": "Generally I find that this work provides a reasonable study on the usefulness of AL with pre-trained models, there are lots of experiments and there are certain interesting findings. Nevertheless, I still have concerns with the training setup and there are still some analyses that I would like to see.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}