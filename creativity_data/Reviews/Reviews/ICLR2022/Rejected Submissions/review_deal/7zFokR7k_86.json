{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper describes a system for learning rules in a quasi-NL format: roughly Horn clauses where a predicate p(X1,...,Xk) is replaced by a natural language pattern interleaving ground tokens and variables.  The method is to propose ground sentences - using one of several task-specific approaches - use anti-unification of pairs to variabilize, and then find a minimal theory from these proposed pairs by reduction to maxsat.\n\nPros:\n - QNL is a neat idea, and makes symbolic rule-learning possible to some NLP tasks\n - The use of maxsat is novel in rule-learning AFAIK\n\nCons:\n - Unification is a highly simplified model of the NL task of cross-document co-reference\n - It's unclear if maxsat process will work in the presence of noise, or how well it scales\n - The datasets use clean text generated from templates or synthetic grammars\n - Experimentally, the generality of the system is not well demonstrated, because there are differences in how it is applied: eg a subset of short examples for scan, input engineering ($TRUE, $FALSE) for RuleTaker, plus the \"heuristics for filtering invalid rules generated by anti-unification”\n - It's not clear if this work really speaks to the main \"point\" of the SCAN and RuleTaker datasets.  These are both the kind tasks that symbolic systems would be expected to do well, and are used as ANN benchmarks because ANNs perform in unexpected ways: worse than one would expect for SCAN, and better for RuleTaker.  They are important for understanding ANNs but I'm not certain what the research benefit is of using them for symbolic methods as a benchmark."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a symbolic system in Quasi-Natural Language, MetaQNL, which is compatible with both logical inference and quasi natural language expressions, and where the basic building blocks are sentences and rules. The authors also propose MetaInduce, which learns to generalize a set of rules that explains the examples in MetaInduce.\n\nMetaInduce consists of three mains steps:\n1) a rule proposer proposes a set of concrete rules as candidates for each individual example. This set may not be fully correct and may not be the minimal explanations, and they are used to prove the example using forward/backward chaining.\n2) the authors apply a symbolic procedure called anti-unification to generate abstract rules from concrete rules.\n3) and finally, proof paths are encoded in MAX-SAT and a subset of all rules are solved using a MAX-SAT solver to find minimal possible explanations\n\nThis paper evaluates its methods on two synthetic datasets, and the authors claim to learn compact models with much less data, and produces answers as well as proofs",
            "main_review": "strengths:\n1) this is indeed novel research problem - using a learning system together with a maxsat solver to make logical inference and identify rules.  There is a spectrum where on one end logical inference instances fully in natural language, and on the other end they are fully symbolic, and this paper falls somewhere inbetween.\n2) The paper is generally well-written, and mathematical part of the paper seems to be correct.\n\nWeakness:\n\nI fail to see the real contributions in this paper.  The only novelty seems to me is MetaQNL and MetaInduce can solve formal systems as well as systems represented in quasi natural languages.  However, are there actual applications that can potentially benefit from this problem formulation?\nOne of the major claims of this paper is it can produces checkable proofs, but isn't Proofwriter also capable of generating proofs which seemed more impressive because it is a fully neural system without any explicit encodings of rules and has the potential of working with real languages.  I get that proofwriter has billions of parameters and MetaInduce learns a system with much fewer parameters, but then what is the difference between this and combinatorial optimization?\n\nFurthermore, I failed to see a way to scale up this method.  In the end, it depends on a maxsat solver, which will become intractable quickly when there are more rules.\n\nIn summary, if the authors can present a real-world application that can potentially benefit from their system while other learning systems fail to do so and evaluate their method on a small real-world dataset, I would be less concerned.\n\nSmall comments and questions:\n1) Would appreciate a few citations to support the claim \"At a glance, this may appear a large departure from the conventional wisdom that learning-based systems, particularly deep networks, are far superior to rule-based systems, as history has demonstrated repeatedly.\"  The authors need to be specific what on tasks.  For example, NNs are far behind SAT solvers on propositional formulas (GQSAT as an example for learning-based system).\n2) I fail to understand why rule proposers need to generate concrete rules, aren't all the rules already included in MetaQNL systems?",
            "summary_of_the_review": "In short, while I acknowledge this work is novel, its setting is not very practical.  This paper would benefit from presenting a potential real-world application, or else some theoretical generalization results on how well their systems can learn.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        {
            "summary_of_the_paper": "This paper proposes an algorithm that learns rules from natural language data, and a symbolic system for manipulating these rules, where existing provers can be applied. The objective is to maximize the number of examples in a test set that are consistent with the proposed mode while minimizing the number of rules in the model. The algorithm consists of three steps - given a training example, it proposes concrete rules, abstracts concrete rules into rules with variables, and prunes the resulting rules. ",
            "main_review": "Pros: \n- The learning algorithm is interesting and the problem of automatically discovering prepositions from examples is important for ATP in formal or informal languages.\n- The paper clearly defines the terms used and explains the methods and experiments well.\n\nCons:\n- As the authors pointed out, the experiments are neither large-scale nor real-world. One result of this is existing methods already achieve good performance, and it's not clear that the proposed method results in better performance.\n- None of the components (theorem prover, rule abstraction, rule pruning) are novel individually.\n\nQuestions:\n1) Are TRUE FALSE and MAPS_TO the only special symbols? It would be helpful to state this. \n2) Are there any unprovable examples in SCAN?\n3) The number of rules and symbols learned by MetaInduce is hard to compare with the number of learned parameters in ProofWriter. Is there a better metric to compare the two methods on? \n4) What are the advantages to using the proposed symbol system instead of first order logic? ",
            "summary_of_the_review": "The paper proposes an interesting solution to an important problem, but as-is the experimental settings and results are not compelling. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The work proposes two new concepts:\n+ MetaQNL: a symbolic system in Quasi-Natural Language. Instead of representing rules in a formal symbolic format such as first order logic rules, in MetaQNL, a rule is represented in a Quasi-Natural Language format which includes words, variables and control symbols. Since the rules are represented in an informal representation. An interesting property of the MetaQNL representation is that it allows to perform backward or forward inference by substitution of variables with sentences. The authors assume that texts can be translated into the MetaQNL format and thus solving the reasoning problems with text input is possible via mining rules from text and backward/forward reasoning with the Quasi-Natural Language.\n+ To mine rule from text, the authors proposes an algorithm called MetaInduce. MetaInduce iterates through the training data several time to build a compact set of rules that trades complexity to prediction accuracy. It is a bottom up rule induction approach where it includes a Rule proposer which propose a concrete rule from a training example and an anti-unification module to abstract the concrete rule with more generalised rules. A pruning process based on MAX-SAT is used to prune the set of rules such that it optimise the regularised objective.",
            "main_review": "Overall the idea is novel and I like the paper presentation. However, I have some concerns regarding its ability to work with real natural language and the reproducibility of the proposed approach.\n\nMAJOR CONCERN 1:\nI have a concern that the rule proposer and the anti-unification require very well-formed of the sentences to be working well. \n\nAs we can see with the example on page 7 figure (a) about anti-unification, the sample shows very simple rules. I wonder how it will work when the sentence is really complicated with real natural language? Also the experiments in RuleTaker only with synthetically generated sentences in a very controlled language, could you please  demonstrate your results with complicated sentences beyond synthetic texts, I think the paraphrased datasets within the RuleTakers even not really natural language but that could be a good exercise for your methods to test on.\n\n\nMAJOR CONCERN 2:\nI worry about reproducibility as the source code is not open but the details explanation of the proposed approaches are missing. For example, from the paper I don't know how the rule proposer work and how the anti-unification is implemented with quasi-natural language. I would suggest to provide very detailed about the methods, with the current information I doubt that people can reimplement your work and reproduce what you have demonstrated.\n\nMAJOR CONERN 3:\nIn the RuleTaker example, it is known that Transformers are good at generalisation when they are trained on queries with depth 3 or greater. Yet transformers are not good at generalisation when it is trained on lower depth queries. Could you please provide comparison results with Transformers when it is trained at lower depths? \nOther minor comments:\n\n\n\"In contrast, our approach does not require a semantic parser, because rules in MetaQNL are directly applicable to natural language.\" ---> This is a strong statement, it requires a support with real natural language examples rather than synthetically generated sentences in the experiments.\n\n\nDefinition 6: what happen if there is no proof for a goal and the goal is proved via the close world assumption?\n\n",
            "summary_of_the_review": "The paper proposes a new concept called quasi-language which allows representing rules in an new informal format that still allows to perform forward or backward reasoning while it is assumed to be mined easily from texts. The experimental results with some datasets with synthetically generated texts show that the methods work very well and advance state-of-the-art results. \nHowever, I doubt the application of the work with natural language input, I explicitly request to perform more experiments with paraphrased datasets in RuleTaker (MAJOR CONCERN 1). I also have a concern about reproducibility as the presentation lacks details of the core components of the proposed algorithm (MAJOR CONCERN 2). I also requested an additional experiment regarding the training data with low depth queries to check the ability of generalization of the proposed approaches. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Traditional research on symbolic reasoning assumes input data are already translated into a format that complies with the formalism of the underlying system. A major struggle for symbolic reasoning is to handle the data coming in a natural form (e.g. images/text) and perform reasoning on the same. Inspired by this challenge, this paper undertakes the problem of converting text inputs into the format of a system formalism. \n\nThe formalism is also proposed by this paper and is called MetaQNL. The framework of MetaQNL is designed keeping in mind the specific need of operating directly on natural language sentences. This formalism supports the natural language sentences with variables within them. This trick alleviates the need of using a semantic parser to parse natural language sentences for the purpose of logical reasoning. Thus, MetaQNL, by design, is conducive to working with natural language inputs. \n\nNext, this paper proposes an algorithm to induce rules from natural language inputs within the MetaQNL framework. This paper doesn’t worry about performing actual deductive reasoning/theorem proving and instead proposes to use existing provers and instead focus on the more challenging problem of rule induction. MetaInduce algorithm draws inspiration from existing ILP approaches. MetaInduce encodes the rule induction problem as a maximum satisfiability (MAX-SAT) problem, which can be solved efficiently by off-the-shelf solvers. The proposed method consists of 3 steps. \n\n1. Given a training example, a rule proposer proposes a set of concrete rules as candidates. This set can be overcomplete/inaccurate. \n2. It generates abstract rules from concrete rules via a symbolic procedure called anti-unification. This is essentially a process of aligning common substring segments across two or more strings.\n3. It encodes the proof paths in MAX-SAT and solves for a subset of all rules using a MAX-SAT solver. \n\nThis paper benchmarks the proposed method on 2 tasks - learning compositional instructions and logical reasoning. For learning compositional instructions, it works on two standard benchmarks: MiniSCAN and SCAN and recovers precisely the ground truth rules. For logical reasoning, the proposed method achieves SOTA on the RuleTaker dataset. \n\n",
            "main_review": "Overall, I liked the scope of this paper, the importance, and non-triviality of the problem, and the novelty of the proposed approach. In my view, it certainly adds a dimension to the literature on symbolic rule learning. The idea of MetaQNL is simple yet effective to handle the natural language inputs within a formal symbolic system. The idea behind MetaInduce is also quite natural. Although there are some weaknesses of the proposed approach, I still feel this is a novel idea and has the potential to yield something big in the future. \n\n__Strength__\n\n- A well-written paper.\n- A very nice literature survey and positioning of the work relative to the prior art.\n- An important problem in the broad space of AI.\n\n__Weakness__\n\n- Time complexity of each of the three steps in MetaInduce is not discussed. It will be good to shed some light on this. \n- As stated in the limitation section, the proposed approach is far from mature but serves as proof of concept.  It does not scale to millions of training examples.",
            "summary_of_the_review": "See my comments in the main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}