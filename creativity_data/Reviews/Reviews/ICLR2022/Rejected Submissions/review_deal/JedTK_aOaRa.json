{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This is an interesting paper discussing differential privacy for multi-label classification. The initial reviews rated the paper with rather extreme scores, therefore I have invited an additional reviewer. This review did not clarify the issues raised by the most critical reviewer, but pointed out that the goal of showing how DP can be enforced in MLC is not fully obtained as there is a lack of the discussion concerning the MLC performance. This is also a problem raised in my comments. Taking this into account, I need to state that the paper is not ready for publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper is proposing differential privacy (DP) solutions for multi-label classification (MLC). In particular, it introduces three novel noisy perturbation methods for the MLC setup, analyzes them in terms of DP, and test them experimentally for accuracy, AUC, and other metrics.",
            "main_review": "Strengths:\n- The paper is relatively clearly written (see also the \"Further comments\" part though)\n- The theoretical results related to privacy are interesting, and are discussed in sufficient depth and details.\n- The solutions are clean and reasonably simple.\n\nWeaknesses:\n- The MLC learning part is not treated carefully. First of all, the canonical MLC metrics are not calculated in the experiments. (In fact, they are not even discussed in the paper.) This is important though, because this is one of the crucial, distinctive aspect of MLC. Therefore, it is not clear, how the proposed solutions perform in terms of these metrics.\n- The proposed algorithms are not compared to the state of art MLC solutions, and thus we don't know how much we lose in terms of the MLC metrics when we want to DP guarantees.\n\nBecause of these two shortcomings, it is hard to assess the contribution of the paper. The main goal of the authors was to show how DP can be enforced in MLC but the results presented in the paper are not sufficient to draw any conclusion: although the DP requirements are analyzed, the MLC performance remains unclear.\n\nFurther comments:\n- The acronym CaPC is used already on p1, but is only explained on p3.\n- Why do you use different notations (d resp. X) for the same notion (datasets) in Def. 1 and Def. 2?\n- What does p_{M(X)}(\\theta) denote in Def. 2?\n- Shouldn't Lemma 2.2 be stated with an inequality?\n- Def. 4 mixes sets with their membership vectors. Which is fine, but this should be mentioned explicitly.\n- Def. 5 is confusing. The meaning of the math formulation is that there is no element appearing in each of the P_i sets, whereas the text means that the P_i sets are all disjoint.\n- Below Def. 6: \"For any mechanism f, \\Delta_p f is maximized when the mechanism’s output for each coordinate i is flipped from predicted to not predicted or vice versa. The pair of teacher votes achieving this differ in each coordinate i.\" Aren't we supposed to work here with teacher votes that differ only by one bit?",
            "summary_of_the_review": "The DP results are solid, but the MLC aspects of the work has serious shortcomings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the design of differentially private multi-label mechanisms. In particular, the authors employ multi-winner voting protocols to existing differentially private single-label learning algorithms e.g. PATE. They consider three multi-label voting protocols -- binary voting, $\\tau$-voting and powerset voting. \n\nBinary voting works by independently applying majority voting on each coordinate. It is obvious that such an aggregation mechanism has does not provide a better privacy guarantee than $k$ applications of binary voting. The privacy guarantee can be improved when the coordinates are dependent. This motivates the authors to consider $\\tau$-voting where the $\\ell_2$-norm of each ballot is bounded by $\\tau$. Finally, in the powerset voting, the voting is done over the universe of all subsets of the alternatives i.e. $2^k$ alternatives.\n\nThe authors make two important observations through experiments. First, when there is high consensus and $\\sigma_G \\rightarrow 0$, binary voting performs best. On the other hand, $\\tau$-voting outperforms with lower consensus and larger values of $\\sigma_G$. Second, even in centralized setting, multi-label methods outperform existing benchmarks.",
            "main_review": "Strengths:\n- The authors consider an interesting problem as designing a privacy preserving mechanism with multi-label outcomes would be applicable in many contexts.\n- I also like the design choices as the proposed mechanisms can be easily implemented with existing DP aggregation mechanisms like PATE. Moreover, the theoretical bounds can be derived by building on top of the existing DP guarantees.\n\nWeaknesses:\n- As far as I understand, the parameter $\\tau$ in the $\\tau$-voting needs to be set based on the domain and there is no automatic way to choose a value.\n- Powerset voting becomes intractable to implement when $k$ is large. But probably there is a way to combine $\\tau$-voting and the powerset voting for large values of $k$.\n\nSome questions for the authors.\n1. From definition 3, it seems like the threshold T is independent of the candidate $i$. However, in definition 4, the constant $T$ depends on the index $i$ and it seems that the mechanism picks an index whose votes exceed $n/2$ (upto noise). So I think the presentation and the definition here is misleading.\n2. How do you define the threshold $T$ in definition 7?\n3. I think there is a range of design options between $\\tau$-voting and powerset voting. For example, you can ask each voter to vote on the best subset of size b. Did the authors consider other alternatives?\n\n",
            "summary_of_the_review": "I think the paper considers an important problem as the design of differentially private aggregation mechanism for multi-label outcomes is applicable in a lot of domains. The proposed mechanisms are also simple and build on top of the existing DP aggregation mechanisms. However, I felt that the experiments were not quite exhaustive. For example, it's not clear if it is beneficial to apply powerset voting on real datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors study differentially private multi-winner voting, which is designed for multi-label learning subject to a privacy constraint meant to limit information leakage about training data to an adversary. They propose three mechanisms: Binary, which essentially runs an existing differentially-private election for each label independently, \\tau voting, which works with votes that have bounded \\ell_2 norm, and powerset voting, which explicitly encodes each possible subset of winners as an alternative in an election and then votes over them. They show that Binary voting (the naive approach) generally outperforms powerset voting as long as there aren’t strong correlations between votes. Lastly, they show that they can use these multi-winner DP techniques to a extend single-label technique, PATE, and empirically demonstrate the effectiveness of their approach.",
            "main_review": "Differentially private multi-label classification is an interesting and well-motivated problem. \n\nThe Binary voting mechanism is quite naive in the sense that separating the problem into k separate elections, each to be evaluated in a differentially private manner (i.e., with Gaussian noise). It’s somewhat surprising that this seems to be optimal among the mechanisms (in the case where voters aren’t constrained in how many candidates they can approve) unless there is a lot of correlation between votes, at which point the (exponentially-sized?) powerset voting method becomes better. \n\nIs it correct to interpret the thresholds T in Definitions 4 and 6 as the cutoff point at which the sum term is at least n/2 (intuitively corresponding to a majority vote)? It could be useful to the reader to have a bit more explanation here.\n\nTo me, the most interesting contribution is that of \\tau voting, where all votes are assumed to have \\ell_2 norm at most \\tau. I have a few questions here: (1) how did you go about choosing \\tau in practice, and do you have proposals for how to choose it on new datasets; and (2) have you thought about an “average-case” approach where the average vote has \\ell_2 norm at most \\tau instead of requiring all votes to have \\ell_2 norm at most \\tau? \n\nI found the writing quality to have some issues (some minor comments below).\n\nMinor comments: \nDefine CaPC\nSection 2.2, CaPC paragraph: model's --> models\nSection 3.2, first paragraph: it is a popular method --> is a popular method\nBefore Definition 6: missing period\nThere are also many missing articles (a/an/the) throughout the paper",
            "summary_of_the_review": "The problem is interesting and the results seem to be quite comprehensive. My main concern is that the voting mechanisms proposed are relatively naive (separate into independent elections or run one big single-winner election). However, the technical results seem robust.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. They propose three new mechanisms. 1. Binary voting operates independently per label through composition. 2. \\tau voting bounds votes optimally in their l2 norm.  3. Powerset voting operates over the entire binary vector by viewing the possible outcomes as a power set. They prove that Powerset voting requires strong correlations between labels to outperform Binary voting. \nThey also use these mechanisms to enable privacy-preserving multi-label learning. They empirically compare techniques with DPSGD on large real-world healthcare data and standard multi-label benchmarks. Their techniques outperform all others in the centralized setting, and show that mechanisms can be used to collaboratively improve models in a multi-site (distributed) setting. \n",
            "main_review": "This paper introduce mechanisms for private multi-winner voting and multi-label learning, which in important in machine learning. The paper structure and the writing are good. These three mechanisms are shown clear, but for the experiment, the choose of privacy term epsilon is somewhat larger than the common privacy requirements. ",
            "summary_of_the_review": "It's good but may be not enough",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers differentially private multi-winner voting. This problem is a generalization of single-winner voting, which is widely used in PATE type private semi-supervised learning. The authors give three private mechanisms and perform empirical comparisons on multi-label semi-supervised learning settings.  Specifically, when there is no total votes constraint on each ballot, the binary mechanism performs report noisy argmax for each candidate. When there is a total votes constraint, the tau mechanism performs l2 clipping first and then performs report noisy argmax for each candidate as the binary mechanism. The powerset mechanism converts the multi-label problem into a single label problem and performs regular report noisy argmax. ",
            "main_review": "Final update: After the discussion period, I still vote for rejection. There are two main reasons.\n1) The authors do not seem to understand the issue I raised and clarified over and over again. Now I started to doubt whether the algorithm implemented in the experiment is end-to-end differentially private with the designed privacy budget. (I) I do not agree that data-dependent privacy is a different privacy guarantee or a relaxed version of differential privacy. I believe it is not private and give my reasons through the Alice Bob example. I understand this is not the first paper to do it under data-dependent privacy and I have made it clear that I strongly disagree with all the other works especially Papernot et al. (II) The authors responded by citing the monograph by Dwork and Roth, which clearly shows that they do not understand the work they quoted and the issue I raised here. In the entire monograph, the differential privacy is based on the worst-case dataset (data-independent). The authors claim that \"data-dependent analysis is an improvement over propose-test-release\". The propose-test-release is an algorithm. The data-dependent privacy guarantee is a weak privacy notion. The data-dependent analysis referred to as data-dependent accuracy bound in this book is about utility analysis, not privacy. I do understand how privacy notion can be an improvement on an algorithm. Most importantly, this paragraph by Dwork and Roth is saying, you can improve your utility with local sensitivity because privacy has to be for the worst-case dataset, but the utility is only for the good dataset. Typically, to propose a private algorithm, you first prove the privacy for any dataset without any assumptions on the data and then prove the utility for some good datasets like Gaussian data. (This monograph explicitly used the word data-dependent **Accuracy** bounds not privacy bounds)  (II) The authors claim that they can fix it by controlling different numbers of queries. However, the authors did not formally prove it. More specifically, in the authors' data-dependent analysis, each query (each sample in the public unlabeled dataset) has a different privacy cost.  I do not see how Alice could determine the number of queries before running it. Now suppose the privacy budget is 10, and Alice has reached 9.9. when Alice found that her last query has privacy cost 0.2 and exceeds the overall privacy budget, she cannot hide it from the public anymore. Thus, the overall process would break the eps=10 privacy promise. I believe this is fixable since there is some new work on private hyper-parameter tuning. But like I said, the main point here is you have to prove it satisfies (data-independent)-DP. (III) My main concern is about the released data being non-private not the epsilon. (IV) The comparison with Bayesian DP is not helpful here. As long as you privacy depends on the dataset, the same problem arises here for both notions. That is why all the other privacy notions are all data-independent. \n\n2)The main contribution is to generalize PATE to multi-label. I think this is a valid problem setting. But the novelty is not enough for being a separate paper considering the fact that the three proposed methods are either minor modifications of prior works or simple generalizations of PATE. The tau mechanism is just a simple l2 generalization of Zhu et al 2019.\n\nUpdate: You are missing the point. Like I said previously, there are many ways to make a \"data-dependent mechanism\" be provably data-independently differentially private. As long as you can prove your end-to-end algorithm is eps-10 differentially private for any input dataset, then you can make the promise. But it is not here in the current version. For example, you could spend some budget checking the dataset first. If it is good, run your mechanism. If it is bad, output fail. And this is exactly propose-test-release. The methods I mentioned below are all end-to-end data-independently differentially private. You could run your algorithm on all the possible input dataset S and find the largest one, i.e. $\\max_S \\varepsilon(S)$. This would also satisfy data-independent DP. The point is you have to prove that your algorithm is data-independently DP. Alice as a privacy engineer cannot make any assumptions about the nature that generates the dataset. There is a nice blog post https://differentialprivacy.org/average-case-dp/. It has a discussion about the pitfalls of Bayesian DP, which is (in my opinion) similar to data-dependent DP (data-dependent DP: $\\varepsilon(S)$, Bayesian DP: $E_{S\\sim P}[\\varepsilon(S)])$, data-independent DP: $\\max_S\\varepsilon(S)$).  I believe the proposed algorithm in this paper can be proved to be data-independently private even without any change to the algorithm itself. You might want to prove the data-independent RDP of Confident GNMax. (Theorem B.2 is data-dependent). Another caveat I forgot to mention is about comparisons with other works. The gap between data-dependent DP and data-independent DP can be huge (also reported in this paper). It is really unfair to compare with data-independent DP methods like DP-SGD.\n\nUpdate regarding DP and data-dependent DP: This is already **irrelevant** to the contributions of this paper and the proposed algorithm since the proposed method can also be analyzed by traditional (data-independent) DP. First of all, I never claimed that the proposed algorithm is not differentially private(or completely vulnerable against membership inference attacks). And I never said data-dependent DP is not formal. My main concern is about the definition of privacy and the wording. My critique of data-dependent DP is based on the fact that this notion is a function of the dataset. Now suppose we are the nature that generates the dataset. Alice is the privacy engineer who runs some private algorithm and releases the output to the public. Bob is the adversary who tries to attack the sensitive information in the dataset with the released information. Differential privacy is a promise made by Alice to defend against attacks such as membership inference attack. Now when Alice claims that her algorithm is eps=10-DP. We would expect that the output to be eps=10-DP differentially private with respect to whatever the dataset we give to Alice. If not, the next time, when we generate a \"bad\" dataset such that it is not eps=10-DP, Bob can perform a membership inference attack with higher accuracy than promised by Alice. Stability-based methods/propose-test-release/smooth-sensitivity-based methods are all adaptive to the dataset by using local sensitivity but satisfy promised privacy budget for whatever input dataset. Different variations of differential privacy like RDP/CDP/zCDP are all with respect to any input datasets(data-independent). However, if you calculate and output an epsilon of your algorithm for a particular dataset (even if it is private with respect to the dataset), you get different epsilons for different datasets (even if you assume the same number of queries). Then how can you make that promise? This is why I believe data-dependent privacy notions are useless. You could and should get a tight and data-independent epsilon by exactly the same algorithm (Confident GNMax). Listing previous papers do not make things correct. Please answer my doubts directly. \n\nAfter the authors' response: 1) I understand this is not the main subject of this paper.  I still think data-dependent privacy is useless. Theoretically, you could come up with a mechanism and a dataset such that data-dependent epsilon is around 1 and data-independent epsilon is 1000, which is practically useless in defending say membership inference attack. I know this may raise some controversy since there are some prior works that compute some data-dependent epsilon, but I would like to emphasize that the reason why differential privacy is used as a gold standard for private data analysis is that it is a worst-case definition. 2) Overall, I think for this specific problem, this paper might be doing the correct thing with the correct analysis. But the technical contributions are not enough. \n\nStrengths: \n1. this problem is an extension of PATE to a multi-label setting, which is practically interesting. \n2. The authors compare three mechanisms empirically. The empirical analysis is solid.\n\nWeaknesses: \n1. Although the considered problem is practically useful, the proposition 3.1 and appendix A are not theoretically interesting. Basically it says there exists example such that the sensitivity is achieved. (Any private algorithm should have tight analysis of sensitivity) The authors claim that this shows binary mechanism is optimal. However, even for single winner setting, we do not know if Report noisy Argmax is optimal or not. (You could potentially improve it by stability-based methods/ propose-test-release, depending on the problems.)\n\n2. It would be better if this paper contains a preliminary section for \"data-dependent privacy bounds\". In general, differential privacy should not be data dependent. So I am a little bit confused when I first read this concept. If I understand this correctly, I think this is related to local sensitivity/ or stability based methods (see Section~3 of [1]). For example, when the top candidate and the second candidate has large gap (high consensus), the local sensitivity can be small. You could potentially improve it by stability-based methods. But the end-to-end algorithm must satisfy data-independent differential privacy.\n\n3. The emprical comparisons with DP-SGD is not fair. DP-SGD algorithms do not assume a public unlabeled dataset. I understand this comparison exists in prior works. It would be better if the authors could acknowledge it. \n\n4. The discussion about related work Zhu et al seems confusing. As far as I know, Zhu et al (2020) is also model agnostic. The only thing different from original PATE is that they replace teacher model with k nearest neighbor classifiers. Unlike DP-SGD, the privacy mechanism does not scale with the architecture of teacher models. Also the improvement of Tau mechanism is from l2 clipping, which is straightforward. As Zhu et al pointed out in their appendix, they do not use stability-based methods( or they called noisy screening/ Confident GNMax) because it is hard for a data sample to have high consensus for every label. I am just curious how this is resolved in this paper.\n\nSome minor comments:\n1. Is there a typo in equation (2)? I am a little bit confused about the brackets.\n\n[1] Vadhan, Salil. \"The complexity of differential privacy.\" Tutorials on the Foundations of Cryptography. Springer, Cham, 2017. 347-450.",
            "summary_of_the_review": "Although this is a good practical problem, I think this paper needs some revision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}