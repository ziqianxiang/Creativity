{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents an explanation-based learning approach that learns from both observations (examples) and explanations paired with examples. It proposes to learn an interpreter that can map from natural language sentences to examples. The authors also develop an evaluation environment and protocols for the tasks.\n\nStrengths:\n- The proposed idea is intuitive and seems general\n- The benchmark dataset is useful resource for future research\n\nWeakness:\n- The motivation of the present problem setup needs more justification\n- The close connection to the line of work on explanation-based learning (especially recent ones in modeling natural language explanations) are not thoroughly discussed and compared.\n- Experiments beyond the game-like datasets will help validate the claims better and justifies that the problem setup has real-world applications"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors introduce a novel learning framework that revolves around learning a map between concepts and their description, where the latter are expressed in a language unknown to the learner.  Two learning problems are defined: (i) learning a new concept from a description of that concepts as well as examples and descriptions of other concepts, (ii) learn a new concept from examples of that concept as well as examples and descriptions of other concepts.  The underlying assumption is that the map (interpreter) from sentences to sets of examples is shared.  The authors propose an environment to evaluate these tasks and a neural architecture to tackle them.  ",
            "main_review": "- The paper is a pleasure to read.\n\n\n- The choice of illustrative examples is excellent.\n\n\n- The contribution is conceptual in nature, which is good.  Alas, the motivation for pursuing this research direction is unclear.  Specifically:\n\n1) It is not clear why the machine should estimate *both* the link between sentences and data and between sentences and sentences *jointly*.\n\n2) It is not clear why the lanugage would be unknown and unstructured.\n\n3) It is not clear what (new?) real-world applications this setup is meant to capture.\n\n(Please note that this setup does not solve the \"semantic gap\" problem, because the semantics of the sentences are implicitly supplied by whoever designs the data set.)\n\nI would advise the authors to ground their motivation in concrete conceptual problems and/or applications.\n\nGiven that the learning framework is *the* key contribution of the paper (the value of the two other major contributions depends on whether the learning setting makes sense), the fact that motivation is lacking/unclear is a serious concern.\n\nThis makes it hard to evaluate the significance of the paper.\n\n\n- The terminology used in the paper feels somewhat misleading/inappropriate.\n\nAs far as I can see, \"phenomena\" are simply *concepts* and the \"explanations\" are intensional *descriptions* thereof, except in a language unknown to the machine (but known to the annotator).  I do not understand what is gained by using this terminology.  Indeed, I find the latter confusing and I expect other readers to be confused by it too.\n\nIn particular, I do not understand the link between \"description\" and \"explanation\", especially considering the causal connotations of the latter term (which are becoming more and more clear as work on causality is being merged into AI and ML.)\n\n\n- The work seems to rely on a rather strict assumption.  In particular, the fact that D_0 should be discriminative for P_0 in L is (as far as I can see) unlikely to hold in practice -- especially considering that D_0 is supposed to contain a \"small set of observations\" -- and it simplifies the learning problem considerably.  In what applications is it reasonable for this asumption to hold?  Is it necessary?  What happens if it doesn't hold in practice?  How costly is it to acquire a D_0 that explicitly satisfies this assumption?  Given that such a D_0 would not be IID, how would this impact statistical learning of P_0?  These issues are touched upon in the conclusions, but they deserves an actual discussion.\n\n\n- The proposed architecture is reasonable but surprisingly involved and the details are hidden in the appendix.  It would be more straightforward to explain in detail the various pieces that make up the architecture (how many, what they take as input and what they spit out) from the get-go, rather than relying on Figure 3, which lacks mathematical precision.\n\n\n- Inferring whether an instance x satisfies a concept P is surprisingly involved as it requires to generate a (presumably large) number of candidate descriptions for P and then counting, for each description, whether x satisfies it.  Presumably this scales poorly with language complexity.  Is this efficient at all?\n\n\n- The experiments are limited to an interesting but entirely synthetic (actually, quite toy) setting.  For instance, as far as I can tell no sub-symbolic inputs are present.  Experiments with real data would have been useful to evaluate the efficacy of the proposed pipeline.  CRNs are compared only against two simpler baselines.  I realize that the focus of the paper is in its conceputal contribution, but a more varied selection of experiments would have been welcome.\n\n\nMinor issues\n------------\n\n- Wouldn't it be more natural to define the interpreter as a map from descriptions to classifiers (indicator functions)?\n\n- It would be good to disambiguate the term \"explanatory learning\" from previous uses of the same term, see \"Explanatory interactive machine learning\" AIES 2019.\n\n- The \"communication problem\" shares some aspects with multitask learning, especially so if the language is compositional and the tasks form a hierarchically (or can be related logically to each other, e.g., P0 is the negation of P1 etc.).  It also shares aspects with few-shot learning.  It may be worth highlighting the connection.",
            "summary_of_the_review": "Potentially great paper with unclear motivation/applications",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes approaching learning a language as a learning problem that is grammar, alphabet, etc. agnostic. Such formulation has resulted in the so called Explanatory Learning (EL) framework that is paired up with an environment to test it. The starting point for learning a language is to extract an interpreter based on a given set of observation and their explanation and then use the interpreter to determine whether an observation belongs to the language, in a binary classification setting. ",
            "main_review": "Questions: \n- In Figure 1, why the first left sequence is correct according to the rule? \n- When tagging unseen structures based on the secret rule discovered by the user, how many unseen structures are required to completely rule out other rules that are marginally different from the secret one. Is this number always 1176 in the current setting? \n- Since some figures don’t have caption, it’s hard to refer to them… In the figure on the lest side of “Metrics.” How the predicted vector is derived based on various rules? \n- As the authors point out the goal here is analogous to that of IPL. Do I understand this correctly that the main advantage of EL to IPL is skipping the translation of the data to logic? If that’s the case, one can learn the translation itself and eliminate the difficulty. Can you elaborate on the advantage of EL over IPL?\n- Can’t T-Acc be changes to something auc based instead that takes care of the permissively problem? \n\n\nOther remarks: \n- This is a language specific problem, where order of the sequence does not necessarily matters. The current title does not  reflect this. Perhaps something like Beyond empiricism in neural language learning or something alike would be a better suit. \n- Some approaches to explanatory AI that aim at generating explanations as well as predictions such as “TED: Teaching AI to Explain its Decisions” are missed here and should be commended on. ",
            "summary_of_the_review": "- Overall, I quite like the idea of the paper due to the generality of the approach, namely learning an interpreter from observation. \n\n- “proving” the discovered rule is indeed the secret one by examining it in practice (i.e., being used to tag unseen structure) as opposed to revealing the rule directly in text is also a very neat idea.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new framework for studying explanation driven machine learning problems called Explanatory Learning. The goal is to learn an interpreter model from explanations paired with observations for a particular phenomenon. The explanations might be in an unknown language, but the explanations paired with observations can be used to learn a good interpreter. Once learnt, the interpreter should be able to follow new explanation for an unseen phenomenon. They refer to this problem as the communication problem. They also define the scientist problem where explanations for the unseen phenomenon is not available. The paper also introduces an Odeen dataset to facilitate experiments with Explanatory Learning. The authors propose a neural network architecture as a solution to the scientist problem. It has two components: a conjecture generator which generates a set of candidate English explanations, and a second interpreter model. Experimental results show better generalization compared to end to end neural systems.  ",
            "main_review": "Strengths: 1. The proposed framework forces us to think about explanations and generalization as first class citizens. \n\nWeaknesses: \n1. The idea sounds very familiar to explanation based learning. It will be good to contrast the two.\n2. My main criticism is that I am not sure we need an entirely new framework for targeting explanation based solutions. Many recent work in NLP try to frame problems in English to get cross task generalization. I would prefer this paper to be positioned as an improvement to such existing approaches, as this is tackling a harder problem class. \n3. The experimental results are all on the new game like dataset. It will be good to see performance of proposed methods on real datasets, or already existing synthetic datasets. The baselines are also weak, so its not clear if the proposed techniques will perform better than other cross task generalization methods.  ",
            "summary_of_the_review": "Although I am all for explanation based learning solutions, I don't think the components introduced in this paper warrants introduction of a new learning framework. I think it will be better to place it as an improvement to existing approaches for cross task generalization. I also found the experimental results to be weak. It was not clear if the proposed techniques will perform better than other cross task generalization methods.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of classification learning where each data point is accompanied by some explanation. The explanation is in some arbitrary language. The paper proposes a way to increase the performance of the classification task by learning to predict the explanation associated with a given data point, and then using that explanation along with the data point to proceed to make a prediction about the data point's label. The paper implements this pipeline using neural networks, and presents empirical results on a new benchmark dataset to demonstrate its performance.  \n",
            "main_review": "Strengths:\n\n- Acknowledges the bilateral nature of explanations in building explainable AI/ML.\n- Offers a new benchmark problem.\n\nWeaknesses:\n\n- Does not fully explicate the assumptions that it is making in terms of the explanations.\n- Does not properly position itself with respect to relevant work in the literature.\n- The somewhat over-the-top philosophical discussion distracts from the essential point.\n\nGeneral remarks:\n\nThe problem acknowledges the need to have bilateral explainability; i.e., build machines that can explain, by first explaining *to* the machines. This is discussed, for example, by Michael, \"Machine Coaching\", IJCAI Workshop on XAI 2019, which seems to be rather relevant to the current paper, especially given that the paper makes an effort to connect to learning theory. Another line of work that seems to be relevant is Mozina et al., “Argument Based Machine Learning”, AIJ 2007, where data points are accompanied by an argument (i.e., an explanation) on why they are labeled as they are, as is the case in the current paper. Ignoring the obvious difference from the current work that these two works assume that explanations are in logic, the underlying theme of the cited papers and the current paper seems considerably close to ignore.  \n\nI found the philosophical positioning of the paper as a study in empiricism vs rationalism to be somewhat far-fetched. I believe it is instructive to take a step back and seek what are the underlying assumptions of the paper, and whether those bring something new to the table. \n\nIt is clear that the label of a data point is a function of the data point and the phenomenon, and not only of the data point. And since the phenomenon is essentially associated with an explanation, then the label is a function of the data point and the explanation. What the authors propose, then, as the pipeline for predicting the label is a form of chaining of learned pieces of knowledge, where one predicts the explanation, and then using that previous prediction one proceeds to predict the label at a second cycle of inferencing. Of relevance here is the work by Valiant, \"Robust Logics\", AIJ 2000, and Michael, \"Simultaneous Learning and Prediction\", KR 2014, as well as some follow-up papers by the authors, that establish the benefits of chaining.\n\nFrom a formal point of view, there is nothing in the explanations that makes them explanations. As the paper says, they are simply strings. Is it really the case that they are arbitrary strings, or should they be strings that are learnable as a function of the data points (or more properly, sets of data points)? That is to say, if each type of explanation for a phenomenon P_j was simply replaced by the number j, would then this still be considered an arbitrary language of explanations, despite having no structure and not being learnable? If indeed, explanations cannot be arbitrary strings, then one needs to carefully state what are the underlying assumptions on the language of explanations. If, on the other hand, explanations can be arbitrary (e.g., explanation j for phenomenon P_j) then this makes the term \"explanation\" rather mood; it is simply another signal in the data (at a meta-level, see my comment below). \n\nAnother aspect of the paper is the two levels of learning problems that exist, as mentioned in the last two paragraphs: the object-level learning problem of learning from a data point and its explanation the label of the data point; and the meta-level learning problem of learning from sets of data points to predict the phenomenon/explanation. Both of these two problems seem to follow the empiricist view, in the sense of the paper, which, as I said above, makes it unclear what the philosophical discussion on empiricism vs rationalism really add to the picture. The conjecture generator seems to be simply a meta-level classifier (albeit a stochastic one). Which brings me back again to the question on whether the explanations need to have some learnable structure.\n\nAdditional points:\n\nThe metric of NRS seems to include in its definition the identification of a nearest neighbor. Why? Shouldn't the goal be to predict the actual explanation? If the algorithm that makes the prediction wishes to use the nearest neighbor to reach that decision, then that would be fine. But the use of the nearest neighbor seems more natural to be part of the algorithm that attempts to solve the problem, not part of the evaluation metric for measuring success.\n\nThe proposed approach to solving the problem seems not to be accompanied by any formal guarantees on its performance. This would typically be compensated by an extensive experimental section, which is not the case here. \n\nI would have found a different narrative for this paper to be more convincing and impactful: the introduction of Odeen as a benchmark problem, and a deeper discussion of its features and parameters, and then present the particular approach, properly placed in the context of relevant work, as a suggested direction for what type of systems would presumably be useful in tackling the Odeen benchmark. \n\n------- After the Author Rebuttal -------\n\nI acknowledge that the authors have made an effort to engage with the points that I raised, and I have increased my score. I believe that the connections with learning theory are much more deep than the brief remarks offered in the revised version of the paper, and I hope that the authors will consider exploring them further in their future work as a formal underpinning of their empirical work. \n",
            "summary_of_the_review": "I find Odeen to be a useful contribution, and one that would raise awareness on the need of certain underused techniques in the machine learning literature. But, the rest of the paper needs to be more clearly and properly placed in the context of existing work in the literature.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}