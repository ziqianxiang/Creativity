{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper investigates the stability-plasticity dilemma in the class incremental learning context. It investigates which model components are eligible to be “reused, added, fixed, or updated” to achieve a good balance. Initially the paper had one supporter (xDnv) who liked the motivation and extensiveness of the ablation. NFc9 and UadE also echoed some similar points about motivation and liked that the work was easy to follow. Reviewers expressed concerns such as incrementality w.r.t. spaceNet, too many hyper-parameters, unclear performance benefit, lack of comparison to SOTA, fixed sequence of classes specified by the authors, not clear how much forgetting is happening in each method (echoed by multiple reviewers), and limited datasets used for evaluations. The authors responded to the critical reviews and provided a revised version of the paper with additional comparisons to rehearsal-free methods and with more datasets (MNIST/FashionMNIST). \n\nFollowing the author response, NFc9 stated that they thought the paper was in better shape with the revisions and upgraded their score claiming it was “closer to acceptance”. Yet, they still had concerns with the practical implications of having too many hyper-parameters. UadE engaged further with the authors but claimed that they avoided the reviewer’s direct concerns. UadE maintained their concerns with the manual ordering of classes and older baselines. I agree that there are several rehearsal or pseudo-rehearsal methods to which they could have compared. Reviewer o6Js did not engage further. Overall this is a borderline paper. I appreciate the authors engaging in the discussion period, though my assessment is that the key issues still remain. This paper could use further development so my recommendation is reject."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work aims at addressing the stability-plasiticity dilemma in the class incremental learning scenario by exploring which model components shold be reused, added, fixed or updated. Authors proposed to make use of existing knowledge in previous tasks, including identifying and adding similar knowledge that could be reusable for forward transfer and preventing dissimilar knowledge from being transferred to avoid forgetting, hence achieving better balance between catastrophic forgetting and forward transfer. Extensive ablation studies are conducted to demonstrate the effectiveness of the proposed method.",
            "main_review": "Pros:\n+ This work is well motivated. Exploring similarity between tasks could be indeed a promising way to address the stability-plasticity dilemma to simultaneously promote forward transfer and avoid catastrophic forgetting. \n+ Extensive ablation studies are conducted to show the proposed method work to some extent.\n\nCons:\n- The organization of method section could be improved.   \nFor example, the first and second steps could be divided into two separate paragraphs, and selective gradient back-propagation could become another paragraph.   \nIn addition, it is better to summarize the categorization of neurons in a layer.\n- There is no explanation on why activation of a neuron could be a good indicator for relevance between classes. What else do authors also attempt?\n- It is suggested to calculate the statistics about how much portion of neurons are reserved, reusable and free for switching between similar tasks, and how much for the switch between dissimilar ones.\n- The proposed method also shares some similarity with PackNet although the PackNet does not explore similarity between tasks but tries to compress the used neurons for each new task. Authors should compare the proposed method with it to see which method is more effective in address the stability-plasticity dilemma.\n[a] A. Mallya and S. Lazebnik, PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning, CVPR 2018.\n- More rehearsal-free CL methods should be compared with. In addition, comparison on larger datasets such as tinyImageNet and also the one across several datasets such as Oxford Flowers, Caltech-UCSD Birds and MIT Scenes should be conducted.",
            "summary_of_the_review": "Good motivation and extensive ablation study. But the method should be described in more details with more insightful explanation in a better organizing way. In addition, it should be compared with more rehearsal-free methods on larger datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "### Summary:\nThe paper studies replay-free continual learning with the focus on the plasticity-stability dilemma.\nMore specifically, the paper proposes the KAN method for class-incremental learning where the forward transfer is achieved by detecting similar knowledge and reusing the first few layers,\nand negative backward transfer can be alleviated sparse connection allocations for different classes of different tasks.\nThe paper also studies a limitation of the softmax layer, which is an interesting contribution.  ",
            "main_review": "### Strengths:\n(1) The paper is well-motivated, well-written, and easy to follow.   \n(2) I like the overall experimental designs where different aspects of continual learning with KAN are studied with several experiments.\n\n\n\n### Weaknesses:\n\nAlthough I enjoyed the experimental design, and agree with the intuition behind KAN, the proposed method have major weaknesses:\n\n(1) First, there are several hyper-parameters involved, such as the number of layers to be reused, the sparsity level, etc. \nWhile the paper studies the impact of layer reuse, this is usually an unknown factor in practice for new and unseen data.\n\n(2) Second, given all the interesting effort for developing KAN, the proposed method does not outperform existing baselines.\nFor instance, when the benchmark is challenging, such as Fig 4. b, KAN, SpaceNet, and even Random Reuse achieve the same performance.\nI believe since the paper is an empirical work, the fact that the proposed method is not performing better than baselines hurts its contribution. \n\n(3) Generally, while reporting metrics, it is not clear how much average forgetting for different methods are (see [1, 2] for the definition of forgetting metric). I think it is important to report these metrics as well, which indicates the negative backward transfer.\n\n\nMisc: \n- In Fig. 1, a fixed connection from the last neuron of the first layer is removed from steps 2 to 3. \nI think there was an error in figure generation. \n- It will be interesting to include the run-time of different algorithms to see how much different modifications by KAN impact the speed. \n\n[1] Chaudhry, Arslan, et al. “Efficient Lifelong Learning with A-GEM.” International Conference on Learning Representations, 2018.   \n[2] Mirzadeh, Seyed Iman, et al. “Understanding the Role of Training Regimes in Continual Learning.” Advances in Neural Information Processing Systems, vol. 33, 2020, pp. 7308–7320.\n\n\n\n-----\n***UPDATE**   \nWhile I am not fully convinced regarding (1), the new version of paper addresses (2) and (3) an I now lean towards accepting this work. \n\n\n ",
            "summary_of_the_review": "I believe the proposed method has its flaws but overall I believe it is an illustrating study and I lean towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposed a class incremental learning method that can exploit the similarity of classes for knowledge transfer. The previous work CAT did this only in the task incremental learning setting. The paper did some analysis in the experiment section to show how the transfer can happen in some limited experiments. ",
            "main_review": "The paper proposed a class incremental learning method that can exploit the similarity of classes for knowledge transfer. The previous work CAT did this only in the task incremental learning setting. \n\nThe paper started off quite interesting as the proposed idea is new. Limited work has been done to improve class incremental learning by transferring knowledge from previous similar classes. However, the work is incomplete as it assumes that the class similarity is given instead of being detected automatically like the CAT system. This significantly reduces the value of this work. \n\nThe experiments section has many issues. It is more like an analysis rather than showing the superiority of the proposed method. The author acknowledged this in Section 5.6. \n\n1. Comparison with state-of-the-art class incremental learning systems is seriously lacking. Many such systems have been published recently. Practically, only SI and SpaceNet are compared, but SI is fairly old and your system is based on SpaceNet. Although existing methods do not explicitly transfer knowledge in the class-IL setting, but they do have implicit sharing and transfer because the new tasks are learned based on the model built for old tasks. Thus, a comparison with them should be included. \n\n2. Only a fixed sequence of classes and tasks arranged by you is used. It raises the question of how you did the arrangement and what about the random sequence or simply follow the original class order in the original data. For CIFAR 100, you used only 16 classes to make 8 tasks. Why did not you use all classes in the full data to make a sequence of 50 tasks. In section 5.4, you said “every two consecutive tasks are dissimilar”. Did you arrange the order of classes to make this happen? You reported a study in figure 2 about the sensitivity of class order, but that involves only two tasks in the task-IL setting with task identity known. \n\n3. In Section 5.2.1, you wrote “to estimate the forward transfer, we focus on the performance of Task 2 assuming the availability of the task identity at inference. Table 1 shows the accuracy of Task 2 using different methods.” Since you are working on class-IL, your assumption that the task identity is known at inference, which is the task-IL case, makes the result not reflecting the of class-IL knowledge transfer. In fact, knowledge transfer of similar classes may make the classification of the old and new classes more difficult. The question is whether this transfer in the task-IL case is beneficial to Class-IL. Also, in the results, you only reported the result of task 2. What about task 1? \n\n4. In section 5.2.2, you reported the same experiment in the class-IL setting and showed that it does not work for class-IL and proposed two constraints. This makes the writing problematic because the constraints should have been given somewhere in the model sections as it should be part of the technique. Using only two tasks to study this is not interesting because when a large number of tasks are learned, it is very hard to know what happens. \n\n5. Figure 4 shows that the forgetting is very serious. Many existing methods do better. \n\n6. Section 5.2.1 and 5.2.2 are quite hard to follow as it has too many details. \n\n7. The architecture is also very simple. Most of continual learning systems for class-IL uses ResNet. \n",
            "summary_of_the_review": "This paper studies an interesting problem of knowledge transfer in the class-IL setting. Existing work only did it in the task-IL setting. However, this work is incomplete as it assumes that class similarities are known, which significantly decreases the value of this work at this stage. There are also a lot of issues in the experimental section. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper address the the problem of continual learning for image classification. This approach focuses on how to reuse, add, fix or update neurons to learn tasks incrementally. The method is based on an existing method name SpaceNET.",
            "main_review": "Strengths:\n- I really appreciated that the paper explicitly tackles the elementary questions of CL: \"we investigate the stability-plasticity dilemma\nto determine which model components are eligible to be reused, added, fixed, or updated to achieve this balance\". Most existing methods consider a fix-size backbone and introduce new regularization or training mechanisms to avoid forgetting but do not explicitly address these problems.\nWeaknesses: \n- First, the method is very incremental with respect to spaceNet. In my understanding, the main difference with this previous approach lies in the lines 3 and 4 of Alg.1.\n-The contribution of the paper and the difference with spaceNet should be clarified. This is discussed only in 3.2 while this should be discussed from the start in Sec.1.\n-In the introduction, the second contribution is not specific to this paper. It was already in SpaceNEt\n-The authors claim that they address only the class incremental learning problem. However, I don't see anything in the method that prevents the use of this method in the case of task-incremental learning. It would strengthen the experimental conclusion significantly.\n-Many baselines should be added to the experiments: DEN,EWC,LWF,MAD\n- The datasets used in the experiments are simple and very similar: cifar 10 or 100. I would recommend adding at least another dataset (Mnist,svhn, tinyimagenet...)\n-The structure of the paper could be improved:\n- Sec 3.3 should not be here. This is not part of the method. This is only the description of a baseline so I would recommend moving this part to the experimental section.\n-Sec 3.2. For a reader that does not know SpaceNet, it's almost impossible to understand.\nin Table1, the performances of KAN  are much worse than the other methods but it uses much fewer parameters. In my understanding, it would be possible to use different hyperparameters to have a more fair comparison allocating a more similar number of parameters? From the reported performances, the superiority of the method is not clear.\n- No readme is provided with the code. So it's hard to replicate the experiments. Comments in the code are very sparse and not very informative.\n\n\ndetail: in figure 1 block (2), a connection is missing in the bottom-left.",
            "summary_of_the_review": "The proposed method is based on an existing method and is very incremental. The presentation of the method could be improved. Experiments are not sufficiently convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}