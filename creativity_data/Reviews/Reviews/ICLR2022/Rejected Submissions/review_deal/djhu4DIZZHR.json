{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a dataset, based on preexisting standardized tests, of elimination/grid-completion-style logical reasoning puzzles expressed in text; available in both Chinese and English (with some of the text coming from semi-automatic translation). The early pretrained MLMs BERT and RoBERTa perform poorly. \n\nThis paper is solidly borderline. Reviewers had some concerns about the motivation and novelty the work, but I think that there is a plausible enough story for where this data will have value that I'm not comfortable rejecting it only on this basis. More worryingly, the initial submission had some fairly serious writing quality and clarity issues, which impacted both the paper *and* the data. It seems like the authors made significant progress on this in the revision and engaged substantially during the discussion, but reviewers were not fully satisfied that the paper was up to ICLR standards, either as an initial submission or after revisions. This is a small detail, but it's a bad sign for the carefulness of the work that the OpenReview abstract is still unreadable, even after a request from a reviewer."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new challenge for large pretrained models (which can of course extend to any other model claiming to store language knowledge): naive logical reasoning. The main contribution of the paper is the construction of a dataset in Chinese and English (termed NAIL) with items for training and evaluating such naive logical reasoning, where the task in the dataset is cast as a standard multi-choice (4-choice in this case) question, with only one correct answer. The main empirical result of the paper consists in showing that pretrained language models, even when trained on naive logical reasoning examples, lag far behind human performance.",
            "main_review": "Naive logical reasoning (NAIL) is quite difficult even for humans: even well trained human experts cannot agree on all the answers (such ambiguous answers were left out from the final dataset), and human non-experts score much lower than experts. Therefore, it is expected that the task will be extremely difficult for language models, and the gap between human and system performance is indeed prominent, and calls for future developments in improving logical ability of large neural models.\n\nAs said, the main contributions of the paper are: 1) creation of the dataset for the NAIL task, 2) showing that some pretrained language models are not as good as humans in solving this task. However, I believe that the paper still does not paint the whole picture and should bring much more detailed analyses on the current gaps, why they happen, and how we can address those gaps.\n\nI am not fully convinced about motivation to have this sort of logical reasoning embedded in the model - these NAIL tasks seem more like 'enigmatic' puzzles, and a stronger motivation is needed to justify why pretrained language models should even need the ability to solve such puzzles: can they be fully trained (with larger datasets) to do well on the puzzles? Another question is, where I expected a deeper discussion: do we need AGI to solve puzzles like the one presented in NAIL? Is it a data problem or truly a human-like reasoning problem? In other words, with even larger models (GPT-3, T5) and more training data, can we expect the models to close this gap to human (non-expert) performance? \n\nOverall, the paper is quite thin on additional insightful experiments and further analyses, going beyond reporting the dataset creation protocol and benchmarking a subset of models on the data, and does not read as a complete piece of work.\n\nWhile a direct link is not apparent at first sight, this work also reminds me very much of the ACL 2020 work of Sahin et al. (https://aclanthology.org/2020.acl-main.115/) - they also showed how pretrained models do lack the skill of iterative reasoning upon knowledge. In other words, supported by a slightly different iterative logical reasoning task than NAIL, they reach exactly the same conclusion. One could read that this lack of ability has been shown in the previous paper, and this work just restates with another similar task, which also diminish novelty of this work a bit.\n\nGiven that imitation examples rely on the same backbone template, and the only variation is actually in the semantics of the examples (i.e., variation is at word-level or 'discourse'-level), I would like to understand more why such imitation examples do help increase the scores: is it only because of a higher semantic coverage, or is it about some spurious artefacts that the model learns by seeing more examples of the same backbone type (e.g., some hidden correlation learned by the model). Would it be possible to create imitation examples automatically and augment training data with such examples? How would the models then behave?\n\nOnly performance of BERT and RoBERTa is attested in the task - I would very much suggest to try out other language models (e.g., T5) and think of possible (auxiliary/intermediate) tasks which could prepare off-the-shelf models to become better at this type of task.",
            "summary_of_the_review": "Similar to some prior work, the paper exposes the inability of (a subset of) pretrained language models (BERT, RoBERTa) to do complex logical reasoning, showcasing that their performance is much lower than what humans score. This is done through the introduction of a new task called naive logical reasoning. Overall, while the NAIL dataset can be quite helpful in guiding future advances on equipping language models with knowledge and methods required to solve such logical 'puzzles', the paper should have done a much better work on the aspects of motivation, discussion, analyses, and higher-level implications of the main results. I feel that more work is needed (see the main review).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work introduces NAIL, a bilingual (English and Chinese) a benchmark for naive logical reasoning, inspired in the kind of questions, involving this aspect contained in standardized exams such as Chinese\nNational Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT).\nTwo different sets are collected in this work. NAIL-E, which are actual examples from both CNCSE and LSAT, translated to English and Chinese respectively, and NAIL-I, a data augmentation approach that uses the structure from existing examples and replaces the three aspects of the problem, subject, predicated and objects in order to create new instances.\nAn qualitative analysis is carried out to reach the conclusion that those three replacings are necessary in order to create new, and more challenging instances.\nThe resulting datasets NAIL-E and NAIL-I are finally splitted into train, dev and test, and examined to test their quality.\nRandom baseline, word match, sliding window, BERT and Roberta (as well as human evaluations carried out by average and expert annotators) are carried out showing the quality of the resulting datasets and the challeding there are for current NLP models.\nExtra experimentation is done to test their transfer learning skills among the full dataset, its split NAIL-E and NAIL-I and using LogiQA, ReClor, RACE as pretriaining steps.\n",
            "main_review": "A really interesting work. A newly introduced and challenging datasets which will be happily received by the scientific community.\nThe paper is well organized and written. All the decisions taken by the authors are properly described and justified.\nMy only concern on a paper of this quality is the lack of experimentation using NAIL as a pre training step for other logical reasoning datasets. If learning the logic of this dataset bring naive reasoning still into a model, that should be reflected into the performance of other logical datasets like LogiQA, ReClor, RACE, but also in CLUTRR, MAthQA, AQuA, HotpotQA among others ...\n",
            "summary_of_the_review": "Please provide a short summary justifying your recommendation of the paper.\nGood paper\nVery useful dataset\nAll reasoning steps in the development are properly justified\nExperimentation is reasonable and accurate\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors manually constructed a dataset that aims to train and evaluate model’s capabilities in naïve logical reasoning. The instances were extracted from Chinese National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT) and those instances were re-written by humans to increase the size of the dataset.\n\n",
            "main_review": "The authors manually constructed a dataset that aims to train and evaluate the model’s capabilities in naïve logical reasoning, which consists of 10,296 instances that were extracted or re-written form Chinese National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT) by humans. \n\nStrengths:\n\nA dataset was built to train and evaluate model’s capabilities in naïve logical reasoning, which consists of 10,296 instances.\n\nWeaknesses:\n\nAll the instances in the dataset were selected or written by humans, and there is no valuable or useful method proposed in the dataset-construction process.\nIt is not clear how they guarantee the quality of translation. There are even many grammatical errors in the examples given in this paper. For example, “Selecting Prince Charming”, “Xiao Li’s ideal gift has following characters”, etc.\nIn my opinion, it seems that the main difficulty of answering the questions in the constructed dataset lies in semantic parsing rather than logical reasoning. If the texts are parsed precisely, the models might achieve high accuracy with an appropriate solving (or searching) technique.\n",
            "summary_of_the_review": "The authors manually constructed a dataset that aims to train and evaluate the model’s capabilities in naïve logical reasoning, which consists of 10,296 instances. The instances were extracted from Chinese National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT) and those instances were re-written by humans to increase the size of the dataset. However, all the instances in the dataset were selected or written by humans, and there is no valuable or useful method proposed in the dataset-construction process. Besides, it is not clear how they guarantee the quality of translation. There are even many grammatical errors in the examples given in this paper. For example, “Selecting Prince Charming”, “Xiao Li’s ideal gift has following characters”, etc. It seems that the main difficulty of answering the questions in the constructed dataset lies in semantic parsing rather than logical reasoning.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new dataset that tests the capability of logical reasoning through multiple-choice reading comprehension. Questions are collected from human exams (Chinese National Civil Servants Examination in Chinese and Law School Admission Test in English) and are augmented by imitating the actual exam examples. The main focus of the questions is understanding of the relationship between subject-predicate-object triples, involving four reasoning types: factuality, ordering, matching, and set operation. In the imitation process, the authors hire people from different backgrounds who are asked to write examples that are logically invariant but semantically different from a given actual example (82 people, 813 paid work hours with fair payment,  and at least 20 imitations per actual example). All collected examples are manually validated by experts and answered by college students for measuring human baseline. Experiments with rule-based (word matching and sliding window) and neural network-based (BERT and RoBERTa) models show that the machine performance (30.10 and 36.15 for the English and Chinese sets respectively) is largely lower than the human baseline (71.31 and 76.38), which indicates that these systems struggle to solve the naive logical reasoning questions.",
            "main_review": "Strengths:\n\n- The proposed dataset is carefully designed and sourced where most of the questions are collected by imitating actual examples in exams. All examples are validated by human experts.\n- The collected questions are difficult for BERT-large and RoBERTa-large which show lower accuracies than the human baseline of college students.\n\nWeaknesses:\n\n- The authors test simple rule-based models and vanilla BERT-large/RoBERTa-large models, failing to imply a future direction for developing systems that perform naive logical reasoning. Ideally, the authors could (modify and) test graph neural network-based systems that are proposed for datasets that require symbolic reasoning (e.g., HotpotQA and DROP).\n- It is a bit obvious that systems pre-trained on ReClor, LogiQA, and RACE do not perform well on NAIL because, as the author mention in Section 4.2, the nature of questions are completely different. Rather, I would like to see it used as an extra training resource for existing multiple-choice datasets including these three datasets. I agree with the authors that the naive logical reasoning needs to be explored as an independent research topic, but I think the authors need to prove that the proposed dataset is useful in some sense in relation to other NLU tasks, datasets, and systems.\n- Likewise, the main claim \"human imitations can significantly help models learn logic from natural text\" is supported only by the experiments on the actual examples contained in NAIL (NAIL-E). The authors prove that the models trained on the imitation examples can perform specific types of reasoning contained in the actual examples, but do not assess their ability of more general logical reasoning such as in ReClor and LogiQA.\n- I'm not sure if the authors can use \"naive\" in the sense that is referred to in the context of Naive Set Theory by Halmos (1970). I guess the notion \"naive\" in Naive Set Theory means that the theory is not described on the top of axioms. Does this imply \"spontaneous, intuitive, and unsystematic\"?\n- There are grammatical errors or typos in the paper. It should be proofread.\n\nQuestions and feedback:\n\n- It is unclear how the authors translate Chinese and English examples. In particular, if it is done after the expert validation, translation errors might affect the human baseline accuracy. Are there any quality controls over the translation?\n- Human baseline (around 70%) looks a bit lower than that of experts (100%). Is there an error analysis about them? Is that by careless mistakes or by being truly difficult for college students?\n- The second paragraph of Section 5 sounds interesting but has only one sentence. Could the authors flesh it out?\n- In the description of the post-checking process, what does \"each of experts has been selected through CNCSE\" mean?\n- Please replace the alias tokens in the Open Review abstract with actual terms.",
            "summary_of_the_review": "The proposed dataset is carefully designed and may be useful for pushing the boundary of assessing the logical reasoning capability of systems through the reading comprehension task, but experiments are insufficient for empirically showing the usefulness of the dataset.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}