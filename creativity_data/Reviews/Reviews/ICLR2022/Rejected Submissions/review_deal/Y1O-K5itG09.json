{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes to use deep ensembles to parameterize a variational Gaussian process posterior, and uses an additional L2 penalty on parameters of the neural networks, and an (MC) NN-GP prior (although the prior is a free design choice). Reviewers appreciated aspects of the paper, finding there to be a minor improvement in uncertainty calibration over regularized deep ensembles, and nice results for the contextual bandit experiments. Ultimately, however, after a healthy and active exchange between reviewers and authors, four out of five reviewers are voting to reject the paper. There is a belief that the paper can be substantially improved from its current form, by carefully accommodating reviewer feedback, but it is not currently at a stage ready for publication.\n\nThere were common themes in the concerns expressed by several reviewers. Many reviewers found the technical contributions incremental. Parametrizing a GP using deep ensembles, or adding L2 regularization, is not itself a major technical contribution, and the variational framework leans heavily on Sun et. al (2019) and work that came before it from Titsias (2009). Similarly, the theoretical contributions were found to be incremental. \n\nThese concerns about the technical contributions may have been counterbalanced if the experimental results had been outstanding or the framing of the paper perceived to be very clear and well justified. However, the experimental results had a mixed reception, with several reviewers noting accuracy was not in fact much better than the simpler regularized deep ensembles, despite some improvements in uncertainty calibration. One reviewer liked the bandit experiments, but wished there was a deeper exploration of this application domain. The current experimental results do not seem to warrant the relative complexity of the approach over simple regularized deep ensembles. \n\nAdditionally, several reviewers found the framing and presentation of the paper needing significant work. The introduction of the L2 regularization terms, for example, was perceived to be overly complex, involving several steps that were not well-motivated.  \n\nSeveral reviewers also found the motivation about making deep ensembles Bayesian unconvincing. A procedure being sensitive to initialization, or unreliable in certain settings, does not mean it does not perform approximate Bayesian inference. For example, variational methods and Laplace approximations can depend on initialization, and could get stuck in poor local optima. Quoting papers referring to deep ensembles as non-Bayesian is also not an argument in itself. The blog post linked by a reviewer is clearly pushing back against these claims, and does address points raised in the discussion, such as unimodal approximations and theoretical guarantees. As reviewers have also noted, several papers have now provided plain deep ensembles with a Bayesian justification, and these papers should be acknowledged. It could be reasonable to argue that your paper makes deep ensembles _more_ Bayesian, and you could potentially try to measure this claim in a concrete way. Or you could simply argue that your approach helps reduce sensitivity to initialization, and represents solutions with lower posterior density, which can be helpful practical contributions and don't need to be tied to claims about the method being Bayesian.\n\nPlease thoughtfully reflect on the reviewer comments in updated versions of the paper. The reviewers put a lot of effort into providing feedback and engaging during the rebuttal period. While the paper has some nice features, there is significant room for improvement on several fronts: technical innovation, experimental investigation, and framing. Improving the framing will help, but working further to also address other concerns will likely be needed to sway reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper develops a deep ensemble-based Gaussian process model and a variational inference procedure to train it. As a result, the authors obtain a method for improved uncertainty and predictions with deep ensembles.",
            "main_review": "## Methodology\n\nThe method is, as far as I understand, the following: \n1. We define an NN-GP Gaussian process prior.\n2. We define a variational Gaussian process family, where the mean and covaraince are parameterized through the deep ensemble.\n3. We introduce a modified ELBO, where (a) we use a tunable weight for the KL, (b) we have an extra regularization term on the weights of the ensemble members and (3) the KL is calculated using the marginal distributions on the train set and additional randomly sampled inputs.\n\nThe authors claim that this is \"theoretically sound\" and \"principled\" \"Bayesian\" method. However, essentially, we are using a variational GP model. If we ignore the $||w_i||_2^2$ term in EQ 13, then the optimal solution of the ELBO optimization problem is recovering the NN-GP model. However, the method seems to work better than the NN-GP in practice, and, if the goal was to approximate NNGP, why wouldn't we just use NNGP? Is the idea that the specific family in which we approximate the posterior of the NN-GP the key component here, and what we want is the closest possible deep ensemble to the NNGP posterior? The motivation isn't fully clear to me.\n\n## Section 3.3\n\nI think section 3.3 is very confusing. Essesntially, the whole purpose of this section is to introduce a regularizer on the norm of the weights of the ensemble components, $||w_i||_2^2$. However the authors introduce it by first talking about constrained VI, than saying that our constraint will be that the accuracy on the train set should be good, then bounding the accuracy on train with the accuracy of ensemble components, and then saying that the accuracy of ensemble components is bounded by the norm of the weights. \n\nI have a few issues with this section:\n1. I think your constrained in the constrained VI should not depend on the data. Restricting your model class to only the functions that get good accuracy on your train set doesn't seem like a valid Bayesian procedure. Accuracy on train should be a part of your likelihood, not constraint on the model class.\n2. I don't think there are bounds on DNN performance that are based just on the $L_2$  norm of the weights.\n3. The regularization on the weights is a simple-enough idea, I think it would be much more clear if you just said that you are adding an extra weight decay term without drawing parallels to constrained VI and generalization bounds.\n\n## Making deep ensembles Bayesian\n\nOne of the goals that the authors repeat multiple times in the paper is \"make deep ensembles a Bayesian method\". In light of [this recent blog-post](https://cims.nyu.edu/~andrewgw/deepensembles/) I would like to ask the authors what exactly constitutes a Bayesian method, and why the proposed procedure is \"more Bayesian\" than the original deep ensembles?\n\nThe recent paper [1] also proposes a way of making deep ensembles \"more Bayesian\", so I think it should be discussed in the paper.\n\n## Experimental results\n\nI think the experimental results are the main strength of the paper. It seems like the method proposed by the authors performs well in a few practical settings, including CIFAR-10 and CIFAR-10-C. However, the performance is only marginally better than regularized deep ensembles on the in-distribution CIFAR-10 benchmarks.\n\nI think the experiments could still be made stronger by including other realistic datasets, perhaps CIFAR-100. \n\n## Questions\n\n- Do you train the ensemble from scratch using Algorithm 1, or do you pretrain the ensemble components first?\n\n\n[1] Repulsive Deep Ensembles are Bayesian\nFrancesco D'Angelo, Vincent Fortuin",
            "summary_of_the_review": "The paper presents a method with promising empirical results. However, the authors frame the main contribution of the paper as making a principled a theoretically sound version of deep ensembles. I do not fully agree with this interpretation given the details of the method. So, I vote for a weak reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a novel perspective on deep ensembles (DE) that aims at providing a Bayesian justification to DE. The proposed approach builds a Gaussian process (GP) using the ensemble members and performs variational inference in the functional space. Moreover, the paper introduces a regularization method that works directly in the function space. The proposed algorithm DE-GP shows better results compared to DE an other well known Bayesian deep learning methods.\n",
            "main_review": "Strengths:\n- The proposed methods is elegant and provides Bayesian justification on top of DE.\n- Extensive experiment evaluation.\n- DE-GP shows good performance compared to DE.\n\nSome questions:\n- Algorithm: How to choose $\\lambda$? \n- Algorithm Line 7: what is U?\n- why is \"the diversity on the points far away from the training data is further fostered\"?\n- why not search over $\\beta$ since it seems to change for different experiments?\n- How sensitive is the performance with the changes of the hyperparameters $\\alpha, \\beta$? \n\nMinor comments:\n- Page 5: Require revising \"given the fact that a voting is incorrect when at least one of the individuals makes a mistake\"\n",
            "summary_of_the_review": "Overall, I find the contributions presented in the paper novel and important to the Bayesian deep learning researchers to know about. Such contributions open the door for more works adapting Bayesian deep learning methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The motivation for using fully Bayesian methods over ensemble methods has been a contentious topic in recent years - while ensemble methods are prized for their relative simplicity, they lack the theoretical framework that grounds fully Bayesian approaches. In this work, the authors propose an interpretation of Deep Ensemble models (DE) under a variational Bayesian framework. In particular, the authors demonstrate how VI can be carried out in the function space, and leverage posterior regularisation on functions to incorporate prior knowledge into the model architecture. The benefits of the proposed approach are verified via an extensive evaluation covering a variety of different problem settings, whereby it appears that the DE-GP consistently yields predictions having superior uncertainty calibration, and without compromising on predictive accuracy.",
            "main_review": "### Strengths\n\n-\tThe paper is very well-structured overall and a pleasure to read. I particularly enjoyed the sections describing the contributions, as well as the brief discussion in Section 3.5 that summarises some of the key properties of the proposed method.\n-\tThe dilemma on whether ensemble methods are just as effective as fully Bayesian methods has been the subject of several papers and discussion panels in recent years - the insights and contributions presented in this paper are thus timely and relevant, and should be of interest to the community. \n-\tThe experiments section is very thorough, and covers a vast set of problems ranging from synthetic examples for visual exposition of the model’s features, regression datasets, and classification problems. The paper also contains an additional experiment on a contextual bandit problem that I particularly appreciated, as it shows how the improved uncertainty estimates returned by this method can be beneficial when used in iterative decision making tasks. The experimental set-ups are well-explained in each case, and I appreciated that the results were all computed from scratch using implementations of existing methods (as opposed to simply copying in results from other papers that could possibly have been computed over slightly different dataset folds and experimental settings).\n\n### Weaknesses\n\n-\tAlthough I previously praised the paper for being well-organised, there are several instances where the writing can be improved. There are quite a few typos and grammatical errors scattered throughout the paper which could easily have been fixed with a proper read-through before submission. The introduction is particularly disappointing and I would recommend rewriting. There are also a few phrases which are ambiguous to a point and I would also reconsider. For example, the reference to ‘graceful BNNs’ in the conclusion is quite odd.\n-\tThe related work section appearing towards the end of the paper does a good job in exhaustively listing the various papers working on areas related to this topic, but offers very little insight into their actual contributions. Perhaps it may be more suitable to place this section earlier in the paper such that any contributions can be more clearly referenced against earlier work?\n-\tWhile interesting, the principal contributions of the paper are fairly incremental, and the authors themselves concede leveraging insights ‘inherited’ from other works as the foundation for some of the key contributions. Even so, this does not dent my overall opinion of the paper.\n-\tThe references need to be properly cleaned up - journal versions and conference proceedings should be cited instead of Arxiv editions of certain papers, while words such as Gaussian need to consistently appear as capitalized in paper titles.\n",
            "summary_of_the_review": "The paper tackles an interesting subject in an intuitive manner, and the extensive experimental evaluation showcases the performance of the model in several different settings. On the downside however, some of the writing needs to be heavily improved, while the connections to related work should also be reconsidered to further highlight the original contributions of this work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose using Deep Ensemble (DE) as basis functions to train a Gaussian Process (GP) with the primary motivation of making DEs more Bayesian. The Evidence Lower BOund (ELBO) being maximized is now optimized via variational inference (VI) over the functional space of bases (i.e. fELBO from [3] Sun et al.) defining the mean and covariance of a GP similar to NN-GP [4]. The paper evaluates the proposed method on standard UCI regression and small-scale MNIST / CIFAR10 image classification tasks in terms of accuracy, likelihood, and uncertainty estimation.",
            "main_review": "## Contributions & Weaknesses\nThe paper shows empirically that DE-GP achieves somewhat better RMSE and accuracy compared to most baselines across a variety of synthetic, regression, and low dimensional vision tasks. However, these gains do not appear significant or well motivated. It is not clear what pain points of deep ensembles \"not being Bayesian'' are being  addressed and what concrete value the DE-GP method brings the other forms of approximate inference against which the method is compared. Like the other Bayesian methods of approximate inference, DEs have empirically shown closer approximations to the predictive posterior distribution, so it is unclear what is meant when seeking \"a more proper Bayesian exposition for DE\", and whether this distinction of DEs being \"non-Bayesian\" is necessary. If the goal is higher fidelity Bayesian inference, this need not be achieved by incorporating a GP. It makes sense to want to improve the generality of approximate models across various tasks, but significance could be better emphasized by specifying an argument against other state of the art methods, either theoretically or empirically, or by highlighting significant advantages / capabilities of the proposed method. \n\nTheoretically, there is little theoretical novelty in setting up the variational inference optimizing framework and defining the mean and covariance of basis functions, as these seem to be trivial adaptations of prior works. In terms of empirical results, there does not seem to be significant differences between NN-GP, DE-GP, and DE in most cases. Figures 1 and 9 are also not super informative in terms of emphasizing differences or giving insight into why or how DE-GP is competent at exploring and averaging uncertainty under a single mode, as noted that ensembles may not always select the best points from each mode. It is also difficult to justify better calibration purely from the visual.\n\n## Correctness & Clarity\nThe paper is well written overall. The proofs and math are clear and in line with prior works (e.g. Theorem 1 of [3] is applied for developing the objective in section 3.2). It could be better specified which parts of the pipeline are original work to emphasise contribution, as it currently only states which parts are borrowed. \n\nIn various parts of the paper, the method is motivated by the fact that DEs \"lack a proper Bayesian justification\". Could this be explained in more specificity? This is confusing and the paper does not clarify how and why certain methods are considered Bayesian or not. It is stated that the objective of the paper is to make DEs more Bayesian, but also noted that doing so via DE-GP may be at the cost of \"trad[ing]-off between theoretical soundness and flexibility in practical usage\". Flexibility should be clarified and justified for the DE-GP, otherwise, it cannot be used as evidence against employing DE in Bayesian inference problems.\n\n## Relation to prior work\nGenerally well-covered relevant works session, cites major works that would be good additional baselines. Methods are developed from past works on basis functions and GPs.\n\n## Additional feedback (comments, suggestions for improvement) \n- Section 3.1 claims that the chosen linear, matrix value kernel is comparable to recent low rank approximations to original kernel matrices. The authors should provide comparisons with methods like Batch Ensemble [1], Rank-1 BNNs [2], etc. to demonstrate this claim. \n- It would be nice to do an ablation to better motivate why VI is necessary and important to consider in Bayesian inference. There should be better characterizations of local approximations including Laplace approximation as well as global behaviours with MCMC, HMC or SGLD. Additionally, negative log likelihood appears better than others, but there is no comparison with NN-GP on CIFAR-10 for example. It would also be good to vary the ensemble size and compare it with NN-GP.\n- As uncertainty quantification is a major focus of most BNN / GP papers, additional methods and baselines should be included in the comparisons: mean field V I (motivating variational inference), AugMix (insights into the effects of data augmentations), ResNet architectures with and without batch normalization (analyze inductive biases contributed by architectures and generally how far off from most performant models).\n- The paper claims that DE-GP endows high complexity in function space. Can you show why this matters? For example, it would be more exciting and interesting to use a mixture of Gaussians as basis and show that you can / cannot achieve non-Gaussian and/or multimodal posteriors.\n- Further uncertainty and robustness analysis can be justified. It could be informative to measure the quality of robustness through additional experiments on tasks that encourage robustness to novel modalities (e.g. textures in vision tasks, corruptions / covariate shapes etc.)\n- Since the GP has a linear kernel, the method is identical to a Bayesian linear layer after a mean-only layernorm stage. Describing this model as an ensemble is misleading as well, since the components are jointly trained in a single objective as opposed to separately.\n\n## Questions for the authors\n1. More emphasis should be placed to detail the problem the paper is hoping to solve. Is it with regards to inference, prior selection, or something else more specifically?\n2. How do the computational costs of training DE-GPs compare to baseline and recent methods? \n3. How is expressivity of the posterior affected by the choice of prior process? Does a \"good\" choice of prior also allow for better tradeoff between cost and uncertainty estimation?\n4. Do you think a choice of non-stationary kernel as prior processes could induce better inductive biases and hence be conducive to stronger OOD detection?\n5. Have you tried experimenting with different ways of doing model aggregation in the ensemble framework?\n\n### References\n[1] https://arxiv.org/abs/2002.06715\n[2] https://arxiv.org/pdf/2005.07186.pdf\n[3] https://arxiv.org/pdf/1903.05779.pdf \n[4] https://arxiv.org/pdf/1711.00165.pdf \n",
            "summary_of_the_review": "Overall this paper demonstrates theoretical soundness and evaluates against standard benchmarks in the space of models with uncertainty estimation. However, it lacks some originality as well as comparisons with additional baselines and ablations to better separate its significance and reason for performing better / worse. There is also no clear indication of the problem being addressed and why being Bayesian matters.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a GP model with a deep kernel defined in terms of a (finite) deep ensemble (DE). A variational approximation and a regularization scheme were introduced to optimize the GP. The proposed methods were then demonstrated on several benchmark regression/classification datasets. \n\nHere, the claimed contributions are (1) a DE-parameterized GP posterior that provides a Bayesian justification for DE; and (b) a regularization scheme that improves the generalization of the trained GP.",
            "main_review": "NOVELTY & SIGNIFICANCE\n\nFirst, I am not sure why this paper is titled deep ensemble as GP posterior while the proposed method instead uses deep ensemble to parameterize the GP posterior. I was under the impression that this paper is about exposing deep ensemble as the natural posterior mean of a GP setup because it aims to provide a Bayesian justification for DE via GP but this is unfortunately not the case -- one can in fact parameterize the GP with any deep network as its kernel but that is not the same as exposing them as inference results of a GP setup, thus providing a Bayesian justification. Given this, the first contribution claimed is kind of moot.\n\nFor the same reason as above, this work also appears incremental to me. It is pretty much a special case of GP with deep kernel but perhaps the point here is the DE is a fine choice for parameterization that can improve performance. As for the second contribution that involves the variational inference and regularization scheme, I have to unfortunately say the same thing that these are also too incremental as they are, as a matter of fact, vanilla inheritance from prior work with little to no adaptation: \n\nThe variational expression is obviously inherited from (Sun, 2019) as the authors mentioned and it is also known even before that by the prior work of Titsias in AISTATS-09 where an ELBO of GP is derived based on a set of inducing points. The regularization is simply a L2 penalty that was well-rationalized by prior work. \n\nGiven the above, I think the key contribution of this work is the empirical investigation of a potential parameterization for a GP with DE, which is fine but marginal given the reported results in Table 1 (i.e. the improvement over DE and rDE is pretty much marginal)\n\nSOUNDNESS\n\nAll derivations appear correct to me.\n\nEXPERIMENT\n\nAs I mentioned above, the results in Table 1 appear positive but are somewhat marginal (in one setting, rDE is still better). In fact, given the marginal differences, maybe it's worth running for more trials to have more stable estimates -- at this point, every model is within the error bar of almost every other model so it is not clear if the performance is really improved. \n\nOn another note, results in the contextual bandit look significant -- perhaps the calibrated uncertainty will show its effect best in an exploit-explore decision making scenario. But I am not sure how the authors set up the uncertainty variance for other non-Bayesian baseline in this experiment? Could the authors clarify this in the rebuttal?\n\nAs a more minor note, using bar plot in Fig 7 is kind of inflating the result differences. The difference between DE-GP and rDE is less than 0.5% and we do not know if this is significant without showing the deviation bar -- if it is similar to what was reported in Table 1 then it is not significant.",
            "summary_of_the_review": "This is an empirical work that aims to demonstrate the uncertainty calibration of DE-GP and its improved performance over a set of DE variants. The results appear somewhat marginal where reported performance of DE-GP is within the error range of other models. More trial runs are necessary to obtain more reliable estimate. Also, I disagree with the claim this work exposes a Bayesian rationalization for DE since this is not what it is doing. Instead, a GP parameterized by DE is introduced and its performance is analyzed empirically.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}