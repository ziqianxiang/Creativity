{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper derives a PAC-Bayes generalization bound for SGD and uses the results to postulate a functional form for the generalization error as a function of the ratio of the learning rate to the batch size. This functional form is then leveraged to develop a kernel function GP hyperparameter optimization.\n\nThe reviewers favorably viewed the novel PAC-Bayes bound, but were not convinced by the subsequent analysis. In particular, the reviewers expressed some skepticism about the soundness and generality of the proposed functional form, and were unconvinced that the method would be useful in practice. As such, I cannot recommend the paper for acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper considers the functional relationship between (time-varying) learning rate & batch size and the generalization error. The contributions are summarized below:\n* Assuming quadratic risk function and the SDE limit, the authors derive a novel theoretical generalization bound when the (learning rate / batch size) ratio monotonically decreases.\n* Inspired by the generalization bound, the authors propose a heuristic functional form of the generalization error w.r.t. the (learning rate / batch size) ratio.\n* The authors conduct experiments on CIFAR10 and CIFAR100 datasets, and show that the heuristic generalization error model can fit the actual generalization curve relatively well.\n* The authors propose a novel kernel function for Bayesian optimization in hyperparameter optimization. The kernel function is inspired by the heuristic generalization error model. Empirical results show that the proposed kernel can match or outperform existing Bayes optimization baselines.\n",
            "main_review": "Strengths:\n* Derives novel generalization bound for time-varying (learning rate / batch size) ratio.\n* Proposes a novel functional form for the generalization error w.r.t. the (learning rate / batch size) ratio, and empirically show that it is empirically realistic for image classification tasks.\n* Proposes a novel model-based hyperparameter optimization method, which outperforms Bayes optimization baselines that search over the uniform range.\n\nWeaknesses:\n* The proposed model-based hyperparameter optimization searches in uniform range --- I find the argument for using this rather than the logarithmic range baselines not convincing. I don’t see how searching in the uniform range is **more theoretically sound** than the logarithmic range. As for practical usefulness, the proposed method can only match the logarithmic-range baselines in final accuracy with slower convergence.\n\nQuestions:\n* To approximate the learning dynamics as an SDE, the learning rate needs to be small. However, in the experiments, the learning rates aren’t necessarily always small throughout training (e.g. 1/(2^i) i=2,..., 12). Also, larger learning rate corresponds to poorly fitted parts of the models (figure 1). This seems to suggest the proposed hyperparameter optimization method won’t work very well for larger learning rate. Does that agree with your experiment results, and any ideas on how to solve this issue?\n* The model treats terms that contain t as constants (e.g. a0, a1, …, a4). This seems to assume the total number of training epochs to be fixed. However, in practice, early stopping is often needed for better generalization. How would the proposed method deal with potential early stopping?\n",
            "summary_of_the_review": "For now, I recommend weak rejection.\n\nI think it is very interesting work that yields novel insights. However the main concerns I have is the limited scope and practical usefulness of the proposed theory / method (seems that it’s limited to a fixed # of epochs and performance isn’t better compared to log-scale Bayes opt methods). I will consider raising my score if my questions are satisfyingly addressed.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper approximates the learning dynamics by an Ornstein-Uhlenbeck process and uses some PAC-Bayes results to obtain a functional form for the generalisation behaviour as a function of the ration of the learning rate to the batch size.  It obtains results for both constant and time-varying parameters.  It then uses these to develop a kernel function for hyper-parameter optimisation using Gaussian Processes modelling.",
            "main_review": "This looked on the face of it like a strong theoretical paper building on some interesting work to solve a hard problem.  The originality seems to be using PAC-Bayes bounds to model the generalisation performance and to extend previous work by He et al. (2019) to time varying parameters.\n\nUnfortunately, the analysis was not very convincing on both of these aspects.  Having obtained a PAC-Bayes bounds the authors show under a reasonable assumption that it is negatively correlated with the ratio of the learning rate to the mini-batch size (a ratio they call k).  They then propose to model this by equation (14).  Given the infinity of functions that are negatively correlated to choose this function with no justification (it certainly didn't come from their PAC-Bayes bound) is puzzling and rather suggests that the PAC-Bayes bound is not really used.  On the time vary-case the authors don't seem to have noticed that in solving the Stochastic Differential Equation they have scaled the time with the learning rate.  Thus technically equation (9) is wrong (A is a function of time and their should be an integral in the exponent), but there whole analysis doesn't make much sense because they haven't really capture the dynamics.  The model completely ignores the fact that the time changes with the learning rate (which is very important when using a fixed number of epochs).  There are a number of other strange assumptions in their model.  For example, Equation (7) suggests that for sufficiently long time and small learning rate \\theta would go to 0, thus minimising the true risk and not the empirical risk.  This is clearly wrong.  Because you have a limited dataset your gradients of  mini-batch have a bias that means they will minimise the empirical risk, Equation (6).\n\nThe reason the authors get away with half models that don't make much sense is because ultimately they are just fitting rather simple empirical data to some function form. (e.g. Equations (15) and (16)).  This is not really a test of any of the analysis being carried out.  The authors start out with what looks like a reasonably principled approach, but then make unjustified assumption and uncontrolled approximations to obtain what they call the functional form.  Another example of this is the use of the mean-valued theorem.  This provides a bound, but that bound could dramatically change the function form.  That is, you can't assume that \\lambda does not depend on n(t).\n\nI am not sure whether this paper really tells us much.  It contains a lot of algebra, but on close examination none of its seems to hold up.  Of course, the authors are trying to solve a very hard question and they should be commended for this, but they need to show that their modelling assumptions make sense.  Throwing away almost everything to come up with some a functional form really isn't informative.",
            "summary_of_the_review": "Superficially this paper looks like a rigorous analysis, but it makes so many dubious assumptions and approximations to ultimately obtaining some rather unconvincing functional forms with enough free parameters to fit almost anything.  This kind of analysis requires far more care, attention to detail and empirical validation than the current paper provides.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors study the dependence of generalization error of trained neural networks on the batch size and the learning rate used in SGD. The primary motivation is to extend the previous works to the case where batch size and learning rate change over the trajectory. The authors present a functional form using PAC-based generalization bounds to model the desired dependence. They conduct extensive experimentation to show that the functional form approximates the generalization error well. In addition, the authors show that a hyperparameter search based on the proposed model outperforms existing hyperparameter optimization libraries like Hyperopt and Optuna.",
            "main_review": "The primary strength of the paper lies in the simplicity of the strategy used in deriving the functional form. The extensive study involving overparametrized neural networks and large-scale vision datasets to show that the functional form approximates the generalization error is quite interesting. The authors also showcase the strength of their proposed model in hyperparameter optimization.\n\n\nQuestions:\n\n1) The covariance matrix $C$ being assumed a constant for all $\\theta$ is a very strong assumption, which is very unlikely to hold true for the overparametrized neural networks in practice. Can the authors somehow loosen the assumption by using some form of average covariance matrix over the trajectory?\n \n2) PP 109-110 in Gardiner, 2004 shows the analytic solution to eq. (4) when $g(\\theta) = A \\theta$. Are the authors assuming the risk as $\\frac{1}{2} \\theta^T A \\theta$ throughout the trajectory starting from some random initialization $\\theta(0)$? That will imply, the authors are only considering a quadratic loss function, which has a single global minimum.\n\nOne way to handle the issues mentioned in (1) and (2) can be the following:\nAssume that after several iterations of SGD, the parameters finally converge close to an attractor local minimum $\\theta^{\\star}$ with hessian $A$. Then, with $\\theta(0)$ starting in a neighborhood around $\\theta^{\\star}$, assuming that the covariance matrix $C$ is nearly a constant in the neighborhood, the authors can get eq. (7) as the analytic solution of the final distribution around the local minimum.\n\n\n3) [Experiments] How were the models initialized in the 60 VGG experiments? Was the initialization the same in all the cases? For each LR / batch size ratio, how does the generalization change across multiple initializations? Does the fitted curve change drastically when one considers multiple initializations for different LR / batch size ratios?\n\n",
            "summary_of_the_review": "Overall, I believe the primary strength of the paper lies in the extensive experimental study to corroborate their claim. The major theorems need some restructuring to remove the confusions that may arise (e.g., see (1) and (2) in my review) from the current state. Hence, my scores are currently on the borderline. I am happy to increase my scores after discussing with the authors and other reviewers in the discussion period.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors study the learning rate over batch size dependence of generalization bounds and try to use them for hyperparameter tuning.\n\nIn section 4, the authors study generalization bounds for convex optimization with the motivation that this helps understand neural networks. In section 5, the authors use the bounds to propose a functional approximation for the generalization based as a function of the n=eta/|S| scaling of the fluctuations around the minimum, including for the possibility that n is time dependent. The generalization error is roughly a sum of integer powers of sqrt(n). In section 6, the authors the authors fit a set of points to the generalization formula they found. In section 7, the authors use this expression to look for optimal hyperparameters and compare them with other hyperparameter tuning methods.",
            "main_review": "\n-> My main concern is that I am afraid the authors analysis only applies to convex optimization. The main reason is that the eigenvalues of the Hessian at the quadratic minimum also depend in the hyperparameter like learning rate (see for example https://arxiv.org/abs/2003.02218).\n\n-> While one might be able to fit the generalization error to this formula, it does not mean that this formula was actually use by the model, because without a no microscopic understanding of the coefficients. \n\n-> The generalization bound seems to depend only on min and max learning rates. This seems rather weak for hyperparameter tuning because there are many possible learning rate functions with same min and max lr and different generalization. Furthermore, the dependence on the batch size is only through eta/S and it has been shown that generalization is more nuanced (see for example https://arxiv.org/abs/1811.03600).\n\n-> For the hyperparameter tuning comparison to be meaningful, I would recommend comparing with a grid search over parameters. Also, as one changes the learning rate/ batch_size the time to convergence might vary.\n\n-> Some experiments use momentum while the theory uses SGD.",
            "summary_of_the_review": "The simplifications of the paper are too big to make any of the claims relevant for real networks. \n\nMain concern is that their functional form only applies to linear regression with SGD.\n\n The functional form that they obtain has very limited predictive power and can be seen as a taylor approximation in sqrt{eta/B}. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}