{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper addresses the problem of offline meta reinforcement learning. The authors build on the FOCAL algorithm, adding intra-task attention and inter-task contrastive representation learning objectives. The resulting FOCAL++ algorithm outperforms several strong baseline, including FOCAL and a theoretical analysis attempting to show that FOCAL++ provably improves on FOCAL is included.\n\nReviewers agreed that the novelty of the proposed approach is limited since attention and contrastive representation learning have been used in the closely related (online) meta-RL setting. At the same time reviewers agreed that the results in the paper and the rebuttal show that FOCAL++ improves on a strong set of baselines.\n\nThe main shared concern was regarding the significance and validity of the theoretical analysis. After considering the rebuttal reviewers voting for both acceptance and rejection were in agreement that there are issues with the theoretical analysis/justification. While we agree with the authors that the algorithmic and experimental part of the paper is strong, we have to base our decision on the state of the whole paper. In the end we decided not to accept the paper because 1) the paper put a significant focus on a theoretical argument the reviewers found problematic and 2) the authors did not modify the paper to sufficiently address these concerns during the available window."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper intends to improve a recent offline meta-RL algorithm (FOCAL) through a modified objective and architecture. The authors propose the use of attention both for up-weighting more informative transition tuples in a batch of adaptation data, as well as within the transition tuples themselves, in order to improve task inference in sparse reward settings. In addition, the authors modify the objective used in FOCAL to form a tighter bound on a \"supervised\" contrastive loss than prior work, though it's not totally clear how meaningful this supervised form of the loss is.",
            "main_review": "Strengths:\n- Description of the method is reasonably easy to follow\n- Interesting experiments on distribution shift and MDP ambiguity\n\nWeaknesses:\n- The scope of the paper is rather narrow (specifically context-based offline meta-RL, mostly focused on improving a single algorithm), which will probably limit impact; consider adding comparisons with [1] or [3]?\n- The technical novelty is limited (essentially, adding attention)\n- It's very difficult to interpret the significance of the main theoretical claim; can the authors justify why L_{sup} is a meaningful lower bound? Where does this loss come from? Unless I'm missing something, the solution of W = 0 would give zero loss, depending on the choice of \\ell\n- The writing gets very difficult to follow in the theoretical sections; variables are not always defined unambiguously and some theoretical statements lack justification/explanation\n\nOther comments:\n- The term \"MDP ambiguity\" doesn't come from [2], but [1] (as far as I know)\n- \"The intuition behind sequence-wise attention is that the attentive context encoder should in principle better capture the correlation in (s, a, s′, r) sequence related to task-specific reward function R(s, a) and transition function P (s′ |s, a), compared to normal MLP layers employed by common context-based RL algorithms.\" MLP layers are already universal function approximators, so it's not clear to me why self-attention is needed to learn a better representation of the tuple. If the tuple. Self-attention seems useful if the sequence is too long to fit into a regular MLP or if the sequence length is variable, but neither of these are the case here, as far as I can tell.\n- It seems to me that the matrix form of the contrastive loss is an implementation detail, rather than a novelty, but maybe I'm missing something...\n- The notation in section 3.2 and 3.3 is a bit confusing, because the same symbol (e.g. bold z) means a matrix in 3.2, but a single vector in 3.3.\n- Without knowing what \\ell is in Eq 9, it's hard to know what the equation means. Currently, it's not clear that this \"supervised contrastive loss\" is a meaningful objective. Assuming that \\ell(0) = 0, doesn't the zero matrix give zero loss?\n- I don't really follow the interpretation of the key vectors as classifiers- wouldn't the query classify whether or not particular keys are of the same class?\n- Definition 3.1: It's unintuitive to me to define the loss for a classifier as the difference between their outputs, as opposed to e.g. KL divergence or cross entropy. Why was this form chosen?\n- Are the queries formed by simple averaging, or the weighted average from self-attention?\n- \"Since we assume no access to the entire task set\" This is strange to me- we're using the task set during training, right?\n- The writing in Sec 3.3 is difficult to follow. In particular, I didn't realize that we were trying to come up with a surrogate objective for the \"supervised loss\" until the middle of page 6. It's also not clear to me that the supervised loss in Eq 9 is a meaningful thing to bound.\n- if the constant is assumed to be zero, I'm not sure definition 3.5 makes sense- doesn't this mean all transitions have reward zero?\n- Section 3.4 says that a proof for Theorem 3.3 is given in the Appendix, but the Appendix lists an \"informal proof\", which is a bit strange\n\n[1] Dorfman & Tamar. Offline Meta-Learning of Exploration. 2020.\n[2] Li et al. Multi-task Batch Reinforcement Learning with Metric Learning. 2020.\n[3] Mitchell et al. Offline Meta-Reinforcement Learning with Advantage Weighting. 2021.",
            "summary_of_the_review": "I have mixed feelings on this paper. Overall, I think the technical novelty and impact of theoretical contributions is potentially limited, but some of the experiments in distribution shift/MDP ambiguity are interesting. Currently I'm not comfortable accepting the paper on account of dubious theoretical contributions and limited scope/technical novelty, but if the authors can convincingly argue why their bound in Theorem 3.1 is meaningful (motivate the supervised version of the loss), add additional baselines to make the paper less specific to just FOCAL/COMRL, and improve the writing in the theoretical sections, I am open to accepting it (but could be convinced otherwise).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors focus on the problem of meta-learning for offline reinforcement learning. To this end, they build \non the FOCAL algorithm by adding to key contributions. First, they add an intra-task attention mechanism. Second, they add\ninter-task contrastive learning. The authors provide a theoretical foundation for their approach and evaluate on control tasks\nin MuJoco. They demonstrate competitive performance over the state of the art.",
            "main_review": "Strengths:\n\n1. The paper focuses on a very important problem in reinforcement learning: meta-learning. The ability to adapt to new tasks and\ntransfer learning is extremely important component of intelligence.\n\n2. The authors firmly ground their approach with mathematical motivation and proofs. \n\n3. The quantitative results on the given Mujoco dataset are competitive and show promise.\n\nWeaknesses:\n\n1. The conceptual novelty is limited. The paper effectivly just takes FOCAL and adds an attention mechanism and a contrastive loss. \n\n2. The approach was evaluated on only one dataset. It is difficult to draw conclusions on the generality of the approach. An evaluation \non another meta-learning environment (such as perhaps Meta-World) would strengthen the case of the paper.",
            "summary_of_the_review": "In general, the authors have proposed a modification to an existing algorithm, FOCAL, which incorporates attention and contrastive learning.\nThey evaluate on only one dataset. In spite of these weaknesses, the approach has strong mathematical foundations, and results on the Mujoco dataset look promising. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This submission tackles the problem of offline meta-RL, where the goal is to learn a policy that can quickly solve new tasks. This policy must be learned on an offline set of (pre-collected) trajectories from multiple tasks, unlike in online meta-RL where data collection and learning are interleaved.\n\nSpecifically, the authors focus on improving a previous method, FOCAL, in 2 aspects:\n\n1. They add an attention mechanism to improve the estimation of the context vector identifying the task. This mechanism attends to both the transitions from one state (and rewards, actions) to the next within a trajectory, and across trajectories within a mini-batch from a single task. The authors show that this mechanism can reduce variance when estimating task context.\n2. They propose to use MoCo (He et al., 2020) to improve and accelerate learning of the task encoder. They include theoretical results showing that MoCo better approximates a supervised objective.\n\nThey conclude with experimental results on point-mass and simple Mujoco tasks, where FOCAL++ compares favorably against FOCAL and contextual variants of meta-RL algorithms.",
            "main_review": "This paper makes a valuable and novel contribution in the sense that offline meta-RL is somewhat under-studied and needs better methods. I also appreciated the focused study on context-based methods, and it seems like the proposed modifications could be widely applicable. However, I have some concerns on experimental and theoretical results.\n\n**Experiments**\n\nThe experimental design leave much to be desired.\n\n1. I do not understand the logic behind \"using sparse-reward data only to learn context encoders\": I assume the policy is learned with dense offline rewards, which seems unrealistic — either the observed rewards are dense or they are sparse. This seems to stem from assuming that encoder and policy are decoupled, but this is likely false: the policy is trained given a particular encoder, and replacing the encoder with a different one will degrade performance. In any case, I wished the paper included results on the standard non-sparse environments.\n2. Similarly, I missed common baselines in offline meta-RL, e.g., MACAW (Mitchell et al., 2021) or BOReL (Dorfman et al., 2020). This is an issue because it's unclear how strong the included baselines (not specifically designed for offline RL) are against those methods, and thus limits potential interest from the community. If instead FOCAL++ was competitive with or outperformed those methods on (sparse & dense reward) tasks, this would be a strong selling point for the paper.\n3. It seems like the proposed components (Attention & MoCo) could be combined with other context-based offline meta-RL methods (e.g., offline RL^2). Is it the case? If yes, showing such results would significantly strengthen the submission.\n4. On the positive side, I appreciated the discussion on MDP ambiguity and sparse rewards (p. 9), which emphasize the importance of Attention (and MoCo) when task identification is challenging.\n\n**Theory**\n\n1. I find the results of theorems 3.1 and 3.2 misguiding. Th. 3.1 is just a matter of renaming the i-th column of $W^\\mu$ as $\\mathbb{E}_k[z^k_i]$, and I don't see a contribution there. As I understand it, Th. 3.2 simply invokes the results of Saunchi et al., 2019, so it seems the contribution is also very minor.\n2. Th. 3.3 is more interesting, especially for context-based methods in sparse-reward environments.\n\n**Minor details**:\n\n- Recent work has already used attention for meta-RL (c.f., claim in 1st bullet point of p. 3): for example, Wang et al., 2021 use a transformer-based policy to meta-learn on tasks in their proposed benchmark.\n- On p. 3, before Eq. 1: \"which usually can be factorized\" is missing \"which\"?\n\n**References**\n\n- Mitchell et al., \"Offline Meta-Reinforcement Learning with Advantage Weighting\", ICML 2021.\n- Dorfman et al., \"Offline Meta Learning of Exploration\", ArXiv 2020.\n- Wang et al., \"Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents\", ArXiv 2021.",
            "summary_of_the_review": "Overall, I think this paper proposes reasonable modifications for context-based offline meta-RL methods. But, I worry that their results focus too narrowly on FOCAL to be of wide interest to the ICLR community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to improve a context-based offline meta-RL algorithm, FOCAL, by several modifications to the context encoder’s architecture and training objective. Specifically, it adds batch-wise gated attention and sequence-wise self-attention in the encoder network, and proposes to replace the classical unsupervised contrastive loss with the InfoNCE objective. Theoretical analysis is done to show the proposed objective is closer to the supervised contrastive loss and the variance of task embedding with an optimised encoder is smaller than the one without batch-wise gated attention. Experiments on a few common meta RL benchmarks and sparsified environments show the superiority of the method over FOCAL and a few other baselines.",
            "main_review": "This paper provides a descent improvement for the SOTA offline meta RL algorithm, FOCAL. The main contribution is a combination of three existing techniques that are widely used in other domains, and therefore the novelty / significance is somewhat limited. The experiment results do show the improved test returns and reduced variance in all the six tasks, and the ablation studies are helpful to understand the benefit of each component and its advantage under distribution shift. Therefore, this work is marginally above the acceptance threshold.\n\nThe theoretical analysis is somewhat interesting, but I am not sure how much value it adds to the paper and the proof in Theorem 3.3 is not solid.\n\nFirst, the equivalence between the matrix-form momentum contrast objective and the mean classifier loss is quite straightforward by observing Eq 8 as the log-soft-max loss with the logits being the linear product of latent vectors. The main theorem 3.2 in Section 3.3 is an application of Lemma 4.3, but I don’t find any details about how it is applied. While we can say L_{sup}^\\mu (13) is closer to the supervised contrastive loss than the unsupervised contrastive loss (14), it is hard to argue that the former serves as a better surrogate simply because of that, neither (13) will learn a better task embedding than (14).\n\nSecond, I’m not sure if theorem 3.3 is rigorously proven in the appendix. A key step of the proof is Eq 31, but the argument in the text between 30 and 31 is rather vague. I do not see how 31 is derived. Even if Theorem 3.3 is true, it only shows that when the context encoder with a batch-wise gated attention is optimised, the variance of the embedding after the attention is smaller than the variance of equally weighted embedding before that layer. But it does not show whether it is smaller than the variance of the learned embedding if one optimises the original context encoder without the attention.\n\nIn the experiment section, it seems the contribution of the contrastive loss is only marginal from Table 1 and 2. How would FOCAL++ perform if we only use the batch-wise and seq-wise attention? Also, I suppose adding the two attention architecture increase the overall size of the context encoder. Will we improve the performance of FOCAL with a similar gain by using a bigger network with the original architecture?\n",
            "summary_of_the_review": "The architecture improvement on FOCAL is a good contribution in practice, while the theoretical analysis part requires some work for more solid discussion and proof.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}