{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This submission has generated sufficient debate, including some messages that, in my viewpoint, have the wrong tone. It may well be that different colleagues see the work in different ways. It is very hard to evaluate submissions in a short time and mistakes can happen. In this case, I think there were and still are misunderstandings and unclarity wrt very crucial points of the paper. This does not mean that the work overall is weak not that there is no contribution. If the content is so interesting (as discussed by authors and multiple reviewers) in some way (which it seems to be), then a better presentation and argumentation will lead to a publication elsewhere soon, but based on all the data that I have here, I recommend rejection. I see to reason to list details about the content and possible concerns, as they should be clear from the multiple messages among authors and reviewers. Best of luck."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper analysis semi-supervised methods taking into account the curation process employed in the creation of popular datasets, like ImageNet. The authors model this process via a generative model which they use to show two common objectives in semi-supervised learning, namely entropy minimisation and pseudo-labelling, are actually lower-bounds on the log-likelihood of the data under this generative model. The paper also shows experimentally, that unlabelled data improves the accuracy of classifiers when the dataset has been curated but actually hurts performance when uncurated datasets are used.",
            "main_review": "### Strengths\n- The theoretical results seem sound and connect popular semi-supervised objectives to the proposed generative model for curated datasets.\n- Using a generative model of the curation process for popular datasets in the context of semi-supervised learning is insightful, even though the model itself has already been proposed in previous work.\n\n### Weaknesses\n- The paper shows entropy minimisation and pseudo-labelling form a lower bound on the log-likelihood of the proposed generative model. However, optimising lower bounds is only useful when the bounds are tight, which does not seem to be the case here. The authors could expand their discussion on the insights these bounds provide.\n- The theoretical results motivates, to some extent, the use of semi-supervised learning when curated data is available but do not tell us much about uncurated data. We could argue that uncurated data does not match the proposed generative model, but then that is just a case of model misspecification which is a rather well-known problem and not directly connected to the theoretical results of the paper. In that light, I do not find the experimental results very insightful.\n- There is no discussion about other works that tried to explain the occasional poor performance of semi-supervised methods. I know of at least one [2] that provides theoretical results showing that unlabelled data (even when matching the distribution of labelled data) might hurt performance if the model is misspecified. They cover classifiers based on generative models, but there might still be parallels worth discussing.\n- Even though the text is well written, I do not find the exposition very clear. For instance, it was not clear if $\\theta$ refers to the parameters of 'true' data-generating process or the parameters of the classifier. Moreover, when I checked [1] for further background information, I found the exact same description, almost word for word, of the generative model for curated datasets. I find this problematic, as at best this is a bad case of text recycling.\n\n### Questions\n- In section 4.1., it is not clear how the annotators are simulated. Could the authors give more details on how the data was generated? Were the annotations just sampled from the categorical distribution defined by the 'true' neural network?\n- Still in section 5.1., how was the test data constructed? This is not entirely clear in the text.\n- Could there not be other factors influencing the poor performance when using uncurated data? For instance, uncurated data could be simply harder to model, with more ambiguous and noisier samples. Did the authors control for this somehow?\n\n### Minor Issues\n- Equation 5 seems a bit redundant. The result follows directly from the assumptions in the generative model, and this equation does not add much to the discussion.\n\n[1] Aitchison, Laurence. \"A statistical theory of cold posteriors in deep neural networks.\" International Conference on Learning Representations. 2021.\n\n[2] Cozman, Fabio Gagliardi, Ira Cohen, and Marcelo Cesar Cirelo. \"Semi-supervised learning of mixture models.\" ICML. Vol. 4. 2003.",
            "summary_of_the_review": "The paper shows that existing semi-supervised objectives are proper lower-bounds on the log-likelihood of a generative model of curated data. These are interesting and valid results, but I am not confident they are very insightful. The experiments essentially show that uncurated data is harder to model, but unless I am missing something, this is not a surprising fact in itself nor is it a direct consequence of the bounds provided. Finally, I do think taking the curation process into account in semi-supervised learning (and other fields in machine learning) is quite promising, but the main methods of the paper, namely the generative model of curated data, are not new to this work.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThis paper uses a generative model for \"curated\" labeled datasets that was initially developed by Aitchison 2020 for another seemingly unrelated purpose (explaining the \"cold posterior\" effect). The assumed task is classification, where there is a set of mutually exclusive possible class we wish to assign to each image. To obtain labels for a training set given specific images (or other example features), the generative model assumes that each example is assigned a label from S independent, identically-distributed annotators. Only if *all* annotators agree is consensus reached and the class label is provided, otherwise the image is considered \"noconsensus\" and is usually not included. It is suggested that many common SSL datasets (e.g. CIFAR-10) use such a consensus curation process, and that even unlabeled sets are subjected to consensus curation.\n\nUsing this model, they provide a principled explanation for the empirical success of several well-known semi-supervised learning (SSL) objectives -- entropy minimization, pseudo-label, and FixMatch -- applicable to discriminative deep learning. The key idea is that each objective can be viewed as a lower bound of the log likelihood of the unlabeled set under the proposed model for curation, assuming that the unlabeled set contains only \"consensus\" images.\n\nIn Sec. 4.1, the paper presents a Bayesian SSL analysis on a toy dataset. The key result is that if unlabeled data is generated without multi-annotator curation, then the unlabeled data does not improve classifier accuracy when included in training using SSL. However, if only consensus examples are included in the unlabeled set, then reasonable improvements in accuracy might be expected from SSL vs. labeled-set-only learning.\n\nIn Sec. 4.2, the paper analyzes astronomy images with 9 class labels, prepared using the Galaxy Zoo dataset (to my knowledge not a common dataset for SSL). The existence of multiple annotators for this dataset allows creating both curated and uncurated versions. Again, they find that for *curated* datasets (where many annotators agree), both test-log-likelihood and test accuracy improve from the inclusion of unlabeled examples. However, for *uncurated* datasets, including too many unlabeled examples \"dramatically worsened\" test-set performance, though small unlabeled sets help.",
            "main_review": "# Strengths\n\n* Focus on the curated aspect of common SSL benchmark datasets\n* Goal of relating objectives to a proper generative model is valuable\n* Insightful use of the same generative model to offer support for 3 different common SSL objectives arising from last 20 years of SSL research\n* Nice toy data experiments in Figure 5, showing impact of more unlabeled data on SSL methods for both curated and uncurated unlabeled sets\n\n# Weaknesses\n\n* Missing previous known connections between Pseudolabel and Entropy regularization\n\nSee W1 below.\n\n* Results on Galaxy Zoo (Fig. 6) could use more explanation: why does performance benefit from a few (not too many) unlabeled examples even in noconsensus case?\n\nSee Q3 below.\n\n\n### W1: Missing previous known connections between Pseudolabel and Entropy regularization\n\nIn the presented background in Sec. 2, entropy regularization in Eq 3 and pseudolabel in Eq 4 are presented as seemingly separate alternatives. I think the authors could do a better job explaining that pseudolabel was originally motivated by entropy minimization -- the original PseudoLabel paper dedicates all of Sec. 3.2 to explaining that \"... our method is equivalent to Entropy Regularization.\"\n\nNot a dealbreaker, but making this connection could help the audience understand why both methods might be lower bounds to the same log likelihood.\n\n\n\n# Questions for Rebuttal\n\n### Q1: What insight about which SSL objectives to prefer is offered by your lower-bound arguments?\n\nThe paper offers justification of several SSL objectives -- pseudolabel, entropy minimization, and fixmatch --- by deriving each as a lower bound of the log likelihood of the label-consensus model.\n\nHowever, there is little insight here about when each bound might be expected to be \"the best\".... does FixMatch always dominate? \n\nThis may seem a bit out of scope, but I think offering some analysis here would help practitioners in thinking about which methods to apply in which circumstances.\n\n\n### Q2: Questions about Fig. 6\n\nWhat do \"exact-\" and \"pseudo-\" prefixes mean here? Can you adjust caption in revision to clarify?\n\n### Q3: Results on Galaxy Zoo (Fig. 6) could use more explanation\n\nIn Fig. 6, seems like both log likelihood and accuracy improve *slightly* with modest unlabeled data in uncurated case, but then decline with large unlabeled data. The authors acknowledge this in the paper by saying \" though the inclusion of a small number of unlabelled points gave very small performance improvements (Fig. 6CD)\".\n\nIs there any insight about why the curves look U shaped here, rather than a strict decline as the theory might suggest? Perhaps the data augmentation related to FixMatch used here is driving this? Would the same trend be seen if we just used entropy minimization?\n\n\n# Minor Presentation Comments\n\nNo need to mention these in rebuttal, but I hope you consider them to improve the paper\n\n### Comments on Figure 2\n\n* I felt Fig. 2 could communicate more than it did. Perhaps in the right-most panel that shows your consensus model, you could somehow visually emphasize that for unlabeled examples, you are assuming that \"consensus\" was reached, and thus that the distribution $p(\\theta | X)$ does not reduce to the prior marginal $p(\\theta)$ under the assumed model?\n\n### Superscript Notation could be improved\n\nThis is somewhat minor and perhaps a personal nitpick, but I dislike the notation $p^S_y(X)$ in Eq. 2. \n\nI'd prefer to see it like this:\n\n$$\n( p_y(X) )^S\n$$\n\nThis makes it clear that the quantity is raised to the $S$-th \npower. The current notation is a bit less clear to me on that front.\n\n\n### Lots of use of Y_s without defining $s$\n\nIn many cases, the paper writes $P( Y_s = y | X, \\theta)$ without defining a value for the index $s$ (as an example, see Eq. 2).  I guess I found this a little confusing, since elsewhere $s$ requires a concrete value to be well understood. I'd rather there was some explicit comment like \"where s can be any value in 1 to S\" or instead some other notation.  \n\n\n### Coments on Figure 3\n\n* Why in Figure 3 are points labeled \"bus\" and \"train\"? Aren't these just toy examples?\n* Not sure that both panels (3A and 3B) are needed in this figure. Could use just one, and verbally explain how unlabeled data would be selected. Or, at very least, the second panel could remove the noconsensus points, to help drive home visually the separation that occurs in the unlabeled set.\n\n\n### Comments on Sec. 4.2\n\nCurrent text says: \"data-curation based theory predicts that SSL should be much more effective on curated than uncurated data\".\n\nI'd clarify that the theory only applies to *SSL methods using the low-density separation principle*. Other SSL principles (e.g. those based on generative models) aren't applicable.",
            "summary_of_the_review": "I recommend accepting the paper. First, it draws attention to an important (yet under-studied) aspect of common SSL datasets like CIFAR-10: they are heavily curated and thus their so-called \"unlabeled\" sets are really just labeled sets with labels redacted. Second, it offers insight into why several widely-used SSL methods (entropy regularization, pseudolabel, and FixMatch) work, by framing each as a lower bound of a log-likelihood for a generative model for how many annotators produce consensus. I thought this was insightful and backed by nice toy data experiments and real data experiments (perhaps the curated vs uncurated versions of GalaxyZoo used here can become nice benchmarks for future SSL methods).\n\nIn the rebuttal and revisions, I hope the authors can address my questions about experimental results and why the bounds say about when each SSL method might be preferred over alternatives.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper shows that low-density separation semi-supervised learning (SSL) objectives can be understood as a lower-bound on a log-probability that arises from a principled generative model of data curation. This gives a theoretical understanding of recent results showing that SSL is more effective when unlabelled data is obtained by throwing away labels from the carefully curated training set, and is less effective when unlabeled data is taken from uncurated images. Experiments on toy data generated from a known model and on real data from Galaxy Zoo confirm the importance of data curation for SSL.",
            "main_review": "Minor comments (language mistakes, etc.):\nFigure 1 caption: The first ``annotators\" should have its first letter capitalized.\nSection 2.1: \"ensure sure\" should be \"ensure\".\nSection 4.1: ``Fig. 5AB\" and ``Fig. 5CD\" should be written as ``Fig. 5A-B\" and ``Fig. 5C-D\" (i.e., with a hyphen inserted between).",
            "summary_of_the_review": "The paper is generally very well written, clearly presented, and a pleasure to read. The problem is well-motivated and well-defined with respect to the background. The technique look sound and novel.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper is built on the previous work (Aitchison, 2021) that provides a generative model of data curation. In this model, the likelihood of a labeled data obtained with consensus is given by product of the probability that each labeler labels correctly. Under this model, the paper shows 1) the log likelihood of the consensus is lower bounded by entropy or pseudo labeling, 2) the log likelihood of the consensus is also lower bounded by the FixMatch type average likelihood of the augmented data. In toy experiments and Galaxy Zoo experiments, authors show that the test likelihood improves when unlabelled curated data is added, but decreases on the uncurated data.",
            "main_review": "Under the generative model of data curation, the paper shows 1) the log likelihood of the consensus is lower bounded by entropy or pseudo labeling, 2) the log likelihood of the consensus is also lower bounded by the FixMatch type average likelihood of the augmented data. These results are potentially interesting. \n\nIn toy experiments and Galaxy Zoo data experiments, authors show that the test likelihood improves when unlabelled curated data is added, but decreases on the uncurated data. However, this is not directly the consequence of the theoretical results. I am not clear on this connection. The theory shows that log likelihood is an upper bound on the entropy, pseudo labelling loss under the generative model of curated data. It does not say what happens when the data is uncurated. Also, I am not clear how the authors measured the test likelihood.\n\nBesides, the technical contribution, both theoretical and experimental are limited. On the experiments, it would be nicer to see the consequences of the theoretical results on modern semi supervised learning approaches and datasets they use.\n\n",
            "summary_of_the_review": "The theoretical and experimental contribution of the work are not enough to justify publication at the current form.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}