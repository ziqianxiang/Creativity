{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents a reinforcement learning technique for problems with continuous actions. The proposed approach consists in learning  a discretization of continuous action spaces from human demonstrations. This discretization returns a fixed number of actions for each input state. By discretizing the action space, any discrete action deep RL technique can be readily applied to the continuous control problem. Experiments reported in the paper show that the proposed approach outperforms several RL baselines such as SAC.\n\nThe key criticism from the reviewers relates to the incremental nature of this paper's contribution. While the precise equation proposed by in this paper for learning discrete actions from demonstrations may be novel, there have been several very similar techniques in the literature. For example, Gaussian Mixture Models (GMMs), a closely related model, have been widely studied in the context of learning policies from demonstrations. \n\nIn summary, the reviewers are not convinced that the paper contains sufficiently novel ideas for an ICLR publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper looks into improving data efficiency in continuous action space reinforcement learning problem by connecting it to its discrete action space correspondence and taking advantage of demonstration data. To be specific, the paper proposes learning a state dependent discretization of the original continuous action space by mimicking actions in demonstration trajectories. This discretization output a fixed number of actions for each input state. In a grid world example, it is visualized that the proposed method learns multimodal demonstrated action better as it can catch different actions in the same state. In the experiment section, the proposed method is compared to baselines in various demonstration and reinforcement learning setting and shows benefit over compared baselines.",
            "main_review": "This paper explores the connection between the continuous actions space and discrete actions space, proposes an automatic discretization. This idea is interesting, the writing is clear and easy to follow. But I have a few concerns on this paper:\n\n(1) The benefit of proposed method relies on the preference of discrete action space over the continuous action space, as it mainly provides an architecture to output multiple continuous actions and evaluate them with discrete action based Q-learning methods. However, this preference is not discussed fully in the paper. The most relevant discussion in the paper is practical comparison between proposed approach and SAC, a continuous action space based baseline. Though the practical performance of the proposed discretization looks better, it is not clear if this advantage is task specific. More analysis on this part could better support the motivation of this paper.\n\n(2) The role of the temperature T of equation (1) is not very clear. The paper states that this hyperparameter controls whether all output actions are similar to the demonstrated action or only one of them is similar to the demonstrated action. But it is not convincing enough. The proposed loss function sums up losses from all output K actions, and the temperature T is the same for all actions and hence does not seem to control significance of any individual action. As a result, the temperature seems to only control the scale of the loss.\n\n(3) Another concern is also related to the proposed loss. The proposed loss does not seem to provide guidance on the diversity of output actions. As they are bound to the same demonstrated action, it is difficult to tell how it can output multiple different actions for the same input. If the model always output similar actions in each candidates for the same input, the significance of the proposed design can be strongly weakened. Because the proposed loss is exactly the ordinary BC when K = 1, as the paper mentions.\n\n(4) As the supplementary (A) section shows, the proposed architecture looks very similar to the Gaussian mixture models. However, the paper does not compare ordinary Gaussian mixture models with the proposed method. The usage of Guassian mixture models in reinforcement learning and imitation learning problem does not seem to be novel, as we can find it in some previous works:\n\nChernova, Sonia, and Manuela Veloso. \"Confidence-based policy learning from demonstration using gaussian mixture models.\" Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems. 2007.\n\nAgostini, Alejandro, and Enric Celaya. \"Reinforcement learning with a Gaussian mixture model.\" The 2010 International Joint Conference on Neural Networks (IJCNN). IEEE, 2010.\n\nChoi, Yunho, Kyungjae Lee, and Songhwai Oh. \"Distributional deep reinforcement learning with a mixture of gaussians.\" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.\n\nThen the novelty of proposed method may also strongly depends on how it is better than related previous works. The novelty and significance of this paper can be shown clearer if convincing analysis and comparison to previous Gaussian mixture models works are provided.",
            "summary_of_the_review": "The paper provides an interesting idea on connecting discrete action space and continuous action space for reinforcement learning and imitation learning. The writing is clear. But some important discussion seems to be insufficient and hence its novelty and significance is not clear enough.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes learning a state-dependent discretization of a continuous action space using demonstrations. The motivation is to enable the use of discrete-action deep RL methods instead of relying on policies with continuous action space parametrization. The authors argue that this results in improved exploration (avoiding the curse of dimensionality). The approach is evaluated on 5 environments in the Adroit and Robodesk environments on Reinforcement Learning with demonstrations, Reinforcement Learning with play data, and Imitation Learning. The authors claim that they outperform state-of-the-art continuous control methods both in terms of sample efficiency and performance on every setup.",
            "main_review": "The problem setting is interesting, timely, and could have a good amount of impact should the technique be adopted. Please consider the following comments;\n\n- The authors “reproduced” the SAC implementation. I am not fully convinced that the implementation is competitive to the original. While running a HP sweep is commendable, it is not enough to ensure that the baselines work as intended, especially in exploration tasks that can have a tremendous impact. One would need to (a) ensure that the implementation can reproduce the results in the original paper in the original environments (mostly Mujoco/Dm control suite/openai gym) and (b) run a number of experiments of Aquagail in these environments to compare performance. While I agree that it might be harder to generate meaningful play data, I do think that this shouldn’t be a problem for the imitation learning experiments. \n\n- Related work on exploration in continuous action spaces is largely missing. While exploration is not easy in this domain, there exists a large body of literature (all the different types of curiosity mechanisms, UCB inspired methods etc.) [2-6] If the approach claims superiority of discrete action spaces over continuous ones, a baseline with principled exploration in continuous action spaces is necessary.\n\n- Why not compare the approach against a hierarchical Rl/Imitation learning approach or a hybrid continuous-discrete baseline? E.g. [1] The authors also claim that this is the most related approach.\n\n- The approach states: “Our objective is to reduce a continuous control problem to a discrete action one, on which we can apply discrete-action RL methods.” I believe this alone is already a great contribution and it is not necessary to frame it as an advantage over continuous action-space RL. The paper makes a number of somewhat handwavy claims why discrete action spaces are superior but while they may sound intuitive they are not substantiated by theory or experiments.\n\n- How does the approach compare to state-of-the-art in offline RL, which provides a similar problem setting?\n\n- 10 seeds is amazing and I would love to see more papers following this direction. Nonetheless, it is not clear what the shaded regions in the plots mean. Is this the 1 standard deviation interval or a confidence interval based on the t-test? What p-value of the confidence interval is chosen? I agree that this is a minor detail point but it should be mentioned.\n\n\n[1] Neunert, M., Abdolmaleki, A., Wulfmeier, M., Lampe, T., Springenberg, T., Hafner, R., ... & Riedmiller, M. (2020, May). Continuous-discrete reinforcement learning for hybrid control in robotics. In Conference on Robot Learning (pp. 735-751). PMLR.\n\n[2] Osband, I., Blundell, C., Pritzel, A., & Van Roy, B. (2016). Deep exploration via bootstrapped DQN. Advances in neural information processing systems, 29, 4026-4034.\n\n[3] Osband, I., Van Roy, B., Russo, D. J., & Wen, Z. (2019). Deep Exploration via Randomized Value Functions. J. Mach. Learn. Res., 20(124), 1-62.\n\n[4] Chen, R. Y., Sidor, S., Abbeel, P., & Schulman, J. (2017). UCB exploration via Q-ensembles. arXiv preprint arXiv:1706.01502.\n\n[5] Seyde, T., Schwarting, W., Karaman, S., & Rus, D. (2020). Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles. 5th Conference on Robot Learning (CoRL)\n\n[6] Bai, C., Wang, L., Han, L., Hao, J., Garg, A., Liu, P., & Wang, Z. (2021, July). Principled exploration via optimistic bootstrapping and backward induction. In International Conference on Machine Learning (pp. 577-587). PMLR.\n\n[7] Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017, July). Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning (pp. 2778-2787). PMLR.\n",
            "summary_of_the_review": "The paper presents an interesting framework for learning the discretization of action spaces. Related work needs to be extended to include exploration, the claims of benefits of discrete over continuous action spaces have to be substantiated, the evaluation has to be done on fair baselines and commonly used environments to allow the comparability. I am happy to adjust my rating should these issues be addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for discrete action-space learning from originally continuous action spaces using demonstrations. In other words, the aim is to utilize demonstration data so as to find a set of K discrete actions in a given state. If expert demonstrations represent an individual greedy optimal policy, the demonstrations sufficiently cover the state space, and the demo-generating optimal policy and the learning agent's policy are fully observing of the state (no partial observability), then K=1 can be sufficient to yield the greedy demo-generating optimal policy. However, in reality, these assumptions are hardly ever fulfilled. In addition, the demonstration data may not even come from an *optimal policy* for a task (e.g. human demonstration is generally suboptimal), or be w.r.t. to any specific downstream task in mind at all (*free play*). In such general cases, learning a discrete subspace of the continuous action space is quite useful. First, it allows us to apply discrete-action methods instead of continuous-action ones (which have been recently shown to be better even without incorporating demonstration data). Second, the reduced action space makes exploration significantly easier. Regarding the latter, this method can be seen as learning a hard bias on exploration (removing a subset of actions from the original downstream action space) and only using those used more frequently in demonstrations. The method used to achieve this is similar in spirit to Behavioral Cloning (BC) and, in fact, becomes equivalent to BC in the extreme case of K=1.  ",
            "main_review": "## Strengths: \n1) Repurposing BC and extending it to serve as a discrete action-space learner is interesting.\n2) Paper is generally well written, and easy to follow. \n3) Choices of methods to combine the proposed approach with and the environments to test in are reasonable. \n4) The breadth of discussing the related work is reasonable. \n\n--- \n## Weaknesses:\n1) Some important missing baselines (some of which are referred to briefly but somewhat dismissed in experiments). \n2) Some missing related works.  \n3) Presentation at some points is misleading or incomplete.\n4) Some discussions missing. \n\n---\n## Issues:\n- **Sec. 1, paragraph 2** \"*[in finite discrete action spaces] exploration is more manageable [and] computing the maximum of the action-value function is straightforward*\"**:** A finite discrete action space can have a much higher intrinsic complexity than a continuous action space. As such, I wouldn't say that in general exploration is more manageable. For instance, finite but large combinatorial action spaces can be much harder to explore with action-composite methods (such as standard DQN) as opposed to when a continuous embedding can be assumed and exploited using PG methods (see, e.g., Dulac-Arnold et al. (2015)). In addition, the exponential complexity of maximization in combinatorial discrete action spaces can fast become computationally intractable --- it isn't straightforward in general.  \n\n- **Sec. 1, paragraph 2:** The end of this paragraph seems to imply two algorithmic choices for continuous-action control: (1) MC/Reinforce-style, (2) Q-learning-style. This could be written much better in my view to encapsulate the full landscape of approaches better (e.g. actor-critic methods). Otherwise, clarify what is meant.\n\n- **Bang-bang controller:** BB controller implies a very specific type of discretization scheme, where only the extrema of the action space are used in the control problem. However, in this paper, you use this to refer to the generic uniform discretization scheme. Not sure if referring to this scheme as BB is really correct or helpful. \n\n- **Sec. 1, paragraph 3:** Andrychowicz et al. (2020) does not assume a causal dependence between action dimensions as far as I know. If they do, please explain in your response. Also, I believe this paragraph is a good place to also refer to approaches that assume independence between action dimensions: (Tavakoli et al., 2018; Tang and Agrawal, 2020; Andrychowicz et al., 2020).\n\n- **Sec. 1, paragraph 3** \"*[...] but they are typically complex and task-specific.*\" **:** Most of these approaches are trivial to implement. Regarding task-specificity, they are no more task-specific than standard model-free methods such as DQN and DDPG are. Regarding being complex, could you clarify in what respect it is meant? Regarding task-specificity, I personally understand that your method relies less on the task specification, but do state more clearly for the reader in what way your method is less task-specific. \n\n- **Sec. 2, paragraphs 1-2:** *r* is used to denote both a *random variable* (when discussing the Q-learning update rule with sampled transitions) and the *reward function* (e.g. when expressing the recursive form of $Q^\\pi$ and $V^\\pi$). Also, while often in papers it is omitted that by \"reward function\" in fact the *expected* reward function is meant, it is useful to state that explicitly.  \n\n- **Sec. 4.1, paragraph 2 (last line):** Action dimensionality of 24 is manageable by all following approaches: \n  - Factored DQN methods (Metz et al., 2017, Tavakoli et al., 2018) and Refs. [2, 3].\n  - Factored PG methods (Tang and Agrawal, 2020).\n\n- **Sec. 4.3, Evaluation & results** \"*Interestingly, the performance of the BB agent decreases with the discretization granularity, well exemplifying the curse of dimensionality of the method.*\" **:** This has been discussed in Ref. [2], where exploiting compositionality has also been shown to be key in solving this problem. At the end of the same paragraph, again it is mentioned that the reason why uniform discretization is not used is that it is not possible to run them in high-dim. action spaces. These statements seem to ignore the line of works discussed at the beginning of your paper's Related Work. Is there a reason for this?\n\n- **Sec. 5, paragraph 1:** \n  - Insert (Tang and Agrawal, 2020): \"*[...] action dimensions are independent (Tavakoli et al., 2018; **Tang and Agrawal, 2020**, Andrychowicz et al., 2020 [...])*\".\n  - Remove (Tang and Agrawal, 2020) from *autoregressive discretization* citations.\n  - After referencing methods that assume independence or those assuming some specific causal dependence, refer also to methods that learn the causal dependence (Ref. [2,4]).     \n\n---\n## Questions:  \n1. Please respond to the questions posed in the *Issues* section of my review.    \n\n2. Why not compare with action representation learning methods (see, e.g., Ref. [1])? This could be quite a valuable comparison, and several possibilities could be explored.\n\n3. Why only try the discrete-action baseline in the low-dimensional case of the Robodesk domain? As you have noted in the related work, there are now a few approaches for scaling discrete-action Q-based and PG methods to high dimensional problems; why not try them?\n\n4. **Figure 6:** How does the tuning process on Relocate work?\n\n5. What is the difference between *Success Rate* and *Sparse Reward Returns* in Fig. 5? Given that sparse rewards are 1 if the goal is achieved, I expected these two to be the same. \n\n6. **Table 8 (and experiments in general):** Choosing hyperparameters that perform best on average across all tasks (including test ones) is not fair. Am I misunderstanding something here, or is there a reason why this makes sense here?\n\n7. Could you position your work w.r.t. the line of work of action-embedding/action-representation/latent-action learners (e.g. in the context of Refs. [1,5])? \n\n--- \n### References:\n\n[1] Chandak, Y., Theocharous, G., Kostas, J., Jordan, S., Thomas, P. (2019). Learning Action Representations for Reinforcement Learning. In *Proceedings of the 36th International Conference on Machine Learning*.\n\n[2] Tavakoli, A., Fatemi, M., Kormushev, P. (2021). Learning to Represent Action Values as a Hypergraph on the Action Vertices. In *Proceedings of the Ninth International Conference on Learning Representations*. \n\n[3] Van de Wiele, T., et al. (2020). Q-learning in enormous action spaces via amortized approximate maximization. *arXiv preprint arXiv:2001.08116*.\n\n[4] Sakryukin, A., Raissi, C., Kankanhalli, M. (2020). Inferring DQN structure for high-dimensional continuous control. In *Proceedings of the 37th International Conference on Machine Learning*.\n\n[5] Merel, J., et al. (2019). Neural probabilistic motor primitives for humanoid control. In *Proceedings of the Seventh International Conference on Learning Representations*.\n\n\n",
            "summary_of_the_review": "I generally like this paper: the proposed method is sound and useful, experiments are insightful and nicely designed, and good discussions around limitations (e.g. pruning optimal actions) are present.\n\nHowever, the paper makes assertions here and there that are not entirely true (e.g. while related work covers a group of methods for addressing the curse of dimensionality, on various occasions this line of work is dismissed and a limited set of baselines are used). Baselines that use uniform discretization and can scale to high dimensions (regarding computation and generalization capabilities) by exploiting compositionality in multi-dimensional spaces are simple to try and would be very informative. \n\nAlso, discussion and comparison with an important class of methods (*action representation learning* methods) are missing. I referred to one important work in this direction (Ref. [1]), but I'm sure there is more on this topic under different names (e.g. latent actions, action embedding learning).  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. As such, one can apply any discrete action deep RL algorithm to continuous problems. In three scenarios: RL with demonstration, RL with play data and imitation learning, the effectiveness is validated. ",
            "main_review": "* This paper proposed an action space discretization method, it claims to not suffer from the curse of dimensionality and not needing any specific assumptions about the task. I agree with the author that this is an important topic that RL community should keep an eye on. However, I might not agree that discrete control problems are harder than continuous control problems. I think the argument in the introduction that since continuous control problem is much harder than discrete ones, so we should turn a continuous problem into a discrete problem is debatable. \n\n* The main idea is to capture the mode of continuous actiosn taken under different states through a Gaussian Mixture Model type of neural network. The key novelty is Equation 1. where Figure 2 explains the key idea very well. \n\n* On the empirical side, the author has considered three settings: RL with demonstration, RL with play data, and imitation learning. The significance of the results is strong.  ",
            "summary_of_the_review": " I think the technical side of the story is sound, the method taken is reasonable and the empirical results are valid. However, I think the novelty of this paper is limited. The idea is rather simple, and learning discretization through a gaussian mixture model type of representation is trivial. I am not quite convinced at the current stage that it reaches an ICLR level of work yet. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}