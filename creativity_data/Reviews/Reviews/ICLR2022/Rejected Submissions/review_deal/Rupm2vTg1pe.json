{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper extends the Contextual Graph Markov Model, a deep unsupervised probabilistic approach. The key idea is to leverage Hierarchical Dirichlet Processes to automatically determine each layer's latent representation's size. The paper conducts experiments on graph classification tasks to show the superiority of the proposed method.\n\nStrength\n* A new method is proposed.\n* The proposed method appears to be sound.\n* Experiments are conducted to demonstrate the effectiveness.\n\nWeakness\n* The novelty and significance of the work are not enough.\n* The improvements on existing methods are not significant.\n* The proposed method is also not so general.\n\n-----------\n\nAfter rebuttal\n\nReviewer ynws, who gave the highest score, says\n\n“I agree with the overall review of the paper by other reviewers. The proposed method is limited to the CGMM model and not generic enough to extend to other more popular graph neural networks. The improvements don't seem to be significant enough as well.”"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is an extension of the Contextual Graph Markov Model, a deep unsupervised probabilistic approach for modeling graph data. The key idea is to leverage Hierarchical Dirichlet Processes, which enables the proposed approach to automatically choose the size of each layer’s latent representation. The authors conduct experiment on graph classification tasks, and the results are quite promising.\n",
            "main_review": "Strengths:\n\n1. The idea of using Hierarchical Dirichlet Processes is quite intuitive.\n\nThe key idea of the paper is to use HDP to automatically decide the size of latent representations in the Contextual Graph Markov Model. To me, the idea of using HDP is intuitive.\n\n2. The proposed model is theoretically sound.\n\nFor Hierarchical Dirichlet Processes, one critical problem is inference, and the authors use Gibbs sampling to deal with that as described in appendix, and I think the derivation is theoretically sound.\n\nWeaknesses:\n\n1. The novelty of the paper is insufficient.\n\nThe key idea is on using HDP in the Contextual Graph Markov Model. However, the idea is not new, as it is a quite common practice to use HDP in unsupervised generative models for automatic model selection. For example, in clustering, HDP has been widely used to automatically decide the number of clusters. Similarly in topic modeling, many works use HDP to decide the number of latent topics. Given these well-known efforts in machine learning, I think the idea of extending the Contextual Graph Markov Model with HDP is not so innovative.\n\nAlso, this paper uses Gibbs sampling for inference, which has also been widely used for HDP, and the Gibbs sampling procedure derived in this paper is not so different from those used in existing works. In this sense, this paper doesn't provide many new insights or ideas in terms of model inference.\n\nTherefore, although the paper has some interesting ideas, I think the overall novelty is insufficient.\n\n2. The improvement over existing methods is not significant.\n\nIn the experiment, the authors compare iCGMM against many existing methods on the task of graph classification. However, on chemical datasets, the results of iCGMM are worse than a few baseline methods (e.g., GIN and DiffPool). On social datasets, the improvement over these baseline methods is also insignificant, given the high standard deviation. In particular, iCGMM only slightly outperforms CGNN, making the advantage of applying HDP to CGMM less convincing.\n",
            "summary_of_the_review": "In summary, this paper applies HDP to CGMM, which is intuitive and theoretically sound. However, the idea of using HDP is not new in unsupervised generative models, and the improvement of iCGMM over CGMM seems insignificant. Thus, I would lean towards a weak reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors propose a Bayesian Non-Parametric (BNP) method to automatically learn the \"structure\" of the graph neural network. Specifically, the authors have proposed a Hierarchical Dirichlet Process model for the automatic inference of latent code's size. ",
            "main_review": "The proposed method is very interesting, since it's an unsupervised method for automatic selection of network structure, as opposed to the more popular supervised AutoML techniques. Traditional inference mechanisms for BNP-based methods are costly to run on larger datasets, hence authors propose a faster Gibbs sampling-based inference method. \n\nThe proposed method performs impressively against supervised learning-based methods and the parametric version of the method, i.e. CGMM. Given the unsupervised nature of the method, the results are encouraging for future work in the direction of BNPs. \n\nI lean towards acceptance of this paper, as it has the potential to encourage research on the integration of BNP methods with Deep Neural Networks. ",
            "summary_of_the_review": "The authors propose an unsupervised Bayesian Non-Parametric method for GNN, which learns the GNN structure in an unsupervised fashion. For scaling-up, the authors propose a faster Gibbs Sampling based inference method. Experimental results indicate the strong performance of the method as compared to supervised learning methods. Overall, the direction is very interesting and could inspire future work at the intersection of BNP and Neural Networks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a mechanism to automate the size selection of each latent representation layer of the Contextual Graph Markov model. The model automatically adjusts the size of the model parameters mitigating the expensive model selection. Moreover, the authors introduce some techniques to scale the \nproposed solution.",
            "main_review": "Strengths:\n1. The paper is well written and easy to follow.\n2. Selecting an optimal model size (trainable parameters of each neural network) is essential to obtain reasonable performance. However, it involves an expensive model selection procedure. This work investigates automatic model size selection without compromising model performance. \n3.  Experimenting with chemical and social network datasets, the authors show the method perform comparable performance with the state-of-the-art baselines for graph classification problem.\n\n\nWeakness:\n1. However, the proposed method is restricted to a specific type of architecture, which is the Contextual Graph Markov model. Though the authors discussed why they have selected this architecture, it somewhat limits the applicability of the approach.  Other than CGMM, there exist powerful adaptive graph representation techniques like GraphSage. Can their proposed method be extended to automate model size selection for those neural models?\n\n2. Please discuss how likely it is to be extended to unsupervised generative model-based (VAE) techniques to obtain a representation of a graph?\n\n3. The authors should provide some comparison with AutoGNN /DGN types of methods in terms of performance accuracy and time of execution.\n\n4. The authors should provide execution time comparison with all existing baselines instead of only their model. Because the time overhead should not be significantly high to reduce the model selection overhead.\n\n5. They should provide the analysis of graph size versus the size of the latent variable, is there any interesting pattern or observation?",
            "summary_of_the_review": "This work investigates automatic model size selection without compromising model performance, which is a very important and timely research question. \n\nHowever, the proposed solution revolves around a specific architecture that limits its applicability. Also, it does not consider the edge properties of graphs, which are extremely important for different graphs like crystal (CGCNN - Phys. Rev. Lett.). \n\nMore model ablation studies for e.g graph size (or structural complexity like wiener index/diameter/density) vs learned latent variable size are required.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper combines the idea from the classical hidden dirichlet processe and CGMM to automate the process of selecting the hyperparameters of CGMM.",
            "main_review": "The authors propose an interesting idea of utilizing the HDP framework to obtain the number of latent variables at each layer. HOwever the gain in the accuracy seems either non-existent or marginal.",
            "summary_of_the_review": "Interesting idea but not very impressive experimental results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}